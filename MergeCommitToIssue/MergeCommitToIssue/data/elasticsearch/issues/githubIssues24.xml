<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Node Stats API: Change the structure of the response (more structured)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/746</link><project id="" key="" /><description>Node Stats API: Change the structure of the response (more structured)
</description><key id="645201">746</key><summary>Node Stats API: Change the structure of the response (more structured)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.16.0</label></labels><created>2011-03-03T22:19:37Z</created><updated>2011-03-03T22:20:15Z</updated><resolved>2011-03-03T22:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-03T22:20:15Z" id="832096">Node Stats API: Change the structure of the response (more structured), closed by f901429aaa1030fe1b51d5fd8b1786bf1e6398eb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices Status / Node Stats: Add (Lucene) index merge stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/745</link><project id="" key="" /><description>Add lucene level merge stats, how long they took, how many ran, and how many are currently running.
</description><key id="645107">745</key><summary>Indices Status / Node Stats: Add (Lucene) index merge stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-03T21:53:00Z</created><updated>2011-03-03T21:53:38Z</updated><resolved>2011-03-03T21:53:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-03T21:53:38Z" id="831981">Indices Status / Node Stats: Add (Lucene) index merge stats, closed by 682ad7e2fc69ff2377de11bdd930b39bce1bada2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: flt_field does not allow to set min_similarity and prefix_length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/744</link><project id="" key="" /><description>Query DSL: flt_field does not allow to set min_similarity and prefix_length
</description><key id="642246">744</key><summary>Query DSL: flt_field does not allow to set min_similarity and prefix_length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-03T02:16:36Z</created><updated>2011-03-03T02:17:09Z</updated><resolved>2011-03-03T02:17:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-03T02:17:09Z" id="828706">Query DSL: flt_field does not allow to set min_similarity and prefix_length, closed by 2909060af8f72c70acf8ee5b8a358fabeea0d605.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>All Field: All field can fail to analyze input data (rare cases)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/743</link><project id="" key="" /><description>All Field: All field can fail to analyze input data (rare cases)
</description><key id="642116">743</key><summary>All Field: All field can fail to analyze input data (rare cases)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-03T00:59:03Z</created><updated>2011-03-07T07:10:28Z</updated><resolved>2011-03-03T08:59:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-03T00:59:35Z" id="828556">All Field: All field cane fail to analyze input data (rare cases), closed by 78c6f951a31d08dcdad94326fcafef8b32c9d213.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete API: Allow to set _parent on it (will simply set the routing value)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/742</link><project id="" key="" /><description>Delete request does not accept a `parent` parameter. It does accept `routing`, and thats what one should set with the parent value, but, it will be nice to have the ability to set the `parent` as well (which will simply set the `routing` value).

This also applies to the bulk API with the `_parent` parameter being used for delete iterms.
</description><key id="641291">742</key><summary>Delete API: Allow to set _parent on it (will simply set the routing value)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-02T19:59:19Z</created><updated>2011-03-02T20:15:14Z</updated><resolved>2011-03-03T04:14:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T20:14:57Z" id="827584">Delete API: Allow to set _parent on it (will simply set the routing value), closed by 4631df9d0111f20116a8f2d631ca67f368d33d33.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bulk delete error occurs on delete of child</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/741</link><project id="" key="" /><description>Example at https://gist.github.com/851188

when trying to bulk delete a child, I am getting a versioning mismatch error even though a version is not specified. The example works correctly the first or second time on a fresh install (when deleting the index between, performing a refresh and waiting for green status on the cluster health), but after that the following error occurs on the delete:

{"took":3,"items":[{"delete":{"_index":"err_test_real","_type":"results","_id":"8a08dfb3af854f07b72b04b977f27f2a:81","error":"VersionConflictEngineException[[err_test_real][0] [results][8a08dfb3af854f07b72b04b977f27f2a:81]: version conflict, current [-1], required [2]]"}}]}
</description><key id="641017">741</key><summary>bulk delete error occurs on delete of child</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mpaul</reporter><labels /><created>2011-03-02T17:53:20Z</created><updated>2013-04-04T18:44:29Z</updated><resolved>2013-04-04T18:44:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T20:01:28Z" id="827544">Heya, issue opened: #742. For now, you can set the parent value in the bulk request to the `_routing`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Last modification date of index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/740</link><project id="" key="" /><description>It would be nice if the status of an index would provide a last modification date.
For example this could be useful when updating the index with changes since the last update.
</description><key id="640452">740</key><summary>Last modification date of index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gregorko</reporter><labels /><created>2011-03-02T14:36:46Z</created><updated>2011-04-12T07:59:12Z</updated><resolved>2011-04-12T07:59:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T19:12:45Z" id="827388">If you store a timestamp in your docs, you can run a stats facet on all docs to get the highest value.
</comment><comment author="gregorko" created="2011-04-12T07:59:12Z" id="988191">Cool.. works very good! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Embedded ES: NPE after re-creating an index and indexing a file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/739</link><project id="" key="" /><description>When starting an embedded ES node:

```
    NodeBuilder nodeBuilder = nodeBuilder();
    nodeBuilder.settings()
            .loadFromClasspath("elasticsearch.yml")
            .put("gateway.type", "none").put("index.store.type", "memory");
    node = nodeBuilder.loadConfigSettings(false).local(true).node();
    client = node.client();
```

Deleting an index, re-creating it and indexing a file:

```
client.admin().indices().delete(deleteIndexRequest(indexName)).actionGet();
client.admin().indices().create(createIndexRequest(indexName)).actionGet();
client.prepareIndex(indexName, type, id)
            .setSource(createIndexJSONForFile(file))
            .setOperationThreaded(false)
            .setListenerThreaded(false)
            .execute()
            .actionGet();
```

I'm getting an NPE (see below).
My configuration file:

```
index:
  mapper:
    dynamic: false
```

Tested with 0.14.3 and 0.15.1.
Works fine if the index isn't deleted and re-created.

```
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
at org.elasticsearch.common.util.concurrent.AbstractFuture$Sync.complete(AbstractFuture.java:320)
at org.elasticsearch.common.util.concurrent.AbstractFuture$Sync.setException(AbstractFuture.java:297)
at org.elasticsearch.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:129)
at org.elasticsearch.action.support.AdapterActionFuture.onFailure(AdapterActionFuture.java:87)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:429)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:335)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:265)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.doExecute(TransportShardReplicationOperationAction.java:106)
at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:131)
at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:118)
at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:71)
at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:61)
at org.elasticsearch.client.node.NodeClient.index(NodeClient.java:120)
at org.elasticsearch.client.action.index.IndexRequestBuilder.doExecute(IndexRequestBuilder.java:315)
at org.elasticsearch.client.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:56)
at org.elasticsearch.client.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:51)
... 39 more
Caused by: java.lang.NullPointerException
at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:270)
at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:180)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:418)
... 50 more
```
</description><key id="639843">739</key><summary>Embedded ES: NPE after re-creating an index and indexing a file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamw</reporter><labels /><created>2011-03-02T08:44:00Z</created><updated>2011-03-03T08:44:56Z</updated><resolved>2011-03-03T08:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T18:18:48Z" id="827201">The exception is not nice, but basically you disable dynamic mapping, and type to index a type that has no mapping.

p.s. Questions are better asked on hte mailing list, I try to keep the issues for issues (that has been partially verified) / feature requests.
</comment><comment author="adamw" created="2011-03-03T08:44:56Z" id="829479">I was just going to write that I do create the mappings, then I spotted then I don't after removing and re-creating the index - so again an error on my side.

Sorry for the trash, and I'll use the forum next time :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot index using java API 0.15.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/738</link><project id="" key="" /><description>When running an index request I get an exception (see below).

The client is obtained in the following way:

```
    new TransportClient().addTransportAddress(new InetSocketTransportAddress("localhost", "9300"));
```

This worked fine in 0.14.3. Using the REST api indexing works fine.

```
    Caused by: org.elasticsearch.transport.RemoteTransportException: [Baron Mordo][inet[/127.0.0.1:9300]][indices/index/shard/index]
    Caused by: java.io.EOFException
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:112) [:]
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readByte(HandlesStreamInput.java:80) [:]
    at org.elasticsearch.common.io.stream.StreamInput.readInt(StreamInput.java:64) [:]
    at org.elasticsearch.common.io.stream.StreamInput.readLong(StreamInput.java:87) [:]
    at org.elasticsearch.action.index.IndexRequest.readFrom(IndexRequest.java:593) [:]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:182) [:]
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:85) [:]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) [:]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) [:]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) [:]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:302) [:]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:317) [:]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:299) [:]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:216) [:]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80) [:]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) [:]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754) [:]
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:51) [:]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545) [:]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540) [:]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274) [:]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261) [:]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349) [:]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280) [:]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200) [:]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [:]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [:1.6.0_22]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [:1.6.0_22]
    at java.lang.Thread.run(Thread.java:680) [:1.6.0_22]
```
</description><key id="639832">738</key><summary>Cannot index using java API 0.15.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamw</reporter><labels /><created>2011-03-02T08:33:54Z</created><updated>2011-03-03T17:49:32Z</updated><resolved>2011-03-03T16:19:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T18:12:00Z" id="827183">You might be connecting to an older version server with a newer version client (or the other way around).
</comment><comment author="adamw" created="2011-03-03T08:19:48Z" id="829421">Ah sorry it was a stupid mistake in one of my scripts :/
Sorry again for wasting your time :)
Adam
</comment><comment author="kimchy" created="2011-03-03T17:49:32Z" id="831119">No problem :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JMX warning in 0.15.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/737</link><project id="" key="" /><description>When starting up ES if some indexes are in the gateway, I get the following warning for each index:

[2011-03-02 09:21:43,424][WARN ][jmx                      ] [Baron Mordo] Could not register object with name: {elasticsearch}:type=Baron Mordo [elasticsearch_ktd9moEbQT6pCO3Xlg9NaA],service=indices,index=adam

This didn't happen in 0.14.3
</description><key id="639821">737</key><summary>JMX warning in 0.15.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamw</reporter><labels /><created>2011-03-02T08:27:12Z</created><updated>2013-04-04T18:44:52Z</updated><resolved>2013-04-04T18:44:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:44:52Z" id="15915852">No more reports of this after 2 years- assume fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Searching while an index is being allocated and no active shards exists within a "shard replication group" can cause search "misses"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/736</link><project id="" key="" /><description>While an index is being created, and a shards replication group is not allocated yet (for example, shard id 2 primary and replicas have not been allocated yet), then when performing a search, it might miss data that was indexed (obviously, on a shard replication group that has at least one active shard).
</description><key id="639430">736</key><summary>Searching while an index is being allocated and no active shards exists within a "shard replication group" can cause search "misses"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-02T03:10:37Z</created><updated>2011-03-02T03:11:10Z</updated><resolved>2011-03-02T03:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T03:11:10Z" id="824926">Searching while an index is being allocated and no active shards exists within a &amp;quot;shard replication group&amp;quot; can cause search &amp;quot;misses&amp;quot;, closed by faefc772a462546b0127b674c8b6d3dccfd61c7e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Twitter River: Automatically reconnect when disconnected from twitter stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/735</link><project id="" key="" /><description>Twitter River: Automatically reconnect when disconnected from twitter stream
</description><key id="639327">735</key><summary>Twitter River: Automatically reconnect when disconnected from twitter stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-02T01:59:45Z</created><updated>2011-03-02T02:00:19Z</updated><resolved>2011-03-02T02:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T02:00:19Z" id="824768">Twitter River: Automatically reconnect when disconnected from twitter stream, closed by 578b752425415f0d41bc1881f9e7b67918eb2dc7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't "double" cache a facet filter / query facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/734</link><project id="" key="" /><description>We always cache a facet filter, even though it can be controlled on the filter level itself. Remove the caching and let it be controlled on the filter itself.
</description><key id="638504">734</key><summary>Don't "double" cache a facet filter / query facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-01T20:12:40Z</created><updated>2011-03-01T20:14:45Z</updated><resolved>2011-03-01T20:14:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-01T20:14:45Z" id="823701">Don&amp;#39;t &amp;quot;double&amp;quot; cache a facet filter / query facet, closed by 60b423b741f96d4aa8662e820ad6bf147ed93168.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for Azure Blob Storage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/733</link><project id="" key="" /><description>With the emphasis of allowing ElasticSearch to store data "in the cloud", it would be great if ElasticSearch could store in more than just two clouds (currently, S3 and RackSpace).

Originally, it seems that there was a general cloud module which used jCloud to provide access to various clouds, but that support has been pulled.

However, Microsoft now has a Java-native Azure API (http://www.windowsazure4j.org/) which can be used to access Azure Blob Storage.

Additionally, to help progress this issue along, I have an Azure account that I would be _more_ than happy to contribute to the cause (with a 10GB limit).

Note that this is _not_ a request for discovery between Azure instances, but rather, just to use Azure storage.  I'll file discovery separately when the need arises. =)
</description><key id="638453">733</key><summary>Add support for Azure Blob Storage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">casperOne</reporter><labels /><created>2011-03-01T19:55:55Z</created><updated>2014-07-08T12:28:13Z</updated><resolved>2014-02-21T14:57:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="simnova" created="2011-05-25T16:19:14Z" id="1236243">I'm interested in this as well.
</comment><comment author="flippyhead" created="2012-03-26T04:40:48Z" id="4687829">As am I.
</comment><comment author="flippyhead" created="2012-03-26T04:41:30Z" id="4687832">Also, I'm curious, could S3 storage be used from instancing running on Azure? Any downside to doing this?
</comment><comment author="peteot" created="2012-05-10T17:04:44Z" id="5630688">Any update?  What about using Windows Azure Drive (XDrive)?

The direct use of the blob API sounds cool, but there should also be some way to use Azure blobs via the XDrive which lets you mount the blob as a drive.  However, I have only seen examples accessing it with .NET code.  

Interested to hear if anyone has ElasticSearch running on Windows Azure....
</comment><comment author="casperOne" created="2012-06-02T06:23:34Z" id="6075287">@flippyhead You _could_ do it, but given that the machines would not be co-located, the performance would probably suffer.
</comment><comment author="jakenuts" created="2012-10-01T17:11:54Z" id="9041427">I'd definitely be interested as well, perhaps we can pool some budget to encourage someone familiar with Azure to implement a good solution?
</comment><comment author="dadoonet" created="2014-02-21T14:57:44Z" id="35737149">Heya!

We released [elasticsearch-cloud-azure plugin](https://github.com/elasticsearch/elasticsearch-cloud-azure).
We can now close this issue I guess.

Note that snapshot and restore on Azure Storage will come pretty soonish.

Thanks @spinscale for pointing me to this issue :-)
</comment><comment author="netgh0st" created="2014-06-11T20:03:58Z" id="45792228">The elasticsearch-cloud-azure is a plugin to discover nodes using Azure environment information. The request was for a completely different feature - supporting native Azure blob storage instead of having to use blob mapped as a VHD (which is yet another level of abstraction on top of IO, with additional redundant triplication of data, because ES triplicates shards while Azure blob storage also triplicates data on the backend).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>prefix inside a dis_max / bool query broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/732</link><project id="" key="" /><description>```
{
"size": 2,
"query": {
    "dis_max": {
        "queries": [
        {
            "prefix": {
                "pauseid": {
                    "value": "alex"
                }
            }
        }]
    }
}
}
```

This gives me
    No query registered for [null]

While this works:

```
{
"size": 2,
"query": {
    "dis_max": {
        "queries": [
        {
            "prefix": {
                "pauseid": "alex"
            }
        }]
    }
}
}
```
</description><key id="637571">732</key><summary>prefix inside a dis_max / bool query broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels><label>bug</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-01T14:41:53Z</created><updated>2011-03-01T23:17:29Z</updated><resolved>2011-03-02T06:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-01T22:36:39Z" id="824244">prefix inside a dis_max / bool query broken, closed by 1d240aaff66c6ed10cc46cb4b745cd8dd58d667d.
</comment><comment author="monken" created="2011-03-01T23:17:29Z" id="824367">Wow! That was quick! Thank you very much!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search after Refresh sometimes returns no results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/731</link><project id="" key="" /><description>We create a new index, add 2 documents using the bulk indexing API and refresh the index. Then we search for each document by a unique indexed field and check we get one result.

We repeat this test 20 times. Between 25% and over 50% of the test runs result in a test failure - we can't find the newly indexed documents in the refreshed index.

Please find our test &amp; logging output here: https://gist.github.com/849165

We think this used to work in the old Elastic we were using: v0.13.1

We're running Elastic v0.15.0, on Java jdk1.6.0_17, on Windows XP Professional Service Pack 3 v2002. 

Hardware:

OS Name Microsoft Windows XP Professional
Version 5.1.2600 Service Pack 3 Build 2600
OS Manufacturer Microsoft Corporation
System Manufacturer Hewlett-Packard
System Model    HP Z600 Workstation
Processor   x86 Family 6 Model 26 Stepping 5 GenuineIntel ~2261 Mhz
Processor   x86 Family 6 Model 26 Stepping 5 GenuineIntel ~2261 Mhz
Total Physical Memory   4,098.00 MB
Available Physical Memory   1.34 GB
Total Virtual Memory    2.00 GB
Available Virtual Memory    1.96 GB
Page File Space 4.83 GB
</description><key id="637482">731</key><summary>Search after Refresh sometimes returns no results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">npryce</reporter><labels /><created>2011-03-01T14:08:10Z</created><updated>2013-04-04T18:45:17Z</updated><resolved>2013-04-04T18:45:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-02T03:14:21Z" id="824932">I've just pushed #736 which might help, can you test it with it?
</comment><comment author="Zorkin" created="2011-03-15T15:19:45Z" id="876552">Thanks. This seems to be fixed in v0.15.2 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Allow to use has_child filter in facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/730</link><project id="" key="" /><description>`has_child` filter is a special type of filter that requires pre processing. Allow to use it in (non global) facets in a similar manner that it can be used in query element. This will also enable it to be used in the new search `filter` element.
</description><key id="636371">730</key><summary>Search: Allow to use has_child filter in facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-01T02:35:15Z</created><updated>2011-03-01T02:45:34Z</updated><resolved>2011-03-01T02:45:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-01T02:45:34Z" id="820820">Search: Allow to use has_child filter in facets, closed by e639ffbc935b00b3d017931022548382676f92bf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Peer Recovery: Batch translog based operation recovery based on size and not just number of operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/729</link><project id="" key="" /><description>Currently, the peer recovery of translog does batching based on operation (defaults to 100). Batching based on size as well is important since there might be big docs that will prove problematic to recover.

The following parameters allow to control peer recovery: 
- `index.shard.recovery.file_chunk_size`: The chunks index files will be broken down to when recovered. Defaults to `100kb`.
- `index.shard.recovery.translog_size`: The batch size of translog operations. Defaults to `100kb`.
- `index.shard.recovery.translog_ops`: The number of operations per batch. Defaults to `1000`.
</description><key id="635899">729</key><summary>Peer Recovery: Batch translog based operation recovery based on size and not just number of operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-28T22:49:19Z</created><updated>2011-02-28T22:49:52Z</updated><resolved>2011-02-28T22:49:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-28T22:49:52Z" id="820164">eer Recovery: Batch translog based operation recovery based on size and not just number of operations, closed by 6097365738e5b166c7adb2b21a6bce54fdf9f2f0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ids with # in them will cause search failures, also, fail when # is used in a type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/728</link><project id="" key="" /><description>Ids with # in them will cause search failures, also, fail when # is used in a type name
</description><key id="635502">728</key><summary>Ids with # in them will cause search failures, also, fail when # is used in a type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-28T20:41:16Z</created><updated>2011-02-28T20:43:59Z</updated><resolved>2011-02-28T20:43:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-28T20:43:59Z" id="819636">Ids with # in them will cause search failures, also, fail when # is used in a type name, closed by 906ec57f20d53b6353e504d69c4e4257d5ba92dd.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Histogram Facet: NPE if using "1w" interval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/727</link><project id="" key="" /><description>When using `1w` interval I get the following NPE exception.

```
org.elasticsearch.search.SearchParseException: [indexName][2]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"*"}},"facets":{"histogram":{"date_histogram":{"field":"date","interval":"1w"}}}}]]
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:470)
at org.elasticsearch.search.SearchService.createContext(SearchService.java:385)
at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:215)
at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.sendExecuteFirstPhase(TransportSearchCountAction.java:74)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:191)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:150)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
at org.elasticsearch.search.facet.datehistogram.DateHistogramFacetProcessor.parse(DateHistogramFacetProcessor.java:170)
at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:88)
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:457)
```
</description><key id="635123">727</key><summary>Date Histogram Facet: NPE if using "1w" interval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>bug</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-02-28T18:17:25Z</created><updated>2011-03-01T18:18:46Z</updated><resolved>2011-03-01T02:31:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-28T18:31:10Z" id="819010">Date Histogram Facet: NPE if using &amp;quot;1w&amp;quot; interval, closed by 47a6065805ee8e589935af505f1c3bba2c57e3a6.
</comment><comment author="lukas-vlcek" created="2011-03-01T13:39:35Z" id="822067">Still not working:

```
org.elasticsearch.search.SearchParseException: [indexName][2]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"query_string":{"query":"*"}},"facets":{"histogram":{"date_histogram":{"field":"date","interval":"1w"}}}}]]
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:470)
at org.elasticsearch.search.SearchService.createContext(SearchService.java:385)
at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:215)
at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.sendExecuteFirstPhase(TransportSearchCountAction.java:74)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:191)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:150)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)
Caused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [histogram]: failed to parse interval [1w], tried both as built in intervals (year/month/...) and as a time format
at org.elasticsearch.search.facet.datehistogram.DateHistogramFacetProcessor.parse(DateHistogramFacetProcessor.java:187)
at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:88)
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:457)
... 10 more
```
</comment><comment author="kimchy" created="2011-03-01T18:18:46Z" id="823161">Right, thats because `w` was not supported as part of the time format, up to `d` was supported. Added `w` support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apparent Deadlock querying a newly created index (using Java API)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/726</link><project id="" key="" /><description>Hi,

We are using the embedded Java API, with Elastic v0.15.0 on Windows XP. We are seeing a search on a newly created index hang infinitely. When we force a delay between creating the index and searching it, we don't see the issue.

We've created a GIST with a unit test to illustrate the issue - https://gist.github.com/847329
</description><key id="634283">726</key><summary>Apparent Deadlock querying a newly created index (using Java API)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">npryce</reporter><labels /><created>2011-02-28T13:53:12Z</created><updated>2013-04-04T18:45:53Z</updated><resolved>2013-04-04T18:45:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-01T01:49:52Z" id="820701">Tried to recreate it, but can't... . Was thinking that it has to do with the fact that it takes time for the shards to be allocated and the search is executed while they are allocated, but I do handle such cases and test for that.

Can you turn on TRACE debugging on the org.elasticsearch.action and gist the log output?
</comment><comment author="npryce" created="2011-03-01T14:00:21Z" id="822117">Hi, we've updated the gist with some log output. We couldn't get any logging from org.elasticsearch.action, so we turned on logging for org.elasticsearch.cluster.action in the hope that this will help you. Let us know if you need any other debug. Btw we are running on Windows XP Professional Service Pack 3. Java jdk1.6.0_17.
</comment><comment author="kimchy" created="2011-03-02T03:13:52Z" id="824931">I've just pushed #736 which might help, can you test it with it?
</comment><comment author="Zorkin" created="2011-03-15T15:18:43Z" id="876549">Behavior is slightly different now in 0.15.2.
Instead of it hanging, we are getting an error.

Failed to execute phase [query_fetch], total failure; shardFailures {[_na_][testindex][0]: No active shards}

Our call to create an index is synchronous.
Not sure if we should be doing things differently - will ask on the mailing list.
</comment><comment author="clintongormley" created="2013-04-04T18:45:53Z" id="15915921">No further reports on this issue. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolate: Failure to percolate with specific query (when creating the _percolate index before the actual index)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/725</link><project id="" key="" /><description>Percolate: Failure to percolate with specific query (when creating the _percolate index before the actual index)
</description><key id="630915">725</key><summary>Percolate: Failure to percolate with specific query (when creating the _percolate index before the actual index)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-26T22:59:18Z</created><updated>2011-02-26T23:07:10Z</updated><resolved>2011-02-26T23:07:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-26T23:07:10Z" id="813862">Percolate: Failure to percolate with specific query (when creating the _percolate index before the actual index), closed by 3b967040da249ce58e848e7c00dad11b42eb3c9f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API: Improve memory usage when executing large bulk requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/724</link><project id="" key="" /><description>Currently, the bulk API does a pass to parse all items, and them performs them. Move to do it one by one to improve memory utilization.
</description><key id="630779">724</key><summary>Bulk API: Improve memory usage when executing large bulk requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-26T21:45:58Z</created><updated>2011-02-26T21:46:37Z</updated><resolved>2011-02-26T21:46:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-26T21:46:37Z" id="813681">Bulk API: Improve memory usage when executing large bulk requests, closed by fdef88debb84af1ac82562ee60bc353dccd24027.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: Rename `discovery.zen.initial_ping_timeout` to `discovery.zen.ping_timeout` (still support the old setting)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/723</link><project id="" key="" /><description>Rename `discovery.zen.initial_ping_timeout` to `discovery.zen.ping_timeout` (still support the old setting). This is mainly since ping can be done not just for the initial discovery process.
</description><key id="630327">723</key><summary>Zen Discovery: Rename `discovery.zen.initial_ping_timeout` to `discovery.zen.ping_timeout` (still support the old setting)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-26T20:24:13Z</created><updated>2011-02-26T20:26:02Z</updated><resolved>2011-02-26T20:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-26T20:26:02Z" id="813514">Zen Discovery: Rename `discovery.zen.initial_ping_timeout` to `discovery.zen.ping_timeout` (still support the old setting), closed by 3cda177b9b571b4edc9a48512c57fb830f3ecc19.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: When _all is disabled, optimize to not gather all entries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/722</link><project id="" key="" /><description>All entries are not gathered if specific `include_in_all` mapping is set on field / object level, but, if `_all` is disabled completely, they are still gathered. Really, don't do that... . Here is a sample mapping when `_all` is disabled:

```
{
    "my_type" : {
        "_all" : {
            "enabled" : false
        }
    }
}
```
</description><key id="628641">722</key><summary>Mapping: When _all is disabled, optimize to not gather all entries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-26T01:54:58Z</created><updated>2011-02-26T02:16:14Z</updated><resolved>2011-02-26T02:16:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-26T02:16:14Z" id="811919">Mapping: When _all is disabled, optimize to not gather all entries, closed by 4634ca5cb8c8ad0a3c725363f3705a4078c04c9c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Filter Cache: Add `resident` type, and `max_size` to `soft`/`weak` types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/721</link><project id="" key="" /><description>Add a `resident` filter cache that will use an LRU for filter cache with default `index.cache.filter.resident.max_size` of `1000`.

Also, add `max_size` to the `soft` and `weak` filter caches as well. By default (`-1`), it is not set.
</description><key id="625676">721</key><summary>Index Filter Cache: Add `resident` type, and `max_size` to `soft`/`weak` types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-25T01:53:36Z</created><updated>2011-02-25T01:54:28Z</updated><resolved>2011-02-25T01:54:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-25T01:54:27Z" id="807916">Index Filter Cache: Add `resident` type, and `max_size` to `soft`/`weak` types, closed by 608c5a838d2842ffad3c77cd910c8975c69d0a19.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Acknowledge problem for put mapping with multiple indices or all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/720</link><project id="" key="" /><description>While using put mapping with multiple indices response returns acknowledged as false. It should return true. 
This is discussed in
http://elasticsearch-users.115913.n3.nabble.com/Problem-with-put-mapping-on-multiple-indices-tp2558832p2558832.html
</description><key id="623736">720</key><summary>Acknowledge problem for put mapping with multiple indices or all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels><label>bug</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-24T12:10:28Z</created><updated>2011-02-24T19:17:33Z</updated><resolved>2011-02-24T19:17:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-24T19:17:33Z" id="806661">Acknowledge problem for put mapping with multiple indices or all indices, closed by ecc1a3cd8cb1aaf836b1f03dc48d56d008be3d0d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Allow to filter out docs based on a minimum score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/719</link><project id="" key="" /><description>Allow to filter docs based on a minimum score, for example:

```
{
    "min_score": 0.5,
    "query" : {
        "term" : { "user" : "kimchy" }
    }
}
```
</description><key id="623032">719</key><summary>Search: Allow to filter out docs based on a minimum score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-02-24T04:54:34Z</created><updated>2016-01-27T13:18:29Z</updated><resolved>2011-02-24T12:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-24T04:55:29Z" id="804309">Search: Allow to filter out docs based on a minimum score, closed by 8a03ca111463fe0ea5ec1e3e90d4ca146b1cb920.
</comment><comment author="lukas-vlcek" created="2011-02-25T08:57:45Z" id="808796">Does it work against the relative document score value only? How about filtering documents based on percentiles of the max score? For example filtering out documents having score below 70% of max score value?
</comment><comment author="kimchy" created="2011-02-25T20:15:38Z" id="810906">Nope, you can't really do that based on percentage since you don't know the max score before a full pass has been done over the hits. I guess we can execute the query twice, but its wasteful.
</comment><comment author="chendo" created="2012-08-22T07:27:56Z" id="7926597">Does this work with More Like This? I tried it in the GET request body as well as the query string.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Add search_type for `count` to return count but still support facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/718</link><project id="" key="" /><description>Currently, a hack to get just the count is to set `size` to `0`. A special `search_type` called `count` can eb used to return just the count (provided in the `total_hits`) and possibly facets. In general, this should replace the `count` API.
</description><key id="622286">718</key><summary>Search: Add search_type for `count` to return count but still support facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-02-23T22:29:06Z</created><updated>2011-02-23T22:29:52Z</updated><resolved>2011-02-23T22:29:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-23T22:29:52Z" id="803362">Search: Add search_type for `count` to return count but still support facets, closed by ee9beda3981c4b4acb5da85779f659bb27988767.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve from hits pagination (duplicates)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/717</link><project id="" key="" /><description>When reducing / sorting the results from several shards, improve the tie breaking between sorted values (like score) by taking the shards into account as well to have a consistent sort.
</description><key id="622015">717</key><summary>Improve from hits pagination (duplicates)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-23T20:57:16Z</created><updated>2011-02-23T20:57:55Z</updated><resolved>2011-02-23T20:57:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-23T20:57:55Z" id="802999">Improve from hits pagination (duplicates), closed by f7100c0698c6808d22736160f4229ea4a780f446.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clear Cache API: Add specific cache clear for `id`, `filter`, `field_data`, `bloom`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/716</link><project id="" key="" /><description>The clear cache API should accept flags to specify which cached to clear. For example `filter=true`, or `field_data=true`. If none are set, clear all caches.
</description><key id="621677">716</key><summary>Clear Cache API: Add specific cache clear for `id`, `filter`, `field_data`, `bloom`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-23T19:16:25Z</created><updated>2011-02-23T19:17:17Z</updated><resolved>2011-02-23T19:17:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-23T19:17:17Z" id="802597">Clear Cache API: Add specific cache clear for `id`, `filter`, `field_data`, `bloom`, closed by 5082ad6d116e5aa38a1333af1e87657bfbcf806b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Twitter river: "user.screen_name" should not be analyzed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/715</link><project id="" key="" /><description>As the subject says. It would make querying much more simple
</description><key id="621423">715</key><summary>Twitter river: "user.screen_name" should not be analyzed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmalves</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-23T17:37:35Z</created><updated>2011-02-23T17:55:55Z</updated><resolved>2011-02-23T17:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-23T17:55:55Z" id="802288">Twitter river: &amp;quot;user.screen_name&amp;quot; should not be analyzed, closed by bcaeb226d5db61a2b54cb89a59bc44432eff6918.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Single node cluster comes up as red with local gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/714</link><project id="" key="" /><description>When bringing up a single node cluster using the local gateway with a configuration similar to the following:

&lt;pre&gt;
gateway:
  type: local
  recover_after_nodes: 1
  recover_after_time: 5m
  expected_nodes: 1
&lt;/pre&gt;


ES 0.15.0 will wait the full 5 min. before starting up.  In ES 0.14.4 it would come up right away.
</description><key id="619348">714</key><summary>Single node cluster comes up as red with local gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirvdrum</reporter><labels><label>bug</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-22T23:08:10Z</created><updated>2011-02-22T23:21:25Z</updated><resolved>2011-02-22T23:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T23:21:25Z" id="799444">Single node cluster comes up as red with local gateway, closed by 1a480071746576e6a7f9fdfc259b47a8da3b4aec.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Add number of docs per node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/713</link><project id="" key="" /><description>Add the number of docs that are allocated on each node (sum of all the docs on all the shards allocated on that node).
</description><key id="618854">713</key><summary>Node Stats: Add number of docs per node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-02-22T20:26:52Z</created><updated>2011-02-22T20:27:21Z</updated><resolved>2011-02-22T20:27:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T20:27:21Z" id="798696">Node Stats: Add number of docs per node, closed by c6f58321e4761b308081dd7498b3a9602ccbd29e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to register custom rivers under a custom type name (And not full class names) </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/712</link><project id="" key="" /><description>Allow to register custom rivers under a custom type name (And not full class names) easily. For example:

```
public class TwitterRiverPlugin extends AbstractPlugin {

    @Inject public TwitterRiverPlugin() {
    }

    @Override public String name() {
        return "river-twitter";
    }

    @Override public String description() {
        return "River Twitter Plugin";
    }


    @Override public void processModule(Module module) {
        if (module instanceof RiversModule) {
            ((RiversModule) module).registerRiver("twitter", TwitterRiverModule.class);
        }
    }
}
```

The `processModule` can register the `twitter` type for the twitter river module that handles it.
</description><key id="618816">712</key><summary>Allow to register custom rivers under a custom type name (And not full class names) </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-02-22T20:12:57Z</created><updated>2011-02-22T20:13:42Z</updated><resolved>2011-02-22T20:13:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T20:13:42Z" id="798640">Allow to register custom rivers under a custom type name (And not full class names), closed by 45ec2c6e21e26567534990d21c2d7508173fba18.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>River not recovered when using single node after shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/711</link><project id="" key="" /><description>There is a race in how recovery happens, and when it fails to recover, it will not recover until another "meta" change will happen in the cluster, like an index being created or a node started.
</description><key id="618487">711</key><summary>River not recovered when using single node after shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-22T18:37:38Z</created><updated>2011-02-22T18:39:02Z</updated><resolved>2011-02-22T18:39:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T18:39:01Z" id="798208">River not recovered when using single node after shutdown, closed by 7dda421cfa090290bc4a1c1eb766aaab45aaf3fb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RabbitMQ river does not reliably resume operation after single-node-cluster restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/710</link><project id="" key="" /><description>In both elasticsearch 0.14.4 and 0.15.0, the RabbitMQ river does not resume operation after a restart of a single-node cluster. Re-indexing does not help, even when deleting the index beforehand; only by creating a new river under a different name can an AMQP connection be reestablished when the service has entered this state.

In 0.14.4, an error akin to the following is emitted:

```
2011-02-22_13:59:49.62013 [13:59:49,619][WARN ] [river.routing            ]
  [Ultra-Marine] failed to get/parse _meta for [logstash_events_201102220700]
  org.elasticsearch.action.NoShardAvailableActionException:
  [_river]3] No shard available for [logstash_events_201102220700#_meta]
```
</description><key id="617790">710</key><summary>RabbitMQ river does not reliably resume operation after single-node-cluster restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">charles-dyfis-net</reporter><labels /><created>2011-02-22T14:49:25Z</created><updated>2011-02-22T18:45:18Z</updated><resolved>2011-02-22T18:45:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T18:45:18Z" id="798229">Fixed as part of #711.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue/696: Support for other exchange types and options in AMQP river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/709</link><project id="" key="" /><description>Add support for `exchange_durable` (default `true`), `queue_durable` (default `true`), and `queue_auto_delete` (default `false`)
</description><key id="617466">709</key><summary>Issue/696: Support for other exchange types and options in AMQP river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">charles-dyfis-net</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-02-22T12:25:08Z</created><updated>2014-06-12T18:25:11Z</updated><resolved>2011-02-23T02:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T18:06:51Z" id="798103">Support for other exchange types and options in AMQP river, closed by a4978bc67e6bace384bd9cc8620aa83b82e0d6b4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get API: Allow to provide a script as a field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/708</link><project id="" key="" /><description>Allow to provide a script as a field, for example `_source.obj1.obj2.name`. Script behavior is the same as a script field in search.
</description><key id="616428">708</key><summary>Get API: Allow to provide a script as a field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-02-22T00:26:49Z</created><updated>2011-02-22T00:28:37Z</updated><resolved>2011-02-22T00:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T00:28:37Z" id="795475">Get API: Allow to provide a script as a field, closed by 953fcbc58c236936c8522217bd56a4bb5579a250.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Add search type `scan` allowing to efficiently scan large result set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/707</link><project id="" key="" /><description>The `scan` search type allows to efficiently scroll a large result set. Its used first by executing a search request with scrolling and a query:

```
curl -XGET 'localhost:9200/_search?search_type=scan&amp;scroll=10m&amp;size=50' -d '
{
    "query" : {
        "match_all" : {}
    }
}
'
```

The `scroll` parameter control the keep alive time of the scrolling request and initiates the scrolling process. The timeout applies per round trip (i.e. between the previous scan scroll request, to the next).

The response will include _no hits_, with two important results, the `total_hits` will include the total hits that match the query, and the `scroll_id` that allows to start the scroll process. From this stage, the `_search/scroll` endpoint should be used to scroll the hits, feeding the next scroll request with the previous search result `scroll_id`. For example:

```
curl -XGET 'localhost:9200/_search/scroll?scroll=10m' -d 'c2NhbjsxOjBLMzdpWEtqU2IyZHlmVURPeFJOZnc7MzowSzM3aVhLalNiMmR5ZlVET3hSTmZ3OzU6MEszN2lYS2pTYjJkeWZVRE94Uk5mdzsyOjBLMzdpWEtqU2IyZHlmVURPeFJOZnc7NDowSzM3aVhLalNiMmR5ZlVET3hSTmZ3Ow=='
```

The "breaking" condition out of a scroll is when no hits has been returned. The `total_hits` will be maintained between scroll requests.

Note, `scan` search type does not support sorting (either on score or a field) or faceting.
</description><key id="616065">707</key><summary>Search: Add search type `scan` allowing to efficiently scan large result set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-02-21T21:58:00Z</created><updated>2011-03-24T20:45:08Z</updated><resolved>2011-02-22T06:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-21T22:11:43Z" id="795063">Search: Add search type `scan` allowing to efficiently scan large result set, closed by 818f3b4d7505e998fd42ac5cce5b622484c480a9.
</comment><comment author="karussell" created="2011-02-22T19:38:31Z" id="798505">I noticed an exception: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/f1bd216132ad4728
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `http` and `transport` simplified host settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/706</link><project id="" key="" /><description>Add: 
- `http.publish_host`, `http.bind_host`, `http.host` (sets both publish and bind).
- `transport.publish_host`, `transport.bind_host`, `transport.host` (sets both publish and bind).
</description><key id="615506">706</key><summary>Add `http` and `transport` simplified host settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-21T17:50:37Z</created><updated>2011-02-21T17:51:39Z</updated><resolved>2011-02-21T17:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-21T17:51:39Z" id="794020">Add `http` and `transport` simplified host settings, closed by 65ca8570276fbd59fbef65f812a2f8a1cb301492.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Terms Stats Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/705</link><project id="" key="" /><description>The `terms_stats` facet combines the `terms` and `stats` facet to group based on a `key_field` unique terms and compute stats (count, total, min, max) on another value field or script. For example:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag_price_stats" : {
            "terms_stats" : {
                "key_field" : "tag",
                "value_field" : "price"
            }
        }
    }
}
```

The `size` parameter controls how many facet entries will be returned. It defaults to `10`. Setting it to `0` will return all terms matching the hits.

Ordering is done by setting `order`, with possible values of `term`, `reverse_term`, `count`, `reverse_count`, `total`, `reverse_total`, `min`, `reverse_min`, `max`, `reverse_max`. Defaults to `term`.

The value computed can also be a script, using the `value_script` provided, in which case the `lang` can control its language, and `params` allow to provide custom parameters (as in other scripted components).

_Note_: It is expected that the will be a single value of the key field per doc, and one or multiple values of the value field.
</description><key id="611982">705</key><summary>Search: Terms Stats Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-02-19T21:08:09Z</created><updated>2014-05-05T11:53:28Z</updated><resolved>2011-02-20T05:37:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-19T21:37:13Z" id="789224">Search: Terms Stats Facet, closed by 352cb74f960fe9d0c91cd44fa61474625e960e03.
</comment><comment author="deinspanjer" created="2011-02-20T05:34:09Z" id="789927">wow. pivot tables for the win!
</comment><comment author="itthuyloi" created="2012-11-02T08:29:02Z" id="10007688">In page: http://www.elasticsearch.org/guide/reference/api/search/facets/terms-stats-facet.html
&lt;quote&gt;
"Note, the terms stats can work with multi valued key fields, or multi valued value fields, but not when both are multi valued (as ordering is not maintained)."
&lt;/quote&gt;
But how to build the query, I want to statistic of "hoursWorked" of a "Task log" have for {groupId and projectId}, please tell me!

I tried: 

&lt;pre&gt;
&lt;code&gt;
{
    "query": {
        "match_all": {
            
        }
    },
    "facets": {
        "hoursWorked_stats": {
            "terms_stats": {
                "key_field": ["groupId",
                "projectId"],
                "value_field": "hoursWorked",
                "size": 1000
            }
        }
    }
}
&lt;/code&gt;
&lt;/pre&gt;

But it's not working! 
</comment><comment author="rbnacharya" created="2014-05-05T11:53:28Z" id="42180123">https://github.com/elasticsearch/elasticsearch/issues/6046 

Count issue in terms stat facet. Please visit the issue.. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify error handling a bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/704</link><project id="" key="" /><description>Just a minor nit as I was going through the code...
</description><key id="610566">704</key><summary>Simplify error handling a bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsuna</reporter><labels /><created>2011-02-18T23:09:49Z</created><updated>2014-07-16T21:56:53Z</updated><resolved>2011-02-19T22:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-19T22:46:38Z" id="789378">Applied, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update tika to 0.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/703</link><project id="" key="" /><description>Update tika to the new release.
</description><key id="610441">703</key><summary>Update tika to 0.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2011-02-18T22:14:03Z</created><updated>2014-07-16T21:56:53Z</updated><resolved>2011-02-20T00:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-20T00:21:15Z" id="789546">Applied.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle cluster join request on a non IO thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/702</link><project id="" key="" /><description>Handle cluster join request on a non IO thread
</description><key id="610359">702</key><summary>Handle cluster join request on a non IO thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.1</label><label>v0.16.0</label></labels><created>2011-02-18T21:42:50Z</created><updated>2011-02-18T21:45:17Z</updated><resolved>2011-02-18T21:45:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-18T21:45:17Z" id="787030">Handle cluster join request on a non IO thread, closed by b7ce5a3f217b26644e51a9b63e80587ed2d2941f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>n-gram token filter not finding partial words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/701</link><project id="" key="" /><description>Hi,

I have been playing around with ElasticSearch for a new project of mine. I have set the default analyzers to use the ngram tokenfilter. This is my elasticsearch.yml file:

```
index:
analysis:
    analyzer:
        default_index:
            tokenizer: standard
            filter: [standard, stop, mynGram]
        default_search:
            tokenizer: standard
            filter: [standard, stop]

    filter:
        mynGram:
            type: nGram
            min_gram: 1
            max_gram: 10
```

I created a new index and added the following document to it:

```
$ curl -XPUT http://localhost:9200/test/newtype/3 -d '{"text": "one two three four five six"}'
{"ok":true,"_index":"test","_type":"newtype","_id":"3"}
```

However, when I search using the query `text:hree` or `text:ive` or any other partial terms, ElasticSearch does not return this document. It returns the document only when I search for the exact term (like `text:two`). 

I have also tried changing the config file such that default_search also uses the ngram token filter, but the result was the same. 

Am I doing something wrong here? If so how do I correct it? If not, why is it not returning the expected result?

Thanks in advance for your time and help.
</description><key id="609841">701</key><summary>n-gram token filter not finding partial words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asleepysamurai</reporter><labels /><created>2011-02-18T18:02:33Z</created><updated>2011-07-06T08:58:32Z</updated><resolved>2011-06-18T16:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="asleepysamurai" created="2011-06-18T16:34:59Z" id="1393816">Okay, apparently my configuration was wrong. 
</comment><comment author="drei01" created="2011-07-06T08:46:39Z" id="1510346">@asleeysamurai what was the problem with your configuration. I am having a similar problem with default_index
</comment><comment author="asleepysamurai" created="2011-07-06T08:58:32Z" id="1510397">@drei01: Check this stack overflow question: http://stackoverflow.com/questions/5044674/elasticsearch-n-gram-tokenfilter-not-finding-partial-words
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory leaks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/700</link><project id="" key="" /><description>I'm having some memory issues with ElasticSearch. To the point that my box totally froze a couple days ago when every process started invoking the oom-killer.

Here are all the infos I could gather so far, if you need anything else, please let me know and I'll try to provide.

```
$ sudo tail -n20 /var/log/syslog
Feb 18 01:36:12 intertubes kernel: grsec: Segmentation fault occurred at 00007f810622e280 in /usr/local/java/jre1.6.0_13/bin/java[java:4911] uid/euid:0/0 gid/egid:0/0, parent /bin/bash[bash:4648] uid/euid:0/0 gid/egid:0/0
Feb 18 01:36:14 intertubes kernel: grsec: Segmentation fault occurred at 00007f810622e8c0 in /usr/local/java/jre1.6.0_13/bin/java[java:4947] uid/euid:0/0 gid/egid:0/0, parent /bin/bash[bash:4648] uid/euid:0/0 gid/egid:0/0
Feb 18 01:36:14 intertubes kernel: grsec: Segmentation fault occurred at 00007f810622e5c0 in /usr/local/java/jre1.6.0_13/bin/java[java:4953] uid/euid:0/0 gid/egid:0/0, parent /bin/bash[bash:4648] uid/euid:0/0 gid/egid:0/0
Feb 18 01:36:28 intertubes kernel: grsec: Segmentation fault occurred at 00007f810622ea40 in /usr/local/java/jre1.6.0_13/bin/java[java:4934] uid/euid:0/0 gid/egid:0/0, parent /bin/bash[bash:4648] uid/euid:0/0 gid/egid:0/0
Feb 18 01:36:28 intertubes kernel: grsec: Segmentation fault occurred at 00007f810622e980 in /usr/local/java/jre1.6.0_13/bin/java[java:4909] uid/euid:0/0 gid/egid:0/0, parent /bin/bash[bash:4648] uid/euid:0/0 gid/egid:0/0

$ java -version
java version "1.6.0_13"
Java(TM) SE Runtime Environment (build 1.6.0_13-b03)
Java HotSpot(TM) 64-Bit Server VM (build 11.3-b02, mixed mode)

$ uname -a
Linux intertubes.ch 2.6.34.6-xxxx-grs-ipv6-64 .#3 SMP Fri Sep 17 16:06:38 UTC 2010 x86_64 GNU/Linux
```

ElasticSearch version is 0.14.2.

Basically in the last 3 days since I started it, it went from 8.4 to 11.4% of memory usage, and it's virtually unused. The syslog dumps are also somewhat worrying, but I'm not sure what they mean or where I can find more info.
</description><key id="608143">700</key><summary>Memory leaks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Seldaek</reporter><labels /><created>2011-02-18T00:43:37Z</created><updated>2011-02-18T17:04:01Z</updated><resolved>2011-02-18T17:43:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-18T00:45:13Z" id="784140">Make sure you use the latest JVM vestion, either 1.6 update 22, or update 23. And its better to post this on the mailing list.
</comment><comment author="Seldaek" created="2011-02-18T09:43:39Z" id="785070">Sorry about that, I'll close this issue then, and try a newer JVM - I'll get on the list if I still have issues. Thanks
</comment><comment author="kimchy" created="2011-02-18T17:04:01Z" id="786141">No problem :). I try to keep the issues for real bugs / feature request. Not saying that this might not actually be a bug, and if it is, it will be fixed ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolating during indexing does not return matched percolation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/699</link><project id="" key="" /><description>curl script to reproduce:
https://gist.github.com/832127

discussed on IRC

environment:
vanilla build of latest 0.15.0 snapshot codebase
</description><key id="607102">699</key><summary>percolating during indexing does not return matched percolation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wireframe</reporter><labels /><created>2011-02-17T17:55:44Z</created><updated>2011-02-17T20:53:17Z</updated><resolved>2011-02-18T02:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-17T18:04:41Z" id="782842">Fixed.
</comment><comment author="wireframe" created="2011-02-17T18:07:45Z" id="782853">@kimchy i just pulled down the latest codebase and rebuilt and still see the issue.
</comment><comment author="kimchy" created="2011-02-17T18:16:23Z" id="782886">Seems to be running fine for me. Maybe its a race condition issue? If you create the `elastic_searchable` index first, wait a bit, and then index with percolation, does that work well then?
</comment><comment author="wireframe" created="2011-02-17T20:53:17Z" id="783401">ok.  it looks good now.

i reset my entire index and ran through the testcase multiple times and it passed every time.

thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>race condition indexing + refreshing + searching with 0.15 snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/698</link><project id="" key="" /><description>curl script to reproduce issue:
https://gist.github.com/b5775ddf2160cf9c8dd7

script fails to find the expected result ~50% of the time.
posted issue after discussed on IRC.

environment:
vanilla 0.15.0 snapshot built directly from github
</description><key id="605628">698</key><summary>race condition indexing + refreshing + searching with 0.15 snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wireframe</reporter><labels /><created>2011-02-17T03:34:18Z</created><updated>2013-04-04T18:46:39Z</updated><resolved>2013-04-04T18:46:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-17T17:31:13Z" id="782717">What is the expected output of the search at the end of the script?
</comment><comment author="wireframe" created="2011-02-17T17:34:37Z" id="782726">the search request should hit document 1047855828 (there's a comment in the curl script pointing to the correct one).  

sorry about the verbosity!
</comment><comment author="wireframe" created="2011-02-17T19:54:04Z" id="783194">I've updated the gist to be a re-runnable bash script exits if the search results are not found.

the script will:
- delete existing index
- create new index with some analyzer options
- index a set of documents
- refresh index
- perform search for specific document

the script performs these operations 25 times and i usually see ~2-3 failures per run.
</comment><comment author="kimchy" created="2011-02-18T17:19:27Z" id="786198">Thanks for creating the script, I ran it for 1000 times, and it always found results. Weird!. Its probably a timing issue, the question is how can I recreate it locally...
</comment><comment author="wireframe" created="2011-02-19T02:56:03Z" id="787694">if it's any help, here are more details on my environment:
macbook pro running snow leopard
java version "1.6.0_22"
Java(TM) SE Runtime Environment (build 1.6.0_22-b04-307-10M3261)
Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03-307, mixed mode)
</comment><comment author="wireframe" created="2011-02-22T16:45:45Z" id="797778">This definitely seems like a mac JVM specific issue.   running the same test script on my linux server with 0.15.0 installed passed the test every time.  I also bumped up the number of iterations to 2000 and it never encountered the race condition.

I obviously won't be deploying elasticsearch on the mac JVM, but it is a bit worrysome for our development team to have to worry about consistency issues between running locally versus production.

do you have any suggestions or potential workarounds?
</comment><comment author="kimchy" created="2011-02-22T20:45:03Z" id="798766">I ran it on my macbook pro and it seems to pass each time. If you add sleep(1) either before the refresh or after, do you still see the problem?
</comment><comment author="clintongormley" created="2013-04-04T18:46:39Z" id="15915975">No further info on this issue, and no further reports - assume fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Shutdown API: On full/_all shutdown, shutdown only data/master nodes, not client nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/697</link><project id="" key="" /><description>Cluster Shutdown API: On full/_all shutdown, shutdown only data/master nodes, not client nodes
</description><key id="605452">697</key><summary>Cluster Shutdown API: On full/_all shutdown, shutdown only data/master nodes, not client nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-17T01:31:59Z</created><updated>2011-02-17T01:33:41Z</updated><resolved>2011-02-17T01:33:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-17T01:33:41Z" id="780489">Cluster Shutdown API: On full/_all shutdown, shutdown only data/master nodes, not client nodes, closed by 9ca8165f3bda0718ac5ff43d8ef7460ee4f099d3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RabbitMQ river plugin only supports direct exchange type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/696</link><project id="" key="" /><description>There is a legitimate use case where one may ElasticSearch to be subscribed to a queue behind a fanout exchange (so not only ElasticSearch, but other client(s) as well, receive copies of the same messages). As such, `org.elasticsearch.river.rabbitmq.RabbitmqRiver` should allow the `type` passed to `channel.exchangeDeclare` to be parameterized.
</description><key id="605227">696</key><summary>RabbitMQ river plugin only supports direct exchange type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">charles-dyfis-net</reporter><labels /><created>2011-02-17T00:03:45Z</created><updated>2011-02-22T18:06:51Z</updated><resolved>2011-02-22T18:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-17T00:05:46Z" id="780234">Sounds good, want to work on a pull request for this change? :)
</comment><comment author="charles-dyfis-net" created="2011-02-22T15:44:24Z" id="797550">Corresponds with https://github.com/elasticsearch/elasticsearch/pull/709
</comment><comment author="kimchy" created="2011-02-22T18:06:51Z" id="798102">Allow exchange type and durability, queue durability and queue auto-delete to be specified; closed by d0780f0f62de1f975a9f19b7399289ffeb189be8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Status: Peer recovery does not compute the index size correctly to report the full size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/695</link><project id="" key="" /><description>Index Status: Peer recovery does not compute the index size correctly to report the full size
</description><key id="605181">695</key><summary>Index Status: Peer recovery does not compute the index size correctly to report the full size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-02-16T23:42:45Z</created><updated>2011-02-16T23:44:03Z</updated><resolved>2011-02-16T23:44:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-16T23:44:03Z" id="780175">Index Status: Peer recovery does not compute the index size correctly to report the full size, closed by 09d650fae77ad487626553b7899b0e82760daec3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>s3 gateway recovery fails when no indices exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/694</link><project id="" key="" /><description>Using 0.14.4 and the s3 gateway.

Repro:
Bring up a new node on its own
Wait until it creates the initial gateway state
Restart the node

The node will never recover the gateway.

If an index is first created before restarting the node, then the gateway recovers normally.

I only tried this with a cluster of one node. Not sure if it is different with multiple nodes.
</description><key id="604961">694</key><summary>s3 gateway recovery fails when no indices exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2011-02-16T22:12:40Z</created><updated>2013-04-04T18:47:15Z</updated><resolved>2013-04-04T18:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:47:15Z" id="15916007">No further reports of this issue, and S3 gateway is deprecated. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"language: no" should be interpreted as "no" (=norwegian), but becomes "false"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/693</link><project id="" key="" /><description>When using the ICU collation filterfor norwegian collation like this

index :
    analysis :
        analyzer :
           myCollation :
                type : custom
                tokenizer : standard
                filter : [myCollator]
        filter :
         myCollator :
           type : icu_collation
           language : no

The language "no"  is interpreted as "false" internally. I ran this in debug mode to check. The parser for the config settings converts the string "no" to "false" when it is expected to be passed as "no" (= Norwegian).
</description><key id="604700">693</key><summary>"language: no" should be interpreted as "no" (=norwegian), but becomes "false"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barsk</reporter><labels /><created>2011-02-16T20:52:05Z</created><updated>2013-04-04T18:48:04Z</updated><resolved>2013-04-04T18:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-16T22:40:20Z" id="779987">Where did you see it getting converted to false? Is that in the yaml code? You can use json config to work around it (and in any case, configuring this per index is preferable).
</comment><comment author="barsk" created="2011-02-17T08:48:40Z" id="781222">I did not have the yaml source so I only debugged the factory classes for the filters. The conversion may be in the yaml code or in your code on top of it, I haven't checked that far.

This code however (IcuCollationTokenFilterFactory.java):
String language = settings.get("language"); 
Returns "false" if the setting is "no". It seems the parser does some boolean conversion where it is not appropriate.

Yes, configuring per index may be appropriate, but I do not think it affects the parser. JSON configuration may be a workaround. Haven't checked. I am using sv locale myself, I just happened to stumble upon the no bug when testing around.
</comment><comment author="skade" created="2011-07-07T14:56:55Z" id="1523285">In YAML (1.1), 'no' and 'yes' are 'false' and 'true' per specification and have to be quoted if you want to describe a string value. So, the parser is actually correct. Norwegians will have to quote their locale.

I cannot find a reference to this in the 1.2 spec, but I assume most Parsers will stick to that behavior.

This has bitten me more then once :/.
</comment><comment author="kimchy" created="2011-07-07T14:58:28Z" id="1523305">I did not know that :), one option is to simply rename the elasticsearch.yml file to elasticsearch.json and provide the configuration in json...
</comment><comment author="skade" created="2011-07-07T15:41:39Z" id="1523654">Yep, but it allows you to close yet another ticket (tm) :). I for myself was just browsing through the ticket list searching for upcoming stuff and remembered that I saw that problem before.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent response behavior between _search and _count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/692</link><project id="" key="" /><description>When a REST call with an invalid query is made to a _search URL in ElasticSearch, it returns a response with a status of "500 Internal Server Error". When the same invalid query is made on "_count", it returns a response with a status of "200 OK", with a simillar error message in the body. I believe the _count call should return exactly the same 500 status response as the _search call, if possible, to keep their behavior consistent and make the display of errors in the client side easier to implement.
</description><key id="603354">692</key><summary>Inconsistent response behavior between _search and _count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Bira</reporter><labels /><created>2011-02-16T12:41:03Z</created><updated>2014-07-08T12:02:34Z</updated><resolved>2014-07-08T12:02:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="raff" created="2011-06-09T00:32:00Z" id="1331101">I would actually prefer if an invalid search pattern would return a client error (like a 400 Bad Request) instead of a 500 Internal Server Error. Then the client could display a meaningful error to the user.

Of course _count should return the same error as _search :)
</comment><comment author="pabluk" created="2013-06-20T07:23:04Z" id="19734777">+1 to keep the same behavior with an invalid query made on "_search" or "_count".
</comment><comment author="spinscale" created="2013-06-24T08:49:18Z" id="19895451">Just verified, it still exists in 0.90.1, I will fix it in order to be consistent and not return a 200 OK in the count response headers even if we send an error in the response body
</comment><comment author="spinscale" created="2013-06-27T10:34:59Z" id="20110139">The search and the count implementation differs, because the `SearchRequest` inherits from `ActionRequest`, where the `CountRequest` inherits from `BroadcoastOperationRequest`.

The `TransportBroadcastOperationAction` casts every occuring exception into a `BroadcastShardOperationFailedException` - when the count request returns with a count response, a `BroadcastShardOperationFailedException` (which now contains the query parsing exception) is only marked as a shard failure instead of a query parsing failure.

This is **why** the two implementations differ. Next step is to think about, how to unify it.
</comment><comment author="kimchy" created="2013-06-27T10:36:33Z" id="20110200">@raff an aside, can you share the sample of where an invalid query in a search request returns 500? We should fix it to return 400, we might missed something...
</comment><comment author="pabluk" created="2013-06-27T12:17:21Z" id="20114267">Here an example of an invalid query that returns 500 using `_search`

``` bash
$ curl -v -XGET 'http://localhost:9200/twitter/tweet/_search?q=OR+OR'
&gt; GET /twitter/tweet/_search?q=OR+OR HTTP/1.1
&gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 500 Internal Server Error
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 5795
&lt; 
```

JSON response

``` json
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[s4MQ1vnkS5egTRBd7DS3NA][twitter][4]: RemoteTransportException[[Cypher][inet[/192.168.0.127:9300]][search/phase/query]]; nested: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"OR OR\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; }{[s4MQ1vnkS5egTRBd7DS3NA][twitter][2]: RemoteTransportException[[Cypher][inet[/192.168.0.127:9300]][search/phase/query]]; nested: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"OR OR\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; }{[RW38JhZITs6UhzMj8W_wZQ][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"OR OR\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; }{[RW38JhZITs6UhzMj8W_wZQ][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"OR OR\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; }{[RW38JhZITs6UhzMj8W_wZQ][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"OR OR\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}}}]]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; }]", 
    "status": 500
}
```

and the same query using `_count`

``` bash
$ curl -v -XGET 'http://localhost:9200/twitter/tweet/_count?q=OR+OR'
&gt; GET /twitter/tweet/_count?q=OR+OR HTTP/1.1
&gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 5037
&lt; 
```

JSON response

``` json
{
    "_shards": {
        "failed": 5, 
        "failures": [
            {
                "index": "twitter", 
                "reason": "BroadcastShardOperationFailedException[[twitter][3] ]; nested: RemoteTransportException[[Cypher][inet[/192.168.0.127:9300]][count/s]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; ", 
                "shard": 3
            }, 
            {
                "index": "twitter", 
                "reason": "BroadcastShardOperationFailedException[[twitter][2] ]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; ", 
                "shard": 2
            }, 
            {
                "index": "twitter", 
                "reason": "BroadcastShardOperationFailedException[[twitter][4] ]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; ", 
                "shard": 4
            }, 
            {
                "index": "twitter", 
                "reason": "BroadcastShardOperationFailedException[[twitter][1] ]; nested: RemoteTransportException[[Cypher][inet[/192.168.0.127:9300]][count/s]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; ", 
                "shard": 1
            }, 
            {
                "index": "twitter", 
                "reason": "BroadcastShardOperationFailedException[[twitter][0] ]; nested: RemoteTransportException[[Cypher][inet[/192.168.0.127:9300]][count/s]]; nested: QueryParsingException[[twitter] Failed to parse query [OR OR]]; nested: ParseException[Cannot parse 'OR OR': Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; nested: ParseException[Encountered \" &lt;OR&gt; \"OR \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    ]; ", 
                "shard": 0
            }
        ], 
        "successful": 0, 
        "total": 5
    }, 
    "count": 0
}
```

I'm using elasticsearch 0.90.2 with the default configuration settings and java 1.6

``` bash
$ java -version
java version "1.6.0_27"
OpenJDK Runtime Environment (IcedTea6 1.12.5) (6b27-1.12.5-0ubuntu0.12.04.1)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
```
</comment><comment author="clintongormley" created="2014-07-08T12:02:34Z" id="48327082">This has been fixed.  Both _search and _count now return 400 bad request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fuzzy Search return Error while using Highlight option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/691</link><project id="" key="" /><description>I am searching by using fuzzy query. If i use "highlight" it
will return error but without "highlight" tag it's working fine. I
here mentioned the queries and results.
1. curl -XGET 'http://localhost:9200/testdatabase/datatable/_search' -
   d '{ "sort" :  [ {"_score" : {"reverse" :true } } ],"query" :
   {"fuzzy" : { "myfield": "test" } }, "from" : 0, "size" :
   20,"highlight" : { "tags_schema" : "styled" ,"pre_tags" :
   [ "&lt;em&gt;" ]  ,"post_tags" :  [ "&lt;/em&gt;" ],"fields": { "myfield" :
   {"number_of_fragments" : 0 },"myfield2" : {"number_of_fragments" :
   0 } } }, "explain" : false,"fields" : ["myfield","myfield2" ] }'

Result:
UnsupportedOperationException[FuzzyQuery cannot change rewrite
method]"},{"reason":"UnsupportedOperationException[FuzzyQuery cannot
change rewrite method]"}]},"hits":{"total":39,"max_score":null,"hits":
[]}}
1. curl -XGET 'http://localhost:9200/testdatabase/datatable/_search' -
   d '{ "sort" :  [ {"_score" : {"reverse" :true } } ],"query" :
   {"fuzzy" : { "myfield": "test" } }, "from" : 0, "size" : 20},
   "explain" : false,"fields" : ["myfield","myfield2" ] }'
   
   For this query it's working fine it returns the result well..
</description><key id="602521">691</key><summary>Fuzzy Search return Error while using Highlight option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">senthilsebi</reporter><labels /><created>2011-02-16T03:51:26Z</created><updated>2013-04-04T18:51:45Z</updated><resolved>2013-04-04T18:51:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:51:44Z" id="15916252">This has been fixed at some stage
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST codes: Improve returned error codes on REST APIs, associate codes with internal exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/690</link><project id="" key="" /><description>REST codes: Improve returned error codes on REST APIs, associate codes with internal exceptions
</description><key id="602069">690</key><summary>REST codes: Improve returned error codes on REST APIs, associate codes with internal exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-15T23:38:17Z</created><updated>2011-02-15T23:41:12Z</updated><resolved>2011-02-15T23:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-15T23:41:12Z" id="776193">REST codes: Improve returned error codes on REST APIs, associate codes with internal exceptions, closed by d1d3f8c4ca39471ff551330eea508d31d9aea2ea.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for #688</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/689</link><project id="" key="" /><description /><key id="600802">689</key><summary>Fix for #688</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-02-15T16:11:12Z</created><updated>2014-07-16T21:56:54Z</updated><resolved>2011-02-16T16:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add new Access-Control-Allow-Headers value into http response header</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/688</link><project id="" key="" /><description>Some http clients require Access-Control-Allow-Headers value to be set for preflight OPTIONS requests. I realized that for example jQuery needs this once I switched from 1.4.x to 1.5
</description><key id="600758">688</key><summary>Add new Access-Control-Allow-Headers value into http response header</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-15T15:59:54Z</created><updated>2011-02-21T20:24:09Z</updated><resolved>2011-02-16T05:54:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-02-15T21:54:32Z" id="775832">Add new Access-Control-Allow-Headers value into http response header, closed by 12a33e6a913093a7cba3feaf98f236a0fef30142
</comment><comment author="kimchy" created="2011-02-15T21:55:21Z" id="775837">Pushed the change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ThreadPool: Refactor into several pools, with configurable types per pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/687</link><project id="" key="" /><description>Refactor the current single thread pool into several pools that can be used based on operation performed. This is handy for example to make sure search is only done on a specific thread pool since it associates resources per thread.

The new thread pools include:
- `index`: for index/delete/bulk operations.
- `search`: For get/count/search operations.

Each pool can have a `type` associated with it, and based on the type, associated parameters. The types can be `cached`, `fixed`, `scaling`, and `blocking`.
</description><key id="599578">687</key><summary>ThreadPool: Refactor into several pools, with configurable types per pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-15T04:36:24Z</created><updated>2011-02-15T05:17:36Z</updated><resolved>2011-02-15T05:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-15T05:17:36Z" id="772974">ThreadPool: Refactor into several pools, with configurable types per pool, closed by 3ed848a495a494538a9071ccd447f23fa07fb7f2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support arbitrary queries for mlt queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/686</link><project id="" key="" /><description>I would like to be able to make MLT queries from arbitrary queries. Here's an example:

```
{
"query" : {
    "term": {
        "link": "http://www.examiner.com/men-s-relationship-advice-in-wilmington/the-masks-of-men" 
    },
    "more_like_this" : {
        "min_query_freq" : 1,
        "max_query_terms" : 12 
    } 
} 
```

}

This should execute a MLT query on the document maching the `{ "link": "http://www.examiner.com/men-s-relationship-advice-in-wilmington/the-masks-of-men" }` criteria. Also, note that I shouldn't need to specify `like_text` from the MLT query because it will be taken from the matching document 
</description><key id="594575">686</key><summary>Support arbitrary queries for mlt queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dfdeshom</reporter><labels /><created>2011-02-12T22:33:06Z</created><updated>2013-04-04T18:53:10Z</updated><resolved>2013-04-04T18:53:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:53:10Z" id="15916348">This is can be easily done with the MLT api, just providing the ID for the document. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: If a field ends up not being highlighted, don't return it in the response.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/685</link><project id="" key="" /><description>currently, its a bit inconsistent, sometimes returning an empty array, sometimes returning a null value. Now, just don't return it.
</description><key id="593650">685</key><summary>Highlighting: If a field ends up not being highlighted, don't return it in the response.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.15.0</label></labels><created>2011-02-12T08:26:48Z</created><updated>2011-02-12T08:27:05Z</updated><resolved>2011-02-12T08:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-12T08:27:05Z" id="765202">pushed over several commits.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter facets documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/684</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/reference/api/search/facets/filter-facet.html
https://github.com/elasticsearch/elasticsearch.github.com/blob/master/guide/reference/api/search/facets/filter-facet.textile

Should "filter" really be "facet_filter"?
</description><key id="593099">684</key><summary>Filter facets documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ibrusic</reporter><labels /><created>2011-02-11T23:12:34Z</created><updated>2011-02-13T21:44:27Z</updated><resolved>2011-02-14T04:28:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-12T13:50:44Z" id="765571">No, the `filter` facet gives you a count of the filter in question. The `facet_filter` can be placed on _any_ facet to further filter out the docs it will be executed on.
</comment><comment author="ibrusic" created="2011-02-13T20:28:34Z" id="768324">Thank you for the clarification.  facet_filter is not documented in the guide, so perhaps having both listed would have lessened my confusion.  Both filters work like a charm!
</comment><comment author="kimchy" created="2011-02-13T21:44:27Z" id="768496">Its documented under the facets section here: http://www.elasticsearch.org/guide/reference/api/search/facets/index.html, since it applies to all facets.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query: boosting query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/683</link><project id="" key="" /><description>The `boosting` class can be used to effectively demote results that match a given query. Unlike the "NOT" clause in `bool` query, this still selects documents that contain undesirable terms, but reduces their overall score.

```
{
    "boosting" : {
        "positive" : {
            "term" : {
                "field1" : "value1"
            }
        },
        "negative" : {
            "term" : {
                "field2" : "value2"
            }
        },
        "negative_boost" : 0.2
    }
}
```
</description><key id="592406">683</key><summary>Query: boosting query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-11T17:58:42Z</created><updated>2011-02-11T19:01:58Z</updated><resolved>2011-02-12T01:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-11T17:59:28Z" id="763631">Query: boosting query, closed by 1b5cdb181a1efb92a4c0ef793a9940f06cbb9fc3.
</comment><comment author="clintongormley" created="2011-02-11T18:57:04Z" id="763813">kimchy - is this a new query type? same level as "filtered"?
</comment><comment author="kimchy" created="2011-02-11T19:01:57Z" id="763822">Yea, its a new query type, same level as all of them, since its an AST, `filtered`, `term`, and the rest.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't fail search if highlight field is missing for a specific type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/682</link><project id="" key="" /><description>If a field is missing the highlights raise an exception:
HighlightPhase:line 57

```
if (mapper == null) {
            throw new SearchException(context.shardTarget(), "No mapping found for [" + field.field() + "]");
}
```

and no results is returned.

Mainly in a multi-documents search the highlight requires that every documents as the similar fields for highlighting.

A probably fix can be:

```
 if (mapper == null) {
         continue;
 }
```
</description><key id="588844">682</key><summary>Don't fail search if highlight field is missing for a specific type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-10T11:09:41Z</created><updated>2011-02-10T17:04:19Z</updated><resolved>2011-02-10T17:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aparo" created="2011-02-10T17:04:19Z" id="759789">Don&amp;#39;t fail search if highlight field is missing for a specific type. Closed by 5ea58a338797c6ea633fb0c8de37680b739f6a45
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API: Failure to index docs that have their ids URL encoded and contain `/`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/681</link><project id="" key="" /><description>For example, this will fail:

```
curl -XPUT http://localhost:9200/backupify/google_calendar/http%3A%2F%2Fwww.google.com -d '{
                                "name" : "Eric Alexander"
                              }'
```
</description><key id="588068">681</key><summary>REST API: Failure to index docs that have their ids URL encoded and contain `/`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-02-10T01:14:54Z</created><updated>2013-04-16T16:01:25Z</updated><resolved>2011-02-10T01:18:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-10T01:18:14Z" id="757691">REST API: Failure to index docs that have their ids URL encoded and contain `/`, closed by 57108c8575b39e7b6c0e7c2ca4bf20f03e6d0912.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scrolling returns strange results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/680</link><project id="" key="" /><description>As discussed under:

http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/5c93b640dee0dfd3

the following integration test can reproduce this behaviour:

https://gist.github.com/819239

Again when I don't specify the index in prepareSearch it works, it also works for shards==1, it also works if id is NOT in the range of the second bulkUpdate.

Hope I did not something completly stupid :/
</description><key id="587074">680</key><summary>Scrolling returns strange results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-02-09T20:46:59Z</created><updated>2011-02-22T14:07:36Z</updated><resolved>2011-02-22T14:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-02-09T21:25:36Z" id="755453">Ok. found it. I need to add the sort:

addSort(NUM, SortOrder.ASC)

then both QUERY_*_FETCH will pass

I guess this is not a bug, only not documented?

BTW for others: this scrolling is very memory intensive. scanning (if implemented) should be used instead: #605
</comment><comment author="karussell" created="2011-02-09T21:28:37Z" id="755475">will this work also under every condition? (it worked for this example)

addSort("_id", SortOrder.ASC)
</comment><comment author="karussell" created="2011-02-22T14:07:36Z" id="797218">as of #707 this can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Null-value fields in custom_score queries causes 'No field' errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/679</link><project id="" key="" /><description>There seems to be a gotcha when using custom_score queries and trying to score based on fields that can contain null values.

Given I have:

```
{"user": {"name": "John", "position": null}}
```

When I query using a custom_score and a script like:

```
{"script": "_score + doc['position'].value"}
```

I get the following error:

```
ElasticSearchIllegalArgumentException[No field found for [position]]; }]
```

If I add another user:

```
{"user": {"name": "Jane", "position": 1}}
```

Everything works fine.

I think the default behavior should be for null-value fields to return null - and not raise an error. 

The examples above might seem lame, but imagine filling the indices with data from a database, where no guarantees can be made that a column won't contain anything other than null-values. You can get around this by typecasting your fields to strings by using mapping, but I definitely suggest that ElasticSearch is able to handle the script value more intelligently.
</description><key id="585660">679</key><summary>Null-value fields in custom_score queries causes 'No field' errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pfeiffer</reporter><labels /><created>2011-02-09T11:33:21Z</created><updated>2017-05-15T12:19:39Z</updated><resolved>2013-04-04T18:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pfeiffer" created="2011-02-09T11:44:00Z" id="753436">Oh - and thanks for an awesome product! :-)
</comment><comment author="kimchy" created="2011-02-09T18:56:58Z" id="754800">Heya,

This happens because that field has not been introduced yet (it has a null value). Once it is introduced (with a single value, into the cluster), then it will work fine. You can also define `null-value` in the mappings that will index a specific null value when that field has is `null`. Since its not introduced, its type can't be derived.
</comment><comment author="pfeiffer" created="2011-02-10T05:08:10Z" id="758081">Yes, I understand that. Wouldn't it be better for the script value to return null or at least to have some kind of function to check for it's existence, instead of raising an exception and causing no results to be returned?

Eg:

```
{"script": "_score + (doc['position'].present ? doc['position'] : 0)"}
```
</comment><comment author="kimchy" created="2011-02-11T01:10:27Z" id="761428">This should work:

```
{"script": "_score + (doc.containsKey('position') ? doc['position'].value : 0)"}
```

Or maybe even this:

```
{"script": "_score + (doc['position'] ? doc['position'].value : 0)"}
```

Even so, with enough data set, where at least one doc has a value for `position` on each shard, you should not need this check, and then the performance of your script will be much better.
</comment><comment author="mandeepsingh3664" created="2017-05-15T12:19:39Z" id="301458360">Fatal error: Uncaught [NoFieldFound]No field found: Contact._PloiciesLastApplicationDate0 Attempted: 1 time(s). thrown 

I am getting the following error while updating the contact.

for example:-

$checkExist = Infusionsoft_DataService::query(new Infusionsoft_Contact(), array('Phone1' =&gt; '(618)-795-0304'));

I am fetching the contact and updating the contact based on the phone number, But it give this error?

Thanks!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms facet on an IP field returns terms as numbers, not IPs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/678</link><project id="" key="" /><description>Fiddling with the facet APi tonight. Found that a facet term query on a field that is an IP address ends up coming back with the 'term' as a string number, not the actual IP address in dotted notation.

My query:

```
'{"size":1, "query": { "query_string": { "query": "clientip:*" } }, "facets": { "foo": { "terms": { "field": "clientip", "size": 4 } } } }
```

Results:
    {
      ...
          "_source" : {..., clientip":["76.21.79.138"] ... }
      ...
      "facets" : {
        "foo" : {
          "_type" : "terms",
          "_field" : "clientip",
          "terms" : [ {
            "term" : "1136881900",
            "count" : 12375
          }, {
            "term" : "3497226231",
            "count" : 4223
          }, {
            "term" : "1123633115",
            "count" : 3215
          }, {
            "term" : "1600953340",
            "count" : 2312
          } ]
        } 
      } 
    } 
</description><key id="585619">678</key><summary>terms facet on an IP field returns terms as numbers, not IPs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-09T11:13:59Z</created><updated>2013-07-26T11:43:04Z</updated><resolved>2011-02-09T19:37:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2011-02-09T11:14:38Z" id="753380">this is observed in 0.14.1, haven't tried other versions.
</comment><comment author="kimchy" created="2011-02-09T19:37:09Z" id="754944">Right, I will push a fix for this. Note that in master (0.15) the ip type is no longer dynamically applied.
</comment><comment author="kimchy" created="2011-02-09T19:37:58Z" id="754950">terms facet on an IP field returns terms as numbers, not IPs, closed by b26d86293f080fd996b4f223efb2fa9f7dbfda4e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>retrieving inner fields of _source does not work in a get </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/677</link><project id="" key="" /><description>If you try to use the fields parameter in a get, for fields which are part of the _source, an error is returned. See :

https://gist.github.com/816572

and

http://elasticsearch-users.115913.n3.nabble.com/how-to-retrieve-specific-source-inner-fields-td2442878.html
</description><key id="583353">677</key><summary>retrieving inner fields of _source does not work in a get </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">buggyvelarde</reporter><labels /><created>2011-02-08T15:48:53Z</created><updated>2011-02-22T00:27:25Z</updated><resolved>2011-02-22T00:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-22T00:27:25Z" id="795472">Will be implemented as part of #708.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: By default, don't return the version per hit, unless `version` is set to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/676</link><project id="" key="" /><description>By default, don't return the version for each search hit, but, instead, allow to either pass a url level parameter called `version` and set it it to `true`, or pass in the search request body `"version" : true"` (on the same level as `explain`).
</description><key id="583339">676</key><summary>Search: By default, don't return the version per hit, unless `version` is set to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-08T15:43:38Z</created><updated>2011-02-08T15:50:41Z</updated><resolved>2011-02-08T15:50:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T15:50:41Z" id="750059">Search: By default, don&amp;#39;t return the version per hit, unless `version` is set to `true`, closed by ac8646deb75b46552e5e7bd41700a29d0a79c45e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_point explicit mapping does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/675</link><project id="" key="" /><description>#### having spent 2 days trying to figure out how to use geo_point explicit mapping

gave me NO SUCCESS. the dynamic mapping seems to be active by default, ignoring any explicit mappings I set, as follows:
## 
#### 1. create the index...

$ curl -XPUT 'localhost:9200/geo/'
{
  "ok" : true,
  "acknowledged" : true
}
## 
#### 2. set explicit mapping

$ curl -XPUT 'localhost:9200/geo/places/_mapping?pretty=true' -d '{ "properties": { "location": { "type" :"geo_point" }}}'
{
  "ok" : true,
  "acknowledged" : true
}

above it gives me an _ok_ and _acknowledged_, but I guess it doesn't really mean anything
## 
#### 3. requesting the _mapping returns the following

curl -XGET 'localhost:9200/geo/places/_mapping?pretty=true'
{
  "geo" : {
    "places" : {
      "properties" : {
      }
    }
  }
}
## 
#### 4. adding a new document

curl -XPUT  'localhost:9200/geo/places/1?pretty=true' -d '{ location: "1.0,1.0" }'
{
  "ok" : true,
  "_index" : "geo",
  "_type" : "places",
  "_id" : "1"
}
## 
#### 5. requesting the _mapping after adding the first document,

$ curl -XGET 'localhost:9200/geo/places/_mapping?pretty=true'
{
  "geo" : {
    "places" : {
      "properties" : {
        "location" : {
          "type" : "string"
        }
      }
    } 
  }
}
## 
#### I've repeated these steps several times with different types for "location" (ie.: array, string) WITHOUT any success.

also, trying to _search using a "filtered"-query, as described in the docs, I had no success!
## 
#### resume

a) explicit mappings don't work

b) geo_point-type seems not to be ready

c) I understand that adopting new technologies can be very cumbersome and time consuming, but having a project documentation that is leading literally nowhere, really does frustrate.

d) I hope there'll be an updated documentation on the _mapping and geo_point issue and how to use explicit mappings properly before I throw in the towel
</description><key id="582000">675</key><summary>geo_point explicit mapping does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">plexorama</reporter><labels /><created>2011-02-07T23:39:29Z</created><updated>2011-02-07T23:43:16Z</updated><resolved>2011-02-07T23:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-07T23:41:17Z" id="748156">Ask this on the mailing list, before you open an issue. your mapping definition is wrong. you need to wrap the "properties" element with the type name.
</comment><comment author="plexorama" created="2011-02-07T23:43:16Z" id="748160">thanks, I'll try that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Disable automatic ip type detection on new fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/674</link><project id="" key="" /><description>Don't do automatic ip type detection for fields, since its more common to have fields that are sometimes with ip format, and sometimes with host names.
</description><key id="581930">674</key><summary>Mapping: Disable automatic ip type detection on new fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-07T23:14:00Z</created><updated>2011-02-07T23:19:22Z</updated><resolved>2011-02-08T07:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-07T23:14:52Z" id="748082">Mapping: Disable automatic ip type detection on new fields, closed by 689b20a518eb9cb6441b01e19a5e72bfb0ff535f.
</comment><comment author="kimchy" created="2011-02-07T23:17:12Z" id="748093">And more over, version like fields are sometimes similar to ip fields :(
</comment><comment author="anuragphadke" created="2011-02-07T23:19:22Z" id="748099">+1
sample input to test:
"version": "3.6.22.0"

returns:
"error":"MapperParsingException[Failed to parse [version]]; nested: ElasticSearchIllegalArgumentException[failed ot parse ip [], not full ip address (4 dots)]; "}}]}

kimchy - u owe me coffee :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>documentation for optimize is inaccurate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/673</link><project id="" key="" /><description>The documentation for optimize doesn't mention the waitForMerge parameter.

Also, the documentation lists the flush and refresh parameters as defaulting to "false", but reading the code it looks to me that they default to "true".
</description><key id="580042">673</key><summary>documentation for optimize is inaccurate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels /><created>2011-02-07T10:52:33Z</created><updated>2011-02-08T13:43:34Z</updated><resolved>2011-02-08T13:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T13:43:03Z" id="749622">Right, will fix the docs. you can do it as well on the elasticsearch.github.com repo, or open issues there :)
</comment><comment author="kimchy" created="2011-02-08T13:43:34Z" id="749624">pushed the change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Opening a closed index doesn't allocate empty shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/672</link><project id="" key="" /><description>Same test script as https://github.com/elasticsearch/elasticsearch/issues/issue/615

The final `count` query now gives: 

```
# [Sun Feb  6 20:24:34 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/es_test_1/_count'  -d '
{
   "match_all" : {}
}
'

# [Sun Feb  6 20:24:34 2011] Response:
# {
#    "count" : 1,
#    "_shards" : {
#       "failures" : [
#          {
#             "index" : "es_test_1",
#             "reason" : "BroadcastShardOperationFailedException[[e
# &gt;             s_test_1][0] No active shard(s)]",
#             "shard" : 0
#          },
#          {
#             "index" : "es_test_1",
#             "reason" : "BroadcastShardOperationFailedException[[e
# &gt;             s_test_1][1] No active shard(s)]",
#             "shard" : 1
#          },
#          {
#             "index" : "es_test_1",
#             "reason" : "BroadcastShardOperationFailedException[[e
# &gt;             s_test_1][3] No active shard(s)]",
#             "shard" : 3
#          },
#          {
#             "index" : "es_test_1",
#             "reason" : "BroadcastShardOperationFailedException[[e
# &gt;             s_test_1][4] No active shard(s)]",
#             "shard" : 4
#          }
#       ],
#       "failed" : 4,
#       "successful" : 1,
#       "total" : 5
#    }
# }
```

If I index 5 docs instead, then all shards are allocated on re-opening
</description><key id="579005">672</key><summary>Opening a closed index doesn't allocate empty shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-02-06T19:30:49Z</created><updated>2011-03-20T14:30:15Z</updated><resolved>2011-03-20T14:30:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-03-20T14:30:15Z" id="895251">Fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make get/delete error codes more consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/671</link><project id="" key="" /><description>When getting or deleting a doc/index/mapping/river/percolator which doesn't exist, we get inconsistent error codes:

```
                            get                 delete
----------------------------------------------------------------
doc                         404                 404
doc (missing type)          500 TypeMissing     404*
doc (missing index)         500 IndexMissing    404*
index                       N/A                 400
mapping                     {}                  {}
mapping (missing index)     {}                  400 
percolator                  404                 404
percolator (no _percolator) 500 IndexMissing    404*
river                       500 TypeMissing     {}
river (no _river)           500 IndexMissing    400** 
template                    {}                  400
```

Should these all return 404's instead?

Also, `delete` auto-creates index and type and `delete_percolator` auto-creates the `_percolator` index, but `delete_river` doesn't create the `_river` index

So for instance:

```
get { index: foo, type: bar, id: 1 }
-&gt; 500 IndexMissing

delete { index: foo, type: bar, id: 1 }
-&gt; 404

get { index: foo, type: bar, id: 1 }
-&gt; 404
```

... which seems a bit weird
</description><key id="578823">671</key><summary>Make get/delete error codes more consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-02-06T17:15:34Z</created><updated>2011-03-20T14:27:12Z</updated><resolved>2011-03-20T14:27:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-02-06T19:49:02Z" id="744200">While we're on it, should a `create_index` request (where the index already exists) return a 409 instead of a 400?
</comment><comment author="clintongormley" created="2011-03-20T14:27:12Z" id="895243">Fixed 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>flush with full=true is broken: this IndexWriter is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/670</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/es_test_1/'
# {"ok":true,"acknowledged":true}

curl -XPOST 'http://127.0.0.1:9200/_flush?error_trace=&amp;full=true&amp;pretty=true' 
# {
#   "ok" : true,
#   "_shards" : {
#     "total" : 41,
#     "successful" : 0,
#     "failed" : 21,
#     "failures" : [ {
#       "index" : "test",
#       "shard" : 0,
#       "reason" : "BroadcastShardOperationFailedException[[test][0] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "test",
#       "shard" : 1,
#       "reason" : "BroadcastShardOperationFailedException[[test][1] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "test",
#       "shard" : 2,
#       "reason" : "BroadcastShardOperationFailedException[[test][2] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "test",
#       "shard" : 3,
#       "reason" : "BroadcastShardOperationFailedException[[test][3] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "test",
#       "shard" : 4,
#       "reason" : "BroadcastShardOperationFailedException[[test][4] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "foo",
#       "shard" : 0,
#       "reason" : "BroadcastShardOperationFailedException[[foo][0] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "foo",
#       "shard" : 1,
#       "reason" : "BroadcastShardOperationFailedException[[foo][1] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "foo",
#       "shard" : 2,
#       "reason" : "BroadcastShardOperationFailedException[[foo][2] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "foo",
#       "shard" : 3,
#       "reason" : "BroadcastShardOperationFailedException[[foo][3] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "foo",
#       "shard" : 4,
#       "reason" : "BroadcastShardOperationFailedException[[foo][4] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "es_test_1",
#       "shard" : 0,
#       "reason" : "BroadcastShardOperationFailedException[[es_test_1][0] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "es_test_1",
#       "shard" : 1,
#       "reason" : "BroadcastShardOperationFailedException[[es_test_1][1] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "es_test_1",
#       "shard" : 2,
#       "reason" : "BroadcastShardOperationFailedException[[es_test_1][2] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "es_test_1",
#       "shard" : 3,
#       "reason" : "BroadcastShardOperationFailedException[[es_test_1][3] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "es_test_1",
#       "shard" : 4,
#       "reason" : "BroadcastShardOperationFailedException[[es_test_1][4] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "bar",
#       "shard" : 0,
#       "reason" : "BroadcastShardOperationFailedException[[bar][0] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "bar",
#       "shard" : 1,
#       "reason" : "BroadcastShardOperationFailedException[[bar][1] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "bar",
#       "shard" : 2,
#       "reason" : "BroadcastShardOperationFailedException[[bar][2] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "bar",
#       "shard" : 3,
#       "reason" : "BroadcastShardOperationFailedException[[bar][3] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "bar",
#       "shard" : 4,
#       "reason" : "BroadcastShardOperationFailedException[[bar][4] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     }, {
#       "index" : "_percolator",
#       "shard" : 0,
#       "reason" : "BroadcastShardOperationFailedException[[_percolator][0] ]; nested: AlreadyClosedException[this IndexWriter is closed]; "
#     } ]
#   }
# }
```
</description><key id="578509">670</key><summary>flush with full=true is broken: this IndexWriter is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-02-06T13:56:25Z</created><updated>2011-02-08T09:50:32Z</updated><resolved>2011-02-08T09:50:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T09:49:28Z" id="749161">this was introduced in master changes, will push a fix...
</comment><comment author="kimchy" created="2011-02-08T09:50:32Z" id="749165">flush with full=true is broken: this IndexWriter is closed, closed by da9f4992e046fb5c79471444ab95fd6b3e9cdc34.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve error when creating a percolator with no query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/669</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/_percolator/foo/test'  -d '
{
   "foo" : "bar"
}
'

# [Sat Feb  5 20:01:56 2011] Response:
# {
#    "error" : "NullPointerException[null value]"
# }
```
</description><key id="577391">669</key><summary>Improve error when creating a percolator with no query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-02-05T19:03:04Z</created><updated>2011-02-08T10:52:57Z</updated><resolved>2011-02-08T10:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T10:52:57Z" id="749309">Improve error when creating a percolator with no query, closed by 37b1415b578a43b45c9acd82095d14e6bca3948b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Occasional search misses after refreshing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/668</link><project id="" key="" /><description>after indexing a document and calling _refresh, the document should be made immediately available for the subsequent search request.

using 0.14.4 and 0.15.0-snapshot, the search fails ~20% of the time.

here is a simple curl script that reproduces the issue.
https://gist.github.com/811312

opened this issue as result of discussion on IRC.
</description><key id="575351">668</key><summary>Occasional search misses after refreshing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wireframe</reporter><labels /><created>2011-02-04T16:39:20Z</created><updated>2013-04-04T18:53:46Z</updated><resolved>2013-04-04T18:53:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T11:02:13Z" id="749326">can you give latest master a go? I think I fixed it (I ran the script many times and did not get a failure)
</comment><comment author="wireframe" created="2011-02-08T12:21:26Z" id="749461">initial tests look good!  I'm not able to reproduce the issue.  If i find anything, i'll open a new issue.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facet: gives different results depending on size value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/667</link><project id="" key="" /><description>In short: insert 816 documents into index and execute terms facet query with count:3 and count:2. We are getting different results!

```
curl -XDELETE 'localhost:9200/index'

curl -XDELETE localhost:9200/_template/x

curl -XPUT localhost:9200/_template/x -d '
{
    "template" : "*",
    "settings" : {
        "number_of_shards" : 3,
        "number_of_replicas" : 1
    },
    "mappings" : {
      "type" : {
        "properties" : {
          "my-id" : { "type" : "string", "store" : "yes", "index" : "not_analyzed", "null_value" : "na",  "include_in_all" : "false" }
        }
      }
    }
}'

# insert 815 documents
i=0
while (( $i &lt; 815 )); do
let i++
curl -XPUT "localhost:9200/index/type/${i}" -d "
{
    \"my-id\" : \"string-${i}\"
}"
done

# insert one extra document with predefined ID
curl -XPUT 'localhost:9200/index/type/oUCrrMOhQBu39woEachSSw' -d "
{
    \"my-id\" : \"string-${i}\"
}"

curl -XPOST 'localhost:9200/index/_refresh'

echo
echo "Good result"
echo

curl -XPOST 'localhost:9200/_search?pretty=1' -d '
{
  "query" : { "match_all" : {}}, "size" : 0,
  "facets" : {
    "my-id" : {
      "terms" : {
        "field" : "my-id",
        "size" : 3,
        "order" : "count"
      }
    }
  }
}'

echo
echo "Bad result"
echo

curl -XPOST 'localhost:9200/_search?pretty=1' -d '
{
  "query" : { "match_all" : {}}, "size" : 0,
  "facets" : {
    "my-id" : {
      "terms" : {
        "field" : "my-id",
        "size" : 2,
        "order" : "count"
      }
    }
  }
}'
```

This outputs two different results:

```
Good result

{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "failed" : 0
  },
  "hits" : {
    "total" : 816,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "facets" : {
    "my-id" : {
      "_type" : "terms",
      "_field" : "my-id",
      "missing" : 0,
      "terms" : [ {
        "term" : "string-815",
        "count" : 2
      }, {
        "term" : "string-99",
        "count" : 1
      }, {
        "term" : "string-98",
        "count" : 1
      } ]
    }
  }
}
```

and

```
Bad result

{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "failed" : 0
  },
  "hits" : {
    "total" : 816,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "facets" : {
    "my-id" : {
      "_type" : "terms",
      "_field" : "my-id",
      "missing" : 0,
      "terms" : [ {
        "term" : "string-99",
        "count" : 1
      }, {
        "term" : "string-98",
        "count" : 1
      } ]
    }
  }
}
```

The bad result is missing the following terms facet:

```
{
    "term" : "string-815",
    "count" : 2
}
```
</description><key id="572259">667</key><summary>Terms Facet: gives different results depending on size value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>non-issue</label></labels><created>2011-02-03T11:05:14Z</created><updated>2014-09-30T10:18:04Z</updated><resolved>2011-02-06T13:25:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="KaySackey" created="2011-02-06T10:23:09Z" id="743261">The size value under 'facet' refers to the maximum number of facets to be returned.

So a size of 2 only returns 2 facets, excluding the facet with count: 2 because it is #3 by the ordering.
</comment><comment author="lukas-vlcek" created="2011-02-06T11:49:48Z" id="743361">Actually, the `order` of facet is set to `count` which means the higher the `count` value the higher the `term` in facet results. So any term with count &gt; 1 should _always_ be listed before terms with count = 1.

What you can see in this example is the result of how Elastic Search implements term facets internally. In fact it is a compromise between accuracy of distributed calculation and efficiency. Note that if you run this example on a single shard index (change `number_of_shards` to 1 in the template) then you get correct result.

As of now there is probably not a simple general solution. I wonder if other distributed systems can handle this type of calculation correctly in such extreme case like in this example without significant performance penalty.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Status: Add primary store size to include only primary shards store sizes, also move index store and translog into their own elements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/666</link><project id="" key="" /><description>Index Status: Add primary store size to include only primary shards store sizes, also move index store and translog into their own elements
</description><key id="571165">666</key><summary>Index Status: Add primary store size to include only primary shards store sizes, also move index store and translog into their own elements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-02T22:49:49Z</created><updated>2011-02-02T22:50:49Z</updated><resolved>2011-02-02T22:50:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-02T22:50:49Z" id="733975">Index Status: Add primary store size to include only primary shards store sizes, also move index store and translog into their own elements, closed by 5fe2615ba7d41dd6596c020db4bd7fb7f248d58f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Failure when sorting on `short` type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/665</link><project id="" key="" /><description>When sorting on `short` type, search fails.
</description><key id="570018">665</key><summary>Search: Failure when sorting on `short` type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-02-02T15:47:13Z</created><updated>2011-02-02T15:55:33Z</updated><resolved>2011-02-02T15:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-02T15:55:33Z" id="732584">Search: Failure when sorting on `short` type, closed by 904bee12bc3a3ea4891fe40e8c894f8a52d174a7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add HTTPS and basic authentication support to elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/664</link><project id="" key="" /><description>This would help with those that are hosting elasticsearch and need security when making calls from another machine which cannot be used with the TCP transport.

HTTPS support should allow for configuration of the SSL certificate from a path, along with any other easily discoverable certificate sources.
</description><key id="566947">664</key><summary>Add HTTPS and basic authentication support to elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">casperOne</reporter><labels /><created>2011-02-01T13:41:51Z</created><updated>2017-02-22T23:25:18Z</updated><resolved>2015-01-27T19:47:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaelcaplan" created="2011-05-31T21:14:32Z" id="1270252">I second this.  I have legal requirements to protect private patient information in transit and at rest.  Native HTTPS, or SSL support would help solve a good piece of this problem.
</comment><comment author="whateverdood" created="2011-06-01T17:02:13Z" id="1275649">+1 - an encrypted transport is required at my client site.
</comment><comment author="michaelcaplan" created="2011-06-01T17:50:17Z" id="1275968">Related to this would be locking down ES node to node communication over SSL as well.
</comment><comment author="whateverdood" created="2011-06-01T22:32:59Z" id="1277707">Yep - I'd like both an HTTPS entry point and SSL inter-node comms.
</comment><comment author="jdzurik" created="2011-06-22T20:18:16Z" id="1420719">I agree +1 for me too.
</comment><comment author="ghost" created="2011-07-21T20:52:37Z" id="1626924">+1000!
</comment><comment author="karmi" created="2011-07-21T21:49:46Z" id="1627341">I don't think there's a real need for supporting this in _ElasticSearch_ itself? Most installations where this is neccessary could easily use a reverse proxy such as Nginx?
</comment><comment author="ghost" created="2011-07-21T21:58:37Z" id="1627406">I think data is very sensitive and shouldn't rely on validation from third parties.

The data is in ES, ES should do the auth since it knows it's own data structure the most.
</comment><comment author="michaelcaplan" created="2011-07-21T22:49:45Z" id="1627710">The simplicity of native encrypted transport is highly attractive.  The complexity and fragility of wrapping all data in transit through an SSL proxy would be a barrier.
</comment><comment author="jdzurik" created="2011-07-22T05:16:47Z" id="1629629">SSL at least  ... I mean it's an http transport layer not some abstract peice of code.
</comment><comment author="jdzurik" created="2011-08-31T02:54:24Z" id="1951972">Well since it seems people want this but dons't sound like its something you want to build in directly ... maybe a plugin would be good? I looked at Nginx and while it looks supper light weight and pretty clean it would add another virtual hop and, when you already have the extra weight of SSL it would be nice to have the ability to access ES directly with the least amount of extra overhead. I wonder if http://www.openssl.org/ would be helpful in integrating.   
</comment><comment author="ibotty" created="2011-09-08T23:29:23Z" id="2046347">i guess a simple tutorial on how to set up nginx would suffice. nginx is not that hard to setup.
</comment><comment author="BenHall" created="2011-11-13T16:14:32Z" id="2722952">I was wondering if anyone was working on this issue, if not I would like to take a shot. 
</comment><comment author="casperOne" created="2011-11-14T02:34:38Z" id="2726101">I don't believe so (and I think Shay has a lot of other things going on _smiles_).  I'd contribute, but I'm a .NET programmer by trade, and don't have the skills in Java to make an effective contribution on this issue.

If you're going to take a shot, note, that they really have to go hand-in-hand; HTTPS is nice, but without some kind of authentication, you don't gain much for it.  Additionally, you'll have to come up with a way to specify the credentials (username/password) of the users that can access the cluster.
</comment><comment author="skade" created="2011-11-23T08:55:35Z" id="2846355">I don't have any use-case for HTTPS (i use Nginx as SSL offloader to great effect), but needed HTTP Basic. You can find my plugin here:

https://github.com/Asquera/elasticsearch-http-basic

My two cents on SSL: Supporting SSL does not only mean implementing all the nuts and bolts (certificate management, etc.), it also has to be efficient and safe. There are other projects (stunnel, nginx, your favourite hardware load balancer) that are much better at doing all that. If you want your elasticsearch to speak ssl externally without configuring nginx - bind it to a local port and put stunnel in front of it. This is a common and tested solution.
</comment><comment author="whateverdood" created="2011-11-23T16:00:44Z" id="2850489">I voted for this ticket because I need ES nodes to be able to internally
communicate via 2-way SSL. It's a customer requirement.

On Wed, Nov 23, 2011 at 3:55 AM, Florian Gilcher &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; I don't have any use-case for HTTPS (i use Nginx as SSL offloader to great
&gt; effect), but needed HTTP Basic. You can find my plugin here:
&gt; 
&gt; https://github.com/Asquera/elasticsearch-http-basic
&gt; 
&gt; My two cents on SSL: Supporting SSL does not only mean implementing all
&gt; the nuts and bolts (certificate management, etc.), it also has to be
&gt; efficient and safe. There are other projects (stunnel, nginx, your
&gt; favourite hardware load balancer) that are much better at doing all that.
&gt; If you want your elasticsearch to speak ssl externally without configuring
&gt; nginx - bind it to a local port and put stunnel in front of it. This is a
&gt; common and tested solution.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/664#issuecomment-2846355
</comment><comment author="casperOne" created="2011-11-26T16:18:02Z" id="2882320">@skade: That isn't always feasible; especially when you are on a cloud infrastructure.  As an example, Azure, which I've been able to get it to run on, won't let me install any of those.

It might be common and tested, but it's not always applicable.
</comment><comment author="prb" created="2011-12-29T18:12:06Z" id="3303307">@skade Your suggestion for using `nginx` or `stunnel` is fine from a whiteboard perspective, but nginx isn't going to deal with cluster communications, and `stunnel` can be a bit of an operational mess.

@BenHall I'd be interested in lending a hand on the issue as well  I would see it as touching two core components, the `transport` module and the `http` module.
</comment><comment author="karmi" created="2011-12-29T18:15:39Z" id="3303359">@prb: That's interesting -- could you elaborate why an Nginx-based proxy (for example) isn't enough? (I don't understand the _nginx isn't going to deal with cluster communications_ part. You can restrict the in-cluster communication based on IPs.)
</comment><comment author="prb" created="2011-12-29T18:27:22Z" id="3303507">Elasticsearch uses the network for both external communications (e.g., over HTTP via the `http` module or via the `transport` module) and internal communications (e.g., shuttling data between nodes, cluster membership, etc.).  All of that communication should be secured, and not all of it is over HTTP.
</comment><comment author="karmi" created="2011-12-29T18:33:39Z" id="3303579">@prb: You can disable `http` on nodes entirely, and they will communicate via `transport`, which is not open, or? You can restrict how nodes communicate. At EC2, you can further restrict access to 9200/9300 to certain IPs, etc. So I still don't get why is auth needed here. Of course, something like Nginx-based proxy is only meant for authorizing access from outside the cluster.
</comment><comment author="asanderson" created="2011-12-29T18:56:14Z" id="3303840">+1 This is a major deficiency from a government security requirements perspective, and may be a blocker for our project moving from Solr to ElasticSearch, since we have our Solr shards locked down via tomcat.
</comment><comment author="kimchy" created="2011-12-30T12:08:37Z" id="3312728">@asanderson you can deploy elasticsearch as a war file within a wen container if you want using the wares plugin (check the transport-wares repo). Questions in the mailing list.
</comment><comment author="asanderson" created="2012-01-02T14:24:54Z" id="3330059">@kimchy Excellent! Other than performance, are there any other disadvantages to deploying via war?
</comment><comment author="kimchy" created="2012-01-02T21:15:49Z" id="3333742">@asanderson not really, same API, quite simply wrapping as well. If you are up to it, would love to get async support to the servlet based on the servlet 3.0 async feature :)
</comment><comment author="asanderson" created="2012-01-03T16:06:07Z" id="3341777">@kimchy good to know. 

One of these days, I'd love to contribute, but right now I don't have the bandwidth. 

Sorry, but the best I can do is to continue to evangelize ElasticSearch.  ;-)

FWIW, I've spent the last 5+ years replacing expensive COTS products with Solr, and I must say that ElasticSearch out-of-the-box seems to address all of Solr's shortcomings, IMHO.

Now, I get to spend the next year or so replacing Solr with ElasticSearch. ;-)
</comment><comment author="imotov" created="2012-02-27T13:31:03Z" id="4193372">We just released Jetty plugin for elasticsearch. It is a drop-in HTTP transport replacement that exposes full power of embedded Jetty including support for SSL, logging, authentication and access control. It is similar to transport-wares plugin that Shay mentioned above, but instead of running elasticsearch inside a web server, it embeds Jetty web server into elasticsearch. See https://github.com/sonian/elasticsearch-jetty for more information.
</comment><comment author="asanderson" created="2012-02-27T15:03:37Z" id="4194990">Outstanding! Great job! It just keeps getting better and better. ;-)
</comment><comment author="ejain" created="2012-04-06T07:20:37Z" id="4991726">So I can set up a reverse proxy or use the Jetty plugin to secure access to elasticsearch, but now I can no longer use the Java API...
</comment><comment author="kevandunsmore" created="2012-05-28T15:31:01Z" id="5967317">There's been some good progress on this issue from plugin contributors (thanks imotov and kimchy) but I've not yet seen anything that addresses the inter node cluster communications.  HTTP proxies like Apache and Nginx won't cut it there, as the communications between nodes are lower level.

So, any form of movement on that front?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Scripts: Allow to access score in facet related scripts using `doc.score`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/663</link><project id="" key="" /><description>In all scripts that can be used in facets, allow to access the current doc score using `doc.score`.
</description><key id="566709">663</key><summary>Search Scripts: Allow to access score in facet related scripts using `doc.score`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-01T11:41:23Z</created><updated>2011-02-01T11:42:37Z</updated><resolved>2011-02-01T11:42:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-01T11:42:37Z" id="728211">Search Scripts: Allow to access score in facet related scripts using `doc.score`, closed by 8927e1dee4b069bbb9cc3f345ee50a6275ae3f5f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: When sorting, allow to pass `track_scores` and set it to `true` to get scores/max_score back</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/662</link><project id="" key="" /><description>When sorting on a field, scores are not computed. By setting `track_scores` to `true`, scores will still be computed and tracked.
</description><key id="566602">662</key><summary>Search: When sorting, allow to pass `track_scores` and set it to `true` to get scores/max_score back</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-02-01T10:38:02Z</created><updated>2011-02-01T10:39:08Z</updated><resolved>2011-02-01T10:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-01T10:39:08Z" id="728080">Search: When sorting, allow to pass `track_scores` and set it to `true` to get scores/max_score back, closed by cc6f65f8b83d133aa18eb771f51e937b19478c8e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: All geo components that accept an array of [lat, lon] to change to do [lon, lat] to conform with GeoJSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/661</link><project id="" key="" /><description>All geo components: `geo_point` type, all the geo related filters and facets, and sort that accept an array in `[lat, lon]`, changed to `[lon, lat]` to conform to GeoJSON.
</description><key id="565727">661</key><summary>Geo: All geo components that accept an array of [lat, lon] to change to do [lon, lat] to conform with GeoJSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-31T23:31:43Z</created><updated>2011-01-31T23:39:22Z</updated><resolved>2011-01-31T23:39:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-31T23:39:22Z" id="726875">Geo: All geo components that accept an array of [lat, lon] to change to do [lon, lat] to conform with GeoJSON, closed by 5da14a7ed1ed6ef2c1817491e3f8cd8a8107948a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot recover from gateway when using the S3 gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/660</link><project id="" key="" /><description>The following scenario causes ES to lock at not being able to recover from gateway:
1. we start with an empty ES instance
2. configure it to use the S3 gateway
3. start
4. stop
5. start
6. check the status - it's constantly "not recovered from gateway"

The workaround seems to be to do any index-creating operation (e.g. PUT) between steps 3 and 4. Then subsequent node starts work without problems.
</description><key id="564022">660</key><summary>Cannot recover from gateway when using the S3 gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamw</reporter><labels /><created>2011-01-31T12:29:57Z</created><updated>2011-03-02T08:14:14Z</updated><resolved>2011-03-02T08:14:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T09:32:30Z" id="749120">This was fixed in master, can you give it a go?
</comment><comment author="adamw" created="2011-03-02T08:14:14Z" id="825444">Works great, thanks! :)

Adam
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE during ES startup when using S3 gateway with the europe region specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/659</link><project id="" key="" /><description>If I specify the region:

```
cloud:
  aws:
    region: eu-west-1
```

Then there's an NPE during startup:

```
Caused by: java.lang.NullPointerException
at com.amazonaws.AmazonWebServiceClient.setEndpoint(AmazonWebServiceClient.java:87)
at org.elasticsearch.cloud.aws.AwsS3Service.client(AwsS3Service.java:97)
at org.elasticsearch.gateway.s3.S3Gateway.&lt;init&gt;(S3Gateway.java:81)
```

That's because AwsS3Service:91 sets the endpoint to null, while AmazonWebServiceClient:87 de-references the endpoint string.

The fix is to set the endpoint to the default, non-null value.

The workaround is not to specify the region in the config file.
</description><key id="563592">659</key><summary>NPE during ES startup when using S3 gateway with the europe region specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamw</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-31T07:59:38Z</created><updated>2011-02-08T09:31:42Z</updated><resolved>2011-02-08T09:31:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-08T09:31:14Z" id="749117">Right, thats a bug, since EU does not have a specific endpoint, then it should not be set.
</comment><comment author="kimchy" created="2011-02-08T09:31:42Z" id="749118">NPE during ES startup when using S3 gateway with the europe region specified, closed by 05283c7f6dae821570e006e722614eb44a984547.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: When fetching _parent, only the _id should be returned, and not type#id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/658</link><project id="" key="" /><description>Search: When fetching _parent, only the _id should be returned, and not type#id
</description><key id="562731">658</key><summary>Search: When fetching _parent, only the _id should be returned, and not type#id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-30T19:16:22Z</created><updated>2011-01-30T19:19:00Z</updated><resolved>2011-01-30T19:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-30T19:19:00Z" id="722919">Search: When fetching _parent, only the _id should be returned, and not type#id, closed by e516051ea536ee633eae17e0af70e2e6084b92a5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread Pool: Increase the keep alive time of threads from 60 seconds to 60 minutes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/657</link><project id="" key="" /><description>Since there are thread local resources associated with threads, it make sense to keep them around longer than just 60 seconds.
</description><key id="562680">657</key><summary>Thread Pool: Increase the keep alive time of threads from 60 seconds to 60 minutes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-30T18:34:07Z</created><updated>2011-01-30T18:34:54Z</updated><resolved>2011-01-30T18:34:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-30T18:34:54Z" id="722862">Thread Pool: Increase the keep alive time of threads from 60 seconds to 60 minutes, closed by 8b9ec890c3c0f2cf29284aabb3d65aef24e523cf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog Flushing: Improve logic, flush not just by operations in the translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/656</link><project id="" key="" /><description>Allow to define several parameters that will control if a flush will happen. They include:
- `flush_threshold_ops.flush_threshold_ops`: After how many operations to flush. Defaults to `20000`.
- `flush_threshold_ops.flush_threshold_size`: Once the translog hits this size, a flush will happen. Defaults to `500mb`.
- `flush_threshold_ops.flush_threshold_period`: The period with no flush happening to force a flush. Defaults to `60m`.

Note, in terms of backward compatibility, the parameter `index.translog.flush_threshold` is now replaced with `index.translog.flush_threshold_ops`. Though, it is still taken into account if set.

The default flush after ops size has changed from `5000` to `20000`.
</description><key id="562490">656</key><summary>Translog Flushing: Improve logic, flush not just by operations in the translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-30T15:30:33Z</created><updated>2011-01-30T15:41:34Z</updated><resolved>2011-01-30T15:41:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-30T15:41:34Z" id="722603">Translog Flushing: Improve logic, flush not just by operations in the translog, closed by 5b4846b0b68af6d4893322ac51a87d6283930497.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Remove redundant data returned as part of facet response (for example, the field name being faceted)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/655</link><project id="" key="" /><description>Facets: Remove redundant data returned as part of facet response (for example, the field name being faceted).
</description><key id="561459">655</key><summary>Facets: Remove redundant data returned as part of facet response (for example, the field name being faceted)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.15.0</label></labels><created>2011-01-29T18:44:00Z</created><updated>2011-01-29T18:45:19Z</updated><resolved>2011-01-29T18:45:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-29T18:45:19Z" id="721138">Facets: Remove redundant data returned as part of facet response (for example, the field name being faceted), closed by 0b09fd0806364c0785fc649b6483f00fb8e8ebf4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy client doesn't bind to any variables outside the closure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/654</link><project id="" key="" /><description>I have serious troubles with Groovy client not being able to search for a simple query.
The code is like this:
    myVariable = 'search string'
    gclient.search {
        source {
            query {
                queryString myVariable
            }
        }
    }

This will fail because _myVariable_ appears to be always null. The reason is that _GXContentBuilder_ violates Groovy property convention in _GXContentBuilder.getProperty_:
    def getProperty(String propName) {
        current[propName]
    }
This is wrong. If builder does not know the property it is supposed to throw MissingPropertyException. Returning _null_ indicates that property, in fact, exists, but has null value.
</description><key id="558794">654</key><summary>Groovy client doesn't bind to any variables outside the closure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spn</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-28T12:32:33Z</created><updated>2011-02-01T12:16:59Z</updated><resolved>2011-02-01T12:16:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-28T20:29:42Z" id="719225">I copied the logic from grails JsonBuilder: https://github.com/grails/grails-core/blob/master/src/java/grails/util/JSonBuilder.java. Not a groovy expert, but wanted to get the same "json feeling" as in grails.
</comment><comment author="spn" created="2011-01-29T11:45:04Z" id="720599">for me the following worked fine:
1. Making GXContentBuilder extends GroovyObjectSupport.
2. Changing getProperty() method to:
       def getProperty(String propName) {
           if (current?.containsKey(propName)) {
               return current[propName]
           } else {
               return super.getProperty(propName)
           }
       }
   https://github.com/spn/elasticsearch-grails-plugin/blob/master/src/groovy/org/grails/plugins/elasticsearch/util/GXContentBuilder.groovy
</comment><comment author="kimchy" created="2011-01-30T13:27:59Z" id="722473">Interesting!. I can't really judge the implications of this change because of my lack of groovy knowledge :). Wondering why grails are not doing the same? Maybe it makes sense to ping the grails mailing list?
</comment><comment author="spn" created="2011-01-31T14:07:17Z" id="724795">well, Grails IS doing the same sometimes eg look at https://github.com/grails/grails-core/blob/master/src/java/grails/spring/BeanBuilder.java

(actually, in your case the class is written in Groovy so you don't need to extend GroovyObjectSupport. But it still has to fallback to standard property resolution)
</comment><comment author="kimchy" created="2011-01-31T14:09:22Z" id="724802">What I mean is that it does not do it for JsnoBuilder, wondering what the reason for that is, and thought it might make sense to ping the list?
</comment><comment author="spn" created="2011-02-01T07:47:57Z" id="727752">That's right. JsonBuilder uses Builder API, which is sort of high-level tree-oriented; it does not require you to manage properties and methods (BuilderSupport class is doing it internally)
</comment><comment author="kimchy" created="2011-02-01T09:26:52Z" id="727928">I'm sorry, I sent the wrong link to the JsonBuilder used in grails. This is the one ES is based on: https://github.com/grails/grails-core/blob/master/src/java/grails/web/JSONBuilder.groovy .
</comment><comment author="spn" created="2011-02-01T11:57:23Z" id="728240">it looks to me you have a small difference from the original JSONBuilder:
    private buildRoot(Closure c) {
        c.delegate = this
        //c.resolveStrategy = Closure.DELEGATE_FIRST
        root = [:]
        current = root
        def returnValue = c.call()
        if (!root) {
            return returnValue
        }
        return root
    }

Looking at GXContentBuilder, c.resolveStrategy is _NOT_ commented out. This is a serious difference, because it affects delegation rules (by default, closure delegates to OWNER_FIRST, and owner is "client" code which uses variables).
</comment><comment author="kimchy" created="2011-02-01T12:16:20Z" id="728279">Right, but then it makes a big difference when used in scripts... . Thats why in the builder you can control what it is (static var...). But, I will change it so the default will be OWNER_FIRST.
</comment><comment author="kimchy" created="2011-02-01T12:16:59Z" id="728281">Groovy client doesn&amp;#39;t bind to any variables outside the closure, closed by 0dfa3dc8a23af7f012eeba3498eadb40dbb29966.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Merge: Improve internal (segment) merging by not initiating it on doc operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/653</link><project id="" key="" /><description>Even with the default `concurrent` merge scheduler of lucene, it might block on document operations when there are no merge threads available to perform a merge.

Now, by default, this will no longer happen. The `index.merge.async` (which defaults to `true`) will cause merge operations not to happen on doc ops, but instead happen in the background. The `index.merge.async_interval` controls how often a merge will be checked if it needs to be initiated, defaults to `1s`.
</description><key id="552958">653</key><summary>Index Merge: Improve internal (segment) merging by not initiating it on doc operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-26T19:46:33Z</created><updated>2011-01-26T21:29:56Z</updated><resolved>2011-01-27T04:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-26T20:40:05Z" id="710517">Index Merge: Improve internal (segment) merging by not initiating it on doc operations, closed by 6c21c30f31851d7b8942990e227aae602c663e8e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic Mapping Templates for otherwise static mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/652</link><project id="" key="" /><description>Dynamic templates can be used to provide additional mapping information for mappings that are dynamic. Dynamic templates are currently not executed for static mappings. It would be great to have static mappings with something like static templates that would allow to provide mapping information for certain groups of fields not in the static mapping. Dynamic mappings are not always appropriate here as they would automatically pick up all fields dynamically whereas I would only like to include a subset of the fields (e.g. a group of fields that adhere to a certain naming scheme).

The group discussion for this request can be found [here](http://elasticsearch-users.115913.n3.nabble.com/Dynamic-Templates-amp-Static-Mappings-td2319851.html)
</description><key id="552282">652</key><summary>Dynamic Mapping Templates for otherwise static mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels /><created>2011-01-26T14:46:00Z</created><updated>2014-11-13T13:18:29Z</updated><resolved>2014-11-13T13:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:56:58Z" id="15916605">You can already do this with dynamic mapping - just specify a path to match
</comment><comment author="clintongormley" created="2014-11-13T13:18:29Z" id="62888695">My comment above was incorrect.  While you can turn on/off dynamic mapping at the type level and within any object/nested field, you can't decide not to add a flat field based purely on the name, eg there is no way of saying:
- add and field matching `foo_*`, but no others, or
- don't add fields matching `foo_*`

While this could be supported, it would be quite an expensive way of doing it as we could potentially have to run the dynamic templates several times on every single document (for every field which has not been added and which will not be added).

I'd say a better solution to this is to use dynamic true/false with objects to namespace allowed/disallowed fields.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Decompounder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/651</link><project id="" key="" /><description>This adds the token filter that uses Lucene to do decompounding, as discussed on the mailing list and in the chat room.
</description><key id="552189">651</key><summary>Analysis: Decompounder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scompt</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-26T14:01:27Z</created><updated>2014-06-15T00:41:04Z</updated><resolved>2011-02-08T09:25:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-29T13:44:11Z" id="720713">Heya,

  Looks good. A few suggestions:
- Can we have the word list be either provided as a file path, or within the settings? Then, then tests should use the ability to provide it as settings (and not write a file as part of the test). I can use that code to do the same for stop words.
</comment><comment author="scompt" created="2011-01-31T07:58:40Z" id="724158">I updated my branch. There's now another static method on the Analysis class that can be used to get a list of words. It looks for &lt;setting_name&gt; first and if it can't be found, it looks for &lt;setting_name&gt;_path to find a file with a list of words.
</comment><comment author="kimchy" created="2011-02-08T09:25:28Z" id="749103">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Allow to pass a search filter, applying only on the query (and not on facets for example)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/650</link><project id="" key="" /><description>When doing things like facet navigation, sometimes only the hits are needed to be filtered by the chosen facet, and all the facets should continue to be calculated based on the original query. The new `filter` element within the search request can be used to accomplish it.

For example, lets create two tweets, with two different tags:

```
curl -XPUT 'localhost:9200/twitter/tweet/1' -d '
{
    "message" : "something blue",
    "tag" : "blue"
}
'

curl -XPUT 'localhost:9200/twitter/tweet/2' -d '
{
    "message" : "something green",
    "tag" : "green"
}
'

curl -XPOST 'localhost:9200/_refresh'
```

We can now search for `something`, and have a `terms` facet. 

```
curl -XPOST 'localhost:9200/twitter/_search?pretty=true' -d '
{
    "query" : {
        "term" : { "message" : "something" }
    },
    "facets" : {
        "tag" : {
            "terms" : { "field" : "tag" }
        }
    }
}
'
```

We get two hits, and the relevant facets with a count of `1` for both `green` and `blue`. Now, lets say the `green` facet is chosen, we can simply add a filter for it:

```
curl -XPOST 'localhost:9200/twitter/_search?pretty=true' -d '
{
    "query" : {
        "term" : { "message" : "something" }
    },
    "filter" : {
        "term" : { "tag" : "green" }
    },
    "facets" : {
        "tag" : {
            "terms" : { "field" : "tag" }
        }
    }
}
'
```

And now, we get only 1 hit back, but the facets remain the same.

Note, if additional filters is required on specific facets, they can be added as a `facet_filter` to the relevant facets.
</description><key id="551899">650</key><summary>Search: Allow to pass a search filter, applying only on the query (and not on facets for example)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.15.0</label></labels><created>2011-01-26T10:52:42Z</created><updated>2011-01-26T10:55:56Z</updated><resolved>2011-01-26T10:55:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-26T10:55:56Z" id="708872">Search: Allow to pass a search filter, applying only on the query (and not on facets for example), closed by 5a4686aee519a1b24a0597dca05c2912b1412a61.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Attachment plugin: ES logging setup clashes with Slf4j package inside Tika</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/649</link><project id="" key="" /><description>If tika-app-0.8.jar is installed in lib folder then config/logging.yml is ignored and logging outputs to the console. I tried to remove org/slf4j package from tika jar and it helped.
</description><key id="551816">649</key><summary>Attachment plugin: ES logging setup clashes with Slf4j package inside Tika</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-01-26T10:00:43Z</created><updated>2013-04-04T18:57:12Z</updated><resolved>2013-04-04T18:57:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-07-02T14:48:40Z" id="1489546">This can be closed? I am not seeing this issue with tike 0.9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script field difference between 0.14 and 0.14.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/648</link><project id="" key="" /><description>Here is a simple test to reproduce:
https://gist.github.com/786580

This ticket was opened from this discussion:
http://elasticsearch-users.115913.n3.nabble.com/Interesting-script-field-nuance-on-0-14-2-td2244658.html

Not a high priority and doesn't block anything from my perspective. Just seemed quirky.

Thanks!
</description><key id="548372">648</key><summary>Script field difference between 0.14 and 0.14.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2011-01-24T23:04:01Z</created><updated>2011-02-01T11:48:10Z</updated><resolved>2011-02-01T11:48:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-01T11:48:09Z" id="728222">This seems to be fixed with the latest upgrade to mvel in master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facets: Allow to get `all_terms` back (possibly with count 0)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/647</link><project id="" key="" /><description>Allow to get all the terms in the terms facet, ones that do not match a hit, will have a count of 0. Note, this should not be used with fields that have many facets.

The parameter on the `terms` facet should be `all_terms` and set it to `true`.
</description><key id="545096">647</key><summary>Terms Facets: Allow to get `all_terms` back (possibly with count 0)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-23T14:07:49Z</created><updated>2011-01-23T14:09:44Z</updated><resolved>2011-01-23T14:09:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-23T14:09:44Z" id="700412">Terms Facets: Allow to get `all_terms` back (possibly with count 0), closed by 95b6184135f23a6e1bcd8aef363025b12679f25c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Integration with Hunspell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/646</link><project id="" key="" /><description>Feature added by @uboness:

Added basic support for hunspell stemming. Hunspell dictionaries will be picked up from a dedicated hunspell directory on the filesystem (defaults to _&lt;path.conf&gt;_/hunspell). Each dictionary is expected to have its own directory named after its associated locale (language). This dictionary directory is expected to hold both the *.aff and *.dic files (all of which will automatically be picked up). For example, assuming the default hunspell location is used, the following directory layout will define the _en_US_ dictionary:

```
- conf
    |-- hunspell 
    |    |-- en_US
    |    |    |-- en_US.dic
    |    |    |-- en_US.aff
```

The location of the hunspell directory can be configured using the `indices.analysis.hunspell.dictionary.location` settings in _elasticsearch.yml_.

Each dictionary can be configured with two settings:
- `ignore_case` - If true, dictionary matching will be case insensitive (defaults to `false`)
- `strict_affix_parsing` - Determines whether errors while reading a affix rules file will cause exception or simple be ignored (defaults to `true`)

These settings can be configured globally in `elasticsearch.yml` using `indices.analysis.hunspell.dictionary.ignore_case` and `indices.analysis.hunspell.dictionary.strict_affix_parsing`, or for specific dictionaries: `indices.analysis.hunspell.dictionary.en_US.ignore_case` and `indices.analysis.hunspell.dictionary.en_US.strict_affix_parsing`.

It is also possible to add `settings.yml` file under the dictionary directory which holds these settings (this will override any other settings defined in the `elasticsearch.yml`).

One can use the hunspell stem filter by configuring it the analysis settings:

``` json
{
    "analysis" : {
        "analyzer" : {
            "en" : {
                "tokenizer" : "standard",       
                "filter" : [ "lowercase", "en_US" ]
            }
        },
        "filter" : {
            "en_US" : {
                "type" : "hunspell",
                "locale" : "en_US",
                "dedup" : true
            }
        }
    }
}
```
## Original Request:

Hunspell is a spell checker and morphological analyzer designed for languages with rich morphology and complex word compounding and character encoding.

[1] Wikipedia, http://en.wikipedia.org/wiki/Hunspell

[2] Source code, http://hunspell.sourceforge.net/

[3] Hunspell-Lucene integration, http://code.google.com/p/lucene-hunspell/

[4] presentation by Chris Male, EuroCon 2010, http://lucene-eurocon.org/slides/European-Language-Analysis-with-Hunspell_Chris-Male.pdf (annotation of his talk can be found here: http://lucene-eurocon.org/sessions-track2-day2.html#5)
</description><key id="544219">646</key><summary>Analysis: Integration with Hunspell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>v0.90.0.Beta1</label></labels><created>2011-01-22T20:56:09Z</created><updated>2013-01-03T18:08:25Z</updated><resolved>2013-01-02T03:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-22T23:29:51Z" id="699564">Are you up for building a plugin for that?
</comment><comment author="lukas-vlcek" created="2011-01-23T19:43:47Z" id="700872">I can give it a try, it can be interesting experiment.
</comment><comment author="xawiers" created="2011-11-30T08:30:30Z" id="2956544">How I could try Hunspell with new 3.5 lucene on elasticsearch?
</comment><comment author="xawiers" created="2011-11-30T09:57:27Z" id="2957354">Suggest config from solr manual, could be similar to this:

```
index:
  analysis:
    filter:
      hunspell_en:
        dictionary: en_GB.dic
        affix: en_GB.aff
        ignoreCase: true
      hunspell_lt:
        dictionary: lt_LT.dic
        affix: lt_LT.aff
        ignoreCase: true
```
</comment><comment author="kimchy" created="2011-11-30T15:22:42Z" id="2960973">Sounds good, do you plan to work on it?
</comment><comment author="xawiers" created="2011-11-30T15:51:17Z" id="2961433">I'm good in using, but not developing in java language.
</comment><comment author="jprante" created="2011-12-29T20:32:09Z" id="3304898">I just wrote a Hunspell plugin, have a look at https://github.com/jprante/elasticsearch-analysis-hunspell
</comment><comment author="uboness" created="2013-01-02T03:46:05Z" id="11799641">Added basic support for hunspell stemming. Hunspell dictionaries will be picked up from a dedicated hunspell directory on the filesystem (defaults to _&lt;path.conf&gt;_/hunspell). Each dictionary is expected to have its own directory named after its associated locale (language). This dictionary directory is expected to hold both the *.aff and *.dic files (all of which will automatically be picked up). For example, assuming the default hunspell location is used, the following directory layout will define the _en_US_ dictionary:

```
- conf
    |-- hunspell 
    |    |-- en_US
    |    |    |-- en_US.dic
    |    |    |-- en_US.aff
```

The location of the hunspell directory can be configured using the `indices.analysis.hunspell.dictionary.location` settings in _elasticsearch.yml_.

Each dictionary can be configured with two settings:
- `ignore_case` - If true, dictionary matching will be case insensitive (defaults to `false`)
- `strict_affix_parsing` - Determines whether errors while reading a affix rules file will cause exception or simple be ignored (defaults to `true`)

These settings can be configured globally in `elasticsearch.yml` using `indices.analysis.hunspell.dictionary.ignore_case` and `indices.analysis.hunspell.dictionary.strict_affix_parsing`, or for specific dictionaries: `indices.analysis.hunspell.dictionary.en_US.ignore_case` and `indices.analysis.hunspell.dictionary.en_US.strict_affix_parsing`.

It is also possible to add `settings.yml` file under the dictionary directory which holds these settings (this will override any other settings defined in the `elasticsearch.yml`).

One can use the hunspell stem filter by configuring it the analysis settings:

``` json
{
    "analysis" : {
        "analyzer" : {
            "en" : {
                "tokenizer" : "standard",       
                "filter" : [ "lowercase", "en_US" ]
            }
        },
        "filter" : {
            "en_US" : {
                "type" : "hunspell",
                "locale" : "en_US",
                "dedup" : true
            }
        }
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight: field array concatenated when term_vector set to with_positions_offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/645</link><project id="" key="" /><description>When using highlighting on an array field the output differs depending on `term_vector` being set to `with_positions_offsets` or `no`. When it is set to `with_positions_offsets` then it produces signle string (concatenation of items from array field). When `term_vector` not set then it produces array of strings, each being highlighted. Example follows:

```
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test

curl -XPUT localhost:9200/test/one/_mapping -d '
{
  "one" : {
    "properties" : { 
      "tags" : {
        "type" : "string",
        "analyzer" : "snowball",
        "term_vector" : "with_positions_offsets"
      }
    }
  }
}'
sleep 1

curl -XPOST localhost:9200/test/one -d '{ "tags" : ["one word","too wordish","three words"]}'
curl -XPOST localhost:9200/_refresh

curl -XGET 'localhost:9200/test/one/_search?pretty=1' -d '
{
  "query" : {
    "query_string" : { 
      "query" : "word",
      "default_field" : "tags"
    }
  },
  "highlight" : {
    "fields" : {
      "tags" : {}
    }
  }
}'
```

This outputs:

```
"hits" : {
  "total" : 1,
  "max_score" : 0.16273327,
  "hits" : [ {
    "_index" : "test",
    "_type" : "one",
    "_id" : "bdTzvCCxRAm6WQ8iWaYBoQ",
    "_version" : 1,
    "_score" : 0.16273327, "_source" : { "tags" : ["one word","too wordish","three words"]},
    "highlight" : {
      "tags" : [ "one &lt;em&gt;word&lt;/em&gt; too wordish three &lt;em&gt;words&lt;/em&gt;" ]
    }
  } ]
}
```

It is just single string: `[ "one &lt;em&gt;word&lt;/em&gt; too wordish three &lt;em&gt;words&lt;/em&gt;" ]`

When `"term_vector" : "with_positions_offsets"` is omitted from `_mapping` in the above example then it outputs:

```
"hits" : {
  "total" : 1,
  "max_score" : 0.16273327,
  "hits" : [ {
    "_index" : "test",
    "_type" : "one",
    "_id" : "PnUrQWUsSdalFiraGK1V3A",
    "_version" : 1,
    "_score" : 0.16273327, "_source" : { "tags" : ["one word","too wordish","three words"]},
    "highlight" : {
      "tags" : [ "one &lt;em&gt;word&lt;/em&gt;", "too wordish", "three &lt;em&gt;words&lt;/em&gt;" ]
    }
  } ]
}
```

We get three strings: `[ "one &lt;em&gt;word&lt;/em&gt;", "too wordish", "three &lt;em&gt;words&lt;/em&gt;" ]`. (Moreover, presence of the second string (`too wordish`) in highlight results is questionable, isn't it?)
</description><key id="543464">645</key><summary>Highlight: field array concatenated when term_vector set to with_positions_offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>v0.15.0</label></labels><created>2011-01-22T11:19:01Z</created><updated>2012-05-31T18:27:33Z</updated><resolved>2011-02-06T13:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-29T13:31:52Z" id="720701">The format in which the response will different because of the different highlighting implementation. I just pushed a fix for the fact that "too wordish" was returned.
</comment><comment author="weiweiwang" created="2011-12-01T05:13:18Z" id="2971007">in which version does this problem fixed? I still have this problem
</comment><comment author="TwP" created="2012-05-31T18:27:33Z" id="6043647">@kimchy, this behavior is still present in ElasticSearch v0.19.3

I ran @lukas-vlcek script above and the same erroneous behavior was observed. When `"term_vector" : "with_positions_offsets"` is used in the mapping the highlighted fields are concatenated into a single string. This behavior differs from the case where term vectors are not generated.

I believe one bug was fixed. In the example above the extra tag string `"two wordish"` was included in the highlight results. This bug is fixed in the case where term vectors are **not** being used for highlighting.

However, when term vectors are enabled and the fast-vector-highlighter is used, all the highlight tags are concatenated together into a single string.

```
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.16273327,
    "hits" : [ {
      "_index" : "test",
      "_type" : "one",
      "_id" : "hx79P59ARRaOylM6yfYH2w",
      "_score" : 0.16273327, "_source" : { "tags" : ["one word","too wordish","three words"]},
      "highlight" : {
        "tags" : [ "one &lt;em&gt;word&lt;/em&gt; too wordish three &lt;em&gt;words&lt;/em&gt;" ]
      }
    } ]
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add phonetic token filter (metaphone, soundex, ...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/644</link><project id="" key="" /><description>Add phonetic token filter, for example:

```
index :
  analysis :
    filter :
      metaphone :
        type : phonetic
        encoder : metaphone
    analyzer :
      custom1 :
        tokenizer : standard
        filter : [metaphone]
```

The `encoder` can be configured with `metaphone`, `soundex`, `caverphone`, `refined_soundex`, `double_metaphone` (uses the commons codec: http://jakarta.apache.org/commons/codec/api-release/org/apache/commons/codec/language/package-summary.html).

The `replace` parameter (defaults to `true`) controls if the token processed should be replaced with the encoded one (set it to `true`), or added (set it to `false`).
</description><key id="540543">644</key><summary>Analysis: Add phonetic token filter (metaphone, soundex, ...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-20T23:56:46Z</created><updated>2011-01-20T23:57:54Z</updated><resolved>2011-01-20T23:57:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-20T23:57:54Z" id="694539">Analysis: Add phonetic token filter (metaphone, soundex, ...), closed by 9801ddeb0dec99674250c4f3386f351edc016d5c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strict dynamic setting: Refuse to index a document with fields not present in the mapping definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/643</link><project id="" key="" /><description>Strict dynamic mapping setting will refuse to index documents with fields that are not explicitly mapped. This can be set on any object level mapping, and inherits by default the root object mapping. Here is an example:

```
{
    "my_type" : {
        "dynamic" : "strict"
    }
}
```

The other possible values for dynamic are the typical boolean values, `false` and `true`, which retain their previous behavior.
</description><key id="540071">643</key><summary>Strict dynamic setting: Refuse to index a document with fields not present in the mapping definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MBaechle</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-20T20:57:58Z</created><updated>2011-02-08T09:38:22Z</updated><resolved>2011-01-22T07:09:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-21T23:09:03Z" id="697635">Strict dynamic setting: Refuse to index a document with fields not present in the mapping definition, closed by ce4f09c2b1d27bb40d384bb620fe0a035a812b31.
</comment><comment author="clintongormley" created="2011-02-06T13:19:22Z" id="743451">This looks broken to me:

```
# [Sun Feb  6 14:17:40 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/bar/'  -d '
{}
'

# [Sun Feb  6 14:17:40 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sun Feb  6 14:17:43 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/bar/bar/_mapping'  -d '
{
   "bar" : {
      "dynamic" : "strict",
      "properties" : {
         "text" : {
            "type" : "string"
         }
      }
   }
}
'

# [Sun Feb  6 14:17:44 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sun Feb  6 14:17:47 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/bar/bar/_mapping' 

# [Sun Feb  6 14:17:47 2011] Response:
# {
#    "bar" : {
#       "bar" : {
#          "dynamic" : "STRICT",
#          "properties" : {
#             "text" : {
#                "type" : "string"
#             }
#          }
#       }
#    }
# }

# [Sun Feb  6 14:17:57 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/bar/bar/1'  -d '
{
   "num" : 5,
   "text" : "foo"
}
'

# [Sun Feb  6 14:17:57 2011] Response:
# {
#    "ok" : true,
#    "_index" : "bar",
#    "_id" : "1",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Sun Feb  6 14:18:08 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/bar/bar/_mapping' 

# [Sun Feb  6 14:18:08 2011] Response:
# {
#    "bar" : {
#       "bar" : {
#          "properties" : {
#             "num" : {
#                "type" : "long"
#             },
#             "text" : {
#                "type" : "string"
#             }
#          }
#       }
#    }
# }
```
</comment><comment author="kimchy" created="2011-02-08T09:38:22Z" id="749132">Right, just pushed a fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add stemming to czech analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/642</link><project id="" key="" /><description>With backport of czech stemmer, use it by default with an updated czech analyzer (until lucene 3.1 is released).
</description><key id="538870">642</key><summary>Analysis: Add stemming to czech analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-20T11:30:19Z</created><updated>2011-01-20T11:31:06Z</updated><resolved>2011-01-20T11:31:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-20T11:31:06Z" id="692600">Analysis: Add stemming to czech analyzer, closed by 034a66263a345c29d1efe1a7fb4c25e2e0f2fb4d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES hangs connection when parent is not specified in bulk index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/641</link><project id="" key="" /><description>Instead of returning an error the connection hangs when a parent is not specified (but required by the mapping) when doing a bulk index.  A Gist with an example can be found at:
https://gist.github.com/787668
</description><key id="538731">641</key><summary>ES hangs connection when parent is not specified in bulk index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-20T10:04:04Z</created><updated>2011-01-20T10:29:13Z</updated><resolved>2011-01-20T10:29:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-20T10:29:13Z" id="692509">ES hangs connection when parent is not specified in bulk index, closed by 04f8b556863353edc38cdd3539dd5ad2d13ba39b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term Count API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/640</link><project id="" key="" /><description>I couldn't find this in the docs, or in the issue tracker, and following on from a discussion I had googled here:

http://elasticsearch-users.115913.n3.nabble.com/Terms-API-for-Spellchecker-td1691838.html

It appears others would like a Term Count API as well. (it apparently used to be in ES, if I read that correctly).

I understand with sharding that it's not as simple as it may be, because with a pathological case of 1 shard having a lot of terms, but another not, it's not easy to get an accurate term count without getting each distinct list from the shards and doing a distinct on them.

A simpler method may just be to expose a result from each shard, something like:

```
{
    "shards": {
        "shard1": 5,
        "shard2": 12,
        "shard37":450
    }
    "range": {
        "min": 450,
        "max": 467
    }
}
```

This is produced from knowing that the absolute minimum number of distinct terms has to be the maximum from an individual shard (when shard37 holds all the unique terms, and the other shards just hold a subset).  The absolute maximum number of distinct terms can only be the sum of the shard counts (in the pathological case where each shard is storing terms no other shard has).

This would have to be very fast to compute, and still useful, but may not satisfy all cases.  The only alternative is to get a unique term stream from each shard and merge them into a distinct list and count .  For very large numbers of terms that could prove a memory hog.

If I knew were to start, I'd have a crack at this, pointers in the direction and I can start to attempt it.
</description><key id="538355">640</key><summary>Term Count API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tallpsmith</reporter><labels /><created>2011-01-20T04:23:33Z</created><updated>2014-03-13T18:32:41Z</updated><resolved>2014-03-13T18:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-13T18:32:41Z" id="37570042">#5426 has just been resolved and allows to compute unique counts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>backporting Czech Stemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/639</link><project id="" key="" /><description /><key id="538303">639</key><summary>backporting Czech Stemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-20T03:16:55Z</created><updated>2014-07-12T17:16:48Z</updated><resolved>2011-02-15T16:10:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove -XX:+AggressiveOpts flag that is turned on by default in startup script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/638</link><project id="" key="" /><description>Seems like it can cause problems with Java 1.6 update 23 on certain operation systems. Should also remove it from the wrapper scripts.
</description><key id="537829">638</key><summary>Remove -XX:+AggressiveOpts flag that is turned on by default in startup script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.15.0</label></labels><created>2011-01-19T22:12:04Z</created><updated>2011-01-19T22:13:28Z</updated><resolved>2011-01-19T22:13:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-19T22:13:28Z" id="691257">Remove -XX:+AggressiveOpts flag that is turned on by default in startup script, closed by e953845058a652d9f8ca1c36bf72bf593e53b80c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No cluster name filtering when sniffing is not enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/637</link><project id="" key="" /><description>The cluster name is no longer honored via the TransportClient unless sniffing is enable don 0.14.2. See this discussion for more details:
http://elasticsearch-users.115913.n3.nabble.com/0-14-2-TransportClient-does-not-appear-to-be-honoring-cluster-name-tp2289073p2289073.html

Thanks!
</description><key id="537500">637</key><summary>No cluster name filtering when sniffing is not enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-19T20:19:00Z</created><updated>2011-01-20T15:28:58Z</updated><resolved>2011-01-20T15:28:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-20T15:28:58Z" id="693067">No cluster name filtering when sniffing is not enabled, closed by 545d8f35df8935ceb21497c9dca904ee52c64543.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolate on Index and Bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/636</link><project id="" key="" /><description>The index and bulk operation (for index bulk items) can now be automatically percolated and return the queries they matched on. The `percolate` parameter can be passed to control which percolation queries should be used (it follows the query string format). For example:

Create an index we will work on:

```
curl -XPUT localhost:9200/test
```

Next, we will register a percolator query with a specific name called `kuku` against the `test` index:

```
curl -XPUT localhost:9200/_percolator/test/kuku?refresh=true -d '{
    "color" : "blue",
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
```

And now, we will index a doc, that has `field1` set to `value1`, and have it percolate:

```
curl -XPUT localhost:9200/test/type1/1?percolate=* -d '{
    "field1" : "value1"
}'
```

We will get this response:

```
{"ok":true,"_index":"test","_type":"type1","_id":"1","_version":2,"matches":["kuku"]}
```

Percolation query filtering is simple to do, lets register another query, for `bubu`, but now with `color` set to `green` and matching on `field2`:

```
curl -XPUT localhost:9200/_percolator/test/bubu?refresh=true -d '{
    "color" : "green",
    "query" : {
        "term" : {
            "field2" : "value2"
        }
    }
}'
```

Now, we can control which queries can be executed, for example:

```
curl -XPUT localhost:9200/test/type1/1?percolate=* -d '{
    "field1" : "value1",
    "field2" : "value2"
}'

curl -XPUT localhost:9200/test/type1/1?percolate=color:green -d '{
    "field1" : "value1",
    "field2" : "value2"
}'

curl -XPUT localhost:9200/test/type1/1?percolate=color:blue -d '{
    "field1" : "value1",
    "field2" : "value2"
}'
```
## Implementation Details

Percolation on index operation is done while optimizing the distributed nature of elasticsearch. Once the index operation is done on the primary shard, it is sent to all the replicas, and while the operation is done on the replicas, the percolation is executed on the node hosting the primary shard. Also, the parsing operation done on the primary shard is reused for the percolation operation.
</description><key id="536763">636</key><summary>Percolate on Index and Bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.15.0</label></labels><created>2011-01-19T14:45:08Z</created><updated>2011-01-19T14:46:14Z</updated><resolved>2011-01-19T14:46:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-19T14:46:14Z" id="689843">Percolate on Index and Bulk, closed by b1d13febbf05360f4d2db1efb0e77609650bfc83.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport CzechStemmer from Lucene 3.1-dev</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/635</link><project id="" key="" /><description>Make [it](http://svn.apache.org/viewvc/lucene/dev/trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/cz/) available for Czech language analysis.
</description><key id="536499">635</key><summary>Backport CzechStemmer from Lucene 3.1-dev</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-01-19T12:11:59Z</created><updated>2011-01-20T11:31:06Z</updated><resolved>2011-01-20T11:31:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-01-20T11:31:06Z" id="692599">added Czech Stemmer, closed by e0fa15a365bff26c4d5b8e950646e78b2f90b52c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClusterStateListener should expose ClusterHealth change events too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/634</link><project id="" key="" /><description>For a range of reasons it would be good for the ClusterStateEventListener callback to be notified of the change in state of the health, or at least expose what the current health is in the state object so that listeners can use coarse grained logic just on that.

I'd like to be able to expose the Health change to an external facility by this way.
</description><key id="535808">634</key><summary>ClusterStateListener should expose ClusterHealth change events too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tallpsmith</reporter><labels><label>feedback_needed</label></labels><created>2011-01-19T02:38:53Z</created><updated>2014-07-19T11:49:39Z</updated><resolved>2014-07-19T11:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T07:59:21Z" id="49405107">Hi @tallpsmith 

Do you still have a use case for this that wouldn't be handled by polling?
</comment><comment author="tallpsmith" created="2014-07-18T22:37:03Z" id="49487140">wow, this one's an old one... :)

I think thinking back on when this issue was created, I think now with all the internal state/perf APIs that are there now for monitoring the need for this is extremely low.

On the other hand, it does seem weird that the ClusterState change events can go out to Listeners and one of the obvious questions any custom listener would have when notified about this change is what affect on the health that is.  If there is any custom code (and right now I don't need any personally) if the event had the new calculated health value as an attribute that would be nice, to save the lookup.

But if no-one else is begging for it, I'm happy to close this.

cheers,

Paul
</comment><comment author="clintongormley" created="2014-07-19T11:49:39Z" id="49507317">Done! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Starting a node with existing data and killing it before its finished it cluster joining / initialization can cause data loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/633</link><project id="" key="" /><description>There is a race condition in the node initialization where if starting and shutting it down before it properly joined the cluster can cause it to discard some of its listed existing local shards. 

There is a way to hack them back into the cluster (ping for details).
</description><key id="535496">633</key><summary>Starting a node with existing data and killing it before its finished it cluster joining / initialization can cause data loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-18T23:16:40Z</created><updated>2011-01-18T23:17:01Z</updated><resolved>2011-01-18T23:17:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-18T23:17:00Z" id="688344">This has been fixed with the several commits done today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facet: Add option include counts where term is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/632</link><project id="" key="" /><description>Say I have two documents like so:

  { "field1": "hello", "field2": "world" }
  { "field1": "moo" }

And I run a terms facet on field2.

It would give me count 1 for "world" but I would also want a count of 1 for null.

(Note: all fields are set to non_analysed in the dynamic_mapping.json)
</description><key id="534716">632</key><summary>Terms Facet: Add option include counts where term is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nmosafi</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-18T18:14:25Z</created><updated>2011-01-18T19:51:42Z</updated><resolved>2011-01-18T19:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-18T19:51:42Z" id="687697">Terms Facet: Add option include counts where term is missing, closed by 574c4552038e53ac292274efdfebb6320ebba764.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort Facets based on document score (or arbitrary script)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/631</link><project id="" key="" /><description>Would it be possible to sort facets against the sumOf(score_i) or even sumOf(score_i*doc_i['someField']) ?

This would be really handy if you have several facets and want only show the most relevant ones. (e.g. trending keywords/urls at jetwick.com)

See also an open issue for Solr:

https://issues.apache.org/jira/browse/SOLR-385
</description><key id="529294">631</key><summary>Sort Facets based on document score (or arbitrary script)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-01-15T23:21:32Z</created><updated>2011-02-22T20:36:46Z</updated><resolved>2011-02-22T20:36:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-16T10:23:58Z" id="681256">Great idea. Requires some work on providing it within the facets framework, and then exposing it in an optimized manner for the terms facet (which gets more interesting in master because of the optimized facets for numeric values). I am going to work on facets a bit more in a few days, ping me on IRC to see that its handled as well... ;)
</comment><comment author="karussell" created="2011-01-16T19:37:48Z" id="681993">ok, I'll ping you :) (I for myself will try to get the rest of the app working with ES ...)

One question:

How would you handle this to perform optimal? I mean, someday I looked into the solr code and I saw that for the normal counts it was just a bitSet.cardinality but for this (nice) feature one would have to iterate over the bitset and IMHO this would be bad for performance. WDYT?

BTW: there was a really cool idea of "tree faceting" from Yonik:

https://issues.apache.org/jira/browse/SOLR-153

which can improve regular facets a lot (not only hierarchival ones!)
</comment><comment author="kimchy" created="2011-01-16T20:02:13Z" id="682035">If thats how Solr does terms facet, then ES does it differently. Regarding the tree faceting, I have been thinking about doing something like pivot based facets, but it requires some thinking to do it in an optimized / distributed manner.
</comment><comment author="karussell" created="2011-02-05T00:23:53Z" id="740824">One more info: it would be necessary to include the 'old' facet count too, so the sum(score) should be an additional info of the (term) facet.

BTW: is it possible to restrict facet counts e.g. to the first 5000 documents only (the .size() method is used for number of facets not of involved docs). 

Maybe that is easier to implement or faster. Then in practise it should have a similar effect of reducing the irrelevant facets.
</comment><comment author="karussell" created="2011-02-22T20:36:46Z" id="798730">closed by #705 IMHO
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added some default values to the config for reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/630</link><project id="" key="" /><description>In packaging up elasticsearch as an RPM for Fedora and centOS it throws a warning about shipping an empty config file. 
The idea is that all config files should at least exist and have some sort of reference to what is in them. Therefore I created this stubbed out config file with default values to make it clear what can be done in the config.  
</description><key id="528016">630</key><summary>Added some default values to the config for reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tavisto</reporter><labels /><created>2011-01-14T22:54:25Z</created><updated>2014-07-16T21:56:55Z</updated><resolved>2011-02-20T00:20:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-19T10:05:00Z" id="689320">Let me think about the best sample parameters that can be set..., and will push it.
</comment><comment author="kimchy" created="2011-02-20T00:20:46Z" id="789543">already added a different set of samples.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes Stats: return no nodes info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/629</link><project id="" key="" /><description>The below script can be used to recreate the issue. First call to `nodes/stats` returns correct data but once the `_search` query is executed then second call to `nodes/stats` return no data for nodes.

```
curl -XPOST 'localhost:9200/x/y?refresh=true&amp;pretty=1' -d '{"x":"y"}'

curl -XGET 'localhost:9200/_cluster/nodes/stats?pretty=1'

curl -XGET 'localhost:9200/x/_search?pretty=1' -d '
{
    "query" : { "match_all" : {}},
    "size" : 0,
    "facets" : {
        "x" : {
            "terms" : {
                "field" : "x",
                "size" : 1
            }
        }
    }
}'

curl -XGET 'localhost:9200/_cluster/nodes/stats?pretty=1'
```

This one was really hard to isolate into such a tiny script.
</description><key id="527868">629</key><summary>Nodes Stats: return no nodes info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-14T21:41:09Z</created><updated>2011-01-29T13:34:14Z</updated><resolved>2011-01-29T13:34:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-01-14T21:46:23Z" id="678887">btw I am almost sure it has nothing to do with facets query, it is something more general as it was happening on node disconnection too
</comment><comment author="kimchy" created="2011-01-15T00:22:01Z" id="679221">Do you recreate it each time? I ran it on a 3 node cluster and the last stats returns fine (running on master, which I believe you are as well?).
</comment><comment author="lukas-vlcek" created="2011-01-15T07:49:08Z" id="679571">It works on single node cluster for me and it recreates it consistently.
I tried on 2 and 3 nodes cluster but it does not recreate it. Please use single node only.
</comment><comment author="kimchy" created="2011-01-29T13:34:14Z" id="720702">Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk Get</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/628</link><project id="" key="" /><description>It would be nice to get several documents at once.

BTW: also that issue would be handy: https://github.com/elasticsearch/elasticsearch/issues/#issue/204
</description><key id="527558">628</key><summary>Bulk Get</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-01-14T19:11:11Z</created><updated>2011-01-21T10:24:41Z</updated><resolved>2011-01-21T10:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-01-21T10:24:41Z" id="695503">This can be done with the special field _id and having lots of them in the query. instead of:

client.prepareGet(indexName, indexType, twitterIdString).actionGet();

I'll close this here ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Twitter updates have broken Twitter River, Update to latest version of Twitter4J</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/627</link><project id="" key="" /><description>Currently the Twitter River is broken, crashing whenever an integer over 2^32-1 is returned.

It seems that this has been fixed in version 2.1.10 of twitter4j, so a simple update of the bundled .jar should be all that's needed.

http://twitter4j.org/jira/browse/TFJ-505
</description><key id="527369">627</key><summary>Twitter updates have broken Twitter River, Update to latest version of Twitter4J</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pib</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-14T17:21:27Z</created><updated>2011-01-14T18:56:10Z</updated><resolved>2011-01-15T02:48:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-14T18:46:42Z" id="678429">I will upgrade and also add some more information now presented in the status data.
</comment><comment author="kimchy" created="2011-01-14T18:48:34Z" id="678433">Twitter updates have broken Twitter River, Update to latest version of Twitter4J, closed by 64e57846234fbd5997cbb2fe66c4d967e978a87d.
</comment><comment author="pib" created="2011-01-14T18:56:10Z" id="678456">Awesome, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Process / Network / Os should only refresh every 5 seconds (refresh_interval to set it)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/626</link><project id="" key="" /><description>Currently, on each node stats call, the process/network/os stats are executed. They should only be refreshed every 5 seconds (by default). Changing the value is done by setting `monitor.os.refresh_interval`, `monitor.process.refresh_interval`, `monitor.network.refresh_interval`
</description><key id="527022">626</key><summary>Node Stats: Process / Network / Os should only refresh every 5 seconds (refresh_interval to set it)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-14T14:00:25Z</created><updated>2011-01-16T10:34:20Z</updated><resolved>2011-01-14T22:01:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-14T14:01:11Z" id="677676">Node Stats: Process / Network / Os should only refresh every 5 seconds (refresh_interval to set it), closed by a8be99b7e07d0b0a00477b1f6581fc83d24d0bba.
</comment><comment author="lukas-vlcek" created="2011-01-14T14:17:41Z" id="677720">Would it be possible to allow (force) refresh per API call as well? This part of the API falls under "admin" section so it should allow also operation that can be risky.

For example something like the following:

```
/nodes/stats?cache=refresh
```
</comment><comment author="kimchy" created="2011-01-14T19:10:35Z" id="678500">Need to think about it, but prefer not to expose that.. . Even if it falls under the admin notion, it needs to be protected.
</comment><comment author="lukas-vlcek" created="2011-01-15T14:03:19Z" id="679953">Would it be at least possible to return the refresh interval value to the client in response? So that the client can learn how frequently it makes sense to pull new data.
</comment><comment author="kimchy" created="2011-01-16T10:34:20Z" id="681264">Sure, added `refresh_interval` to respective `info` request (on `os`, `process` and `network`).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `prefer_local` flag to analyze and percolate request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/625</link><project id="" key="" /><description>A flag called `prefer_local` to control if, when hitting a node where a relevant local shard is allocated on it, should it be preferred for execution or not (if not, go through the random round robin option). Defaults to `true`, which means locals are preferred.
</description><key id="525783">625</key><summary>Add `prefer_local` flag to analyze and percolate request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-13T22:51:20Z</created><updated>2011-01-13T23:02:10Z</updated><resolved>2011-01-13T23:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-13T23:02:10Z" id="676254">Add `prefer_local` flag to analyze and percolate request, closed by 38d10d19bc75f59f23cffc37f0a6bfa8c4de5812.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/624</link><project id="" key="" /><description>The percolator allows to register queries against an index, and then send `percolate` requests which include a doc, and getting back the queries that match on that doc out of the set of registered queries.

Think of it as the revert operation of what ES does by nature. Instead of sending docs, indexing them, and then running queries. One sends queries, registers them, and then sends docs and finds out which queries match that doc.

As an example, a user can register an interest (a query) on all tweets that contain the word "elasticsearch". For every tweet, one can percolate the tweet against all registered user queries, and find out which ones matched.
## Quick Sample

First, create an index we will work with:

```
curl -XPUT localhost:9200/test
```

Next, we will register a percolator query with a specific name called `kuku` against the `test` index:

```
curl -XPUT localhost:9200/_percolator/test/kuku -d '{
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
```

And now, we can percolate a document and see which queries match on it (note, its not really indexed!):

```
curl -XGET localhost:9200/test/type1/_percolate -d '{
    "doc" : {
        "field1" : "value1"
    }
}'
```

And the matches are part of the response:

```
{"ok":true,"matches":["kuku"]}
```
## Filtering Executed Queries

Since the registered percolator queries are just docs in an index, one can filter the queries that will be used to percolate a doc. For example, we can add a `color` field to the registered query:

```
curl -XPUT localhost:9200/_percolator/test/kuku -d '{
    "color" : "blue"
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
```

And then, we can percolate a doc that only matches on blue colors:

```
curl -XGET localhost:9200/test/type1/_percolate -d '{
    "doc" : {
        "field1" : "value1"
    },
    "query" : {
        "term" : {
            "color" : "blue"
        }
    }
}'
```
## How to Works

The `_percolator` which holds the repository of registered queries is just a another index in ES. The query is registered under a concrete index that exists (or will exist) in ES. That index name is represented as the type in the `_percolator` index (a bit confusing, I know...).

The fact that the queries are stored as docs in another index (`_percolator`) gives us both the persistency nature of it, and the ability to filter out queries to execute using another query :).

The `_percolator` index uses the `index.auto_expand_replica` setting to make sure that each data node will have access _locally_ to the registered queries, allowing for fast query executing to filter out queries to run against a percolated doc.

The `percolate` API uses the whole number of shards as percolating processing "engines", both primaries and replicas. In our above case, if the `test` index has 2 shards with 1 replica, 4 shards will round robing in handing percolate requests. (dynamically) increasing the number of replicas will increase the number of percolation power.

Note, percolate request will prefer to be executed locally, and will not try and round robin across shards if a shard exists locally on a node that received a request (for example, from HTTP). Its important to do some roundrobin in the client code among ES nodes (in any case its recommended).
</description><key id="524625">624</key><summary>Percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.15.0</label></labels><created>2011-01-13T14:19:50Z</created><updated>2011-02-08T20:18:48Z</updated><resolved>2011-01-13T22:20:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-13T14:20:52Z" id="674805">Percolator, closed by 180d225016d70c0598aecb892330dd3744462aa7.
</comment><comment author="medcl" created="2011-01-14T05:48:14Z" id="676863">wow,great feature~
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A setting to auto expand the number of replicas of an index (based on data nodes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/623</link><project id="" key="" /><description>Allow to set `index.auto_expand_replicas` to have the cluster automatically expand the number of an replicas an index has based on the data nodes there are in the cluster.

For example, setting: `index.auto_expand_replicas` to `0-all` will have the number of replicas always expand to the full number of data nodes in the cluster.

Setting it to `1-all` will do the same as above, just with a lower bounding value of keeping 1 replica settings always, even if there is just one node.

Setting it to `2-4` will bound the number of replicas between 2 and 4.
</description><key id="522057">623</key><summary>A setting to auto expand the number of replicas of an index (based on data nodes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.15.0</label></labels><created>2011-01-12T14:26:54Z</created><updated>2011-03-21T14:16:34Z</updated><resolved>2011-01-12T14:27:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-12T14:27:48Z" id="671905">A setting to auto expand the number of replicas of an index (based on data nodes), closed by 85b6a982d4464e40907db6dfab5a6ec15d91d1ba.
</comment><comment author="clintongormley" created="2011-03-17T14:38:00Z" id="885254">Any chance that we can set `auto_expand_replicas` using the `update_settings` API?
</comment><comment author="kimchy" created="2011-03-21T14:16:34Z" id="898457">Sure, I will create a different issue for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count API </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/622</link><project id="" key="" /><description>Count API not working for child query...
1. How to get document count in an index using child query.
   
     I am using the query to get count is......
   
     curl -XGET 'http://localhost:9200/dbtest/parent/_count' -d
   '{"has_child" : {"type" : "child", "query" :{ "term" :
   {       "myfield": "test" } }} }'
   
    But i am getting like this...........

{"count":0,"_shards":{"total":2,"successful":0,"failed":2,"failures":
[{"index":"dbtest","shard":
0,"reason":"BroadcastShardOperationFailedException[[dbtest]0] ];
nested:"},{"index":"dbtest","shard":
1,"reason":"BroadcastShardOperationFailedException[[dbtest]1] ];
nested: "}]}} 
</description><key id="521224">622</key><summary>Count API </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">senthilsebi</reporter><labels /><created>2011-01-12T04:01:31Z</created><updated>2013-04-04T18:59:42Z</updated><resolved>2013-04-04T18:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:59:42Z" id="15916775">No further information after 2 years - assuming this has been fixed? If not please reopen with a full recreation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shared Gateway: Allow to set the number of concurrent streams doing snapshot operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/621</link><project id="" key="" /><description>Allow to control the number of concurrent streams (globally, per node) that will be allowed to do the snapshot operations. The setting for `s3` is `gateway.s3.concurrent_streams`, for `fs` is `gateway.fs.concurrent_streams`, and for `hdfs` is `gateway.s3.concurrent_streams`.

All default to `5` concurrent streams.
</description><key id="520381">621</key><summary>Shared Gateway: Allow to set the number of concurrent streams doing snapshot operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-11T19:57:10Z</created><updated>2011-01-11T20:04:53Z</updated><resolved>2011-01-11T20:04:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-11T20:04:53Z" id="670048">Shared Gateway: Allow to set the number of concurrent streams doing snapshot operations, closed by a0a714e6a5e806638372ed2e963c09028c8ab0df.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Add `byte` type (8bit signed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/620</link><project id="" key="" /><description>Add the `byte` numeric type for values  of -128 and a maximum value of 127 (inclusive).
</description><key id="519619">620</key><summary>Mapper: Add `byte` type (8bit signed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-11T13:49:26Z</created><updated>2011-01-11T14:01:25Z</updated><resolved>2011-01-11T14:01:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-11T14:01:25Z" id="669087">Mapper: Add `byte` type (8bit signed), closed by ba9a12e201f6acf383c2f65fd83616710d545176.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic mapping doesn't work after using index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/619</link><project id="" key="" /><description>here is the recreation&#65306;

echo "creating template"
curl -XPUT  http://localhost:9200/_template/temp12 -d'{"mappings":{"mytype":{"properties":{"identity":{"precision_step":4,"type":"float","boost":1.0,"include_in_all":true,"index":"analyzed","store":"no"},"datetime":{"precision_step":4,"type":"date","boost":1.0,"include_in_all":true,"index":"analyzed","store":"no"}}}},"template":"tte*","order":0,"settings":{"number_of_shards":3,"number_of_replicas":2}}'

curl -XPUT http://localhost:9200/tte124 -d'{"settings":{"number_of_shards":3,"number_of_replicas":2}}'

echo " //that works"

curl -XPUT http://localhost:9200/tte123/type/123 -d'{"a":543}'
echo "//time_out with dynamic type and data &#65292;data with 123 will missing"
</description><key id="519587">619</key><summary>dynamic mapping doesn't work after using index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2011-01-11T13:29:23Z</created><updated>2011-01-12T13:39:07Z</updated><resolved>2011-01-12T13:39:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2011-01-12T13:39:07Z" id="671796">specified index  with 2 replicas,but started only 1 node,by default, an index request will be executed only if there is a quorum of shards active (in this case, 2). since we have only 1 node, we have 1 shard and no replicas allocated.and there will be failure.
issue closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Long system-wide freezes, non-gc related</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/618</link><project id="" key="" /><description>Using Amazon EC2 17GB ram 2 core machines, ES heap size 12GB / index buffer 2GB, mlockall=true (and ulimit -l = unlimited), file handles 65k

Machines will occasionally, and apropos of nothing, seize up for as long as 3 or 4 minutes.  This isn't a GC storm -- we see sequential-numbered ParNew sweeps bracketing the pauses, and they don't seem to correlate with high-memory conditions. A background process like 
    ifstat 30 &amp;
will continue to give output (so there's /some/ network), but the shell is fully unresponsive and no new commands will launch. The logs show no behavior whatsoever: you'll see a line at say 1:41:02 and then the next one will be at 1:45:03.

This only happens while ES is under load. The whole cluster will block when one of these pauses happens.  I can't reproduce it at will, but it happens often enough to be a real problem.   We're beating on ES pretty hard -- under load typically means 25 writers trying to saturate the disk I/O of 16 data nodes, and with merge_factor at 30

My only ideas are:
- an interaction with the VM
- the system is being starved of some resource that's not making it into the logs.
</description><key id="516219">618</key><summary>Long system-wide freezes, non-gc related</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrflip</reporter><labels /><created>2011-01-09T22:13:55Z</created><updated>2011-01-27T05:45:19Z</updated><resolved>2011-01-27T05:45:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-10T13:15:19Z" id="666187">I worked with another user that was experiencing this and it was due to a bug in the JVM. Upgrading to sun JVM update 23 seems to solve the problem, can you give it a go?
</comment><comment author="mrflip" created="2011-01-27T05:45:19Z" id="713902">Since using maverick (10.10) the issue has gone away.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If the cluster does not contain any indices then index templates do not survive cluster restart.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/617</link><project id="" key="" /><description>Test is simple:
1. start fresh empty cluster
2. create new index template (_template)
3. restart cluster
4. index template is not present

Test that index template survive if there is an index
1. start fresh empty cluster
2. create new index
3. create new index template (_template)
4. restart cluster
5. index template is present
</description><key id="515576">617</key><summary>If the cluster does not contain any indices then index templates do not survive cluster restart.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-09T12:38:51Z</created><updated>2011-01-11T21:39:00Z</updated><resolved>2011-01-11T21:39:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-01-09T12:54:44Z" id="664171">Also if `filter_indices` is applied in _cluster/state/ REST API then index templates are filtered out.
</comment><comment author="lukas-vlcek" created="2011-01-09T12:57:14Z" id="664173">Oops! I closed this ticket and there does not seem to be an easy way to reopen it again... (agh, I don't think I like the GitHub ticket web interface as all.)
</comment><comment author="kimchy" created="2011-01-11T21:39:00Z" id="670339">If the cluster does not contain any indices then index templates do not survive cluster restart, closed by f5a9f2d94817f83156f46c67020f880c462824ed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery is too aggressive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/616</link><project id="" key="" /><description>When a cluster is under heavy load, the recovery mechanism can drive it into failure.

If a node becomes so overloaded it falls off the network, the other nodes begin immediately replicating its data.  This increases _their_ load, increasing the chance some other node fails.

Furthermore, when nodes wake up they make foolish choices about whether to release their shards.  We frequently see a node with say 18 shards flush most of them and start recovering a different set of shards back to itself, even though none of the shards in question have received writes. This imposes a major additional cluster load.

Finally, if the master node fails the cluster seems to become confused about which shards are primary, leading to data loss.

We're using the s3 gateway but have seen similar problems when using local

Proposed:
- A setting to delay recovery after failure detection. This will damp out the oscillation caused by an overloaded node falling down.
- Improve the shard distribution algorithm so that nodes don't see as much churn when restarted
- Don't be as aggressive about deleting data: retain shards unless disk space demands or they become out of date
</description><key id="515336">616</key><summary>Recovery is too aggressive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrflip</reporter><labels /><created>2011-01-09T05:49:45Z</created><updated>2011-09-23T14:27:29Z</updated><resolved>2011-09-23T14:27:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrflip" created="2011-01-09T21:40:50Z" id="664947">We had to do a full cluster restart after the last failed load; it took about 14 hours to reload ~150GB/machine (16 shards, 9 indices). The load would have been dramatically faster if the nodes pulled from the s3 gateway rather than doing local mode recovery

As a workaround we're '_close'ing all indexes for bulk load; this seems to improve matters significantly.
</comment><comment author="kimchy" created="2011-09-23T14:27:29Z" id="2179086">This has been implemented in both allowing to control concurrent recoveries per node, concurrent relocation cluster wide, and, allow to set the bytes per second recovery will use.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_open is not recovering indices correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/615</link><project id="" key="" /><description>I create an index, add a doc, close it, reopen it, wait for recovery, but the document is gone:

```
# [Sat Jan  8 21:31:55 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/'  -d '
{}
'

# [Sat Jan  8 21:31:55 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Jan  8 21:32:15 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/bar/1'  -d '
{
   "text" : "test"
}
'

# [Sat Jan  8 21:32:15 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "1",
#    "_type" : "bar"
# }

# [Sat Jan  8 21:32:31 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/foo/_count'  -d '
{
   "match_all" : {}
}
'

# [Sat Jan  8 21:32:31 2011] Response:
# {
#    "count" : 1,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }

# [Sat Jan  8 21:32:43 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPOST 'http://127.0.0.1:9200/foo/_close' 

# [Sat Jan  8 21:32:43 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Jan  8 21:32:51 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPOST 'http://127.0.0.1:9200/foo/_open' 

# [Sat Jan  8 21:32:51 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Jan  8 21:33:20 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/_cluster/health?wait_for_status=green' 

# [Sat Jan  8 21:33:20 2011] Response:
# {
#    "number_of_data_nodes" : 3,
#    "relocating_shards" : 0,
#    "active_shards" : 30,
#    "status" : "green",
#    "cluster_name" : "es_test",
#    "active_primary_shards" : 15,
#    "timed_out" : false,
#    "initializing_shards" : 0,
#    "number_of_nodes" : 3,
#    "unassigned_shards" : 0
# }

# [Sat Jan  8 21:33:25 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/foo/_count'  -d '
{
   "match_all" : {}
}
'

# [Sat Jan  8 21:33:25 2011] Response:
# {
#    "count" : 0,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }
```
</description><key id="514792">615</key><summary>/_open is not recovering indices correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-08T20:34:42Z</created><updated>2011-01-09T21:31:22Z</updated><resolved>2011-01-09T21:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-09T21:31:22Z" id="664933">/_open is not recovering indices correctly, closed by d4246da7b3fa9222d2176a2715836d323090f9b3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>refresh param to bulk not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/614</link><project id="" key="" /><description>Passing `refresh=true` to the `bulk` command does not seem to have any effect.

```
# [Sat Jan  8 20:01:30 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/'  -d '
{}
'

# [Sat Jan  8 20:01:30 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Jan  8 20:01:34 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPOST 'http://127.0.0.1:9200/_bulk?refresh=true'  -d '
{"index" : {"_index" : "foo", "_id" : 1, "_type" : "bar"}}
{"text" : "foo"}
{"index" : {"_index" : "foo", "_id" : 2, "_type" : "bar"}}
{"text" : "bar"}
'

# [Sat Jan  8 20:01:34 2011] Response:
# {"items" : [{"index" : {"ok" : true, "_index" : "foo", "_id" : "1
# &gt; ", "_type" : "bar"}}, {"index" : {"ok" : true, "_index" : "foo"
# &gt; , "_id" : "2", "_type" : "bar"}}]}

# [Sat Jan  8 20:01:34 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/foo/_count'  -d '
{"match_all" : {}}'

# [Sat Jan  8 20:01:34 2011] Response:
# {"count" : 0, "_shards" : {"failed" : 0, "successful" : 5, "total
# &gt; " : 5}}
```

I also tried setting `index.refresh_interval: -1` and passing `refresh=true` as above, and the index doesn't refresh at all, until I pass it an actual `/_refresh`
</description><key id="514706">614</key><summary>refresh param to bulk not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2011-01-08T19:16:36Z</created><updated>2011-02-06T21:56:17Z</updated><resolved>2011-01-10T04:12:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-09T20:12:02Z" id="664804">refresh param to bulk not working, closed by d650705a03b9ee8b1140ea8b17cb618c974d3119.
</comment><comment author="clintongormley" created="2011-02-06T21:56:17Z" id="744459">OK - refreshes are now being triggered, but the refresh is still async, which makes for clumsy client code.

In other words, I do the bulk index, having passed `refresh=true` and then I expect to be able to continue directly on to the next statement in my code, which expects the index to have been refreshed.

However, what is happening is that the refresh is queued, and by the time I send my next request, it still hasn't completed.  This means that I'm having to wait an unspecified amount of time (because I don't know how long the refresh will take) before I can continue running my code.

Should `refresh=true` not block?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Sometimes highlighting returns empty fragments even thought there should be</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/613</link><project id="" key="" /><description>Highlighting: Sometimes highlighting returns empty fragments even thought there should be
</description><key id="513688">613</key><summary>Highlighting: Sometimes highlighting returns empty fragments even thought there should be</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.3</label><label>v0.15.0</label></labels><created>2011-01-08T01:18:15Z</created><updated>2011-01-08T01:19:40Z</updated><resolved>2011-01-08T01:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-08T01:19:40Z" id="661896">Highlighting: Sometimes highlighting returns empty fragments even thought there should be, closed by 5e4a1e8863cfb8b4444da95c28083bda13ddc008.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Solr Compatible Query and Response Format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/612</link><project id="" key="" /><description>To ease the migration from solr it would be a great benefit for ES if there would be the ability of solr compatible query and response formats. Implemented as plugin or java api?

As in (indexing) and output (querying) format xml should be the first try, because the java binary could be too complicated and json isn't supported for indexing yet.

For solr I'm expecting the single index queries (no shards) because I have it that way and its easier ;-)

one 'complicate' query with facets (without highlighting) can look like:

q=&amp;facet=true&amp;facet.date=%7B%21ex%3Ddt%7Ddt&amp;facet.date.start=NOW%2FDAY-6DAYS&amp;facet.date.end=NOW%2FDAY%2B1DAY&amp;facet.date.gap=%2B1DAY&amp;facet.sort=count&amp;facet.limit=10&amp;facet.field=dest_title_1_s&amp;f.dest_title_1_s.facet.mincount=1&amp;f.dest_title_1_s.facet.limit=12&amp;f.tag.facet.mincount=2&amp;f.tag.facet.limit=20&amp;facet.query=dt%3A%5BNOW%2FHOURS-8HOURS+TO+_%5D&amp;facet.query=retw_i%3A%5B5+TO+_%5D&amp;facet.query=retw_i%3A%5B20+TO+_%5D&amp;facet.query=retw_i%3A%5B50+TO+_%5D&amp;fq=user%3Ameysrh&amp;start=0&amp;rows=15

there can be also update, deleteByQuery, commit and optimize. All except the update query should be simple.

The update xml looks like:

   &lt;add&gt;
    &lt;doc&gt;
    &lt;field name="id"&gt;9885A004&lt;/field&gt;
    &lt;field name="name"&gt;Canon PowerShot SD500&lt;/field&gt;
    &lt;field name="category"&gt;camera&lt;/field&gt;
    &lt;field name="features"&gt;3x optical zoom&lt;/field&gt;
    &lt;field name="features"&gt;aluminum case&lt;/field&gt;
    &lt;field name="weight"&gt;6.4&lt;/field&gt;
    &lt;field name="price"&gt;329.95&lt;/field&gt;
    &lt;/doc&gt;
   &lt;/add&gt;

response xml looks like

  &lt;response&gt;
  &lt;responseHeader&gt;
    &lt;status&gt;0&lt;/status&gt;&lt;QTime&gt;1&lt;/QTime&gt;
  &lt;/responseHeader&gt;
  &lt;result numFound="2" start="0"&gt;
   &lt;doc&gt;
    &lt;str name="id"&gt;MA147LL/A&lt;/str&gt;
    &lt;str name="name"&gt;Apple 60 GB iPod Black&lt;/str&gt;
   &lt;/doc&gt;
   &lt;doc&gt;
    &lt;str name="id"&gt;EN7800GTX/2DHTV/256M&lt;/str&gt;
    &lt;str name="name"&gt;ASUS Extreme N7800GTX&lt;/str&gt;
   &lt;/doc&gt;
  &lt;/result&gt;
  &lt;/response&gt;

taken from here: http://www.xml.com/pub/a/2006/08/09/solr-indexing-xml-with-lucene-andrest.html

facets are added either before or after the results:

```
&lt;lst name="facet_counts"&gt;
&lt;lst name="facet_queries"&gt;
&lt;int name="dt:[NOW/HOURS-8HOURS TO *]"&gt;15&lt;/int&gt;
&lt;int name="dt:[* TO NOW/DAY-6DAYS]"&gt;40&lt;/int&gt;
&lt;int name="retw_i:[5 TO *]"&gt;0&lt;/int&gt;
&lt;int name="retw_i:[20 TO *]"&gt;0&lt;/int&gt;
&lt;int name="retw_i:[50 TO *]"&gt;0&lt;/int&gt;
&lt;/lst&gt;
&lt;lst name="facet_fields"&gt;
&lt;lst name="tag"&gt;
&lt;int name="meysrh"&gt;13&lt;/int&gt;
&lt;int name="lg"&gt;11&lt;/int&gt;
...
&lt;/lst&gt;
&lt;/lst&gt;
&lt;lst name="dest_title_1_s"&gt;
&lt;int name="_1241812383_&#220;berTwitter -.."&gt;1&lt;/int&gt;
&lt;int name="_1404625014_TwitLonger: &#9786; ..""&gt;1&lt;/int&gt;
&lt;int name="_1404721733_TwitLonger: .."&gt;1&lt;/int&gt;
&lt;int name="&#220;berTwitter - romynandariz.."&gt;1&lt;/int&gt;
&lt;/lst&gt;
&lt;/lst&gt;
&lt;lst name="facet_dates"&gt;
&lt;lst name="dt"&gt;
&lt;int name="2011-01-02T00:00:00Z"&gt;0&lt;/int&gt;
&lt;int name="2011-01-03T00:00:00Z"&gt;0&lt;/int&gt;
&lt;int name="2011-01-04T00:00:00Z"&gt;0&lt;/int&gt;
&lt;int name="2011-01-05T00:00:00Z"&gt;0&lt;/int&gt;
&lt;int name="2011-01-06T00:00:00Z"&gt;0&lt;/int&gt;
&lt;int name="2011-01-07T00:00:00Z"&gt;30&lt;/int&gt;
&lt;int name="2011-01-08T00:00:00Z"&gt;0&lt;/int&gt;
&lt;str name="gap"&gt;+1DAY&lt;/str&gt;
&lt;date name="end"&gt;2011-01-09T00:00:00Z&lt;/date&gt;
&lt;/lst&gt;
&lt;/lst&gt;
&lt;/lst&gt;
```
</description><key id="513620">612</key><summary>Solr Compatible Query and Response Format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-01-08T00:14:45Z</created><updated>2011-12-13T23:37:28Z</updated><resolved>2011-05-21T22:24:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-05-21T22:24:11Z" id="1215751">A solr "a-like" query builder was developed here:

https://github.com/karussell/Jetwick/blob/master/src/main/java/de/jetwick/es/JetwickQuery.java

date math is not supported but simply convert your date into string.

See parseQuery and toString method for the important stuff (a lot of methods can be dropped because you won't need them)

The xml response should be easily convertable though. I'm not anylonger interesting in this feature so I'll close it.
</comment><comment author="mattweber" created="2011-12-13T23:37:28Z" id="3132761">Also take a look at the Mock Solr Plugin:
https://github.com/mattweber/elasticsearch-mocksolrplugin
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"took" for "count" API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/611</link><project id="" key="" /><description>Just like the result of "search" and "bulk" now include "took" - the elapsed time in milliseconds &#8211; it would be nice to have this timing for "count" as well.
</description><key id="513576">611</key><summary>"took" for "count" API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aferreira</reporter><labels /><created>2011-01-07T23:55:44Z</created><updated>2013-04-04T19:00:01Z</updated><resolved>2013-04-04T19:00:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:00:01Z" id="15916795">You can get this using `search_type=count` in the search API
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE for when passing null date to content builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/610</link><project id="" key="" /><description>Minor issue ...

The same check against null should be for date type otherwise it leads to NPE:

```
  public XContentBuilder value(Date date, DateTimeFormatter dateTimeFormatter) throws IOException {
    if (date == null) {
        generator.writeNull();
    } else {
    return value(dateTimeFormatter.print(date.getTime()));
    }
}
```
</description><key id="513120">610</key><summary>NPE for when passing null date to content builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-07T19:52:51Z</created><updated>2011-01-07T21:29:10Z</updated><resolved>2011-01-07T21:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-07T21:29:10Z" id="661457">NPE for when passing null date to content builder, closed by 7364159d6915fbbeae02335f02dc333839dffbd9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boolean fields in custom_score fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/609</link><project id="" key="" /><description>I'm trying to write a custom_score script like this:

 &lt;pre&gt;{"script": "(doc['boolean_field'].value ? 100 : 0)"}&lt;/pre&gt;

Mapping shows:

&lt;pre&gt;
 {"boolean_field": {"type": "boolean"}}
&lt;/pre&gt;


However, the script fails with:

&lt;pre&gt;
CompileException[[Error: expected Boolean; but found: java.lang.String]
&lt;/pre&gt;


It works if I rewrite my condition to:
 &lt;pre&gt;{"script": "(doc['boolean_field'].value == 'true' ? 100 : 0)"}&lt;/pre&gt;
</description><key id="511833">609</key><summary>Boolean fields in custom_score fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pfeiffer</reporter><labels><label>discuss</label></labels><created>2011-01-07T04:27:31Z</created><updated>2014-07-18T07:56:55Z</updated><resolved>2014-07-18T07:56:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-07T13:25:20Z" id="660328">Right, thats because there is no specific "doc" type for boolean, so it ends up using the string one. There should be a doc type for boolean, I will work on adding one.
</comment><comment author="pfeiffer" created="2011-01-11T03:59:33Z" id="668207">For some reason I can't get the .value == "true" comparison to work either on boolean fields, unless I specifically map the field as a 'string'.

Any ideas when this can be implemented? Would be thankful if this could be fixed in 0.15.0. Thanks! :-)
</comment><comment author="mschulkind" created="2013-02-23T05:15:32Z" id="13985455">I'm still having trouble with this. Any fix coming?
</comment><comment author="clintongormley" created="2013-02-23T13:47:01Z" id="13990252">Boolean fields are implemented internally as string fields with one of the terms `T` or `F`, so you can do:

```
{ "script": "doc['bool_field'].value == 'T' ? 'true' : 'false' " } 
```
</comment><comment author="damienalexandre" created="2013-07-03T09:55:34Z" id="20405742">Thx for the workaround @clintongormley!

I :+1: for this issue too. 
And it look like Booleans are a real issue with ElasticSearch, already got an issue for them in Facets. There is an active discussion about this topic in #2462.
</comment><comment author="clintongormley" created="2014-07-18T07:56:55Z" id="49404927">Closing in favour of #4678 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make URI query decoding more robust</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/608</link><project id="" key="" /><description>With the current ES code, exceptions or infinite loops can be generated while parsing the URI query of a request.

This can be seen with the following examples:

```
curl -XGET localhost:9200/test/_analyze?t&amp;text=this+is+a+test
# the exception stack trace shows up in logs

curl -XGET localhost:9200/test/_analyze?t1&amp;t2&amp;text=this+is+a+test
# never returns, never ends
```

The changes suggested take care of these issues.
</description><key id="511548">608</key><summary>Make URI query decoding more robust</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aferreira</reporter><labels><label>enhancement</label><label>v0.14.2</label><label>v0.15.0</label></labels><created>2011-01-07T00:16:56Z</created><updated>2014-06-14T02:51:49Z</updated><resolved>2011-01-07T12:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-07T12:35:48Z" id="660228">Heya,

  Thanks for the work!. I added it, and also improved a bit the logic to handle properly cases of empty names with no "=", as well as improve the perf for query component decoding (inspired by netty)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo-distance in scripts (like custom_score)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/607</link><project id="" key="" /><description>Feature request for distance calculator method in custom_score queries, eg.:

```
doc['geo_point_field'].distance(lat, lon)
```

The following methods are allowed:
- `doc['geo_point_field'].distance(lat, lon)`: Returns the distance in miles form lat/lon.
- `doc['geo_point_field'].distanceInKm(lat, lon)`: Returns the distance in km lat/lon.
- `doc['geo_point_field'].geohashDistance(geohash)`: Returns the distance in miles from geohash.
- `doc['geo_point_field'].geohashDistanceInKm(geohash)`: Returns the distance in km from geohash.
</description><key id="510329">607</key><summary>Geo-distance in scripts (like custom_score)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pfeiffer</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-06T14:18:12Z</created><updated>2011-01-07T13:22:23Z</updated><resolved>2011-01-07T13:22:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-07T13:22:23Z" id="660322">Geo-distance in scripts (like custom_score), closed by b9be6d9ea724de2c9a66a7d5870a3abca7ba6f8b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snowball</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/606</link><project id="" key="" /><description>This adds the snowball stemmer to elastic search directly.

This time with the analyzer I added support for those languages where stopwords were already available, namely Dutch, English (default if no language attribute), French, German and German2. Additional stopwords attribute also supported e.g;

```
index:
    analysis:
        analyzer:
            default:
                type: snowball
                language: French
                stopwords: [avec, dans]
```

Snowball filter also supported as before;

```
index:
    analysis:
        analyzer:
            default:
                type: custom
                tokenizer: whitespace
                filter: [snowball]
        filter:
            snowball:
                type: snowball
                language: German2
```
</description><key id="509431">606</key><summary>Snowball</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harryf</reporter><labels /><created>2011-01-06T00:57:12Z</created><updated>2014-06-12T08:05:49Z</updated><resolved>2011-01-07T08:25:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="harryf" created="2011-01-07T00:25:25Z" id="659255">Thanks to adding. Closing this pull
</comment><comment author="bikash119" created="2011-03-09T11:16:28Z" id="851261">can any one help me , how to add analyzer to each field being indexed using java api? pls pls pls..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Have ability for new search type 'scan' that avoids sorting for quick retrieval of slices of data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/605</link><project id="" key="" /><description>Based on a discussion with Shay about having the ability to quickly retrieve all the _id and _version values for all items (or a subset) in an index.  More formally being able to request a set of fields to return.  He proposed a new type of search called 'scan'.  

One use case for this is to be able to compare the state in the index with an external source of 'truth', allowing the client to perform these tests:
- Which items are in the source, but _not_ in the index (a missed insert, or a botched delete, completely missing in the sequence, this is why sorting is important, because it's a sorted stream comparison, but I think the verification client should handle the sort to simplify it for ES)
- Which items are in the index but _not_ in the source (a missed delete for the index, need to be purged)
- Which items are in the index, but the version (timestamps/checksums) do not match - these will need to be reindexed from the source to be accurate.

This 'scan' type of request does not require any sorting or scoring at all, it is simply a bulk 'pull'.

In an ideal world the result stream could be consumed by a client which is comparing it with another stream from the external source of truth (after sorting by this client to do efficient 'diffs').

See this thread for more background:  http://elasticsearch-users.115913.n3.nabble.com/Just-Pushed-Versioning-td2189167.html
</description><key id="509106">605</key><summary>Have ability for new search type 'scan' that avoids sorting for quick retrieval of slices of data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tallpsmith</reporter><labels /><created>2011-01-05T22:13:04Z</created><updated>2011-07-27T17:35:33Z</updated><resolved>2011-07-27T17:35:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-02-09T21:45:30Z" id="755642">+1 then we can avoid memory intensive scrolling and #680 can be closed ;)

Also similar to #492
</comment><comment author="karussell" created="2011-02-22T14:08:18Z" id="797224">as of #707 this can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow disabling of some auto-mapping classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/604</link><project id="" key="" /><description>I am indexing unknown json data (unknown at code-deployment-time) and in some cases a field I index may be an IP string or a hostname string. If an IP string gets indexed first, ES maps that field to IP and the next hostname index will fail with:

org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [@fields.HOSTNAME]
Caused by: org.elasticsearch.ElasticSearchIllegalArgumentException: failed ot parse ip [pipes.yahoo.com], not full ip address (4 dots)

Being able to disable automatic mapping to certain classes would be useful (in the config file and/or making it an index/type setting)
</description><key id="509025">604</key><summary>Allow disabling of some auto-mapping classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels /><created>2011-01-05T21:35:27Z</created><updated>2011-06-21T03:24:18Z</updated><resolved>2011-06-20T23:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="darkhelmet" created="2011-06-20T16:37:46Z" id="1403902">Dates are a pain for me. It will see something, think it's a date, then the next thing blows up, because it's obviously not a date. Despite all my efforts to try to set the mapping, some things still sneak through in weird cases, and I'd rather just turn off automatic date mapping.
</comment><comment author="kimchy" created="2011-06-20T23:25:23Z" id="1406683">Agreed, I actually disabled dynamic ip detection based on this issue feedback. Can you open a seprate issue for disabling dates? Just so its more clear (and simpler for me to manage).
</comment><comment author="darkhelmet" created="2011-06-21T03:24:18Z" id="1407714">Boom. #1051
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add to node stats the number of times field cache was evicted due to memory constraints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/603</link><project id="" key="" /><description>Add to node stats the number of times field cache was evicted due to memory constraints
</description><key id="508861">603</key><summary>Add to node stats the number of times field cache was evicted due to memory constraints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.2</label><label>v0.15.0</label></labels><created>2011-01-05T20:11:08Z</created><updated>2011-01-05T20:37:30Z</updated><resolved>2011-01-05T20:37:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-05T20:37:30Z" id="655901">Add to node stats the number of times field cache was evicted due to memory constraints, closed by 66d63055df170453c21add6e82d2605c1f47daff.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `resident` field cache type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/602</link><project id="" key="" /><description>Add `resident` field cache type (`index.cache.field.type`) that does not unloads field cache when there are memory constraints. This can help understanding the memory requirements of an application, and not getting into constant unloading and loading of field cache on memory shortage.
</description><key id="508745">602</key><summary>Add `resident` field cache type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.2</label><label>v0.15.0</label></labels><created>2011-01-05T19:23:08Z</created><updated>2011-01-05T19:23:59Z</updated><resolved>2011-01-05T19:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-05T19:23:59Z" id="655698">Add `resident` field cache type, closed by 754b0d7a0f27b10456a8d6a8df9898c6abe4914c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>feature request : highlight same field twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/601</link><project id="" key="" /><description>Hi ,

I would like to highlight a field twice :

```
           1 - number of fragments to 0 ==&gt; get all highlighted
           2- number of fragments to x ==&gt; get some fragments highlighted
```

in one search.

from a json perspective this will not work

curl -XGET 'http://localhost:9200/testindex_index/_search' -d  '{"highlight": {"fields": {"fulltext": {"number_of_fragments": 0},"fulltext": {"number_of_fragments": 5}},"post_tags": ["&lt;/em&gt;"],"pre_tags": ["&lt;em&gt;"]},"query": {"query_string": {"query": "test"}},"size": 1}'

since its a dict.

The only thing i can think of is adding another field "fulltextcopy" to my model and reindex all .

Unfortunately the fulltext field is really large and i have about 100m docs. If i am not mistaken even by setting store to false will increase the size of my data folder.

Something nice would be the ability to add for example fieldname__copy  and then es would strip of __copy and call lucene highlighter on that filed.

Thanks
</description><key id="508071">601</key><summary>feature request : highlight same field twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">locojay</reporter><labels /><created>2011-01-05T14:33:51Z</created><updated>2013-07-16T07:35:44Z</updated><resolved>2013-07-16T07:35:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T07:35:44Z" id="21025856">Closing this one for the lack of activity. If you fill this is important please feel free to reopen and explain more.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: The result of highlighting for a hit can contain data from another document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/600</link><project id="" key="" /><description>Highlighting: The result of highlighting for a hit can contain data from another document
</description><key id="507987">600</key><summary>Highlighting: The result of highlighting for a hit can contain data from another document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.2</label><label>v0.15.0</label></labels><created>2011-01-05T13:44:29Z</created><updated>2011-01-05T13:46:01Z</updated><resolved>2011-01-05T13:46:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-05T13:46:01Z" id="654766">Highlighting: The result of highlighting for a hit can contain data from another document, closed by 31231531e117e257d7a2dcdccf89b5f179cde8a6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API: Add how long the bulk API took (in milliseconds) to the response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/599</link><project id="" key="" /><description>Bulk API: Add how long the bulk API took (in milliseconds) to the response
</description><key id="507872">599</key><summary>Bulk API: Add how long the bulk API took (in milliseconds) to the response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-05T12:12:25Z</created><updated>2011-01-05T12:12:58Z</updated><resolved>2011-01-05T12:12:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-05T12:12:58Z" id="654623">Bulk API: Add how long the bulk API took (in milliseconds) to the response, closed by 6258915205d21ba0174ec94833d77b5f705a2397.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snowball Analyzer Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/598</link><project id="" key="" /><description>Following on from [this discussion](http://elasticsearch-users.115913.n3.nabble.com/Folding-German-characters-like-umlauts-tp2176078p2176078.html;cid=1294063947726-860) on the mailing list, put together a plugin wrapping lucenes Snowball Stemmer.

For English use cases, the plugin contains an AnalyzerProvider that returns a SnowballAnalyzer hard wired to use an English SnowballFilter and English stopwords. Stopwords were the reason to hard wire to English, as it can't be guaranteed there are default stopword lists available for all languages available to the Snowball Stemmer. This can be used like;

```
index:
    analysis:
        analyzer:
            default:
                type: snowball
```

For non English use, the SnowballFilter can be used as part of a CustomAnalyzer, for example;

```
index:
    analysis:
        analyzer:
            default:
                type: custom
                tokenizer: whitespace
                filter: [snowball]
        filter:
            snowball:
                type: snowball
                language: German2
```

Apologies for not creating a topic branch for this - getting used to git and github and failed to spot that - commits to master this time.
</description><key id="507222">598</key><summary>Snowball Analyzer Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harryf</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-05T02:14:19Z</created><updated>2014-06-13T08:38:56Z</updated><resolved>2011-01-05T13:41:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-05T09:17:31Z" id="654347">I was thinking about it a bit after the discussion in the mailing list as well..., and I think it makes sense to add it to core by default, not as a plugin (the snowball analyzer jar is just 100k). I can take what you did and do it, or are you up for another pull request :) ?
</comment><comment author="harryf" created="2011-01-05T13:41:38Z" id="654759">Sounds good - I'll go for another pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Video of creating a new plugin in Intellij</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/597</link><project id="" key="" /><description>Partly me as I'm an Intellij newb ( but liking it ) but would be great to see the general steps you perform when creating a new elasticsearch plugin. Would help plugin developers I think
</description><key id="505472">597</key><summary>Video of creating a new plugin in Intellij</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harryf</reporter><labels /><created>2011-01-04T10:49:22Z</created><updated>2013-04-04T19:00:39Z</updated><resolved>2013-04-04T19:00:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-07T13:33:11Z" id="660347">Agreed!, there should be more videos I think. A good one would be to create a project from scratch and write a plugin for it (not within ES codebase). Are you up for it after creating your own already? :)
</comment><comment author="harryf" created="2011-01-07T19:59:39Z" id="661169">Still got a fair bit to learn there. To get the plugin to work ended up hacking files directly in the .idea/ directory so I could get Intellij to see the lucene snowball jar, for example. And setting up the paths right for main and test directories was done on the filesystem also. Failed to figure it out via the GUI. Hence this bug.

But if I get there, will make some videos.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API to get the terms used in a "more_like_this" query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/596</link><project id="" key="" /><description>It would be useful to get the terms that ES uses internally when doing a "more_like_this" query for a given document. This would allow us to launch this query against a different index or a different database.

Of course this assumes the index where we want to apply this query is similar to the index where the document is in, or we risk having a set of "relevant" terms to describe the document that are not that relevant in the new index, but I think that for big indexes this feature will be useful.

One real world example where I want to apply this is to search using Google news API for news articles related to a particular document.
</description><key id="505469">596</key><summary>API to get the terms used in a "more_like_this" query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">e98cuenc</reporter><labels /><created>2011-01-04T10:47:16Z</created><updated>2013-07-16T08:12:58Z</updated><resolved>2013-07-16T08:12:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T08:12:58Z" id="21027338">The MLT endpoint offers the option to search another index than the one in which the document is (see `search_indices` param on http://www.elasticsearch.org/guide/reference/api/more-like-this/ ). Internally it doesn't do more than getting the text of the document and using it with the MoreLikeThis query, which is something you can easily simulate in other searches to elasticsearch.

As to search in other sources, like google, we are working on a term vectors endpoint which will allow you to get the information about the individual words within the documents, their frequency etc. You will be able to use it to construct smart queries to outside sources. See: https://github.com/elasticsearch/elasticsearch/pull/3115

I'm closing this for now, if you feel these are not sufficient - please feel free to reopen and discuss.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow ./bin/plugin to install plugins from the local filesystem / custom url</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/595</link><project id="" key="" /><description>Allow to install plugins from a custom location (url or file location). For example, to install the output of a `gradle release` run, one can use (assuming extracted under `build/distributions`):

```
 build/distributions/elasticsearch-0.15.0-SNAPSHOT/bin/plugin -url build/distributions/plugins/ -install lang-groovy
```

The `-url` parameter accepts either a file based location, or a url. For file based location, the structure supported is either a flat dir with the plugin(s) there, or a `plugin-name/plugin-file` structure.

Note, this is a breaking change since now one can only list a single plugin name in `-install`, though it can be repeated several times to install several plugins in a single command.
</description><key id="505459">595</key><summary>Allow ./bin/plugin to install plugins from the local filesystem / custom url</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harryf</reporter><labels><label>breaking</label><label>feature</label><label>v0.15.0</label></labels><created>2011-01-04T10:42:37Z</created><updated>2011-01-06T14:08:13Z</updated><resolved>2011-01-06T16:46:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-06T08:46:53Z" id="657161">Allow ./bin/plugin to install plugins from the local filesystem / custom url, closed by 26c5f6c48241fddfc8e50f46b27d7aaf62aa3708.
</comment><comment author="harryf" created="2011-01-06T14:08:13Z" id="657718">Many thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/594</link><project id="" key="" /><description>The versioning feature allows to handle conflicts when doing a get/search and then index/delete. Each document has a version number, and each index and delete (tombstone) increase the version number.

A conflict is detected when providing a version parameter to the index/delete request and there is a mismatch between the version provided and the _real time version_ of the current document. If a version number is not provided, then the operation is forced without doing any version check.

Search and get operations return the version number associated with the hit under `_version`. Note, the version number reflects the near real time status of the search. In other words, the version number will point to the doc when the index was last refreshed.

Nice side affects of this feature include returning on delete if the document was actually found or not, as well as fixing a rare out of order replication sync between a primary and its replica (under heavy changes of the same doc within a short period of time).

Added features include:
1. Index with create flag will now fail if there is already an indexed doc. This basically allows to use create as a putIfAbsent logic.
2. Indexing a document that does not exists will be as fast as with create flag set to true. So no need to set it anymore to improve fresh data indexing performance.

The bulk operation supports providing a version number as well using `_version` on each bulk item.
## Upgrade Notes

The versioning support is backward compatible, though only new operations on documents will cause the versioning system to start and kick in for that doc.
</description><key id="504859">594</key><summary>Versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.15.0</label></labels><created>2011-01-04T02:03:03Z</created><updated>2011-01-05T12:22:58Z</updated><resolved>2011-01-04T10:04:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-04T02:04:49Z" id="651099">Versioning, closed by 45c1ab06b3d50f63667b1df63beb04bd7e0c9da2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script: Allow to pass `-noout` or `-Des.logging.console=false` to disable console logging even when running `elasticsearch -f`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/593</link><project id="" key="" /><description>Script: Allow to pass `-noout` or `-Des.logging.console=false` to disable console logging even when running `elasticsearch -f`
</description><key id="504704">593</key><summary>Script: Allow to pass `-noout` or `-Des.logging.console=false` to disable console logging even when running `elasticsearch -f`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2011-01-03T23:21:07Z</created><updated>2014-07-08T11:56:05Z</updated><resolved>2014-07-08T11:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T11:56:05Z" id="48326587">No discussion for 3 years. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Add a `timed_out` element indicating if the search request timed out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/592</link><project id="" key="" /><description>Search: Add a `timed_out` element indicating if the search request timed out
</description><key id="503372">592</key><summary>Search: Add a `timed_out` element indicating if the search request timed out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-03T13:24:45Z</created><updated>2011-01-03T13:25:36Z</updated><resolved>2011-01-03T13:25:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-03T13:25:36Z" id="648886">Search: Add a `timed_out` element indicating if the search request timed out, closed by 9335b3a9e6e85e16f9de3f2397072d7e4845251b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Date Histogram Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/591</link><project id="" key="" /><description>The current histogram facet works with dates, but has some problems when handling them, specifically, it does not allow for custom time zones to be set, as well as does not support `month` and `year` intervals (only lower level weeks/days/hours).

The `date_histogram` allows for specific handling when the (key) field faceting on is of type date. Here is an example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "histo1" : {
            "date_histogram" : {
                "field" : "field_name",
                "interval" : "day"
            }
        }
    }
}
```
## Interval

The `interval` allows to set the interval at which buckets will be created for each hit. It allows for the constant values of `year`, `month`, `day`, `hour`, `minute`. 

The specific constant values also support setting rounding by appending `:` to it, and then the rounding value. For example: `day:ceiling`. The values are:
- `floor`: (the default), rounds to the lowest whole unit of this field.
- `ceiling`: Rounds to the highest whole unit of this field.
- `half_floor`: Round to the nearest whole unit of this field. If the given millisecond value is closer to the floor or is exactly halfway, this function behaves like `floor`. If the millisecond value is closer to the ceiling, this function behaves like `ceiling`.
- `half_ceiling`: Round to the nearest whole unit of this field. If the given millisecond value is closer to the floor, this function behaves like `floor`. If the millisecond value is closer to the ceiling or is exactly halfway, this function behaves like `ceiling`.
- `half_even`: Round to the nearest whole unit of this field. If the given millisecond value is closer to the floor, this function behaves like `floor`. If the millisecond value is closer to the ceiling, this function behaves like `ceiling`. If the millisecond value is exactly halfway between the floor and ceiling, the ceiling is chosen over the floor only if it makes this field's value even.

It also support time value setting like `1.5h` (up to `w` for weeks).
## Time Zone

By default, times are stored as UTC milliseconds since the epoch. Thus, all computation and "bucketing" / "rounding" is done on UTC. It is possible to provide a `zone` value, which will cause all computations to take the relevant zone into account. The time returned for each bucket/entry is milliseconds since the epoch of the provided time zone.

The `zone` value accepts either a numeric value for the hours offset, for example: `"zone" : -2`. It also accepts a format of hours and minutes, like `"zone" : "-02:30"`.

Another option is to provide a time zone accepted as one of the values listed here: http://joda-time.sourceforge.net/timezones.html.
## Value Field

The `date_histogram` facet allows to use a different key (of type date) which controls the bucketing, with a different value field which will then return the `total` and `mean` for that field values of the hits within the relevant bucket. For example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "histo1" : {
            "histogram" : {
                "key_field" : "timestamp",
                "value_field" : "price",
                "interval" : "day"
            }
        }
    }
}
```
## Script Value Field

A script can be used to compute the value that will then be used to compute the `total` and `mean` for a bucket. For example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "histo1" : {
            "histogram" : {
                "key_field" : "timestamp",
                "value_script" : "doc['price'].value * 2",
                "interval" : "day"
            }
        }
    }
}
```
</description><key id="501829">591</key><summary>Search: Date Histogram Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.15.0</label></labels><created>2011-01-02T13:07:11Z</created><updated>2011-01-02T13:08:01Z</updated><resolved>2011-01-02T13:08:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-02T13:08:01Z" id="646405">Search: Date Histogram Facet, closed by 07d361816e3d3130d107959da14c1570f7fefccd.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>more_like_this and more_like_this_field broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/590</link><project id="" key="" /><description>Hiya

Something has changed in v 0.14 - more_like_this and more_like_this_field no longer work.

Use this to create a test index:

```
# [Sat Jan  1 20:02:05 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/es_test_1/'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/'
curl -XPUT 'http://127.0.0.1:9200/_all/type_1/_mapping'  -d '
{
   "type_1" : {
      "_source" : {
         "compress" : 1
      },
      "_all" : {
         "store" : "yes",
         "term_vector" : "with_positions_offsets"
      },
      "properties" : {
         "num" : {
            "store" : "yes",
            "type" : "integer"
         },
         "date" : {
            "format" : "yyyy-MM-dd HH:mm:ss",
            "type" : "date"
         },
         "text" : {
            "store" : "yes",
            "type" : "string"
         }
      }
   }
}
'
curl -XPUT 'http://127.0.0.1:9200/_all/type_2/_mapping'  -d '
{
   "type_2" : {
      "_source" : {
         "compress" : 1
      },
      "_all" : {
         "store" : "yes",
         "term_vector" : "with_positions_offsets"
      },
      "properties" : {
         "num" : {
            "store" : "yes",
            "type" : "integer"
         },
         "date" : {
            "format" : "yyyy-MM-dd HH:mm:ss",
            "type" : "date"
         },
         "text" : {
            "store" : "yes",
            "type" : "string"
         }
      }
   }
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : 2,
   "date" : "2010-04-02 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : 3,
   "date" : "2010-04-03 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : 4,
   "date" : "2010-04-04 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : 5,
   "date" : "2010-04-05 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : 6,
   "date" : "2010-04-06 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : 7,
   "date" : "2010-04-07 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : 8,
   "date" : "2010-04-08 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : 9,
   "date" : "2010-04-09 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : 10,
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : 11,
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : 12,
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : 13,
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : 14,
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : 15,
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : 16,
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : 17,
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : 18,
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : 19,
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : 20,
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : 21,
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : 22,
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : 23,
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : 24,
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : 25,
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : 26,
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : 27,
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : 28,
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : 29,
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/30'  -d '
{
   "num" : 31,
   "date" : "2010-05-01 00:00:00",
   "text" : "foo"
}
'
```

Then test these queries:

```
curl -XGET 'http://127.0.0.1:9200/_all/_count'  -d '
{
   "mlt" : {
      "like_text" : "foo bar baz",
      "min_term_freq" : 1
   }
}
'

# [Sat Jan  1 20:02:10 2011] Response:
# {
#    "count" : 0,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }

# [Sat Jan  1 20:02:10 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/_all/_count'  -d '
{
   "mlt_field" : {
      "text" : {
         "like_text" : "foo bar baz",
         "min_term_freq" : 1
      }
   }
}
'

# [Sat Jan  1 20:02:10 2011] Response:
# {
#    "count" : 0,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```
</description><key id="501157">590</key><summary>more_like_this and more_like_this_field broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-01-01T19:08:39Z</created><updated>2011-03-20T14:26:40Z</updated><resolved>2011-03-20T14:26:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-01-02T13:23:35Z" id="646420">OK - the difference seems to be `min_doc_freq`.  Set this to `3` and the count is positive.  However, the terms `foo`, `bar` and `baz` exist in more than  3 docs.
</comment><comment author="clintongormley" created="2011-01-02T15:33:12Z" id="646571">The commit which changed this behaviour is: 92b3ae3f7387b4b4df4d78256b7f8d58c58fc2e0
</comment><comment author="kimchy" created="2011-01-07T13:32:09Z" id="660345">hey, was it because the min_doc_freq is only applicable on the shard level at the end (as we talked on IRC)?
</comment><comment author="clintongormley" created="2011-01-07T13:38:25Z" id="660356">It looks like - the behaviour definitely changed with commit 92b3ae3, but I'm not sure that it is any less "correct"
</comment><comment author="clintongormley" created="2011-03-20T14:26:40Z" id="895242">Nothing to fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scrolling still broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/589</link><project id="" key="" /><description>Hiya

Scrolling is still broken in version 0.14.1 - see:

```
# [Sat Jan  1 19:00:24 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/'  -d '
{}
'

# [Sat Jan  1 19:00:25 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Jan  1 19:03:22 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPOST 'http://127.0.0.1:9200/_bulk'  -d '
{"index" : {"_index" : "foo", "_id" : 1, "_type" : "bar"}}
{"num" : 1}
{"index" : {"_index" : "foo", "_id" : 2, "_type" : "bar"}}
{"num" : 2}
{"index" : {"_index" : "foo", "_id" : 3, "_type" : "bar"}}
{"num" : 3}
{"index" : {"_index" : "foo", "_id" : 4, "_type" : "bar"}}
{"num" : 4}
{"index" : {"_index" : "foo", "_id" : 5, "_type" : "bar"}}
{"num" : 5}
{"index" : {"_index" : "foo", "_id" : 6, "_type" : "bar"}}
{"num" : 6}
{"index" : {"_index" : "foo", "_id" : 7, "_type" : "bar"}}
{"num" : 7}
{"index" : {"_index" : "foo", "_id" : 8, "_type" : "bar"}}
{"num" : 8}
{"index" : {"_index" : "foo", "_id" : 9, "_type" : "bar"}}
{"num" : 9}
{"index" : {"_index" : "foo", "_id" : 10, "_type" : "bar"}}
{"num" : 10}
'

# [Sat Jan  1 19:03:22 2011] Response:
# {"items" : [{"index" : {"ok" : true, "_index" : "foo", "_id" : "1
# &gt; ", "_type" : "bar"}}, {"index" : {"ok" : true, "_index" : "foo"
# &gt; , "_id" : "2", "_type" : "bar"}}, {"index" : {"ok" : true, "_in
# &gt; dex" : "foo", "_id" : "3", "_type" : "bar"}}, {"index" : {"ok" 
# &gt; : true, "_index" : "foo", "_id" : "4", "_type" : "bar"}}, {"ind
# &gt; ex" : {"ok" : true, "_index" : "foo", "_id" : "5", "_type" : "b
# &gt; ar"}}, {"index" : {"ok" : true, "_index" : "foo", "_id" : "6", 
# &gt; "_type" : "bar"}}, {"index" : {"ok" : true, "_index" : "foo", "
# &gt; _id" : "7", "_type" : "bar"}}, {"index" : {"ok" : true, "_index
# &gt; " : "foo", "_id" : "8", "_type" : "bar"}}, {"index" : {"ok" : t
# &gt; rue, "_index" : "foo", "_id" : "9", "_type" : "bar"}}, {"index"
# &gt;  : {"ok" : true, "_index" : "foo", "_id" : "10", "_type" : "bar
# &gt;  "}}]}

# [Sat Jan  1 19:04:24 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/foo/_search?scroll=5m'  -d '
{"sort" : ["num"], "fields" : [], "query" : {"match_all" : {}}, "size" : 2}'

# [Sat Jan  1 19:04:24 2011] Response:
# {"hits" : {"hits" : [{"sort" : [1], "_score" : null, "_index" : "
# &gt; foo", "_id" : "1", "_type" : "bar"}, {"sort" : [2], "_score" : 
# &gt; null, "_index" : "foo", "_id" : "2", "_type" : "bar"}], "max_sc
# &gt; ore" : null, "total" : 10}, "_shards" : {"failed" : 0, "success
# &gt; ful" : 5, "total" : 5}, "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NDg
# &gt; 6MnZNYWs3X0ZScTJER09mTmFDME1ldzs0Njoydk1hazdfRlJxMkRHT2ZOYUMwTW
# &gt; V3OzQ5OjJ2TWFrN19GUnEyREdPZk5hQzBNZXc7NDc6MnZNYWs3X0ZScTJER09mT
# &gt; mFDME1ldzs1MDoydk1hazdfRlJxMkRHT2ZOYUMwTWV3Ow==", "took" : 3}

# [Sat Jan  1 19:04:44 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll_id=cXVlcnlUaGVuRmV0Y2g7NDg6MnZNYWs3X0ZScTJER09mTmFDME1ldzs0Njoydk1hazdfRlJxMkRHT2ZOYUMwTWV3OzQ5OjJ2TWFrN19GUnEyREdPZk5hQzBNZXc7NDc6MnZNYWs3X0ZScTJER09mTmFDME1ldzs1MDoydk1hazdfRlJxMkRHT2ZOYUMwTWV3Ow%3D%3D' 

# [Sat Jan  1 19:04:44 2011] Response:
# {"hits" : {"hits" : [{"sort" : [3], "_score" : null, "_index" : "
# &gt; foo", "_id" : "3", "_type" : "bar"}, {"sort" : [4], "_score" : 
# &gt; null, "_index" : "foo", "_id" : "4", "_type" : "bar"}], "max_sc
# &gt; ore" : null, "total" : 10}, "_shards" : {"failed" : 0, "success
# &gt; ful" : 5, "total" : 5}, "took" : 2}

# [Sat Jan  1 19:04:48 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll_id=cXVlcnlUaGVuRmV0Y2g7NDg6MnZNYWs3X0ZScTJER09mTmFDME1ldzs0Njoydk1hazdfRlJxMkRHT2ZOYUMwTWV3OzQ5OjJ2TWFrN19GUnEyREdPZk5hQzBNZXc7NDc6MnZNYWs3X0ZScTJER09mTmFDME1ldzs1MDoydk1hazdfRlJxMkRHT2ZOYUMwTWV3Ow%3D%3D' 

# [Sat Jan  1 19:04:48 2011] Response:
# {"hits" : {"hits" : [{"sort" : [7], "_score" : null, "_index" : "
# &gt; foo", "_id" : "7", "_type" : "bar"}], "max_score" : null, "tota
# &gt; l" : 5}, "_shards" : {"failures" : [{"reason" : "SearchContextM
# &gt; issingException[No search context found for id [46]]"}, {"reaso
# &gt; n" : "SearchContextMissingException[No search context found for
# &gt;  id [50]]"}], "failed" : 2, "successful" : 3, "total" : 5}, "to
# &gt;  ok" : 1}

# [Sat Jan  1 19:04:50 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll_id=cXVlcnlUaGVuRmV0Y2g7NDg6MnZNYWs3X0ZScTJER09mTmFDME1ldzs0Njoydk1hazdfRlJxMkRHT2ZOYUMwTWV3OzQ5OjJ2TWFrN19GUnEyREdPZk5hQzBNZXc7NDc6MnZNYWs3X0ZScTJER09mTmFDME1ldzs1MDoydk1hazdfRlJxMkRHT2ZOYUMwTWV3Ow%3D%3D' 

# [Sat Jan  1 19:04:50 2011] Response:
# {"hits" : {"hits" : [], "max_score" : null, "total" : 3}, "_shard
# &gt; s" : {"failures" : [{"reason" : "SearchContextMissingException[
# &gt; No search context found for id [46]]"}, {"reason" : "SearchCont
# &gt; extMissingException[No search context found for id [49]]"}, {"r
# &gt; eason" : "SearchContextMissingException[No search context found
# &gt;  for id [50]]"}], "failed" : 3, "successful" : 2, "total" : 5},
# &gt;   "took" : 1}
```

ta

clint
</description><key id="501035">589</key><summary>Scrolling still broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-01-01T18:08:14Z</created><updated>2011-03-20T14:25:57Z</updated><resolved>2011-03-20T14:25:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-02-09T21:40:40Z" id="755579">as workaround you need to sort against _id or something

see #680
</comment><comment author="clintongormley" created="2011-03-20T14:25:56Z" id="895241">The issue was that I was not passing the `scroll` parameter to `_scroll` and retrieving the new `_scroll_id`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>-Des.config=/path/to/config/file doesn't replace $ES_HOME/elasticsearch.conf, just appends to it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/588</link><project id="" key="" /><description>When I start ES using:

```
./elasticsearch -f -Des.config=/path/to/config/file
```

it still reads the config in `$ES_HOME/elasticsearch.conf` as well which means that my live settings are interfering with the settings I want for my test suite.

Change it so that the explicit conf will be the only one used.
</description><key id="500961">588</key><summary>-Des.config=/path/to/config/file doesn't replace $ES_HOME/elasticsearch.conf, just appends to it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-01T17:24:58Z</created><updated>2011-01-03T10:54:01Z</updated><resolved>2011-01-03T10:54:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-03T10:12:04Z" id="648640">Is this a regression, or a change in behavior request? The code has not changed, and it loads the config/elasticsearch.yml file, and then loads the provided one using the explicit configuration merging the two (with the explicit one overriding the config/ settings).
</comment><comment author="clintongormley" created="2011-01-03T10:36:30Z" id="648670">No, not  a regression.  But, for instance, in my test suite, I use my own config, and I don't want config in the elasticsearch.conf file to interfere with that (eg tests failed because i have the setting `auto_create_index: 0`)

Is there no way of saying: use this config file INSTEAD of the default?
</comment><comment author="kimchy" created="2011-01-03T10:39:27Z" id="648672">&gt; Is there no way of saying: use this config file INSTEAD of the default?

No, not currently. My line of thinking was that you can provide "base" settings in the default settings file, and then use custom ones depending on what you run. But, I see where it might be confusing, so I can definitely change this behavior if it make more sense.
</comment><comment author="kimchy" created="2011-01-03T10:54:01Z" id="648690">Des.config=/path/to/config/file doesn&amp;#39;t replace $ES_HOME/elasticsearch.conf, just appends to it, closed by 5e029865a80318fdee195d56941f22a3ec890056.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Histogram Facet: Improve perf / memory by defaulting to just providing counts with no totals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/587</link><project id="" key="" /><description>When just executing simple facet on a field (not script), totals are computed as well as count. Default to just compute counts, and return them. If totals are also needed, just add `value_field` with the same field name to return to the previous behavior.

This improves both facet execution time (2x on a 1 million hit set) and memory utilization.
</description><key id="500893">587</key><summary>Histogram Facet: Improve perf / memory by defaulting to just providing counts with no totals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.15.0</label></labels><created>2011-01-01T15:27:26Z</created><updated>2011-01-01T15:28:10Z</updated><resolved>2011-01-01T15:28:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-01T15:28:10Z" id="645397">Histogram Facet: Improve perf / memory by defaulting to just providing counts with no totals, closed by aec720218d0b685c60a2c31c2c766dda7da23c2b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>invalid json is produced for get and search if original document contains invalid json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/586</link><project id="" key="" /><description>ES is too lenient in parsing the json in an index operation, and outputs the source faithfully, thereby producing invalid json responses. Elastic Search should either reject invalid json in a source document, or modify the json in the source when outputting so that valid json is always produced. Attemping to parse json in a browser (using JSON.parse) or in in Perl using JSON:XS (see http://search.cpan.org/~drtech/ElasticSearch-0.27/lib/ElasticSearch.pm#get ) will fail with a parse error

At least one problem is that json object names MUST be quoted whereas ES accepts unquoted names.

ES should never produce invalid JSON.

```
# invalid json is accepted to index
curl -XPUT 'http://localhost:9200/hack/test/1 -d '{ foo: "bar" }'
# invalid json and is returned from a get
curl -XGET 'http://localhost:9200/hack/test/1 
{"_index":"hack","_type":"test","_id":"1", "_source" : {foo:"bar"}}
```
</description><key id="500648">586</key><summary>invalid json is produced for get and search if original document contains invalid json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mobz</reporter><labels /><created>2011-01-01T01:34:38Z</created><updated>2014-07-21T15:53:04Z</updated><resolved>2011-01-01T20:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-01T10:09:22Z" id="645250">Basically, ES parses the json, and if it passes parsing, then as far as ES is concerned, its a valid json. The parser ES uses is instructed (it was like that form the start) to allow for unquoted field names in the json, I am not sure I can change it now. 

The idea I had for allowing for unquoted json structures is the benefit in it both in parsing speed and json size. It might have been a mistake to allow for that (by default), but I am not sure I can change it now...
</comment><comment author="mobz" created="2011-01-01T10:19:07Z" id="645252">if you accept invalid json at input, but then modify it to be valid before it is output, I would not think that would break any applications. any parser should accept { foo: "bar" } or { "foo" : "bar" }, unless it is non-conforming in the first place.

My client app is an ES browser, and will be run against clusters where the strictness of the input is not guaranteed. I can not use a non-conforming json parser (eg eval) because of security considerations.
</comment><comment author="kimchy" created="2011-01-01T10:30:43Z" id="645256">The process of regenerating a json based on the input is not something that ES is going to do because of performance implications.
</comment><comment author="mobz" created="2011-01-01T12:32:07Z" id="645338">fair enough, we'll just have to work around it.
</comment><comment author="kimchy" created="2011-01-01T13:15:06Z" id="645349">I think that its such an uncommon case, that you don't need specifically handle it.
</comment><comment author="oaubert" created="2014-07-21T15:53:04Z" id="49624343">This is in fact not such an uncommon case when testing things. And the https://github.com/elasticsearch/kibana/issues/1088 issue illustrates the kind of side effects this can have: a mostly working kibana dashboard, except for the table component (because it requires the source). While I can agree on the performance issues that would be incurred by regenerating correct data, I +1 the first part of the suggestion (making the parser more strict, and just refuse incorrect JSON, instead of polluting the database with invalid JSON values that will bite at unexpected places).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Allow to highlight on fields without term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/585</link><project id="" key="" /><description>Currently, in order to highlight fields, `term_vector` needs to be set to `with_positions_offsets`. Using the simple highlighter in lucene, even fields without the mapping can be highlighted (and not required to be stored as well, since the content is automatically extracted from source if needed).

Note, this is a slower highlighting process, with bigger memory requirements during highlighting. The benefit of using it is the fact that the index size can be smaller.

Also, only a single `pre_tag` and `post_tag` are supported.
</description><key id="500261">585</key><summary>Highlighting: Allow to highlight on fields without term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.2</label><label>v0.15.0</label></labels><created>2010-12-31T16:55:39Z</created><updated>2010-12-31T16:57:42Z</updated><resolved>2010-12-31T16:57:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-31T16:57:42Z" id="644467">Highlighting: Allow to highlight on fields without term vectors, closed by e6b4834768ae949992629d8f531efc240718e14b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master based operations (create index, delete index) to automatically retry on retryable cluster blocks (like recovery from gateway)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/584</link><project id="" key="" /><description>Master based operations (create index, delete index) to automatically retry on retryable cluster blocks (like recovery from gateway)
</description><key id="500064">584</key><summary>Master based operations (create index, delete index) to automatically retry on retryable cluster blocks (like recovery from gateway)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2010-12-31T12:09:12Z</created><updated>2011-01-23T12:22:19Z</updated><resolved>2011-01-23T12:22:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-23T12:22:19Z" id="700299">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facet: Performance and memory improvements when faceting numeric fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/583</link><project id="" key="" /><description>Currently, handling terms facet on numeric fields treats them like strings. Specific numeric type based terms facet handling improves performance (2-3x, over a 1 million hits search) and memory allocation.
</description><key id="499074">583</key><summary>Terms Facet: Performance and memory improvements when faceting numeric fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2010-12-30T17:45:01Z</created><updated>2010-12-30T17:45:57Z</updated><resolved>2010-12-30T17:45:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-30T17:45:57Z" id="642964">Terms Facet: Performance and memory improvements when faceting numeric fields, closed by 82298d890c3549c9d62ed8cec39eeb068bc7b7b0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation suggests deprecated (?) cloud plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/582</link><project id="" key="" /><description>Minor issue but tripped me up as a newbie - the mention of a "Cloud Plugin" in the documentation menu - http://screencast.com/t/sNJO3vB50c - had me trying

```
 ./bin/plugin -install cloud
```

Leading to

```
-&gt; Installing cloud Failed to install cloud, reason: Can't get http://elasticsearch.googlecode.com/svn/plugins/cloud/elasticsearch-cloud-0.13.1.zip
```

...plus further confusion looking at http://elasticsearch.googlecode.com/svn/plugins/cloud/

Eventually figured out I wanted;

```
./bin/plugin -install cloud-aws
```
</description><key id="498826">582</key><summary>Documentation suggests deprecated (?) cloud plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harryf</reporter><labels /><created>2010-12-30T14:55:14Z</created><updated>2010-12-31T09:25:33Z</updated><resolved>2010-12-31T09:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-31T09:25:33Z" id="644031">Yea, there used to be a cloud plugin, but then renamed to be a specific one (cloud-aws). I have pushed a change to the right side menu to not call it cloud plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy Plugin not loaded (groovy not enabled as pluggable script provider)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/581</link><project id="" key="" /><description>Groovy Plugin not loaded (groovy not enabled as pluggable script provider)
</description><key id="498783">581</key><summary>Groovy Plugin not loaded (groovy not enabled as pluggable script provider)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2010-12-30T14:20:47Z</created><updated>2010-12-30T14:21:18Z</updated><resolved>2010-12-30T14:21:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-30T14:21:18Z" id="642602">Groovy Plugin not loaded (groovy not enabled as pluggable script provider), closed by a450deee5f4d7b0126b4f25cbf5de16e0d53f1d6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete By Query: Types are not serialized over network, causing them to be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/580</link><project id="" key="" /><description>This mainly applied to the Java transport client, as usually the `DeleteByQueryRequest` does not get serialized.
</description><key id="498630">580</key><summary>Delete By Query: Types are not serialized over network, causing them to be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.15.0</label></labels><created>2010-12-30T12:30:58Z</created><updated>2010-12-30T12:43:33Z</updated><resolved>2010-12-30T12:43:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-30T12:43:33Z" id="642483">Delete By Query: Types are not serialized over network, causing them to be ignored, closed by fea93b7fea385810eec918f57d338099d82ee2b0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Non-data master nodes and non-master data nodes fail to store data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/579</link><project id="" key="" /><description>A non-master data node can successfully join a cluster, but fails on index creation.
A non-data master is unable to store state between restarts.

The local gateway init seems to have a bug here:
gateway/local/LocalGateway.java: lazyInitialize():
        if (!clusterService.localNode().masterNode() || !clusterService.localNode().dataNode())
==&gt;
        if (!clusterService.localNode().masterNode() &amp;&amp;!clusterService.localNode().dataNode()) 

The second change involves more settings, and I am unsure what other implications it may have (and if things need to change elsewhere).  But at least this if statement cause problems:

env/NodeEnvironment.java: NodeEnvironment constructor:
if (!settings.getAsBoolean("node.data", true) || settings.getAsBoolean("node.client", false) ||
 !settings.getAsBoolean("node.master", true)) {  

Should this just be  if(settings.getAsBoolean("node.client", false))  instead  ?
</description><key id="498235">579</key><summary>Non-data master nodes and non-master data nodes fail to store data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">theborg</reporter><labels><label>bug</label><label>v0.14.2</label><label>v0.15.0</label></labels><created>2010-12-30T03:29:34Z</created><updated>2010-12-30T10:10:54Z</updated><resolved>2010-12-30T10:10:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-30T09:37:16Z" id="642227">Nice catch!. Regarding the NodeEnvironment check, it can't be just `node.client`, since `node.master` and `node.data` can be explicitly set, so they need to be checked as well, its simply that the check is wrong. I will write a test case that recreates and this and push a fix.
</comment><comment author="kimchy" created="2010-12-30T10:10:54Z" id="642286">Non-data master nodes and non-master data nodes fail to store data, closed by ed996c3e850b047d52e36e7e865e3f39ccfd7ab6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST Create Index: Not taking JSON index settings into account unless wrapped in settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/578</link><project id="" key="" /><description>Using JSON based settings does not work in 0.14 unless wrapped in `settings`.

This will not work:

```
curl -XPUT 'http://localhost:9200/test/' -d '
{
    "index" : { 
        "number_of_shards" : 3, 
        "number_of_replicas" : 0
    }
}'
```

And this will work:

```
curl -XPUT 'http://localhost:9200/test/' -d '
{
    "settings" : {
        "index" : { 
            "number_of_shards" : 3, 
            "number_of_replicas" : 0
        }
    }
}'
```

This bug happens as a result of the feature in 0.14 adding the ability to specify mappings in the REST index create request.

Yaml based settings still work.
</description><key id="496502">578</key><summary>REST Create Index: Not taking JSON index settings into account unless wrapped in settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.1</label><label>v0.15.0</label></labels><created>2010-12-29T10:08:57Z</created><updated>2010-12-29T10:26:48Z</updated><resolved>2010-12-29T18:12:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-29T10:12:00Z" id="640180">Fixed.
</comment><comment author="lukas-vlcek" created="2010-12-29T10:26:48Z" id="640213">Also the following works fine too:

```
curl -XPUT 'http://localhost:9200/test/' -d ' {
    settings : { number_of_shards : 3, number_of_replicas : 0 }
}
```

Note: you do not have to explicitly specify "index" section inside "settings". See #541
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory Index Store: Separate to two buffer size types, and fix bugs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/577</link><project id="" key="" /><description>Fix bugs associated with the memory index store (LUCENE issue: https://issues.apache.org/jira/browse/LUCENE-2292, all tests now pass with it).

Also, separate the buffer cache into two buffer types, small and large. small will be used for files known to be small, and large for all the rest.

The new settings for the cache are:
- `cache.memory.direct`: Should the memory be allocated outside of the JVM heap. Defaults to `true`.
- `cache.memory.small_buffer_size`: The small buffer size, defaults to `1kb`.
- `cache.memory.large_buffer_size`: The large buffer size, defaults to `1mb`.
- `cache.memory.small_cache_size`: The small cache size, defaults to `10mb`.
- `cache.memory.large_cache_size`: The large cache size, defaults to `500mb`.

Note, the cache is global on the node level, and all shards allocated (each with its own lucene Directory) share the same cache.
</description><key id="490662">577</key><summary>Memory Index Store: Separate to two buffer size types, and fix bugs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>bug</label><label>feature</label><label>v0.14.0</label></labels><created>2010-12-23T21:20:53Z</created><updated>2010-12-23T21:21:47Z</updated><resolved>2010-12-23T21:21:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-23T21:21:47Z" id="632850">Memory Index Store: Separate to two buffer size types, and fix bugs, closed by f2eae5b6057d562f47768eebade3adcc93326f6a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: Increase the default transport.tcp.connect_timeout from 1s to 30s, also, add `network.tcp.connect_timeout` to conform with other common network settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/576</link><project id="" key="" /><description>Transport: Increase the default transport.tcp.connect_timeout from 1s to 30s, also, add `network.tcp.connect_timeout` to conform with other common network settings
</description><key id="489916">576</key><summary>Transport: Increase the default transport.tcp.connect_timeout from 1s to 30s, also, add `network.tcp.connect_timeout` to conform with other common network settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-23T12:19:10Z</created><updated>2010-12-23T12:20:21Z</updated><resolved>2010-12-23T12:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-23T12:20:21Z" id="631954">Transport: Increase the default transport.tcp.connect_timeout from 1s to 30s, also add `network.tcp.connect_timeout` to conform with other common network settings, closed by 6dcc04b59c7011c3661fd412aef82e6c94e069b1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Excessive mapping parsing when cluster state changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/575</link><project id="" key="" /><description>The generation of mappings source based on the parsed mapping can be different and cause the detection of different mappings to be of, causing excessive mapping parsing when cluster state changes.
</description><key id="489661">575</key><summary>Excessive mapping parsing when cluster state changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-12-23T07:42:09Z</created><updated>2010-12-23T07:53:16Z</updated><resolved>2010-12-23T07:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-23T07:53:16Z" id="631632">Excessive mapping parsing when cluster state changes, closed by 5ac42f2a4f098c66bfc1452960b4b5f3d90c7eac.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud AWS: Change endpoint parameters to distinguish between ec2 and s3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/574</link><project id="" key="" /><description>Currently, the same endpoint parameter (`cloud.aws.endpoint`) is used for both ec2 and s3, which is wrong, since both have different endpoints for the same region.

Rename them to an ec2 endpoint (`cloud.aws.ec2.endpoint`), and s2 endpoint (`cloud.aws.s3.endpoint`). The list of endpoints can be found here: http://aws.amazon.com/articles/3912.

In order to further simplify the settings and have one region setting that will automatically set ec2 and s3 endpoints, as well as automatically set the region constraint when creating a bucket on s3, the `cloud.aws.region` setting can be set with the following values: `us-east-1`, `us-west-1`, `ap-southeast-1`, `eu-west-1`.
</description><key id="489643">574</key><summary>Cloud AWS: Change endpoint parameters to distinguish between ec2 and s3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-23T07:13:06Z</created><updated>2010-12-23T07:13:55Z</updated><resolved>2010-12-23T07:13:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-23T07:13:55Z" id="631593">Cloud AWS: Change endpoint parameters to distinguish between ec2 and s3, closed by 9361e3bd2b94d2d4f6265247ba7ac23d554f2bf5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>non-exist filed could cause empty search result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/573</link><project id="" key="" /><description>missing all field when specified a non-exist filed,and we should ignore the bad guy.

and if i specify the  fields={} ,the result may looks strange:

query:http://localhost:9200/app10000/v1/_search?q=%2bage%3a%5b1+TO+30%5d&amp;fields={}&amp;from=0&amp;size=5

result:
{
    "took": 15,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 1,
        "hits": [
            {
                "_index": "app10000",
                "_type": "v1",
                "_id": "key2",
                "_score": 1,
                "fields": {
                    "{}": {
                        "age": 23
                    }
                }
            },
            {
                "_index": "app10000",
                "_type": "v1",
                "_id": "key1",
                "_score": 1,
                "fields": {
                    "{}": {
                        "age": 22
                    }
                }
            }
        ]
    }
}

the  weird place:
 "fields": {
                    "{}": {
                        "age": 23
                    }
                }
</description><key id="485900">573</key><summary>non-exist filed could cause empty search result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2010-12-21T10:23:44Z</created><updated>2011-04-20T05:11:20Z</updated><resolved>2011-04-20T05:11:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-21T11:03:05Z" id="626918">this happens because of the fact that they are now fetched from source. Improve the logic there, so now it should ignore fields that have no mappings / can't be fetched from source.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Allow to pass `&amp;fields=` without any fields to cause only id and type to be returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/572</link><project id="" key="" /><description>When using the search body API, an empty array will cause no fields to be loaded. When using the URI option, handle an empty `fields` parameter to cause no source to be loaded as well.
</description><key id="485875">572</key><summary>Search API: Allow to pass `&amp;fields=` without any fields to cause only id and type to be returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-21T10:01:07Z</created><updated>2010-12-21T10:02:03Z</updated><resolved>2010-12-21T10:02:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-21T10:02:03Z" id="626820">Search API: Allow to pass `&amp;amp;fields=` without any fields to cause only id and type to be returned, closed by 303525488534259054fbf014489406dbd8f3ddf6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>problem about getting field: _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/571</link><project id="" key="" /><description>i have a scenario to fetch only the document's id,and 
in previous version,i just write the query like this:
http://localhost:9200/app10000/v1/_search?q=%2bage%3a%5b1+TO+30%5d&amp;fields=_id&amp;from=0&amp;size=5
and that works,
and now in 0.14,something seems to have changed,

and it tells:

{
    "took": 20,
    "_shards": {
        "total": 5,
        "successful": 3,
        "failed": 2,
        "failures": [
            {
                "reason": "PropertyAccessException[[Error: could not access: _id; in class: org.elasticsearch.search.lookup.SourceLookup]\n[Near : {... Unknown ....}]\n             ^\n[Line: 1, Column: 0]]"
            },
            {
                "reason": "PropertyAccessException[[Error: could not access: _id; in class: org.elasticsearch.search.lookup.SourceLookup]\n[Near : {... Unknown ....}]\n             ^\n[Line: 1, Column: 0]]"
            }
        ]
    },
    "hits": {
        "total": 2,
        "max_score": 1,
        "hits": []
    }
}
</description><key id="485832">571</key><summary>problem about getting field: _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2010-12-21T09:29:20Z</created><updated>2011-04-20T05:11:40Z</updated><resolved>2011-04-20T05:11:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>New feature: Ability to configure terms facet for returning term values with zero count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/570</link><project id="" key="" /><description>I'd like to create TermsFacet which will return all possible terms
(just like global facet), but numbers should be taken from current
query/filtering (just like in non-global facet).
In other words: my facet should work just like usual non-global
TermsFacet, but in results there should be always all possible terms
for particular field (probably most of them with count 0).
That's only for category purposes (not analyzed text field) so I think
it'd be usually less than 10 terms.

That kind of facet is needed to implement an object browser with facet-based filtering.
When somebody restricts search to objects of one category, then other
categories should be returned also for him to change his selection.

Additionally it would be perfect to have a possibility of defining two
types of filters:
strong filters - one kind of filter that always restricts browsed
objects (user isn't allowed to see these objects at all)
weak filters - second kind of filter that limits browsed objects (user
selects a filter to limit shown objects).

Then facet values should be calculated that way:
1. documents in index are restricted by strong filters
2. check what terms exists in those set of documents - they will be
facet values (terms) to return
3. restrict documents by weak filters
4. calculate document counts for terms from 2. and return all those
values (even zeros) 

Issue created because of thread:
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/39a45b70516a68a9/
</description><key id="484071">570</key><summary>New feature: Ability to configure terms facet for returning term values with zero count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Avatah</reporter><labels /><created>2010-12-20T12:47:37Z</created><updated>2011-01-24T18:43:58Z</updated><resolved>2011-01-25T00:35:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-23T14:10:06Z" id="700413">pushed #647, which allows to get all the terms back.
</comment><comment author="Avatah" created="2011-01-24T16:35:04Z" id="703026">Great. What about second functionality (strong and weak filters)?
</comment><comment author="kimchy" created="2011-01-24T18:43:58Z" id="703480">This is more problematic to implement, you can get to this behavior using global facets though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Specialized HistogramFacet, DateHisogram is needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/569</link><project id="" key="" /><description>Users have run into confusion while working with the HistogramFacet and using dates.  When the HistogramFacet creates its buckets, it uses the internal UTC representation of the date, which can cause issues for users which need to support other timezones.  Please add a customized HisogramFacet which allows the user to specify the timezone to use for calculations.
</description><key id="484044">569</key><summary>Specialized HistogramFacet, DateHisogram is needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkroll</reporter><labels /><created>2010-12-20T12:35:04Z</created><updated>2011-01-02T13:44:45Z</updated><resolved>2011-01-02T13:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rkroll" created="2011-01-02T13:44:45Z" id="646445">This has been fixed in 591
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Search API: Passing a the query as json fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/568</link><project id="" key="" /><description>The search API allows to pass the query itself to execute in json/binary format (and not using the builders). This fails when combined with smile.
</description><key id="483806">568</key><summary>Java Search API: Passing a the query as json fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-12-20T09:30:43Z</created><updated>2010-12-20T09:32:29Z</updated><resolved>2010-12-20T09:32:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-20T09:32:29Z" id="623949">Java Search API: Passing a the query as json fails, closed by efc95dc00a8844d9e86427a9d36b9a7b16b55a42.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable calling mlockall by default, can be enabled by setting bootstrap.mlockall to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/567</link><project id="" key="" /><description>Using mlockall tries to disable swapping (see #464). This seems to cause ES to freeze on some systems when starting up. Disable it by default.
</description><key id="482265">567</key><summary>Disable calling mlockall by default, can be enabled by setting bootstrap.mlockall to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-19T00:30:02Z</created><updated>2010-12-19T00:31:05Z</updated><resolved>2010-12-19T00:31:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-19T00:31:05Z" id="621805">Disable calling mlockall by default, can be enabled by setting bootstrap.mlockall to `true`, closed by 96a2950ab5136d3e39d33eb510de438eca0839d2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add a callback/event for connection status changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/566</link><project id="" key="" /><description>this is a an enhancement request. 

currently the only way to monitor a node status is using a scheduler calling the cluster health API. it would be nice to be able to define a callback for node connection state changes.

see http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/70d6731b1a643896/d647e10ff842370a
</description><key id="481185">566</key><summary>add a callback/event for connection status changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tomerd</reporter><labels /><created>2010-12-18T02:38:16Z</created><updated>2014-07-08T11:55:33Z</updated><resolved>2014-07-08T11:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T11:55:33Z" id="48326557">No discussion for 4 years. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java TransportClient: By default, don't sniff other nodes and use addresses provided as is</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/565</link><project id="" key="" /><description>By default, don't sniff other data nodes in the cluster, and use the addresses provided as is (important when working outside of the cloud, using the public address).

Add `client.transport.sniff` which can be set to `true` to enable sniffing and using all `data` nodes in the cluster. This is how it can be set:

```
TransportClient client = new TransportClient(settingsBuilder().put("client.transport.sniff", true).build());
```
</description><key id="480481">565</key><summary>Java TransportClient: By default, don't sniff other nodes and use addresses provided as is</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-17T17:30:55Z</created><updated>2010-12-17T17:31:51Z</updated><resolved>2010-12-17T17:31:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-17T17:31:51Z" id="619153">Java TransportClient: By default, don&amp;#39;t sniff other nodes and use addresses provided as is, closed by 415bb5d7f3c745f331be60aa810fd1f92fcc16eb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tcp Transport: Connection pool between nodes and different connection types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/564</link><project id="" key="" /><description>Some messages sent by one node to another requires high priority (such as the ping messages to detect liveness). Others require lower priority since they include large payloads (recovery, bulk), and the rest are, well, the rest.

Introduce connection pooling between one node and another, with `high`, `low`, and `med`. By default, `high` is set to 1, `med` is set to `7`, and low is set to `2`.

This mainly comes from the problem of sending big bulk requests on the same channel causing other messages to get delayed.
</description><key id="476399">564</key><summary>Tcp Transport: Connection pool between nodes and different connection types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-15T18:04:56Z</created><updated>2010-12-15T18:11:33Z</updated><resolved>2010-12-15T18:11:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-15T18:11:33Z" id="614131">Tcp Transport: Connection pool between nodes and different connection types, closed by d6bab1a892cbfdfaa46d8d0657ac0028255866b2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery: Allow to control `concurrent_streams` (per node) during recovery </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/563</link><project id="" key="" /><description>Currently, the `cluster.routing.allocation.concurrent_recoveries` allows to set the concurrent recoveries of _shards_ per node.

Each recovery of a shard consists of copying over the index files. Currently, all are copied over concurrently. The new `index.shard.recovery.concurrent_streams` allows to set the number of concurrent streams (index files) that can be recovered from a node (globally on the node level). It defaults to `5`.
</description><key id="470796">563</key><summary>Recovery: Allow to control `concurrent_streams` (per node) during recovery </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-12T22:00:55Z</created><updated>2011-04-10T07:18:13Z</updated><resolved>2011-04-10T07:18:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-10T07:18:13Z" id="979175">Already implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Fields: If a field is not stored, automatically extract it from _source (without the need for _source prefix)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/562</link><project id="" key="" /><description>There is an option to load specific fragments from `_source`, using it as a prefix in the search request. But, it can be automatically detected if the field is not stored, or a (json) object is referenced.
</description><key id="469894">562</key><summary>Search Fields: If a field is not stored, automatically extract it from _source (without the need for _source prefix)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-12T02:52:05Z</created><updated>2010-12-12T03:00:46Z</updated><resolved>2010-12-12T03:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-12T03:00:46Z" id="605809">Search Fields: If a field is not stored, automatically extract it from _source (without the need for _source prefix), closed by bc04243a2b60c8ee3309f232a24e4c8ba7368a65.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Automatically use the field values extracted from _source if not stored explicitly in the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/561</link><project id="" key="" /><description>The current need to have the field stored in order to be highlighted (term_vector is still needed) can be wasteful, especially when the `_source` is also stored.

When a specific field requested is not stored in the mapping, it will be automatically extracted from the `_source` (the source wil be loaded and parsed) and used for highlighting.

Note, this holds for cases where an array type is "in the middle" of the expression. For example: `attachments.body` where `attachments` is a an array.
</description><key id="469880">561</key><summary>Highlighting: Automatically use the field values extracted from _source if not stored explicitly in the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.0</label></labels><created>2010-12-12T02:39:49Z</created><updated>2010-12-12T02:40:46Z</updated><resolved>2010-12-12T02:40:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-12T02:40:46Z" id="605794">Highlighting: Automatically use the field values extracted from _source if not stored explicitly in the mapping, closed by 216b2ab91256ed6bc6ec513249e16b30716095e4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close API might cause index data to be wiped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/560</link><project id="" key="" /><description>The close API might cause the index data for that index to be wiped.
</description><key id="468177">560</key><summary>Close API might cause index data to be wiped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-12-10T20:19:05Z</created><updated>2010-12-10T20:20:06Z</updated><resolved>2010-12-10T20:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-10T20:20:06Z" id="603559">Close API might cause index data to be wiped, closed by a914865c4506468f8a9ffee7a6f10400beef3ecf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo search fails to find some docs when wrapping over IDL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/559</link><project id="" key="" /><description>It seems that the geo_bounding_box query fails to find some documents when the box crosses over the IDL (international date line). To reproduce (in 0.13.1):

```
curl -XPUT 'http://localhost:9200/test' -d'{ index : { number_of_shards : 1, number_of_replicas : 0 } }';

curl -XPUT 'http://localhost:9200/test/place/_mapping' -d'
{
    "place" : {
        "_all" : {"enabled" : false},
        "properties" : {
            "user_id" : {"type" : "integer", "index" : "not_analyzed"},
            "title" : {"type" : "string", "boost" : 1.5, "analyzer" : "eulang"},
            "notes" : {"type" : "string", "analyzer" : "eulang"},
            "tags" : {"type" : "string", "index_name" : "tag", "boost" : 1.5, "analyzer" : "eulang"},
            "created_on" : {"type" : "date", "format" : "YYYY-MM-DD HH:mm:ss"},
            "geo" : {"type" : "geo_point"},
            "privacy" : {"type" : "integer", "index" : "not_analyzed"}
        }
    }
}'

curl http://localhost:9200/test/place/10 -d'{"user_id": 880, "privacy": 2000, "notes": "", "tags": [], "created_on": "2010-12-09 08:05:17", "title": "Place in Stockholm", "geo": {"lat": 59.328355000000002, "lon": 18.036842}}';

curl http://localhost:9200/test/place/20 -d'{"user_id": 534, "privacy": 2000, "notes": "", "tags": [], "created_on": "2010-11-26 10:22:38", "title": "Place in Montr&#233;al", "geo": {"lat": 45.509526999999999, "lon": -73.570986000000005}}';
```

This fails to find the doc titled "Place in Stockholm":

```
curl 'http://localhost:9200/test/_search' -d'
{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "term": {
                            "user_id": 880
                        }
                    },
                    {
                        "geo_bounding_box": {
                            "geo": {         
                                "bottom_right": {
                                    "lat": -66.668903999999998, 
                                    "lon": 113.96875            
                                },                              
                                "top_left": {                   
                                    "lat": 74.579421999999994,  
                                    "lon": 143.5                
                                }                               
                            }                                   
                        }                                       
                    }                                          
                ]                                              
            },
            "query": {
                "match_all": {}
            }
        }
    }
}'
{"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

But this does find the place title "Place in Montr&#233;al":

```
curl 'http://localhost:9200/test/_search' -d'
{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "term": {
                            "user_id": 534
                        }
                    },
                    {
                        "geo_bounding_box": {
                            "geo": {
                                "bottom_right": {
                                    "lat": -66.668903999999998,
                                    "lon": 113.96875
                                },
                                "top_left": {
                                    "lat": 74.579421999999994,
                                    "lon": 143.5
                                }
                            }
                        }
                    }
                ]
            },
            "query": {
                "match_all": {}
            }
        }
    }
}'
{"_shards":{"total":1,"successful":1,"failed":0},"hits" {"total":1,"max_score":1.0,"hits":[{"_index":"test","_type":"place","_id":"20","_score":1.0, "_source" : {"user_id": 534, "privacy": 2000, "notes": "", "tags": [], "created_on": "2010-11-26 10:22:38", "title": "Place in Montr&#233;al", "geo": {"lat": 45.509526999999999, "lon": -73.570986000000005}}}]}}
```

Both of those places fall within the bounding box, though.
</description><key id="466311">559</key><summary>Geo search fails to find some docs when wrapping over IDL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-12-09T23:46:33Z</created><updated>2010-12-10T06:39:28Z</updated><resolved>2010-12-10T06:39:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-10T06:39:28Z" id="601873">Geo search fails to find some docs when wrapping over IDL, closed by 2a7f964ca23370896032182d6521cffa8d6b6a24.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add restore_index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/558</link><project id="" key="" /><description>Hiya

Sometimes things go wrong and indices get lost - today I had memory issues, and somewhere between restarting nodes and having them timeout while waiting for master (because master was also having memory issues) one of my two indices disappeared.

What I'd like to be able to do is to `cp -a data/ backup/` every night, so that if I do have the situation that one index disappears, I can restore just that index from the backup copy.  

Is this doable?

ta

clint
</description><key id="464921">558</key><summary>Add restore_index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-12-09T11:28:30Z</created><updated>2012-08-04T11:36:26Z</updated><resolved>2012-08-04T11:36:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aromasca" created="2012-02-09T13:46:45Z" id="3887508">+1
This is really something we are also eager for, we would love to be able to backup every 2-4-12Hours all of the data via a backup api or an elastic cron job that would be online so we can restore stuff if anything goes missing.

As for your issue, we have encountered these kinds of indices lost, sometime just closing and opening the index helped recover the data.

Avi
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add terms/in query, alias terms filter to be in filter as well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/557</link><project id="" key="" /><description>Add a `terms` (or `in`) queries, that allows to match on documents containing the either one or more values out of the values provided in the query. Here is an example:

```
{
    "in" : {
        "tags" : [ "blue", "pill" ]
        "minimum_match" : 1
    }
}
```

Also, add an alias to the `terms` filter to `in` as well.
</description><key id="464463">557</key><summary>Add terms/in query, alias terms filter to be in filter as well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.0</label></labels><created>2010-12-09T05:24:33Z</created><updated>2010-12-09T05:25:50Z</updated><resolved>2010-12-09T13:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-09T05:25:19Z" id="599034">Add terms/in query, alias terms filter to be in filter as well, closed by 167d35807c5ce4b8e10228180dcf58b14f3ce826.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mvel parsing for script_fields is wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/556</link><project id="" key="" /><description>1 + 2 \* _source.field is parsed as (1 + 2) \* _source.field instead of 1 + (2 \* _source.field) for example

How to reproduce: see https://gist.github.com/734132
</description><key id="464072">556</key><summary>Mvel parsing for script_fields is wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luddic</reporter><labels /><created>2010-12-08T23:47:07Z</created><updated>2013-04-04T18:30:59Z</updated><resolved>2013-04-04T18:30:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-13T20:16:47Z" id="608907">Yea, seems like a bug in mvel (recreated it simply just with mvel). Pinged the mailing list, will report back on the status.
</comment><comment author="kimchy" created="2010-12-15T22:58:24Z" id="614863">This was fixed in a snapshot build of mvel, once a formal version is out, it will be included.
</comment><comment author="luddic" created="2010-12-15T23:12:33Z" id="614903">Ok thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aliased analyzers cause index deletion / cleanup failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/555</link><project id="" key="" /><description>When using aliased analyzers, they can cause index deletion / cleanup to fail.
</description><key id="463280">555</key><summary>Aliased analyzers cause index deletion / cleanup failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-12-08T17:29:59Z</created><updated>2010-12-08T17:30:56Z</updated><resolved>2010-12-08T17:30:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-08T17:30:56Z" id="597482">Aliased analyzers cause index deletion / cleanup failure, closed by 34f3f3f79e6ca260b9bb93d8ba0f7fccebb29429.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>System hanged up if I run elasticsearch.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/554</link><project id="" key="" /><description>I am using virtual machine VMware.
OS: Debian lenny.
ElasticSearch Version: 0.13.1
I've successfully tested Cassandra, Hbase, Mongo and MySQL using this miachine.
But elasticsearch hang up machine immediately and I cannot to connect to it by SSH.
I've tried to run it on Windows. All is OK.
</description><key id="462767">554</key><summary>System hanged up if I run elasticsearch.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trurl123</reporter><labels /><created>2010-12-08T12:49:29Z</created><updated>2010-12-09T15:05:26Z</updated><resolved>2010-12-09T15:05:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-08T16:53:14Z" id="597385">Strange... . Can you start elasticsearch with: `elasticsearch -Dbootstrap.mlockall=false`, maybe that would help. Is there a chance to start maybe 0.12 or 0.11, and see if it still happens.

Also, start it with logging set to TRACE, lets see what happens.
</comment><comment author="trurl123" created="2010-12-09T14:28:33Z" id="599906">Versions 0.11 and 0.12 work fine. Version 0.13 eats all memory and "-Dbootstrap.mlockall=false" does not help.
</comment><comment author="trurl123" created="2010-12-09T15:05:26Z" id="600000">I used -Xmx key and it helps me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent / Child Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/553</link><project id="" key="" /><description>The parent/child documents support allows to define a parent relationship from a child type to a parent type. 
## Mapping

The relationship is defined using a simple mapping definition at the child level mapping. For example, in case of a `blog` type and a `blog_tag` type child document, the mapping for `blog_tag` should be:

```
{
    "blog_tag" : {
        "_parent" : {
            "type" : "blog"
        }
    }
}
```

The above defines a parent mapping, and the type of the parent.
## Indexing

When indexing a child document, it is important that it will be routed to the same shard as the parent. This uses the routing capability. When indexing a doc with a parent id, it is automatically set as the routing value (unless the routing value is explicitly defined). Indexing a document with a parent id is simple:

```
curl -XPUT localhost:9200/blogs/blog_tag/1122?parent=1111 -d '
{
    "tag" : "something"
}
'
```

There is an option to set `_parent` in each bulk index item as well.
## Querying

There are several mechanisms to query child documents.  The idea of child filter / query is that its inner query is run against the child documents, and the result of it are parent docs matching those child documents.

The way it is implemented is that the child queries are first run on their own, with the results "joining" the parent documents. Then, the main query runs with the results of the child query, which includes the parent docs.
# `has_child`

The first is the `has_child` filter and `has_child` query (which is a simple `constant_score` query wrapping the `has_child` filter):

```
{
    "has_child" : {
        "type" : "blog_tag"
        "query" : {
            "term" : {
                "tag" : "something"
            }
        }
    }
}
```

The `type` is the child type to query against. The parent type to return is automatically detected based on the mappings.

The query (and filter), do no scoring, and the "join" process of matching which parent doc the child doc matches is done _on each matching child doc_.
# `top_children`

The `top_children` query basically runs the child query with an estimated hits size, and out of this hit docs, aggregates it into parent docs. If there aren't enough parent docs matching the requested from/size search request, then it is run again with a wider (more hits) search.

The `top_children` also provide scoring capabilities, with the ability to specify `max`, `sum` or `avg` as the `score` type.

One downside of using the `top_children` is that if there are more child docs matching the required hits when executing the child query, then the `total_hits` result of the search response will be incorrect.

How many hits are asked for in the first child query run is controlled using the `factor` parameter (defaults to `5`). For example, when asking for 10 docs with from 0, then the child query will execute with 50 hits expected. If not enough parents are found (in our example, 10), and there are still more child docs to query, then the search hits are expanded my multiplying by the `incremental_factor` (defaults to `2`).

The required parameters are the `query` and `type` (the child type to execute the query on). Here is an example with all different parameters, including the default values:

```
{
    "top_children" : {
        "type": "blog_tag",
        "query" : {
            "term" : {
                "tag" : "something"
            }
        }
        "score" : "max",
        "factor" : 5,
        "incremental_factor" : 2
    }
}
```
## Faceting

Faceting on the child query phase (on the results of the query executed) can be done by specifying a `scope` with a custom name in the query / filter. All facets now accept a `scope` to run on (similar to global set to `true`), and can now be executed on docs matching the child query.
## Query Performance

In general, the `top_children` performance will be much better than the `has_child` performance. This is because joining the child to its parent is done in the `top_children` case against the expected number of hits returned, while in the `has_child` case, it is executed against _all_ child docs matching the child query.
## Memory Considerations

With the current implementation, all `_id` values are loaded to memory (heap) in order to support fast lookups, so make sure there is enough mem for it.
</description><key id="461696">553</key><summary>Parent / Child Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.0</label></labels><created>2010-12-07T22:15:20Z</created><updated>2015-06-29T12:41:24Z</updated><resolved>2010-12-08T06:17:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-07T22:17:01Z" id="595398">Parent / Child Support, closed by 54437c1bd3125c1e5cddbd1cad4a26fca03c002b.
</comment><comment author="medcl" created="2010-12-14T09:53:13Z" id="610295">this feature really rocks!
</comment><comment author="apatrida" created="2010-12-15T16:18:48Z" id="613854">A few months back, I found this interesting solution to parent/child documents that can be indexed at the same time...  (notes that follow were from that time, it has progressed I am sure since)

http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene

And implemented a base version here:

https://issues.apache.org/jira/browse/LUCENE-2454

It does look to have the problems mentioned in the JIRA issue:
- Doesn't work well when parent/child docs cross index segment
- Consumes more bits in things that track a bit per doc in the index (i.e. filters/facets)
- Should consider rolling child docs into the returned parent
- Probably messes with IDF and scoring
</comment><comment author="gpstathis" created="2012-02-04T00:46:58Z" id="3807079">Is there a way to disable the expected number of hits returned by the child query in `top_children`? Basically having it executed against all child docs matching the child query like `has_child` with the understanding that performance would indeed suffer? Or does one just have to estimate a really large `factor`? 
</comment><comment author="doublebyte1" created="2015-06-29T12:16:29Z" id="116631480">I assume this is a bug on ES documentation, since the parent on the bulk request is set with "parent", rather than "_parent": https://www.elastic.co/guide/en/elasticsearch/guide/current/indexing-parent-child.html (sorry to post it here, I don't know where else to put it).
</comment><comment author="clintongormley" created="2015-06-29T12:41:24Z" id="116642836">@doublebyte actually both forms are accepted
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregate store_size of index alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/552</link><project id="" key="" /><description>It would be nice if foo/_status returned a store_size which is the sum of all the store_sizes pointed to by the foo alias.
</description><key id="459820">552</key><summary>Aggregate store_size of index alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2010-12-07T04:40:11Z</created><updated>2014-07-08T11:55:04Z</updated><resolved>2014-07-08T11:55:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T11:55:04Z" id="48326532">Returned by the `_all` key in stats. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>EC2 Discover: Support filtering instances by tags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/551</link><project id="" key="" /><description>Discovering based upon security group is a good start, but discovery should optionally be dependent upon instance tags.

This is important for keeping staging, production, etc. environments separate (where security groups are used for functional division).

The settings to set it use the `discovery.ec2.tag.` prefix. For example, setting `discovery.ec2.tag.stage` to `dev`, will only filter instances with a tag with key set to `stage` and its value set to `dev`.
</description><key id="458923">551</key><summary>EC2 Discover: Support filtering instances by tags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mLewisLogic</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-06T18:35:15Z</created><updated>2010-12-16T04:45:10Z</updated><resolved>2010-12-16T11:15:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-16T03:14:32Z" id="615312">Heya, just added some data into the issue, and will push it soon. Would be great if you can test drive master.
</comment><comment author="kimchy" created="2010-12-16T03:15:07Z" id="615315">EC2 Discover: Support filtering instances by tags, closed by 698f67a31aa02fe0ce6f2375b8a6d3f30d0a7b0f.
</comment><comment author="mLewisLogic" created="2010-12-16T04:45:10Z" id="615436">Not coding this week or next. I'll check it out after the 27th.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicate results in from: size:</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/550</link><project id="" key="" /><description>I'm not sure if i'm doing something wrong here, but on something like the following...(elasticsearch 0.13)

{ "from":0,"size" :10, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"mp3_"}}}
{ "from":10,"size" :10, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"mp3_"}}}
{ "from":20,"size" :10, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"mp3*"}}}

I occasionally get a duplicate result back[seems to be towards the top of the result set... anywhere from 1-4 records in).  (Happens if I specify an _id on index creation or let elastic search generate one for me.)  A duplicate result = _id being the same, as well as my _source.recid being the same. If I use a large "size" parameter then I don't get the duplicate results.  Is there something i'm misunderstanding?  If i happen to check for duplicates manually I never appear to get the proper unique entities. (ie... elasticsearch returns a report 259 records, but if I get one duplicate entry I will only get 258.

But again, if I do specify a large enough size so that the result set all fits in, then there is no problem...  I am sure that when adding to the search that everything is unique.

Tnx!
</description><key id="458448">550</key><summary>Duplicate results in from: size:</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jaydee2190</reporter><labels /><created>2010-12-06T15:12:29Z</created><updated>2013-04-04T18:32:14Z</updated><resolved>2013-04-04T18:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Jaydee2190" created="2010-12-06T15:14:13Z" id="591534">(That should be two queries being displayed above!)
</comment><comment author="kimchy" created="2010-12-06T16:26:16Z" id="591702">I am not sure I understand, do you get duplicate results when you issue a single query, or do you get duplicate results when you issue the 3 above queries (i.e. you get id 1122 when for the result of from : 0 to : 10, but also into from :10 and to 20)?
</comment><comment author="Jaydee2190" created="2010-12-06T16:29:30Z" id="591709">Single queries are fine.  So yes, duplicate id's (very few, but consistent), when doing different from: size
</comment><comment author="kimchy" created="2010-12-06T16:30:38Z" id="591711">Do you index while you search? This might happen as things might change, between one query to the other.
</comment><comment author="Jaydee2190" created="2010-12-06T16:45:27Z" id="591775">No, indexing is closed...  depending on my size: the duplicate record changes...

This might be related though, i'm sorting by my recid...

results with large enough size: although the record's aren't sorted properly...(close though)

{ "from":0,"size" :300, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"mp3*"}}}
{"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":259,"max_score":1.0,"hits":[{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"99d6db196ef437877453f9a00ff5180f","_score":1.0,"fields":{"_source.recid":5}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"57f29593e2a89bbd22dc557c0c74cd20","_score":1.0,"fields":{"_source.recid":4}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"dd626fe16dbb67637a919fbd8cfb5134","_score":1.0,"fields":{"_source.recid":11}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"0864f79e60cd35f3a54aedf16583ae21","_score":1.0,"fields":{"_source.recid":15}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"4558d60a6af918f35ade70a289e9f6e8","_score":1.0,"fields":{"_source.recid":16}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"7839222fafd865855fa9043ae55ac961","_score":1.0,"fields":{"_source.recid":12}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"1ca15e9c70a1b6bb71f123e471bafba7","_score":1.0,"fields":{"_source.recid":35}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"f82e139f29e3eb6350609881653e6c5f","_score":1.0,"fields":{"_source.recid":18}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"cc668a892a3adc373a3971bdb93503a8","_score":1.0,"fields":{"_source.recid":30}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"952417adc02af3b2a90495d534d0a0fc","_score":1.0,"fields":{"_source.recid":21}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3f1c94be5e5db8fc8efe7f0cb3a6a048","_score":1.0,"fields":{"_source.recid":39}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"960e7d922fe23a317693caf6171d3fe5","_score":1.0,"fields":{"_source.recid":40}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"ef6dde7e550a2b60378f3c00523fadff","_score":1.0,"fields":{"_source.recid":47}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"86a4c32f9db8e4eab4eadc7859c243bf","_score":1.0,"fields":{"_source.recid":53}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"ec5c807c2f94852f8d532b46e1c0e042","_score":1.0,"fields":{"_source.recid":42}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"a51c065741273a1459edadaae2c08d00","_score":1.0,"fields":{"_source.recid":57}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"784aa0fda6cb843c55c1c7d85fb0a630","_score":1.0,"fields":{"_source.recid":45}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"b57fa3a05c70f28372d32466a320f754","_score":1.0,"fields":{"_source.recid":62}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d0058a8b5295e5463fff2dfe4eec5568","_score":1.0,"fields":{"_source.recid":71}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"cd3196f10c0852ccca38b47fc3fda537","_score":1.0,"fields":{"_source.recid":84}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d473f42dd566982fad30460b52b2384d","_score":1.0,"fields":{"_source.recid":72}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"f190d1ac6842576d60b3144cad3d2110","_score":1.0,"fields":{"_source.recid":85}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"6e9d50aab3eadac2fc2ca20e2449d29a","_score":1.0,"fields":{"_source.recid":95}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"faa93d0541e276484cc8a28019d40c07","_score":1.0,"fields":{"_source.recid":98}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"8ea018a249706d2a5f1b53fa3a4f5f43","_score":1.0,"fields":{"_source.recid":143}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"2bb4f7f54b7248a0cf4a525f277dcc49","_score":1.0,"fields":{"_source.recid":119}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"df022680144ac31fd0369f8191e2b649","_score":1.0,"fields":{"_source.recid":131}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"51ddb50cec0b66a2326cae8eee42cefc","_score":1.0,"fields":{"_source.recid":169}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"ea74ae260174925af0da0087e71d3f7f","_score":1.0,"fields":{"_source.recid":135}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"8b4f3d4743c0b0eb944028383992758c","_score":1.0,"fields":{"_source.recid":224}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"7f2e332fb8d9a0927543ff0116a619ef","_score":1.0,"fields":{"_source.recid":200}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"afa5e8084bb3cbf43c34d8df326c3451","_score":1.0,"fields":{"_source.recid":172}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"be6fb10d6d8dc4cc2eacd8022c9a63e7","_score":1.0,"fields":{"_source.recid":261}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"05528534d3556bafd4343ac320e7bb19","_score":1.0,"fields":{"_source.recid":272}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"a0337f78e48f410f2ef31d6ae8a19f7d","_score":1.0,"fields":{"_source.recid":297}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"12373782553fdb3f72bfa45ccac9256f","_score":1.0,"fields":{"_source.recid":206}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3358bf067d3303e95c948fc49c04f1fe","_score":1.0,"fields":{"_source.recid":241}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c3d7f0fa771efa64bcf70af6bde379cf","_score":1.0,"fields":{"_source.recid":185}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"7ed74b32299a10bc05fc1381a92ae8a2","_score":1.0,"fields":{"_source.recid":249}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"22f485636dbf46f5642767136d8a114f","_score":1.0,"fields":{"_source.recid":299}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"8a1104582e9d13c8fa0abdac87ab0a0c","_score":1.0,"fields":{"_source.recid":225}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"eea4be6a3fd1fb64b8abc416f65e1398","_score":1.0,"fields":{"_source.recid":257}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d2417a16ef3fb79c10ea7fbc5f2609b4","_score":1.0,"fields":{"_source.recid":235}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3f40d841bdb4389881d0bf771f50d1b9","_score":1.0,"fields":{"_source.recid":221}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3225ef4cee140cf7fa2650fbba856cac","_score":1.0,"fields":{"_source.recid":323}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"12c19c4249323108af0e6d284e3ab49c","_score":1.0,"fields":{"_source.recid":243}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"9230dad741f2fc255376bd8b8bb884a6","_score":1.0,"fields":{"_source.recid":238}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"f5dcfd10dc9fbe1ec61c3c1557851dcf","_score":1.0,"fields":{"_source.recid":280}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"fb2d7f15a900672688e4642e488a4dc1","_score":1.0,"fields":{"_source.recid":256}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c38359f19fd84dd2ae700c252875f5c8","_score":1.0,"fields":{"_source.recid":348}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c86ad7cec169e4378d4555d41465e4ae","_score":1.0,"fields":{"_source.recid":251}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"16a58ef0ef01abeb5cc4360f98b51ae0","_score":1.0,"fields":{"_source.recid":268}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"4ae02af3799254ec4274cf777da069cc","_score":1.0,"fields":{"_source.recid":356}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"535b0cebae112f8ca68a04d385d147ae","_score":1.0,"fields":{"_source.recid":269}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"a296cb32b25b42e15a53513a3694d410","_score":1.0,"fields":{"_source.recid":281}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3e6b8fb19f78133bfed69ace61cc9941","_score":1.0,"fields":{"_source.recid":315}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"87ee1d7f4683d4aca3eb992f088412f2","_score":1.0,"fields":{"_source.recid":328}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"9fbd5ba2d6bee54d99ee7eb6a6c7d21c","_score":1.0,"fields":{"_source.recid":292}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d465d1692fc9b7d157427f13a8f2f66c","_score":1.0,"fields":{"_source.recid":397}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3a5a5dc94a9d5948321ff8f7a6e0444d","_score":1.0,"fields":{"_source.recid":289}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c24a0bc696e6fdfcff4014b3ab1f9c0b","_score":1.0,"fields":{"_source.recid":298}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"97cac7eb3694190cc3f75ba60c3726c1","_score":1.0,"fields":{"_source.recid":404}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"9888db4958fb50ae7f050ad0fd3d9f47","_score":1.0,"fields":{"_source.recid":300}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c891b2799bd0044bdf33994129187a2f","_score":1.0,"fields":{"_source.recid":307}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"dbd401302bbac9a4377ff9e7fa684f74","_score":1.0,"fields":{"_source.recid":316}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"7224fcffb9401b2751b054fe0f1313f2","_score":1.0,"fields":{"_source.recid":318}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"b0bfdd7f8e3271c10928b13a91cd9ea3","_score":1.0,"fields":{"_source.recid":374}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d60a1ea1b9ad57e61b4fda4508406a90","_score":1.0,"fields":{"_source.recid":378}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"ce10bd65c782f00d618a7048aef4d5fc","_score":1.0,"fields":{"_source.recid":352}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"bc3de3cf967f3e096895a3bb5e116f65","_score":1.0,"fields":{"_source.recid":334}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"b25df7be21d99528d176c22af106f23a","_score":1.0,"fields":{"_source.recid":382}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"0e78c14c86421dfffd6c66967f975504","_score":1.0,"fields":{"_source.recid":401}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"f71b5df45111e75d9e54fb6d20d74b19","_score":1.0,"fields":{"_source.recid":466}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"5b726ba23b522d292b3151b41ab5a029","_score":1.0,"fields":{"_source.recid":366}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"8080122cfb37bf2a1e97d3ad7cfcc0a4","_score":1.0,"fields":{"_source.recid":422}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"7723a629e6ef93ea470cf96433d0f757","_score":1.0,"fields":{"_source.recid":429}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c1ee65f24975523384f4edae9019561f","_score":1.0,"fields":{"_source.recid":386}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"2d0ff83f33bfbc2136e2be5cd48c1078","_score":1.0,"fields":{"_source.recid":388}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"ab4677a2087fd8741fc2cc33116d6f5e","_score":1.0,"fields":{"_source.recid":446}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"a181c1ae6629b5b4b05f5ee36e8be31b","_score":1.0,"fields":{"_source.recid":436}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"5b81f0c39daddc21bc5e08a894178b54","_score":1.0,"fields":{"_source.recid":511}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"224b3ce006bf9f7b719fb56f2ec7bb88","_score":1.0,"fields":{"_source.recid":512}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"5c8fae6d35f30605c2d8235e8fb1a56d","_score":1.0,"fields":{"_source.recid":403}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"45894b24d9e37973c2057c0853973a9c","_score":1.0,"fields":{"_source.recid":472}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"a0a909c0634d35295e1fb4b2938fa89e","_score":1.0,"fields":{"_source.recid":473}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"1bdf703b2c77cc07444998e08b6d5fa3","_score":1.0,"fields":{"_source.recid":409}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"1fec0360d31e1c145ca6a68b39ed6b3c","_score":1.0,"fields":{"_source.recid":467}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c877c53651f3fe7f8f31fd9973a85d49","_score":1.0,"fields":{"_source.recid":414}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"a2b0a6323ce26e24290be70895a00da2","_score":1.0,"fields":{"_source.recid":474}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"0a36f75c1370be1ebd780e0a25d518a1","_score":1.0,"fields":{"_source.recid":469}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"1e18913215820666aef31d99bbd52bde","_score":1.0,"fields":{"_source.recid":480}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"dc6f71fd1982ce324f4ba1463b0c808b","_score":1.0,"fields":{"_source.recid":424}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"3f6c3e6c30be86e632211cb9cff2f581","_score":1.0,"fields":{"_source.recid":470}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"bb5b5e90ebc14d7e78581aa68fea9077","_score":1.0,"fields":{"_source.recid":441}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"e31b26c0be59274c0f0b2767029c809a","_score":1.0,"fields":{"_source.recid":506}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"749ec3a9e31a8f82da9306fe0b416e73","_score":1.0,"fields":{"_source.recid":448}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"b90983290d0d77ccec96c0c1e3f28027","_score":1.0,"fields":{"_source.recid":457}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"6e1c838276158b731dce851700858b99","_score":1.0,"fields":{"_source.recid":560}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"addfcf54c30f5b89804d8410951fba66","_score":1.0,"fields":{"_source.recid":514}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"983bc03485c68679083daaec92a2d010","_score":1.0,"fields":{"_source.recid":504}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"4b6347e5f120918b0af039d86dd97b38","_score":1.0,"fields":{"_source.recid":527}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"98ce6c5d0db58d9ae1878784356a7b83","_score":1.0,"fields":{"_source.recid":471}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"1b7b2e75f21b59bdeb7bd47cf2fe2aa1","_score":1.0,"fields":{"_source.recid":475}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"74374837e6844ec3bc169b5bfbb50277","_score":1.0,"fields":{"_source.recid":489}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"f185fd8af0d994eb9fe5adf4bdb82a1c","_score":1.0,"fields":{"_source.recid":517}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"c239a508de1c404512d8a8e642545d32","_score":1.0,"fields":{"_source.recid":558}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d071999bce8a375b5a1a8a1bc03c7ec0","_score":1.0,"fields":{"_source.recid":468}},{"_index":"cf24507a073800945523e6366dc72108","_type":"ft","_id":"d30390a2257ca0f128a8c1af15583143","_score":1.0,"fields":{"_source.recid":559}},{"_index":"cf24507a073800945523e6366dc72108",
</comment><comment author="Jaydee2190" created="2010-12-06T16:47:28Z" id="591783">Hmmm... well for ease of reading i'm getting my recid back in this order
5,4,11,15,16,12,35,18,30,21,39, etc
</comment><comment author="kimchy" created="2010-12-06T16:49:10Z" id="591789">Did not fully understand what you are trying to say in the last comment. Can you _gist_ a simple recreation?  I will have a look.
</comment><comment author="Jaydee2190" created="2010-12-06T16:53:02Z" id="591797">Hopefully easier to view!  (New to Git)

git://gist.github.com/730558.git
</comment><comment author="kimchy" created="2010-12-06T17:06:04Z" id="591839">I mean a recreation, where you have curl requests to index relevant data, and then issue the queries that show the problem.
</comment><comment author="Jaydee2190" created="2010-12-06T17:07:47Z" id="591841">Oh sure, I can do that!  I could send you the index as well(its 2meg), but let me create some curl statements...
</comment><comment author="kimchy" created="2010-12-06T17:08:46Z" id="591848">Simple curl statements in a bash file would be great, so I can easily run them or convert them into a test cast.
</comment><comment author="Jaydee2190" created="2010-12-06T17:46:00Z" id="591946">Hi Kimchy(Shay?)

This gist will create 407 records
https://gist.github.com/730627

And the following(at least on my OSX 10.6.5 computer give weird results)

407 records... source_recid #348 comes back twice... In this test setup if I have larger sizes(like 50 or 100 then all works fine), but my initial index with 20,000 documents was a lot more sensitive

curl -XGET 'http://localhost:9200/b04c4e4156f501b4f6c12b6dd650881d/ft/_search?pretty=true' -d'{ "from":355,"size" :5, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"edge*"}}}'

curl -XGET 'http://localhost:9200/b04c4e4156f501b4f6c12b6dd650881d/ft/_search?pretty=true' -d'{ "from":360,"size" :5, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"edge*"}}}'

This shows the strange sort order

curl -XGET 'http://localhost:9200/b04c4e4156f501b4f6c12b6dd650881d/ft/_search?pretty=true' -d'{ "from":0,"size" :550, "fields" : "_source.recid", "query" : { "query_string": {"sort":"_source.recid","default_operator" : "AND","query":"wav*"}}}'
</comment><comment author="kimchy" created="2010-12-06T17:54:23Z" id="591975">cool, thanks, I will have a look later (in the middle of another feature). But, what you show is importnat, so ping me back in a few days if you don't hear back ;)
</comment><comment author="kimchy" created="2010-12-08T02:07:42Z" id="595883">Hi,

  Ran the test, and I can't recreate it. I started `0.13.1`, indexed all the docs, then ran the two search requests (one with `from: 355` and one with `from: 360`), and did not get duplicates based (the `348`) result.

Regarding the sorting, you do not specify the sorting correctly. First, you really should not sort with `_source....`, as it loads the whole json, parses it and extracts the value for each hit in order to be sorted. In your case, its simple, you should simply specify the `recid` as sorting field. Here is the query you want to run: 

```
curl -XGET 'http://localhost:9200/b04c4e4156f501b4f6c12b6dd650881d/ft/_search?pretty=true' -d '
{ 
    "from":0, 
    "size" :550, 
    "fields" : "_source.recid", 
    "sort": [
        { "recid" : {} }
    ],
    "query" : { "query_string": {"default_operator" : "AND","query":"wav*"}}
}
'
```
</comment><comment author="Jaydee2190" created="2010-12-08T02:09:56Z" id="595885">Ok, tnx, i'll take a look again tomorrow... maybe i'll try another machine just in case...
</comment><comment author="Jaydee2190" created="2010-12-08T12:55:44Z" id="596867">I just tried 0.13.1 on my same machine, and the same results unfortunately.

_id : 5d430a69bb9aff53231feea1dcef7ddc is returned twice (the curl commands and output here)

https://gist.github.com/733248

I'll try on my laptop this afternoon... but in case it matters...Java 1.6.0.jdk running 64 bit OSX 10.6.5 Intel Core 2 Duo
</comment><comment author="Jaydee2190" created="2010-12-08T13:27:26Z" id="596925">By the way, your correction on my use of sort :[{"recid" :{}}] works though (bringing back all queries they are sorted properly, which was another issue I was having)
</comment><comment author="kimchy" created="2010-12-08T17:00:54Z" id="597411">I think I know why it might happen..., can you run it with explain set to `true`? Lets see if all docs get the same score in this case.
</comment><comment author="Jaydee2190" created="2010-12-08T18:05:29Z" id="597560">sure...here's all results (although just look for 348 to see it repeat)

https://gist.github.com/733649
</comment><comment author="kimchy" created="2010-12-08T18:33:06Z" id="597638">Right, so, because all have the same score, and its a distributed search, maintaining an ordered result set is problematic. I think I can make it a bit better, need to think a bit... :)
</comment><comment author="Jaydee2190" created="2010-12-08T18:34:41Z" id="597642">Good luck!  I'm in awe with elasticsearch.
</comment><comment author="Jaydee2190" created="2010-12-08T18:34:41Z" id="597643">Good luck!  I'm in awe with elasticsearch.
</comment><comment author="tfreitas" created="2011-09-28T23:14:10Z" id="2231593">Is this closed?
</comment><comment author="clintongormley" created="2013-04-04T18:32:14Z" id="15915103">Solution provided with `preference` parameter to search
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 3.0.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/549</link><project id="" key="" /><description>Fixes important possible index corruptions and memory leak (happens on long running indexing jobs).
</description><key id="454491">549</key><summary>Upgrade to Lucene 3.0.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.1</label><label>v0.14.0</label></labels><created>2010-12-03T18:43:02Z</created><updated>2010-12-03T18:48:56Z</updated><resolved>2010-12-03T18:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-03T18:48:56Z" id="586821">Upgrade to Lucene 3.0.3, closed by a90684aaa52e75d6e53d64cf0f453a0e79ee4d33.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use multiple data dirs (allows striping IO across multiple disks)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/548</link><project id="" key="" /><description>Elasticsearch is often IO-bound during indexing: on EC2 elasticsearch would tap out at ~12-14k wreq/s to one disk. By using multiple independent data_esnode processes on each machine, we were able to get 10k-12k wreqs/s on each of 3 disks, almost tripling throughput.

If I could give ES a list of dirs for path.data, we could adjust the processes per machine independently of the drives per machine. Using RAID might also address this, but adds significant complexity and risk to the process of instantiating a machine in an elastic cloud.

Feature request:
- allow path.data to be an array:
  
  path:
    data: ["/es1/data", "/es2/data", "/es3/data"]
- when Lucene allocates a new set of index files, choose the dir with the highest available space, and put it at [data_dir]/nodes/[node number]/indices/[index_name]/[shard_number]
- This will give a significant performance increase even over RAID as merges will draw from distinct disks
- Cassandra uses this approach; I believe they are even clever about making compactions in general go from one set of disks to a distinct one.

Ramifications (via kimchy on IRC): need to get between Lucene and how it handles files; and "join" based operations, like "list files under shard dir" will need to be aggregated across mount points.
</description><key id="448521">548</key><summary>Use multiple data dirs (allows striping IO across multiple disks)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrflip</reporter><labels /><created>2010-12-01T02:46:00Z</created><updated>2011-09-23T14:29:50Z</updated><resolved>2011-09-23T14:29:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-23T14:29:50Z" id="2179110">Implemented in #1356.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Optimize single shard search to `QUERY_AND_FETCH` from any other search type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/547</link><project id="" key="" /><description>A single shard search (either though routing setting, or when using a single shard index) should be optimized to `QUERY_AND_FETCH` search type from any other type. The DFS phase is only needed when multiple shards are in play, and the "then fetch" optimization is only relevant when hitting more shards.
</description><key id="448472">547</key><summary>Search API: Optimize single shard search to `QUERY_AND_FETCH` from any other search type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-01T01:58:39Z</created><updated>2010-12-01T01:59:16Z</updated><resolved>2010-12-01T01:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-01T01:59:16Z" id="579471">Search API: Optimize single shard search to `QUERY_AND_FETCH` from any other search type, closed by d9610ed0a1ed422117df717e335dc1afac8c78ea.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Setting: Add `index.refresh_interval` to simplify setting instead of "index.engine.robin.refresh_interval` (still works)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/546</link><project id="" key="" /><description>The idea of engines is to have different implementations for index/search engines. The current one is robin, but, settings like refresh interval (how often an index gets refreshed automatically) should have simplet setting name. Though it might not apply to future engine implementation, it should be used if it does apply to them.
</description><key id="448445">546</key><summary>Index Setting: Add `index.refresh_interval` to simplify setting instead of "index.engine.robin.refresh_interval` (still works)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-12-01T01:38:13Z</created><updated>2010-12-01T01:44:17Z</updated><resolved>2010-12-01T01:44:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-12-01T01:44:17Z" id="579446">Index Setting: Add `index.refresh_interval` to simplify setting instead of &amp;quot;index.engine.robin.refresh_interval` (still works), closed by 3ea19a514bdd6ef6ada54eb2557244218ff35ba5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Scroll API: Allow to provide the scroll id as the body of the scroll request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/545</link><project id="" key="" /><description>Allow to provide the search scroll id as the (full) body of the search scroll request.
</description><key id="444075">545</key><summary>Search Scroll API: Allow to provide the scroll id as the body of the scroll request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-11-29T11:59:01Z</created><updated>2010-11-29T11:59:46Z</updated><resolved>2010-11-29T11:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-29T11:59:46Z" id="572681">Search Scroll API: Allow to provide the scroll id as the body of the scroll request, closed by c0945831e86ddf17a573d381b9eb0b40c2e2196c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FastVectorHighlighter returns a null vector of fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/544</link><project id="" key="" /><description>Fixed case that FastVectorHighlighter returns a null vector of fragments
</description><key id="443157">544</key><summary>FastVectorHighlighter returns a null vector of fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2010-11-28T22:07:24Z</created><updated>2014-07-16T21:56:57Z</updated><resolved>2011-02-12T14:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-11T16:32:15Z" id="763331">I can't apply this patch since it includes all this changes (even though it has a single diff with the latest change). Somehow you need to restart your fork / close this pull request and start fresh or something, weird!. I have pushed the change anyhow so it will make it to 0.15.
</comment><comment author="aparo" created="2011-02-12T14:11:50Z" id="765602">thanks

I' ll reset my branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduce the number of concurrent recoveries per node to 2 from number_of_processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/543</link><project id="" key="" /><description>By default, the number of concurrent recoveries happening on a node is controlled by the number of processors. The concurrent recoveries are shards being allocated on a node and recover from a target node / gateway, or a shard being recovered from.

This should be lowered to a smaller value so there won't be a io load on strong machines with a lot of processors.
</description><key id="442687">543</key><summary>Reduce the number of concurrent recoveries per node to 2 from number_of_processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-11-28T14:30:01Z</created><updated>2010-11-28T14:35:54Z</updated><resolved>2010-11-28T14:35:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-28T14:35:54Z" id="571085">Reduce the number of concurrent recoveries per node to 2 from number_of_processors, closed by 6d9576c217fabef81bc77e5f91da3b4163502d4e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index / Index Template: Allow to define `_default_` mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/542</link><project id="" key="" /><description>Allow to define `_default_` mapping on an index (or an index template) when creating the index or the template. The `_default_` mapping, similar in concept to the one that can be defined on the config/mapping, gets merged into any new type mapping introduced for that specific index.
</description><key id="442009">542</key><summary>Index / Index Template: Allow to define `_default_` mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.0</label></labels><created>2010-11-27T21:27:51Z</created><updated>2010-11-27T21:30:37Z</updated><resolved>2010-11-27T21:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-27T21:30:37Z" id="570242">Index / Index Template: Allow to define `_default_` mapping, closed by cfa8c9aa7955655374afab61e0049ef3e08182d2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Index API: Allow to provide mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/541</link><project id="" key="" /><description>Allow to provide mappings as well as settings in the create index API:

```
curl -XPOST localhost:9200/test -d '
{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_source" : { "enabled" : false }
        }
    }
}
'
```
</description><key id="441090">541</key><summary>Create Index API: Allow to provide mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-11-26T22:26:37Z</created><updated>2010-11-26T22:28:53Z</updated><resolved>2010-11-26T22:28:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-26T22:28:53Z" id="569035">Create Index API: Allow to provide mappings. closed by 526f28f47981bfa83d79f5c09b2f6b5724649b78.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/540</link><project id="" key="" /><description>Index templates allows to define templates that will automatically be applied to indices created. The templates include both settings and mappings, and a simple pattern template that controls if the template will be applied to the index created. For example:

```
curl -XPUT localhost:9200/_template/template_1 -d '
{
    "template" : "te*",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_source" : { "enabled" : false }
        }
    }
}
'
```

Defines a template named `template_1`, with a template pattern of `te*`. The settings and mappings will be applied to any index name that matches the `te*` template.
## Removing an Index Template

Index templates are identified by a name (in the above case `template_1`) and can be delete as well:

```
curl -XDELETE localhost:9200/_template/template_1
```
## Getting an Index Template

Getting a specific index template can be done using (for example):

```
curl -XGET localhost:9200/_template/template_1
```
## Multiple Templates Matching

Multiple index templates can potentially match an index, in this case, both the settings and mappings are merged into the final configuration of the index. The order of the merging can be controlled using the `order` parameter, with lower `order` being applied first, and higher orders overriding them. For example:

```
curl -XPUT localhost:9200/_template/template_1 -d '
{
    "template" : "*",
    "order" : 0
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_source" : { "enabled" : false }
        }
    }
}
'

curl -XPUT localhost:9200/_template/template_2 -d '
{
    "template" : "te*",
    "order" : 1
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_source" : { "enabled" : true }
        }
    }
}
'
```

The above will disable storing the `_source` on all `type1` types, but for indices of that start with `te*`, source will still be enabled. Note, for mappings, the merging is "deep", meaning that specific object/property based mappings can easily be added/overridden on higher order templates, with lower order templates providing the basis.

Note, when creating an index, settings can be specified as well. Those will be merged as well, replacing template settings. In the future, mappings will also be allowed to be defined on index creation for the REST API.
</description><key id="440401">540</key><summary>Index Templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.0</label></labels><created>2010-11-26T13:44:27Z</created><updated>2010-11-26T13:45:39Z</updated><resolved>2010-11-26T13:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-26T13:45:39Z" id="568229">Index templates, closed by ebd6316db9be1ef69ddc0a929bb225ec6d9b7ed6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New facet feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/539</link><project id="" key="" /><description>What I am trying to achieve : 

we have an index with documents that look like : 
{ 
&#160; currency:'USD', 
&#160; product:'toaster', 
&#160; value: 23.4 
}, 
{ 
&#160; currency:'GBP', 
&#160; product:'toaster', 
&#160; value: 13.4 
} 

I want to produce factes for 'currency' &amp; 'product' &#160;where the value of each term is not a count , its a sum of values. 

The expected result would be something like : 

facets": { 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "currencyFacet": { 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "_type": "terms", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "_field": "currency", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "terms": [ 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; { 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "term": "GBP", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "count": 1, 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "value": 13.4 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; }, 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; { 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "term": "USD", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "count": 1, 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "value": 23.4 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; } 
} 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; ] 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; }, 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "productFacet": { 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "_type": "terms", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "_field": "product", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "terms": [ 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; { 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "term": "toaster", 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "count": 2, 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; "value":36.8 

&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; }, 
] 
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; } 
&#160; &#160; &#160; &#160; } 
} 
</description><key id="439305">539</key><summary>New facet feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Zorkin</reporter><labels /><created>2010-11-25T19:31:15Z</created><updated>2014-05-23T18:22:15Z</updated><resolved>2011-03-17T09:31:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Zorkin" created="2010-11-25T19:33:14Z" id="567116">Need to specify  the numeric field to operate on. Will be nice to get full stats like the stats facet  on that but less critical. 
</comment><comment author="kimchy" created="2011-03-17T09:31:09Z" id="884359">Implemented in #705.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>special character in query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/538</link><project id="" key="" /><description>hi,
           i am trying to search value with special character in elastic search index. but no result found.

Ex: query:"senthil("

OR

query:"senthil&amp;"

OR
{"query":{"term":{"title":senthil("}}}

Please Help
</description><key id="436465">538</key><summary>special character in query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">senthilsebi</reporter><labels /><created>2010-11-24T11:23:22Z</created><updated>2010-11-24T11:29:49Z</updated><resolved>2010-11-24T11:29:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-24T11:29:49Z" id="563823">please don't open issues for questions, post it on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: JMS Discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/537</link><project id="" key="" /><description>Our application stack includes Grid Gain and a clustered JMS Server. We can make Grid Gain use JMS for discovery. It would be great if we could also use JMS for elastic search discovery.
</description><key id="436452">537</key><summary>Feature Request: JMS Discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keteracel</reporter><labels /><created>2010-11-24T11:09:50Z</created><updated>2011-05-09T20:54:39Z</updated><resolved>2011-05-09T20:54:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="keteracel" created="2011-05-09T20:54:39Z" id="1126230">we no longer require this, zen is satisfactory for our needs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Add how long the search took (in milliseconds)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/536</link><project id="" key="" /><description>Add `took` to the search response, which is the time it took to execute the search request in milliseconds.
</description><key id="435675">536</key><summary>Search API: Add how long the search took (in milliseconds)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-11-23T23:43:52Z</created><updated>2010-11-24T11:27:48Z</updated><resolved>2010-11-24T11:27:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-24T11:23:19Z" id="563808">I'll change the title a bit and add it to search responses.
</comment><comment author="kimchy" created="2010-11-24T11:27:48Z" id="563818">Search API: Add how long the search took (in milliseconds), closed by d150ac2da418d30c5cfbabe47f27cc31e6f5b397.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API: Add refresh flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/535</link><project id="" key="" /><description>Add refresh flag to bulk API (a REST param, for example: `/_bulk?refresh=true`) to the relevant shards post the bulk operation.
</description><key id="435605">535</key><summary>Bulk API: Add refresh flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-11-23T22:57:37Z</created><updated>2010-11-23T22:59:22Z</updated><resolved>2010-11-23T22:59:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-23T22:59:22Z" id="562827">Bulk API: Add refresh flag, closed by 51273587dedb78ceba2b8f905fbd2e3ac330c237.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a well-annotated configuration file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/534</link><project id="" key="" /><description>Once initial setup is complete, it would be good to have a detailed settings file to work from.

Here's my take: https://gist.github.com/711993
</description><key id="434730">534</key><summary>Provide a well-annotated configuration file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrflip</reporter><labels /><created>2010-11-23T16:21:07Z</created><updated>2013-04-04T18:32:23Z</updated><resolved>2013-04-04T18:32:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:32:23Z" id="15915112">Done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide API to recover full configuration setting values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/533</link><project id="" key="" /><description>An API call that dumped the machine configuration will make tuning, setup and initial education much easier.

Elasticsearch's behavior can be affected by variables coming from the .in.sh, the commandline, the runner script, the elasticsearch.yml, logging.yml and perhaps more. Since some variables change how those files are loaded, tracing the origin of a setting can be very confusing.

Right now the workaround is to set various log variables to DEBUG and watch as each module announces its birth.
</description><key id="434718">533</key><summary>Provide API to recover full configuration setting values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrflip</reporter><labels /><created>2010-11-23T16:17:23Z</created><updated>2014-07-08T11:54:31Z</updated><resolved>2014-07-08T11:54:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T11:54:30Z" id="48326499">ES logging and APIs have greatly improved since 2010. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Cache: Invalidate the query cache when mappings change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/532</link><project id="" key="" /><description>When mapping change, queries now might be constructed differently, invalidate it in case it does change.
</description><key id="434396">532</key><summary>Query Cache: Invalidate the query cache when mappings change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.1</label><label>v0.14.0</label></labels><created>2010-11-23T13:07:52Z</created><updated>2010-11-23T13:26:48Z</updated><resolved>2010-11-23T13:26:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-23T13:26:48Z" id="561422">Query Cache: Invalidate the query cache when mappings change, closed by 4a3e5b63484c922225c47404a06e415d11c09e2f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete Mapping API: Wrongly deleting existing mapping information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/531</link><project id="" key="" /><description>Existing mapping information is not correctly deleted, causing old mapping definitions to still hang around and affect the system.
</description><key id="434375">531</key><summary>Delete Mapping API: Wrongly deleting existing mapping information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.1</label><label>v0.14.0</label></labels><created>2010-11-23T12:46:14Z</created><updated>2010-11-23T12:56:29Z</updated><resolved>2010-11-23T12:56:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-23T12:56:29Z" id="561371">Delete Mapping API: Wrongly deleting existing mapping information, closed by 9479ac636ed76b5a5426ff9165fe78539dbccf95.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support date and ip types for `numeric_range` filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/530</link><project id="" key="" /><description>Since both `date` and `ip` types are stored internally as numbers, the `numeric_range` filter should support them as well.
</description><key id="431272">530</key><summary>Support date and ip types for `numeric_range` filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkroll</reporter><labels><label>enhancement</label><label>v0.14.0</label></labels><created>2010-11-21T17:25:14Z</created><updated>2010-11-22T14:18:11Z</updated><resolved>2010-11-22T14:18:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-22T14:16:09Z" id="558879">Numeric range filter does not support dates or ips (which are stored as numbers internally). I will change the title of the issue to reflect that and fix it.
</comment><comment author="kimchy" created="2010-11-22T14:18:11Z" id="558883">Support date and ip types for `numeric_range` filter, closed by db1f7e09f37ba0d281a4ef30e4eb4085f71fa462.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API: An API to analyzer custom text based on an optional analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/529</link><project id="" key="" /><description>An API to analyze custom text provided ("within" a specific analyzer). The REST endpoint is:

```
curl -XGET localhost:9200/{index}/_analyze
```

For example:

```
curl -XGET localhost:9200/test/_analyze?text=this is a test
```

An `analyzer` can also be provided to use a different analyzer than the default indexing analyzer for the index:

```
curl -XGET localhost:9200/test/_analyze?analyzer=whitespace&amp;text=this is a test
```

The text can also be provided as the request body.

The `format` of the response can also be provided, by default it is `detailed` (breaking each token information into a json object). The `text` format provides the analyzed data in a single text response.
</description><key id="431161">529</key><summary>Analyze API: An API to analyzer custom text based on an optional analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.14.0</label></labels><created>2010-11-21T15:32:48Z</created><updated>2011-01-05T09:24:49Z</updated><resolved>2010-11-21T23:33:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-21T15:33:52Z" id="557128">Analyze API: An API to analyzer custom text based on an optional analyzer, closed by b4113d57d30e6e88773186ed28654aec2c6ab946.
</comment><comment author="clintongormley" created="2011-01-03T15:06:31Z" id="649052">Documentation missing for `analyze`
</comment><comment author="lukas-vlcek" created="2011-01-05T00:17:36Z" id="653653">Documentation added http://www.elasticsearch.com/docs/elasticsearch/rest_api/analyze/
</comment><comment author="kimchy" created="2011-01-05T09:24:49Z" id="654367">Analyze was actually documented (under admin/indices/analyze), just not listed in the navigation tree.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation for IP mapping is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/528</link><project id="" key="" /><description>http://www.elasticsearch.com/docs/elasticsearch/mapping/ip is 404.
</description><key id="430515">528</key><summary>Documentation for IP mapping is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels /><created>2010-11-20T22:00:01Z</created><updated>2010-11-21T02:07:07Z</updated><resolved>2010-11-21T06:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-20T22:44:23Z" id="556283">ouch, right..., fixed.
</comment><comment author="abh" created="2010-11-21T02:07:07Z" id="556529">Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>log4j syslog appender don't load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/527</link><project id="" key="" /><description>Hi, current elasticsearch cannot use syslog appender in log4j, because of this:

```
log4j:ERROR Could not instantiate class [org.apache.log4j.SyslogAppender].
java.lang.ClassNotFoundException: org.apache.log4j.SyslogAppender
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:169)
    at org.apache.log4j.helpers.Loader.loadClass(Loader.java:179)
    at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:320)
    at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:121)
    at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:664)
    at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:647)
    at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:544)
    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:440)
    at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:367)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:111)
    at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:92)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:153)
```

Correct class must be:
    org.apache.log4j.net.SyslogAppender
(not 'log4j.' directly but 'log4j.**net.**')

Maybe other **net.** appenders have same problems
</description><key id="428452">527</key><summary>log4j syslog appender don't load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luddic</reporter><labels><label>bug</label><label>v0.13.1</label><label>v0.14.0</label></labels><created>2010-11-19T13:25:03Z</created><updated>2010-11-20T14:37:42Z</updated><resolved>2010-11-20T04:08:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-19T13:28:13Z" id="553402">How do you configure it?
</comment><comment author="luddic" created="2010-11-19T13:38:04Z" id="553427">Something like this in logging.yml:

```
  syslog:
    type: syslog
    sysloghost: 10.11.11.71
    facility: local0
    layout:
      type: pattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"
```
</comment><comment author="kimchy" created="2010-11-19T20:07:50Z" id="554421">Right, I see where the problem is, will push a fix. For now, you can just specify the full classname in the type, it should work.
</comment><comment author="kimchy" created="2010-11-19T20:08:34Z" id="554424">log4j syslog appender don&amp;#39;t load, closed by 03f66b0c45068e0a6d7b3144b51c11b1c57c964b.
</comment><comment author="luddic" created="2010-11-20T14:37:42Z" id="555563">Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trivial Change: Add tika jars to intellij project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/526</link><project id="" key="" /><description>Previously there was a "library" but it didn't have any jars in.

This is second pull request. Sorry not done this before. The previous one had too many commits in.
</description><key id="426201">526</key><summary>Trivial Change: Add tika jars to intellij project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">time4tea</reporter><labels /><created>2010-11-18T14:03:22Z</created><updated>2014-07-16T21:56:57Z</updated><resolved>2011-02-20T00:23:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-21T10:07:05Z" id="556805">Hey, its strange, tika is already associated as a library for the mapper-attachment module..., its not on your project?
</comment><comment author="time4tea" created="2010-11-21T16:43:31Z" id="557204">there was a library called tika but strangely it didn't seem to have any classes in it.
</comment><comment author="kimchy" created="2011-02-20T00:23:29Z" id="789554">Its part of IDEA project files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trivial Change - Add tika jar files to tika lib so plugin compiles in intellij</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/525</link><project id="" key="" /><description>Thats it - not much.
</description><key id="425105">525</key><summary>Trivial Change - Add tika jar files to tika lib so plugin compiles in intellij</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">time4tea</reporter><labels /><created>2010-11-17T22:35:55Z</created><updated>2014-07-16T21:56:58Z</updated><resolved>2010-11-18T14:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-18T00:09:57Z" id="549466">I am assuming you want to use the couchdb river to use Tika? There will also need to be a gradle change in this case (simple to do), but, I think that the couchdb river should not know about tika, it should assume that the attachment plugin is installed, and use it to index the attachment (the attachment plugin already uses Tika).
</comment><comment author="time4tea" created="2010-11-18T01:16:01Z" id="549590">not sure how easy that will be. in any case this is just to get the attachment plugin to compile. previously it didn't.... (at least not in idea)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Routing: Allow to define `path` on the `_routing` mapping, to automatically extract the routing from it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/524</link><project id="" key="" /><description>Allow to define a `path` for the `_routing` mapping, allowing to use a document field value as the routing value. The `path` allows for dot notation to access inner objects and fields. For example:

```
{
    "comment" : {
        "_routing" : {
            "required" : true,
            "path" : "blog.post_id"
        }
    }
}
```

The above will route the following document using the value "111222" :

```
{
    "text" : "the comment text"
    "blog" : {
        "id" : "111222"
    }
}
```

A note, when using `_path` and not providing the `_routing` specifically in the API, there might be an additional parsing stage to extract the routing before directing the operation to the relevant shard.

Also, there is a validation stage as well if the routing is provided as part of the API and a path is specified.
</description><key id="424199">524</key><summary>Routing: Allow to define `path` on the `_routing` mapping, to automatically extract the routing from it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-11-17T15:05:21Z</created><updated>2010-11-17T15:05:57Z</updated><resolved>2010-11-17T15:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-17T15:05:57Z" id="547979">Routing: Allow to define `path` on the `_routing` mapping, to automatically extract the routing from it, closed by 44775c2aa84d1bfb22ea369b94a26f1f4143b6ce.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for expiring out documents / TTL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/523</link><project id="" key="" /><description>It would be really nice if there was a way to automatically expire out old documents from a particular index.  In the simplest form it would be a TTL per index.  Once a document's age (based on insert time or maybe another  configurable field), passes the TTL it should be removed from the index.
</description><key id="422234">523</key><summary>Support for expiring out documents / TTL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gwittel</reporter><labels /><created>2010-11-16T16:29:38Z</created><updated>2011-09-26T20:42:19Z</updated><resolved>2011-09-26T20:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-18T00:13:28Z" id="549473">Agreed, it would be a nice feature... . Need to think a bit how to do it in the most optimized manner.... .
</comment><comment author="coffeepac" created="2011-07-27T22:44:30Z" id="1668551">I would also love this.  Currently I am writing a job to prune old documents by issuing delete statements.
</comment><comment author="gwittel" created="2011-09-26T19:26:34Z" id="2202663">It sounds like this has been handled in Issue #1316 for v0.18.  Unless there's something else to be done, feel free to close.
</comment><comment author="kimchy" created="2011-09-26T20:42:19Z" id="2203640">Yea, missed this issue, closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Routing: When specify in the mapping _routing required, a delete without explicit routing value should automatically be broadcasted to all shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/522</link><project id="" key="" /><description>When specify in the mapping _routing required, a delete without explicit routing value should automatically be broadcasted to all shards
</description><key id="422201">522</key><summary>Routing: When specify in the mapping _routing required, a delete without explicit routing value should automatically be broadcasted to all shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-11-16T16:03:43Z</created><updated>2010-11-16T16:31:16Z</updated><resolved>2010-11-16T16:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-16T16:31:16Z" id="545554">Routing: When specify in the mapping _routing required, a delete without explicit routing value should automatically be broadcasted to all shards, closed by a04d8ec5ad9d3023771e4cb0295adb7e7c953626.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Histogram Facet: keys are wrongly ordered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/521</link><project id="" key="" /><description>The histogram facet returns the results in an ordered manner (by default, keys). It is currently broken for keys that are "far" apart.
</description><key id="422169">521</key><summary>Histogram Facet: keys are wrongly ordered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.0</label></labels><created>2010-11-16T15:45:55Z</created><updated>2010-11-16T15:46:10Z</updated><resolved>2010-11-16T15:46:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-16T15:46:10Z" id="545438">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Routing: Allow to specify on the `_routing` mapping that its required, and fail index operations that do not provide one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/520</link><project id="" key="" /><description>In the mapping definition, allow to define on the `_routing` element that it is required (by settings `"requried" : true`. This will hint difference APIs to take it into account. Start with the index api and fail the index operation if routing is not provided (also when provided within the bulk API).
</description><key id="421686">520</key><summary>Routing: Allow to specify on the `_routing` mapping that its required, and fail index operations that do not provide one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-16T10:59:39Z</created><updated>2010-11-16T13:10:50Z</updated><resolved>2010-11-16T13:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-16T13:10:50Z" id="545135">Routing: Allow to specify on the `_routing` mapping that its required, and fail index operations that do not provide one, closed by 02981f6101b558d444d61da936d7e6689a567979.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping with uppercased properites</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/519</link><project id="" key="" /><description>&lt;/delete&gt;
after put a mapping to set property "Name" with something
it was supposed to get property "Name",actually what i got is "_name"...
&lt;/delete&gt;
issue closed,cuz it doesn't happen in master build version
</description><key id="421423">519</key><summary>Mapping with uppercased properites</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2010-11-16T06:31:36Z</created><updated>2010-11-17T15:41:41Z</updated><resolved>2010-11-16T16:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-16T08:59:45Z" id="544760">Yea, was just about to point that I fixed it in master... :). No need to delete everything (title and content), just close it ;)
</comment><comment author="medcl" created="2010-11-17T15:37:23Z" id="548054">sorry for that,i just didn't find the button to close or delete the issue,I really regret it after  cleared that... ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Rename _attributes to _meta</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/518</link><project id="" key="" /><description>Renamed the _attributes notation to add metadata information on a mapping to `_meta`. Defining it is now done using `_meta` and not `_attributes`.
</description><key id="420909">518</key><summary>Mapping: Rename _attributes to _meta</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.13.0</label></labels><created>2010-11-15T23:25:26Z</created><updated>2010-11-15T23:34:06Z</updated><resolved>2010-11-15T23:34:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-15T23:34:06Z" id="543947">Mapping: Rename _attributes to _meta, closed by 8a8a6d5547776776494194d83f280ad9caa7fc42.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Histogram Facet: Allow to define a key field and value script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/517</link><project id="" key="" /><description>Allow for the combination of having a `key_field` and `value_script` in a histogram facet.
</description><key id="419003">517</key><summary>Histogram Facet: Allow to define a key field and value script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-15T13:02:05Z</created><updated>2010-11-15T13:02:41Z</updated><resolved>2010-11-15T13:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-15T13:02:41Z" id="541576">Histogram Facet: Allow to define a key field and value script, closed by 5c6c4bfb5a8571fde0d290fce7dcade235722189.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: `date` type to also allow providing values in numeric (milliseconds since epoch)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/516</link><project id="" key="" /><description>On top of supporting dates formatted as string, also support providing the values as milliseconds since epoch value.
</description><key id="418862">516</key><summary>Mapper: `date` type to also allow providing values in numeric (milliseconds since epoch)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-15T12:12:16Z</created><updated>2010-11-15T12:13:00Z</updated><resolved>2010-11-15T12:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-15T12:13:00Z" id="541505">Mapper: `date` type to also allow providing values in numeric (milliseconds since epoch), closed by b1db5c43d6f970781a95822ea71ffe125b76986e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Store the routing (if provided) under a `_routing` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/515</link><project id="" key="" /><description>When providing routing, store it under the `_routing` field. This will allow to later on reindex the relevant document (otherwise, the routing might be lost...).
</description><key id="418695">515</key><summary>Mapper: Store the routing (if provided) under a `_routing` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-15T09:34:08Z</created><updated>2010-11-15T09:35:07Z</updated><resolved>2010-11-15T09:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-15T09:35:07Z" id="541304">Mapper: Store the routing (if provided) under a `_routing` field, closed by 6d214d69b9f6b144dec0b3ad9d231a873a5638d7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support to reindex into an http endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/514</link><project id="" key="" /><description>Probably not a high priority or must have feature, but this would be extremely convenient.

This would allow one to:
- Switch gateways
- Upgrade to new version with incompatible gateway
- Clone an entire cluster 

Based on this discussion here:
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/43b534ca360d808f?pli=1
</description><key id="417543">514</key><summary>Add support to reindex into an http endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-11-14T01:17:13Z</created><updated>2014-02-21T13:52:06Z</updated><resolved>2014-02-21T13:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-21T13:52:06Z" id="35731580">I think you should be fine here with snapshot/restore in 1.0?

Closing this for now (feel free to reopen if I misunderstood you)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail to create index when alias on other index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/513</link><project id="" key="" /><description>Starting with clean ES installation:

&lt;pre&gt;
 curl -XPOST http://localhost:9200/abc/x -d '{ "name" : "a" }'
 curl -XPOST http://localhost:9200/xyz/x -d '{ "name" : "b" }'
 curl -XPOST http://localhost:9200/_aliases -d '{ "actions" : [ {"add":{"index":"abc", "alias":"xyz"}} ] }'
 curl -XPOST http://localhost:9200/_shutdown
&lt;/pre&gt;


then when starting ES again

&lt;pre&gt;
 ./bin/elasticsearch -f
&lt;/pre&gt;


I get the following log messages:

&lt;pre&gt;
[2010-11-13 12:03:24,159][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: initializing ...
[2010-11-13 12:03:24,162][INFO ][plugins                  ] [Foster, Bill] loaded []
[2010-11-13 12:03:25,533][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: initialized
[2010-11-13 12:03:25,533][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: starting ...
[2010-11-13 12:03:25,685][INFO ][transport                ] [Foster, Bill] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.2.3:9300]}
[2010-11-13 12:03:28,716][INFO ][cluster.service          ] [Foster, Bill] new_master [Foster, Bill][R_kJChj2TtC1vyrqUK74QA][inet[/192.168.2.3:9300]], reason: zen-disco-join (elected_as_master)
[2010-11-13 12:03:28,750][INFO ][discovery                ] [Foster, Bill] elasticsearch/R_kJChj2TtC1vyrqUK74QA
[2010-11-13 12:03:28,828][INFO ][http                     ] [Foster, Bill] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.2.3:9200]}
[2010-11-13 12:03:28,829][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: started
[2010-11-13 12:03:29,444][INFO ][cluster.metadata         ] [Foster, Bill] [abc] creating index, cause [gateway], shards [5]/[1], mappings [x]
[2010-11-13 12:03:29,447][ERROR][gateway.local            ] [Foster, Bill] failed to create index [xyz]
[2010-11-13 12:03:29,914][INFO ][cluster.metadata         ] [Foster, Bill] [abc] created and added to cluster_state
&lt;/pre&gt; 


Note the ERROR ^^

What is even more strange is that if I use index names [a] instead [abc] and [b] instead [xyx] then it works fine.
</description><key id="416560">513</key><summary>Fail to create index when alias on other index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-11-13T11:10:41Z</created><updated>2010-11-13T11:16:03Z</updated><resolved>2010-11-13T11:16:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2010-11-13T11:16:02Z" id="538646">ticket created twice, see #512
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail to create index when alias on other index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/512</link><project id="" key="" /><description>Starting with clean ES installation:

&lt;pre&gt;
 curl -XPOST http://localhost:9200/abc/x -d '{ "name" : "a" }'
 curl -XPOST http://localhost:9200/xyz/x -d '{ "name" : "b" }'
 curl -XPOST http://localhost:9200/_aliases -d '{ "actions" : [ {"add":{"index":"abc", "alias":"xyz"}} ] }'
 curl -XPOST http://localhost:9200/_shutdown
&lt;/pre&gt;


then when starting ES again

&lt;pre&gt;
 ./bin/elasticsearch -f
&lt;/pre&gt;


I get the following log messages:

&lt;pre&gt;
[2010-11-13 12:03:24,159][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: initializing ...
[2010-11-13 12:03:24,162][INFO ][plugins                  ] [Foster, Bill] loaded []
[2010-11-13 12:03:25,533][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: initialized
[2010-11-13 12:03:25,533][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: starting ...
[2010-11-13 12:03:25,685][INFO ][transport                ] [Foster, Bill] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.2.3:9300]}
[2010-11-13 12:03:28,716][INFO ][cluster.service          ] [Foster, Bill] new_master [Foster, Bill][R_kJChj2TtC1vyrqUK74QA][inet[/192.168.2.3:9300]], reason: zen-disco-join (elected_as_master)
[2010-11-13 12:03:28,750][INFO ][discovery                ] [Foster, Bill] elasticsearch/R_kJChj2TtC1vyrqUK74QA
[2010-11-13 12:03:28,828][INFO ][http                     ] [Foster, Bill] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.2.3:9200]}
[2010-11-13 12:03:28,829][INFO ][node                     ] [Foster, Bill] {elasticsearch/0.13.0-SNAPSHOT/2010-11-13T10:04:15}[8348]: started
[2010-11-13 12:03:29,444][INFO ][cluster.metadata         ] [Foster, Bill] [abc] creating index, cause [gateway], shards [5]/[1], mappings [x]
[2010-11-13 12:03:29,447][ERROR][gateway.local            ] [Foster, Bill] failed to create index [xyz]
[2010-11-13 12:03:29,914][INFO ][cluster.metadata         ] [Foster, Bill] [abc] created and added to cluster_state
&lt;/pre&gt; 


Note the ERROR ^^

What is even more strange is that if I use index names [a] instead [abc] and [b] instead [xyx] then it works fine.
</description><key id="416559">512</key><summary>Fail to create index when alias on other index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-11-13T11:10:14Z</created><updated>2010-11-13T21:02:03Z</updated><resolved>2010-11-13T21:02:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-13T16:52:56Z" id="538970">The problem is that its not allowed to create an alias with the same name as an existing index. Its not checked when creating the alias, but checked when creating the index (from the gateway), and fails. I will push a fix so it will cause the alias operation to fail (and print the cause for the index creation failure).
</comment><comment author="lukas-vlcek" created="2010-11-13T18:17:12Z" id="539092">And would it be allowed to have two indices with the same alias? Index A and B both having alias C?
</comment><comment author="lukas-vlcek" created="2010-11-13T18:23:11Z" id="539104">BTW: why my test case did fail for indices [abc] and [xyz] but did NOT fail for indices [a] and [b]?
</comment><comment author="kimchy" created="2010-11-13T18:32:27Z" id="539112">Yes, a single alias can point to two indices, and then, when you search on it, it will be performed against both indices. Of course, indexing against the alias will not work.

I am not sure regarding a and b, can you recreate this now? Maybe the aliasing was nto the same?
</comment><comment author="lukas-vlcek" created="2010-11-13T20:58:15Z" id="539334">Seems to work correctly now (it does not allow to create alias with index name anymore). Do you want me to close this ticket?
</comment><comment author="kimchy" created="2010-11-13T20:59:35Z" id="539336">Yea, if its ok by you, then go ahead and close it. Nice catch!
</comment><comment author="lukas-vlcek" created="2010-11-13T21:02:03Z" id="539339">Closing, the world is a better place now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Allow to provide pattern field names when using query_string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/511</link><project id="" key="" /><description>When using `query_string` query, allow to use simple pattern matching to allow to expand the fields searched on to the ones matching the pattern. For example, a document in the following form:

```
{
    "name" : {
        "first" : "shay",
        "last" : "banon"
    }
}
```

And a query string to query any field under `name`:

```
{
    "query_string" : {
        "fields" : [ "name.*" ],
        "query" : "test"
    }
}
```

The matching is done on the full name of the fields, the index name, and the stand alone name, so allows for interesting matching capabilities. The matching is done using the simple pattern matching support (Match a String against the given pattern, supporting the following simple pattern styles: "xxx*", "*xxx", "*xxx*" and "xxx*yyy" matches (with an arbitrary number of pattern parts), as well as direct equality.).
</description><key id="415965">511</key><summary>Query DSL: Allow to provide pattern field names when using query_string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">berndlutz</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-11-12T22:56:57Z</created><updated>2011-03-15T21:42:46Z</updated><resolved>2010-11-13T08:26:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-13T00:26:22Z" id="538261">Query DSL: Allow to provide pattern field names when using query_string query, closed by 38d77f8cf30e358597e255b5d8d865f243175aca.
</comment><comment author="clintongormley" created="2010-11-13T11:57:06Z" id="538676">Does this also work within the query_string itself? as in `"foo bar name.*:kimchy"`
</comment><comment author="kimchy" created="2010-11-13T15:57:35Z" id="538889">No, this can be added as a separate feature, though you should be able to control similar aspects (use dis max or boolean query, tie breaker, and so on).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch allows an alias to point to multiple indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/510</link><project id="" key="" /><description>```
curl -XPUT 'http://localhost:9200/index1'
curl -XPUT 'http://localhost:9200/index2'
curl -XPOST 'http://localhost:9200/_aliases' -d'
{
    "actions" : [
        { "add" : { "index" : "index1", "alias" : "index_alias" } }
    ]
}'
{"ok":true}
curl -XPOST 'http://localhost:9200/_aliases' -d'
{
    "actions" : [
        { "add" : { "index" : "index2", "alias" : "index_alias" } }
    ]
}'
{"ok":true}
```

This is with 0.12.1.
</description><key id="413464">510</key><summary>ElasticSearch allows an alias to point to multiple indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels /><created>2010-11-11T18:37:06Z</created><updated>2016-12-01T18:03:08Z</updated><resolved>2011-11-25T14:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-12T05:49:43Z" id="536174">Thats intentional, this allows you to point a single alias to more than one index, and then, search on the alias which will cause searching on all the indices it points to.
</comment><comment author="karussell" created="2011-11-25T13:00:32Z" id="2874327">close?
</comment><comment author="kimchy" created="2011-11-25T14:39:03Z" id="2875019">yea, I will close it...
</comment><comment author="LindaPulickal" created="2016-12-01T18:03:08Z" id="264246618">Cool feature. Came useful to me right now :)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Quick documention fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/509</link><project id="" key="" /><description>http://www.elasticsearch.com/docs/elasticsearch/java_api/query_dsl/
AKA
https://github.com/elasticsearch/elasticsearch.github.com/blob/master/docs/elasticsearch/java_api/query_dsl/index.textile

QueryBuilder qb3 = filteredQuery(
should be 
QueryBuilder qb3 = filtered(
</description><key id="409709">509</key><summary>Quick documention fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ibrusic</reporter><labels /><created>2010-11-09T21:48:30Z</created><updated>2010-11-12T08:15:09Z</updated><resolved>2010-11-11T22:22:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-09T22:04:38Z" id="530199">Thanks for spotting that. I actually tried to maintain an "xxxQuery" notation in the builders (so it won't mix with xxxFilter). I have pushed a `filteredQuery` builder.
</comment><comment author="ibrusic" created="2010-11-11T14:22:50Z" id="534461">I assumed it was the code that was out of sync, but it is easier to tell somebody their documentation is wrong and not their code. :)
</comment><comment author="kimchy" created="2010-11-12T08:15:09Z" id="536329">At least with me, don't worry about telling me my code is wrong :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write RabbitMQ river responses to another queue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/508</link><project id="" key="" /><description>ES should write success/failure responses to another queue, so that the producer can re-push failed content or act accordingly.
</description><key id="409687">508</key><summary>Write RabbitMQ river responses to another queue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels /><created>2010-11-09T21:34:25Z</created><updated>2013-04-04T18:33:31Z</updated><resolved>2013-04-04T18:33:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:33:31Z" id="15915183">Not sure this is still relevant, but if so, should be reopened in the rabbitmq river issues list
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster health API should return RED status (on applicable levels) when an index has not recovered from the gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/507</link><project id="" key="" /><description>Cluster health API should return RED status (on applicable levels) when an index has not recovered from the gateway
</description><key id="409413">507</key><summary>Cluster health API should return RED status (on applicable levels) when an index has not recovered from the gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-09T19:24:54Z</created><updated>2010-11-09T19:25:36Z</updated><resolved>2010-11-09T19:25:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-09T19:25:36Z" id="529755">Cluster health API should return RED status (on applicable levels) when an index has not recovered from the gateway, closed by 4c6af6afa5793a5d4ac17e669b6346f0825971d1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping:  add threshold to _source field compression </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/506</link><project id="" key="" /><description>`_source` field can be compressed, but should have a minimum threshold at which compression kicks in, otherwise the field remains uncompressed.

The threshold can be set using `compress_threshold` parameter, which accepts a byte size value (`100b`, `10kb`, `5g`). The `compress_threshold` can be set without explicitly setting `compress` to `true`.

Also, it allows for smart mapper merging allowing to change the value on the fly.
</description><key id="409212">506</key><summary>Mapping:  add threshold to _source field compression </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-09T17:46:08Z</created><updated>2010-11-10T19:04:05Z</updated><resolved>2010-11-10T19:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-10T19:04:05Z" id="532507">apping: add threshold to _source field compression, closed by 70a0e110d4dc5cb05410c85b8ca3c3925aafcd82.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add maximum token count for tokenized fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/505</link><project id="" key="" /><description>Add a maximum token count for tokenized fields so that they are indexed up until N tokens and then the rest is discarded.  This is a typical use case where some people consider the first part of a document to contain the important aspects, and the rest might be repetition.  But, there are the opposite case where everything is important.  So allow the truncation for the tokenized terms.  If not by token count, then by size (depth into the source material that the tokenizer is reading from).

Note, this is not for the stored version, that should be set independently by size.
</description><key id="409102">505</key><summary>Add maximum token count for tokenized fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T16:55:00Z</created><updated>2013-04-04T18:33:50Z</updated><resolved>2013-04-04T18:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:33:50Z" id="15915204">Done, with the truncate token filter
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a Maximum size for attachements (reject on failure)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/504</link><project id="" key="" /><description>For an attachment field, specify a maximum allowed attachment size.  Attachments bigger than the size can result in error, or silent ignoring of the attachment (option?  if not just go with error)
</description><key id="409096">504</key><summary>Add a Maximum size for attachements (reject on failure)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T16:53:00Z</created><updated>2013-08-09T11:17:24Z</updated><resolved>2013-08-09T11:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-10T14:04:56Z" id="19200383">What about setting `http.max_content_length`? This does not reflect the size of one document, but might help already, if you add some more space for the rest of your document.

if you still need a complete implementation, please create an issue at https://github.com/elasticsearch/elasticsearch-mapper-attachments/issues
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a maximum size for stored fields (truncate)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/503</link><project id="" key="" /><description>For a stored field, add a max size at which they are truncated.  Of couse this can be done on the client, but it allows central enforcement of things to protect the index and search engine's health.
</description><key id="409092">503</key><summary>Add a maximum size for stored fields (truncate)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>feedback_needed</label></labels><created>2010-11-09T16:52:18Z</created><updated>2014-08-08T09:43:35Z</updated><resolved>2014-08-08T09:43:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T07:55:29Z" id="49404815">Hi @jaysonminard 

Apologies for the slow response. We've discussed this issue and are struggling to come up with a use case.  Do you still think this is a good idea, and if so, why?
</comment><comment author="clintongormley" created="2014-08-08T09:43:35Z" id="51582145">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow separation of indexing from query traffic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/502</link><project id="" key="" /><description>Maybe this exists, and I just haven't found it yet... 

A way to specify isolation between indexing and query traffic (i.e. query slaves).  Indexing at high rates should not hurt query performance, so the ability to have isolated query slaves updated at lesser intervals and indexers that are near-realtime (for the backend system to use) would be a nice split.
</description><key id="409060">502</key><summary>Allow separation of indexing from query traffic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T16:37:25Z</created><updated>2013-04-04T18:34:43Z</updated><resolved>2013-04-04T18:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-09T16:40:15Z" id="529324">Note, it is possible that the indexing nodes could still act as replicas for querying and allow fallback to them in a worse case; or not.

Parameters such as:
- Number of replicas of data
- Rate of replicate to query slaves
- Allow query fallback to indexers 

I guess for replicas N that are &gt; 1 you can have dedicated indexers.  So maybe it is simple a true/false flag to dedicate indexers, along with the rate of replication and the allowance to fallback to indexers for querying in emergencies.
</comment><comment author="clintongormley" created="2013-04-04T18:34:43Z" id="15915256">There are now separate threadpools for searching, indexing etc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add HTML and XML core types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/501</link><project id="" key="" /><description>I'm asking this as much as an open question than really as a request.

Would it make sense to bring HTML and XML datatypes into the system so that they can be handled specifically in special ways?  I.e. pick the right default analysis chain for HTML and XML right off the bat, or make it easier for the API to expose the results in document centric ways, or ?

Just thinking out loud on this one...
</description><key id="409040">501</key><summary>Add HTML and XML core types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T16:29:27Z</created><updated>2014-07-08T11:52:20Z</updated><resolved>2014-07-08T11:52:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2012-08-26T22:47:39Z" id="8040101">The underlying jackson library supports XML; YML support was recently activated in a change, it's likely that XML support would be just as easy. HTML5 has its own processing standard; it'd require a different library.
</comment><comment author="spinscale" created="2013-07-05T10:20:21Z" id="20511020">It should be pretty straight forward to create such mapping types as a plugin without the need to change anything of the elasticsearch core, regarding indexing.

Not sure what you mean with 'expose the results in document centric ways' though
</comment><comment author="clintongormley" created="2014-07-08T11:52:20Z" id="48326364">No discussion for over a year. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a variety constraint to querying and hit collection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/500</link><project id="" key="" /><description>Ok, this is a new one as not sure I've seen it in another product but it is a good alternative to just pure scoring and pushing docs to the top.

When searching, I should be able to ask that I get back a "variety" of results by type, or a field value and a min/max count of each.  So for example, I get the best ranked results but I want 5 books, 5 pictures, 5 songs, and 5 movies total rather than 15 books, 5 songs eating up my first page.  NOTE:  I am not asking that they be grouped together, but that there is a limit to how many of each can make that response to avoiding flooding out the other docs of various types (this is a perfect job for a VarietyConstrainedHitCollector which I'm sure doesn't exist).

Basically a variety constraint on what is returned.  

You can implement this client-side but it would be more efficient down in the server.  And doing multiple queries by types is wasteful as well and doesn't allow them to be scored together by relevancy.

I'm not sure yet what to do about paging or scrolling or even if those would be supported.  This is more of a first-page type of feature, but if you CAN solve paging and scrolling it would be interesting.  I'm thinking on those more now but recording this before I forget.

It's also similar to some searches we built in the past that show a summary page of the first 10 and last 10 of the sort order (show me the least and most expensive of matching items) which is a similar variety constraint and more efficiently done in the engine while it has things in memory; rather than doing follow-on queries.
</description><key id="408938">500</key><summary>Add a variety constraint to querying and hit collection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T15:34:19Z</created><updated>2014-05-23T14:23:30Z</updated><resolved>2014-05-23T14:23:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-02-09T21:43:37Z" id="755631">Duplicate of #256 !?

Solved via field collapsing / results grouping in Solr ... but not sure if for the distributed case too ...
</comment><comment author="clintongormley" created="2014-05-23T14:23:30Z" id="44014977">Closed by #6124
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-select facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/499</link><project id="" key="" /><description>Current facets are single select, in that once a facet is applied you cannot see the counts of the other values within the same term to be able to apply another value (which would be OR instead of AND against the existing filters).

Solr implements this by creating a group, or "tag" on the actions that means that they exclude each other from interfering with the counts. 

This allows facet selection for things like...

&gt; show me all "Red" shoes as I see there are 20 of those
&gt; oh, and show me "Blue" shoes as well (which had 5 before and after "Red" being selected)
&gt; great, now I have 25 shoes to look at!
</description><key id="408920">499</key><summary>Multi-select facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T15:22:51Z</created><updated>2013-04-04T18:36:18Z</updated><resolved>2013-04-04T18:36:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-09T15:24:36Z" id="529083">Oh, the reason I mention the Solr tags is that it is flexible in that the facet counting and facet value filtering do not have to be so perfectly lined up to have them work with each other and could actually cross fields if you desired (I can't think of that use case, but I'm sure it could exist).

Date ranges are another area where you do a histogram of the dates, and let them select a subset, but still want the overall histogram present even after so that they can reselect a broader date range again.
</comment><comment author="kimchy" created="2010-11-15T17:19:58Z" id="542702">I am not sure I follow exactly what you mean. Currently, in elasticsearch, you can specify that a facet will run globally (not bounded by the current query), and bounded by the current query (i.e. it will aggregate results only for documents matching the search query). Every facet type in elasticsearch accepts a filter (one of the many options you have in the query dsl) to further restrict the docs it will run on. Is that what you are after?
</comment><comment author="karussell" created="2010-12-20T22:05:30Z" id="625869">(WARN: I'm not aware of the elasticsearch stuff only solr)

I think, he means the following: when I do a filter ala fieldA:value then all other facet queries (fieldA:value2, fieldA:value3) will get a count of zero. For simplicity we assume that that field is not a multivalue field. 

This 'hiding' behaviour is ok but a lot of times you still want to display the counts of the other facets as well. So you'll have to use ugly !ex and !tag stuff in your query. This is called local parameter (hack ;-))

See http://wiki.apache.org/solr/SimpleFacetParameters#Multi-Select_Faceting_and_LocalParams
</comment><comment author="karussell" created="2011-01-24T14:07:33Z" id="702626">@kimchy: now that I understand better how ES works I would like to ask if the following is possible.

When I would use normal range/date facets with ES and I 'select' (use the facet as filter) one of the entries then the other counts will disappear. That means the user needs to 'go back' to again see the counts in all ranges.  When I would set them 'global' this would be perfect

EXCEPT that now even if I filter e.g. for language:english the range facets won't change, which can be fixed using additional filters but that is suboptimal IMHO, because every facet which needs this feature would need those additional filters.

Please, see jetwick.com where the solr tag hack is implemented.

Or do I need to use a 'scope' variable for the range facets in ES and another one for the other filters? Or is this 'grouping' not possible yet in ES?
</comment><comment author="kimchy" created="2011-01-25T21:49:27Z" id="707412">Still not sure I understand, does "moving" the relevant facets to be a global facet (scope == global") and not a "main" facet is the problem? Maybe I can introduce something like a "search filter", that will only get applied for the query results, but will not apply to facets (or you can choose if it gets applied to them or not)?
</comment><comment author="kimchy" created="2011-01-26T10:58:07Z" id="708880">I have just pushed #650, I think it does what you are after.
</comment><comment author="karussell" created="2011-01-26T10:58:18Z" id="708881">Yes, I think we are talking about the same :)

It would be handy when one can say: this filter gets applied to only _some_ facets. E.g. when I'm using the language filter I can specify that it wouldn't have an effect on the language facets but it will have an effect on the date facets. The same for the date filter.

But all filters (in my use case) should have an effect on the search results...

I'm not sure if there are use cases to make it more complicated: defining exclusion groups etc like it is possible with the "!tag !ex" in Solr. You _tag_ facets with names and when filtering you can _ex_clude them ...
</comment><comment author="kimchy" created="2011-01-26T11:19:29Z" id="708923">@karussell: have you seen #650? I think that that with this one, and the ability to configure a custom filter on a facet (and the ability to control if a facet is bounded by the query or run globally), it gives you all the possible options you want. 
</comment><comment author="karussell" created="2011-01-26T21:00:03Z" id="710603">@kimchy sorry, didn't notived that one. and yes: you can now close this here! Thanks a lot I'll try that!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Time limited search saves lives</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/498</link><project id="" key="" /><description>There are common cases where you have a tuned engine and at your 98 percentile you have great searches, but those last few percent can run on forever!  Time limited search comes to the rescue in returning whatever results it has at the moment along with a flag indicating it was incomplete (debug info can say which shards fail to return in time).  So instead of 2,492,222 results you might end up with 2,300,190 but your app survives to query another day.

Lucene has a time limited hit collector for this purpose, so its ready to go!  (its there since at least 2.4 http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/TimeLimitedCollector.html)

It can be configured at the index level for a default, and overridden in a specific query (i.e. allow more time for a more complicated query).
</description><key id="408889">498</key><summary>Time limited search saves lives</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T15:06:25Z</created><updated>2013-04-04T18:36:26Z</updated><resolved>2013-04-04T18:36:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-12T08:40:55Z" id="536355">Its already provided on the search API level, just pass `timeout` in the URL (for REST request) with a time value (like `5m` for 5 minutes).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add higher level attributes to field mappings to prevent needing knowledge of low-level settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/497</link><project id="" key="" /><description>Right now, to create a field that is facetable, you need to know it should not be analyzed.  You could also just have an attribute "facetable" or "sortable" which helps users know more outright that their field can be used for those actions without having to know all of the other settings that might be involved in dealing with those cases.

Same for term_vector, omit_norms, and omit_term_freq_and_positions -- what are the higher level cases we could define instead of having to know the nuts and bolts of how the engine works to solve those cases?  like "allowPositionSearches" for making sure positions are there. ... you get the idea

Again, there are plenty of low level lucene imlpementations and engines out there.  Let's make this the easy-to-use-without-having-to-be-a-lucene-expert version, but still as powerful!
</description><key id="408866">497</key><summary>Add higher level attributes to field mappings to prevent needing knowledge of low-level settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T14:52:39Z</created><updated>2013-04-05T10:13:19Z</updated><resolved>2013-04-05T10:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-09T14:53:32Z" id="529015">see #496 for related reasons
</comment><comment author="apatrida" created="2010-11-09T15:15:35Z" id="529067">highlighting is another one that is hit by this, look at issue #69 where to highlight you must have...

&gt; Note, in order to highlight, the field in question must be stored and have termVector of with_positions_offsets.

Great, another case where someone can get it wrong, but with "highlightable" as an attribute on a mapping, these can be guaranteed to be turned on.
</comment><comment author="clintongormley" created="2013-04-05T10:13:19Z" id="15948192">I had a good long think about this issue last night. On one side, i like it, on the other I don't :)  Elasticsearch aims to be easy to use, but still flexible and powerful enough to be really awesome.  There is a continuum between configurability and ease of use. And assumptions that we impose in order to be easy to use can limit the amount of flexibility we can provide.

We don't want to be Lucene-level configurable, because that is too low level. On the other hand, we don't want to be like those photo-editing apps that you used to get with digital cameras.  You know, the ones with the big buttons for idiots.

You opened this issue two years ago and since then, I think ES has chosen its level.  Your suggestions are great, but are things that can be implemented easily in an application on top of ES, which I think is the right place for them now.

I'm trying to clear out old tickets that are no longer relevant, and my temptation is to keep this one open. But honestly, I don't think it'll ever be implemented, so I'm closing it :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add query information to the mapping system</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/496</link><project id="" key="" /><description>Like most search systems, the focus in the ES schema is on indexing.

Well, the search system is about SEARCHING too!  

The schema should be able to specify query type information such as:
- sortable (and default single sort order, which could be "score")
- facetable (and default facet fields)
- facet style (single select vs. multi select)
- displayable (client-side hint)
- internal (don't hand out past our internal walls)
- searchable / filterable
- range filterable
- ...

Which can be used as a default for queries, or overridden in the query itself.  Some are hard fast rules (sorry, but you just aren't allowed to search this!)

It also adds a protection that the index isn't used in ways that are dangerous (i.e. using unexpected additional facet fields or sort orders could cause memory to exceed the allocated VM size that the system was tuned for).  

If it is ok to define the indexing properties in the schema, it should be perfectly fine to express the query side as well.  Otherwise, a whole other schema implemented in parallel (either hard coded -- ICK!, or dynamic and rebuilt by everyone using ES).

You add this, and I can build a generic test UI for ES that does a very nice job of using this schema to handle searching on any arbitrary ES index.
</description><key id="408742">496</key><summary>Add query information to the mapping system</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T13:38:07Z</created><updated>2013-04-05T13:16:44Z</updated><resolved>2013-04-05T13:16:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-09T14:53:11Z" id="529012">see #497 for related reasons
</comment><comment author="apatrida" created="2010-11-09T15:14:26Z" id="529063">add "highlightable" :)
</comment><comment author="clintongormley" created="2013-04-05T13:16:44Z" id="15954868">Closing this for the same reasons as #497 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow custom data to be added to the mapping system</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/495</link><project id="" key="" /><description>Currently, you can add mappings for fields, basically building a schema where you want to override the default behaviour.

But can you add custom data to that?  Since you have a central store to describe your documents, it would be nice if you could add metadata that helps the application use that data as well.  Otherwise they have to store it in a similar schema in another system for that purpose.  

By adding more data into the mappings, you can write more dynamic clients that can consume the index without having to have so much knowledge about every field but can read it from the schema as needed.

I'm not too caught up on this one, was just thinking out loud.
</description><key id="408733">495</key><summary>Allow custom data to be added to the mapping system</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-09T13:31:00Z</created><updated>2011-01-29T18:05:02Z</updated><resolved>2011-01-29T18:05:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-09T14:54:21Z" id="529018">Its already there: http://www.elasticsearch.com/docs/elasticsearch/mapping/attributes/.
</comment><comment author="apatrida" created="2010-11-09T14:59:18Z" id="529032">Perfect, if only I would read every page of the docs before I start playing I'd be better off.  But then again, I'd get less done...  Thanks!
</comment><comment author="kimchy" created="2011-01-29T18:05:02Z" id="721034">closing, its there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parsing a deeply nested json might fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/494</link><project id="" key="" /><description>Increasing the path buffer should be done after the index is incremented, not before, since then the remove might get out of bounds.
</description><key id="408332">494</key><summary>Parsing a deeply nested json might fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.0</label></labels><created>2010-11-09T07:46:18Z</created><updated>2010-11-09T08:26:24Z</updated><resolved>2010-11-09T08:26:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-09T08:26:24Z" id="528315">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow invalid values to be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/493</link><project id="" key="" /><description>For datatypes that expect a specific format, allow a variation of the type that is a "softie" in that it allows bad things to just be ignored.  This is important for unclean data, and although other work may allow you to write a document processing plugin to ES that can clean data (i.e. parse human readable dates into a standard form), it is sometimes likely you can have some rough data that you don't want to kill your indexing.  Especially for fields that are not that important but you want a best attempt to insert the document.

You then could also consider marking documents that have validation errors so they could be later rescanned and reindexed given the stored JSON when a cleaner is added that would possibly resolve the problem.  For example, mark the document as validation error for field XX and later search for those docs and ask the system "reindex document &lt;id&gt;" from its own stored form.

So 3 things here:
- be able to mark a field mapping to allow invalid data to be ignored (discarded) on a field by field basis
- have the system mark the record as not passing validation at a per-field level
- be able to ask the system to reindex by ID using _stored JSON or even by reindex-by-query (#492)
</description><key id="406003">493</key><summary>Allow invalid values to be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-08T03:22:19Z</created><updated>2013-04-04T18:38:02Z</updated><resolved>2013-04-04T18:38:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-09T12:17:23Z" id="528668">Currently, the only field type that qualifies this is the `date` type field. We can add something like that to it, and in case its fails to parse, just don't index it. You can always query for documents that don't have this field and then handle it? If the actual value still needs to be stored as well, it can be a multi_field mapping, one with plane `string` type, and one with `date` in this "soft" format.
</comment><comment author="apatrida" created="2010-11-09T16:23:32Z" id="529253">Numeric fields that don't parse wouldn't qualify (or are they handled in creating the JSON or previous?)
</comment><comment author="kimchy" created="2010-11-09T17:04:26Z" id="529406">Yea, json already handles it, since it has native types for numbers.
</comment><comment author="apatrida" created="2010-11-09T17:46:49Z" id="529520">dandy.
</comment><comment author="clintongormley" created="2013-04-04T18:38:02Z" id="15915456">Also, there is now a flag for ignoring malformed values
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex from _source by document ID or Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/492</link><project id="" key="" /><description>Be able to ask the system to reindex from the saved JSON by document ID or query.  This is useful once we have ES style plugins for manipulating documents that might later change and therefore cause you to want to reindex some set of documents.
#490 and #491 would let you query by a set of documents indexed before the required change.

If you are going to store the JSON, you can take advantage of that by reindex requests.

This might also allow the system to handle schema changes in the future more automatically by reindexing to the new analyzer over time in batch.
</description><key id="406002">492</key><summary>Reindex from _source by document ID or Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>:Reindex API</label><label>adoptme</label><label>high hanging fruit</label><label>stalled</label></labels><created>2010-11-08T03:22:02Z</created><updated>2016-02-28T21:53:32Z</updated><resolved>2016-01-13T16:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-02-09T21:46:42Z" id="755649">Similar to #605
</comment><comment author="clintongormley" created="2014-07-18T07:52:45Z" id="49404629">Blocked by #6914
</comment><comment author="eskibars" created="2016-01-13T16:27:46Z" id="171350598">I'm closing this in favor of the combination of #15201 / #2230 / #15125 where we've made some significant progress
</comment><comment author="apatrida" created="2016-02-17T18:59:55Z" id="185349902">Great, I've been waiting a bit for this, glad to see it coming along!  Nice work, I read the log of discussions in #15125, very nice.
</comment><comment author="clintongormley" created="2016-02-28T21:53:32Z" id="189951446">@apatrida Just a bit :) Issue #492!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a auto-timestamp field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/491</link><project id="" key="" /><description>Documents should be able to have a timestamp field added to them on the fly.  I'm not sure if this should be in the "stored" JSON or rather record meta-data, but I think as meta-data outside the record it lets you do interesting things based on index date.
</description><key id="405996">491</key><summary>Add a auto-timestamp field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-08T03:18:33Z</created><updated>2014-04-25T14:21:02Z</updated><resolved>2011-08-29T15:02:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-08T03:19:41Z" id="525081">see #490 for alternative, although both are useful
</comment><comment author="kimchy" created="2010-11-08T08:57:55Z" id="525489">I was thinking about this feature for some time. The main problem here is the distributed nature, meaning that you will need to get the clocks sync'ed since shards can relocate. It can be provided as an option, either to specify it externally, or for it to be generated by ES.

Wasn't pushing this feature ahead since it can always be implemented outside of ES, just add a timestamp field to the json doc you index.
</comment><comment author="kimchy" created="2011-08-29T15:02:53Z" id="1931744">Implemented in #1285.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a version clock/identifier to documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/490</link><project id="" key="" /><description>The system can manage a version clock/identifier for documents, so every new insert increases the version by 1 across the entire index (or system).  This is useful for reindexing, soft purge-reloads (process add/updates, delete all with a version &lt; N).

You must be able to query the index to ask its current value for the version number.
</description><key id="405994">490</key><summary>Add a version clock/identifier to documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-08T03:16:47Z</created><updated>2014-04-25T14:21:02Z</updated><resolved>2011-05-21T20:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-08T03:18:58Z" id="525078">a system-wide time-stamp is also an appropriate answer here, although time can be mismanaged more than just an ever-increasing integer for some of the intended usages. See #491 for time-stamp, but I still think a vector-clock type answer is useful in different ways.
</comment><comment author="apatrida" created="2010-11-08T03:19:27Z" id="525080">did I close this on accident, please re-open.
</comment><comment author="kimchy" created="2010-11-09T12:21:10Z" id="528675">Its certainly possible, and then the question is what level the version works on, it can be on a "resource" level (a document with type and id), index leve, or cluster wide level.

All are not simple to implement. The resource level means that a real time mapping of id to version need to exists (either in memory, or in the index, which then requires real time aspect) and then handle concurrency.

Index level and cluster wide level incremental version means that it needs to be maintained across the cluster (the versioning)...
</comment><comment author="apatrida" created="2010-11-09T12:54:08Z" id="528739">Yep, it is hard to do, and very hard for the user to implement.  It is "clustering magic" that only the system can do really well...  after the developers go through pain and suffering to build it.  It's the cluster-wide vector-clock style problem, but well discussed in the world (I think Cassander just did it, people do it on ZooKeeper, and so on).  

You have the choice of "perfect accuracy" or getting it somewhat close.  A system-wide timestamp is another approach where you just have to be close.  You can state the guarantee of the version number, or of the timestamp and people work around the level you can provide.  A bit loose at first, but tighter later.  Cassandra just went through this, not sure what their approach was, and ZooKeeper folks of course do it.  But I'm not saying their approaches are right/wrong or desirable (knowing not what they did).

At the index level might be fine  or System wide would be fine.  I can't think of a reason (for my use cases) why either would be a problem.  And a bit of loose accuracy isn't always a problem as you can always ask for a "loose idea of what the current number is" or a "sync them all up and give me an accurate number for sure" when you request it from outside the cluster.  And inside the cluster it can be basically a reasonable approximate (i.e. handing out blocks to each node that consume them, but on sync-up they may discard their blocks to get somewhat back in order again; similar to how I think Oracle does sequence numbers in that they aren't always contiguous but are basically in order).

So in short, timestamps might be easier (let the sys admin maintain similar clocks on all the servers, they seem to be good at that) and a version number can come later which might help with synchronization and balancing nodes and other fun things if those ever come about.
</comment><comment author="ppearcy" created="2010-11-12T18:45:00Z" id="537533">FYI, as ES should not be your primary data store, it makes a good amount of sense to have this value propagated in from whatever your primary data store is. 

Granted, if your primary store doesn't have this feature, you're out of luck, but then again, if your primary data store doesn't have this, you probably don't need it in your ES setup.

(disclosure: I am not an ES developer and this is just my personal thoughts)

Thanks,
Paul
</comment><comment author="karussell" created="2011-05-21T20:04:33Z" id="1215376">This can be closed as of #594 ?
</comment><comment author="kimchy" created="2011-05-21T20:25:44Z" id="1215437">Right, it can be closed. Also, @ppearcy request is solved with `version_type` set to `external`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Making language a first-class citizen</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/489</link><project id="" key="" /><description>Ok, so the index and document type are both first class citizens, but language needs to be right up there at the top as well.

In multi-lingual applications, documents tend to have the same structure, but require different analyzers for each language type.  So a few things should change to support this:
- Field or Type mappings to analyzers should be one-to-many, where you may have a default analyzer and a language specific analyzer for each.
- Scoping of queries can be by language, or _all
- Physical indexes should be divided by language so that each ES "index" basically has a subindex by language so that documents on that language use the appropriate analyzers consistently for indexing and querying

Language handling is a typical problem, that is always ignored and treated with hacks or patterns of use...  instead let's get it right for ES.
</description><key id="405846">489</key><summary>Making language a first-class citizen</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2010-11-08T00:23:40Z</created><updated>2014-07-08T11:49:51Z</updated><resolved>2014-07-08T11:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2010-11-08T00:43:57Z" id="524943">#485 and #487 provide a way to handle this, but I would still rather see it pushed to the top.  

for example, index, type and id are treated as very important:

```
host:port/index/type/id
```

and language should fit into that top level, as it isn't always in the document (could be, but why?

A change like this is better early rather than late, as unless it is placed at the end it can be breaking, or if you detect a language code (ISO 2 or 3 chars) you would have to differentiate it in some other way.

The "hack" way of doing this is to create indexes per language, such as

```
host:port/index-en/
```

And then copy all of your field/type mappings across with the aliases meaning something new.
</comment><comment author="apatrida" created="2010-11-08T00:46:31Z" id="524948">Actually, #485 does not handle this other than for cases such as knowing a doc has a special indexing case that is compatible with a common query analyzer.  But languages are rarely this.  With #485 you can index the document differently but then your search query would be analyzed how?  You need each set of language specific docs to have the query analyzed for that set specifically during a query.

So #485 helps with some indexing cases, but isn't an answer for language specific collections.  
</comment><comment author="otisg" created="2010-11-08T01:54:40Z" id="525003">+1 (or how to make Github email you new comments on this issue)
</comment><comment author="apatrida" created="2010-11-08T04:04:01Z" id="525130">Hey Otis, I wouldn't mind the Github "watch" command, and also one to check my 2AM bad gramar when typing new issues... sheesh.
</comment><comment author="kimchy" created="2010-11-08T08:54:31Z" id="525483">Actually, I have thought about it and I see the previous issue as an enabled for it (allowing to choose an analyzer based on a json doc field). 

I don't necessarily agree that the way to model language support is to have different index for each language. Its one option, certainly, but you should also be able to handle several languages within the same index.

In order to support (both) options, I was thinking that the following features are needed:
- Have an `_lang` field, that will automatically detect the language of the doc based on another field(s) or `_all`.  That `_lang` field can then be used to control the analyzer that will be used simply by pointing the new `_analyzer` mapping to it. Also, this field can be used to filter docs based on specific languages in a query.
- For query time, it really only applies to `query_string` and `field` queries, which use an analyzer. You can already control the analyzer that will be used in the query DSL.  I was thinking that these queries will also support a `user_query` parameter, which will also go through the same language detection mechanism, and the analyzer wil be chosen based on that.
- The language detection will also be provided as an api (give it text, and it will guess the language). This can then be used to have a different index per lang for example.
</comment><comment author="apatrida" created="2010-11-08T12:54:59Z" id="525859">So instead of stating "index per language" I should really say "collection per language."

Your solution above is great for a user searching in one language for a matching collection of documents in a matching language, or all documents by using one query analyzer for all collections assuming there is some base compatibility between analyzers (allowing that they may vary in areas such as stop words and stemming but that you can still go for an exact match).

(by the way, stop words is not a great solution and we should look at bringing common grams or other variations of those into ES for larger document sets)

So just keep in mind a few possibilities:
- User locales usually can indicate their language preference, but doesn't always match the language of their query
- Language of the query may not always match the language of the documents they intend to search (i.e. a English researching talking about French poetry)
- Language may not be accurately detectable for shorter queries (the longer the better)
- Language of the query isn't always useful when they are searching for entities like people's names within the fulltext or fields
- Regardless of language of the query, you might want to search across one or more of the language collections (but possibly boost documents with a matching language, but allowing the others in as well for higher recall)
- Mixed language documents (i.e. a English researching talking about French poetry) -- which are not easy to solve anyway, just mentioning for completeness

Now, these issues mentioned above don't negate your solution, but could help round it all off into being one of the first engines with good language support that doesn't feel bolted on as an after-though, or dealt with as a work-around.
</comment><comment author="kimchy" created="2010-11-08T13:17:19Z" id="525892">Right, all the cases you suggested are relevant. What I try with ES is to build the engine that can support different usages types, and then build features that make more features dead easy. For example, in my purposed solution, the only new feature in ES is language detection, which if the client is doing, then all features are there already for ES. Thats why I pushed the `_anlayzer` feature fast, without it, it would have been impossible to implement something like what I suggested by the user.

Regarding your points, are you then raising them in benefit of the index per language solution? You can easily implement it now, without additional features from ES. Just create an index per language, you can already search across indices, and define a different search analyzer per index, which will cause the query string to be analyzed using each index own language analyzer.

What I am after is that major (breaking?) feature that ES is missing for better language support (even the `_analyzer` feature was not a major one, as it "fell" nicely into ES model, as expected :) since I had it in compass already).
</comment><comment author="apatrida" created="2010-11-08T13:38:20Z" id="525944">I have a good set of mixed language documents and a good use case to try them on with ES, let me see how it plays out under real world use.  Maybe Otis has thoughts being a linguistic guy himself?
</comment><comment author="apatrida" created="2010-11-08T13:43:04Z" id="525955">After saying that, I realize that it still bothers me that you have to patch together your own story for multiple languages.  That you have a nice way of mapping types to analyzers, then have to override them completely when you go multi-lingual.  Why?  The type system is incomplete (to me)...

I will most likely have one analyzer stack for say my Document Title, another for Document Author, another for the Abstract and Body of the document and so on.  Therefore for each field I have different analysis.  Now for multi-lingual sets of documents, I have to override how many things for each language?  I think it is an amazing feature to say that my type fo document title (say called "bibtitle") can have a analyzer definition for each language under that type definition.  The same for each of the other differing fields.  

Then I can forget about the language aspect when sending documents and take that out of the business logic of the code (where you are forcing me to consider it down to the level of selecting analyzers).  And when querying I can forget about it again and let my linguistic folks tune the engine without my code having to think about it when passing in a well constructed query.

I think we need a step more in ES to make languages easy to use and transparent outside the definition of the index.  It is a big step, but a place that ES could once again show it cares to make it incredibly easy.
</comment><comment author="apatrida" created="2010-11-08T13:46:05Z" id="525961">Your previous solutions would then be ways of overriding this nice behavior (from above) rather than as the main mechanism for doing a fairly common task (being multi-lingual).  

Remember, that once you get out of the simple search cases, most search systems start to hit very detailed analyzer stacks and for many languages at once.  The default analyzers can rarely last past the first few days of a new project, and it is common that field after field will see different analysis.  

Then toss in dismax and we have a workable system!
</comment><comment author="kimchy" created="2010-11-08T17:19:44Z" id="526575">I am not sure I agree that the best way to model your searchable document model is to have fields that can have different analyzers. Instead, a document will usually consist of a specific language. This option is better in terms of search, since you don't need to dismax your query (which is expensive), or deciding which analyzer to use on which field.

Note that you can still define explicit field level analyzers, but the "default" analyzer for a document can be the language based one.

Then, its just a matter of specifying that analyzer. It can either be done by creating an index per language, in which case the default analyzer for that index will be the language specific one (defined in the settings for that index, on index creation). Or, it can be based on `_analyzer` with its values (like `lang_en`, `lang_de`, ...) will map to Lucene language specific analyzers.

You can do both now with elasticsearch. The language detection option is a nice add on (which can be done on the client side as well, as far as I know, for example, perl has the best language detection lib around). But, for example, you might already know the language in question (maybe because it has a mime type, or many different ways to do it), and then its just a matter of suffixing the index name with the language code or setting the language in the `_analyzer`.

When searching, it depends on which route your took. If you have an index per language, you can execute a search against a single index (based on a specific language), or query all indices. Because each index has its default analyzer set to the specific document, the query string will automatically be analyzed using it when the search is executed on that index (shards).

If using the `_analyzer` option, and without automatic language detection, you can also specify the analyzer for the `query_string` or `field` analyzer.

Thats the main two options that I see to use multi language, and both are possible with elasticsearch. Everything on top of it is sugar coating, which is very important, but the main output here is that elasticsearch has the constructs to support it, and that the sugar coating is _optional_.

Some examples of sugar coating can be automatic language detection, and then using it to control indexing and search (as explained before). It can automatically control the `_analyzer` value, but, the most important thing is that its _optional_, since you, the user using elasticsearch, might have better knowledge of the language of the document.
</comment><comment author="apatrida" created="2010-11-09T12:18:41Z" id="528672">&gt; I am not sure I agree that the best way to model your searchable document model is
&gt; to have fields that can have different analyzers. Instead, a document will usually 
&gt; consist of a specific language.

A document is usually one language, sure.  But not all fields are treated equal.  For example, stemming an "Author" field may be undesirable, and for a "Title" field you might want to not eliminate the same set of stop words as you would from full text, and for a "PartNo" field you might want to tokenize based on rules of that identifier and not just on what is typically considered whitespace, and for a "FullText" field you might use common grams (or something better) instead of stop word removal, and on ...

Sure, you may not have a different analyzer for every field, but you might have 5-6 analyzers on a decently structured document.

If you make that hard, you then make it hard to have really good precision or the right amount of recall.

&gt; ...and then its just a matter of suffixing the index name with the language code...

Assuming you want to search one language at a time.  You can actually use one query to search all languages, but then it has to be analyzed per collection of documents (matching each language).  And maybe that is possible now, does searching a list of indexes have each one do its own analysis?  (I would think so, but figured I should ask).

&gt; Everything on top of it is sugar coating...

Yes, and I think ES can stand out for having that sugar coating here since most other search engines give the same answer "oh, it is possible, but you do all the work" which means a large body of programmers repeating the exact same code, or work around, or making mistakes if they have no clue that language matters as much as it does.  In fact if you add this "sugar", we in the community could then add a bunch of great default analyzers and field types for common cases and they would just work if someone said "this is an English" or "this is a French" or "this is a Japanese" document.  We could make the ES search community that much better by having support for languages right out of the box if only they just say "this document is language xx."  Otherwise, they have to learn about analyzers, have to pick from a list and maybe get it right, probably not.

&gt; Some examples of sugar coating can be automatic language detection

I think this is less valuable as you probably do know the language of your document, and likely the language of the user.  But it is useful in some cases.  Right now I'm asking for the layer beneath that:  have a way to express the language of the document that allows type/analyzer mapping to pick a language specific analyzer for the given field.  Then we have good default analyzers for each language we happen to collect good analyzers for and a good set of field types for things people will run into but have no idea how to handle well unless they have been doing search for years.

&gt; This option is better in terms of search, since you don't need to dismax your query (which is expensive)

I can't let you off the hook for this one either :)  Dismax is "more" expensive, but it is many times worth the price.  Let me price that one out for myself as I can still have Dismax while searching 500 million records in subsecond times if I want to...  Dismax does not mean "slow," it means "slower." 
</comment><comment author="kimchy" created="2010-11-09T12:25:20Z" id="528684">ok, I admit I got confused now. What are you suggesting that ES is missing and could make it handle languages better? More analyzers outside of the default Lucene ones? Any explicit feature?
</comment><comment author="apatrida" created="2010-11-09T13:09:10Z" id="528767">Things you can do for us:
- make sure we can create new field types beyond the basics ourselves (done!)
- make a definition for analyzer be able to include variations by language, as well as a default as a fallback (i.e. sometimes you fallback to one language, sometimes you have a specific fallback not attached to a language)
- set a default language for an index
- have a standard way of defining the language of a document just as you have for the type and ID of the document and use this in determining the appropriate analyzer
- when overriding the analyzer at a field level or document level you can express the analyzer name, but also force it to a language as well with analyzer name + language identifier (i.e. in Canada you typically have Title_en and Title_fr in the same document and want to force those into those specific languages, but other times you want the language of the document to decide).
- embrace the fact that dismax has value

And we the community can...
- give you a better set of analyzers for different languages
- give you a better set of analyzers for other types of content typically found in fields
- give you better concepts than stopword removal
- give you better stemmers
- help setup a strong list of default out-of-the-box mappings for ES for many languages

And yes, the default Lucene analyzers and definitions are underwhelming and things like stopwords removal are fairly outdated, and the stemmers are sloppy and overly agressive (which is fine if you have ways for the system to know and handle that, otherwise it isn't) and should not be used all over the place.
</comment><comment author="apatrida" created="2010-11-09T13:26:16Z" id="528811">While I'm listing it all here in this "restructure how analyzers are connected to fields" topic that I've gotten myself into...  You might as well consider the full package of ideas!

(note:  this could be there, I just haven't seen it yet)

Things like analyzed/not_analyzed, term_vector, omit_norms, omit_term_freq_and_positions, analyzer (+index/search), include_in_all ...  Can those be defaulted in a type?  And can you create quick user-defined types based on these settings?  i.e. variations of String.  Creating type mappings with all of the settings would make it easy to create types such as:
- identifier (ISBN, ISSN, ...)
- bibliographic title
- names
- facet
- ...

Something to think on...  extending the type system so that you can share more of these definitions across fields and across indexes even.  

Would be nice to then be able for people to contribute a good base type system for e-commerce, or another good one for document style searching, or another good one for bibliographic metadata records, etc.
</comment><comment author="apatrida" created="2010-11-09T16:25:01Z" id="529258">I guess my summary still doesn't cover that language sharding or sub-index assignment which needs to vary if the analyzers vary -- but that is beneath the user's view so can just be decided with what works best as the mechanism.
</comment><comment author="ghost" created="2012-08-05T03:35:35Z" id="7507444">Is there anything new regarding this? Has language detection of POSTed text been implemented?
</comment><comment author="clintongormley" created="2014-07-08T11:49:51Z" id="48326170">No discussion for years. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get Action: Allow to pass refresh forcing getting latest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/488</link><project id="" key="" /><description>The `refresh` parameter can be set to `true` (or `1`) to force getting latest.
</description><key id="405683">488</key><summary>Get Action: Allow to pass refresh forcing getting latest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-07T21:50:02Z</created><updated>2010-11-07T21:50:55Z</updated><resolved>2010-11-07T21:50:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-07T21:50:55Z" id="524666">Get Action: Allow to pass refresh forcing getting latest, closed by c095d72439c7403ad90b68449eed86ea417edccb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyzer selection based on json doc field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/487</link><project id="" key="" /><description>Provide a way for the analyzer to be selectable at indexing/query time, as Shay suggested, drive the analyzer used based on a field in the json doc.

A concrete use case is to have many languages, with specific stop words for each of them of course, but with the same document structure for all of them. Right now there is no way to select the analyzer at indexing time, so you end up with multiple document mappings for the same structure but with different analyzers on each.
</description><key id="405419">487</key><summary>Analyzer selection based on json doc field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sebaes</reporter><labels /><created>2010-11-07T18:25:32Z</created><updated>2010-11-08T00:47:47Z</updated><resolved>2010-11-08T03:02:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-07T19:02:17Z" id="524365">Already implemented :): #485.
</comment><comment author="apatrida" created="2010-11-08T00:47:47Z" id="524950">#485 does not, see #489...  Indexing the docs differently at index time does not align them with the correct analysis for query time.  You CAN intermix documents in the same set with different analysis at indexing that is compatible with a common query analyzer; but good linguistics support will rarely be that simple.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Allow to alias analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/486</link><project id="" key="" /><description>Allow to alias analyzers allowing to lookup the same analyzer using several possible names. For example:

```
index :
  analysis :
    analyzer :
      standard :
        alias: [alias1, alias2]
        type : standard
        stopwords : [test1, test2, test3]
```

Then the `standard` analyzer can also be referenced (where applicable) using the `alias1` and `alias2` names.
</description><key id="405395">486</key><summary>Analysis: Allow to alias analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-07T18:10:07Z</created><updated>2010-11-07T18:11:04Z</updated><resolved>2010-11-07T18:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-07T18:11:04Z" id="524257">Analysis: Allow to alias analyzers, closed by e51523385d38a29786e757c44547f6a835d3f20a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: An analyzer mapper allowing to control the index analyzer of a document based on a document field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/485</link><project id="" key="" /><description>The `_analyzer` mapping allows to use a document field property as the name of the analyzer that will be used to index the document. The analyzer will be used for any field that does not explicitly defines an `analyzer` or `index_analyzer`.

Here is a sample mapping:

```
{
    "type1" : {
        "_analyzer" : {
            "path" : "my_field"
        }
    }
}
```

The above will use the value of the `my_field` to lookup an analyzer registered under it. For example, indexing a the following doc:

```
{
    "my_field" : "whitespace"
}
```

Will cause the `whitespace` analyzer to be used as the index analyzer for all fields without explicit analyzer setting.

The default `path` value is `_analyzer`, so the analyzer can be driven for a specific document by setting `_analyzer` field in it. If custom json field name is needed, an explicit mapping with a different `path` should be set.
</description><key id="405272">485</key><summary>Mapper: An analyzer mapper allowing to control the index analyzer of a document based on a document field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-11-07T16:17:49Z</created><updated>2010-11-12T12:59:40Z</updated><resolved>2010-11-08T00:18:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-07T16:18:38Z" id="524028">Mapper: An analyzer mapper allowing to control the index analyzer of a document based on a document field, closed by 171fa4a7e8d92cdc904ad999efe36726d3d635a3.
</comment><comment author="sebaes" created="2010-11-09T03:37:40Z" id="527994">Hi Shay,
I couldn't make this feature work, I think it is broken, take a look at: AnalyzerMapper
line 85: String value = context.doc().get(path);

When indexing a document with:
curl -XPUT 'http://sd:5100/test/s/ZZZ' -d '
{
"analyzer_field":"es_analyzer",
"description":"an mp3",
"title":"mp3"
}
'

value is null, because context.doc() doesn't contain the analyzer field in it.
I don't know exactly how to fix the problem though.

Sebastian.
</comment><comment author="kimchy" created="2010-11-09T06:57:13Z" id="528214">Did you set a mapping for type `s` with `_analyzer` definition that points to `analyzer_field`? The `analyzer_field` was just an example for the field name that will control the analysis.

Maybe there should be a default for this, for example, if you place in the json `_analyzer` property, then it will be used, so you won't even need to define a mapping unless you want the analyzer to be driven by a different json field. I will change that.
</comment><comment author="sebaes" created="2010-11-09T07:05:54Z" id="528224">Yes I did, the file looks like:

{
    "s" : {
        "_all" : {"enabled" : false},
        "_source" : {"enabled" : false},
        "_analyzer" : {"path" : "analyzer_field"},
        "dynamic" : false,
        "properties" : {
            "title" : {
                "type" : "string",
                "store" : "yes",
                "index" : "analyzed",
                "analyzer" : "all_analyzer",
                "term_vector" : "with_positions_offsets"
            },
            "description" : {
                "type" : "string",
                "store" : "yes",
                "index" : "analyzed",
                "term_vector" : "with_positions_offsets"
            },
...
and I've used a field named "analyzer_field" in my JSON as you can see in my previous post.

I will try the "_analyzer" default one

Thanks
</comment><comment author="kimchy" created="2010-11-09T07:11:47Z" id="528227">I see the problem, because the mapping you have is not dynamic, then you need to explicitly define the "analyzer_field" in the properties mapping so it will get added to the document. Same will happen with `_analyzer` you will need to add it to the `properties` part. Why don't you use dynamic?
</comment><comment author="sebaes" created="2010-11-09T07:18:51Z" id="528232">I see, (well, sort of trying to).

I didn't use dynamic mappings as a safety measure, to rule out wrong mappings from my client code, so I won't push invalid unsearchable data to an index, with the overhead it involves taking into account the scale and scarcity of a SSD space for example. Do you think this is not a good idea?

So, what are my options? What do you mean by define an "_analyzer" or "analyzer_field" in the properties mapping? Adding another field like "title" but containing what?

Thanks
Sebastian.
</comment><comment author="kimchy" created="2010-11-09T08:32:54Z" id="528319">add a mapping called in the same name (like analyzer_field) of type string.
</comment><comment author="sebaes" created="2010-11-09T19:39:03Z" id="529795">Hi Shay,
I added the field and everything works now, also tried dynamic mapping and works too. The only thing I noticed that wasn't ok is that the "_analyzer" field gets added to Lucene as well (checked with Luke with dynamic and not dynamic settings), and it shouldn't because it's a "control" field, not data.
Is it feasible to remove it from being added?
Thanks,
Sebastian.
</comment><comment author="sebaes" created="2010-11-09T19:43:52Z" id="529812">To be more precise I added:
            "_analyzer" : {"type" : "string"},
I thought in adding index=no and store=no, but I didn't see any example with both options disabled, is that the way to prevent that field to be added?
</comment><comment author="sebaes" created="2010-11-09T20:37:03Z" id="529946">Answering to myself, so somebody looking at the issue can use the information. I tried both index and store in "no" and it works:

```
        "_analyzer" : {"type" : "string","store" : "no","index" : "no"},
```
</comment><comment author="sebaes" created="2010-11-09T21:44:44Z" id="530155">The past post wasn't accurate, I made a mistake interpreting the results, the one that works is:
           "_analyzer" : {
                "type" : "string",
                "index" : "no",
                "store" : "yes"
            },
What happened in case of both "no" is that "_analyzer" is ignored, but Standard analyzer, with english stop words set kicks in and that's why I thought it was working.

Shay, is there a way to avoid adding that "_analyzer" stored field in every document?
Thanks,
Sebastian.
</comment><comment author="kimchy" created="2010-11-12T12:59:40Z" id="536750">Pushed support for specifying `index` set to `no` (`store` defaults to `no`). I would argue that almost in all cases you would want it indexed (maybe set to `not_analyzed`) so later on you can query on it and see what docs where indexed how.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>document level analyzer (analyzer, index_analyzer, search_analyzer) are not serialized (and not maintained across restarts)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/484</link><project id="" key="" /><description>document level analyzer (analyzer, index_analyzer, search_analyzer) are not serialized (and not maintained across restarts)
</description><key id="404986">484</key><summary>document level analyzer (analyzer, index_analyzer, search_analyzer) are not serialized (and not maintained across restarts)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.0</label></labels><created>2010-11-07T11:12:57Z</created><updated>2010-11-07T11:13:45Z</updated><resolved>2010-11-07T11:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-07T11:13:45Z" id="523651">document level analyzer (analyzer, index_analyzer, search_analyzer) are not serialized (and not maintained across restarts), closed by 598225f8338bf6bd19ee0d4bf6b7124619c493fc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an option to display max_open_files, by setting -Des.max-open-files to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/483</link><project id="" key="" /><description>When `-Des.max-open-files` is set, will log the maximum open files that elasticsearch process can open. Note, this is done by trying to open files until the limit is reached, so this is the best way for the test, though its not enabled by default for startup performance reasons.
</description><key id="404390">483</key><summary>Add an option to display max_open_files, by setting -Des.max-open-files to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-06T20:46:14Z</created><updated>2010-11-06T20:47:37Z</updated><resolved>2010-11-06T20:47:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-06T20:47:37Z" id="522810">Add an option to display max_open_files, by setting -Des.max-open-files to `true`, closed by 998bde082017df2e556c7286d27c9542d7ddea99.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Allow to control (globally) the max clause count for `bool` query (defaults to 1024)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/482</link><project id="" key="" /><description>Query DSL: Allow to control (globally) the max clause count for `bool` query (defaults to 1024). The setting is `index.query.bool.max_clause_count`.
</description><key id="403378">482</key><summary>Query DSL: Allow to control (globally) the max clause count for `bool` query (defaults to 1024)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-05T22:59:12Z</created><updated>2016-09-20T12:56:25Z</updated><resolved>2010-11-06T05:59:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-05T22:59:48Z" id="521343">Query DSL: Allow to control (globally) the max clause count for `bool` query (defaults to 1024), closed by 129b9a3938b423f4622c42adbd6ce6cfd1d7539e.
</comment><comment author="senthilsebi" created="2010-11-24T11:04:38Z" id="563776">how to set max_clause_count in query. any samples???
</comment><comment author="kimchy" created="2010-11-24T11:16:31Z" id="563798">in the elasticsearch configuration file (the yml file for example), set the mentioned setting. See more here: http://www.elasticsearch.com/docs/elasticsearch/setup/configuration/#Settings.
</comment><comment author="pranmitt" created="2016-09-20T12:56:25Z" id="248293205">Is there any serious performance impact on passing huge number of terms in a query ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fs Gateway: Add (native) file lock to ensure two nodes in a split brain are not updating same gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/481</link><project id="" key="" /><description>Add file lock to gateway to ensure two nodes in a split brain are not updating same gateway. This should be able to prevent index corruption with two clusters split brained trying to write to same gateway.

In order to disable it, set `gateway.fs.snapshot_lock` to `false`.
</description><key id="403264">481</key><summary>Fs Gateway: Add (native) file lock to ensure two nodes in a split brain are not updating same gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-05T22:12:51Z</created><updated>2010-11-06T21:28:02Z</updated><resolved>2010-11-06T21:28:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-06T21:28:02Z" id="522870">Fs Gateway: Add (native) file lock to ensure two nodes in a split brain are not updating same gateway, closed by 6b952f6719e1b257d44aef99095d3aa710b377b9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException executing fetch phase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/480</link><project id="" key="" /><description>Here is stack trace from the log:

[2010-11-05 12:25:10,983][DEBUG][action.search.type       ] [Ashcan] [98] Failed to execute fetch phase
java.lang.NullPointerException
    at org.elasticsearch.index.store.memory.ByteBufferFile.numberOfBuffers(ByteBufferFile.java:62)
    at org.elasticsearch.index.store.memory.ByteBufferIndexInput.switchCurrentBuffer(ByteBufferIndexInput.java:92)
    at org.elasticsearch.index.store.memory.ByteBufferIndexInput.seek(ByteBufferIndexInput.java:82)
    at org.apache.lucene.index.TermVectorsReader.readTermVector(TermVectorsReader.java:415)
    at org.apache.lucene.index.TermVectorsReader.get(TermVectorsReader.java:265)
    at org.apache.lucene.index.TermVectorsReader.get(TermVectorsReader.java:286)
    at org.apache.lucene.index.SegmentReader.getTermFreqVector(SegmentReader.java:1175)
    at org.apache.lucene.index.DirectoryReader.getTermFreqVector(DirectoryReader.java:471)
    at org.apache.lucene.search.vectorhighlight.FieldTermStack.&lt;init&gt;(FieldTermStack.java:83)
    at org.apache.lucene.search.vectorhighlight.FastVectorHighlighter.getFieldFragList(FastVectorHighlighter.java:119)
    at org.apache.lucene.search.vectorhighlight.FastVectorHighlighter.getBestFragments(FastVectorHighlighter.java:113)
    at org.elasticsearch.search.highlight.HighlightPhase.execute(HighlightPhase.java:77)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:181)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:299)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:311)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.access$200(TransportSearchQueryThenFetchAction.java:60)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$1.run(TransportSearchQueryThenFetchAction.java:114)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)
</description><key id="402033">480</key><summary>NullPointerException executing fetch phase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nikoloz</reporter><labels /><created>2010-11-05T08:36:09Z</created><updated>2010-11-15T08:18:44Z</updated><resolved>2010-11-15T16:18:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-05T16:17:58Z" id="520176">can you provide a recreation? I am assuming you are using the in memory index?
</comment><comment author="kimchy" created="2010-11-05T16:17:58Z" id="520177">can you provide a recreation? I am assuming you are using the in memory index?
</comment><comment author="nikoloz" created="2010-11-15T08:18:43Z" id="541184">Sorry for late response. Yes, we were using memory index. We'were indexing documents (approximately 20k) and also querying indexes when above mentioned exception happened. Not sure why it's happened.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting bug where fragments are broken when number_of_fragments is 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/479</link><project id="" key="" /><description>This is happening on version 0.12.1. When highlighting is specified, some of the highlight fields come complete, and some are broken. When those documents are access directly by providing the direct index as "from" and setting "size" as 1, it comes fine: Reproduce code is below (bash script):

```
curl -XPUT 'http://server:9200/indexname/highbug/_mapping' -d '
{
    "highbug":
    {
        "properties": 
        {

            "body":
            {
                "store": "yes",
                "term_vector": "with_positions_offsets",
                "type": "string"
            },
            "title":
            {
                "store": "yes",
                "term_vector": "with_positions_offsets",
                "type": "string"
            }
        }
    }
}
';

for (( i = 0; i &lt; 20; ++i ))
do
    curl -XPUT "http://server:9200/indexname/highbug/$i" -d "
    {
        \"body\":  \"Test body\",
        \"date\":  \"$(date +%s)\",
        \"title\": \"This is a test on the highlighting bug present in elasticsearch\"
    }";
done
```

The call below returns the full highlighted fragment for the 1st and 5th results, and the 2nd through 4th results return "hting &lt;em&gt;bug&lt;/em&gt; present in elasticsearch" instead of "This is a test on the highlighting &lt;em&gt;bug&lt;/em&gt; present in elasticsearch".

```
curl -XGET 'http://server:9200/indexname/highbug/_search?pretty=true' -d '
{"query":{"query_string":{"query":"bug"}},"from":0,"size":5,"highlight":{"fields":{"title":{"number_of_fragments":0}}}}
';
```
</description><key id="401576">479</key><summary>Highlighting bug where fragments are broken when number_of_fragments is 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jessiehernandez</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-11-05T00:42:33Z</created><updated>2010-11-18T18:53:06Z</updated><resolved>2010-11-18T18:53:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-18T18:53:06Z" id="551341">Highlighting bug where fragments are broken when number_of_fragments is 0, closed by fdb98b1dcb1cbdbb6dcf7d4be9cf1f0598963d2f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve performance when searching across multiple types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/478</link><project id="" key="" /><description>Instead of using a boolean filter wrapping the different type options, use a terms filter, which will both executed faster and improve caching behavior when searching across multiple types.
</description><key id="400800">478</key><summary>Improve performance when searching across multiple types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-04T18:29:53Z</created><updated>2010-11-04T18:34:57Z</updated><resolved>2010-11-04T18:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-04T18:34:56Z" id="518167">Improve performance when searching across multiple types, closed by 99a3e615ab7ebd7c97c9c12a1048face76c91f41.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>constant_score query might apply deletes wrongly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/477</link><project id="" key="" /><description>`constant_score` query might apply deletes wrongly, resulting in deleted docs returning when searching.
</description><key id="400784">477</key><summary>constant_score query might apply deletes wrongly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.0</label></labels><created>2010-11-04T18:17:37Z</created><updated>2010-11-04T18:18:16Z</updated><resolved>2010-11-04T18:18:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-04T18:18:16Z" id="518122">constant_score query might apply deletes wrongly, closed by bbd63f0ffef611842315044f4275a341ce7110cf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Add `path_match` for full object navigation path matching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/476</link><project id="" key="" /><description>Currently, `match` only maps on the field name of the json object/property for the mapping to be applied on. The `path_match` allows to set full matching including the concatenated "path" up to the field. In a similar to `unmatch`, there is also an option for `path_unmatch`. For example, using the following data:

```
{
    "_id" : "1",
    "name" : "top_level",
    "obj1" : {
        "name" : "obj1_level",
        "obj2" : {
            "name" : "obj2_level"
        }
    }
}
```

The following mapping can now be applied:

```
{
    "person" : {
        "dynamic_templates" : [
            {
                "template_1" : {
                    "path_match" : "obj1.obj2.*",
                    "mapping" : {
                        "store" : "no"
                    }
                }
            },
            {
                "template_2" : {
                    "path_match" : "obj1.*",
                    "mapping" : {
                        "store" : "yes"
                    }
                }
            }
        ]
    }
}
```
</description><key id="399786">476</key><summary>Mapper: Add `path_match` for full object navigation path matching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-11-04T08:54:51Z</created><updated>2016-04-14T11:51:49Z</updated><resolved>2010-11-04T08:55:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-04T08:55:26Z" id="516849">Mapper: Add `path_match` for full object navigation path matching, closed by e2d6f82cd3df721e5b41f0c6c2258b994168d7eb.
</comment><comment author="hannayurkevich" created="2016-04-14T11:51:49Z" id="209898782">Hi @kimchy!

Is it possible to specify several path patterns (applied as OR) in the same template?

I would like to apply the same template to several fields.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"-Dproperty.name" settings from command line are ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/475</link><project id="" key="" /><description>Startup string: ./bin/elasticsearch -Dcluster.name=webs-elasticsearch -f

In version 0.11.0 this command string would result in the proper cluster.name being assigned as "webs-elasticsearch". However, in version 0.12.1 the cluster.name stays as the default (elasticsearch).

Is cluster.name no longer respected if set from the command line? Are there other settings that are ignored now also?

EDIT: -Dnetwork.host is also ignored from the command line startup
</description><key id="398450">475</key><summary>"-Dproperty.name" settings from command line are ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fucema</reporter><labels /><created>2010-11-03T17:11:58Z</created><updated>2011-01-29T18:21:29Z</updated><resolved>2011-01-29T18:21:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-03T17:35:19Z" id="515376">Hi,

   Just tested this, and what you try and set does not work on both 0.11 and 0.12. Its basically because you need the setting with `es.` when provided using `-D`. For example: `-Des.cluster.name=test`.
</comment><comment author="kimchy" created="2011-01-29T18:21:29Z" id="721072">The example in the docs was wrong, an `es.` prefix should be added to prefix the settings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some (all?) startup properties are not respected when starting ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/474</link><project id="" key="" /><description>Startup string: ./bin/elasticsearch -Dcluster.name=webs-elasticsearch -f

In version 0.11.0 this command string would result in the proper cluster.name being assigned as "webs-elasticsearch". However, in version 0.12.1 the cluster.name stays as the default (elasticsearch). 

Is cluster.name no longer respected if set from the command line? Are there other settings that are ignored now also?
</description><key id="398430">474</key><summary>Some (all?) startup properties are not respected when starting ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fucema</reporter><labels /><created>2010-11-03T17:04:23Z</created><updated>2010-11-03T17:10:24Z</updated><resolved>2010-11-03T17:10:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fucema" created="2010-11-03T17:08:36Z" id="515249">"-Dnetwork.host=some_value" is not being used also
</comment><comment author="fucema" created="2010-11-03T17:10:24Z" id="515260">Deleting and recreating with proper labels.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create specific `data` location for indices (move from work)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/473</link><project id="" key="" /><description>Initially, the idea was that work consisted of transient data, but, with the local gateway and the fact that indices gets reused with shared gateway, the data location (where indices are stored) is quite important.

Now, the following are the paths:

`path.data`: path to where data will be placed. Defaults to `ES_HOME\data`.
`path.logs`: path to logs location. Defaults to `ES_HOME\logs`.
`path.work`: path to temporary data created. Defaults to `ES_HOME\work`.

In order to upgrade to new version, the simplest thing to do is rename `work` to `data`. If `path.work` is set to point to the data location storage, change it to `path.data`.
</description><key id="397782">473</key><summary>Create specific `data` location for indices (move from work)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.13.0</label></labels><created>2010-11-03T12:38:42Z</created><updated>2010-11-03T12:41:39Z</updated><resolved>2010-11-03T12:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-03T12:41:39Z" id="514462">Create specific `data` location for indices (move from work), closed by 6804c02e97628700e589498dbf56fde2f41aa617.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change single operation shard hashing to only use id, and not id and type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/472</link><project id="" key="" /><description>Currently, single operation hashing (index/delete/get) is using the type as part of the hashing to decide which shard to direct to. It make more sense to just use the id for several reasons. 

First, the new routing control capability will allow to direct docs to be placed in the same placement of another doc (blog, and commends for example) just based on that doc id (the routing when indexing a comment can use the blog post id, and thats it).

There are future features where this type of hashing will really simplify them, so it make sense to make this change now.

This change will require to reindex the data. In order to revert back to using the type for hashing as well, set `cluster.routing.operation.use_type` to `true`. This means that a cluster can be started by setting this flag, start another cluster, and reindex data from the old cluster into the new cluster.
</description><key id="397547">472</key><summary>Change single operation shard hashing to only use id, and not id and type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.13.0</label></labels><created>2010-11-03T10:47:09Z</created><updated>2010-11-03T10:47:41Z</updated><resolved>2010-11-03T10:47:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-03T10:47:41Z" id="514189">Change single operation shard hashing to only use id, and not id and type, closed by 92b3ae3f7387b4b4df4d78256b7f8d58c58fc2e0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add `field_masking_span` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/471</link><project id="" key="" /><description>Field masking span maps to lucene FieldMaskingSpanQuery. It accepts a span query as an inner `query`, and the field in a `field` element.
</description><key id="396768">471</key><summary>Query DSL: Add `field_masking_span` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-02T21:49:58Z</created><updated>2013-05-07T14:48:10Z</updated><resolved>2010-11-03T04:50:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-02T21:50:36Z" id="513168">Query DSL: Add `field_masking_span` query, closed by 8d454ba2939dc343d58c2ea476729c585f1c28a3.
</comment><comment author="clintongormley" created="2011-01-03T14:16:01Z" id="648965">`field_masking_span` is not documented: http://www.elasticsearch.com/docs/elasticsearch/rest_api/query_dsl/
</comment><comment author="clintongormley" created="2011-01-08T21:19:15Z" id="663253">Hmm - does `field_masking_span` work? I'm getting `No query parser registered for [field_masking_span]]` with this query:

```
curl -XGET 'http://127.0.0.1:9200/_all/_count'  -d '
{
   "field_masking_span" : {
      "query" : {
         "span_term" : {
            "text" : "foo"
         }
      },
      "field" : "num"
   }
}
'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API: Allow to control document shard routing, and search shard routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/470</link><project id="" key="" /><description>Currently, when indexing or deleting documents, they are hashed based on the type and id and routed to a shard based on the hash result. When searching, the search request is a broadcast request to all shards within an index.

Sometimes, it make sense to control this routing. For example, indexing blob posts for a specific user might be routed based on the user id. If routing is controlled, then when performing a search on posts for only that user, only the relevant shard that match the user id can be queried, resulting in much faster search and overhaul less load on the system.

The `index` and `delete` operation now allow for a `routing` parameter to be specified. When specified, that routing value will be used to control the shard placement.

The `bulk` operation allows to specify `_routing` for each item to control the routing for that index/delete/create operation.

The `get` operation allows to specify a `routing` parameter as well, to specify which shard the doc will be fetched from. Note, in our blog post partitioned by user example, doing a lookup just by the post id will not be enough and probably will not find anything, the user id will also need to be specified in the `routing` parameter.

The `search` and `count` operations accept a `routing` parameter as well, controlling which _shards_ the search will be executed on. The `routing` parameter accepts a comma separated list of the routing values to use, and the all relevant shards will execute the query.

Note, even when specifying a search for a specific user blog posts using the `routing` parameter set to the user id, filtering only the user posts is still needed by, for example, adding a `term` filter with the user id.

The `delete_by_query` operation also accepts a `routing` parameter, which is a comma separated list of routing values of controlling which shards the delete query will be executed on.

One question that might arise is why not use indices to get the same behavior. For example, create an index per user. The reason is that indices have a much lower limit on how many of them can be created. A single machine can easily support millions of users with millions of posts. Creating an index per user will mean millions of indices, which is problematic, as even with a single shard per index, it does mean millions of lucene indices.
</description><key id="396193">470</key><summary>API: Allow to control document shard routing, and search shard routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-11-02T17:54:36Z</created><updated>2010-11-02T18:30:25Z</updated><resolved>2010-11-03T00:56:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-02T17:56:50Z" id="512471">API: Allow to control document shard routing, and search shard routing, closed by a62f1f3e0dc0716918945d5d9ff48503c90ccb2c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Merge: Change default `index.merge.scheduler.max_thread_count` from ` to be adaptive based on processor count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/469</link><project id="" key="" /><description>Change the default `index.merge.scheduler.max_thread_count` to be `Math.max(1, Math.min(3, Runtime.getRuntime().availableProcessors() / 2))`.
</description><key id="395443">469</key><summary>Index Merge: Change default `index.merge.scheduler.max_thread_count` from ` to be adaptive based on processor count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-02T12:00:24Z</created><updated>2010-11-02T12:01:07Z</updated><resolved>2010-11-02T12:01:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-02T12:01:07Z" id="511482">Index Merge: Change default `index.merge.scheduler.max_thread_count` from ` to be adaptive based on processor count, closed by 3fe2851dae81a85b8d5f55689d3f70f22c776959.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rivers: Add a _status doc for each river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/468</link><project id="" key="" /><description>A status doc `_status`, under `_river/[river name]/_status`, that exposes the general status of the river, which includes the node the river is running on. This is automatically created for all rivers.
</description><key id="395288">468</key><summary>Rivers: Add a _status doc for each river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-11-02T10:08:30Z</created><updated>2010-11-02T10:09:57Z</updated><resolved>2010-11-02T10:09:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-02T10:09:57Z" id="511309">Rivers: Add a _status doc for each river, closed by 2cdaf6357b525d32b54667dd6ae48dd3a01dfe85.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve Java API support for Index Alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/467</link><project id="" key="" /><description>Hi, as talked in the mailing list I am opening a feature request to improve the Java API for index alias management.
Basically to provide a type safe access to the aliases, or the aliases per index. Not only add/remove as there are now, but also get().
I was told there is an option to use indexStatus.settings().getAsArray("index.aliases"), so I could work ok with that, but this can be added as a low priority enhancement.
Thanks,
Sebastian.
</description><key id="394736">467</key><summary>Improve Java API support for Index Alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sebaes</reporter><labels /><created>2010-11-02T00:30:18Z</created><updated>2013-04-04T18:27:08Z</updated><resolved>2013-04-04T18:27:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-02T08:16:35Z" id="511146">Just answered on the mailing list as well, but the intended way to get the aliases is to use the `IndexMetaData`, and getting it using the cluster state API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possible (rare) shard index corruption / different doc count on recovery (gateway / shard)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/466</link><project id="" key="" /><description>The problem stems from the reusing of existing index files when doing recovery. Checksums should be added, but, in order to have it performant, they are computed on write.

This menas that existing indices will still work, but might suffer from it. New internal index files will get checksummed, and eventually the index will be fully checksummed, though, it is recommended to reindex the data.
</description><key id="394371">466</key><summary>Possible (rare) shard index corruption / different doc count on recovery (gateway / shard)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.0</label></labels><created>2010-11-01T20:58:38Z</created><updated>2010-11-01T23:15:46Z</updated><resolved>2010-11-02T04:00:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-11-01T21:00:26Z" id="510254">Possible (rare) shard index corruption / different doc count on recovery (gateway / shard), closed by 4ff1b429f1351c17ae5f3a4338a7ac33db49ef70.
</comment><comment author="ppearcy" created="2010-11-01T23:03:54Z" id="510528">SWEET!!! THANK YOU!!!!

When you say reindex in this case, would an inplace rebuild suffice or does one need to do a rebuild in a new index and swap in? 
</comment><comment author="kimchy" created="2010-11-01T23:08:47Z" id="510541">What do you mean by inplace rebuild? Reindex into the same index? If so, it is recommended to reindex data into a fresh index (so less deleted docs in lucene).
</comment><comment author="ppearcy" created="2010-11-01T23:09:40Z" id="510544">Yeah, exactly, I meant reindex into the same index. Thanks for the details. 
</comment><comment author="ppearcy" created="2010-11-01T23:13:36Z" id="510555">However. I thought optimization/merges removed deleted records. Either way, will take the recommended approach.

Thanks
</comment><comment author="kimchy" created="2010-11-01T23:15:46Z" id="510561">Yes, they do (eventually), though they can be expensive. If a full reindex is required, then you might as well index into a new index. Thats why aliases are there by the way, you can work with a logical name for an index (alias), and then switch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On full cluster restart, replicas transaction logs are not getting cleaned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/465</link><project id="" key="" /><description>On full cluster restart, when a replica recovers its state from the primary shard, its previous transaction log is not getting cleaned.
</description><key id="392070">465</key><summary>On full cluster restart, replicas transaction logs are not getting cleaned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.13.0</label></labels><created>2010-10-31T12:34:17Z</created><updated>2010-10-31T12:36:16Z</updated><resolved>2010-10-31T12:36:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-31T12:36:16Z" id="507209">On full cluster restart, replicas transaction logs are not getting cleaned, closed by feb854b74264e28649c4e16d8ef993e14be31df5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When on linux, force it not to swap elasticsearch process (jvm)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/464</link><project id="" key="" /><description>This uses the same mechanism in cassandra (thanks!) and requires an optional dependency on jna.

Note, in order for this to be effective, make sure to set min and max memory to the same value.
</description><key id="390685">464</key><summary>When on linux, force it not to swap elasticsearch process (jvm)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-10-30T17:39:43Z</created><updated>2010-11-03T19:07:54Z</updated><resolved>2010-10-31T00:40:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-30T17:40:32Z" id="504465">When on linux, force it not to swap elasticsearch process (jvm), closed by 96a1ad6335a9b736aed1f561c9c2b94b6d876a19.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index: Move compound format setting to `index.compound_format` (old setting still supported)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/463</link><project id="" key="" /><description>Setting compound format setting should be set using `index.compound_format` (defaults to `false`). The old setting (`index.merge.policy.use_compound_file` is still supported).
</description><key id="390624">463</key><summary>Index: Move compound format setting to `index.compound_format` (old setting still supported)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-10-30T16:23:48Z</created><updated>2010-10-30T16:24:35Z</updated><resolved>2010-10-30T16:24:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-30T16:24:35Z" id="504375">Index: Move compound format setting to `index.compound_format` (old setting still supported), closed by 1feb43a0af0ae4149351e70788c065ae8678aaac.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MetaData does not return existing indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/462</link><project id="" key="" /><description>Hi,

We've just upgraded from 0.11.0 to 0.12.1. On startup we check to see whether an index exists. If it doesn't, we try to create it. The code we use for this is:

```
if(!client.admin().cluster().state(clusterStateRequest()).actionGet().getState().metaData().getIndices().containsKey("valuations")) {
    client.admin().indices().create(request).actionGet();
}
```

This used to work fine in 0.11.0. However, in 0.12.1 we find the getIndices() always returns an empty map. We then try to create the index, and get an IndexAlreadyExistsException.

Jim
</description><key id="387143">462</key><summary>MetaData does not return existing indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JimCross</reporter><labels /><created>2010-10-28T14:28:00Z</created><updated>2013-04-04T18:26:51Z</updated><resolved>2013-04-04T18:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-28T16:35:52Z" id="499903">Can you check if this is a timing issue? Can you do state request after the create index failure, and see if you get the indices back?

Also, which client do you use, node client, or transport client?
</comment><comment author="JimCross" created="2010-11-01T10:06:01Z" id="508702">We're using the node client.

If I run the app with no persisted data, then after the index is created it does appear in the map of indices in the metadata.
However, if I start the app up with persisted data, even if I let it sleep for ages the index does not appear in the map of indices in the metadata.

James
</comment><comment author="kimchy" created="2010-11-01T10:32:01Z" id="508734">can you create a simple test case for this? If I understood the scenario, its working on my end...
</comment><comment author="clintongormley" created="2013-04-04T18:26:51Z" id="15914822">No further info in last two years - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Ip Type Support (ipv4), auto detection with dynamic mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/461</link><project id="" key="" /><description>An `ip` mapping type allowing to store ipv4 addresses in a numeric form allowing to easily sort, and range check it. It has exactly the same mapping options as `date` (except for the format).
</description><key id="386786">461</key><summary>Mapper: Ip Type Support (ipv4), auto detection with dynamic mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-10-28T10:01:52Z</created><updated>2010-10-28T10:36:39Z</updated><resolved>2010-10-28T17:02:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-28T10:02:28Z" id="499125">Mapper: Ip Type Support (ipv4), auto detection with dynamic mapping, closed by 4579c04a9ea4a167a6ed87ce676d16736089f729.
</comment><comment author="medcl" created="2010-10-28T10:36:39Z" id="499185">nice~
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restart Index or Re Index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/460</link><project id="" key="" /><description>hi
       how re-index the specific Elastic search index. 
is there any API for this.

```
  (or)
```

i need to restart ES.

Note: i have more than 20  index with huge data. i cant able to restart ES for a specific field creation.

Pls Help Me

Thanks &amp; Regards
   SenthilSebi
</description><key id="386569">460</key><summary>Restart Index or Re Index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">senthilsebi</reporter><labels /><created>2010-10-28T06:20:50Z</created><updated>2010-10-28T08:57:26Z</updated><resolved>2010-10-28T08:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-28T08:55:27Z" id="499013">please ask questions on the mailing list.
</comment><comment author="kimchy" created="2010-10-28T08:57:26Z" id="499018">closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MultiField mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/459</link><project id="" key="" /><description>hi,
     i have mapped multi field for an index like this
{
    "tweet" : {
        "properties" : {
            "name" : {
                "type" : "multi_field",
                "fields" : {
                    "name" : {"type" : "string", "index" : "analyzed"},
                    "untouched" : {"type" : "string", "index" : "not_analyzed"}
                }
            }
        }
    }
}

and indexing data's to this index. i want add one more field as multifield. i added using ignore conflicts. the field also added. but when i search the records with that specfic fields no data found. error raised. the error is no mapping found for that specfic field.

Note: if i add as single field after indexing. its working.

pls help me.

Thanks &amp; Regards
senthilsebi
</description><key id="386525">459</key><summary>MultiField mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">senthilsebi</reporter><labels /><created>2010-10-28T05:35:15Z</created><updated>2011-01-29T18:20:44Z</updated><resolved>2011-01-29T18:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aparo" created="2010-10-28T05:40:19Z" id="498755">When you change the mappings in a multifield, to see the results you need to reindex the documents. 
</comment><comment author="kimchy" created="2011-01-29T18:20:44Z" id="721071">closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MultiField mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/458</link><project id="" key="" /><description>hi,
     i have mapped multi field for an index like this
{
    "tweet" : {
        "properties" : {
            "name" : {
                "type" : "multi_field",
                "fields" : {
                    "name" : {"type" : "string", "index" : "analyzed"},
                    "untouched" : {"type" : "string", "index" : "not_analyzed"}
                }
            }
        }
    }
}

and indexing data's to this index. i want add one more field as multifield. i added using ignore conflicts. the field also added. but when i search the records with that specfic fields no data found. error raised. the error is no mapping found for that specfic field.

Note: if i add as single field after indexing. its working.

pls help me.

Thanks &amp; Regards
senthilsebi
</description><key id="386505">458</key><summary>MultiField mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">senthilsebi</reporter><labels /><created>2010-10-28T05:18:53Z</created><updated>2010-10-28T08:58:02Z</updated><resolved>2010-10-28T08:58:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-28T08:57:58Z" id="499020">please ask questions on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport Thrift: Upgrade to thrift 0.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/457</link><project id="" key="" /><description /><key id="385910">457</key><summary>Transport Thrift: Upgrade to thrift 0.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-10-27T21:12:19Z</created><updated>2010-10-27T21:13:02Z</updated><resolved>2010-10-27T21:13:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-27T21:13:02Z" id="498002">Transport Thrift: Upgrade to thrift 0.5.0, closed by b8708f276d9f51dda1db724a1b290063504d6626.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Revert back and have range, terms, and prefix filter _cache set to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/456</link><project id="" key="" /><description>Basically, the idea of filters and when they make sense is when they are cacheable, i.e. repeated in several queries. So, range filter will mostly be used when filters can be reused across queries (like age &gt; 10). So, it make sense to default it to `_cache` set to `true`. Same applies to `terms`, `prefix` (and `term` which was already changed.

Need to properly doc when to use filters, and when to use queries!.
</description><key id="385574">456</key><summary>Query DSL: Revert back and have range, terms, and prefix filter _cache set to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-27T18:25:22Z</created><updated>2010-10-27T18:34:09Z</updated><resolved>2010-10-27T18:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-27T18:34:09Z" id="497549">Query DSL: Revert back and have range, terms, and prefix filter _cache set to true, closed by 60080494ae6de9781ac00e5ff9a47250befcd9d7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Change default operation threading to `thread_per_shard` from `single_thread`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/455</link><project id="" key="" /><description>Note, this only affects cases when the search request is executed on several shards within the same node that accepted the request.
</description><key id="385493">455</key><summary>Search: Change default operation threading to `thread_per_shard` from `single_thread`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-27T17:51:13Z</created><updated>2010-10-27T17:51:56Z</updated><resolved>2010-10-27T17:51:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-27T17:51:56Z" id="497411">Search: Change default operation threading to `thread_per_shard` from `single_thread`, closed by 185f5a9e1811893f19ac4679dfe7900ef84705f9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"geo_distance" parsing bugs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/454</link><project id="" key="" /><description>These add a few more tests related to the parsing of "geo_distance" filter &#8211; which expose some minor bugs with the handling of "distance" / "unit" parameters.

Then two changes to fix DistanceUnit and GeoDistanceFilterParser classes.

---

A summary of the issues is:
-  distance: number, unit: "mi"   x   distance: "string", unit: "mi"  result in different behavior
-  distance: x, unit: "km"   parses distance as x_1.609_1.096 in miles
-  distance: "12mi", unit: "km"    does not work as    distance: "12mi"
</description><key id="383203">454</key><summary>"geo_distance" parsing bugs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aferreira</reporter><labels><label>bug</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-26T17:23:11Z</created><updated>2014-06-29T17:55:11Z</updated><resolved>2010-10-26T18:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-26T18:27:59Z" id="494571">great catch!, will push in a few..., for some reason adding @Test to the DistanceUnitTests class makes it run, it looks like it ran when you run it (all green), but nothing actually runs as you noted, very misleading....
</comment><comment author="kimchy" created="2010-10-26T18:53:15Z" id="494672">pushed to both master and 0.12 branch, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: `term` filter to have `_cache` set to `true` by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/453</link><project id="" key="" /><description>Since `term` filter is usually used for common and repeating terms (tags, country, and such), default the `_cache` to `true`, meaning it will be strongly cached.
</description><key id="382599">453</key><summary>Query DSL: `term` filter to have `_cache` set to `true` by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-26T11:39:30Z</created><updated>2010-10-26T11:40:21Z</updated><resolved>2010-10-26T11:40:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-26T11:40:21Z" id="493528">Query DSL: `term` filter to have `_cache` set to `true` by default, closed by 5804e9132a3d798b1b1df07e39cce7dcc5ab1dd1,
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thrift: Response might get corrupted with extra data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/452</link><project id="" key="" /><description>when doing a search query use thrift transport(http client is fine),it sounds like that some unnecessary json block was appended after right result,that made the result  on invalid state(when parsing the result),and it was happed with that: i have some random query,and each the result maybe different,for example,it was supposed to return the following result:
{
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 0,
        "max_score": null,
        "hits": []
    }
}

but actually what i got is:

{"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}s":[{"_index":"index","_type":"default","_id":"a4d3805b-be55-495e-b13a-0af9b49c94a8","_score":1.411927, "_source" : {"ID": "52441", "ValidateID": "mao", "User_ULID": "44", "UserName": "05505013", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-19 14:34:56.130"}},{"_index":"index","_type":"default","_id":"d13db255-6bf2-4bbc-b0f3-c8fbd88043c5","_score":1.411927, "_source" : {"ID": "52444", "ValidateID": "111111", "User_ULID": "44", "UserName": "06721214", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-19 14:51:24.600"}},{"_index":"index","_type":"default","_id":"98cf1ec5-1640-4bf1-b8e0-4b8ae2454b20","_score":1.411927, "_source" : {"ID": "52445", "ValidateID": "redapple", "User_ULID": "44", "UserName": "06523126", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-19 14:53:52.257"}},{"_index":"index","_type":"default","_id":"3d0a1f80-86f5-4203-8834-54f58da041f7","_score":1.411927, "_source" : {"ID": "52452", "ValidateID": "198711251919", "User_ULID": "44", "UserName": "05715028", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-19 15:22:01.630"}},{"_index":"index","_type":"default","_id":"76eb76d2-9029-4c3a-ba58-fca37de44fe3","_score":1.411927, "_source" : {"ID": "52456", "ValidateID": "23281227", "User_ULID": "44", "UserName": "06721175", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-19 15:57:05.037"}},{"_index":"index","_type":"default","_id":"b90841e0-2936-45a1-a276-43f4c6a9fd75","_score":1.411927, "_source" : {"ID": "52464", "ValidateID": "10631008", "User_ULID": "41", "UserName": "10631008", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-19 18:56:18.930"}},{"_index":"index","_type":"default","_id":"c68f5f39-3ced-4ec7-9c01-8608a51d1040","_score":1.411927, "_source" : {"ID": "52486", "ValidateID": "819122", "User_ULID": "44", "UserName": "05525028", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-20 14:21:09.787"}},{"_index":"index","_type":"default","_id":"b8646c8f-013b-4db0-be9b-e1af0d44a7cb","_score":1.411927, "_source" : {"ID": "52487", "ValidateID": "04514166", "User_ULID": "44", "UserName": "04514166", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-20 14:23:03.913"}},{"_index":"index","_type":"default","_id":"a88af3c2-f2af-4fc2-90c3-af08c40ab43f","_score":1.411927, "_source" : {"ID": "52490", "ValidateID": "vv127127", "User_ULID": "44", "UserName": "06721187", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-20 15:14:30.507"}},{"_index":"index","_type":"default","_id":"dba12596-bebb-4af9-9d2e-2c31d8eb8c3e","_score":1.411927, "_source" : {"ID": "52491", "ValidateID": "7758521", "User_ULID": "44", "UserName": "05713129", "UserType": "0", "Islock": "0", "ExcelId": "NULL", "CreateTime": "2007-01-20 15:22:21.083"}}]}}
</description><key id="382575">452</key><summary>Thrift: Response might get corrupted with extra data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels><label>bug</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-26T11:10:16Z</created><updated>2010-10-26T12:59:04Z</updated><resolved>2010-10-26T12:59:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-26T12:59:04Z" id="493663">Thrift: Response might get corrupted with extra data, closed by 2460ee8072c385bf672db628d14554685d954656.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query: match_all query (also in query_string: *:*) is very slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/451</link><project id="" key="" /><description>match all query was optimized in 0.12, and there is a corner case where excessive processing is done due to it. Keep the optimization, but fix the processing.
</description><key id="382417">451</key><summary>Query: match_all query (also in query_string: *:*) is very slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-26T08:46:10Z</created><updated>2010-10-26T08:54:16Z</updated><resolved>2010-10-26T08:54:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-26T08:54:16Z" id="493312">Query: match_all query (also in query_string: _:_) is very slow, closed by a78d4d96bf1ad56450666111577c5099e4da542e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: `term`, `term`, `prefix`, and `range` filter are now weakly cached, for more strong caching, set `_cache` to true.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/450</link><project id="" key="" /><description>Currently, `term`, `terms`, `prefix`, and `range` filters are cached by default (since the way they are implemented means caching them does not require extra processing). They still retain memory and "fight" with other caching structures for space.

Change that by default, this filters will be weakly cached. They will be evicted first before other caching constructs (especially field cache).

When using those filters with high chances of reuse (those filters will be constantly used by other queries), set `_cache` to `true` to enable more strong caching.
</description><key id="380694">450</key><summary>Query DSL: `term`, `term`, `prefix`, and `range` filter are now weakly cached, for more strong caching, set `_cache` to true.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-25T13:52:00Z</created><updated>2010-10-25T13:59:25Z</updated><resolved>2010-10-25T13:59:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-25T13:59:25Z" id="490902">Query DSL: `term`, `term`, `prefix`, and `range` filter are now weakly cached, for more strong caching, set `_cache` to true, closed by bc4121c06b4475ca81461261ace93cd863a02d29.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: `constant_score` and `filtered` queries cache filters by default, remove it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/449</link><project id="" key="" /><description>With the updated fine grained control if filters are cached or not, there is no need to cache the filters on the `constant_score` or `filtered` query level. Default `_cache` to false (really, it usually should not even be set).
</description><key id="380575">449</key><summary>Query DSL: `constant_score` and `filtered` queries cache filters by default, remove it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.1</label><label>v0.13.0</label></labels><created>2010-10-25T11:50:49Z</created><updated>2010-10-25T11:52:22Z</updated><resolved>2010-10-25T11:52:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-25T11:52:22Z" id="490683">Query DSL: `constant_score` and `filtered` queries cache filters by default, remove it, closed by deada942e5c85f49e7452291c4242cb41b784eef.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't fail when highlighting is done on non mapped fields, and expose REST endpoint to plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/448</link><project id="" key="" /><description>Don't fail when highlighting is done on non mapped fields, and expose REST endpoint to plugins
</description><key id="378334">448</key><summary>Don't fail when highlighting is done on non mapped fields, and expose REST endpoint to plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2010-10-23T21:12:56Z</created><updated>2014-07-16T21:56:59Z</updated><resolved>2011-02-10T17:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aparo" created="2011-02-10T13:15:06Z" id="759057">Github tries to merge my preview pull request.
I cleaned the old ones.
</comment><comment author="kimchy" created="2011-02-10T17:04:36Z" id="759790">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices API: Add open and close index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/447</link><project id="" key="" /><description>Allow to close and index and open an index. A closed index has almost no overhead on the cluster (except for maintaining its metadata), and is blocked for read/write operations. A closed index can be opened which will then go through the normal recovery process.

The REST endpoints are POST for `/{index}/_close` and `/{index}/_open`.
</description><key id="378259">447</key><summary>Indices API: Add open and close index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-10-23T19:50:09Z</created><updated>2010-10-23T19:52:23Z</updated><resolved>2010-10-23T19:52:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-23T19:52:23Z" id="488241">Indices API: Add open and close index, closed by ca7a7467dc4ea4bcdb1469fa3614aacf0df55ef4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Query String _missing_ and _exists_ syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/446</link><project id="" key="" /><description>The `_exists_` and `_missing_` syntax allows to control docs that have fields that exists within them (have a value) and missing. The syntax is: `_exists_:field1`, `_missing_:field` and can be used anywhere a query string is used (`query_string` and `field` query).
</description><key id="376681">446</key><summary>Query DSL: Query String _missing_ and _exists_ syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-10-22T15:50:59Z</created><updated>2010-10-22T15:51:27Z</updated><resolved>2010-10-22T15:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-22T15:51:27Z" id="486223">Query DSL: Query String _missing_ and _exists_ syntax, closed by 9237dafef96d0b0a11b377bf6961955d98609688.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add `exists` and `missing` filters to filter documents where a field either has a value or not in them.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/445</link><project id="" key="" /><description>The `exists` filter will only include documents where the field has a value in them, and `missing` where a field does not have a value in them.

The syntax is simple:

```
{
    "exists" : { "field" : "my_field" }
}
```

The filter result is always cached, and it can be named by setting `_name`.
</description><key id="376396">445</key><summary>Query DSL: Add `exists` and `missing` filters to filter documents where a field either has a value or not in them.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-10-22T12:48:35Z</created><updated>2010-10-22T13:14:17Z</updated><resolved>2010-10-22T13:14:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-22T13:14:17Z" id="485883">Query DSL: Add `exists` and `missing` filters to filter documents where a field either has a value or not in them, closed by f63ee3158a2d967b0fb755d00006cad885709e3c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write Consistency Level for index/delete/delete_by_query/bulk with one/quorum/all. Defaults to quorum.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/444</link><project id="" key="" /><description>When performing a "write" operation, allow to control if it can be performed only when `one` shard is active, a `quorum` of shards are active (within a replication group), or `all`.

This can be controlled per API call (REST parameter is `consistency` and can be set to `one`, `quorum`, or `all`. It defaults to a node level setting of `action.write_consistency` which in turn defaults to `quorum`. This basically means that the default is `quorum`.

What does this means? Basically, in a 1 shard with 2 replicas, there will have to be at least 2 shards active (`quorum`) within the cluster for the operation to be performed.

In a 1 shard with 1 replica, at least 1 shard will need to be active (in this case `quorum` and `one` is the same).
</description><key id="375816">444</key><summary>Write Consistency Level for index/delete/delete_by_query/bulk with one/quorum/all. Defaults to quorum.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.13.0</label></labels><created>2010-10-22T00:50:29Z</created><updated>2013-12-17T06:52:34Z</updated><resolved>2010-10-22T07:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-22T00:51:09Z" id="484985">Write Consistency Level for index/delete/delete_by_query/bulk with one/quorum/all. Defaults to quorum, closed by 5d1d927e09826c4e7593680a602e12789723c128.
</comment><comment author="clintongormley" created="2010-11-05T10:21:45Z" id="519590">What happens if a quorum isn't available - will the write hang or throw an error? 
</comment><comment author="kimchy" created="2010-11-05T16:16:38Z" id="520170">It will wait for the provided timeout parameter for the expected consistency level to be met, and if its not, it will bail with an exception.
</comment><comment author="ppearcy" created="2010-11-10T07:04:27Z" id="531020">Hey, 
  This should never effect consistency of search results, correct? Fundamentally, what does this setting get you? Maybe, a extra durability in case of a getting killed before snapshotting? 

Not sure if this is completely on topic for this, but when a doc is indexed, it can never fail on one replica and succeed on another, correct? I ask, because I have been and still am seeing minor consistency issues when I test bringing nodes up and down on latest master.

Thanks!
</comment><comment author="ppearcy" created="2010-11-10T16:45:33Z" id="532102">Err... nevermind about the second part... Everything has stayed consistent on 0.13-SNAPSHOT. 
</comment><comment author="kimchy" created="2010-11-10T20:12:26Z" id="532664">No, its just for writes, it basically allows to control for a write to succeed only if a specific number of shards to be available.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge mapping fails if original mapping does not have a index_analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/443</link><project id="" key="" /><description>If an index field mapping does not have an index_analyzer, during a mapping update which sets an analyzer, the mapping merge will fail. This seems like a change which should be permissable.

Current field mapping
subject: {
omit_term_freq_and_positions: false
index_name: "subject"
index: "no"
omit_norms: true
store: "no"
boost: 1
term_vector: "no"
type: "string"
}

Updated mapping:

"subject":{
"type":"string"
"analyzer":"fulltext_tokenizer"                // local custom mapping, used elsewhere
"omit_norms":true
"term_vector":"yes"
}

Casues a MergeMapping failure
Merge failed with failures {[mapper [subject] has different index values, mapper [subject] has different term_vector values, mapper [subject] has different index_analyzer, mapper [subject] has different search_analyzer]} 

Caused by: org.elasticsearch.transport.RemoteTransportException: [DM-ADSEARCHD102.dev.local][inet[/10.2.20.160:9300]][indices/mapping/put]
Caused by: org.elasticsearch.index.mapper.MergeMappingException: Merge failed with failures {[mapper [subject] has different index values, mapper [subject] has different term_vector values, mapper [subject] has different index_analyzer, mapper [subject] has different search_analyzer, mapper [primarywsodcompany] has different index_analyzer, mapper [primarywsodcompany] has different search_analyzer, mapper [primarywsodissue] has different index_analyzer, mapper [primarywsodissue] has different search_analyzer]}
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$3.execute(MetaDataMappingService.java:164)
    at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:154)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)

Occurs in 0.11 and 0.12

Thanks,

David
</description><key id="375332">443</key><summary>Merge mapping fails if original mapping does not have a index_analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbenson</reporter><labels /><created>2010-10-21T19:49:58Z</created><updated>2014-07-08T11:47:42Z</updated><resolved>2014-07-08T11:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-27T22:35:58Z" id="498183">I am not sure that it make sense to allow for this since analysis really changes the way text is tokenized, so part of the docs will match differently than others (it can't be applied to old docs).
</comment><comment author="ppearcy" created="2010-10-28T01:24:52Z" id="498451">Hey,
  Agreed that the analyzer can never be changed on an indexed field, but here index=no. The motivation is to be able to upgrade a retrievable field to searchable. The field is not be getting analyzed and there can't be any matching done on it. 

The alternative is to upgrade to a multi-field and have a .searchable version, with the original version a useless artifact. 

Any docs in the index would need to be resubmitted either way(I think).

Thanks,
Paul
</comment><comment author="ppearcy" created="2010-10-29T19:20:05Z" id="502972">FYI, was able to work around this case by manually hacking the meta. 
</comment><comment author="kimchy" created="2010-10-29T19:23:35Z" id="502982">I think I missed something, if `index` is set to `no`, why do you need to set the index analyzer? Are there `subject` fields on different types?
</comment><comment author="ppearcy" created="2010-10-29T19:25:52Z" id="502989">Hey, index=no was initially set on a field when it was just retrievable. We are not setting any analyzer, but it looks like elastic search is defaulting it. However, we then want to upgrade the field to searchable and set the correct analyzer. 

Make sense? 
</comment><comment author="kimchy" created="2010-10-29T19:28:05Z" id="503001">What do you mean by "when it was just retrievable"? You mean you haven't indexing anything to it yet?
</comment><comment author="ppearcy" created="2010-10-29T20:04:14Z" id="503089">Regarding your question, we had data populated for the ones where we had index=no, store=no. So, the data only existed in the JSON _source field. 

I think this stems from initially not getting dynamic mappings to work (have them working now) and some legacy config from that. 

Please correct me if I am wrong, but what we should be doing is:
1) Setting dynamic mapping to false
2) For any fields we want retrievable only, not set any mappings for them
3) This results in them being in the _source field, but not have any mapping set yet and basically ignored by underlying search. 
4) If we need to then make this field searchable, we set the mapping at that point, no need to upgrade. 
</comment><comment author="clintongormley" created="2014-07-08T11:47:42Z" id="48326028">No discussion for 4 years. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Client - Allow to set facets in binary format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/442</link><project id="" key="" /><description>Currently, one can set the query (and filter) as binary formats directly on the search request builder. Allow to set the facets in a similar manner.

Also, on the groovy client, expose setFacets that accepts a Closure allowing to explicitly set just the facets using a closure.
</description><key id="374880">442</key><summary>Java Client - Allow to set facets in binary format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mcclured</reporter><labels><label>enhancement</label><label>v0.15.0</label></labels><created>2010-10-21T15:49:31Z</created><updated>2011-01-30T13:53:26Z</updated><resolved>2011-01-30T13:53:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-01-30T13:53:26Z" id="722506">Java Client - Allow to set facets in binary format, closed by 94c632b79b18f6e9de721d2a35bd4cc7bd1fae36.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduce multiple indices overhead, don't create Indices data on nodes that don't hold any shard for an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/441</link><project id="" key="" /><description>Internal optimization. There is data associated with each index (parsed mappings, custom analyzers, and os on). Optimize to not create those index level structures if there is no shard of that index allocated on a node.
</description><key id="371558">441</key><summary>Reduce multiple indices overhead, don't create Indices data on nodes that don't hold any shard for an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.13.0</label></labels><created>2010-10-20T00:56:42Z</created><updated>2010-10-20T00:57:56Z</updated><resolved>2010-10-20T00:57:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-20T00:57:56Z" id="479857">Reduce multiple indices overhead, don't create Indices data on nodes that don't hold any shard for an index, closed by ae5bc20959a002e37b5d5ad3e3e02490ffd28338.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Camel case with node IDs containig an underscore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/440</link><project id="" key="" /><description>Hiya

Sometimes the node IDs include an underscore, eg "fujK74JaTCCAwlCH_ByXlg", but when you switch to using camelCase, these is changed to "fujK74JaTCCAwlCHByXlg" which, presumably, is incorrect:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/state?error_trace=1&amp;case=camelCase' 

# [Mon Oct 18 17:04:29 2010] Response:
# {
#    "clusterName" : "elasticsearch",
#    "blocks" : {},
#    "masterNode" : "c_v5CJzcRyW_59y6KzwDvw",
#    "allocations" : [],
#    "routingNodes" : {
#       "unassigned" : [],
#       "nodes" : {}
#    },
#    "routingTable" : {
#       "indices" : {}
#    },
#    "nodes" : {
#       "cV5CJzcRyW59y6KzwDvw" : {
#          "transportAddress" : "inet[/192.168.5.103:9300]",
#          "name" : "Clive",
#          "attributes" : {}
#       }
#    },
#    "metadata" : {
#       "indices" : {}
#    }
# }
```

Note the difference with camel case - the master node is correct, but the node name isn't

```
curl -XGET 'http://127.0.0.1:9200/_cluster/state?error_trace=1&amp;case=camelCase' 

# [Mon Oct 18 17:04:47 2010] Response:
# {
#    "clusterName" : "elasticsearch",
#    "blocks" : {},
#    "masterNode" : "c_v5CJzcRyW_59y6KzwDvw",
#    "allocations" : [],
#    "routingNodes" : {
#       "unassigned" : [],
#       "nodes" : {}
#    },
#    "routingTable" : {
#       "indices" : {}
#    },
#    "nodes" : {
#       "cV5CJzcRyW59y6KzwDvw" : {
#          "transportAddress" : "inet[/192.168.5.103:9300]",
#          "name" : "Clive",
#          "attributes" : {}
#       }
#    },
#    "metadata" : {
#       "indices" : {}
#    }
# }
```
</description><key id="368380">440</key><summary>Camel case with node IDs containig an underscore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-10-18T15:05:23Z</created><updated>2011-03-20T13:56:54Z</updated><resolved>2011-03-20T13:56:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-18T17:04:17Z" id="476052">pushed a fix for it.
</comment><comment author="clintongormley" created="2011-03-20T13:56:54Z" id="895190">Fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve automatic publish network address logic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/439</link><project id="" key="" /><description>Order the network interfaces by index, and then find the first non loopback address. This should give better out of the box experience, as VPN and vm ones are defined after the main one (en0/en1).
</description><key id="368216">439</key><summary>Improve automatic publish network address logic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-18T13:18:21Z</created><updated>2010-10-18T13:19:21Z</updated><resolved>2010-10-18T13:19:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-18T13:19:21Z" id="475540">Improve automatic publish network address logic, closed by 3a52c2f7f4f73615f2409a02298d5fbaaca53095.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: plugins should now be in extracted format under `ES_HOME/plugins`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/438</link><project id="" key="" /><description>Plugins should now be in their extracted form under `ES_HOME/plugins`. The `plugin -install` command does that automatically now, so no need to change anything. Building from master will require extracting a plugin into the plugins directory.
</description><key id="367927">438</key><summary>Plugins: plugins should now be in extracted format under `ES_HOME/plugins`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-18T09:13:26Z</created><updated>2010-10-18T09:14:13Z</updated><resolved>2010-10-18T09:14:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-18T09:14:13Z" id="475225">Plugins: plugins should now be in extracted format under `ES_HOME/plugins`, closed by 019c844dd1b44668d0d84020faa655eb54ee0a52.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Allow to configure `date_formats` only on the root object mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/437</link><project id="" key="" /><description>Currently, one can define `date_formats` on all level of object mappers (for dynamic identification of date fields). This is not easy to handle merging (like adding a new format) as well as more data stored per mapper. Move to only allow to define `date_formats` on the root object mapper and it will apply to all object mappers.
</description><key id="366992">437</key><summary>Mapper: Allow to configure `date_formats` only on the root object mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-17T17:56:05Z</created><updated>2010-10-17T17:57:07Z</updated><resolved>2010-10-17T17:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-17T17:57:07Z" id="474224">Mapper: Allow to configure `date_formats` only on the root object mapper, closed by 0a3d187e6a37af8e74ae54d9263d4659d5a8aab5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Statistical Facet: Allow to compute statistical facets on more than one field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/436</link><project id="" key="" /><description>Allow statistical facet to compute the numeric stats on more than one field. The format is simple:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "stat1" : {
            "statistical" : {
                "fields" : ["num1", "num2"]
            }
        }
    }
}
```
</description><key id="366808">436</key><summary>Statistical Facet: Allow to compute statistical facets on more than one field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-17T14:21:44Z</created><updated>2013-11-05T13:47:32Z</updated><resolved>2010-10-17T14:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-17T14:22:29Z" id="473982">Statistical Facet: Allow to compute statistical facets on more than one field, closed by 5d54e36964cbaaeb5a87c78867f6c17e26d4ad5d.
</comment><comment author="ventralnet" created="2013-11-05T13:47:32Z" id="27773301">Any chance that this functionality could be added to the statistical terms facet?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Numeric Range Filter -  A filter that uses the field data cache to perform numeric tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/435</link><project id="" key="" /><description>The field data cache loads all the values for a given field to memory and is used when sorting on a field or using it in facets.

The `numeric_range` filter (which has exactly the same format as `range` filter) uses field data cache to perform the numeric cache, which can provide much faster results than using the plain `range` filter, at the expense of more memory consumption (but if its already loaded since its sorted on or used in facets, no harm done...).

By default, this filter is _NOT_ cached (has `_cache` set to `false`). Note, caching a filter  is caching the _result_ of a filter for faster subsequent execution of the same filter (with same parameters).
</description><key id="365711">435</key><summary>Query DSL: Numeric Range Filter -  A filter that uses the field data cache to perform numeric tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-16T13:38:08Z</created><updated>2010-10-16T13:38:48Z</updated><resolved>2010-10-16T13:38:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-16T13:38:48Z" id="472757">Query DSL: Numeric Range Filter - A filter that uses the field data cache to perform numeric tests, closed by d9f966d83c60602067d903b975cc7bbfb5d4aa95.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>suggestion: index creation throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/434</link><project id="" key="" /><description>Hi,

The ability to set a maximum indexing rate (measured in any reasonable way) would help to ensure that a too eager indexing task doesn't overload the server: I am mostly thinking in the context of a mapreduce job (or any other highly parallelized task) creating an index. 

A dynamic property would be best. 

(my apologies if suggestions should go to the mailing list)
</description><key id="364619">434</key><summary>suggestion: index creation throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">angelf</reporter><labels /><created>2010-10-15T17:34:52Z</created><updated>2013-06-10T13:47:42Z</updated><resolved>2013-06-10T13:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-15T19:00:17Z" id="471642">hey, yea that make sense, though because it is distributed, the question is how to store the rate, either cluster wide, or per node. There are many other stats that I want to get nodes be aware of that will be able to control aspects like shard allocation as well as throttling indexing operations.

I hope to start and tackle it post this version.
</comment><comment author="angelf" created="2010-10-18T10:17:02Z" id="475286">Good to know you were thinking about it! Thanks.
</comment><comment author="spinscale" created="2013-06-10T13:47:42Z" id="19199338">Closing. Throttling has been implemented since quite some time. See http://www.elasticsearch.org/guide/reference/index-modules/store/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local Gateway: Don't block meta operations (delete index) on an index that is not recovered due to not all shards being available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/433</link><project id="" key="" /><description /><key id="363781">433</key><summary>Local Gateway: Don't block meta operations (delete index) on an index that is not recovered due to not all shards being available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-15T09:04:23Z</created><updated>2010-10-15T09:05:00Z</updated><resolved>2010-10-15T09:05:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-15T09:05:00Z" id="470212">Local Gateway: Don't block meta operations (delete index) on an index that is not recovered due to not all shards being available, closed by 6c9120a51b42312ad041cfd918cdf82ae69a5934.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rare node corruption during reallocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/432</link><project id="" key="" /><description>Issue details:
- Seen on 0.10-Snapshot and today on 0.11.0
- There was a rolling Node restart occurring that triggered this and this was done to add a new analyzer (I'd stated on IRC that the cluster was steady state and that is not the case)
- This implies to me that the corruption occurs when recovering from another node as opposed to recovery from the gateway.
- Starts with a single node. Can be corrected by clearing work directory for that node and forcing recovery from good node.
- Can be propogated to gateway and other nodes during recovery
- The REST API appears to route around the troubled index, while the java API toggles between success and failure as it round robins
- No exceptions show up in the elastic search log files. I had log levels up at DEBUG when this started and can provide any of the shard relocation details, but I don't see anything meaningful. 

Walking the logs, here is everything that I see related:

http://gist.github.com/627215

I can help out in anyway necessary to help track this down. 
</description><key id="363200">432</key><summary>Rare node corruption during reallocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-10-14T22:33:25Z</created><updated>2010-11-10T16:46:26Z</updated><resolved>2010-11-10T16:46:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-14T22:47:53Z" id="469511">Good catch on the relocation bit, now I can narrow down my test for this. Let me run some long running test that tries and simulate what you did, lets see if I manage to recreate this....
</comment><comment author="kimchy" created="2010-10-16T23:29:29Z" id="473394">Heya,

   I ran a test for 24 hours with nodes doing a rolling restart of each node while I have 5 proceses keep indexing (ended up indexing 5 billion docs). All went well... . What I did in the test is that after a restart of a node, I waited till the cluster health was green before restarting another node, maybe thats the difference between how you did the rolling restart (trying to guess where the problem might be).
</comment><comment author="ppearcy" created="2010-10-20T03:21:54Z" id="480074">Hey, sorry for the delayed response, missed this update. 

Always waiting for a green state before bouncing the next server. 

That sounds like a good stress test. Grasping at straws, but is the work directory being cleared on the restarted node? In my case, the work directory was not cleared on node restart. Also, are all the field types being represented? I'm using string and date/numeric fields, not sure why it'd matter, though.

We'll be moving up to 0.12 shortly and will keep our eyes out. I added a monitor on my side that should allow me to catch it when it happens. Also, have some metrics flowing to maybe correlate with some other condition.

Thanks
</comment><comment author="kimchy" created="2010-10-20T08:31:25Z" id="480562">I had two tests, one that deletes the work dir each time, and one that doesn't. Both seem to work... . 
</comment><comment author="ppearcy" created="2010-10-20T21:49:48Z" id="482217">Cool, thanks. Let's see if I can hit this condition again and see what other information I can collect that will be of use. 

Thanks!
</comment><comment author="ppearcy" created="2010-10-21T17:28:45Z" id="484225">Hey, came across this again, and this time it was 100% for sure from a recovery from the gateway. Not sure how to explain the inconsistencies of when I see this. 

There was a full cluster shutdown. I am not using the local gateway, at the moment, as you suggested. I don't see any relevant logging. 

Maybe it would make sense to update the test to include a full cluster restarts? Wouldn't hurt. 

Thanks again. Know this is one of those painful ones to get to the bottom of. 
</comment><comment author="kimchy" created="2010-10-21T18:04:58Z" id="484282">The rest I ran included only peer shard recovery, not gateway recovery. How do you know its coming from gateway recovery, is it because of the full cluster shutdown and then startup?
</comment><comment author="ppearcy" created="2010-10-21T18:11:15Z" id="484290">Yeah, in _this_ case it was from a full recovery from the gateway after a full cluster shutdown. 

Thanks
</comment><comment author="kimchy" created="2010-10-21T18:30:59Z" id="484337">I see... . Ok, let me create a similar (long running) test that does also full cluster recovery.
</comment><comment author="ppearcy" created="2010-10-26T19:51:45Z" id="494841">FYI, I am still hitting this issue. How can one tell from the logs if a recovery occurred from the gateway or a master node? 
</comment><comment author="kimchy" created="2010-10-26T19:53:11Z" id="494845">Set `index.gateway: DEBUG` for gateway info, and `index.shard.recovery: DEBUG` for shard recovery info.
</comment><comment author="kimchy" created="2010-11-01T11:43:42Z" id="508843">hey, spent most of this week trying to simulate this, and finally managed to. I am working on a fix (mostly done) and will be pushed to master. Will update more once I push it.
</comment><comment author="ppearcy" created="2010-11-01T15:44:22Z" id="509373">Awesome!!! I am extremely grateful for the work involved in tracking this down, know it wasn't easy. Thanks!!!
</comment><comment author="ppearcy" created="2010-11-10T16:46:26Z" id="532106">Hey, 
  Up on 0.13 and have been bouncing things and haven't hit this. Before, I would have by now. I believe this is fixed.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CouchDB River: Allow to define a javascript that can munge the changes stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/431</link><project id="" key="" /><description>A javascript can be provided within the `couchdb` settings that can munge the changes stream. The json provided to the script is under a var called `ctx` with the relevant seq stream change (for example, `ctx.doc` will refer to the document, or `ctx.deleted` is the flag if its deleted or not).

Note, this feature requires the `lang-javascript` plugin.

The `ctx.doc` can be changed and its value can will be indexed (assuming its not a deleted change). Also, if `ctx.ignore` is set to `true`, that change seq will be ignore and not applied.

Here is an example setting that adds `field1` with value `value1` to all docs.

```
{
    "type" : "couchdb",
    "couchdb" : {
        "script" : "ctx.doc.field1 = 'value1'"
    }
}
```
</description><key id="362560">431</key><summary>CouchDB River: Allow to define a javascript that can munge the changes stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-14T15:56:40Z</created><updated>2010-10-14T15:58:00Z</updated><resolved>2010-10-14T15:58:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-14T15:58:00Z" id="468677">CouchDB River: Allow to define a javascript that can munge the changes stream, closed by ed9d9aa358229ab67f89491ce2615bc1835afeb8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CouchDB River: Deletes not handles correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/430</link><project id="" key="" /><description>Deletes are not handled correctly in the couchdb river, resulting in not applying them.
</description><key id="362532">430</key><summary>CouchDB River: Deletes not handles correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.12.0</label></labels><created>2010-10-14T15:38:08Z</created><updated>2010-10-15T08:57:40Z</updated><resolved>2010-10-15T08:57:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-15T08:57:39Z" id="470204">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Allow to define scripts within `config/scripts`, automatically compiled and can be referenced by name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/429</link><project id="" key="" /><description>Allow to provide scripts to be precompiled on node startup within the `config/scripts` directory. Scripts end with the common lang extension (`mvel`, `js`, `py`, `groovy`).

The name of  a script is the name of the file, prefixed by internal directories. For example, if the script exists in `config/scripts/group1/group2/test.py`, the script name will be `group1_group2_test`.

The script name can then be used in any element that accepts a script, replacing the actual script content.
</description><key id="362390">429</key><summary>Scripting: Allow to define scripts within `config/scripts`, automatically compiled and can be referenced by name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-14T14:13:59Z</created><updated>2010-10-14T14:14:49Z</updated><resolved>2010-10-14T14:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-14T14:14:49Z" id="468439">Scripting: Allow to define scripts within `config/scripts`, automatically compiled and can be referenced by name, closed by ae05ce0551f23c7709e30b3ce26ed0ce4223ebdb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Allow to place "extracted" plugins under the `plugins` directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/428</link><project id="" key="" /><description>Any folder placed within the plugins directory will automatically be added to the classpath (its root), and any jars within it will be added as well.
</description><key id="362186">428</key><summary>Plugins: Allow to place "extracted" plugins under the `plugins` directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-14T12:03:22Z</created><updated>2010-10-14T12:04:04Z</updated><resolved>2010-10-14T12:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-14T12:04:04Z" id="468193">Plugins: Allow to place "extracted" plugins under the `plugins` directory, closed by e0488fa87e61469a8c68340857f3151b359112e6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to close (and re-open) an index in ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/427</link><project id="" key="" /><description>Close an index to minimize/eliminate the overhead. 
Once an index is closed ES will just maintain the index metadata and nothing else (with the fact that its closed / blocked). Shards will be deallocated, no active lucene indices will be active (shards). Then, when opening an index, it will be "recovered" using the usual recovery mechanism that is done on full cluster restart (only for that index).
http://elasticsearch-users.115913.n3.nabble.com/Using-Many-Indexes-tp1696219.html
</description><key id="361346">427</key><summary>Ability to close (and re-open) an index in ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">berkay</reporter><labels /><created>2010-10-13T21:26:22Z</created><updated>2010-10-23T19:58:38Z</updated><resolved>2010-10-23T19:58:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-23T19:58:38Z" id="488245">closing, implemented in #447.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an Update action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/426</link><project id="" key="" /><description>We can create an update action that is able to do simple updating operation on a Document such as:
- change values in place,
- add new values,
- remove values,
  ...

A good reference could be the monogodb update command: http://www.mongodb.org/display/DOCS/Updating
</description><key id="361318">426</key><summary>Add an Update action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2010-10-13T21:11:14Z</created><updated>2013-04-04T18:24:19Z</updated><resolved>2013-04-04T18:24:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alheim" created="2011-10-19T09:01:23Z" id="2453478">That would be soooo nice. Today we need to get and reindex. For big / complex documents, there is a network overload.
</comment><comment author="Paikan" created="2011-10-19T09:07:45Z" id="2453530">An Update action is under developement right now and should be released soon hopefully.

It will be even more flexible than mongodb API as you will be able to update your document using scripts (native, mvel, javascript, python, groovy).

As a side note it is important to note that the update action will basically do the same thing as a client would do now i.e. get the doc, update it and reindex it entirely. 

I think we will have to wait for lucene 4.0 or later for real partial updates on a per field basis.
</comment><comment author="aparo" created="2011-10-20T07:23:40Z" id="2465570">Yes I also watching the update repository.
</comment><comment author="plaflamme" created="2011-11-28T15:39:27Z" id="2898394">+1

Would be nice to see this as a core feature in ES.
</comment><comment author="bryangreen" created="2011-12-20T02:17:03Z" id="3213235">+1
</comment><comment author="heyarny" created="2011-12-22T18:52:07Z" id="3253022">+1
Any progress on this?
</comment><comment author="Paikan" created="2012-01-04T10:19:02Z" id="3352799">Shouldn't we close this issue as it has been addressed by commit 83d5084 ?
</comment><comment author="clintongormley" created="2013-04-04T18:24:19Z" id="15914662">Update functionality added
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build fails with exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/425</link><project id="" key="" /><description>When we did our last build on the 8th, the github master worked ok.

Now (as of 9077bb6528f843c) it's failing with

`
309 tests completed, 1 failure

FAILURE: Build failed with an exception.
- Where:
  Build file '/pkg/src/elasticsearch/modules/elasticsearch/build.gradle'
- What went wrong:
  Execution failed for task ':elasticsearch:test'.
  Cause: There were failing tests. See the report at /pkg/src/elasticsearch/modules/elasticsearch/build/reports/tests.
- Try:
  Run with -s or -d option to get more details. Run with -S option to get the full (very verbose) stacktrace.

BUILD FAILED

Total time: 1 mins 57.161 secs

`
</description><key id="360097">425</key><summary>Build fails with exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels /><created>2010-10-13T06:43:03Z</created><updated>2013-04-04T18:23:44Z</updated><resolved>2013-04-04T18:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abh" created="2010-10-13T07:03:24Z" id="465423">```
[root@dev2.la elasticsearch]# ./gradlew -s build release
:jarjar:compileJava UP-TO-DATE
:jarjar:processResources UP-TO-DATE
:jarjar:classes UP-TO-DATE
:jarjar:jar UP-TO-DATE
:test-testng:compileJava UP-TO-DATE
:elasticsearch:compileJava UP-TO-DATE
:test-testng:processResources UP-TO-DATE
:elasticsearch:processResources UP-TO-DATE
:test-testng:classes UP-TO-DATE
:elasticsearch:classes UP-TO-DATE
:test-testng:jar UP-TO-DATE
&gt; Building &gt; :elasticsearch:jar[root@dev2.la elasticsearch]# 
[root@dev2.la elasticsearch]# 
[root@dev2.la elasticsearch]# ./gradlew -s build release
:jarjar:compileJava UP-TO-DATE
:jarjar:processResources UP-TO-DATE
:jarjar:classes UP-TO-DATE
:jarjar:jar UP-TO-DATE
:test-testng:compileJava UP-TO-DATE
:elasticsearch:compileJava UP-TO-DATE
:test-testng:processResources UP-TO-DATE
:elasticsearch:processResources UP-TO-DATE
:test-testng:classes UP-TO-DATE
:elasticsearch:classes UP-TO-DATE
:test-testng:jar UP-TO-DATE
:elasticsearch:jar
[ant:jar] error while reading original manifest in file: /pkg/src/elasticsearch/modules/elasticsearch/build/libs/elasticsearch-0.12.0-SNAPSHOT.jar.merged.jar due to error in opening zip file
:test-testng:javadoc UP-TO-DATE
:elasticsearch:javadoc UP-TO-DATE
:elasticsearch:javadocJar UP-TO-DATE
:elasticsearch:sourcesJar UP-TO-DATE
:benchmark-micro:compileJava
:benchmark-micro:processResources UP-TO-DATE
:benchmark-micro:classes
:benchmark-micro:jar UP-TO-DATE
:test-testng:assemble UP-TO-DATE
:elasticsearch:assemble
:benchmark-micro:assemble
:test-testng:compileTestJava UP-TO-DATE
:elasticsearch:compileTestJava UP-TO-DATE
:benchmark-micro:compileTestJava
:test-testng:processTestResources UP-TO-DATE
:elasticsearch:processTestResources UP-TO-DATE
:benchmark-micro:processTestResources UP-TO-DATE
:test-testng:testClasses UP-TO-DATE
:elasticsearch:testClasses UP-TO-DATE
:benchmark-micro:testClasses
:test-testng:test UP-TO-DATE
:elasticsearch:test
[23:44:17,275][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,292][INFO ][cluster.routing.allocation] Adding two nodes and performing rerouting
[23:44:17,302][INFO ][cluster.routing.allocation] Start the primary shards
[23:44:17,303][INFO ][cluster.routing.allocation] Start the replica shards
[23:44:17,303][INFO ][cluster.routing.allocation] Start another node and perform rerouting
[23:44:17,304][INFO ][cluster.routing.allocation] find the replica shard that gets relocated
[23:44:17,305][INFO ][cluster.routing.allocation] kill the node [node1] of the primary shard for the relocating replica
[23:44:17,305][INFO ][cluster.routing.allocation] make sure all the primary shards are active
[23:44:17,310][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,311][INFO ][cluster.routing.allocation] Adding one node and performing rerouting
[23:44:17,312][INFO ][cluster.routing.allocation] Add another node and perform rerouting
[23:44:17,313][INFO ][cluster.routing.allocation] Start the primary shards
[23:44:17,315][INFO ][cluster.routing.allocation] Reroute, nothing should change
[23:44:17,315][INFO ][cluster.routing.allocation] Fail backup shards on node2
[23:44:17,321][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,322][INFO ][cluster.routing.allocation] Adding two nodes and performing rerouting
[23:44:17,322][INFO ][cluster.routing.allocation] Start the shards (primaries)
[23:44:17,323][INFO ][cluster.routing.allocation] Start the shards (backups)
[23:44:17,323][INFO ][cluster.routing.allocation] Adding third node and reroute
[23:44:17,327][INFO ][cluster.routing.allocation] Fail the shards on node 3
[23:44:17,327][INFO ][cluster.routing.allocation] Do another reroute, should try and assign again to node 3
[23:44:17,328][INFO ][cluster.routing.allocation] Start the shards on node 3
[23:44:17,332][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,333][INFO ][cluster.routing.allocation] Adding two nodes and performing rerouting
[23:44:17,333][INFO ][cluster.routing.allocation] Start the primary shard (on node1)
[23:44:17,333][INFO ][cluster.routing.allocation] Start the backup shard (on node2)
[23:44:17,333][INFO ][cluster.routing.allocation] Adding third node and reroute and kill first node
[23:44:17,338][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,339][INFO ][cluster.routing.allocation] Adding two nodes and performing rerouting
[23:44:17,339][INFO ][cluster.routing.allocation] Start the primary shard (on node1)
[23:44:17,340][INFO ][cluster.routing.allocation] start another node, replica will start recovering form primary
[23:44:17,341][INFO ][cluster.routing.allocation] start another node, make sure the primary is not relocated
[23:44:17,346][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,346][INFO ][cluster.routing.allocation] start two nodes and fully start the shards
[23:44:17,350][INFO ][cluster.routing.allocation] start all the primary shards, replicas will start initializing
[23:44:17,351][INFO ][cluster.routing.allocation] now, start 8 more nodes, and check that no rebalancing/relocation have happened
[23:44:17,352][INFO ][cluster.routing.allocation] start the replica shards, rebalancing should start
[23:44:17,353][INFO ][cluster.routing.allocation] complete relocation, other half of relocation should happen
[23:44:17,361][INFO ][cluster.routing.allocation] complete relocation, thats it!
[23:44:17,366][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,366][INFO ][cluster.routing.allocation] Adding one node and performing rerouting
[23:44:17,366][INFO ][cluster.routing.allocation] Start all the primary shards
[23:44:17,371][INFO ][cluster.routing.allocation] Building initial routing table with 50 indices
[23:44:17,376][INFO ][cluster.routing.allocation] Adding 25 nodes
[23:44:17,385][INFO ][cluster.routing.allocation] Adding additional 25 nodes, nothing should change
[23:44:17,387][INFO ][cluster.routing.allocation] Marking the shard as started
[23:44:17,396][INFO ][cluster.routing.allocation] Building initial routing table with 10 indices
[23:44:17,397][INFO ][cluster.routing.allocation] Starting 3 nodes and rerouting
[23:44:17,398][INFO ][cluster.routing.allocation] Start two more nodes, things should remain the same
[23:44:17,399][INFO ][cluster.routing.allocation] Now, mark the relocated as started
[23:44:17,402][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,403][INFO ][cluster.routing.allocation] Adding one node and rerouting
[23:44:17,404][INFO ][cluster.routing.allocation] Marking the shard as failed
[23:44:17,409][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,409][INFO ][cluster.routing.allocation] Adding one node and performing rerouting
[23:44:17,409][INFO ][cluster.routing.allocation] Rerouting again, nothing should change
[23:44:17,410][INFO ][cluster.routing.allocation] Marking the shard as started
[23:44:17,410][INFO ][cluster.routing.allocation] Starting another node and making sure nothing changed
[23:44:17,410][INFO ][cluster.routing.allocation] Killing node1 where the shard is, checking the shard is relocated
[23:44:17,410][INFO ][cluster.routing.allocation] Start another node, make sure that things remain the same (shard is in node2 and initializing)
[23:44:17,410][INFO ][cluster.routing.allocation] Start the shard on node 2
[23:44:17,414][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,415][INFO ][cluster.routing.allocation] Adding one node and performing rerouting
[23:44:17,415][INFO ][cluster.routing.allocation] Add another node and perform rerouting, nothing will happen since primary shards not started
[23:44:17,415][INFO ][cluster.routing.allocation] Start the primary shard (on node1)
[23:44:17,416][INFO ][cluster.routing.allocation] Reroute, nothing should change
[23:44:17,416][INFO ][cluster.routing.allocation] Start the backup shard
[23:44:17,416][INFO ][cluster.routing.allocation] Kill node1, backup shard should become primary
[23:44:17,416][INFO ][cluster.routing.allocation] Start another node, backup shard should start initializing
[23:44:17,420][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,421][INFO ][cluster.routing.allocation] Adding one node and performing rerouting
[23:44:17,423][INFO ][cluster.routing.allocation] Add another node and perform rerouting, nothing will happen since primary not started
[23:44:17,424][INFO ][cluster.routing.allocation] Start the primary shard (on node1)
[23:44:17,425][INFO ][cluster.routing.allocation] Reroute, nothing should change
[23:44:17,425][INFO ][cluster.routing.allocation] Start the backup shard
[23:44:17,426][INFO ][cluster.routing.allocation] Add another node and perform rerouting
[23:44:17,426][INFO ][cluster.routing.allocation] Start the shards on node 3
[23:44:17,431][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,431][INFO ][cluster.routing.allocation] start one node, do reroute, only 3 should initialize
[23:44:17,432][INFO ][cluster.routing.allocation] start initializing, another 3 should initialize
[23:44:17,433][INFO ][cluster.routing.allocation] start initializing, another 3 should initialize
[23:44:17,434][INFO ][cluster.routing.allocation] start initializing, another 1 should initialize
[23:44:17,435][INFO ][cluster.routing.allocation] start initializing, all primaries should be started
[23:44:17,439][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,439][INFO ][cluster.routing.allocation] start one node, do reroute, only 3 should initialize
[23:44:17,440][INFO ][cluster.routing.allocation] start initializing, another 2 should initialize
[23:44:17,440][INFO ][cluster.routing.allocation] start initializing, all primaries should be started
[23:44:17,441][INFO ][cluster.routing.allocation] start another node, replicas should start being allocated
[23:44:17,441][INFO ][cluster.routing.allocation] start initializing replicas
[23:44:17,442][INFO ][cluster.routing.allocation] start initializing replicas, all should be started
[23:44:17,445][INFO ][cluster.routing.allocation] Building initial routing table
[23:44:17,445][INFO ][cluster.routing.allocation] Adding two nodes and performing rerouting
[23:44:17,446][INFO ][cluster.routing.allocation] Start all the primary shards
[23:44:17,446][INFO ][cluster.routing.allocation] Start all the replica shards
[23:44:17,446][INFO ][cluster.routing.allocation] add another replica
[23:44:17,447][INFO ][cluster.routing.allocation] Add another node and start the added replica
[23:44:17,447][INFO ][cluster.routing.allocation] now remove a replica
[23:44:17,447][INFO ][cluster.routing.allocation] do a reroute, should remain the same
[23:44:25,360][INFO ][transport                ] bound_address {local[1]}, publish_address {local[1]}
[23:44:25,360][INFO ][transport                ] bound_address {local[2]}, publish_address {local[2]}
Test org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPingTests FAILED
[23:44:26,536][INFO ][transport                ] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:26,545][INFO ][transport                ] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
[23:44:29,368][INFO ][index.gateway            ] serialized commit_point {
  "version" : 1,
  "name" : "test",
  "type" : "GENERATED",
  "index_files" : {
    "file1" : {
      "physical_name" : "file1_p",
      "length" : 100
    },
    "file2" : {
      "physical_name" : "file2_p",
      "length" : 200
    }
  },
  "translog_files" : {
    "t_file1" : {
      "physical_name" : "t_file1_p",
      "length" : 100
    },
    "t_file2" : {
      "physical_name" : "t_file2_p",
      "length" : 200
    }
  }
}
[23:44:32,357][INFO ][transport                ] bound_address {local[3]}, publish_address {local[3]}
[23:44:32,358][INFO ][transport                ] bound_address {local[4]}, publish_address {local[4]}
[23:44:32,364][INFO ][transport                ] bound_address {local[5]}, publish_address {local[5]}
[23:44:32,364][INFO ][transport                ] bound_address {local[6]}, publish_address {local[6]}
[23:44:32,387][INFO ][transport                ] bound_address {local[7]}, publish_address {local[7]}
[23:44:32,388][INFO ][transport                ] bound_address {local[8]}, publish_address {local[8]}
[23:44:32,399][INFO ][transport                ] bound_address {local[9]}, publish_address {local[9]}
[23:44:32,399][INFO ][transport                ] bound_address {local[10]}, publish_address {local[10]}
[23:44:32,408][INFO ][transport                ] bound_address {local[11]}, publish_address {local[11]}
[23:44:32,408][INFO ][transport                ] bound_address {local[12]}, publish_address {local[12]}
[23:44:32,798][WARN ][transport                ] Transport response handler timed out, action [sayHelloTimeoutDelayedResponse], node [[A][local[11]]]
[23:44:33,270][INFO ][transport                ] bound_address {local[13]}, publish_address {local[13]}
[23:44:33,270][INFO ][transport                ] bound_address {local[14]}, publish_address {local[14]}
[23:44:33,480][INFO ][transport                ] [A] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:33,482][INFO ][transport                ] [B] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
[23:44:33,539][INFO ][transport                ] [A] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:33,547][INFO ][transport                ] [B] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
[23:44:33,656][INFO ][transport                ] [A] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:33,659][INFO ][transport                ] [B] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
[23:44:33,687][INFO ][transport                ] [A] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:33,687][INFO ][transport                ] [B] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
[23:44:33,776][INFO ][transport                ] [A] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:33,781][INFO ][transport                ] [B] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
[23:44:34,107][WARN ][transport                ] [B] Transport response handler timed out, action [sayHelloTimeoutDelayedResponse], node [[A][inet[/10.50.3.52:9300]]]
[23:44:34,536][INFO ][transport                ] [A] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.50.3.52:9300]}
[23:44:34,542][INFO ][transport                ] [B] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.50.3.52:9301]}
Test org.elasticsearch.util.ThreadLocalsTests FAILED
309 tests completed, 2 failures

FAILURE: Build failed with an exception.

* Where:
Build file '/pkg/src/elasticsearch/modules/elasticsearch/build.gradle'

* What went wrong:
Execution failed for task ':elasticsearch:test'.
Cause: There were failing tests. See the report at /pkg/src/elasticsearch/modules/elasticsearch/build/reports/tests.

* Try:
Run with -S option to get the full (very verbose) stacktrace.

* Exception is:
org.gradle.api.tasks.LocationAwareTaskExecutionException: Build file '/pkg/src/elasticsearch/modules/elasticsearch/build.gradle'
Execution failed for task ':elasticsearch:test'.
    at org.gradle.api.internal.tasks.DefaultTaskExecuter.executeActions(DefaultTaskExecuter.java:63)
    at org.gradle.api.internal.tasks.DefaultTaskExecuter.execute(DefaultTaskExecuter.java:41)
    at org.gradle.api.internal.project.taskfactory.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:32)
    at org.gradle.api.internal.project.taskfactory.ExecutionShortCircuitTaskExecuter.execute(ExecutionShortCircuitTaskExecuter.java:50)
    at org.gradle.api.internal.tasks.SkipTaskExecuter.doExecute(SkipTaskExecuter.java:57)
    at org.gradle.api.internal.tasks.SkipTaskExecuter.execute(SkipTaskExecuter.java:35)
    at org.gradle.api.internal.tasks.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:32)
    at org.gradle.api.internal.AbstractTask.execute(AbstractTask.java:231)
    at org.gradle.execution.DefaultTaskGraphExecuter.executeTask(DefaultTaskGraphExecuter.java:167)
    at org.gradle.execution.DefaultTaskGraphExecuter.doExecute(DefaultTaskGraphExecuter.java:160)
    at org.gradle.execution.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:78)
    at org.gradle.execution.TaskNameResolvingBuildExecuter.execute(TaskNameResolvingBuildExecuter.java:161)
    at org.gradle.execution.DelegatingBuildExecuter.execute(DelegatingBuildExecuter.java:54)
    at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:153)
    at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:107)
    at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:75)
    at org.gradle.launcher.Main.execute(Main.java:93)
    at org.gradle.launcher.Main.main(Main.java:42)
    at org.gradle.launcher.GradleMain.main(GradleMain.java:49)
    at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:37)
    at org.gradle.wrapper.Wrapper.execute(Wrapper.java:58)
    at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:42)
Caused by: org.gradle.api.GradleException: There were failing tests. See the report at /pkg/src/elasticsearch/modules/elasticsearch/build/reports/tests.
    at org.gradle.api.tasks.testing.Test.executeTests(Test.java:331)
    at org.gradle.api.internal.BeanDynamicObject.invokeMethod(BeanDynamicObject.java:158)
    at org.gradle.api.internal.CompositeDynamicObject.invokeMethod(CompositeDynamicObject.java:93)
    at org.gradle.api.tasks.testing.Test_Decorated.invokeMethod(Unknown Source)
    at org.gradle.util.ReflectionUtil.invoke(ReflectionUtil.groovy:23)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$2.execute(AnnotationProcessingTaskFactory.java:131)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$2.execute(AnnotationProcessingTaskFactory.java:129)
    at org.gradle.api.internal.tasks.DefaultTaskExecuter.executeActions(DefaultTaskExecuter.java:55)
    ... 21 common frames omitted

BUILD FAILED

Total time: 48.503 secs
[root@dev2.la elasticsearch]# 
```
</comment><comment author="kimchy" created="2010-10-13T09:42:11Z" id="465624">Hey,

   Yea, that ThreadLocalTests if flacky, haven't figured out why (it does not fail on my machine). Its not really important for you, just run `gradlew clean release` which will just create an installation without running the tests.
</comment><comment author="clintongormley" created="2013-04-04T18:23:44Z" id="15914630">Old bug - has been fixed at some stage
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Fine grained filter caching control with sensible defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/424</link><project id="" key="" /><description>A more fine grained control over filters and if they should be cached or not. Caching a filter basically means building a fast in memory lookup of doc_id -&gt; bit (0 does not match, 1 match). For some filters, it make sense to default to cache them, and some not. All allow to change the default behavior by setting  `_cache`  to either `true` or `false`.

The following filters are cached by default: `prefix`, `term`, `terms`, `range`.

The following filters are not cached by default: `and`, `bool`, `fquery`, `geo_bounding_box`, `geo_distance`, `geo_polygon`, `not`, `or`, `script`.

The ones that are not cached are already either fast since they work against cached field data, or the chances of executing the same filter are low enough that it does not make sense to default it.

There are cases though where it does make sense to cache it. For example, executing a common filter for distance from a certain point should enable caching.
</description><key id="359733">424</key><summary>Query DSL: Fine grained filter caching control with sensible defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-12T23:37:00Z</created><updated>2010-10-12T23:37:56Z</updated><resolved>2010-10-12T23:37:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-12T23:37:56Z" id="464862">Query DSL: Fine grained filter caching control with sensible defaults, closed by 8a8a4d648aeffd8b196f31a866de2c12a5b5663b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: Breakout data nodes vs non-data nodes in the cluster health page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/423</link><project id="" key="" /><description>It'd be useful when looking at cluster health which how many data nodes there are. We're running an app that acts as a non-data node and this view would be very useful to disambiguate the cluster health. 

Not a high priority and just a nice to have, as this data can be pulled from the cluster state API.

Thanks!
</description><key id="358918">423</key><summary>Feature Request: Breakout data nodes vs non-data nodes in the cluster health page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-10-12T16:00:15Z</created><updated>2010-10-27T22:40:25Z</updated><resolved>2010-10-18T03:36:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2010-10-13T00:31:14Z" id="464952">Also, could the cluster name be included in the cluster health, as well? 
</comment><comment author="kimchy" created="2010-10-17T17:06:45Z" id="474163">Hey,

   I've added the cluster name. Regarding the separation of data nodes and non data nodes, you can get that info from the node info API. Not sure it make sense to add more info to the cluster health, as it will complicate it... .
</comment><comment author="ppearcy" created="2010-10-17T20:36:51Z" id="474467">Cool, thanks. Yeah, if adding data vs non-data node to that complicates things, not worth it.
</comment><comment author="kimchy" created="2010-10-27T22:40:25Z" id="498191">Thought about it a bit more, not sure why I said it does not make sense. Added `number_of_data_nodes` to the response in master (0.13)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Base64 encoding for UUID generation (auto generated doc ids, and node ids)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/422</link><project id="" key="" /><description>This will be a big win for auto generated node ids, as it will have less data to store per id.
</description><key id="357639">422</key><summary>Use Base64 encoding for UUID generation (auto generated doc ids, and node ids)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-11T22:16:38Z</created><updated>2014-06-09T10:29:47Z</updated><resolved>2010-10-11T22:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-11T22:17:10Z" id="462083">Use Base64 encoding for UUID generation (auto generated doc ids, and node ids), closed by c3cb5a3e349e36d4cd0f948331253032051a623a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API: Rename `index` to `_index`, `type` to `_type` and `id` to `_id`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/421</link><project id="" key="" /><description>The bulk API is not consistent with the other APIs where the index name `_index`, type `_type`, and id `_id` are used. Rename it.
</description><key id="355574">421</key><summary>Bulk API: Rename `index` to `_index`, `type` to `_type` and `id` to `_id`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.12.0</label></labels><created>2010-10-10T17:30:51Z</created><updated>2010-10-10T17:32:04Z</updated><resolved>2010-10-10T17:32:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-10T17:32:04Z" id="459477">Bulk API: Rename `index` to `_index`, `type` to `_type` and `id` to `_id`, closed by a14b73bc13b3bc1d060229928854b23e0385ed4a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RegexTermQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/420</link><project id="" key="" /><description>With missing files.
</description><key id="354907">420</key><summary>RegexTermQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2010-10-09T21:50:24Z</created><updated>2014-07-16T21:56:59Z</updated><resolved>2011-02-10T13:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-23T20:01:31Z" id="488249">hey, mind going for another pull request? with master up to date?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RegexTermQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/419</link><project id="" key="" /><description>This is the RegexTermQuery implementation. Tested remotely and with a test in SimpleIndexQueryParserTests.java.

I try a pull request, but the system tried to fire all commits that I have done and reverted in my branch.
</description><key id="354904">419</key><summary>RegexTermQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2010-10-09T21:48:32Z</created><updated>2014-07-16T21:56:59Z</updated><resolved>2011-02-10T13:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Python Plugin: Allow to use python for scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/418</link><project id="" key="" /><description>Allow to use python as scripts (`lang` set to `python`). `doc`, `_fields`, and `_source` are dicts.
</description><key id="354780">418</key><summary>Python Plugin: Allow to use python for scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-09T18:59:20Z</created><updated>2010-10-09T18:59:48Z</updated><resolved>2010-10-09T18:59:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-09T18:59:48Z" id="458351">Python Plugin: Allow to use python for scripts, closed by a754ebacd49f8cdf75009d9d65fc6f9efe4560f9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JSON / as escape character is not decoded in 0.11.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/417</link><project id="" key="" /><description>This bug report is the result of a problem I had in this topic: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/dd5a93a8a0c6d52a/f0d473fc61fa7138#f0d473fc61fa7138

/ does seem to be an escapable character, which is not handled as such by Elasticsearch, although json.org (http://www.json.org/string.gif) and the jackson source (http://svn.jackson.codehaus.org/browse/jackson/trunk/src/java/org/codehaus/jackson/impl/ReaderBasedParser.java?r=HEAD#l881) and the PHP source code (http://svn.php.net/repository/php/php-src/branches/PHP_5_3/ext/json/json.c at line 352), all handle it as escape character.

We use PHP to communicate with elasticsearch, which uses json_encode, which escapes the / character. So when pushing in base64 attachments, it gives me the following error:
     Caused by: org.elasticsearch.common.jackson.JsonParseException:
Failed to decode VALUE_STRING as base64 (MIME-NO-LINEFEEDS): Illegal
character '\' (code 0x5c) in base64 content 

It could be that it is only with base64 content and not string content, but i'm not sure, because that I did not test yet.
</description><key id="354421">417</key><summary>JSON / as escape character is not decoded in 0.11.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meghuizen</reporter><labels /><created>2010-10-09T09:30:56Z</created><updated>2013-04-04T18:22:43Z</updated><resolved>2013-04-04T18:22:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-10T17:20:06Z" id="459464">It looks like its a problem in base64 decoding. In jackson there are 4 options, the default one is MIME_NO_LINEFEEDS. Maybe, the MIME type should be used? I can change it and you can test it out if you want. Here are the variants: http://jackson.codehaus.org/1.6.0/javadoc/org/codehaus/jackson/Base64Variants.html.
</comment><comment author="meghuizen" created="2010-10-19T17:02:07Z" id="478763">After reading what the base64 encoding options are, my first impression is that MIME_NO_LINEFEEDS is the correct default, which should be used, which you use already. So let me first define a test case. I've already pulled the repository, so I can test it. Because changing this could also lead a bit of incompatibility via other languages/bridges.

So please leave this open for a while.
</comment><comment author="kimchy" created="2010-10-19T17:21:32Z" id="478819">Hey,

  Thanks for the effort!. One option can be to provide an option to define how to decode the base64 (in what format).
</comment><comment author="clintongormley" created="2013-04-04T18:22:43Z" id="15914563">No more comments after 2 years - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Twitter River: Support filter stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/416</link><project id="" key="" /><description>http://dev.twitter.com/pages/streaming_api_methods

Filter stream can be configured to support `tracks`, `follow`, and `locations`. The configuration is the same as the twitter API (a single comma separated string value, or using json arrays). Here is an example:

```
{
    "type" : "twitter",
    "twitter" : {
        "user" : "me",
        "passowrd" : "123456",
        "filter" : {
            "tracks" : "test,something,please",
            "follow" : "111,222,333",
            "locations" : "-122.75,36.8,-121.75,37.8,-74,40,-73,41"
        }
    }
}
```

Here is an array based configuration example:

```
{
    "type" : "twitter",
    "twitter" : {
        "user" : "me",
        "passowrd" : "123456",
        "filter" : {
            "tracks" : ["test", "something"],
            "follow" : [111, 222, 333],
            "locations" : [ [-122.75,36.8], [-121.75,37.8], [-74,40], [-73,41]]
        }
    }
}
```
</description><key id="353975">416</key><summary>Twitter River: Support filter stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-08T21:40:09Z</created><updated>2010-10-08T22:55:49Z</updated><resolved>2010-10-09T05:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-08T22:39:17Z" id="457330">Twitter River: Support filter stream, closed by 8b03b914f94e175091636fed34b9ec598de0cb6e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thrift: Rename Status.CONTINUE to Status.CONT (reserved word)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/415</link><project id="" key="" /><description /><key id="353916">415</key><summary>Thrift: Rename Status.CONTINUE to Status.CONT (reserved word)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.12.0</label></labels><created>2010-10-08T21:06:04Z</created><updated>2010-10-08T21:06:38Z</updated><resolved>2010-10-08T21:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-08T21:06:38Z" id="457174">Thrift: Rename Status.CONTINUE to Status.CONT (reserved word), closed by ee2fabb9ddb5cc8f17051a64b6237356927bac80.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo Overhaul (work with multiple locations)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/414</link><project id="" key="" /><description>The current geo support does not work well when a document as multiple locations. For this reason, the structure of how geo information is indexed requires changing.

Note, sadly, this requires both reindexing of the data if geo features are used, and requires to predefine the `geo_point` type for geo location objects (where before, just an object with `lat` and `lon` fields were enough).

The `geo_point` has been changed. Now, a string representation (lat,lon) is always stored under the object name, for example:

```
{
    "pin" : {
        "location" : {
            "lat" : 40.12,
            "lon" : -71.34
        }
    }
}
```

Will cause a field called `location` with the "40.12,-71.34" value to be stored. lat/lon are not stored by default now, since they are not needed, geohash can still be enabled as well.

The new options for `geo_point` are:
- `lat_lon`: Set to `true` to also index a `lat` and `lon` fields.
- `geohash`: Set to `true` to also index a `geohash` (under `pin.location.geohash` in our example).
- `geo_precision`: The geohash precision. Defaults to `12`.

When loading a geo point in a script using the `doc[...]` notion, a value (for example: `doc['location'].value`) returns a `GeoPoint`, which allows to access `lat`, `lon`, and `geohash` within the script (much simpler to be used in scripts compared to previously).
</description><key id="353841">414</key><summary>Geo Overhaul (work with multiple locations)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>bug</label><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-08T20:30:50Z</created><updated>2010-10-13T01:34:53Z</updated><resolved>2010-10-09T03:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-08T20:31:28Z" id="457095">Geo Overhaul (work with multiple locations), closed by 6314c2460cc55cba5f7c5c7826b63b4278eaffb1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: query_string / field to use the optimized match_all query when using * (or *:*)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/413</link><project id="" key="" /><description /><key id="351488">413</key><summary>Query DSL: query_string / field to use the optimized match_all query when using * (or *:*)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-07T14:47:59Z</created><updated>2010-10-07T14:48:33Z</updated><resolved>2010-10-07T14:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-07T14:48:33Z" id="453996">Query DSL: query_string / field to use the optimized match_all query when using \* (or _:_), closed by d0bf743ab4ebb2654fe183777ccb396ba703673e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RSS river with support for pubsubhubbub</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/412</link><project id="" key="" /><description>What also whould be nice as river, would be a RSS river. Which will index the RSS via pubsubhubbub when possible, and if not, use a polling method. With an option to index the rssitem.link (the source article) as attachment. 
</description><key id="351447">412</key><summary>RSS river with support for pubsubhubbub</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meghuizen</reporter><labels /><created>2010-10-07T14:31:30Z</created><updated>2012-03-13T10:02:08Z</updated><resolved>2012-03-13T10:02:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gotime" created="2010-10-07T17:51:58Z" id="454469">just a note, pubsubhubbub uses atom, not rss. This would be a pretty cool river.
</comment><comment author="skurfuerst" created="2011-08-15T05:50:19Z" id="1804955">+1 for RSS / atom river!
</comment><comment author="dadoonet" created="2011-09-17T20:54:52Z" id="2124065">FYI I start writing a plugin for RSS feeds : http://dadoonet.github.com/rssriver/
</comment><comment author="dadoonet" created="2012-03-02T04:20:02Z" id="4280145">Heya

Just wondering if this issue should be closed as the RSS river exist now as an external plugin ?
</comment><comment author="meghuizen" created="2012-03-13T10:02:08Z" id="4470354">From my opinion as starter of this thread, yes it can be closed. Thanks for the plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing multiple attachments gives java.lang.OutOfMemoryError: Java heap space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/411</link><project id="" key="" /><description>Indexing multiple attachments gives java.lang.OutOfMemoryError: Java heap space.

I add multiple attachments to the search index and it gives me this. Even on searching after I added these attachments, it gives me the same error on search. I think it has something te do with indexing the attachment itself.

The command to run elasticsearch:

```
/usr/bin/java -Xms256m -Xmx1g -Xss128k -Djline.enabled=true -XX:+AggressiveOpts -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/elasticsearch-0.11.0/work/heap -Delasticsearch -Des.path.home=/opt/elasticsearch-0.11.0 -Des-foreground=yes -cp :/opt/elasticsearch-0.11.0/lib/elasticsearch-0.11.0.jar:/opt/elasticsearch-0.11.0/lib/*:/opt/elasticsearch-0.11.0/lib/sigar/* org.elasticsearch.bootstrap.Bootstrap
```

environment info:
    root@myserver:~# lsb_release -a
    No LSB modules are available.
    Distributor ID: Ubuntu
    Description:    Ubuntu 9.10
    Release:    9.10
    Codename:   karmic
    root@app1:~# uname -a
    Linux myserver 2.6.31-22-server #65-Ubuntu SMP Thu Sep 16 16:33:54 UTC 2010 x86_64 GNU/Linux
    root@myserver:~# /usr/bin/java -version
    java version "1.6.0_20"
    Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
    Java HotSpot(TM) 64-Bit Server VM (build 16.3-b01, mixed mode)

JAVA backtrace:
    java.lang.OutOfMemoryError: Java heap space
    Dumping heap to /opt/elasticsearch-0.11.0/work/heap ...
    Heap dump file created [1033493546 bytes in 4.057 secs]
    Exception in thread "elasticsearch[Captain Atlas&#8206;][tp]-pool-2-thread-9"     java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOfRange(Arrays.java:3209)
        at java.lang.String.&lt;init&gt;(String.java:215)
        at java.lang.StringBuilder.toString(StringBuilder.java:430)
        at org.elasticsearch.common.jackson.util.TextBuffer.contentsAsString(TextBuffer.java:329)
        at org.elasticsearch.common.jackson.impl.JsonParserBase.getText(JsonParserBase.java:294)
        at org.elasticsearch.common.xcontent.json.JsonXContentParser.text(JsonXContentParser.java:74)
        at org.elasticsearch.index.mapper.xcontent.XContentStringFieldMapper.parseCreateField(XContentStringFieldMapper.java:137)
        at org.elasticsearch.index.mapper.xcontent.XContentStringFieldMapper.parseCreateField(XContentStringFieldMapper.java:40)
        at org.elasticsearch.index.mapper.xcontent.XContentFieldMapper.parse(XContentFieldMapper.java:282)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeValue(XContentObjectMapper.java:414)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:340)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeObject(XContentObjectMapper.java:361)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeArray(XContentObjectMapper.java:397)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:334)
        at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:358)
        at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:308)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:230)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:136)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:62)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:385)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.access$400(TransportShardReplicationOperationAction.java:212)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:282)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
</description><key id="351437">411</key><summary>Indexing multiple attachments gives java.lang.OutOfMemoryError: Java heap space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meghuizen</reporter><labels /><created>2010-10-07T14:26:19Z</created><updated>2015-10-19T06:59:59Z</updated><resolved>2013-04-04T18:38:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="meghuizen" created="2010-10-07T14:27:57Z" id="453958">Because the heap gets 1g, I'm not uploading the dump, which is currently full with attachments.
</comment><comment author="clintongormley" created="2013-04-04T18:38:41Z" id="15915486">No further reports of this issue - assuming fixed
</comment><comment author="redmouthch" created="2015-10-19T06:31:50Z" id="149116036">Same issue... With thousends of documents (PDF's). What can i do? /usr/bin/java -Xms256m -Xmx1g
</comment><comment author="dadoonet" created="2015-10-19T06:59:59Z" id="149122120">@redmouthch Increase your memory settings.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term Facet: Use a script that provides the facets to index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/410</link><project id="" key="" /><description>Term facets support providing script to further filter terms (by using the `term` variable). A `script_field` option allow to provide a script that actually provide the terms to use. The `script_field` (or `script`) with automatically be used when no `field` or `fields` are provided.

As an example, a search request (that is quite "heavy") can be executed and use either `_source` itself or `_fields` (for stored fields) without needing to load the terms to memory ( at the expense of much slower execution of the search, and causing more IO load).

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "my_facet" : {
            "terms" : {
                "script_field" : "_source.my_field",
                "size" : 10
            },
        }
    }
}
```

or

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "my_facet" : {
            "terms" : {
                "script_field" : "_fields['my_field']",
                "size" : 10
            },
        }
    }
}
```

As a side note, joining the results of one or more term facets simply requires using the same facet name ;).
</description><key id="350585">410</key><summary>Term Facet: Use a script that provides the facets to index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-06T23:46:19Z</created><updated>2010-10-06T23:49:20Z</updated><resolved>2010-10-06T23:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-06T23:49:20Z" id="452763">Term Facet: Use a script that provides the facets to index, closed by cf5ed1d177611c7a34e6f8cbd494d5b6175509f5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local Gateway: Possible failure to allocate shards to nodes when more than one index exists in the cluster (on full cluster restart)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/409</link><project id="" key="" /><description>The title says it all.
</description><key id="348604">409</key><summary>Local Gateway: Possible failure to allocate shards to nodes when more than one index exists in the cluster (on full cluster restart)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.12.0</label></labels><created>2010-10-05T22:34:27Z</created><updated>2010-10-05T22:35:17Z</updated><resolved>2010-10-05T22:35:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-05T22:35:17Z" id="449945">Local Gateway: Possible failure to allocate shards to nodes when more than one index exists in the cluster (on full cluster restart), closed by 31d94b19a1d6bd4de3380627baaff1dd9235ce6d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default pattern tokenizer doesn't work with numbers correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/408</link><project id="" key="" /><description>I thought the default pattern tokenizer would do exactly what I needed by default, but fails on single term number fields (maybe others, too). The intent is to break terms on any non-alphanumeric or _ letter. 

Here is my working config:
      verity_tokenizer :
        type: pattern
        lowercase: true
        pattern: '(?:(?!\w).)+'
        stopwords: _none_
        flags: DOTALL

Here are configs that don't work:
      verity_tokenizer :
        type: pattern
        lowercase: true
        stopwords: _none_

```
  verity_tokenizer :
    type: pattern
    lowercase: true
    pattern: '\W+'
    stopwords: _none_
```

Data like this yields no terms:
{feedid: "727"}

I could probably put together a CURL recreation, if it's really needed, but figured that something this simple, it wouldn't be. 

Thanks,
Paul
</description><key id="347897">408</key><summary>Default pattern tokenizer doesn't work with numbers correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-10-05T16:33:48Z</created><updated>2013-07-16T14:23:49Z</updated><resolved>2013-07-16T14:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T14:23:49Z" id="21045095">I just tried it out with the latest release and everything seems to work. If this is a problem for please, please reopen by all means. Reproduction cURLS will be awesome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Couchdb river plugin - ignore design documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/407</link><project id="" key="" /><description /><key id="347535">407</key><summary>Couchdb river plugin - ignore design documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-05T12:59:04Z</created><updated>2014-07-16T21:57:00Z</updated><resolved>2010-10-05T13:53:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-05T13:53:41Z" id="448702">pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Couchdb river plugin - ignore design documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/406</link><project id="" key="" /><description /><key id="347081">406</key><summary>Couchdb river plugin - ignore design documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels /><created>2010-10-05T06:31:16Z</created><updated>2014-07-16T21:57:00Z</updated><resolved>2011-02-20T00:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-05T11:02:42Z" id="448444">Hey, merging the patch into elasticsearch (curl http://github.com/elasticsearch/elasticsearch/pull/406.patch | git am) fails. Is there a chance for a clean patch without the merging? (its a simple fix, I can do it myself, but you deserve to be on the commit log :)  )
</comment><comment author="mahendra" created="2010-10-05T13:01:00Z" id="448583">I created a new pull request #407 - can you please try that and close this one. I guess I messed up somewhere in my git merge and it was sending my older pull request along. (The one for putting = in filter_params). If it fails, feel free to patch it to yourself. I don't mind not being in the commit log. Thanks, btw :-)
</comment><comment author="kimchy" created="2011-02-20T00:21:40Z" id="789547">Closing this pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term Facet: Allow to execute it against several fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/405</link><project id="" key="" /><description>Term facet to allow to provide several fields to execute on. The format is simply to replace the "field" with "fields" that accepts an array of field names.

From discussion:  http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/75b20f260549eabc

Sample use case:
Given:

```
"doc": {
    "desc": "some words here",
    "field" : "yet more text"
},
"doc": {
    "desc": "some more words",
    "field" : "even more words"
}
```

Return:  "words" (3), "more" (3), "some" (2), "here" (1), "yet" (1), "text" (1), "even" (1) 
</description><key id="345861">405</key><summary>Term Facet: Allow to execute it against several fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjsuhonos</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-04T14:24:58Z</created><updated>2010-10-08T09:29:51Z</updated><resolved>2010-10-06T20:53:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-06T13:53:11Z" id="451268">implemented.
</comment><comment author="otisg" created="2010-10-08T02:07:43Z" id="455401">i.e. add up counts from more than 1 field?  Cool.
</comment><comment author="kimchy" created="2010-10-08T09:29:51Z" id="455853">yea. You can actually do that today by executing several facets on different field and given them the same name.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Add `gateway.expected_nodes` for state recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/404</link><project id="" key="" /><description>Currently, one can set `gateway.recover_after_nodes` setting to control after how many nodes within the cluster recovery will start. The `gateway.recover_after_time` sets the time to wait till recovery happens once the nodes are met.

The missing setting in this scenario is the ability to set how many nodes are "expected" to be in the cluster. For example, setting:

```
gateway:
    recover_after_nodes: 1
    recover_after_time: 5m
    expected_nodes: 2
```

In an expected 2 nodes cluster will cause recovery to start 5 minutes after the first node is up, but once there are 2 nodes in the cluster, recovery will begin immediately (without waiting).
</description><key id="345852">404</key><summary>Gateway: Add `gateway.expected_nodes` for state recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-10-04T14:19:47Z</created><updated>2010-10-04T14:20:39Z</updated><resolved>2010-10-04T14:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-04T14:20:39Z" id="446392">Gateway: Add `gateway.expected_nodes` for state recovery, closed by 1f49eb0b9def66b0c9ae4542a9c3df1cf99f0ffd.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wikipedia River: A river to index wikipedia</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/403</link><project id="" key="" /><description>A simple river to index wikipedia. Create it using: 

```
curl -XPUT localhost:9200/_river/my_river/_meta -d '
{
    "type" : "wikipedia"
}
'
```

The default download is the latest wikipedia dump (`http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2`). It can be changed using:

```
{
    "type" : "wikipedia",
    "wikipedia" : {
        "url" : "&lt;&lt;url to link to wikipedia dump&gt;&gt;"
    }
}
```

The index name defaults to the river name, and the type defaults to `page`. Both can be changed in the `index` section:

```
{
    "type" : "wikipedia",
    "index" : {
        "name" : "my_index",
        "type" : "my_type",
        "bulk_size" : 100
    }
}
```
</description><key id="344915">403</key><summary>Wikipedia River: A river to index wikipedia</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-03T20:18:03Z</created><updated>2010-10-03T20:22:55Z</updated><resolved>2010-10-03T20:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-03T20:22:55Z" id="445158">Wikipedia River: A river to index wikipedia, closed by c4d17860a1e4ee48cb62ae2cd8a04e87f6f3dde4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripts: Allow to access `_fields` providing access to stored fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/402</link><project id="" key="" /><description>On top of `doc` (which access cached fields tokens), and `_source` (which loads the actual source and parses it), the new `_fields` allow to access specific _stored_ fields (mapped with `store` set to `yes`.

For example: `_fields['my_field_name'].value` or `_fields['my_field_name'].values`.

This can be handy to just load specific fields and not loading the full source and parsing it for each hit. `doc` is provides the fastest execution at the expense of memory.
</description><key id="344474">402</key><summary>Scripts: Allow to access `_fields` providing access to stored fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-03T10:51:20Z</created><updated>2010-10-03T20:22:55Z</updated><resolved>2010-10-03T20:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-03T20:22:55Z" id="445157">Scripts: Allow to access `_fields` providing access to stored fields, closed by c9a47a126f5dc9301547f26246b84475f7999da6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JavaScript Plugin: Allow to use javascript for scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/401</link><project id="" key="" /><description>Allow to use javascript for scripts (in `custom_score`, `script_fields`, and different facets). Tha `lang` should be set to either `js` or `javascript`.
</description><key id="344212">401</key><summary>JavaScript Plugin: Allow to use javascript for scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-03T00:20:15Z</created><updated>2010-10-03T00:20:47Z</updated><resolved>2010-10-03T00:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-03T00:20:47Z" id="444155">JavaScript Plugin: Allow to use javascript for scripts, closed by 5b8bc333bdbfe582fab8c8a83fa601616fbd185d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>./bin/plugin list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/400</link><project id="" key="" /><description>The plugin script should be able to retrieve a list of available plugins from the remote site. 
At the moment, installing plugins requires too much insider knowledge
</description><key id="343543">400</key><summary>./bin/plugin list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-10-02T08:15:34Z</created><updated>2013-04-04T18:18:18Z</updated><resolved>2013-04-04T18:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-04T10:42:12Z" id="446091">Agreed!. I will work on it.
</comment><comment author="clintongormley" created="2013-04-04T18:18:18Z" id="15914285">No longer relevant
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch, NUMA and swap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/399</link><project id="" key="" /><description>Hiya

There is an interesting article about MySQL and swap when running on linux systems with NUMA architecture - may be relevant to ES as well:

http://jcole.us/blog/archives/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/

The summary is: 

with a single large process running on linux with NUMA, it is possible that the process will swap because memory usage is unbalanced between processors, and a single processor's local memory is full.

By starting the command with: `numactl --interleave all $command` instead, memory usage is balanced and you avoid swapping while memory is available.
</description><key id="343513">399</key><summary>ElasticSearch, NUMA and swap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-10-02T07:11:58Z</created><updated>2015-02-10T10:50:06Z</updated><resolved>2011-03-20T13:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-04T10:41:45Z" id="446090">I cam across this article as well, very interesting... . Requires some testing...
</comment><comment author="clintongormley" created="2011-03-20T13:56:08Z" id="895188">Handled with `bootstrap.mlockall`
</comment><comment author="cvasii" created="2015-02-10T10:50:06Z" id="73680107">If I start elasticsearch with ES_HEAP_SIZE equal to 50% of the RAM on a NUMA architecture with 2 cores is it possible to have issues regarding the 2 nodes that will be created specific to NUMA?

And how exactly boostrap.mlockall handles this? Assuming a box with 16 Gb RAM and two cores, which will lead to Nodes 0 and 1 created by the NUMA system. We don't want swapping and we disable it, but if node 0 exhausted off free memory, node 1 hasn't, and a process scheduled on node 0 needs something, wouldn't this lead to a crash?

We are deploying to a NUMA system (unix OS) with ES_HEAP_SIZE=50% of RAM, bootstrapp.mlockall = true, file descriptors = 64000, ulimit -l unlimited and sometimes the box crashes. 

Trying now to get more details from logs, maybe check if the OOM killer logged something and If I found more I will come back with details.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy Plugin: Allow to run use groovy for scripts (where applicable)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/398</link><project id="" key="" /><description>There are several places where scripts can be used, for example, `custom_score`, `script_fields`, and in different facets implementations.

Allow to run groovy code as the script. All constructs now support passing a `lang` parameter, just set it to `groovy`. To the default script engine from `mvel` to `groovy`, set `script.default_lang` to `groovy`.
</description><key id="343292">398</key><summary>Groovy Plugin: Allow to run use groovy for scripts (where applicable)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-01T23:15:57Z</created><updated>2010-10-01T23:22:14Z</updated><resolved>2010-10-01T23:22:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-01T23:22:14Z" id="442919">Groovy Plugin: Allow to run use groovy for scripts (where applicable), closed by 9e8ebd46e813f1cc1283afaf58a3bdea25e2ec80.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Dynamic Template Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/397</link><project id="" key="" /><description>Dynamic templates allow to define mapping templates that will be applied when dynamic introduction of fields / objects happens. 

For example, we might want to have all fields to be stored by default, or all `string` fields to be stored, or have `string` fields to always be indexed as `multi_field` once `analyzed` and once `not_analyzed`. Here is a simple example:

```
{
    "person" : {
        "dynamic_templates" : [
            {
                "template_1" : {
                    "match" : "multi*",
                    "mapping" : {
                        "type" : "multi_field",
                        "fields" : {
                            "{name}" : {"type": "{dynamic_tpye}", "index" : "analyzed", "store" : "yes"},
                            "org" : {"type": "{dynamic_type}", "index" : "not_analyzed", "store" : "yes"}
                        }
                    }
                }
            },
            {
                "template_2" : {
                    "match" : "*",
                    "match_mapping_type" : "string",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            }
        ]
    }
}
```

The above mapping will create a `multi_field` mapping for all field names starting with `multi`, and will map all `string` types to be `not_analyzed`.

The `dynamic_templates` section can be placed only on the root object. and it is applied to all inner objects / fields.

Dynamic templates are named to allow for simple merge behavior. A new mapping, just with a new template can be "put" and that template will be added, or if it has the same name, the template will be replaced.

The `match` allow to define matching on the field name. An `unmatch` option is also available to exclude fields if they do match on `match`. The `match_mapping_type` controls if this template will be applied only for dynamic fields of the specified type (as guessed by the json format).

The format of all the matching is `simple` format, allowing to use `*` as a matching element supporting simple patterns such as `xxx*`, `*xxx`, `xxx*yyy` (with arbitrary number of pattern types), as well as direct equality. The `match_pattern` can be set to `regex` to allow for regular expression based matching.

The `mapping` element provides the actual mapping definition. The `{name}` keyword can be used and will be replaced with the actual dynamic field name being introduced. The `{dynamic_type}` (or `{dynamicType}`) can be used and will be replaced with the mapping derived based on the field type (or the derived type, like `date`).

Complete generic settings can also be applied, for example, to have all mappings be stored, just set:

```
{
    "person" : {
        "dynamic_templates" : [
            {
                "store_generic" : {
                    "match" : "*",
                    "mapping" : {
                        "store" : "yes"
                    }
                }
            }
        ]
    }
}
```
</description><key id="342551">397</key><summary>Mapper: Dynamic Template Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.12.0</label></labels><created>2010-10-01T14:55:30Z</created><updated>2010-10-14T10:56:14Z</updated><resolved>2010-10-01T21:56:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-01T14:56:11Z" id="441851">Mapper: Dynamic Template Support, closed by 99fcfde307347671c78faba4d0c57caff70afff0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>river-couchdb parse exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/396</link><project id="" key="" /><description>Just after having issued the creation of a river on a couchdb.

org.elasticsearch.ElasticSearchParseException: Failed to derive xcontent from [10, 32, 34, 116, 121, 112, 101, 34, 32, 58, 32, 34, 99, 111, 117, 99, 104, 100, 98, 34, 44, 10, 34, 99, 111, 117, 99, 104, 100, 98, 34, 32, 58, 32, 123, 10, 34, 104, 111, 115, 116, 34, 32, 58, 32, 34, 115, 116, 111, 114, 101, 46, 105, 102, 97, 100, 46, 111, 114, 103, 34, 10, 44, 10, 34, 112, 111, 114, 116, 34, 32, 58, 32, 53, 57, 56, 52, 44, 10, 34, 100, 98, 34, 32, 58, 32, 34, 97, 114, 116, 115, 114, 95, 100, 101, 118, 101, 108, 111, 112, 109, 101, 110, 116, 34, 44, 10, 34, 102, 105, 108, 116, 101, 114, 34, 32, 58, 32, 110, 117, 108, 108, 10, 125, 10, 125]
        at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:132)
</description><key id="342312">396</key><summary>river-couchdb parse exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amedeo</reporter><labels /><created>2010-10-01T12:32:31Z</created><updated>2010-10-01T13:44:35Z</updated><resolved>2010-10-01T13:44:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-10-01T12:37:59Z" id="441595">can you create a simple curl recreation that fills coucdb with the data that causes this failure?
</comment><comment author="amedeo" created="2010-10-01T12:56:28Z" id="441619">ops, my bad, wrong command missing a "{". the error message lead me to think there was a problem on the data parser. sorry
</comment><comment author="kimchy" created="2010-10-01T13:42:21Z" id="441695">I see ;), so all is well?, can we close this issue?
</comment><comment author="amedeo" created="2010-10-01T13:44:34Z" id="441701">yes please. thanks for the great project and the integration with couchdb ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter parameters not formed properly in couchdb river plugin, closes #394</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/395</link><project id="" key="" /><description /><key id="342222">395</key><summary>Filter parameters not formed properly in couchdb river plugin, closes #394</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels /><created>2010-10-01T10:45:28Z</created><updated>2014-06-28T05:31:28Z</updated><resolved>2011-02-20T00:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-02-20T00:23:02Z" id="789552">Applied, but never closed...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter parameters not formed properly in couchdb river plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/394</link><project id="" key="" /><description>The "=" sign is missing in the code.
</description><key id="342219">394</key><summary>Filter parameters not formed properly in couchdb river plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels /><created>2010-10-01T10:37:29Z</created><updated>2010-10-01T11:56:36Z</updated><resolved>2010-10-01T11:56:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mahendra" created="2010-10-01T10:41:45Z" id="441475">This was originally introduced by Issue #389
</comment><comment author="mahendra" created="2010-10-01T10:53:16Z" id="441485">have sent a pull request for this issue #395
</comment><comment author="mahendra" created="2010-10-01T11:56:36Z" id="441545">Filter parameters not formed properly in couchdb river plugin, closed by c657c7c6d06f4fc601a9f1390b61a14e3f753895
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thrift: Perl namespace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/393</link><project id="" key="" /><description>Hiya

Please could you change the Perl namespace in the elasticsearch.thrift file to: `ElasticSearch.Transport.Thrift.Backend`

thanks

clint
</description><key id="340402">393</key><summary>Thrift: Perl namespace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-09-30T10:02:19Z</created><updated>2011-03-20T13:55:33Z</updated><resolved>2011-03-20T13:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-09-30T10:04:01Z" id="439033">Hmm, I'm wondering if that is the right thing to do, actually. It is correct for my API, but won't be if anybody else wants to build a Perl Thrift API separate from my stuff.  I can update the namespace locally before generating anyway.

What do you think?

c
</comment><comment author="kimchy" created="2010-09-30T11:11:42Z" id="439110">Yea, that was my line of thought. People will change to use whatever namespace they want. I just copied the thrift structure from cassandra, and it contained  the namespaces, so I kept them.
</comment><comment author="clintongormley" created="2011-03-20T13:55:33Z" id="895186">Not an issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>thrift definition `params` is conflict with c#'s keyword, rename to `parameters`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/392</link><project id="" key="" /><description>in thrift idl elasticsearch.thrift:

struct RestRequest {
    1: required Method method,
    2: required string uri
    3: optional map&lt;string, string&gt; params
    4: optional map&lt;string, string&gt; headers
    5: optional binary body
}

the params is a keyword in .net,though can hack it with a @before params,maybe is should avoid this.
</description><key id="340223">392</key><summary>thrift definition `params` is conflict with c#'s keyword, rename to `parameters`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.12.0</label></labels><created>2010-09-30T07:46:49Z</created><updated>2010-09-30T08:07:11Z</updated><resolved>2010-09-30T08:07:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-30T08:07:10Z" id="438852">thrift definition `params` is conflict with c#'s keyword, rename to `parameters`, closed by 06c7c4a9ac822ec46d0bc6ee06bd60ab828f251b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`/_cluster/nodes/stats` is broken in 0.11</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/391</link><project id="" key="" /><description>Hiya

The nodes stats command is broken:

```
curl -XGET 'http://localhost:9200/_cluster/nodes/stats?pretty=1'
{
  "cluster_name" : "es_test",
  "nodes" : {
  }
}
```

Nodes info still works:

```
curl -XGET 'http://localhost:9200/_cluster/nodes?pretty=1'
{
  "cluster_name" : "es_test",
  "nodes" : {
    "2edbcc68-935f-4e59-b899-c1d112c7d825" : {
      "name" : "Neurotap",
      "transport_address" : "inet[/127.0.0.1:9302]",
      "attributes" : {
      },
      "http_address" : "inet[/127.0.0.1:9202]",
      "os" : {
        "cpu" : {
          "vendor" : "Intel",
          "model" : "Core(TM)2 Duo CPU     T9800  @ 2.93GHz",
          "mhz" : 2934,
          "total_cores" : 2,
          "total_sockets" : 1,
          ....
```
</description><key id="339071">391</key><summary>`/_cluster/nodes/stats` is broken in 0.11</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.12.0</label></labels><created>2010-09-29T17:10:09Z</created><updated>2010-09-30T06:35:17Z</updated><resolved>2010-09-30T06:35:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-30T06:35:17Z" id="438730">`/_cluster/nodes/stats` is broken in 0.11, closed by f66f0218ca5223218131a74ad93260df792fe997.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support http auth in couchdb river plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/390</link><project id="" key="" /><description>Some couchdb setups will require HTTP auth for listening on changes interface. The couchdb river plugin needs support for the same.

Should add `user` and `password` under the `couchdb` section to support it.
</description><key id="338336">390</key><summary>Support http auth in couchdb river plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-09-29T10:48:34Z</created><updated>2010-09-30T18:58:11Z</updated><resolved>2010-09-30T18:58:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-30T18:58:11Z" id="440091">Support http auth in couchdb river plugin, closed by 1af53bce0d18df485300ca4e1c16c1efefe09c65.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CouchDB River: Support couchdb filter query parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/389</link><project id="" key="" /><description>Couchdb filters support query parameters. It will be good to support this in the Elasticsearch couchdb river plugin.

Details of the couchdb filters are documented at 
http://blog.couchone.com/post/468392274/whats-new-in-apache-couchdb-0-11-part-three-new.

The parameters can be provider under:

```
{
    "couchdb" : {
        "filter" : "test",
        "filter_params" : {
            "param1" : "value1",
            "param2" : "value2"
        }
    }
}
```
</description><key id="338333">389</key><summary>CouchDB River: Support couchdb filter query parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels><label>enhancement</label><label>v0.12.0</label></labels><created>2010-09-29T10:45:35Z</created><updated>2010-09-30T18:47:14Z</updated><resolved>2010-09-30T18:47:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-30T18:47:14Z" id="440069">CouchDB River: Support couchdb filter query parameters, closed by 735ad0d43d1a35aaae108868e25b84c3721418f7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy Plugin: Rename from client-groovy to lang-groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/388</link><project id="" key="" /><description /><key id="333955">388</key><summary>Groovy Plugin: Rename from client-groovy to lang-groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.11.0</label></labels><created>2010-09-26T19:07:47Z</created><updated>2010-09-26T19:08:00Z</updated><resolved>2010-09-26T19:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-26T19:08:00Z" id="430910">renamed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index / Delete API: Add a refresh parameter to automatically refresh the relevant data for search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/387</link><project id="" key="" /><description>Add a refresh parameter to the index / delete APIs to do a refresh on the relevant shard the index / delete operation occurred on. Note, this should not be used with "heavy" indexing as it adds an overhead to each call.
</description><key id="333508">387</key><summary>Index / Delete API: Add a refresh parameter to automatically refresh the relevant data for search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-26T08:20:59Z</created><updated>2010-09-26T08:21:17Z</updated><resolved>2010-09-26T08:21:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-26T08:21:17Z" id="430316">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>setHighlighterPostTags sets pre tags instead of post</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/386</link><project id="" key="" /><description>In SearchRequestBuilder.java the setHighlighterPostTags() method sets the pre-tags instead of the post-tags.
</description><key id="332119">386</key><summary>setHighlighterPostTags sets pre tags instead of post</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pzelnip</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-09-24T21:49:33Z</created><updated>2010-09-24T22:01:20Z</updated><resolved>2010-09-24T22:01:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-24T22:01:20Z" id="428585">setHighlighterPostTags sets pre tags instead of post, closed by 2288c5d6702a0bbb395de843e78d91a3197e115f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>*:* query fails throug rest api on 0.11 snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/385</link><project id="" key="" /><description>http://localhost:9200/_search?q=STAR:STAR

Where STAR = \* 
(Github was giving some pain)

Returns:
{
error: "JsonParseException[Unexpected character (':' (code 58)): expected a valid value (number, String, array, object, 'true', 'false' or 'null') at [Source: org.elasticsearch.common.io.stream.LZFStreamInput@6ca21eeb; line: 1, column: 2]]"
}

Thanks!
</description><key id="331863">385</key><summary>*:* query fails throug rest api on 0.11 snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-09-24T18:45:46Z</created><updated>2010-09-25T01:00:40Z</updated><resolved>2010-09-25T01:00:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2010-09-24T18:54:27Z" id="428242">Hmmm... It appears this is index specific and probably can't be reproduced. Let me play around a little more. 
</comment><comment author="kimchy" created="2010-09-24T19:09:11Z" id="428264">Do you use compression somewhere in the code? Was there an exception logged?
</comment><comment author="ppearcy" created="2010-09-25T01:00:39Z" id="428811">Sorry for the noise, I don't think this is an ES issue. I think it maybe related to manually clearing an index from the gateway without updating an alias. Should have been going through proper APIs. There was also ongoing work by a co-worker that could have been related. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default to not using compound file format for the index (increases number of open files, but faster indexing and recovery)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/384</link><project id="" key="" /><description>The title says it all. The increase should be around 5x number of open files per shard. Note, this is just open files for the index, not TCP connections.

In order to bring it back, set `index.merge.policy.use_compound_file` to `true`.
</description><key id="331198">384</key><summary>Default to not using compound file format for the index (increases number of open files, but faster indexing and recovery)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-24T09:45:47Z</created><updated>2010-09-24T09:48:22Z</updated><resolved>2010-09-24T16:46:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-24T09:46:48Z" id="427198">Default to not using compound file format for the index (increases number of open files, but faster indexing and recovery), closed by 485f90414098b984ffb6649fcee24696d0e85e7e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: Allow ES to pick up updates to logging.yml at run time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/383</link><project id="" key="" /><description>In order to update any log levels, the ES node must be restarted. It'd be great if the changes could be picked up at runtime either through a mechanism similar to log4j's configureAndWatch or via REST command to reload the logging settings.

The key benefit of this, is that it will allow users to capture errors that disappear with a node restart. 

Thanks!
Paul
</description><key id="330425">383</key><summary>Feature Request: Allow ES to pick up updates to logging.yml at run time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2010-09-23T19:47:15Z</created><updated>2011-09-23T14:26:18Z</updated><resolved>2011-09-23T14:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-27T12:37:45Z" id="431909">Agreed, that would be nice. I was thinking of tacking this maybe a bit differently, and expose a cluster wide "update_logging" API. What do you think?
</comment><comment author="ppearcy" created="2010-09-27T15:19:40Z" id="432264">That would be pretty awesome. You have my vote.

Thanks!
</comment><comment author="kimchy" created="2011-09-23T14:26:17Z" id="2179072">This feature is implemented using the cluster update settings API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>River CouchDB Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/382</link><project id="" key="" /><description>A river plugin that automatically indexes couchdb using the `_changes` continuos stream API.

Creating the river is simple:

```
curl -XPUT 'localhost:9200/_river/db1/_meta' -d '{
    "type" : "couchdb",
    "couchdb" : {
        "host" : "localhost",
        "port" : 5984,
        "db" : "db1",
        "filter" : null
    },
    "index" : {
        "index" : "db1",
        "type" : "db1",
        "bulk_size" : "100",
        "bulk_timeout" : "10ms",
    }
}'
```

The above creates a river named `db1`, which will index a couchdb database named "db1" into an index `db1` and type `db1`. The values provided are all the default valued, with the couchdb database name defaults to the river name, and the index/type default to the couchdb database name.
</description><key id="329079">382</key><summary>River CouchDB Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-23T00:20:33Z</created><updated>2010-09-23T00:26:21Z</updated><resolved>2010-09-23T00:26:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-23T00:26:21Z" id="424212">River CouchDB Plugin, closed by ecaaeb5250090139f32f275372460232fbfbee97.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default thread pool to scaling from cached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/381</link><project id="" key="" /><description /><key id="328035">381</key><summary>Change default thread pool to scaling from cached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-22T13:20:06Z</created><updated>2010-09-22T13:20:40Z</updated><resolved>2010-09-22T13:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-22T13:20:40Z" id="422926">Change default thread pool to scaling from cached, closed by f301138e6f087b77fde342ac2e60849156422311.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>River RabbitMQ Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/380</link><project id="" key="" /><description>Allow to have a river plugin that automatically index changes by listening to a rabbitmq queue. The format of the message is the same as the bulk API (#371), here is an example:

```
{ "index" : { "index" : "test", "type" : "type1", "id" : "1" }
{ "type1" : { "field1" : "value1" } }
{ "delete" : { "index" : "test", "type" : "type1", "id" : "2" } }
{ "create" : { "index" : "test", "type" : "type1", "id" : "1" }
{ "type1" : { "field1" : "value1" } }
```

Creating a rabbitmq river is simple:

```
curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{
    "type" : "rabbitmq",
}'
```

There are several configuration that can be set along with the `type` on an `rabbitmq` element:
- `host`: The host to connect to, defaults to `localhost`.
- `port`: The port to connect to, defaults to the default AMQP port.
- `user`: The user name. Default to not being set.
- `password`: The password. Defaults to not being set.
- `vhost`: The virtual host.
- `queue`: The queue name. Defaults to `elasticsearch`.
- `exchange`: The exchange name. Defaults to `elasticsearch`.
- `routing_key`: The routing key. Defaults to `elasticsearch`.

On the `index` element, the following can be controlled:
- `bulk_size`: The total bulk size when aggregating messages.
- `bulk_timeout`: One a first message is received, how much time to wait to try and aggregate more messages to a single bulk.
</description><key id="326969">380</key><summary>River RabbitMQ Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-21T20:26:20Z</created><updated>2010-09-23T21:08:45Z</updated><resolved>2010-09-22T03:26:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-21T20:26:30Z" id="421323">implemented.
</comment><comment author="monadic" created="2010-09-23T21:08:45Z" id="426291">nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexOutOfBoundsException[index (0) must be less than size (0)]; </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/379</link><project id="" key="" /><description>Hiya

I'm getting these `IndexOutOfBoundsException[index (0) must be less than size (0)];` errors, which are kinda meaningless, on queries that work 99% of the time.

There are some errors in the logs, but they don't correspond to the times of these query errors:

Query errors:
## 05:38

```
ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: IndexOutOfBoundsException[index (0) must be less than size (0)]; 
With vars:
$VAR1 = {
        'request' =&gt; {
            'cmd' =&gt;
                '/iannounce_object/anniversary,bestwish,birthday,memorial,specialday,wedding/_search?search_type=dfs_query_then_fetch',
            'data' =&gt; {
                'sort'   =&gt; [ { '_score' =&gt; 'desc' } ],
                'fields' =&gt; [],
                'from'   =&gt; 0,
                'query'  =&gt; {
                    'filtered' =&gt; {
                        'filter' =&gt; {
                            'bool' =&gt; {
                                'must' =&gt; [
                                    { 'term' =&gt; { 'source' =&gt; 'tmmi_wvle' } },
                                    { 'term' =&gt; { 'status' =&gt; 'active' } },
                                    { 'term' =&gt; { 'region' =&gt; 'tmmi' } },
                                    { 'term' =&gt; { 'parent_id' =&gt; '441895' } }
                                ]
                            }
                        },
                        'query' =&gt; {
                            'dis_max' =&gt; {
                                'queries' =&gt; [ {
                                        'query_string' =&gt; {
                                            'fields' =&gt; [ 'name' ],
                                            'boost'  =&gt; '1',
                                            'query' =&gt;
                                                '"HUGH ANDREW DAVID KNAPP"~9'
                                        }
                                    },
                                    {   'filtered' =&gt; {
                                            'filter' =&gt; {
                                                'term' =&gt; { 'has_name' =&gt; 0 }
                                            },
                                            'query' =&gt; {
                                                'query_string' =&gt; {
                                                    'fields' =&gt;
                                                        [ 'short_text' ],
                                                    'boost' =&gt; '1.3',
                                                    'query' =&gt;
                                                        '"HUGH ANDREW DAVID KNAPP"~9'
                                                }
                                            }
                                        }
                                    },
                                    {   'query_string' =&gt; {
                                            'boost' =&gt; 1,
                                            'query' =&gt;
                                                'HUGH ANDREW DAVID KNAPP'
                                        }
                                    }
                                ],
                                'tie_breaker' =&gt; '0.7'
                            }
                        }
                    }
                },
                'size' =&gt; 300
            },
            'method' =&gt; 'GET'
        },
        'status_code' =&gt; 500,
        'response'    =&gt; {
            'error' =&gt;
                'ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: IndexOutOfBoundsException[index (0) must be less than size (0)]; '
        },
        'server'     =&gt; 'http://192.168.10.51:9200',
        'status_msg' =&gt; 'Internal Server Error'
    };
```
## 07:05

```
ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: IndexOutOfBoundsException[index (0) must be less than size (0)]; 
With vars:
$VAR1 = {
    'request' =&gt; {
        'cmd' =&gt; '/ia_object/notice/_search?search_type=dfs_query_then_fetch',
        'data' =&gt; {
            'sort'   =&gt; [ { '_score' =&gt; 'desc' } ],
            'fields' =&gt; [],
            'from'   =&gt; 0,
            'query'  =&gt; {
                'filtered' =&gt; {
                    'filter' =&gt; {
                        'bool' =&gt; {
                            'must' =&gt; [
                                { 'term' =&gt; { 'sub_type'    =&gt; 'obit' } },
                                { 'term' =&gt; { 'status'      =&gt; 'active' } },
                                { 'term' =&gt; { 'location_id' =&gt; '25' } }
                            ]
                        }
                    },
                    'query' =&gt; {
                        'dis_max' =&gt; {
                            'queries' =&gt; [ {
                                    'query_string' =&gt; {
                                        'fields' =&gt; [ 'name' ],
                                        'boost'  =&gt; '1',
                                        'query'  =&gt; '"Dorothy DAYLE"~7'
                                    }
                                },
                                {   'filtered' =&gt; {
                                        'filter' =&gt;
                                            { 'term' =&gt; { 'has_name' =&gt; 0 } },
                                        'query' =&gt; {
                                            'query_string' =&gt; {
                                                'fields' =&gt; [ 'text' ],
                                                'boost'  =&gt; '1.3',
                                                'query' =&gt; '"Dorothy DAYLE"~7'
                                            }
                                        }
                                    }
                                },
                                {   'query_string' =&gt; {
                                        'boost' =&gt; 1,
                                        'query' =&gt; 'Dorothy DAYLE'
                                    }
                                }
                            ],
                            'tie_breaker' =&gt; '0.7'
                        }
                    }
                }
            },
            'size' =&gt; 300
        },
        'method' =&gt; 'GET'
    },
    'status_code' =&gt; 500,
    'response'    =&gt; {
        'error' =&gt;
            'ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: IndexOutOfBoundsException[index (0) must be less than size (0)]; '
    },
    'server'     =&gt; 'http://192.168.10.51:9200',
    'status_msg' =&gt; 'Internal Server Error'
};
```

Errors in non-master:

```
[18:30:50,391][WARN ][monitor.jvm              ] [Uncle Ben Parker] Long GC collection occurred, took [12.5s], breached threshold [10s]
[23:07:16,048][WARN ][monitor.jvm              ] [Uncle Ben Parker] Long GC collection occurred, took [10.3s], breached threshold [10s]
```

Errors in master:

```
[05:14:32,291][DEBUG][action.search.type       ] [Rock] [4845938] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [4845938], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:262)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[05:14:32,293][DEBUG][action.search.type       ] [Rock] [4845936] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [4845936], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:262)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)



[05:29:08,976][DEBUG][action.search.type       ] [Rock] [2444873] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [ia_object_1283933184][4]: query[filtered(+_all:margaret +_all:jordan)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWrapper(publish_date:[1284422400000 TO *]) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(location_id:[23 TO 23])))],from[0],size[300],sort[&lt;score&gt;]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:130)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term margaret not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
        at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
        at org.apache.lucene.search.spans.SpanWeight.&lt;init&gt;(SpanWeight.java:51)
        at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight.&lt;init&gt;(AllTermQuery.java:61)
        at org.elasticsearch.common.lucene.all.AllTermQuery.createWeight(AllTermQuery.java:55)
        at org.apache.lucene.search.BooleanQuery$BooleanWeight.&lt;init&gt;(BooleanQuery.java:188)
        at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:362)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.Query.weight(Query.java:101)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:100)
        at org.apache.lucene.search.Searcher.search(Searcher.java:98)
        at org.apache.lucene.search.Searcher.search(Searcher.java:108)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:126)
        ... 8 more
[05:29:08,976][DEBUG][action.search.type       ] [Rock] [2444873] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2444873], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:262)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[05:29:08,983][DEBUG][action.search.type       ] [Rock] [2444874] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2444874], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:190)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[05:29:31,785][DEBUG][action.search.type       ] [Rock] [2445049] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [ia_object_1283933184][3]: query[filtered((name:"roy light"~7 | filtered(text:"roy light"~7^1.3)-&gt;FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3e3f4f7) | (_all:roy _all:light))~0.7)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@fdb5bf46) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(location_id:[30 TO 30])))],from[0],size[300],sort[&lt;score&gt;]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:130)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term roy not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
        at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
        at org.apache.lucene.search.PhraseQuery$PhraseWeight.&lt;init&gt;(PhraseQuery.java:122)
        at org.apache.lucene.search.PhraseQuery.createWeight(PhraseQuery.java:264)
        at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.&lt;init&gt;(DisjunctionMaxQuery.java:107)
        at org.apache.lucene.search.DisjunctionMaxQuery.createWeight(DisjunctionMaxQuery.java:184)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.Query.weight(Query.java:101)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:100)
        at org.apache.lucene.search.Searcher.search(Searcher.java:98)
        at org.apache.lucene.search.Searcher.search(Searcher.java:108)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:126)
        ... 8 more
[05:29:31,786][DEBUG][action.search.type       ] [Rock] [2445050] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [ia_object_1283933184][0]: query[filtered((name:"roy light"~7 | filtered(text:"roy light"~7^1.3)-&gt;FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3e3f4f7) | (_all:roy _all:light))~0.7)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@fdb5bf46) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(location_id:[30 TO 30])))],from[0],size[300],sort[&lt;score&gt;]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:130)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term roy not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
        at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
        at org.apache.lucene.search.PhraseQuery$PhraseWeight.&lt;init&gt;(PhraseQuery.java:122)
        at org.apache.lucene.search.PhraseQuery.createWeight(PhraseQuery.java:264)
        at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.&lt;init&gt;(DisjunctionMaxQuery.java:107)
        at org.apache.lucene.search.DisjunctionMaxQuery.createWeight(DisjunctionMaxQuery.java:184)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.Query.weight(Query.java:101)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:100)
        at org.apache.lucene.search.Searcher.search(Searcher.java:98)
        at org.apache.lucene.search.Searcher.search(Searcher.java:108)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:126)
        ... 8 more
[05:29:31,804][DEBUG][action.search.type       ] [Rock] [2445050] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2445050], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:190)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[05:29:31,880][DEBUG][action.search.type       ] [Rock] [2445049] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2445049], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:262)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)






[06:51:44,352][DEBUG][action.search.type       ] [Rock] [2488219] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [iannounce_object_1283872186][0]: query[filtered(+_all:amy +_all:hotson)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWrapper(org.elasti
csearch.common.lucene.search.TermFilter@3d4c17e4) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3d3f2f85) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter
@3a1bb115) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@40ea8da2) +FilterCacheFilterWrapper(parent_id:[1 TO 1])))],from[0],size[300],sort[&lt;score&gt;]: Query Failed [Failed to execute 
main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:130)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term amy not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
        at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
        at org.apache.lucene.search.spans.SpanWeight.&lt;init&gt;(SpanWeight.java:51)
        at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight.&lt;init&gt;(AllTermQuery.java:61)
        at org.elasticsearch.common.lucene.all.AllTermQuery.createWeight(AllTermQuery.java:55)
        at org.apache.lucene.search.BooleanQuery$BooleanWeight.&lt;init&gt;(BooleanQuery.java:188)
        at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:362)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.Query.weight(Query.java:101)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:100)
        at org.apache.lucene.search.Searcher.search(Searcher.java:98)
        at org.apache.lucene.search.Searcher.search(Searcher.java:108)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:126)
        ... 8 more
[06:51:44,353][DEBUG][action.search.type       ] [Rock] [2488220] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [iannounce_object_1283872186][2]: query[filtered(+_all:amy +_all:hotson)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWrapper(org.elasti
csearch.common.lucene.search.TermFilter@3d4c17e4) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3d3f2f85) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter
@3a1bb115) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@40ea8da2) +FilterCacheFilterWrapper(parent_id:[1 TO 1])))],from[0],size[300],sort[&lt;score&gt;]: Query Failed [Failed to execute 
main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:130)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term amy not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
        at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
        at org.apache.lucene.search.spans.SpanWeight.&lt;init&gt;(SpanWeight.java:51)
        at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight.&lt;init&gt;(AllTermQuery.java:61)
        at org.elasticsearch.common.lucene.all.AllTermQuery.createWeight(AllTermQuery.java:55)
        at org.apache.lucene.search.BooleanQuery$BooleanWeight.&lt;init&gt;(BooleanQuery.java:188)
        at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:362)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.Query.weight(Query.java:101)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:100)
        at org.apache.lucene.search.Searcher.search(Searcher.java:98)
        at org.apache.lucene.search.Searcher.search(Searcher.java:108)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:126)
        ... 8 more
[06:51:44,353][DEBUG][action.search.type       ] [Rock] [2488219] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2488219], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:190)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[06:51:44,354][DEBUG][action.search.type       ] [Rock] [2488220] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2488220], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:190)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
```
</description><key id="326069">379</key><summary>IndexOutOfBoundsException[index (0) must be less than size (0)]; </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-09-21T09:28:59Z</created><updated>2012-08-04T11:37:00Z</updated><resolved>2012-08-04T11:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-09-21T17:14:30Z" id="420872">More random errors:

```
[16:45:46,071][DEBUG][action.search.type       ] [Rock] [5833008] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:156)
        at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:100)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:141)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:445)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[16:45:46,073][DEBUG][action.search.type       ] [Rock] [5833009] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:156)
        at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:100)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:141)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:445)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[16:45:46,075][DEBUG][action.search.type       ] [Rock] [5833008] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:156)
        at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:100)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:141)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:445)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[16:45:46,077][DEBUG][action.search.type       ] [Rock] [5833009] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:156)
        at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:100)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:141)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:445)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[16:45:46,079][DEBUG][action.search.type       ] [Rock] [5833010] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:156)
        at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:100)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:141)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:445)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[16:45:46,082][DEBUG][action.search.type       ] [Rock] [5833010] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:156)
        at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:100)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:141)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:445)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:195)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)










[16:50:12,372][DEBUG][action.search.type       ] [Rock] [2934788] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [iannounce_object_1283872186][4]: query[filtered(+_all:smithurst +_all:elsie +_all:may)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWr
apper(org.elasticsearch.common.lucene.search.TermFilter@66fa1702) +FilterCacheFilterWrapper(org.elasticsearch.common.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(parent_id:[4323320 TO 4323320]))
)],from[0],size[300],sort[&lt;score&gt;]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:130)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term smithurst not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
        at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
        at org.apache.lucene.search.spans.SpanWeight.&lt;init&gt;(SpanWeight.java:51)
        at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight.&lt;init&gt;(AllTermQuery.java:61)
        at org.elasticsearch.common.lucene.all.AllTermQuery.createWeight(AllTermQuery.java:55)
        at org.apache.lucene.search.BooleanQuery$BooleanWeight.&lt;init&gt;(BooleanQuery.java:188)
        at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:362)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
        at org.apache.lucene.search.Query.weight(Query.java:101)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:100)
        at org.apache.lucene.search.Searcher.search(Searcher.java:98)
        at org.apache.lucene.search.Searcher.search(Searcher.java:108)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:126)
        ... 8 more









[17:15:55,179][DEBUG][action.search.type       ] [Rock] [2953639] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [2953639], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:279)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:262)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[17:15:55,185][DEBUG][action.search.type       ] [Rock] [5877973] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
[17:15:55,192][DEBUG][action.search.type       ] [Rock] [5877973] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
[17:15:55,223][DEBUG][action.search.type       ] [Rock] [5877972] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Uncle Ben Parker][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException



[18:06:13,863][DEBUG][action.search.type       ] [Rock] [ia_object_1283933184][1], node[7500316d-3f57-4677-a169-2788cb532db7], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@5
9149939] while moving to second phase
java.lang.NullPointerException
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:101)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:218)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$100(TransportSearchTypeAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:196)
        at org.elasticsearch.search.action.SearchServiceTransportAction$2.handleResponse(SearchServiceTransportAction.java:142)
        at org.elasticsearch.search.action.SearchServiceTransportAction$2.handleResponse(SearchServiceTransportAction.java:135)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:142)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:102)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:302)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:317)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:299)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
```
</comment><comment author="kimchy" created="2010-09-21T22:00:19Z" id="421611">Hi,

There are three types of exceptions here that are important to investigate:
- java.lang.IllegalArgumentException: df for term roy not available

This happens when using the `dfs_...` type query.
- NullPointException in InternalSearchHits.writeTo(InternalSearchHits.java:156)

Happens when trying to serialize the result of a fetch request back to the calling node.
- failed ... moving to second phase, NullPointerException at TransportSearchQueryThenFetchAction.java:101

Fails when trying to move to the fetch phase on the query_then_fetch execution.

To be honest, I don't really understand why any of these exceptions should happen, very strange!. I know its probably hard, but is there a chance for a recreation? Does it happen when you execute similar searches on the same dataset consistently?

Regarding the context not found exception, I have improved the timeout logic in upcoming 0.11, so hopefully this will help solve the problem. It might also relate to the other problems.
</comment><comment author="clintongormley" created="2010-09-21T23:21:01Z" id="421755">Re chance of recreation - no. I only have a few types of queries running against the cluster, and almost none of them fail. So these are aberrations. They typically work, until they fail :)

The one thing I have noted though is that they only happen when there is more than one node running.  When I was running on a single node, I didn't get these errors.
</comment><comment author="clintongormley" created="2012-08-04T11:37:00Z" id="7501159">This has been fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Twitter River Plugin: A river that index twitter sample stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/378</link><project id="" key="" /><description>A twitter river plugin that index twitter same stream. Here is how it gets created:

```
curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{
    "type" : "twitter",
    "twitter" : {
        "user" : "twitter_user",
        "password" : "twitter_password"
    }
}'
```

The index it indexes data into is called by default the same as the river name, and the `_type` is `status`. Controlling it using:

```
curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{
    "type" : "twitter",
    "twitter" : {
        "user" : "twitter_user",
        "password" : "twitter_password"
    },
    "index" : {
        "index" : "my_index_1",
        "type" : "timeline"
    }
}'
```

The indexing is done in bulking of `100` bulk items. Control it the `bulk_size` setting within `index`.

The twitter river is provided as a plugin, and will be installed using `plugin -install river-twitter`.
</description><key id="325396">378</key><summary>Twitter River Plugin: A river that index twitter sample stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-20T21:09:11Z</created><updated>2010-09-23T00:27:17Z</updated><resolved>2010-09-21T04:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-20T21:12:36Z" id="418795">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>River: A pluggable river (indexer like) support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/377</link><project id="" key="" /><description>A river is a pluggable entity running within elasticsearch cluster pulling data (or being pushed with data) that is then indexed into the cluster.

A river is composed of a unique name and a type. The type is the type of the river (out of the box, there is the `dummy` river that simply logs that its running). The name uniquely identifies the river within the cluster. For example, one can run an river called `my_river` with type `dummy`.

Rivers are singletons within the cluster. They get allocated automatically to one of the nodes and run. If that node fails, an river will be automatically allocated to another node.

River allocation on nodes can be controlled on each node. The `node.river` can be set to `_none_` disabling any river allocation to it. The `node.river` can also include a comma separated list of either river names or types controlling the rivers allowed to run on it. For example: `my_river1,my_river2`, or `dummy,twitter`.

Rivers require both meta data (what type they are, and additional information) that forms the "settings" of the river, and possibly need to store runtime state (indexed up to data X, continue from it in case of failover). Everything is driven by working an internal index called `_river`.

In that index (`_river`), each `_type` in the index (mapping) corresponds to an river name (do not confuse it with the river type). The `_meta` document id is required and includes the settings of that river. It must include at least the river type. In order to delete a river, a simple delete of the mapping type (river name) can be done.

With the fact that the river(s) information is stored as an index, it is fully persistent, and allows for very frequent state storage (under one or more documents).

Sounds confusing, but its really simple, here is an example for creating the `dummy` river with the name `my_river`:

```
curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{
    "type" : "dummy"
}'
```

And deleting the river is:

```
curl -XDELETE 'localhost:9200/_river/my_river'    
```
</description><key id="325380">377</key><summary>River: A pluggable river (indexer like) support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-20T21:03:50Z</created><updated>2010-09-21T11:39:13Z</updated><resolved>2010-09-21T04:04:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-20T21:04:08Z" id="418766">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Add `recover_after_data_nodes` and `recover_after_master_nodes`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/376</link><project id="" key="" /><description>Allow to have more control over when to perform the initial state recovery on full cluster start on data nodes (`gateway.recover_after_data_nodes`) or master nodes (`gateway.recover_after_master_nodes`). The original `recover_after_nodes` includes both data and master nodes.
</description><key id="323769">376</key><summary>Gateway: Add `recover_after_data_nodes` and `recover_after_master_nodes`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-19T20:45:40Z</created><updated>2010-09-19T20:46:20Z</updated><resolved>2010-09-19T20:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-19T20:46:20Z" id="416422">Gateway: Add `recover_after_data_nodes` and `recover_after_master_nodes`, closed by 25246902cc389d90ac4ab589077ea3b4736e5e9f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shared Storage Gateway (fs/s3): Wrong snapshotting of transaction log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/375</link><project id="" key="" /><description>Transaction logs are being persisted wrongly to the shared gateway storage, causing the full translog to be persisted each time (and not the delta), causing both wrong data stored in the gateway, and possible duplicates created when recovering.
</description><key id="322724">375</key><summary>Shared Storage Gateway (fs/s3): Wrong snapshotting of transaction log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-09-18T19:33:08Z</created><updated>2010-09-18T19:33:48Z</updated><resolved>2010-09-18T19:33:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-18T19:33:48Z" id="415097">Shared Storage Gateway (fs/s3): Wrong snapshotting of transaction log, closed by 2372f481aa3763779d7b06d30155eec68c2e9c07.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facet: Allow to provide `_index` as the field, resulting in facets on indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/374</link><project id="" key="" /><description>When searching across several indices, sometimes it make sense to return a facet that includes a count of hits per index. This is similar in notion to terms facet (consider the index name as if it was a field stored in the index).

A simple `terms` facet definition, with `_index` as the field name will return a facet containing the mentioned info.
</description><key id="319444">374</key><summary>Terms Facet: Allow to provide `_index` as the field, resulting in facets on indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-16T13:11:06Z</created><updated>2010-09-16T22:22:54Z</updated><resolved>2010-09-16T22:22:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-16T22:22:54Z" id="411874">Terms Facet: Allow to provide `_index` as the field, resulting in facets on indices, closed by 4b87f63bed54353df505df28004ce67a364a2a39.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping API: Delete Mapping (with data)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/373</link><project id="" key="" /><description>Allow to delete a mapping (type) with its data. The REST endpoint is `/{index}/{type}` with `DELETE` method.
</description><key id="319394">373</key><summary>Mapping API: Delete Mapping (with data)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-16T12:34:39Z</created><updated>2010-09-16T22:22:54Z</updated><resolved>2010-09-16T22:22:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-16T22:22:54Z" id="411873">Mapping API: Delete Mapping (with data), closed by f49f3e169aaec2a1ea46496b4c01d8427e97058c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>List all entries matching a query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/372</link><project id="" key="" /><description>A query parameter something like "all" can be provided to list all entries matching a query, without providing the size parameter

Forum
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/9ee16268288bcb4d#
</description><key id="317226">372</key><summary>List all entries matching a query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Olric</reporter><labels /><created>2010-09-15T10:41:50Z</created><updated>2013-04-04T18:02:26Z</updated><resolved>2013-04-04T18:02:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:02:26Z" id="15913416">Instead of this, we have the scroll API, which allows you to keep pulling results from a query. This is safer than allowing `all`, esp for big queries
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/371</link><project id="" key="" /><description>Expose bulk API for indexing and deleting docs. The REST AP endpoint is `/_bulk` and it follows the following structure (for json):

```
action_and_meta_data\n
optional_source\n
action_and_meta_data\n
optional_source\n
....
```

for example:

```
{ "index" : { "index" : "test", "type" : "type1", "id" : "1" } }
{ "type1" : { "field1" : "value1" } }
{ "delete" : { "index" : "test", "type" : "type1", "id" : "2" } }
{ "create" : { "index" : "test", "type" : "type1", "id" : "1" } }
{ "type1" : { "field1" : "value1" } }
```

A note on the format. The idea here is to make processing of this as fast as possible. As some of the actions will be needed to be redirected to other shards that exists on other nodes, only the action meta_data is parsed on the receiving node side. Also, zero copy buffers can be used on the source directly writing segments relevant to each action source to the network.

Client libraries using this protocol should try and strive to do something similar on the client side, and reduce as much as possible the creation of buffers.

The result is a full formatted json, with all the actions performed (in the same order), with possible error field indicating for each one in case of failure (on an item level).

Note, in the end, the full data needs to be represented on each server, so indexing 5GB of data should be broken down and not executed in a single batch.

If using the HTTP API, make sure that the client does not send HTTP chunks, as this will slow things down.
</description><key id="317212">371</key><summary>Bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-15T10:21:13Z</created><updated>2010-09-28T11:24:16Z</updated><resolved>2010-09-15T17:22:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-15T10:22:15Z" id="408168">Bulk API, closed by 3afe4da55078e7b14eb4f7ef38d897c7f0f7f13d.
</comment><comment author="rboulton" created="2010-09-28T09:53:21Z" id="434167">In the example, the "index" and "create" lines have a missing } at the end of the line.
</comment><comment author="kimchy" created="2010-09-28T11:24:16Z" id="434277">thanks, fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud discovery link in the docs is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/370</link><project id="" key="" /><description>http://www.elasticsearch.com/docs/elasticsearch/modules/discovery/

The link to Cloud on that page doesn't work.
</description><key id="315968">370</key><summary>Cloud discovery link in the docs is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels /><created>2010-09-14T18:11:32Z</created><updated>2010-09-15T19:49:42Z</updated><resolved>2010-09-15T19:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-15T19:49:42Z" id="409145">thanks!, fixed to point to the ec2 discovery.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Upgrading a simple mapping to multi_field mapping fails on merge conflicts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/369</link><project id="" key="" /><description>This can still be done by setting the `ignore_conflicts` flag to `true`.
</description><key id="314657">369</key><summary>Mapper: Upgrading a simple mapping to multi_field mapping fails on merge conflicts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-09-13T22:09:32Z</created><updated>2010-09-13T22:10:13Z</updated><resolved>2010-09-13T22:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-13T22:10:13Z" id="404878">Mapper: Upgrading a simple mapping to multi_field mapping fails on merge conflicts, closed by 85160ae3413f6d34f13025a126347d3bf14b4b85.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>exec the java process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/368</link><project id="" key="" /><description>Hi,

We run most of our system tools under daemontools.  bin/elasticsearch would start a separate process for the java process; so daemontools would terminate the shell process but not the java process when asking it to restart.  launchd will have similar issues.

The patch makes it exec in foreground mode, too, which fixes the problem.
</description><key id="313223">368</key><summary>exec the java process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels /><created>2010-09-12T22:37:56Z</created><updated>2014-07-16T21:57:02Z</updated><resolved>2011-02-20T00:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-13T12:36:06Z" id="403685">One note on this, if you are running in this mode (foreground), its a shame for elasticsearch to log both to console and files, so disable the console logging in the logging.yml file.
</comment><comment author="kimchy" created="2011-02-20T00:22:13Z" id="789549">Applied but never closed....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContent refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/367</link><project id="" key="" /><description>Several changes done in xcontent. This changes might imply backward changes for Java clients.
- Removed BinaryXContentBuilder and TextXContentBuilder, unified to XContentBuilder.
- Removed XSON and instead using the SMILE format. 
</description><key id="313143">367</key><summary>XContent refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-12T21:19:23Z</created><updated>2010-09-12T21:20:40Z</updated><resolved>2010-09-12T21:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-12T21:20:40Z" id="402737">XContent refactoring, closed by 38aeba438d6d14e2af913b7bcb0d8fee3dc1ffa2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting `path.work` does not affect the logs location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/366</link><project id="" key="" /><description /><key id="312227">366</key><summary>Setting `path.work` does not affect the logs location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-09-11T19:22:08Z</created><updated>2010-09-11T19:22:55Z</updated><resolved>2010-09-11T19:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-11T19:22:55Z" id="401375">Setting `path.work` does not affect the logs location, closed by ed5ffd6591a06c4e3587979cb985fd9f0dc2850e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: `match_all` filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/365</link><project id="" key="" /><description>Add a `match_all` filter.
</description><key id="311992">365</key><summary>Query DSL: `match_all` filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-11T13:00:55Z</created><updated>2010-09-11T13:01:37Z</updated><resolved>2010-09-11T13:01:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-11T13:01:37Z" id="400967">Query DSL: `match_all` filter, closed by 0718757612c0725059eb411f31560f80978e33bb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Allow to name filters and return per hit the filters it matched on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/364</link><project id="" key="" /><description>This really make sense for `or` / `bool` filters. Now, each filter can accept a `_name` in its top level definition, for example: 

```
{
    "filtered" : {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "terms" : {
                "name.last" : ["banon", "kimchy"],
                "_name" : "test"
            }
        }
    }
}
```

The search response will include for each hit the `matched_filters` it matched on.

Note, the query filter had to be enhanced in order to support this. In order to set a name, the `fquery` filter should be used, which wraps a `query` (just so there will be a place to set a name for it), for example:

```
{
    "filtered" : {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "fquery" : {
                "query" : {
                    "term" : { "name.last" : "banon" }
                },
                "_name" : "test"
            }
        }
    }
}
```
</description><key id="311893">364</key><summary>Search API: Allow to name filters and return per hit the filters it matched on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-11T09:23:17Z</created><updated>2010-09-11T09:38:29Z</updated><resolved>2010-09-11T09:38:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-11T09:38:29Z" id="400811">Search API: Allow to name filters and return per hit the filters it matched on, closed by eccc7d5ef21dade9bd14d3a3adaf60e664582ac0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo search across anti-meridian doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/363</link><project id="" key="" /><description>It seems that ES does not like bounding boxes that extend across the 
antimeridian (180&#176; meridian). If I specify a box with top left corner 
to the left of the meridian, and bottom right on the right, it treats 
it as if the box wrapped around the wrong (long) way around the Earth.

```
curl -XPUT 'http://localhost:9200/bug/' -d'{ index: { number_of_shards: 1, number_of_replicas: 0 } }'
curl -XPUT 'http://localhost:9200/bug/test/_mapping' -d '{
  "test" : {
      "properties" : {
            "user" : {"type" : "string", "index" : "not_analyzed"},
            "title" : {"type" : "string", "boost" : 2.0},
            "notes" : {"type" : "string"},
            "tags" : {"type" : "string", "index_name" : "tag"},
            "created" : {"type" : "date", "format" : "YYYY-MM-DD HH:mm:ss"},
            "geo" : {"type" : "geo_point"},
            "privacy" : {"type" : "integer"}
        }
    }
}'

curl -XPUT "http://localhost:9200/bug/test/1" -d'{"tags": ["geotagged", "w", "incendio", "puglia", "vacanze", "peschici", "estate2007", "leconseguenzedellincendio"], "notes": "", "created": "2007-08-22 02:52:15", "privacy": 1000, "user": "liberodicrederci", "title": "incendio peschici", "geo": {"lat": 21.5, "lon": -158.2}}'
curl -XPUT "http://localhost:9200/bug/test/2" -d'{"tags": ["geotagged", "w", "incendio", "puglia", "vacanze", "peschici", "estate2007", "leconseguenzedellincendio"], "notes": "", "created": "2007-08-22 02:52:15", "privacy": 1000, "user": "liberodicrederci", "title": "incendio peschici", "geo": {"lat": 29.5, "lon": -108.6}}'

curl -XPOST "http://localhost:9200/bug/test/_search?pretty=true" -d'
{
    "query": {
        "constant_score": {
            "filter": {
                "geo_bounding_box": {
                    "geo": {
                        "top_left" : {
                            "lat" : 32.5,
                            "lon" : 169.1
                        },
                        "bottom_right" : {
                            "lat" : 5.8,
                            "lon" : -140.6
                        }
                    }
                }
            }
        }
    }
}'
```
</description><key id="309370">363</key><summary>Geo search across anti-meridian doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-09-09T16:34:22Z</created><updated>2010-11-09T15:11:11Z</updated><resolved>2010-09-10T20:04:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-10T13:04:18Z" id="399304">Geo search across anti-meridian doesn't work, closed by 4e661c165a7590f8ffb5c01e00030b9daa1b8abc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search failure when no results are found and sorting by score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/362</link><project id="" key="" /><description>Using ES 0.10.

To reproduce:

```
curl -XPUT 'http://localhost:9200/bug/' -d'{ index: { number_of_shards: 1, number_of_replicas: 0 } }'
curl -XPUT 'http://localhost:9200/bug/test/_mapping' -d '    {
    "test" : {
        "properties" : {
            "user" : {"type" : "string", "index" : "not_analyzed"},
            "title" : {"type" : "string", "boost" : 2.0},
            "notes" : {"type" : "string"},
            "tags" : {"type" : "string", "index_name" : "tag"},
            "created" : {"type" : "date", "format" : "YYYY-MM-DD HH:mm:ss"},
            "geo" : {"type" : "geo_point"},
            "privacy" : {"type" : "integer"}
        }
    }
}'


curl -XPUT "http://localhost:9200/bug/test/1" -d'{"tags": ["geotagged", "w", "incendio", "puglia", "vacanze", "peschici", "estate2007", "leconseguenzedellincendio", "geo:lat=41942574", "geo:lon=16012316"], "notes": "", "created": "2007-08-22 02:52:15", "privacy": 1000, "user": "liberodicrederci", "title": "incendio peschici", "geo": {"lat": 41.942574, "lon": 16.012315999999998}}'
curl -XPUT "http://localhost:9200/bug/test/2" -d'{"tags": ["columbus", "georgia", "ranger", "banquet", "airborne", "fortbenning", "ftbenning", "75thrra"], "notes": "", "created": "2007-08-21 18:22:20", "privacy": 1000, "user": "rangerpete", "title": "20070811-191543-1D3-PSP_3298.JPG", "geo": {"lat": 32.461877999999999, "lon": -84.995496000000003}}'

curl -XPOST "http://localhost:9200/bug/test/_search?pretty=true" -d'
{
    "sort": [
        "_score",
        {"created": {"reverse": true}}
    ],
    "from": 0, "size": 10,
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "thisquerydoesnotexist",
                    "fields": ["title", "notes", "tags"],
                    "default_operator": "AND",
                    "use_dis_max": true
                }
            },
            "filter": {
                "geo_bounding_box": {
                    "geo": {
                         "top_left" : {
                            "lat" : 47.55,
                            "lon" : -124.10
                        },
                        "bottom_right" : {
                            "lat" : 31.06,
                            "lon" : -77
                        }
                    }
                }
            }
        }
    }
}'
```

Full backtrace:

```
[10:49:53,546][DEBUG][action.search.type       ] [USAgent] [photos][0], node[ed4fd2d5-fa99-41ba-b0c0-9c5d46b0472d], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@28acaa28] while moving to second phase
java.lang.IllegalArgumentException: field can only be null when type is SCORE or DOC
    at org.apache.lucene.search.SortField.initFieldType(SortField.java:217)
    at org.apache.lucene.search.SortField.&lt;init&gt;(SortField.java:127)
    at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:106)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:85)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:218)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$100(TransportSearchTypeAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:196)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:130)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:77)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:194)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:153)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:637)
```
</description><key id="308358">362</key><summary>Search failure when no results are found and sorting by score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreiz</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-09-08T21:57:41Z</created><updated>2010-09-09T12:48:01Z</updated><resolved>2010-09-09T12:48:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-09T12:48:01Z" id="397271">Search failure when no results are found and sorting by score, closed by b31b0e979c616251ed75b18a87193bd76b75eff3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: `geo_point` to support passing array of [lat, lon]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/361</link><project id="" key="" /><description>Allow to pass array of `lat` and `lon` as the value for a geo point (similar to what the search components support). Something like: `location : [40, 70]`. Also support array of arrays, similar to `location : [ [40, 70], [50, 80] ]`.
</description><key id="305888">361</key><summary>Mapper: `geo_point` to support passing array of [lat, lon]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-07T14:22:32Z</created><updated>2010-09-07T16:38:12Z</updated><resolved>2010-09-07T16:38:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-07T16:38:12Z" id="393290">Mapper: `geo_point` to support passing array of [lat, lon], closed by ad5945f141489dcf4a43c31d667353a303ffc743.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to configure a common logger prefix using `es.logger.prefix` system prop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/360</link><project id="" key="" /><description>ElasticSearch uses logger names like "node", "plugins", "discovery" etc. and there are a lot of loggers. When it is run as a standalone server, there won't be problem as all the code are supposed to be ElasticSearch code. 

However, when ES is used in a separated Java application as client, the logger mess up with my application's log. I use log4j.xml and I have to add a log of custom loggers entries because ES's ain't under its own package.

My suggestion:
1. I suppose your objective is to make the server log easier to read. I suggest to use full class name as logger name, and provide a default log4j layout pattern that doesn't show the full class name. Log4j's pattern is quite flexible.
http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html
   For example, for the category name "a.b.c" the pattern %c{2} will output "b.c". 

For the majority of cases, %c{1} should work. Except some cases such as "cluster.service" that %c{1} won't show a meaningful logger name.
1. a workaround is to use a System property to define a prefix for the logger. So when ES is run in a client environment, we can set a property like -Des.logger.prefix=com.elasticsearch. and then we can use a single log4j category to capture all the logs.
</description><key id="305779">360</key><summary>Allow to configure a common logger prefix using `es.logger.prefix` system prop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mingfai</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-07T13:21:23Z</created><updated>2010-09-08T09:16:46Z</updated><resolved>2010-09-08T09:16:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-08T09:15:44Z" id="394866">Agreed, I will add a system property to allow to define a common logger prefix named `-Des.logger.prefix`.
</comment><comment author="kimchy" created="2010-09-08T09:16:46Z" id="394867">Allow to configure a common logger prefix using `es.logger.prefix` system prop, closed by a768016779e3bdbddf310d2d861e74830a62d16c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: `geo_point` type to allow to set `store` on the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/359</link><project id="" key="" /><description>Allow to control if the lat / lon will be stored in the index.
</description><key id="305650">359</key><summary>Mapper: `geo_point` type to allow to set `store` on the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-07T11:10:05Z</created><updated>2010-09-07T16:38:12Z</updated><resolved>2010-09-07T16:38:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-07T16:38:12Z" id="393289">Mapper: `geo_point` type to allow to set `store` on the mapping, closed by c045b4d0ef382709347d324b2ff8c1b514bd6e7c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor fix to the doc about XContentFactory's package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/358</link><project id="" key="" /><description>At http://www.elasticsearch.com/docs/elasticsearch/java_api/index/, 
  import static org.elasticsearch.util.xcontent.XContentFactory.*;"

for the current release, the package is moved and the following should be used:

  import static org.elasticsearch.common.xcontent.XContentFactory.*;
</description><key id="305151">358</key><summary>Minor fix to the doc about XContentFactory's package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mingfai</reporter><labels /><created>2010-09-07T02:30:18Z</created><updated>2010-09-07T06:47:34Z</updated><resolved>2010-09-07T06:47:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-07T06:47:34Z" id="392180">right, will commit a fix to the docs (thats another repo).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Automatically identify "script" fields on the field elements in search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/357</link><project id="" key="" /><description>Allow to specify scripts also in the fields / URI when searching by automatically identifying them as such (basically, contains either `_source.` or `doc[`. The "name" of the field will be the script in this case.
</description><key id="303845">357</key><summary>Search API: Automatically identify "script" fields on the field elements in search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahendra</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-06T04:38:39Z</created><updated>2010-09-07T17:19:34Z</updated><resolved>2010-09-07T01:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-06T18:55:56Z" id="391404">earch API: Automatically identify "script" fields on the field elements in search, closed by f270fc00d23674a78802d5343eb195b10943b28d.
</comment><comment author="ppearcy" created="2010-09-07T17:19:34Z" id="393388">Sweet, great enhancement to provide a more unified interface. 

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to specify highlighter parameters on a per field level basis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/356</link><project id="" key="" /><description>Now it is possible to override global highlighter settings at the field level. On top of this it is also possible to specify "fragment_type" : "content" which will return highlighted content of the field (no fragments).
</description><key id="302746">356</key><summary>Allow to specify highlighter parameters on a per field level basis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-04T20:02:22Z</created><updated>2014-06-19T12:43:53Z</updated><resolved>2010-09-06T00:47:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-05T17:46:19Z" id="389791">Few changes that I plan to push:
- The global setting are not used. Also, you can't rely on the order the elements arrive in the json (so the globals are set before the fields).
- You can't rely on hashcode to provide uniqueness when constructing the map key.
- I am going to simplify things a bit in how thins are parsed (no need for global element).
- Also, single fragment builder will be used when setting number of fragments to 0. 
</comment><comment author="kimchy" created="2010-09-05T17:47:49Z" id="389795">Allow to specify highlighter parameters on a per field level basis, closed by cc1eac147a5278b46dfa18481dbbd2ed2461aa19.
</comment><comment author="lukas-vlcek" created="2010-09-05T19:32:06Z" id="389893">Cool, just the javadoc needs to be updated according to the changes: http://github.com/elasticsearch/elasticsearch/blob/cc1eac147a5278b46dfa18481dbbd2ed2461aa19/modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java#L44
Replace "fragment_type" : "content" with "number_of_fragments" : 0
</comment><comment author="lukas-vlcek" created="2010-09-06T22:22:11Z" id="391689">There is one major issue. If numberOfFragments is 0 then the Highlighter does not produce any fragments. Dirty workaround is to add the following line of code after this line: http://github.com/elasticsearch/elasticsearch/blob/cc1eac147a5278b46dfa18481dbbd2ed2461aa19/modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java#L100

```
field.numberOfFragments(1);
```
</comment><comment author="lukas-vlcek" created="2010-09-06T22:25:04Z" id="391696">I think it needs to be set to any number greater then 0. I did not check the Lucene source code but after I did the above change then the highlighter correctly yielded whole field content with highlighted text.
</comment><comment author="kimchy" created="2010-09-06T22:28:48Z" id="391700">applied.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Index / Update Settings: Automatically prefix all settings with `index.`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/355</link><project id="" key="" /><description>Automatically prefix all settings passed to create index and update settings APIs with `index.`. This should make the API calls simpler and in any case, those settings are only index level settings.
</description><key id="302393">355</key><summary>Create Index / Update Settings: Automatically prefix all settings with `index.`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-04T10:54:46Z</created><updated>2010-09-04T11:05:02Z</updated><resolved>2010-09-04T11:05:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-04T11:05:02Z" id="388595">Create Index / Update Settings: Automatically prefix all settings with `index.`, closed by 42b2e60af20448f3d47cfc630bc0656291945699.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thrift Plugin: Expose REST interface using thrift</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/354</link><project id="" key="" /><description>Expose the REST interface of elasticsearch using thrift. Thrift should provide better performance over http (hopefully). Since thrift provides both the wire protocol and the transport, it should make using it simpler (thought its lacking on docs...).

The thrift file is: http://github.com/elasticsearch/elasticsearch/blob/master/plugins/transport/thrift/elasticsearch.thrift.

Settings are:
- `thrift.type`: Can be either `threadpool`, `threadpool_framed`, `nonblocking`, and `hsha`. Defaults to `threadpool` (the default on hbase and cassandra).
- `thrift.port`: The port to bind to. Defaults to `9500-9600`.
- `thrift.protocol`: Either `compact` or `binary`. Defaults to `binary`.
</description><key id="302085">354</key><summary>Thrift Plugin: Expose REST interface using thrift</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-03T23:45:07Z</created><updated>2010-09-03T23:46:53Z</updated><resolved>2010-09-04T06:45:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-03T23:45:52Z" id="388132">Thrift Plugin: Expose REST interface using thrift, closed by 4f08801bb989be3c2dd03a44e644c7df6b19cf65.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Intelligent query routing to shards based on feedback </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/353</link><project id="" key="" /><description>I believe that all queries are getting round robined between shards. This works well when all the servers have identical h/w and all are all healthy and in a steady state w/ caches built up.

However, in a smaller cluster (and probably a bigger), one slow node can drag down performance disproportionately. 

If there was feedback back into the shard router from response times,  it would be able route _most_ traffic around shards with warming caches, nodes with failing h/w, out of mem conditions, and properly distribute traffic to slower vs faster h/w in the cluster. 

I'd like to propose a simple model to use for routing traffic without sharing data. Each node will make the decision where to route based on its own response time history. The most recent response times should receive the most weight. The current trailing response time number should be computed by taking the last trailing response time and averaging it with the current. This would make the current response time average receive 50% of its weight from the last response time, but that should probably be tweaked based on testing. 

So, each node can keep a response time metric for every shard. When it chooses which shard to route to, the ones with the lowest current response time should be hit with a frequency inversely proportional to the current trailing response time. 

This scheme is definitely up for debate and there are a lot of other ways to accomplish this, but I think response time is the key metric. 

I would consider this yet another killer feature that differentiates itself and from my perspective the highest priority feature/bug request that I have out there. 

I would also be happy to play around with possible solutions to see how they work on my set up.

Thanks,
Paul
</description><key id="302058">353</key><summary>Intelligent query routing to shards based on feedback </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-09-03T23:10:40Z</created><updated>2014-07-08T11:46:35Z</updated><resolved>2014-07-08T11:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-01-09T04:27:42Z" id="3407223">+1
and i think routing by shard or filtered by shard in querydsl will be a great feature if we have,it will be more efficiently in some scenarios such as data exporting,if we can query by shard,we can exporting data shard by shard,do paging within  one shard,no additional data will be cross-collected.
,@kimchy can you support these feature
</comment><comment author="spinscale" created="2013-06-10T13:59:07Z" id="19200018">is there anything you cannot do with shard allocation as described in http://www.elasticsearch.org/guide/reference/index-modules/allocation/ - which allows you allocate shards with a big write load on bigger machines.

Using elasticsearch to calculate metrics as a platform for lots of different use cases is pretty tough. IMO it makes more sense to offload this decision to you as a developer/engineer who has to scale the cluster. But opinions differ here I guess :-)
Using solely response times as a performance metric might lead to complex queries being treated as slow, even thoug they are just slow by nature... (again, just showing some off the complexity with this approach)
</comment><comment author="clintongormley" created="2014-07-08T11:46:35Z" id="48325940">No discussion for over a year. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local Gateway: Expose `gateway.local.initial_shards` to control when to recover and index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/352</link><project id="" key="" /><description>The possible values are `quorum`, `full`, `full-1`, or a number. This will control the minimum number of shards to recover from are required before an index can be recovered. For example, for an index with 2 replicas, `quorum` will require at least 2 shards to exists for each shard replication group.

The default is `quorum`.
</description><key id="301467">352</key><summary>Local Gateway: Expose `gateway.local.initial_shards` to control when to recover and index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-03T15:37:56Z</created><updated>2010-09-03T15:38:36Z</updated><resolved>2010-09-03T15:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-03T15:38:36Z" id="387287">Local Gateway: Expose `gateway.local.initial_shards` to control when to recover and index, closed by b40f0bc5a4187b8a070b9caf0de38017d537d331.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>groovy - performing search using byte[] causes parse exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/351</link><project id="" key="" /><description>When performing a search using closures in setQuery, everything works fine:

```
 ex:  node.client.prepareSearch("test").setQuery({term(user:"kimchy")}).gexecute();
```

If you pass a byte[] that represents the exact same closure then you will get a parse exception (see error below, bogus comma added right before "query"):

```
 ex:  node.client.prepareSearch("test").setQuery("{term(user:\"kimchy\")}".getBytes()).gexecute();
```

ERROR 2010-09-02 08:47:55,945 [main] Failed to execute phase [query], total failure; shardFailures {[38274707-cc69-459d-ad20-7c7b2fbc1a70][test][2]: RemoteTransportException[[Frigga][inet[/127.0.0.1:9300]][search/phase/query]]; nested: SearchParseException[[test][2]: query[null],from[-1],size[-1]: Parse Failure [Failed to parse [{, "query" : {term(user:"kimchy")}}]]]; nested: JsonParseException[Unexpected character (',' (code 44)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name
 at [Source: [B@29d54d9a; line: 1, column: 3]];
</description><key id="301184">351</key><summary>groovy - performing search using byte[] causes parse exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mcclured</reporter><labels /><created>2010-09-03T10:55:58Z</created><updated>2010-09-05T18:44:05Z</updated><resolved>2010-09-05T18:44:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-04T18:30:58Z" id="389008">Thats because the expected format is json, so when you construct a String, it should be json: `{ "term" : { "user" : "kimchy" } }`. Also, I just pushed a String version of setQuery and not just byte[] one.
</comment><comment author="mcclured" created="2010-09-05T18:44:05Z" id="389855">yeah, sorry about that... thought I had tested json string as well with same results but not the case.  thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting MasterNotDiscovered Exception after configuring CLUSTER</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/350</link><project id="" key="" /><description>After configuring a separate cluster in elasticsearch.yml. Indexing is not working properly, not able to do a bulk indexing ..

 can any one explain the easiest way to configure separate CLUSTER (not the default one) in step by step.

 thanks advance.
regards,
Deepak
</description><key id="301031">350</key><summary>Getting MasterNotDiscovered Exception after configuring CLUSTER</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raindeepak4u</reporter><labels /><created>2010-09-03T08:28:15Z</created><updated>2013-04-04T18:06:14Z</updated><resolved>2013-04-04T18:06:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-03T09:52:22Z" id="386755">Explaining is not done on issues, send a question to the mailing list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Allow for script fields to extract parts of the stored _`source`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/349</link><project id="" key="" /><description>The script fields support in the search API to allow to extract source elements. The extract element (and its "tree" of data) is then streamed back to the user. Here is an example:

```
{
    "query" : {  "match_all" : {} },
    "script_fields" : {
        "test1" : { "script" : "_source.obj1.arr" }
    }
}
```

Note the `_source` keyword here to navigate the json like model.

Its important to understand the difference between `doc['my_field'].value` and `_source.my_field`. The first, using the `doc` keyword, will cause the terms for that field to be loaded to memory (cached), which will result in faster execution, but more memory consumption. Also, it only allows for simple valued fields (can't return a json object from it).

The `_source` on the other hand causes the source to be loaded, parsed, and then only the relevant part of the json is returned.

Since its a script, logic can also be applied here.
</description><key id="299857">349</key><summary>Search API: Allow for script fields to extract parts of the stored _`source`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-09-02T18:18:26Z</created><updated>2010-09-02T18:29:58Z</updated><resolved>2010-09-02T18:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-02T18:29:58Z" id="385523">Search API: Allow for script fields to extract parts of the stored _`source`, closed by cd28afe9500d17fd3320dc2d197ec34798ebd292.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JMX: Disable creating by default an RMI JMX connector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/348</link><project id="" key="" /><description>Change the default `jmx.create_connector` from `true` to `false` when running as server. This will reduce the overhead of RMI, and especially RMI GC (on large heaps).

Also, by default, when a connector is created, set the default RMI GC to `36000000`.
</description><key id="299182">348</key><summary>JMX: Disable creating by default an RMI JMX connector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-02T11:15:17Z</created><updated>2010-09-02T18:29:58Z</updated><resolved>2010-09-02T18:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-02T18:29:58Z" id="385522">JMX: Disable creating by default an RMI JMX connector, closed by 7bd08d638db8eb04aac2d5e8dc5c488d3d146bf8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Default to `local` gateway (replace `none` gateway)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/347</link><project id="" key="" /><description>For a better out of the box experience, default to the `local` gateway instead of the `none` one. In order to revert back to the `none` gateway, set `gateway.type` to `none` in the config file or settings.
</description><key id="298305">347</key><summary>Gateway: Default to `local` gateway (replace `none` gateway)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-09-01T20:20:30Z</created><updated>2010-09-01T20:21:39Z</updated><resolved>2010-09-01T20:21:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-01T20:21:39Z" id="383792">Gateway: Default to `local` gateway (replace `none` gateway), closed by 042af200e80930aafedb8a6f85eab12d06501189.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Client jar build target to reduce size of dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/346</link><project id="" key="" /><description>There should be a build target that creates the smallest possible client jar when client applications wish to just use the Client api to store documents (instead of joining as a Node or using the rest apis).
</description><key id="298156">346</key><summary>Client jar build target to reduce size of dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels /><created>2010-09-01T18:52:52Z</created><updated>2013-04-04T18:06:47Z</updated><resolved>2013-04-04T18:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-01T19:05:19Z" id="383676">You mean the Transport Client? I am not sure how much can be saved. Jackson, Netty, Guice and google collections would still need to be used, and they form the major chunk of the jar size.
</comment><comment author="cwensel" created="2010-09-01T19:10:47Z" id="383688">yeah, sorry, the TransportClient.

also needs the core lucene jar(s) per my last build from master.

in my case i already have jackson in the cp, so departing from the uberjar model might be a win too. (hadoop already has like a billion dependencies as well already in my cluster)
</comment><comment author="kimchy" created="2010-09-01T19:22:18Z" id="383704">yea, the uber jar is there to not force jar version hell on the client. Lucene jars should not be needed on the TransportClient. I currently have not way to enforce it with the current build structure, but ping me with the exception you get, and I will fix it.
</comment><comment author="cwensel" created="2010-09-01T19:28:05Z" id="383712">was a UTF call on some lucene class if i remember correctly. will try and get a proper stack trace
</comment><comment author="clintongormley" created="2013-04-04T18:06:47Z" id="15913617">Nothing added after 3 years - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>EC2 Discovery should optionally bind to nodes who are members of all groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/345</link><project id="" key="" /><description>the default behavior is to bind to a node who is a member of any listed group.

an optional alternative approach where only nodes who are members of all listed groups will be added to the cluster.

for example, having security groups elasticsearch and staging a cluster should bind itself to all nodes with at least those two groups. 

this would allow for another independent cluster to exist bound to elasticsearch and production security groups.
</description><key id="296941">345</key><summary>EC2 Discovery should optionally bind to nodes who are members of all groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-08-31T23:48:08Z</created><updated>2010-09-01T18:32:20Z</updated><resolved>2010-09-01T18:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cwensel" created="2010-08-31T23:49:52Z" id="381903">have created a patch in this branch
http://github.com/cwensel/elasticsearch/tree/groupdiscovery

it adds the boolean property: any_group, which is true by default

a value of false requires candidate notes to have all listed groups
</comment><comment author="kimchy" created="2010-09-01T18:32:20Z" id="383616">EC2 Discovery should optionally bind to nodes who are members of all groups, closed by 38cae95ebcd0ac2b9329159b25576dea088ed130.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Frequent "error: "MasterNotDiscoveredException[]" when starting in EC2 w/ S3 gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/344</link><project id="" key="" /><description>Running 0.10.0 w/ aws cloud plugin. 

I have been playing around with EC2 + S3 in order to do performance testing. Overall, things work pretty well. 

The main problem I keep running into is that a new node will fail to discover the cluster. When I check cluster health on the node, it displays:
error: "MasterNotDiscoveredException[]"

The nodes just get stuck in this state and I then have to manually connect to the ones that hang and restart elasticsearch. 

This seems to happen about 50% of the time and when it occurs nothing appears in the log. 

This is not a blocking issue for me, as I am just doing performance testing, but if I really wanted to deploy in the cloud, this would prevent it (or at the very least, I'd need to write some monitor scripts to bounce ES when it gets in this state).

Thanks,
Paul
</description><key id="296603">344</key><summary>Frequent "error: "MasterNotDiscoveredException[]" when starting in EC2 w/ S3 gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-31T19:52:55Z</created><updated>2010-10-11T16:36:01Z</updated><resolved>2010-09-04T04:35:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-09-01T12:25:04Z" id="382747">Hi Paul, 

   Can you the logging of discovery to TRACE for a node that does not connect, and gist it?
</comment><comment author="ppearcy" created="2010-09-03T20:31:33Z" id="387874">Hey,
  Let me see what I can do. My cloud based testing is finished, but will fire up a couple of instances to see if I can capture this. 

Thanks,
Paul
</comment><comment author="ppearcy" created="2010-09-03T21:35:44Z" id="387961">I spent some time playing around with my config. It seems to be related to the first time the ec2 instance starts up. Unfortunately, my AMI has the default logging set. I updated the value on the three instances I was running and did various reboot orderings and they never had issues connecting for that case. 

So, it seems that it is related to the first start up on ec2 and may be some sort of race condition with the discovery. 

Sorry, but I don't think I can provide the info needed to see this ticket through.

Since that is the case, I am going to close this. 

Apologies for the noise. 

Thanks,
Paul
</comment><comment author="kimchy" created="2010-09-04T17:23:33Z" id="388961">No problem. Obviously it happened, if it does happen to you again, would love to nail it.
</comment><comment author="angelf" created="2010-10-02T11:22:10Z" id="443425">We are finding this error when launching a large number of instances at once (for example, adding 4 instances to a cluster that was previously made of 3 instances). On the other hand, if we launch them one by one auto-discovery works fine. 

Is this to be expected?

If the error is found restarting each instance process one-by-one fixes it. It takes about 5 minutes for the master to be found.
</comment><comment author="kimchy" created="2010-10-04T10:43:26Z" id="446092">Can you post the logs of those 4 instances you start at once? gist it if possible.
</comment><comment author="angelf" created="2010-10-04T11:36:06Z" id="446144">Yep: is this logging conf (http://gist.github.com/609558) ok or should I increase the log level? My apologies but I do not understand the logging configuration enough to know where to increase the log level for EC2 discovery.

Thanks!
</comment><comment author="kimchy" created="2010-10-04T12:30:14Z" id="446194">The default one should be ok, I want to see on the 4 new nodes started what they discovered. You can, on the same indentation level as `action`, put `discovery.ec2: TRACE`, for extra information on what was discovered through the ec2 api.
</comment><comment author="angelf" created="2010-10-05T16:36:19Z" id="449103">Ok. We first started 4 nodes and all of them connected to master without problems. We then stopped them, waited for about 1 hour and repeated the test again. In this second test two instances connected correctly to the master and two stayed "orphan". We left them like that for about 2 hours and the "orphan" instances never connected. 

This gist http://gist.github.com/611846 contains the log files of one instance that could connect and another that couldn't.

Obviously all of them used the same AMI. 

Just let me know if there's anything we can do to help you analyze the issue. 
</comment><comment author="angelf" created="2010-10-05T16:38:58Z" id="449110">We have also tested network connectivity between instances and the 'orphan' instances are able to connect to the others without problem (we tested ping and HTTP to 9200 port) 
</comment><comment author="kimchy" created="2010-10-09T15:39:52Z" id="458109">Hey,

   sorry for the late response, looked at the logs, the suspicious thing on the bad node is the last addition, which has address of null (which I did not think can happen). I just pushed support to ignore such addresses, I think that this is the problem. Can you give master a go?
</comment><comment author="angelf" created="2010-10-11T11:32:45Z" id="460603">So far it looks good: I have not been able to reproduce the problem since upgrading to master, I also did some specific tests launching up to 15 instances at once and all discovered the master every time. 

Thanks!
</comment><comment author="kimchy" created="2010-10-11T16:36:01Z" id="461226">great!, if there are any other problems, please report :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: a `local` gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/343</link><project id="" key="" /><description>The idea of the local gateway is to allow not to use a shared storage based gateway (as are the current gateways, like `fs`, `s3`, and `hdfs`). The `local` gateway allows to perform full cluster recovery using only the local information stored on each node. This include both recovering the full cluster state (indices created, mappings defined) as well as each index data.

It is important to configure the `gateway.recover_after_nodes` setting to include most of the expected nodes to be started after a full cluster restart. This will insure that the latest cluster state is recovered.
</description><key id="296485">343</key><summary>Gateway: a `local` gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.11.0</label></labels><created>2010-08-31T18:52:16Z</created><updated>2010-08-31T18:53:03Z</updated><resolved>2010-08-31T18:53:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-31T18:53:02Z" id="381320">implemented, still requires some more testing, but basics are working
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memcached transport does not support version, npe when no uri</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/342</link><project id="" key="" /><description>The memcached transport does not support 'version' in text mode, and version, quit, and stats neither have a uri, causing a npe.
</description><key id="295183">342</key><summary>Memcached transport does not support version, npe when no uri</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels><label>enhancement</label><label>v0.11.0</label></labels><created>2010-08-30T18:48:47Z</created><updated>2010-09-01T18:32:20Z</updated><resolved>2010-09-01T18:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cwensel" created="2010-08-30T18:49:37Z" id="379371">This branch has fixes in MemcachedDecoder and some minor enhancements.
http://github.com/cwensel/elasticsearch/tree/versionanduri
</comment><comment author="kimchy" created="2010-09-01T18:32:20Z" id="383617">Memcached transport does not support version, npe when no uri, closed by 46b58ce77f8129c595a1b4db0eb9b4d1bec356ed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Failure to handle null valued objects ("obj1" : null)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/341</link><project id="" key="" /><description>When index some of my documents I got java.lang.NullPointerException (some of documents has been indexed successfully ). There is because in some documents ['serialized_object']['avatars'] is null and in some this field is empty object (and few have not empty objects)

For index "contacts" I have install the following mapping:

es_index_mapping = {
        "contact": {
            "properties" : {
                "id": {
                    "type": "string",
                    "store": "yes"
                },
                "owner_id": {
                    "type": "string",
                    "index": "not_analyzed",
                    "store": "yes"
                },
                "company_id": {
                    "type": "string",
                    "index": "not_analyzed",
                    "store": "yes"
                },
                "serialized_object": {
                    "type": "object",
                    "index": "not_analyzed",
                    "store": "yes",
                    # Do not parse fields['dates'] as date b/c its often contains a noise
                    # and ES prevent to index entire objects
                    "properties" : {
                        "fields": {
                            "type": "object",
                            "properties" : {
                                "dates": {
                                    "type": "object",
                                    "properties" : {
                                        "value": {
                                            "type": "string"
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "fields": {
                    "type": "object",
                    "store": "yes",
                    # Do not parse fields.dates as date b/c its often contains a noise
                    # and ES prevent to index entire objects
                    "properties" : {
                        "dates": {
                            "type": "string",
                        }
                    }
                },
                "is_account": {
                    "type": "boolean",
                    "store": "yes"
                },
                "updated" : {
                    "type" : "date",
                    "store": "yes",
                    "index" : "not_analyzed",
                    "format" : "yyyy-MM-dd'T'HH:mm:ssZ"
                },
            }
        }
    }

Here a traceback:
[1a5e7e97-67e5-4344-bc55-828c704c5419], [P], s[STARTED]: Failed to execute [index {[contacts][contact][4bf571e8304e747bb9000081], source[{"serialized_object": {"updated": "2010-08-30T15:43:06+0000 , "account_id": null, "tags": [], "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": [{"note": "", "modificator": "", "value": "sergei", "sequenc ": 0}], "phone": [], "company_name": [], "address": [], "email": [{"note": "", "modificator": "other", "value": "sergei@kovbasyuk.com.ua", "sequence": 0}, {"note": "", "modificator": "home", "valu ": "webmans2@gmail.com", "sequence": 1}]}, "object_type": "contact", "avatars": null, "is_account": false, "company_id": "4bec433c304e745ad3000009", "id": "4bf571e8304e747bb9000081", "owner_id": " bec433c304e745ad3000007"}, "updated": "2010-08-30T15:43:06+0000", "account_id": null, "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": ["sergei , "1ZjHE5vngEUsRvj0J"], "phone": [], "company_name": [], "address": [], "email": ["sergei@kovbasyuk.com.ua", "webmans2@gmail.com", "1ZjHE5vngEUsRvj0J"], "name": ["sergei", "1ZjHE5vngEUsRvj0J"]}, " ompany_id": "4bec433c304e745ad3000009", "is_account": false, "id": "4bf571e8304e747bb9000081", "owner_id": "4bec433c304e745ad3000007"}]}]
java.lang.NullPointerException
        at org.elasticsearch.index.mapper.FieldMapper$Names.&lt;init&gt;(FieldMapper.java:50)
        at org.elasticsearch.index.mapper.xcontent.XContentFieldMapper$Builder.buildNames(XContentFieldMapper.java:176)
        at org.elasticsearch.index.mapper.xcontent.XContentStringFieldMapper$Builder.build(XContentStringFieldMapper.java:69)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeValue(XContentObjectMapper.java:440)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:336)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeValue(XContentObjectMapper.java:405)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeArray(XContentObjectMapper.java:397)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:330)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeObject(XContentObjectMapper.java:357)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:328)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeObject(XContentObjectMapper.java:357)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:328)
        at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:362)
        at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:312)
        at org.elasticsearch.index.shard.service.InternalIndexShard.innerIndex(InternalIndexShard.java:227)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:219)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:133)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:60)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:381)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.access$400(TransportShardReplicationOperationAction.java:208)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:278)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
</description><key id="294995">341</key><summary>Mapper: Failure to handle null valued objects ("obj1" : null)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">an2deg</reporter><labels><label>bug</label><label>v0.11.0</label></labels><created>2010-08-30T16:25:15Z</created><updated>2010-08-31T10:34:00Z</updated><resolved>2010-08-31T10:34:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-30T16:27:49Z" id="379088">Is there a chance for a "curl" recreation of this bug? Easier for me to write a test that recreates it then, and make sure its fixed.
</comment><comment author="an2deg" created="2010-08-31T07:14:22Z" id="380251">Can't reproduce it with curl, but here it:

curl -XPUT 'http://localhost:9200/contacts3/'
curl -XPUT 'http://localhost:9200/contacts3/contact3/_mapping' -d '
{"contact3": {"properties": {"serialized_object": {"index": "not_analyzed", "type": "object", "properties": {"fields": {"type": "object", "properties": {"dates": {"type": "object", "properties": {"value": {"type": "string"}}}}}}, "store": "yes"}, "fields": {"type": "object", "properties": {"dates": {"type": "string"}}, "store": "yes"}, "company_id": {"index": "not_analyzed", "type": "string", "store": "yes"}, "is_account": {"type": "boolean", "store": "yes"}, "id": {"type": "string", "store": "yes"}, "owner_id": {"index": "not_analyzed", "type": "string", "store": "yes"}}}}'

curl -XPUT 'http://localhost:9200/contacts3/contact3/4bf571e8304e747bb9000081' -d '
{"serialized_object": { "account_id": null, "tags": [], "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": [{"note": "", "modificator": "", "value": "sergei", "sequenc ": 0}], "phone": [], "company_name": [], "address": [], "email": [{"note": "", "modificator": "other", "value": "sergei@kovbasyuk.com.ua", "sequence": 0}, {"note": "", "modificator": "home", "value": "webmans2@gmail.com", "sequence": 1}]},  "updated": "2010-08-30 15:43:06+0000", "object_type": "contact", "avatars": null, "is_account": false, "company_id": "4bec433c304e745ad3000009", "id": "4bf571e8304e747bb9000081", "owner_id": " bec433c304e745ad3000007"}, "account_id": null, "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": ["sergei", "1ZjHE5vngEUsRvj0J"], "phone": [], "company_name": [], "address": [], "email": ["sergei@kovbasyuk.com.ua", "webmans2@gmail.com", "1ZjHE5vngEUsRvj0J"], "name": ["sergei", "1ZjHE5vngEUsRvj0J"], "avatars": null}, "company_id": "4bec433c304e745ad3000009", "is_account": false, "id": "4bf571e8304e747bb9000081", "owner_id": "4bec433c304e745ad3000007", "updated": "2010-08-30 15:43:06+0000"}
'
</comment><comment author="kimchy" created="2010-08-31T08:21:59Z" id="380343">can you recreate this in a different way so I can easily run it?
</comment><comment author="an2deg" created="2010-08-31T09:33:49Z" id="380425">Ok, here is updated test case:
1) Index first document with "avatars": {}
2) Then index document with "avatars": null and get java.lang.NullPointerException

curl -XDELETE 'http://localhost:9200/contacts3/'

curl -XPUT 'http://localhost:9200/contacts3/'

curl -XPUT 'http://localhost:9200/contacts3/contact3/_mapping' -d '
{"contact3": {"properties": {"serialized_object": {"index": "not_analyzed", "type": "object", "properties": {"fields": {"type": "object", "properties": {"dates": {"type": "object", "properties": {"value": {"type": "string"}}}}}}, "store": "yes"}, "fields": {"type": "object", "properties": {"dates": {"type": "string"}}, "store": "yes"}, "company_id": {"index": "not_analyzed", "type": "string", "store": "yes"}, "is_account": {"type": "boolean", "store": "yes"}, "id": {"type": "string", "store": "yes"}, "owner_id": {"index": "not_analyzed", "type": "string", "store": "yes"}}}}'

curl -XPUT 'http://localhost:9200/contacts3/contact3/4bf571e8304e747bb9000081' -d '
{"serialized_object": { "account_id": null, "tags": [], "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": [{"note": "", "modificator": "", "value": "sergei", "sequenc ": 0}], "phone": [], "company_name": [], "address": [], "email": [{"note": "", "modificator": "other", "value": "sergei@kovbasyuk.com.ua", "sequence": 0}, {"note": "", "modificator": "home", "value": "webmans2@gmail.com", "sequence": 1}]}, "updated": "2010-08-30 15:43:06+0000", "object_type": "contact", "avatars": {}, "is_account": false, "company_id": "4bec433c304e745ad3000009", "id": "4bf571e8304e747bb9000081", "owner_id": " bec433c304e745ad3000007"}, "account_id": null, "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": ["sergei", "1ZjHE5vngEUsRvj0J"], "phone": [], "company_name": [], "address": [], "email": ["sergei@kovbasyuk.com.ua", "webmans2@gmail.com", "1ZjHE5vngEUsRvj0J"], "name": ["sergei", "1ZjHE5vngEUsRvj0J"], "avatars": {}}, "company_id": "4bec433c304e745ad3000009", "is_account": false, "id": "4bf571e8304e747bb9000081", "owner_id": "4bec433c304e745ad3000007", "updated": "2010-08-30 15:43:06+0000"} '

curl -XPUT 'http://localhost:9200/contacts3/contact3/4bf571e8304e747bb9000082' -d '
{"serialized_object": { "account_id": null, "tags": [], "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": [{"note": "", "modificator": "", "value": "sergei", "sequenc ": 0}], "phone": [], "company_name": [], "address": [], "email": [{"note": "", "modificator": "other", "value": "sergei@kovbasyuk.com.ua", "sequence": 0}, {"note": "", "modificator": "home", "value": "webmans2@gmail.com", "sequence": 1}]}, "updated": "2010-08-30 15:43:06+0000", "object_type": "contact", "avatars": null, "is_account": false, "company_id": "4bec433c304e745ad3000009", "id": "4bf571e8304e747bb9000082", "owner_id": " bec433c304e745ad3000007"}, "account_id": null, "fields": {"website": [], "dates": [], "last_name": [], "description": [], "title": [], "first_name": ["sergei", "1ZjHE5vngEUsRvj0J"], "phone": [], "company_name": [], "address": [], "email": ["sergei@kovbasyuk.com.ua", "webmans2@gmail.com", "1ZjHE5vngEUsRvj0J"], "name": ["sergei", "1ZjHE5vngEUsRvj0J"], "avatars": null}, "company_id": "4bec433c304e745ad3000009", "is_account": false, "id": "4bf571e8304e747bb9000082", "owner_id": "4bec433c304e745ad3000007", "updated": "2010-08-30 15:43:06+0000"} '

On last command I get {"error":"ReplicationShardOperationFailedException[[contacts3][1] ]; nested: "} in console and traceback in logs
</comment><comment author="kimchy" created="2010-08-31T10:33:09Z" id="380499">found the problem, writing a test for it and will push a fix shortly
</comment><comment author="kimchy" created="2010-08-31T10:34:00Z" id="380500">Mapper: Failure to handle null valued objects ("obj1" : null), closed by a3efa21d718e3f1230a5fdbbc530919e9031df7f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Intelligent shard distribution on node shutdown with replicas=0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/340</link><project id="" key="" /><description>When a node is shutdown, while running with replicas=0, there should be an option to redistribute all of the shards before shutting down.
</description><key id="294394">340</key><summary>Intelligent shard distribution on node shutdown with replicas=0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-30T01:13:28Z</created><updated>2013-02-08T01:48:20Z</updated><resolved>2013-02-08T01:32:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kitchen" created="2013-02-08T01:22:36Z" id="13272427">+1

also, it might be nice if even if you're running with replicas &gt; 0 that it'll redistribute the shards on the node prior to shutdown, so that the cluster status always remains green. This prevents issues where you shut one node down and another crashes at the same time and you lose some shards.
</comment><comment author="kimchy" created="2013-02-08T01:32:25Z" id="13272736">There is an option today, one can use shard allocation control to make sure the relevant node is not part of the shard placement. This will cause all shards to be moved away from the mentioned node. Once they moved, you can safely shutdown the node.

Simplest way is to use the cluster update settings API, for example:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
    }
}'
```

Regarding shutting down a node and losing another, its important to remember that the node being shutdown can always be brought back up and the data it holds will be used if needed.
</comment><comment author="kitchen" created="2013-02-08T01:48:20Z" id="13273178">very cool, thanks! I'll take a look at the allocation control API and see what other fun things it can do :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exceptions with 2 nodes and replicas = 0 after shards have stabilizeed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/339</link><project id="" key="" /><description>I was playing around with master last night in a distributed set up
with two servers and replicas = 0, while running queries on 50
threads.

When I ran similar tests with replicas=1, I didn't have any issues. I
plan to use replicas=1 in the real world and this testing was just for
performance numbers.

I was running a single node and added another. Once half the shards have been reallocated to the new box and I was in a steady state, I saw intermittent exceptions like this:
http://gist.github.com/551611

These are quite unexpected as there was nothing in flux. I was running
all the queries against a single node and this is the one that
reported the errors.
</description><key id="294392">339</key><summary>Exceptions with 2 nodes and replicas = 0 after shards have stabilizeed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-30T01:11:27Z</created><updated>2013-04-04T18:08:01Z</updated><resolved>2013-04-04T18:08:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:08:01Z" id="15913682">Bug against old version which hasn't been reported again - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception with replicas=0 and shard reallocating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/338</link><project id="" key="" /><description>I was playing around with master last night in a distributed set up
with two servers and replicas = 0, while running queries on 50
threads.

When I ran similar tests with replicas=1, I didn't have any issues. I
plan to use replicas=1 in the real world and this testing was just for
performance numbers.

```
One test that I ran was adding a new node. While this was occurring, I
saw large swaths of exceptions for queries that look like this:
http://gist.github.com/551622

These appear to be failures related to the shard reallocating.
```
</description><key id="294391">338</key><summary>Exception with replicas=0 and shard reallocating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-30T01:09:47Z</created><updated>2013-04-04T18:17:36Z</updated><resolved>2013-04-04T18:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:17:36Z" id="15914237">This appears to be a duplicate of another old bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing return fields in plugin attachment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/337</link><project id="" key="" /><description>ES is unable to return the processed title, author and metadata fields even if they are marked as stored.

Here there is a testcase script, with result for every command.

http://pastebin.com/x2F0gamz

Hi,
   Alberto
</description><key id="292571">337</key><summary>Missing return fields in plugin attachment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2010-08-27T16:33:57Z</created><updated>2010-11-10T16:21:26Z</updated><resolved>2010-11-10T16:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aparo" created="2010-11-10T16:21:26Z" id="532015">The data is in fields.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Improve `or` and `and` filters to accept just array of filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/336</link><project id="" key="" /><description>Simplify the DSL for `or` and `and` filters removing the need for `filters` inner element. Here is an example:

```
{
    "filtered" : {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "and" : [
                {
                    "term" : { "name.first" : "shay1" }
                },
                {
                    "term" : { "name.first" : "shay4" }
                }
            ]
        }
    }
}
```
</description><key id="289799">336</key><summary>Query DSL: Improve `or` and `and` filters to accept just array of filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-25T15:06:04Z</created><updated>2010-08-25T15:06:37Z</updated><resolved>2010-08-25T15:06:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-25T15:06:37Z" id="371060">Query DSL: Improve `or` and `and` filters to accept just array of filters, closed by 9d615a4f0b9cd908dc730cdf69c175d33efea592.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ThreadPool Module configuration is not parsed, ThreadPoolModule.configure() is wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/335</link><project id="" key="" /><description>With the following node yml configuration ThreadPool settings does not change : 
threadpool:  
   type: scaling
   scaling: 
     min: 1
     max: 5
     scheduled_size: 5

The thread pool type is still cached from : http://localhost:9200/_cluster/nodes

After digging the source code we found that : ThreadPoolModule.configure() is wrong : 
Class&lt;? extends Module&gt; moduleClass = settings.getAsClass("transport.type", CachedThreadPoolModule.class, "org.elasticsearch.threadpool.", "ThreadPoolModule");

i guess it should be as : settings.getAsClass("threadpool.type"

Abdurrahim Eke
www.ifountain.com
</description><key id="289761">335</key><summary>ThreadPool Module configuration is not parsed, ThreadPoolModule.configure() is wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ekelog</reporter><labels /><created>2010-08-25T14:17:22Z</created><updated>2010-08-25T14:35:35Z</updated><resolved>2010-08-25T21:28:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-25T14:28:16Z" id="370979">fixed it in master.
</comment><comment author="ekelog" created="2010-08-25T14:35:35Z" id="370997">thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing Buffer Size: Refine default setting to 10% (from 40%) and add more settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/334</link><project id="" key="" /><description>The default indexing buffer size (`indices.memory.index_buffer_size`) is too high (`40%` of total memory), change it to `10%`. Also, if percentage is used, allow to set `min_index_buffer_size` (defaults to `48mb`) and `max_index_buffer_size` which defaults to unbounded.

The indexing buffer size is divided between all the different shards. So, if there are 5 shards allocated, each will have `index_buffer_size / 5` indexing buffer size.
</description><key id="288507">334</key><summary>Indexing Buffer Size: Refine default setting to 10% (from 40%) and add more settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-24T15:10:58Z</created><updated>2010-08-24T15:12:51Z</updated><resolved>2010-08-24T15:12:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-24T15:12:51Z" id="369170">Indexing Buffer Size: Refine default setting to 10% (from 40%) and add more settings, closed by 49b4659322c31d842b073d43bfe21ad32a13c6cb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Refactor gateway storage system to remove possible corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/333</link><project id="" key="" /><description>In general, the new implementation works (in spirit) in the same manner git works. Each snapshot is a commit point, that stores files in the gateway into an auto generated name, and finally, a commit point is written with the "directory" which maps between this pseudo name to physical name, and the size. The new design allows for more resiliency when it comes to corruption. It also allows for exciting future features like saving a commit point and restoring from it, or automatically create a commit point each day for the last 5 days and be able to rollback to a specific commit point.
</description><key id="287474">333</key><summary>Gateway: Refactor gateway storage system to remove possible corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.10.0</label></labels><created>2010-08-23T19:12:24Z</created><updated>2010-08-23T19:13:55Z</updated><resolved>2010-08-23T19:13:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-23T19:13:55Z" id="367658">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should escape \r\n in exception message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/332</link><project id="" key="" /><description>see
  new StreamCorruptedException("Expecting \r\n after data block");

in
  MemcachedDecoder
</description><key id="286340">332</key><summary>Should escape \r\n in exception message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-22T18:26:55Z</created><updated>2010-08-23T19:19:51Z</updated><resolved>2010-08-23T19:19:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-23T19:19:51Z" id="367668">Should escape \r\n in exception message, closed by 8079b5def5b87d0204a9c74677533c097c851b99.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards Allocation: Only rebalance a shard if all its instances are already active</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/331</link><project id="" key="" /><description>This helps reduce the number of concurrent relocation happening. When reblancing, only relocate shards that have all its instances active.
</description><key id="285422">331</key><summary>Shards Allocation: Only rebalance a shard if all its instances are already active</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-21T14:48:54Z</created><updated>2010-08-21T14:49:30Z</updated><resolved>2010-08-21T14:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-21T14:49:30Z" id="364935">Shards Allocation: Only rebalance a shard if all its instances are already active, closed by 3117341f44a31276655d2efb6c1ef957e37405cc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Health API: Add `initializing_shards` and `unassigned_shards` to the response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/330</link><project id="" key="" /><description>Add the number of initializing shards (possible being recovered) and unassigned shards (shards that are not assigned to any node) to the response (on all levels).
</description><key id="285372">330</key><summary>Cluster Health API: Add `initializing_shards` and `unassigned_shards` to the response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-21T13:02:17Z</created><updated>2010-08-21T13:03:37Z</updated><resolved>2010-08-21T20:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-21T13:03:36Z" id="364872">Cluster Health API: Add `initializing_shards` and `unassigned_shards` to the response, closed by 38e6649a7e364ec728ed4b953e3b5ba575f1f6ae.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't set "no stopwords" on analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/329</link><project id="" key="" /><description>It appears that my empty stopwords for a pattern tokenizer is getting ignored. Doing a facet on the field shows me that AN is not making it into the term list. 

Here is my elastic search config:

  gateway:
    type: fs
    fs:
      location: /data/elasticsearch/
  index:
    analysis :
      analyzer :
        piped_space_semi :
          type: pattern
          lowercase: true
          pattern: '[&amp;||;| ]+'
          stopwords: []
  path:
    logs: /data/elasticsearch/logs/

Here is a test to reproduce this. It creates the index, adds a mapping to use to tokenizer, submits two docs and then shows the bug with the two queries.

curl -XPUT http://localhost:9200/testindex/

curl -XPUT http://localhost:9200/testindex/testindex/_mapping -d '{"testindex": {"date_formats": ["date_optional_time"], "dynamic": false, "properties": {"testsymbol": {"omit_norms": true, "type": "string", "analyzer": "piped_space_semi"}}, "_source": {"compress": true}}}'

curl -XPUT 'http://localhost:9200/testindex/testindex/test1' -d '
{ testsymbol: "US;MSFT" }
'

curl -XPUT 'http://localhost:9200/testindex/testindex/test2' -d '
{ testsymbol: "US;AN" }
'
# Matches both - Should only match US;AN

http://localhost:9200/testindex/testindex/_search?q=%22US%20AN%22
# Matches correct

http://localhost:9200/testindex/testindex/_search?q=%22US%20MSFT%22
</description><key id="283900">329</key><summary>Can't set "no stopwords" on analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-19T21:55:20Z</created><updated>2014-01-10T20:27:41Z</updated><resolved>2010-08-20T07:22:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-20T00:21:40Z" id="362872">Yea, thats a problem.. . The problem with how configuration is done is that there is no way to identify empty array (== no stop words) and not set (== default stop words). I am going to push a fix that will allow for `stopwords: _none_` meaning no stopwords.
</comment><comment author="kimchy" created="2010-08-20T00:22:49Z" id="362875">Can't set "no stopwords" on analyzer, closed by c0552bdc7099a93a57e8b574f43b0daefc07a9d1.
</comment><comment author="kimchy" created="2010-08-20T00:23:02Z" id="362876">pushed a fix, can you test it?
</comment><comment author="ppearcy" created="2010-08-20T00:23:14Z" id="362878">Awesome and as always thanks! 
</comment><comment author="ppearcy" created="2010-08-20T00:24:11Z" id="362879">Yeah, will have an update for you tomorrow. Just started rebuilding content for a different reason anyways. 
</comment><comment author="kimchy" created="2010-08-20T00:24:23Z" id="362880">no problem, its pretty late here, would love someone looking over my shoulder ;). And I fixed it for all analyzers, not just pattern.
</comment><comment author="ppearcy" created="2010-08-20T00:25:37Z" id="362883">Cool... will have details later tonight (my time) for you.
</comment><comment author="ppearcy" created="2010-08-20T07:32:22Z" id="363252">Looks good to me. Thanks!
</comment><comment author="mrkamel" created="2014-01-10T20:25:12Z" id="32062491">Would be really great to have the

```
stopwords: _none_
```

in the docs. Searched several hours for a fix and finally found the `_none_` solution in the source code.
Please let me know if i can help to extend the docs.
</comment><comment author="clintongormley" created="2014-01-10T20:27:41Z" id="32062694">@mrkamel with pleasure :)

See https://github.com/elasticsearch/elasticsearch/tree/master/docs/reference/analysis
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Failure to read full translog from the gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/328</link><project id="" key="" /><description>When recovering from the gateway, there might be an error where not all translog operations will be read from it (accompanied with a WARN log message). This has very small chances of happening.

Note, the fix will require a full cluster restart with the flush API called before the shutdown.
</description><key id="281819">328</key><summary>Gateway: Failure to read full translog from the gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-18T11:29:20Z</created><updated>2010-08-18T11:29:59Z</updated><resolved>2010-08-18T11:29:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-18T11:29:59Z" id="360071">Gateway: Failure to read full translog from the gateway, closed by 2259ef671b2fd9656d039440cd6525e30f6abe67.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>boolean values cannot be searched with expected queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/327</link><project id="" key="" /><description>When searching for a document with a boolean value, I must search for the values "T" or "F" instead of the more intuitive true/"true" and false/"false". It also appears that the values 1 and 0 do not work as substitutes in the query, regardless of whether they are numbers or strings.

I should be able to use true, "true", "T", 1, and "1" to search for true values.
</description><key id="281366">327</key><summary>boolean values cannot be searched with expected queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-17T21:28:32Z</created><updated>2010-08-18T19:50:08Z</updated><resolved>2010-08-18T19:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-17T22:19:06Z" id="359418">Which queries did you try and execute?
</comment><comment author="grantr" created="2010-08-17T23:20:01Z" id="359467">I was using bool queries, but it can also be demonstrated with curl and query strings:

curl -XPOST http://127.0.0.1:9200/test_index/test -d '{"boolean_field":true}'
curl -XPOST http://127.0.0.1:9200/test_index/test -d '{"boolean_field":false}'
curl http://127.0.0.1:9200/test_index/test/_count?q=boolean_field:false
curl http://127.0.0.1:9200/test_index/test/_count?q=boolean_field:0
curl http://127.0.0.1:9200/test_index/test/_count?q=boolean_field:F
curl http://127.0.0.1:9200/test_index/test/_count?q=boolean_field:true
curl http://127.0.0.1:9200/test_index/test/_count?q=boolean_field:1
curl http://127.0.0.1:9200/test_index/test/_count?q=boolean_field:T
</comment><comment author="kimchy" created="2010-08-18T12:10:59Z" id="360112">ok, found the problem. In any case, just a note, with your example above, you should issue a refresh after you index, so you verify what you indexed is now searchable. In any case, I will fix it for query string types, it should work for other queries, but if it doen't ping...
</comment><comment author="kimchy" created="2010-08-18T12:11:31Z" id="360113">boolean values cannot be searched with expected queries, closed by 91aada270640a16eddcf58d1624319c2cd05b7b4.
</comment><comment author="grantr" created="2010-08-18T19:50:08Z" id="360815">Forgot about the refresh, thanks! I just checked out the new version and it works with query strings and bool queries.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filtered more_like_this doesn't parse correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/326</link><project id="" key="" /><description>Hiya

This query fails to parse:

```
curl -XGET 'http://127.0.0.1:9200/_search'  -d '
{
   "query" : {
      "filtered" : {
         "query" : {
            "more_like_this" : {
               "like_text" : "joe bloggs"
            }
         },
         "filter" : {
            "term" : {
               "source" : "foo"
            }
         }
      }
   }
}
'
```

with the error:

```
[filtered] requires 'filter' element]
```

However these two queries parse correctly:

```
curl -XGET 'http://127.0.0.1:9200/_search'  -d '
{
   "query" : {
      "filtered" : {
         "query" : {
            "match_all" : {}
         },
         "filter" : {
            "term" : {
               "source" : "foo"
            }
         }
      }
   }
}
'
```

and

```
curl -XGET 'http://127.0.0.1:9200/_search'  -d '
{
   "query" : {
        "more_like_this" : {
           "like_text" : "joe bloggs"
        }
   }
}
'
```
</description><key id="281253">326</key><summary>filtered more_like_this doesn't parse correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-17T19:11:33Z</created><updated>2010-08-18T11:36:48Z</updated><resolved>2010-08-18T11:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-18T11:36:48Z" id="360077">filtered more_like_this doesn't parse correctly, closed by de71a1ce996d36fd6dad1703f3dc4510f77c7657.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search_type=query_and_fetch does not return the requested size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/325</link><project id="" key="" /><description>I don't think search results in the query_and_fetch case are being properly combined after search execution to each shard.

If I have an index with 5 shards, the number of results returned are always 5x of what I requested when using query_and_fetch. 

So, run this to create 5 documents:

curl -XPUT 'http://localhost:9200/twitter/user/1' -d '
{ name: "Shay Banon" }
'
curl -XPUT 'http://localhost:9200/twitter/user/2' -d '
{ name: "Shay Banon" }
'
curl -XPUT 'http://localhost:9200/twitter/user/3' -d '
{ name: "Shay Banon" }
'
curl -XPUT 'http://localhost:9200/twitter/user/4' -d '
{ name: "Shay Banon" }
'
curl -XPUT 'http://localhost:9200/twitter/user/5' -d '
{ name: "Shay Banon" }
'

This query is correct and returns 1 result:
http://localhost:9200/twitter/_search?q=_:_&amp;size=1

While this one returns 5 results:
http://localhost:9200/twitter/_search?q=_:_&amp;size=1&amp;search_type=query_and_fetch

Thanks
</description><key id="280308">325</key><summary>search_type=query_and_fetch does not return the requested size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-16T21:46:03Z</created><updated>2010-08-17T15:37:45Z</updated><resolved>2010-08-17T15:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-17T07:15:14Z" id="358123">Thats by design. Since query_and_fetch gets "size" results from all the different shards, it returns them to the user. I think I documented it here: http://www.elasticsearch.com/docs/elasticsearch/rest_api/search/search_type/.
</comment><comment author="ppearcy" created="2010-08-17T15:37:45Z" id="358800">Duh, yeah, it totally is explained there. Sorry about that. 

Will close this out and read the docs better next time around :-)

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document IDs need to be URL decoded when indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/324</link><project id="" key="" /><description>Here is a simple test to reproduce:

curl -XPUT 'http://localhost:9200/twitter/user/ki%20mchy' -d '
{ name: "Shay Banon" }
'

This should result in:
"_id":"ki mchy"

Instead, we end up with the literal text and not the url decoded form:
"_id":"ki%20mchy"
</description><key id="280066">324</key><summary>Document IDs need to be URL decoded when indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-16T17:55:18Z</created><updated>2010-08-17T18:23:22Z</updated><resolved>2010-08-17T18:23:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2010-08-16T17:55:52Z" id="357266">Btw, I am running master downloaded this morning. 
</comment><comment author="grantr" created="2010-08-16T21:46:18Z" id="357602">I have also noticed this. My ruby client needs to unescape _id as a special case.
</comment><comment author="kimchy" created="2010-08-17T07:46:45Z" id="358154">Recreated, will push a fix for this later today.
</comment><comment author="kimchy" created="2010-08-17T18:23:22Z" id="359074">Document IDs need to be URL decoded when indexed, closed by 7833cb1c76b032c1e8788793db222fb9ce2f2074.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Add `compress` option to `_source` mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/323</link><project id="" key="" /><description>Allow to enabled compression on `_source` mapping by setting `compress` to `true`. For example:

```
{
    "tweet" {
        "_source" : {
            "compress" : true
        }
     }
}
```

One can dynamically change the source mapping (by merging a new mapping def, for example, just the above). Documents that have already been indexed will remain with the original setting, while new ones will take the new setting into account.
</description><key id="279079">323</key><summary>Mapping: Add `compress` option to `_source` mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-15T18:23:41Z</created><updated>2010-11-09T17:01:41Z</updated><resolved>2010-08-16T01:28:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-15T18:28:14Z" id="355897">implemented.
</comment><comment author="apatrida" created="2010-11-09T16:30:01Z" id="529278">Can this have a threshold at which to compress?  (i.e. don't compress things under 2K)
</comment><comment author="kimchy" created="2010-11-09T17:01:41Z" id="529397">No there isn't, but should be simple to add since the system can already handle cases where some sources are compressed and some are not (which is the more difficult part to implement), open an issue for it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery: Using transport compression explicitly when recovering an index from a peer shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/322</link><project id="" key="" /><description>When performing recovery from a peer shard, always use compression to send the data over the wire, regardless of what is globally configured on the transport level.
</description><key id="278639">322</key><summary>Recovery: Using transport compression explicitly when recovering an index from a peer shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-15T00:01:10Z</created><updated>2010-08-15T00:01:43Z</updated><resolved>2010-08-15T00:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-15T00:01:43Z" id="355250">Recovery: Using transport compression explicitly when recovering an index from a peer shard, closed by c18904eb96134f51f13357b4c029a7d442db8e81.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: add global compression support compressing all internal transport communication (using lzf)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/321</link><project id="" key="" /><description>By setting `transport.tcp.compress` to `true`, all communication between nodes will be compressed using lzf (offers a very good  tradeoff between compression size and cpu overhead).

This can be very handy for environments where network is not as fast as one would hope it will be.
</description><key id="278637">321</key><summary>Transport: add global compression support compressing all internal transport communication (using lzf)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-14T23:56:44Z</created><updated>2010-08-14T23:57:36Z</updated><resolved>2010-08-14T23:57:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-14T23:57:36Z" id="355248">Transport: add global compression support compressing all internal transport communication (using lzf), closed by 1ee2f80e688e875824b56071ea54543b579e3ba6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add logic to handle OR queries with non-grouped negatives</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/320</link><project id="" key="" /><description>Here are the details:
May want to consider adding logic in the case of non-nested negatives
in OR clauses.

For example
indexid:"test" OR -indexid:"test"
-&gt;
indexid:"test" OR (-indexid:"test" AND _:_)

Right now, with ES when doing query generation from a syntax tree with
this, I am adding grouping around negative OR clauses to accommodate.

So:
indexid:"test" OR -indexid:"test"
-&gt;
indexid:"test" OR (-indexid:"test")

Which I believe ES then interprets as:
indexid:"test" OR (-indexid:"test" AND _:_) 

A few more details are here:
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/9e80a32a3e8827f1/5e48ef4c9f596b16#5e48ef4c9f596b16

I would consider this a low  priority request. 

Thanks!
</description><key id="277794">320</key><summary>Add logic to handle OR queries with non-grouped negatives</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-13T18:40:47Z</created><updated>2014-01-25T13:44:45Z</updated><resolved>2014-01-25T13:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T08:46:11Z" id="21028822">Is this still an issue for you? it's more a lucene level feature as it relates to the lucene query parsing.  We try to solve it with the elasticsearch query DSL. Does that work for you?
</comment><comment author="clintongormley" created="2014-01-25T13:44:45Z" id="33289220">No comment in 6 months, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Chunk based storage broken, fails to recover from gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/318</link><project id="" key="" /><description>When configuring (or using the default) chunk based storage (where big files are broken down into chunks) then recovery from gateway fails. This is critical since, by default, the s3 gateway chunks with `100mb`.
</description><key id="277406">318</key><summary>Gateway: Chunk based storage broken, fails to recover from gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-13T11:02:53Z</created><updated>2010-08-13T11:04:03Z</updated><resolved>2010-08-13T11:04:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-13T11:04:03Z" id="353435">Gateway: Chunk based storage broken, fails to recover from gateway, closed by d12c757824a362ed64500fa3ecd42ff92efd51c0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices Status: Expose recovery (gateway and peer) and snapshot status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/317</link><project id="" key="" /><description>Expose both recovery (either from gateway or another peer) and snapshot status, including both real time status and the last status.
</description><key id="277188">317</key><summary>Indices Status: Expose recovery (gateway and peer) and snapshot status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-13T03:38:24Z</created><updated>2010-08-18T14:14:24Z</updated><resolved>2010-08-18T14:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-18T14:14:23Z" id="360259">implemented (over several commits)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: custom score script, allow to use `_score` as well as `score` as the underlying query score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/316</link><project id="" key="" /><description /><key id="276474">316</key><summary>Query DSL: custom score script, allow to use `_score` as well as `score` as the underlying query score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-12T15:41:00Z</created><updated>2010-08-12T15:41:42Z</updated><resolved>2010-08-12T15:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-12T15:41:41Z" id="352068">Query DSL: custom score script, allow to use `_score` as well as `score` as the underlying query score, closed by 2bd9a63467e3e9b57d8960df5516fcae64f78951.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add `char_filter` on top of `tokenizer`, `filter`, and `analyzer`. Add an `html_strip` char filter and `standard_html_strip` analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/315</link><project id="" key="" /><description>The analysis process in Lucene allows also for `char_filter` to be used which are filters done on the actual character stream before the tokenization process. Allow to configure custom `char_filter` and provide an implementation for html stripping called `html_strip`. 

Also, add a `standard_html_strip` analyzer that combines the standard analyzer with an html_strip char filter.

Here are some examples how to configure it using both `yaml` and `json`:

YAML:

```
index :
  analysis :
    tokenizer :
      standard :
        type : standard
    char_filter :
      my_html :
        type : html_strip
        escaped_tags : [xxx, yyy]
        read_ahead : 1024
    filter :
      stop :
        type : stop
        stopwords : [test-stop]
      stop2 :
        type : stop
        stopwords : [stop2-1, stop2-2]
    analyzer :
      standard :
        type : standard
        stopwords : [test1, test2, test3]
      custom1 :
        tokenizer : standard
        filter : [stop, stop2]
      custom2 :
        tokenizer : standard
        char_filter : [html_strip, my_html]
```

JSON:

```
{
    "index" : {
        "analysis" : {
            "tokenizer" : {
                "standard" : {
                    "type" : "standard"
                }
            },
            "char_filter" : {
                "my_html" : {
                    "type" : "html_strip",
                    "escaped_tags" : ["xxx", "yyy"],
                    "read_ahead" : 1024
                }
            },
            "filter" : {
                "stop" : {
                    "type" : "stop",
                    "stopwords" : ["test-stop"]
                },
                "stop2" : {
                    "type" : "stop",
                    "stopwords" : ["stop2-1", "stop2-2"]
                }
            },
            "analyzer" : {
                "standard" : {
                    "type" : "standard",
                    "stopwords" : ["test1", "test2", "test3"]
                },
                "custom1" : {
                    "tokenizer" : "standard",
                    "filter" : ["stop", "stop2"]
                },
                "custom2" : {
                    "tokenizer" : "standard",
                    "char_filter" : ["html_strip", "my_html"]
                }
            }
        }
    }
}
```
</description><key id="276444">315</key><summary>Analysis: Add `char_filter` on top of `tokenizer`, `filter`, and `analyzer`. Add an `html_strip` char filter and `standard_html_strip` analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-12T15:15:01Z</created><updated>2010-08-19T12:09:54Z</updated><resolved>2010-08-12T22:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-12T15:16:40Z" id="352028">Analysis: Add `char_filter` on top of `tokenizer`, `filter`, and `analyzer`. Add an `html_strip` char filter, closed by 98bc8285ea4d169e4a2448239ca1819c96fa98b2.
</comment><comment author="clintongormley" created="2010-08-19T11:40:53Z" id="361739">This works well. What does the `escaped_tags` arg do? I experimented a bit, but it didn't seem to make any difference.

Given that this html_strip filter (plus standard analyser) will be a frequent requirement, any chance of making it one of the default analysers available by default?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term Facets: Add `reverse_count` and `reverse_term` to `order` options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/314</link><project id="" key="" /><description>Allow to also order term facets in reverse order either by count or term.
</description><key id="276319">314</key><summary>Term Facets: Add `reverse_count` and `reverse_term` to `order` options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-12T12:26:00Z</created><updated>2013-09-12T12:57:23Z</updated><resolved>2010-08-12T15:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-12T15:16:40Z" id="352027">Term Facets: Add `reverse_cout` and `reverse_term` to `order` options, closed by e29925684abfe82afce8100506cadaa526eaec9c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backing up and Restoring a Specific Index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/313</link><project id="" key="" /><description>Description:
Allow an index level granularity of backing up and restoring of indexes.

Use Case:
In a multi-tenant application, where there is an index per tenant, there is often a need to restore an index to a previous "snapshot".
It is even possible that a tenant owner / admin will be allowed to "go back in time" to a previous version of the index.

Feature Request:
1. Allow a per index backup and restore.
2. Right after the restore, the newest document in the index should be the newest document in the snapshot we restored from. Documents related to this index in the transaction log or the work folders should be removed.

A possibly more complicated feature request:
Have an API to define index periodic "save points". e.g. create a save point every 10 minutes.
Have another API to restore an index to a specific save point.
An additional API that would let us list the current available save points.

The process of backing up an index to NFS would be:
1. Define a periodic snapshot of every X minutes.
2. Snapshot the NFS every X minutes (with the S3 integration this can be automatic)

The process of restoring:
1. Restore a snapshot from NFS.
2. List the available snapshots (through API)
3. Restore to the latest snapshot.
(with S3, this could be nicer: list the available snapshot then retrieve from S3 and restore in one API call)
- In the advanced feature request, the periodic mechanism can be left for some external scheduler.

See also: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/fa111bbdcfe5872a/1ce33465200d66e1#1ce33465200d66e1
</description><key id="275457">313</key><summary>Backing up and Restoring a Specific Index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talsalmona</reporter><labels /><created>2010-08-11T16:59:19Z</created><updated>2013-07-23T09:52:17Z</updated><resolved>2013-07-23T09:52:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2010-11-11T04:04:01Z" id="533557">oh,time machine,cool feature
</comment><comment author="spinscale" created="2013-07-23T09:52:17Z" id="21403838">Closing this as a duplicate of #3070 (even though it is the older one)
See #2458 for more info
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove memory monitor and move translog operations threshold to shard level setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/312</link><project id="" key="" /><description>Now, with the improved memory management in 0.9 (hook into cleaning the cache when readers gets closed, and automatic setting of indexing buffer size), and the fixed bugs in Lucene for memory leaks, there is really no need for the memory cleaner.

The only other aspect that it was doing is flushing a shard based on the number of translog operations. This logic should really exists on the shard level and should be moved there.

The new setting for flush on translog operations is `index.translog.flush_threshold`, and defaults to `5000`.
</description><key id="274648">312</key><summary>Remove memory monitor and move translog operations threshold to shard level setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-10T22:38:50Z</created><updated>2010-08-11T14:59:11Z</updated><resolved>2010-08-11T14:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-11T14:59:11Z" id="350223">Remove memory monitor and move translog operations threshold to shard level setting, closed by ee26d55296f7ff31d11c666721696c1a06baaa7a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>EC2 Discovery ignoring instances with more than one security group</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/311</link><project id="" key="" /><description>If an instance belongs to more than one security group, security group filtering during peer zen discovery throws out the instance if the group being searched for isn't the first group name.
</description><key id="274434">311</key><summary>EC2 Discovery ignoring instances with more than one security group</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-10T19:12:31Z</created><updated>2010-08-10T19:16:23Z</updated><resolved>2010-08-10T19:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-10T19:16:22Z" id="349095">EC2 Discovery ignoring instances with more than one security group, closed by 23b8b81e1598e5995724f34b88e07afa6c265055.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Automatically filter by type for facets that use explicit field names that are prefixed by the type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/310</link><project id="" key="" /><description>When using facets that use explicit field names, and the field name is prefixed by type (person.name), then automatically filter the facet to only work on person type docs. (this is the same way queries work).
</description><key id="274090">310</key><summary>Facets: Automatically filter by type for facets that use explicit field names that are prefixed by the type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2010-08-10T14:00:21Z</created><updated>2010-08-10T16:51:00Z</updated><resolved>2010-08-10T16:51:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-10T16:51:00Z" id="348892">Facets: Automatically filter by type for facets that use explicit field names that are prefixed by the type, closed by f2018e2f86d6fed0acb3e27ae99723337a60fc7a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappers: also use `name` as a smart lookup for field/property on top of indexName and fullName</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/309</link><project id="" key="" /><description>Currently, when trying to resolve a mapping definition when searching and other parts of the system, the fullName (a.b.c) and the indexName (based on the path type), in that order, are used to try and find the mappings. Add to that lookup process also the "pure" name of a property (even if its embedded in objects).
</description><key id="274027">309</key><summary>Mappers: also use `name` as a smart lookup for field/property on top of indexName and fullName</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-10T12:58:31Z</created><updated>2010-08-10T16:51:00Z</updated><resolved>2010-08-10T16:51:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-10T16:51:00Z" id="348891">Mappers: also use `name` as a smart lookup for field/property on top of indexName and fullName, closed by 61bb9d0ff9dfdb101cb9653e837c6d610503441c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting API: Apply highlighting to REST GET operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/308</link><project id="" key="" /><description>I would like to see an option to get `fields` or `_source` value with specific fields highlighted.
One way how to get similar result now is using term query on an ID field (to get one specific document) which would allow to use highlight DSL but still getting full content of some field highlighted would require either NullFragmenter (which is not implemented now) or setting value of `fragment_size` to some arbitrary large number.
</description><key id="272373">308</key><summary>Highlighting API: Apply highlighting to REST GET operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-08-08T20:13:54Z</created><updated>2010-12-14T11:35:17Z</updated><resolved>2010-12-14T11:35:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-12T18:42:58Z" id="352352">Not sure I understand. If you are using GET, what do you want to highlight?
</comment><comment author="lukas-vlcek" created="2010-12-14T11:35:17Z" id="610432">I think this already obsolete. Closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting API: Add support for custom FragmentsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/307</link><project id="" key="" /><description>I would like to see possibility to use NullFragmenter and provide custom implementation of Fragmenter (a built-in Fragmenter that is aware of sentence boundaries would be cool).
</description><key id="272372">307</key><summary>Highlighting API: Add support for custom FragmentsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-08-08T20:11:50Z</created><updated>2013-05-23T09:32:00Z</updated><resolved>2013-05-23T09:32:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-22T13:58:18Z" id="18280026">Hey Lukas,

As you can implement your own custom highlighter now, this should work now, see https://github.com/elasticsearch/elasticsearch/pull/3046.

Please close if it works out for you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Sort: `_geo_distance` sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/306</link><project id="" key="" /><description>Allow to sort by `_geo_distance`. Here is an example:

```
{
    "sort" : [
        {
            "_geo_distance" : {
                "pin.location" : [-40, 70],
                "order" : "asc",
                "unit" : "km"
            }
        }
    ],
    "query" : {
        "term" : { "user" : "kimchy" }
    }
}
```
</description><key id="272337">306</key><summary>Search Sort: `_geo_distance` sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-08T18:58:34Z</created><updated>2010-08-09T14:24:15Z</updated><resolved>2010-08-09T01:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-08T18:58:53Z" id="346156">implemented.
</comment><comment author="otisg" created="2010-08-09T13:30:58Z" id="346995">What is the role of "unit" here?  That is, the distance sort should always return hits in the same order regardless of unit of length, shouldn't it?  Thanks.
</comment><comment author="kimchy" created="2010-08-09T14:24:15Z" id="347071">Yes, it will. This ties into the ability to get the sort value(s) for each hit, and allows you to get it either in `km` or `miles`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Return sort values per hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/305</link><project id="" key="" /><description>Each search hit will now return the sort value(s) that was used for it.
</description><key id="272328">305</key><summary>Search API: Return sort values per hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-08T18:53:32Z</created><updated>2010-08-09T14:23:32Z</updated><resolved>2010-08-09T01:53:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-08T18:53:43Z" id="346146">implemented.
</comment><comment author="otisg" created="2010-08-09T13:29:38Z" id="346992">Sorry if this is an naive question, but why is that?  Don't all hits for a single search result share the same sort value(s)?  In other words, it's the search result that has all of its hits sorted a certain (same) way, no?  Thanks.
</comment><comment author="kimchy" created="2010-08-09T14:23:32Z" id="347066">Each search hit for a search request will have a different sort value, this allows to get those values as part of the response (and possibly display them). For example, in the geo_distance sort option, you would get the distane of each hit from the point provided.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facets: Allow to use custom scripts to control the term used or its inclusion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/304</link><project id="" key="" /><description>Allow to define a script for terms facet to control the actual term that will be used in the term facet collection, and also optionally control its inclusion or not.

The script can either return a boolean value, with `true` to include it in the facet collection, and `false` to exclude it from the facet collection.

Another option is for the script to return a `string` controlling the term that will be used to count against. The script execution will include the `term` variable which is the current field term used.

For example:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag",
                "size" : 10,
                "script" : "term + 'aaa'"
            }
        }
    }
}
```

And using the boolean feature:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag",
                "size" : 10,
                "script" : "term == 'aaa' ? true : false"
            }
        }
    }
}
```
</description><key id="272326">304</key><summary>Terms Facets: Allow to use custom scripts to control the term used or its inclusion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-08T18:52:33Z</created><updated>2010-08-08T18:52:45Z</updated><resolved>2010-08-08T18:52:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-08T18:52:44Z" id="346145">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facets: Add `order` option allowing to order either based on term or count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/303</link><project id="" key="" /><description>Allow to control the ordering of the terms facets, to be ordered either by `count` or `term`. The default it `count` which is the current behavior. Here is an example:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag",
                "size" : 10,
                "order" : "term"
            }
        }
    }
}
```
</description><key id="272323">303</key><summary>Terms Facets: Add `order` option allowing to order either based on term or count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-08T18:48:12Z</created><updated>2010-08-08T18:48:26Z</updated><resolved>2010-08-08T18:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-08T18:48:26Z" id="346140">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Settings API: Allow to dynamically change the number of replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/302</link><project id="" key="" /><description>A new API, update index / indices settings, allowing to change specific index level settings. The first setting supported is the `index.number_of_replicas` setting allowing to dynamically change the number of replicas an index has. 

The REST endpoint is `/_settings` (to update all indices) or `{index}/_settings` to update one (or more) indices settings. The body of the request includes the updated settings, for example:

```
{
    "index" : {
        "number_of_replicas" : 4
    }
}
```

The above will change the number of replicas to 4 from the current number of replicas. Here is a curl example:

```
curl -XPUT 'localhost:9200/my_index/_settings' -d '
{
    "index" : {
        "number_of_replicas" : 4
    }
}
'
```
</description><key id="272320">302</key><summary>Update Settings API: Allow to dynamically change the number of replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-08T18:45:38Z</created><updated>2015-02-16T09:45:50Z</updated><resolved>2010-08-09T01:46:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-08T18:46:22Z" id="346137">implements.
</comment><comment author="otisg" created="2010-08-09T13:26:04Z" id="346989">Nice!  So when the number of replicas is increased, what happens? e.g.
- Does ES start copying indices from master node to additional nodes?
- Does replication start immediately?
- Can one throttle the replication speed? (because sudden network and disk IO will affect query throughput)
- Can one reduce the number or replicas?
- In case of replica reduction, do indices get physically removed from disk?

Thanks.
</comment><comment author="kimchy" created="2010-08-09T14:22:19Z" id="347064">- Indices are not copied from the master node, they are recovered from a primary shard that is allocated "somewhere" in the cluster.
- Yes.
- There is built in throttling to not perform recovery from / to many shards at the same time.
- Yes.
- Yes.
</comment><comment author="clintongormley" created="2010-09-04T10:28:14Z" id="388565">Btw, this isn't in the ES docs yet
</comment><comment author="ayush21792" created="2014-10-28T07:00:51Z" id="60716793">Could you please tell how can we change index replica using java code???
I have done it through curl request but stuck while doing it through java code
</comment><comment author="imotov" created="2014-10-31T11:43:36Z" id="61249220">@ayush21792 you can find plenty of examples of using java API in elasticsearch [integration tests](https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/indices/settings/UpdateNumberOfReplicasTests.java#L75). Saying that, the best place to ask questions like these is elasticsearch mailing list. We are trying to use github issues to track bugs and feature requests.
</comment><comment author="ayush21792" created="2014-11-05T09:35:51Z" id="61781347">Thank you so much....
Could you please help me on another issue..When I index very large data of
12.7 million records,my elastic search server is terminating after 8.3
million records and showing the error of java heap memory overflow and the
error persists even after increasing memory.I am using 2gb java heap memory
and have also tried for 1gb and 3gb but result is same(halting at 8.3
million records).I have tried with both java api and es server both are
giving the same result.I am using windows 7.So could u please help me if
there is any setting which i am ignoring.

Thank You

On Fri, Oct 31, 2014 at 5:14 PM, Igor Motov notifications@github.com
wrote:

&gt; @ayush21792 https://github.com/ayush21792 you can find plenty of
&gt; examples of using java API in elasticsearch integration tests
&gt; https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/indices/settings/UpdateNumberOfReplicasTests.java#L75.
&gt; Saying that, the best place to ask questions like these is elasticsearch
&gt; mailing list. We are trying to use github issues to track bugs and feature
&gt; requests.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/302#issuecomment-61249220
&gt; .
</comment><comment author="ayush21792" created="2015-02-16T09:45:50Z" id="74482276">Ayush Gupta would like to connect on LinkedIn. How would you like to respond?

Accept: https://www.linkedin.com/blink?simpleRedirect=0SejoPcjoTe3AVcP0PdP8UdzARfkh9rCZFt65QqnpKqioTmloRmk9fqjRVpkJApn9xq7cCej5vs7xBnTpKqjRHpipOpmhKqmRBsyRQs6lzoS4JoyRDtCVFnSRJrScJr6RBfmtKqmJzon9Q9ClQqnpKimtBkClOs3Rx9CoJrmFxpCYTdCAJu6hUsC9EbjRBfP9SbSkLrmZzbCVFp6lHrCBIbDtTtOYLeDdMt7hE&amp;msgID=I5968273076624846849_500&amp;markAsRead=

View Ayush Gupta&amp;#39;s profile: https://www.linkedin.com/blink?simpleRedirect=ej5vs7xBnTpKqjRHpipOpmhKqmRBsyRQs6lzoS4JoyRDtCVFnSRJrScJr6RBfmtKqmJzon9Q9DpMrzRQ9CoJrmFxpCYTdCAJu6hUsC9EbjRBfP9SbSkLrmZzbCVFp6lHrCBIbDtTtOYLeDdMt7hE&amp;msgID=I5968273076624846849_500&amp;markAsRead=

You are receiving Reminder emails for pending invitations. Unsubscribe here: https://www.linkedin.com/blink?simpleRedirect=1JrSd5cylytmxQqmt5cylVr71Bsz0Q9j8ScjwOd3AJd38Qe6pzcjlApCdAojkVe3oNdj1Ae3sRojxxe6kOdjhzczdBe6oTciQMczcOdP8Jqk8O9nBIs6lOfmNFomRB9z0Sc30OfmhF9zoVdzcNdzsUejAPc3cTczwSejkZp6BD9zANnT1UplZSrCAZqSkCoDlPrDkJpyRzoClJnSRJrScJr6RBfmtKqmJzon9Q9CZLpPRQ9CoJrmFxpCYTdCAJu6hUsC9EbjRBfP9SbSkLrmZzbCVFp6lHrCBIbDtTtOYLeDdMt7hE&amp;msgID=I5968273076624846849_500&amp;markAsRead=

You received an invitation to connect. LinkedIn will use your email address to make suggestions to our members in features like People You May Know. Unsubscribe here: https://www.linkedin.com/blink?simpleRedirect=1CbmRGompLdPpFbnxAu79yq2QZp6BB9ztAi6sQejoQh71ydkkVdQJIjlBNoTcTgnBes5hds7pFqlFvlnx1gQhIe6wJm39KjzsPdngRpjxjgmpScllRiPBOpS8Sum8UjPlIukpUciReojdMoT1xjRFarll7tD5Jhm5zi7B9qBd5rmVeu7dHh3xek4lkmB9ejBhVsSMVk3xmiARzh4VorPtGozhUkDllgk51gmhbtmQPh6RqcjB8kk4Zp6BLr2oVclZMu6lvtCVFfmJB9B4ScSh5hSxmhD9JhB51fmVBqSZkp6BJ9CVRr3RQ9CoJrmFxpCYTdCAJu6hUsC9EbjRBfP9SbSkLrmZzbCVFp6lHrCBIbDtTtOYLeDdMt7hE&amp;amp;msgID=I5968273076624846849_500&amp;amp;markAsRead= Learn why we included this at the following link: https://www.linkedin.com/blink?simpleRedirect=0Ue3sQfmh9pmNzqnhOoioVclZMu6lvtCVFfmJB9CNOlmlzqnpOpldOpmRLt7dRoPRx9CoJrmFxpCYTdCAJu6hUsC9EbjRBfP9SbSkLrmZzbCVFp6lHrCBIbDtTtOYLeDdMt7hE&amp;msgID=I5968273076624846849_500&amp;markAsRead=
&amp;copy; 2014, LinkedIn Corporation. 2029 Stierlin Ct. Mountain View, CA 94043, USA
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTML tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/301</link><project id="" key="" /><description>The need to index fields containing HTML is a frequent use case. I think that it is important to have a built-in tokenizer which can handle HTML (ie remove tags and decode entities).

At the moment, in my client code I do the following:

```
$value =~ s/&lt;[^&gt;]+&gt;/ /g;   # replace any &lt;.....&gt; extents with a single space
$value =~ s/\s+/ /g;           # replace multiple spaces with a single space
$value =~ s/^ //;                # trim leading whitespace
$value =~ s/ $//;                # trim trailing whitespace
decode_entities($value);  # translate all HTML entities to the equiv UTF-8 char
```

This is sufficient to convert HTML to text suitable for indexing by the default analyzer - doesn't need to do any more than this.

Any chance of getting this built in?
</description><key id="272181">301</key><summary>HTML tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-08-08T14:59:37Z</created><updated>2010-09-21T21:16:58Z</updated><resolved>2010-09-21T21:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-18T21:54:10Z" id="360991">Is the latest html char filter addition #315 to construct your own analyzer that can strip out html good for this? If so, can I close this issue?
</comment><comment author="kimchy" created="2010-09-21T21:16:58Z" id="421486">closing the issue, the htmp strip has been added.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memcach: StreamCorruptedException when loading a cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/300</link><project id="" key="" /><description>Am getting the following exception to stderr when attempting to load a 3 node cluster.

This may be related to the NPE from XContentObjectMapper (see previous issue report).

I've never seen this failure during load of a stand-alone server.

Using the spy memcached library in Text protocol mode to load the cluster.

java.io.StreamCorruptedException: Expecting 
 after data block
    at org.elasticsearch.memcached.netty.MemcachedDecoder.decode(MemcachedDecoder.java:177)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:282)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:51)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)
</description><key id="270237">300</key><summary>Memcach: StreamCorruptedException when loading a cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-06T01:05:38Z</created><updated>2010-08-23T19:21:46Z</updated><resolved>2010-08-23T19:21:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-09T09:45:46Z" id="346814">seems like a problematic frame was sent (or something similar). Any chance for a recreation (no problem with a programs that creates this with load, I will run it locally).
</comment><comment author="cwensel" created="2010-08-09T22:13:18Z" id="347843">fwiw, just received this with a stand alone server (no cluster) using text mode memcached client.
(was testing possible npe fix in XContentObjectMapper)
</comment><comment author="cwensel" created="2010-08-22T21:19:36Z" id="366117">turns out this issue is caused by the StringBuffer in MemcachedDecoder is not being reset to length zero if the text mode header is not fully available. 

this change seems to resolve the problem:

```
            if (!done) {
                sb.setLength(0); // proposed fix
                buffer.resetReaderIndex();
                return null;
            }
```
</comment><comment author="kimchy" created="2010-08-23T19:20:44Z" id="367670">good catch, will push a fix.
</comment><comment author="kimchy" created="2010-08-23T19:21:46Z" id="367673">Memcach: StreamCorruptedException when loading a cluster, closed by 02b74dfb630951b2f1c7fb6fc7c6b0a94d6018f7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE with XContentObjectMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/299</link><project id="" key="" /><description>Am getting the following NPE somewhat randomly, yet frequently, with a fresh server install during bulk loading. 

Sometimes a restart after catching the exception will result in a stable runtime with no exceptions during bulk loading.

  java.lang.NullPointerException
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:335)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeValue(XContentObjectMapper.java:405)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeArray(XContentObjectMapper.java:397)
        at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:330)
        at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:336)
        at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:288)
        at org.elasticsearch.index.shard.service.InternalIndexShard.innerIndex(InternalIndexShard.java:233)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:225)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:130)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:57)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:381)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.access$400(TransportShardReplicationOperationAction.java:208)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:278)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)
</description><key id="270236">299</key><summary>NPE with XContentObjectMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-06T01:03:07Z</created><updated>2010-08-15T19:41:23Z</updated><resolved>2010-08-16T02:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cwensel" created="2010-08-07T17:55:20Z" id="345304">what seems to be a fix for this symptom is to forward through null tokens

at line 319 in XContentObjectMapper:

```
    while(token==null){
        token = parser.nextToken(); 
    }
```
</comment><comment author="cwensel" created="2010-08-07T20:14:55Z" id="345423">ok, a bad idea. seems to create an infinite loop. 
</comment><comment author="kimchy" created="2010-08-09T09:43:40Z" id="346812">Not sure, had a look at the code, seems like the parsing got mixed up. It might be a valid exception (i.e. sending a json structure that does not conform to the previous one for the same type) and just needs a better exception. Or it might be a bug ;). It would really help if you manage to recreate this, though I am sure its probably not simple...
</comment><comment author="cwensel" created="2010-08-09T17:20:41Z" id="347424">unfortunately the failure is random. 

sending the exact same stream of values to a stand-alone instance does not reproduce the problem every time or the same way when it does fail. even if i rm -rf work before each server start and load.

it also shows up on mac osx and ubuntu with open-jdk.
</comment><comment author="kimchy" created="2010-08-09T17:35:20Z" id="347445">can you maybe provide the complete test or indexing code that causes it? I don't mind running it several times till it happens.
</comment><comment author="cwensel" created="2010-08-09T20:55:30Z" id="347726">threw together a Node client (not a transport one) for bulk loading and am still seeing the same random NPE. was hoping this was a side-effect of using the memcached transport (and cient api). seems not.
</comment><comment author="cwensel" created="2010-08-09T21:47:57Z" id="347783">fyi, looks like this line should have a return?
                synchronized (mutex) {
                    objectMapper = mappers.get(currentFieldName);
                    if (objectMapper != null) {
                        objectMapper.parse(context);
                        // add a return here?!
                    }

otherwise there is a risk the context is parsed twice, leading to the npe?
</comment><comment author="cwensel" created="2010-08-09T21:49:53Z" id="347787">sorry, that's line 365 in XContentObjectMapper
</comment><comment author="cwensel" created="2010-08-09T22:20:30Z" id="347852">here is a strawman fix
http://github.com/cwensel/elasticsearch/commit/ab08a36d0695e353500fa2fbd916ed9059f64281

will let you know if the npe returns.
</comment><comment author="kimchy" created="2010-08-10T06:19:41Z" id="348216">nice catch!. Pushed fix to master. Not sure if it relates to the NPE you get.
</comment><comment author="kimchy" created="2010-08-15T19:19:17Z" id="355963">hey, do you still get this after the commit? if not, can we close this?
</comment><comment author="cwensel" created="2010-08-15T19:20:12Z" id="355964">I haven't seen it since the change. go ahead and close and i'll open a new issue if it pops back up.
</comment><comment author="kimchy" created="2010-08-15T19:41:23Z" id="355993">cool, I said it before, but I tend to repeat myself ;), nice catch!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memcached plugin fails with IndexOutOfBounds exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/298</link><project id="" key="" /><description>When using the spy memcached java libraries in Binary protocol mode, a java.lang.IndexOutOfBoundsException is thrown by the server and dumped to stderr.

Switching to Text mode seems to be a workaround.

java.lang.IndexOutOfBoundsException
        at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
        at org.elasticsearch.memcached.netty.MemcachedDecoder.decode(MemcachedDecoder.java:113)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:282)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:51)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)
</description><key id="270173">298</key><summary>Memcached plugin fails with IndexOutOfBounds exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwensel</reporter><labels /><created>2010-08-05T23:36:41Z</created><updated>2013-06-10T13:49:35Z</updated><resolved>2013-06-10T13:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-07T21:20:43Z" id="345468">Can you post a recreation for this?
</comment><comment author="cwensel" created="2010-08-07T23:25:11Z" id="345538">see 
http://github.com/cwensel/elasticsearch/commit/50a059fa1046717ec4d9fd5f3e85eb5dba292cbd

I can't see the server side logs (not sure where to look). but for me only the binary client craps out.
</comment><comment author="kimchy" created="2010-08-08T07:39:03Z" id="345783">cool, I will have a look at this later today.
</comment><comment author="kimchy" created="2010-08-09T17:44:57Z" id="347459">ok, seems like spy sends setq commands when sending several sets one after the other without waiting for a response. This (setq) is not supported, and I just committed a fix to at least gracefully log and disconnect when something like that happens.

Not too sure what to do with setq, seems like not to return a response unless there is an error, but did not see anywhere in the spy code that handles it. Very strange the way spy works... .

Lets ask a different questions, why do you want to use the memcached interface?
</comment><comment author="cwensel" created="2010-08-09T18:02:35Z" id="347485">we are planning on bulk loading both membase and elastic search from Cascading/Hadoop clusters and did not want to write two integration interfaces for the loading. there is much value in having one client api for multiple systems.

feels like all the stability errors we are seeing stem from the memcache transport. i'll re-code our bulk loaders to use the rest api instead and see if we see the same problems.
</comment><comment author="kimchy" created="2010-08-09T18:19:33Z" id="347506">Yes, it is definitely the memcache transport. Do you use Java (or JVM) to bulk load your data?
</comment><comment author="cwensel" created="2010-08-09T18:38:32Z" id="347545">yes, java. the bulk loading for now is just a manual step. once we have things working end-to-end we will begin loading directly from hadoop/cascading (java also).
</comment><comment author="kimchy" created="2010-08-09T18:43:16Z" id="347555">cool, then in this case, I would suggest going native with the java API, its more scalable and faster.
</comment><comment author="cwensel" created="2010-08-09T18:57:18Z" id="347578">the challenge is that we don't want to embed a 9M jar + dependencies into our hadoop cluster code (which has to be pushed to AWS for every test run).

is there a reasonably small java client api (or build target from gradle) we can use to embed in our client side app?
</comment><comment author="kimchy" created="2010-08-09T19:00:37Z" id="347581">you can use the transport client, and you won't need the lucene jars. Other than that, the rest of the jars are required.
</comment><comment author="kimchy" created="2010-08-09T20:49:03Z" id="347720">I meant that the elasticsearch jar is required. Its a beefy one, but not that many deps are there (guice, joda, netty, and jackson). I see your point though, not sure if the effort taken into building an HTTP based client is worth it though, but can't really say...
</comment><comment author="spinscale" created="2013-06-10T13:49:35Z" id="19199434">Closing this old one. Work is done on the elasticsearch-hadoop project which should tackle this issue. See https://github.com/elasticsearch/elasticsearch-hadoop
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API hang on a bogus call</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/297</link><project id="" key="" /><description>Let me know if there are any questions.

Here is a test in python to reproduce. No need to have index test or doctype test setup.

from httplib import HTTPConnection

path = 'i/test/test/101-DSTENVIRONMENT01\n-143352270'
body = '{"blah": "101"}'

conn = HTTPConnection('dm-adsearchd101', 9200)
conn.request('PUT', path, body)
response = conn.getresponse()

print ('passed')
</description><key id="269301">297</key><summary>REST API hang on a bogus call</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-05T04:07:29Z</created><updated>2010-08-11T16:07:28Z</updated><resolved>2010-08-11T21:59:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-11T10:18:10Z" id="349921">recreated this, and its simply a problem parsing this line in the http layer. For now, what I am going to do is to close the open connection when such an exception happens, so at the very least, it will not hang.
</comment><comment author="kimchy" created="2010-08-11T14:59:12Z" id="350224">REST API hang on a bogus call, closed by fbea92e688980a2b453dca8694e900e82955ea67.
</comment><comment author="ppearcy" created="2010-08-11T16:07:28Z" id="350309">Thank you!!! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to return all stored fields with a *</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/296</link><project id="" key="" /><description>As a convenience allow all fields to be returned with something like this:
fields=*

Thanks!
</description><key id="268375">296</key><summary>Ability to return all stored fields with a *</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>enhancement</label><label>v0.10.0</label></labels><created>2010-08-04T07:14:09Z</created><updated>2010-08-04T17:36:32Z</updated><resolved>2010-08-04T17:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-04T17:36:32Z" id="341285">Ability to return all stored fields with a *, closed by 8c5fdf4aeec0d1cbfdca94571276f480ff9b022e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After gateway recovery, mappings keep being applied on each cluster change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/295</link><project id="" key="" /><description>The fix will also improve the create index API, by first validation that the index and the mappings provided are valid.
</description><key id="268324">295</key><summary>After gateway recovery, mappings keep being applied on each cluster change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-08-04T05:49:25Z</created><updated>2010-08-04T06:02:18Z</updated><resolved>2010-08-04T06:02:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-04T06:02:18Z" id="340480">After gateway recovery, mappings keep being applied on each cluster change, closed by a44d30bb61e0a32ee76b2d2f48ab2411c0e5c5ba.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Polygon based filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/294</link><project id="" key="" /><description>A filter allowing to include hits that only fall within a polygon of points. Here is an example:

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_polygon" : {
                "person.location" : {
                    "points" : [
                        {"lat" : 40, "lon" : -70},
                        {"lat" : 30, "lon" : -80},
                        {"lat" : 20, "lon" : -90}
                    ]
                }
            }
        }
    }
}
```

Other formats supported:

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_polygon" : {
                "person.location" : {
                    "points" : [
                        [40, -70],
                        [30, -80],
                        [20, -90]
                    ]
                }
            }
        }
    }
}
```

and,

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_polygon" : {
                "person.location" : {
                    "points" : [
                        "40, -70",
                        "30, -80",
                        "20, -90"
                    ]
                }
            }
        }
    }
}
```

and 

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_polygon" : {
                "person.location" : {
                    "points" : [
                        "drn5x1g8cu2y",
                        "30, -80",
                        "20, -90"
                    ]
                }
            }
        }
    }
}
```
</description><key id="267935">294</key><summary>Geo: Polygon based filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-03T18:51:43Z</created><updated>2011-03-09T09:50:08Z</updated><resolved>2010-08-04T01:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-03T18:52:26Z" id="339793">Geo: Polygon based filter, closed by 959eb0e703a0bb23b55ccfc07f4350b71eaaf25c.
</comment><comment author="otisg" created="2010-08-09T13:20:49Z" id="346985">What is that "drn5x1g8cu2y" in the last example?  Thanks.
</comment><comment author="kimchy" created="2010-08-09T14:20:49Z" id="347062">Thats a geohash.
</comment><comment author="richardsyeo" created="2011-03-05T13:08:05Z" id="837601">I am having a problem getting the geo location stuff to work with v0.15 on windows 7 x64. Sun JDK 1.6.0.20.

I have done many things successfully with elasticsearch but just struggling with this. Also why is pin.location required, i.e. why is pin outer required? I was following examples.

Remember on windows need to use " instead of '. The " need to be repeated 3 times to escape.

The following command returns success...

\gnu\bin\curl\curl -XPUT "http://localhost:9200/scratch/test/12345" -d "{"""pin""":{"""MyId""":12345,"""location""":{"""type""":"""geo_point""","""lat""":40.12,"""lon""":-71.34}}}"

The following errors

\gnu\bin\curl\curl -XGET "http://localhost:9200/scratch/test/_search?pretty=true" -d "{"""filtered""":{"""query""":{"""match_all""":{}},"""filter""":{"""geo_polygon""":{"""pin.location""":{"""points""":[{"""lat""":40,"""lon""":-70},{"""lat""":30,"""lon""":-80},{"""lat""":20,"""lon""":-90}]}}}}}"

{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[AlmTJezeRUir7yIqVQ5cWA][s
cratch][0]: SearchParseException[[scratch][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filtered\":{\"query\":{\
"match_all\":{}},\"filter\":{\"geo_polygon\":{\"pin.location\":{\"points\":[{\"lat\":40,\"lon\":-70},{\"lat\":30,\"lon\":-80},{\"lat
\":20,\"lon\":-90}]}}}}}]]]; nested: SearchParseException[[scratch][0]: from[-1],size[-1]: Parse Failure [No parser for element [fil
tered]]]; }{[AlmTJezeRUir7yIqVQ5cWA][scratch][1]: SearchParseException[[scratch][1]: from[-1],size[-1]: Parse Failure [Failed to par
se source [{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"geo_polygon\":{\"pin.location\":{\"points\":[{\"lat\":40,\"lon\
":-70},{\"lat\":30,\"lon\":-80},{\"lat\":20,\"lon\":-90}]}}}}}]]]; nested: SearchParseException[[scratch][1]: from[-1],size[-1]: Par
se Failure [No parser for element [filtered]]]; }{[AlmTJezeRUir7yIqVQ5cWA][scratch][2]: SearchParseException[[scratch][2]: from[-1],
size[-1]: Parse Failure [Failed to parse source [{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"geo_polygon\":{\"pin.loca
tion\":{\"points\":[{\"lat\":40,\"lon\":-70},{\"lat\":30,\"lon\":-80},{\"lat\":20,\"lon\":-90}]}}}}}]]]; nested: SearchParseExceptio
n[[scratch][2]: from[-1],size[-1]: Parse Failure [No parser for element [filtered]]]; }{[AlmTJezeRUir7yIqVQ5cWA][scratch][3]: Search
ParseException[[scratch][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filtered\":{\"query\":{\"match_all\":{}},\
"filter\":{\"geo_polygon\":{\"pin.location\":{\"points\":[{\"lat\":40,\"lon\":-70},{\"lat\":30,\"lon\":-80},{\"lat\":20,\"lon\":-90}
]}}}}}]]]; nested: SearchParseException[[scratch][3]: from[-1],size[-1]: Parse Failure [No parser for element [filtered]]]; }{[AlmTJ
ezeRUir7yIqVQ5cWA][scratch][4]: SearchParseException[[scratch][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"filt
ered\":{\"query\":{\"match_all\":{}},\"filter\":{\"geo_polygon\":{\"pin.location\":{\"points\":[{\"lat\":40,\"lon\":-70},{\"lat\":30
,\"lon\":-80},{\"lat\":20,\"lon\":-90}]}}}}}]]]; nested: SearchParseException[[scratch][4]: from[-1],size[-1]: Parse Failure [No par
ser for element [filtered]]]; }]",
  "status" : 500
}

[2011-03-05 13:04:26,294][DEBUG][action.search.type       ] [Stonecutter] [scratch][4], node[AlmTJezeRUir7yIqVQ5cWA], [P], s[STARTED
]: Failed to execute [org.elasticsearch.action.search.SearchRequest@9c4dfd]
org.elasticsearch.search.SearchParseException: [scratch][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"filtered":{
"query":{"match_all":{}},"filter":{"geo_polygon":{"pin.location":{"points":[{"lat":40,"lon":-70},{"lat":30,"lon":-80},{"lat":20,"lon
":-90}]}}}}}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:416)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:331)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:165)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:132)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearc
hQueryThenFetchAction.java:76)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeActio
n.java:191)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:
75)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:150)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.search.SearchParseException: [scratch][4]: from[-1],size[-1]: Parse Failure [No parser for element [fil
tered]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:401)
        ... 10 more
</comment><comment author="kimchy" created="2011-03-08T05:59:11Z" id="846050">Can you create a recreation and gist it, and then post it on the mailing list? (check http://www.elasticsearch.org/help).
</comment><comment author="richardsyeo" created="2011-03-09T09:34:59Z" id="850991">Got it working eventually. Unfortunately the examples were not clear / misleading / being new to elasticsearch.

We found a subsequent comment somewhere which mentioned that geo_point no longer automatically detected.

I manually defined the index and specified type as geo_point. Extra " because via windows command line...

{"""location""":{"type":"""geo_point"""}}

I didn't wrap with outer "pin".

I then imported data.

We then had to wrap the "filtered" with a "query":{ } which is not shown in examples.

Here is example of query for windows command line which returns matches for UK and Ireland...

\curl -XGET "http://localhost:9200/scratch/test/_search?pretty=true" -d "{"""query""":{"""filtered""":{"""query""":{"""match_all""":{}},"""filter""":{"""geo_bounding_box""":{"""location""":{"""top_left""":{"""lat""":60,"""lon""":-10},"""bottom_right""":{"""lat""":50,"""lon""":-0}}}}}}}"

Useful website for working out long and lats.
http://www.getlatlon.com/
</comment><comment author="kimchy" created="2011-03-09T09:39:24Z" id="851001">Great. In the geo polygon filter it states that the geo_type is _required_: http://www.elasticsearch.org/guide/reference/query-dsl/geo-polygon-filter.html.
</comment><comment author="richardsyeo" created="2011-03-09T09:50:08Z" id="851025">Thanks. As a newbie to elasticsearch I wasn't clear what this meant. I tried passing as part of query explicitly. If examples were a little more complete and included actual curl command would greatly assist for novices. ElasticSearch is looking very promising!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script based sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/293</link><project id="" key="" /><description>Allow to sort based on custom scripts, here is an example:

```
{
    "query" : {
        ....
    },
    "sort" : {
        "_script" : { 
            "script" : "doc['field_name'].value * factor",
            "type" : "number",
            "params" : {
                "factor" : 1.1
            },
            "order" : "asc"
        }
    }
}
```

Note, it is recommended, for single custom based script based sorting, to use `custom_score` query instead as sorting based on score is faster.
</description><key id="267807">293</key><summary>Script based sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-03T16:22:18Z</created><updated>2010-08-03T16:22:31Z</updated><resolved>2010-08-03T16:22:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-03T16:22:31Z" id="339518">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add the ability to store the index name in the doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/292</link><project id="" key="" /><description>This would allow to execute terms facets on "_index" field.

By default, this is disabled, in order to enable it, the following mapping need to be set:

```
{
    "tweet" : {
        "_index" : { "enabled" : true }
    }
}
```
</description><key id="267533">292</key><summary>Add the ability to store the index name in the doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-03T11:56:17Z</created><updated>2010-11-09T16:57:45Z</updated><resolved>2010-08-05T00:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-04T17:36:31Z" id="341284">Add the ability to store the index name in the doc, closed by d72de60b6f77b147ca3ec9ac0cb407c5be5ead07.
</comment><comment author="lukas-vlcek" created="2010-08-09T14:09:29Z" id="347038">The following does not work as expected I think. Note the result for the facets, the type facet works fine but index does not.

```
curl -XDELETE 'http://localhost:9200/foo/'
curl -XPUT 'http://localhost:9200/foo/' -d '{ index : { number_of_shards : 1, number_of_replicas : 0 }}'
curl -XPUT 'http://localhost:9200/foo/bar/_mapping' -d '
{
  bar : { "_index" : { "enabled" : true } }
}'
curl -XPUT 'http://localhost:9200/foo/bar/1' -d '{"field1":"value1","field2":"1"}'
curl -XPUT 'http://localhost:9200/foo/bar/2' -d '{"field1":"value2","field2":"2"}'
curl -XPUT 'http://localhost:9200/foo/bar/3' -d '{"field1":"value3","field2":"3"}'
curl -XPOST 'http://localhost:9200/_refresh'
curl -XGET 'http://localhost:9200/foo/bar/_search?pretty=1' -d '
{
  "size" : 0,
  "query" : { "match_all":{}},
  "facets" : {
    "type" : {
      "terms" : { "field" : "_type"}
    },
    "index" : {
      "terms" : { "field" : "_index" }
    }
  }
}'
```
</comment><comment author="lukas-vlcek" created="2010-08-09T14:35:54Z" id="347094">Response I get:

```
{"ok":true,"acknowledged":true}
{"ok":true,"acknowledged":true}
{"ok":true,"acknowledged":true}
{"ok":true,"_index":"foo","_type":"bar","_id":"1"}
{"ok":true,"_index":"foo","_type":"bar","_id":"2"}
{"ok":true,"_index":"foo","_type":"bar","_id":"3"}
{"ok":true,"_shards":{"total":5,"successful":5,"failed":0}}
{
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "facets" : {
    "type" : {
      "_type" : "terms",
      "_field" : "_type",
      "terms" : [ {
        "term" : "bar",
        "count" : 3
      } ]
    },
    "index" : {
      "_type" : "terms",
      "_field" : "_index",
      "terms" : [ ]      // &lt;------ ??? How can I get results here
    }
  }
}
```
</comment><comment author="apatrida" created="2010-11-09T16:22:20Z" id="529249">Is this storing the index name in the doc, or just returning it in the results as-if it is in the doc?
</comment><comment author="kimchy" created="2010-11-09T16:57:45Z" id="529383">it stores the index name in the doc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>49 empty indexes with same field mappings won't start up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/291</link><project id="" key="" /><description>This is reproducible on 0.9 and 0.9.1 snapshot. I am using a single node.

Run this test file:
http://gist.github.com/505870

It's not quite runnable as is but can be easily executed with a few lines of your chosen scripting language. It consists of alternating path and JSON against the path. First creates 49 indexes and then applies a single mapping to all of them. 

After this is shutdown the cluster and bring it back up. After 30 seconds the the service responds with a green status. After about 15 more minutes it goes red with a shard count of zero and gets stuck there. The whole time the CPU is churning and it seems to be repeatedly applying the mappings.

I was able to reproduce with fewer indexes,  ~15, and this occurs 100% of the time.

My ES config file looks like:

gateway:
  type: fs
  fs:
    location: /data/elasticsearch/

index:
  analysis :
    analyzer :
      piped :
        type: pattern
        lowercase: true
        pattern: ||;
        stopwords: []

path:
  logs: /data/elasticsearch/logs/

Please let me know if there are any questions or need anything else.

Thanks!
Paul
</description><key id="267288">291</key><summary>49 empty indexes with same field mappings won't start up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-08-03T05:17:13Z</created><updated>2010-08-04T07:11:41Z</updated><resolved>2010-08-04T07:11:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-04T06:03:46Z" id="340483">I found a problem and fixed it at #295. It basically means that the mapping was parsed and re-added on each cluster change. I ran your test after the fix and things seem to be stable. If you get a chance, you can try master and see if it gets fixed for you?
</comment><comment author="ppearcy" created="2010-08-04T07:11:41Z" id="340547">Looks great. This speeds up my start up time for the config I wasn't having issues with by 5-10x, as well. Many thanks. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: bounding box filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/290</link><project id="" key="" /><description>A filter allowing to filter hits based on a point location using a bounding box. Assuming the following indexed document:

```
{
    "pin" : {
        "location" : {
            "lat" : 40.12,
            "lon" : -71.34
        }
    }
}
```

Then the following simple query can be executed with a `geo_bounding_box` filter:

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_bounding_box" : {
                "pin.location" : {
                    "top_left" : {
                        "lat" : 40.73,
                        "lon" : -74.1
                    },
                    "bottom_right" : {
                        "lat" : 40.717,
                        "lon" : -73.99
                    }
                }
            }
        }
    }
}
```
## Accepted Formats

In much the same way the geo_point type can accept different representation of the geo point, the filter can accept it as well:

_Lat Lon As Properties_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_bounding_box" : {
                "pin.location" : {
                    "top_left" : {
                        "lat" : 40.73,
                        "lon" : -74.1
                    },
                    "bottom_right" : {
                        "lat" : 40.717,
                        "lon" : -73.99
                    }
                }
            }
        }
    }
}
```

_Lat Lon As Array_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_bounding_box" : {
                "pin.location" : {
                    "top_left" : [40.73, -74.1],
                    "bottom_right" : [40.717, -73.99]
                }
            }
        }
    }
}
```

_Lat Lon As String_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_bounding_box" : {
                "pin.location" : {
                    "top_left" : "40.73, -74.1",
                    "bottom_right" : "40.717, -73.99"
                }
            }
        }
    }
}
```

_Geohash_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_bounding_box" : {
                "pin.location" : {
                    "top_left" : "drm3btev3e86",
                    "bottom_right" : "drm3btev3e86"
                }
            }
        }
    }
}
```
## `geo_point` Type

The filter does not require the `geo_point` type to be set. It assumes that the location object indexed includes a lat and lon numeric values. The `geo_point` type follows these rules and allows for more flexible indexing options / structure, but is not required.
## Multi Location Per Document

The filter can work with multiple locations / points per document. Once a single location / point matches the filter, the document will be included in the filter.
</description><key id="267102">290</key><summary>Geo: bounding box filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-02T22:50:59Z</created><updated>2010-08-03T16:26:12Z</updated><resolved>2010-08-03T05:51:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-02T22:51:59Z" id="338439">Geo: bounding box filter, closed by de8ffaf5fbe809f9738e24ac09b19063bdd2379c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Filter based facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/289</link><project id="" key="" /><description>Similar to the `query` facet, provide a `filter` facet that returns a count of the hits matching the filter. Faster than `query` facet.
</description><key id="266664">289</key><summary>Facets: Filter based facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-02T16:17:31Z</created><updated>2010-08-02T16:18:10Z</updated><resolved>2010-08-02T16:18:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-02T16:18:10Z" id="337835">Facets: Filter based facet, closed by dc6ef326d91993e3c6922e931f8383f2f238efd0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Rename the ability to filter a facet from `filter` to `facet_filter`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/288</link><project id="" key="" /><description>Mainly due to the upcoming `filter` facet type. Now, filtering a facet will look as follows:

```
{
    "facets" : {
        "wow_facet" : {
            "facet_type" : {
                ...
            },
            "facet_filter" : {
                "term" : { "user" : "kimchy"}
            }
        }
    }
}
```
</description><key id="266662">288</key><summary>Facets: Rename the ability to filter a facet from `filter` to `facet_filter`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.10.0</label></labels><created>2010-08-02T16:16:21Z</created><updated>2010-08-02T16:18:25Z</updated><resolved>2010-08-02T16:18:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-02T16:18:25Z" id="337836">applied.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Facets: Range Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/287</link><project id="" key="" /><description>`range` facet allow to specify a set of ranges and get both the number of docs (count) that fall within each range, and aggregated data either based on the field, or using another field. Here is a simple example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "range1" : {
            "range" : {
                "field" : "field_name",
                "ranges" : [
                    { "to" : 50 },
                    { "from" : 20, "to" : 70 }
                    { "from" : 70, "to" : 120 }
                    { "from" : 150 }
                ]
            }
        }
    }
}
```

Another option which is a bit more DSL enabled is to provide the ranges on the actual field name, for example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "range1" : {
            "range" : {
                "my_field" : [
                    { "to" : 50 },
                    { "from" : 20, "to" : 70 }
                    { "from" : 70, "to" : 120 }
                    { "from" : 150 }
                ]
            }
        }
    }
}
```
## Key and Value

The `range` facet allow to use a different field to check if it doc falls within a range, and another field to compute aggregated data per range (like total). For example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "range1" : {
            "range" : {
                "key_field" : "field_name",
                "value_field" : "another_field_name",
                "ranges" : [
                    { "to" : 50 },
                    { "from" : 20, "to" : 70 }
                    { "from" : 70, "to" : 120 }
                    { "from" : 150 }
                ]
            }
        }
    }
}
```
## Script Key and Value

Sometimes, some munging of both the key and the value are needed. In the key case, before it is checked if it falls within a range, and for the value, when the statistical data is computed per range scripts can be used. Here is an example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "range1" : {
            "range" : {
                "key_script" : "doc['date'].date.minuteOfHour",
                "value_script" : "doc['num1'].value",
                "ranges" : [
                    { "to" : 50 },
                    { "from" : 20, "to" : 70 }
                    { "from" : 70, "to" : 120 }
                    { "from" : 150 }
                ]
            }
        }
    }
}
```
## Date Ranges

The range facet support also providing the range as string formatted dates.
</description><key id="265857">287</key><summary>Search Facets: Range Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-01T17:07:48Z</created><updated>2013-02-23T10:45:33Z</updated><resolved>2010-08-02T00:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-01T17:08:37Z" id="336623">Search Facets: Range Facet, closed by ef861a6b7b97994567cd2a4cdd6ac14fb7c79d64.
</comment><comment author="EugeneLiang" created="2013-02-23T10:45:33Z" id="13988312">is there an example for range when it is a string formatted date ?

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: `geo_distance` facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/286</link><project id="" key="" /><description>The `geo_distance` facet is a facet providing information for ranges of distances from a provided `geo_point` including count of the number of hits that fall within each range, and aggregation information (like total). Assuming the following sample doc:

```
{
    "pin" : {
        "location" : {
            "lat" : 40.12,
            "lon" : -71.34
        }
    }
}
```

Here is an example that create a `geo_distance` facet from a `pin.location` of `40,-70`, and a set of ranges.

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : {
                    "lat" : 40,
                    "lon" : -70
                },
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```
## Accepted Formats

In much the same way the geo_point type can accept different representation of the geo point, the filter can accept it as well:

_Lat Lon As Properties_

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : {
                    "lat" : 40,
                    "lon" : -70
                },
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

_Lat Lon As Array_

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : [40, -70],
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

_Lat Lon As String_

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : "40, -70",
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

_Geohash_

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : "drm3btev3e86",
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```
## Ranges

When a `to` or `from` are not set, they are assumed to be unbounded. Ranges are allowed to overlap, basically, each range is treated by itself.
## Options
- `unit`: The unit the ranges are provided in. Defaults to `km`. Can also be `mi` or `miles`.
- `distance_type`: How to compute the distance. Can either be `arc` (better precision) or `plane` (faster). Defaults to `arc`.
## Value Options

On top of the count of hits falling within each range, aggregated data can be provided (total) as well. By default, the aggregated data will simply use the distance calculated, but the value can be extracted either using a different numeric field, or a script. Here is an example of using a different numeric field:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : "drm3btev3e86",
                "value_field" : "num1",
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

And here is an example of using a script:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "geo1" : {
            "geo_distance" : {
                "pin.location" : "drm3btev3e86",
                "value_script" : "doc['num1'].value * factor",
                "params" : {
                    "factor" : 5
                }
                "ranges" : [
                    { "to" : 10 },
                    { "from" : 10, "to" : 20 },
                    { "from" : 20, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

Note the params option, allowing to pass parameters to the script (resulting in faster script execution instead of providing the values within the script each time).
## `geo_point` Type

The facet does not require the `geo_point` type to be set. It assumes that the location object indexed includes a lat and lon numeric values. The `geo_poin`t type follows these rules and allows for more flexible indexing options / structure, but is not required.
## Multi Location Per Document

The facet can work with multiple locations per document.
</description><key id="265693">286</key><summary>Geo: `geo_distance` facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-08-01T10:30:40Z</created><updated>2010-08-01T17:08:37Z</updated><resolved>2010-08-01T17:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-01T17:08:37Z" id="336622">Geo: `geo_distance` facet, closed by b8b21a3363ef531efc6bd9fe9bf32d98a23212ae.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search fails when sorting on a field that has no values in the hits returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/285</link><project id="" key="" /><description>When executing a search with sorting on a specific string field, and that field has no values in all the hits matched / returned, search will fail.
</description><key id="265275">285</key><summary>Search fails when sorting on a field that has no values in the hits returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-07-31T13:40:42Z</created><updated>2010-07-31T13:41:21Z</updated><resolved>2010-07-31T13:41:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-31T13:41:21Z" id="335758">Search fails when sorting on a field that has no values in the hits returned, closed by dfb68c631046d054e047b78bfaa654a524870b74.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Wrong check for `config/mappings/[index_name]` causing failure to create index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/284</link><project id="" key="" /><description>When using the ability to add built in mappings by placing them under `config/mappings`, if the `[index_name]` directory does not exists, the creation of the index fails.
</description><key id="265263">284</key><summary>Mapper: Wrong check for `config/mappings/[index_name]` causing failure to create index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-07-31T12:43:43Z</created><updated>2010-07-31T12:43:56Z</updated><resolved>2010-07-31T12:43:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-31T12:43:56Z" id="335736">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ThreadLocals not cleared</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/283</link><project id="" key="" /><description>I am running ElasticSearch 0.9.0. My web application uses the Java API to connect to the server. After closing the client node, several API objects are still referenced by ThreadLocals. As a consequence the web application can not be garbage collected after being undeployed. This creates a memory leak.

```
30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap
SCHWERWIEGEND: The web application [] created a ThreadLocal with key of type [null] (value [org.elasticsearch.common.inject.InjectorImpl$1@19d44df]) and a value of type [java.lang.Object[]] (value [[Ljava.lang.Object;@149895]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.
30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap
SCHWERWIEGEND: The web application [] created a ThreadLocal with key of type [null] (value [org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom$1@2d8a59]) and a value of type [org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom] (value [org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom@1d60b6a]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.
30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap
SCHWERWIEGEND: The web application [] created a ThreadLocal with key of type [null] (value [org.elasticsearch.common.inject.InjectorImpl$1@19d44df]) and a value of type [java.lang.Object[]] (value [[Ljava.lang.Object;@1876a6d]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.
```
</description><key id="264498">283</key><summary>ThreadLocals not cleared</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">daspilker</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2010-07-30T13:19:17Z</created><updated>2016-04-22T12:06:02Z</updated><resolved>2016-04-22T12:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-04T10:40:36Z" id="340733">looks like a guice issue (I jarjar guice into elasticsearch)... . I will try and track it down.
</comment><comment author="daspilker" created="2010-10-20T09:33:38Z" id="480684">I can still reproduce the ThreadLocal leak in org.elasticsearch.common.inject.InjectorImpl with v0.11.0. I think they fixed this in the Guice trunk, so we have to wait for Guice 3.0.
</comment><comment author="alf" created="2012-09-26T14:02:59Z" id="8890753">What's the status of this? I'm on 0.19.9 and I see the above memory leak.
</comment><comment author="dadoonet" created="2012-09-26T14:10:57Z" id="8891038">AFAIK the issue is not fixed by the guice team :-(
</comment><comment author="janpalko" created="2013-05-07T08:44:37Z" id="17530416">Is there any progress on the bug?
</comment><comment author="glade-at-gigwell" created="2014-02-27T21:32:05Z" id="36293964">We see these messages every time we shutdown tomcat, even without load.  We are calling the Client.close() methods on all instances during the shutdown process in response to the container life cycle.  Is this issue still being worked on?
</comment><comment author="joeweoj" created="2014-10-24T11:47:42Z" id="60376667">Has any progress been made on this bug? the referenced bug https://github.com/elasticsearch/elasticsearch/issues/3386 has been closed in favour of this one
</comment><comment author="nandini-r" created="2015-07-31T05:47:46Z" id="126573233">Is this bug fixed ? I am getting the below memory leak log in tomcat  ::                                             SEVERE: The web application created a ThreadLocal with key of type [
org.elasticsearch.common.inject.InjectorImpl$1](value [org.elasticsearch.common
.inject.InjectorImpl$1@35ad2c3]) and a value of type [java.lang.Object[]](value
 [[Ljava.lang.Object;@566ace8c]) but failed to remove it when the web applicatio
n was stopped. Threads are going to be renewed over time to try and avoid a prob
able memory leak.
</comment><comment author="clintongormley" created="2015-08-05T10:49:39Z" id="127954016">@nandini-r what version are you on?
</comment><comment author="nandini-r" created="2015-08-13T03:39:33Z" id="130520588">@clintongormley Elastic search version i am using is 1.4.3 .  Sorry for the late reply
</comment><comment author="clintongormley" created="2015-08-13T10:22:48Z" id="130604498">@nandini-r  OK - lots has changed since then, I would upgrade and see if the problem still exists.  Please let us know either way.
</comment><comment author="nandini-r" created="2015-08-17T08:55:18Z" id="131735695">@clintongormley I tried with 1.7.1 and still the same log appears
</comment><comment author="jsmucr" created="2015-08-21T07:33:03Z" id="133315629">We're dealing with this issue as well. This pretty much spoils any attempts to hot swap new versions of our application automatically.
</comment><comment author="nik9000" created="2015-08-24T13:34:39Z" id="134202497">I've stolen this issue from @kimchy and added it to my queue of things to look it. Its not particularly high on that list but I'll get to it eventually. If anyone feels like working on it then please comment. Or just self assign if you have that power.
</comment><comment author="nandini-r" created="2015-08-25T06:43:07Z" id="134501609">@clintongormley  @nik9000  Wanted to mention this : Not sure if those threads are getting closed slowly because when I try adding delay of say 20-30 s , logs are not seen when I stop webapp . Though it is sometimes reproducible when I use 'reload' webapp option.
</comment><comment author="arosso" created="2015-08-28T18:37:56Z" id="135856313">+1 - This one causes us some major issues with continuous deploys to dev/qa/prd. What we've found with Java and PermGen, is that if there is even the smallest leak of an object, upon a reload _all_ classes will be leaked. Basically the classloader can't release the dependency tree of the web app. Even with a PermGen of 512Mb we cannot redeploy our web app more than once before things blow up. All other dependencies we have (and we have quite a bit) have various hooks to close/shutdown when the web app is unloaded. Sometimes we have to dig for them but there's usually a way.

I've seen various patches mentioned. Has anyone found a workaround while an official fix is being worked on?
</comment><comment author="arivazhagan-jeganathan" created="2016-01-04T11:08:54Z" id="168647870">Hi I am using elasticsearch 1.5.1 and Java 7. Upgrading to Java 8 is not an option right now since I am into production deployment.  Is there any workaround available till this issue gets fixed?  Or Is there any fix already available?
</comment><comment author="pengchengluo" created="2016-04-20T06:34:28Z" id="212280344">I still have the problem for elasticsearch 2.3.1

org.apache.catalina.loader.WebappClassLoaderBase.checkThreadLocalMapForLeaks The web application [ROOT] created a ThreadLocal with key of type [org.elasticsearch.common.inject.InjectorImpl$1](value [org.elasticsearch.common.inject.InjectorImpl$1@2f5768cb]) and a value of type [java.lang.Object[]](value [[Ljava.lang.Object;@51ceb57]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread not stopped after closing node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/282</link><project id="" key="" /><description>I am running ElasticSearch 0.9.0. My web application uses the Java API to connect to the server. After closing the client node, a thread started by the API is not stopped. As a consequence the web application can not be garbage collected after being undeployed. This creates a memory leak.

```
30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SCHWERWIEGEND: The web application [] appears to have started a thread named [org.elasticsearch.common.inject.internal.Finalizer] but has failed to stop it. This is very likely to create a memory leak. 
```
</description><key id="264491">282</key><summary>Thread not stopped after closing node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daspilker</reporter><labels /><created>2010-07-30T13:14:29Z</created><updated>2013-07-25T02:53:43Z</updated><resolved>2010-10-20T09:26:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-08-01T19:40:37Z" id="336722">Its in google collections (guava-library), there is an open issue about it: http://code.google.com/p/guava-libraries/issues/detail?id=92.
</comment><comment author="daspilker" created="2010-10-20T09:26:34Z" id="480676">I can not reproduce this with v0.11.0, so I guess the guava update fixed this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to query for null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/281</link><project id="" key="" /><description>Wanted to open a feature request based on this discussion:
http://elasticsearch-users.115913.n3.nabble.com/Query-for-null-td982648.html;cid=1280471258939-52#a982648

Using null_value is an acceptable hack, but doesn't fit the "Your Data, Your Search" mantra. 

Thanks!
</description><key id="264264">281</key><summary>Ability to query for null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-07-30T07:46:52Z</created><updated>2010-10-22T16:39:49Z</updated><resolved>2010-10-22T16:39:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2010-07-30T22:57:07Z" id="335405">Actually, I have discovered a syntax that works for this case:
(-companynames:[\* TO *])

Not the cleanest, but does not require any data munging. 

Based on this, feel free to reject this request, although a cleaner query would be nice. 
</comment><comment author="ppearcy" created="2010-07-30T22:57:47Z" id="335409">Hmmm... github took out my star characters. The range is [STAR TO STAR]
</comment><comment author="ppearcy" created="2010-10-22T16:39:49Z" id="486339">Looks like this was added under a different ticket. Thanks!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put Mapping: When using a single node and updating a mapping, it is not marked as `acknowledged`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/280</link><project id="" key="" /><description /><key id="263340">280</key><summary>Put Mapping: When using a single node and updating a mapping, it is not marked as `acknowledged`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.10.0</label></labels><created>2010-07-29T12:20:43Z</created><updated>2010-07-29T16:22:45Z</updated><resolved>2010-07-29T16:22:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-29T16:22:45Z" id="333420">Put Mapping: When using a single node and updating a mapping, it is not marked as `acknowledged`, closed by 4f407e18aa3f4cdf9da3085af8d6cf0ad2fc4ae9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Filter by distance based on location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/279</link><project id="" key="" /><description>The ability to define a query DSL filter, allowing to include only hits that exists within a specific distance from a geo point. Assuming the following indexed json:

```
{
    "pin" : {
        "location" : {
            "lat" : 40.12,
            "lon" : -71.34
        }
    }
}
```

Then the following simple query can be executed with a `geo_distance` filter:

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance" : {
                "distance" : "12km"
                "pin.location" : {
                    "lat" : 40,
                    "lon" : -70
                }
            }
        }
    }
}
```
## Accepted Formats

In much the same way the `geo_point` type can accept different representation of the geo point, the filter can accept it as well:

_Lat Lon As Properties_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance" : {
                "distance" : "12km"
                "pin.location" : {
                    "lat" : 40,
                    "lon" : -70
                }
            }
        }
    }
}
```

_Lat Lon As Array_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance" : {
                "distance" : "12km"
                "pin.location" : [40, -70]
            }
        }
    }
}
```

_Lat Lon As String_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance" : {
                "distance" : "12km"
                "pin.location" : "40,-70"
            }
        }
    }
}
```

_Geohash_

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance" : {
                "distance" : "12km"
                "pin.location" : "drm3btev3e86"
            }
        }
    }
}
```
## Options

The following are options allowed on the filter:
- `distance`: The distance to include hits in the filter. The distance can be a numeric value, and then the `distance_unit` (either `mi`/`miles` or `km` can be set) controlling the unit. Or a single string with the unit as well.
- `distance_type`: How to compute the distance. Can either be `arc` (better precision) or `plane` (faster). Defaults to `arc`.
## `geo_point` Type

The filter does not require the `geo_point` type to be set. It assumes that the location object indexed includes a `lat` and `lon` numeric values. The `geo_point` type follows these rules and allows for more flexible indexing options / structure, but is not required.
## Multi Location Per Document

The `geo_distance` filter can work with multiple locations / points per document. Once a single location / point matches the filter, the document will be included in the filter.
</description><key id="262213">279</key><summary>Geo: Filter by distance based on location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-07-28T17:45:38Z</created><updated>2016-12-06T05:43:52Z</updated><resolved>2010-07-28T18:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-28T18:13:56Z" id="331100">implemented.
</comment><comment author="mikenereson" created="2013-01-07T17:55:49Z" id="11962459">In examples, missing `,` after  `"distance" : "12km"`
</comment><comment author="gauraangkhurana" created="2016-12-06T05:31:34Z" id="265064743">What goes before this json request ? 

I tried this 'geo_distance' filter query about 20 times but it gives me this error.

My Code : 

```
`curl` -XGET localhost:9200/resume/_search -d  ' { {
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance" : {
                "distance" : "12km"
                "pin.location" : {
                    "lat" : 40,
                    "lon" : -70
                }
            }
        }
    }
}
```

The error produced is 

{"error":{"root_cause":[{"type":"parsing_exception","reason":"Unknown key for a START_OBJECT in [filtered].","line":1,"col":19}],"type":"parsing_exception","reason":"Unknown key for a START_OBJECT in [filtered].","line":1,"col":19},"status":400}


Kindly help me.</comment><comment author="dadoonet" created="2016-12-06T05:43:52Z" id="265066143">Ask questions on discuss.elastic.co

Filtered query has been removed in 5.0. See breaking changes in doc</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Support `geo_point` type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/278</link><project id="" key="" /><description>Mapper type called `geo_point` to support geo based points. The declaration looks as follows:

```
{
    "pin" : {
        "properties" : {
            "location" : {
                "type" : "geo_point"
            }
        }
    }
}
```
## Input Structure

The above mapping defines a `geo_point`, which accepts different formats. The following formats are supported:

_Lat Lon As Properties_

```
{
    "pin" : {
        "location" : {
            "lat" : 40.12,
            "lon" : -71.34
        }
    }
}
```

_Lat Lon As Array_

```
{
    "pin" : {
        "location" : [40.12, -71.34]
    }
}
```

_Lat Lon As String_

```
{
    "pin" : {
        "location" : "41.12,-71.34"
    }
}
```

_Geohash_

```
{
    "pin" : {
        "location" : "drm3btev3e86"
    }
}
```
## Index Fields

The `geo_point` type creates two floating point fields, called `lat` and `lon`. In our example above, two fields will be create, the first `location.lat` and the second is `location.lon`. Though probably not needed, there is an option also to create a third field called `geohash` with the `geohash` value (represented as `location.geohash`). In most cases, those specific fields are not needed when using geo capabilities, and only `location` will be needed (for example, with `geo_distance` filter).
## Mapping Options
- `resolution`: controls the floating point resolution of the stored fields. Defaults to `64`, but can also be `32`.
- `geohash`: set to `true` to store also the geohash in the index. defaults to `false`.
- `geohash_precision`: sets the geohash precision, defaults to `12`.
</description><key id="261963">278</key><summary>Geo: Support `geo_point` type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.10.0</label></labels><created>2010-07-28T13:48:23Z</created><updated>2011-01-31T05:35:12Z</updated><resolved>2010-07-29T01:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-28T18:06:30Z" id="331081">Geo support: geo_point type and geo_distance filter, closed by 194e6cbff60899a00db08798924f60764882e9b7, #279
</comment><comment author="decitre" created="2011-01-29T16:32:01Z" id="720882">I have a GeoCouch feeding an es index through _changes.
Documents are compatible with geojson spec.
Is there a way to extend geo_point as a Lon Lat As Array ?
</comment><comment author="kimchy" created="2011-01-30T21:18:32Z" id="723157">Can you post a sample format?
</comment><comment author="decitre" created="2011-01-30T21:45:41Z" id="723226">In the following example, the geo coordinates are provided for both ES ("location" member) and GeoJSON ("coordinates" member):

```
{
  "_id": "p996602367",
  "_rev": "1-2b5e6ba6707a19c9bf0a00a9da67cda8",
  "type": "Feature",
  "geometry": {
      "type": "Point",
      "coordinates": [
          7.637420177459717,
          47.80836868286133
      ]
  },
  "children": {},
  "feature_code": 7312,
  "parents": {
      "1016": [
          "a20174512"
      ],
      "1022": [
          "l67331525"
      ]
  },
  "location": {
      "lat": 47.80836868286133,
      "lon": 7.637420177459717
  },
  "properties": {
      "attributes": {
          "11": {
              "value": "WERDERSTRASSE",
              "language": "GER"
          },
          "10": "77",
          "ON": {
              "value": "BUDGET",
              "language": "GER"
          },
          "BN": {
              "value": "BUDGET",
              "language": "GER"
          },
          "%C": "443",
          "TL": {
              "value": "+(49)-(7631)-foobar",
              "language": "GER"
          }
      }
  }
}
```
</comment><comment author="kimchy" created="2011-01-30T22:31:23Z" id="723327">I have just mailed the mailing list to ask if its ok to make this change in ES (breaks backward comp.). If nobody objects too greatly, I can change everywhere to use the array format defined in GeoJSON (that includes queries and facets as well).
</comment><comment author="decitre" created="2011-01-31T05:35:12Z" id="723995">That's very "agile" !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Factes: Allow to provide regex controlling which terms should be included</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/277</link><project id="" key="" /><description>Allow to define a regex to control which terms should be included. The `regex` parameter controls the actual regular expression. The `regex_flags` control the regular expression.
</description><key id="259612">277</key><summary>Terms Factes: Allow to provide regex controlling which terms should be included</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-26T09:22:33Z</created><updated>2010-07-26T18:54:02Z</updated><resolved>2010-07-26T18:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-26T18:54:02Z" id="327731">Terms Factes: Allow to provide regex controlling which terms should be included, closed by 65284ba2baf17de84483bee5760c0085833442d3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add pattern analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/276</link><project id="" key="" /><description>Pattern analyzer uses regular expressions to separate text into tokens. It can act as a combination of the `letter`, `lower_case`, `whitespace`, and `stop` filter.

Sample configuration can be:

```
 index:
    analysis:
        analyzer:
            my_analyzer:
                type: pattern
                lowercase: true
                pattern: \W+
                flags: DOTALL|MULTILINE
                stopwords: ["something", "else"]
```
</description><key id="259164">276</key><summary>Analysis: Add pattern analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-25T19:40:10Z</created><updated>2010-07-25T19:40:58Z</updated><resolved>2010-07-25T19:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-25T19:40:58Z" id="326226">Analysis: Add pattern analyzer, closed by ac7c8cb650935f5b67343b28a88af8ea8db07467.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Revise dynamic mapping (into default), merge default to new mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/275</link><project id="" key="" /><description>The breaking changes are the configuration for default (previously dynamic) mapping location and names. `dynamic-mapping.json` renamed to `default-mapping.json`, and the setting `index.mapper.dynamic_mapping_location` renamed to `index.mapper.default_mapping_location`.

The feature include automatic inclusion of the default mapping in any other mappings, with the actual mapping overriding whatever is set in the default mapping. This means that default mapping definitions can be set and will be applied to any mappings, either fully dynamically created ones, or explicit ones.
</description><key id="258962">275</key><summary>Mapping: Revise dynamic mapping (into default), merge default to new mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.09.0</label></labels><created>2010-07-25T13:59:10Z</created><updated>2010-07-25T18:31:30Z</updated><resolved>2010-07-25T18:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-25T18:31:30Z" id="326136">Mapping: Revise dynamic mapping (into default), merge default to new mappings, closed by 477a24efc6045e36d5ffc5db2da50e9f72f547c0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Dynamic mapping definitions are ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/274</link><project id="" key="" /><description /><key id="258860">274</key><summary>Mapping: Dynamic mapping definitions are ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-07-25T09:02:32Z</created><updated>2010-07-25T18:31:30Z</updated><resolved>2010-07-25T18:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-25T18:31:30Z" id="326135">Mapping: Dynamic mapping definitions are ignored, closed by 1884c4219a5cc92eef2e8db947f51249972a3bf4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud Plugin: Remove (replaced with AWS specific cloud plugin)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/273</link><project id="" key="" /><description>Other cloud providers will come later on...
</description><key id="258536">273</key><summary>Cloud Plugin: Remove (replaced with AWS specific cloud plugin)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.09.0</label></labels><created>2010-07-24T20:40:49Z</created><updated>2010-07-24T20:41:03Z</updated><resolved>2010-07-24T20:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-24T20:41:03Z" id="325290">removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermsRequest#regexp(String) does not return this</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/272</link><project id="" key="" /><description>It looks like this

```
public void regexp(String regexp) {
    this.regexp = regexp;
}
```

which prevents chaining the calls like with

```
public TermsRequest prefix(String prefix) {
    this.prefix = prefix;
    return this;
}
```
</description><key id="257380">272</key><summary>TermsRequest#regexp(String) does not return this</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cpesch</reporter><labels /><created>2010-07-23T11:20:13Z</created><updated>2010-08-15T19:22:00Z</updated><resolved>2010-08-15T19:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-23T15:00:21Z" id="324085">terms request has been removed in 0.9 (the terms facet in search replaces it).
</comment><comment author="kimchy" created="2010-08-15T19:21:58Z" id="355965">closing this since the terms API was removed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST Search API: Change `score` to `_score` to denote sorting by hit score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/271</link><project id="" key="" /><description /><key id="256775">271</key><summary>REST Search API: Change `score` to `_score` to denote sorting by hit score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.09.0</label></labels><created>2010-07-22T18:23:13Z</created><updated>2010-07-22T20:40:01Z</updated><resolved>2010-07-22T20:40:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-22T20:40:01Z" id="323159">REST Search API: Change `score` to `_score` to denote sorting by hit score, closed by 2a3130c649b832a056d85f7defed52ac4901c284.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Sending a request that fails to parse can cause file leaks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/270</link><project id="" key="" /><description>The search context is being released, but too late in the game and the file leaks are aggregated. The context should be when the failure happens.
</description><key id="255423">270</key><summary>Search: Sending a request that fails to parse can cause file leaks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-07-21T13:58:17Z</created><updated>2010-07-21T16:55:22Z</updated><resolved>2010-07-21T16:55:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-21T16:55:22Z" id="321267">Search: Sending a request that fails to parse can cause file leaks, closed by 8ec7ee66a9ca266bf0d961c28f67d121e37dc389.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Health API: Add `wait_for_nodes` (accepts "N", "&lt;N", "&gt;N", "&lt;=N", and "&gt;=N")</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/269</link><project id="" key="" /><description /><key id="255376">269</key><summary>Cluster Health API: Add `wait_for_nodes` (accepts "N", "&lt;N", "&gt;N", "&lt;=N", and "&gt;=N")</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-21T13:11:49Z</created><updated>2010-07-21T16:55:22Z</updated><resolved>2010-07-21T16:55:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-21T16:55:22Z" id="321266">Cluster Health API: Add `wait_for_nodes` (accepts "N", "&lt;N", "&gt;N", "&lt;=N", and "&gt;=N"), closed by 48d33ec70a8ea6f872006e46d1c8e0cdd48a2c81.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fields param not documented in search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/268</link><project id="" key="" /><description>In http://www.elasticsearch.com/docs/elasticsearch/rest_api/query_dsl/query_string_query/ you give an example using the `fields` parameter, but don't explain how it is used and how it differs from `default_field`
</description><key id="252568">268</key><summary>fields param not documented in search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-19T11:51:52Z</created><updated>2010-07-19T13:26:23Z</updated><resolved>2010-07-19T13:26:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-19T12:14:23Z" id="317782">Isn't it documented? There is a whole section about running against multiple fields, and the options you can set when running with it.
</comment><comment author="clintongormley" created="2010-07-19T12:20:10Z" id="317795">Where? it's not on that page, which is where I would expect to see it.
</comment><comment author="kimchy" created="2010-07-19T12:58:53Z" id="317833">Starting from "The query_string query can also run against multiple fields...".
</comment><comment author="clintongormley" created="2010-07-19T13:26:23Z" id="317854">True :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Doc error in span_first</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/267</link><project id="" key="" /><description>The docs for `span_first` are incorrect - the second `span_first` should be `span_term`:

http://www.elasticsearch.com/docs/elasticsearch/rest_api/query_dsl/span_first_query/

as in:

```
{
    "span_first" : {
        "match" : {
            "span_term" : { "user" : "kimchy" }
        },
        "end" : 3
    }
}    
```
</description><key id="251419">267</key><summary>Doc error in span_first</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-17T14:57:51Z</created><updated>2010-07-17T16:32:28Z</updated><resolved>2010-07-17T16:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-17T16:32:27Z" id="316274">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_score and max_score are null when a sort order is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/266</link><project id="" key="" /><description>When any `sort` order is used in a search, then `_score` and `max_score` are `null`, even when the `sort` is the default `{"score": "desc"}`

```
curl -XGET 'http://127.0.0.1:9200/_all/_search'  -d '
{
   "fields" : [],
   "query" : {
      "match_all" : {}
   },
   "size" : 1
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "_index" : "iannounce_object_1279359740",
#             "_id" : "2838632",
#             "_type" : "group"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 14703
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }



curl -XGET 'http://127.0.0.1:9200/_all/_search'  -d '
{
   "sort" : [
      {
         "score" : "desc"
      }
   ],
   "fields" : [],
   "query" : {
      "match_all" : {}
   },
   "size" : 1
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : null,
#             "_index" : "iannounce_object_1279359740",
#             "_id" : "2838632",
#             "_type" : "group"
#          }
#       ],
#       "max_score" : null,
#       "total" : 14703
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```
</description><key id="251411">266</key><summary>_score and max_score are null when a sort order is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-17T14:14:47Z</created><updated>2011-03-21T23:53:15Z</updated><resolved>2011-03-21T23:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-19T18:49:48Z" id="318379">Fixed the part with just score and "desc", all others will still not return the score itself, it requires a bit more work.
</comment><comment author="kimchy" created="2011-03-21T23:53:15Z" id="900915">You can now pass track_scores in the search request, and it will compute the scores even when sorting (at the expense of more processing needed).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>restart whole cluster is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/265</link><project id="" key="" /><description>Hiya

I started two nodes with `./bin/elasticsearch` and once settled, issued a:

```
curl -XPOST 'http://127.0.0.1:9200/_cluster/nodes/_restart'
```

The nodes restart, but no node is made master, so they are essentially zombies.  See the logs here:
http://gist.github.com/479454
</description><key id="251369">265</key><summary>restart whole cluster is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-17T11:48:50Z</created><updated>2013-11-22T10:09:02Z</updated><resolved>2011-03-20T13:52:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-03-20T13:52:16Z" id="895182">restart currently disabled in master
</comment><comment author="btiernay" created="2013-07-03T17:18:47Z" id="20430975">@clintongormley: Just curious why this was disabled. Was there a technical reason? Will it be reenabled in the future? Seems to be a useful feature :)
</comment><comment author="clintongormley" created="2013-07-03T17:59:45Z" id="20433727">I can confidently say that there was, indeed, a technical reason, but I'm not sure what it was :)  I think there were problems with restarting the node reliably, not sure.
</comment><comment author="faxm0dem" created="2013-11-22T09:59:34Z" id="29061368">what's the right way to restart/reload a node then?
</comment><comment author="dadoonet" created="2013-11-22T10:09:02Z" id="29061919">@faxm0dem If you are using linux distribution for example, you can run something like:

``` sh
sudo service elasticsearch restart
```

On windows platform, you can use the Services tool and restart from here.

If you have started elasticsearch with `bin/elasticsearch -f`, just `CTRL+C` to stop the node and restart it again with the same command.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AWS Cloud Plugin: Implement AWS specific cloud plugin with S3 gateway and EC2 discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/264</link><project id="" key="" /><description>Replacing the previous cloud plugin, implements an AWS specific cloud plugin that provides both S3 gateway and EC2 discovery.
## Installation

Installing the cloud plugin (once `0.9` is out) can be done using: `bin/plugin -install cloud-aws`.

h2. Generic Settings

A generic configuration of the AWS that includes the account and key can be set using:

```
cloud:
  aws:
    access_key: XXXX
    secret_key: YYYY
```
## S3 Gateway

The S3 gateway is configured using:

```
gateway:
 type: s3
 s3:
   bucket: bucket_name_goes_here
```

The `s3` gateway allows for the following settings:
- `bucket`: The bucket name, required.
- `region`: The region, defaults to `us-standard`. Can be one of `us-west`, `EU`, and `ap-southeast-1`.
## EC2 Discovery

The EC2 based discovery can be set using:

```
discovery:
  type: ec2
```

The `ec2` discovery allows for the following settings:
- `host_type`: The host that will be used to connect to other instances. Can be `private_ip`, `public_ip`, `private_dns`, and `public_dns`. Defaults to `private_dns`.
- `groups`: A set of security groups (either as array, or comma separated) to filter instances to connect to. Defaults to no filtering at all.
- `availability_zones`: A set of availability zones (either as array, or comma separated) to filter instances to connect to. Defaults to no filtering at all.
</description><key id="251111">264</key><summary>AWS Cloud Plugin: Implement AWS specific cloud plugin with S3 gateway and EC2 discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-07-16T22:41:11Z</created><updated>2010-07-24T20:58:40Z</updated><resolved>2010-07-24T20:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-24T20:58:40Z" id="325301">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>max_score == NaN</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/263</link><project id="" key="" /><description>Hiya

I was incorrect about `NaN` being valid JSON. Apparently:

```
Finite numbers are stringified as if by String(number). NaN and Infinity regardless of sign are represented as the string null.
```

So for situations when there IS no `max_score`, I'd suggest using zero instead of `null` or `"NaN"`

clint
</description><key id="250493">263</key><summary>max_score == NaN</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-16T09:12:32Z</created><updated>2010-07-17T10:52:57Z</updated><resolved>2010-07-17T05:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-16T11:22:21Z" id="314860">zero is problematic, since it does not indicate a "not valid" max_score, but maybe with a disclaimer that states that you need to check total hits first?
</comment><comment author="clintongormley" created="2010-07-16T11:31:56Z" id="314873">What would be a "not valid" max_score?  You could return `null` but I can't see why that would be useful.
</comment><comment author="kimchy" created="2010-07-16T18:09:46Z" id="315372">Thats what I meant, there isn't really a non valid score (especially with custom scoring now) that I can return to indicate the fact that there isn't a score associated with this result.
</comment><comment author="kimchy" created="2010-07-16T22:38:51Z" id="315695">Going to return a null value for the `max_score` in such cases, will push shortly.
</comment><comment author="kimchy" created="2010-07-16T22:39:18Z" id="315696">max_score == NaN, closed by e61dc78c21a706ff997bbccd851a2ff88907d5af.
</comment><comment author="clintongormley" created="2010-07-17T10:45:58Z" id="316050">`_score` should probably also be `null` rather than `NaN`
</comment><comment author="kimchy" created="2010-07-17T10:52:57Z" id="316053">yea, fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put Mapping: When updating existing mappings, the request returns with acknowledged `false`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/262</link><project id="" key="" /><description>It will hang for the full timeout (`10s`). Note, the actual update of the mapping works, its just the acknowledge part was misbehaving. 
</description><key id="249455">262</key><summary>Put Mapping: When updating existing mappings, the request returns with acknowledged `false`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-07-15T17:03:18Z</created><updated>2010-07-15T17:03:52Z</updated><resolved>2010-07-15T17:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-15T17:03:52Z" id="313766">Put Mapping: When updating existing mappings, the request returns with acknowledged `false`, closed by cfa56cb3d60ba02930759d0be984340a60e6962d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Querying mapping on a non-master throws an error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/261</link><project id="" key="" /><description>Start three local nodes, and run this script: http://gist.github.com/477132

Note: don't change the :9201 to :9200

You will see that the GET _mapping request against :9200 works, but the one against :9201 fails
</description><key id="249390">261</key><summary>Querying mapping on a non-master throws an error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-07-15T15:51:31Z</created><updated>2010-07-15T16:03:55Z</updated><resolved>2010-07-15T16:03:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-15T16:03:54Z" id="313677">Querying mapping on a non-master throws an error, closed by c987e8a534c25873a1d52c5c5714fd14adbfac6c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog: Implement a file system based translog and make it the default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/260</link><project id="" key="" /><description>Currently, there is an on going in memory translog (per shard) holding all the operations done between flushes. There is a memory monitor to control if there are memory problems and then do a flush to clean it (on the most expensive shards) as well as auto flush each N (5000) number of operations. Still, this memory can be of good use other than the translog.

A file system based translog will hold the changes on a file. There is no need to flush / fsync it since its not really used for full shutdown recovery (the gateway is there for that) so it should be fast enough.

It should be used as the default as well for better out of the box experience.
</description><key id="247092">260</key><summary>Translog: Implement a file system based translog and make it the default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-07-14T11:53:35Z</created><updated>2010-07-14T17:49:17Z</updated><resolved>2010-07-14T17:49:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T17:49:17Z" id="312240">Translog: Implement a file system based translog and make it the default, closed by 95ba62f83dfa05990d2165484330cdd0792064d8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet results vary depending on size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/259</link><project id="" key="" /><description>Run this to setup your docs:

```
curl -XPUT 'http://127.0.0.1:9200/es_test_1/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/_all/type_1/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/_all/type_2/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XGET 'http://127.0.0.1:9200/_cluster/health?timeout=5s&amp;wait_for_status=green' 
# {
#    "active_primary_shards" : 10,
#    "timed_out" : false,
#    "relocating_shards" : 0,
#    "active_shards" : 20,
#    "status" : "green"
# }


curl -XPOST 'http://127.0.0.1:9200/_refresh' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 20,
#       "total" : 20
#    }
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : "2",
   "date" : "2010-04-2 00:00:00",
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "1",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : "3",
   "date" : "2010-04-3 00:00:00",
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "2",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : "4",
   "date" : "2010-04-4 00:00:00",
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "3",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : "5",
   "date" : "2010-04-5 00:00:00",
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "4",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : "6",
   "date" : "2010-04-6 00:00:00",
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "5",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : "7",
   "date" : "2010-04-7 00:00:00",
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "6",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : "8",
   "date" : "2010-04-8 00:00:00",
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "7",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : "9",
   "date" : "2010-04-9 00:00:00",
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "8",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : "10",
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "9",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : "11",
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "10",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : "12",
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "11",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : "13",
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "12",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : "14",
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "13",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : "15",
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "14",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : "16",
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "15",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : "17",
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "16",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : "18",
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "17",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : "19",
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "18",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : "20",
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "19",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : "21",
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "20",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : "22",
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "21",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : "23",
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "22",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : "24",
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "23",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : "25",
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "24",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : "26",
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "25",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : "27",
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "26",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : "28",
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "27",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : "29",
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "28",
#    "_type" : "type_2"
# }


curl -XGET 'http://127.0.0.1:9200/_cluster/health?timeout=5s&amp;wait_for_status=green' 
# {
#    "active_primary_shards" : 10,
#    "timed_out" : false,
#    "relocating_shards" : 0,
#    "active_shards" : 20,
#    "status" : "green"
# }


curl -XPOST 'http://127.0.0.1:9200/_refresh' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 20,
#       "total" : 20
#    }
# }
```

Then check the facet results when varying the `size` parameter:

```
curl -XGET 'http://127.0.0.1:9200/_all/_search'  -d '
{
   "query" : {
      "match_all" : {}
   },
   "facets" : {
      "foo" : {
         "terms" : {
            "size" : 3,
            "field" : "text"
         }
      }
   },
   "size" : 0
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : "NaN",
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    },
#    "facets" : {
#       "foo" : {
#          "_field" : "text",
#          "terms" : [
#             {
#                "count" : 16,
#                "term" : "foo"
#             },
#             {
#                "count" : 16,
#                "term" : "baz"
#             },
#             {
#                "count" : 16,
#                "term" : "bar"
#             }
#          ],
#          "_type" : "terms"
#       }
#    }
# }

curl -XGET 'http://127.0.0.1:9200/_all/_search'  -d '
{
   "query" : {
      "match_all" : {}
   },
   "facets" : {
      "foo" : {
         "terms" : {
            "size" : 2,
            "field" : "text"
         }
      }
   },
   "size" : 0
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : "NaN",
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    },
#    "facets" : {
#       "foo" : {
#          "_field" : "text",
#          "terms" : [
#             {
#                "count" : 16,
#                "term" : "foo"
#             },
#             {
#                "count" : 13,
#                "term" : "baz"
#             }
#          ],
#          "_type" : "terms"
#       }
#    }
# }



curl -XGET 'http://127.0.0.1:9200/_all/_search'  -d '
{
   "query" : {
      "match_all" : {}
   },
   "facets" : {
      "foo" : {
         "terms" : {
            "size" : 1,
            "field" : "text"
         }
      }
   },
   "size" : 0
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : "NaN",
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    },
#    "facets" : {
#       "foo" : {
#          "_field" : "text",
#          "terms" : [
#             {
#                "count" : 8,
#                "term" : "foo"
#             }
#          ],
#          "_type" : "terms"
#       }
#    }
# }
```
</description><key id="246570">259</key><summary>Facet results vary depending on size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-13T21:10:40Z</created><updated>2010-07-14T17:49:17Z</updated><resolved>2010-07-14T17:49:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T08:29:37Z" id="311479">Ahh, the joy of distributed systems. I am aware of this problem. Let me try and push the fix for this... . Basically, in typical systems, you would get better term distributions and won't have this problem.
</comment><comment author="kimchy" created="2010-07-14T17:49:17Z" id="312239">Facet results vary depending on size, closed by a6bd64f30db2f1a5a4c1c64a98de35f6fb72019c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refreshing after creating or deleting an index throws an error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/258</link><project id="" key="" /><description>Hiya

I start 3 nodes, create an index, then call refresh.  Depending on timing it either succeeds or fails with an internal server error:

```
curl -XPUT 'http://127.0.0.1:9200/es_test_2/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPOST 'http://127.0.0.1:9200/_refresh' 
# {
#    "error" : "IndexMissingException[[es_test_2] missing]"
# }
```
</description><key id="246555">258</key><summary>Refreshing after creating or deleting an index throws an error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-13T21:00:46Z</created><updated>2010-07-15T15:42:43Z</updated><resolved>2010-07-15T15:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-07-13T21:03:46Z" id="310819">Here is another example - if the index isn't quite ready, ES throws an error:

```
curl -XPUT 'http://127.0.0.1:9200/es_test_1/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_2/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XGET 'http://127.0.0.1:9200/_cluster/health?timeout=5s&amp;wait_for_status=green' 
# {
#    "active_primary_shards" : 10,
#    "timed_out" : false,
#    "relocating_shards" : 0,
#    "active_shards" : 20,
#    "status" : "green"
# }


curl -XPOST 'http://127.0.0.1:9200/_refresh' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 20,
#       "total" : 20
#    }
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1,es_test_2/test/_mapping'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XGET 'http://127.0.0.1:9200/_cluster/health?timeout=5s&amp;wait_for_status=green' 
# {
#    "active_primary_shards" : 10,
#    "timed_out" : false,
#    "relocating_shards" : 0,
#    "active_shards" : 20,
#    "status" : "green"
# }


curl -XPOST 'http://127.0.0.1:9200/_refresh' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 20,
#       "total" : 20
#    }
# }


curl -XGET 'http://127.0.0.1:9200/es_test_1/test/_mapping' 
# {
#    "error" : "RemoteTransportException[[Ironclad][inet[/127.0.0.1
# &gt;    :9300]][/cluster/state]]; nested: "
# }
```
</comment><comment author="kimchy" created="2010-07-14T20:52:06Z" id="312577">I can't recreate both issues (tried running it several times). Have you tried latest master?
</comment><comment author="clintongormley" created="2010-07-15T15:42:43Z" id="313647">This wasn't actually the problem - opening a new issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refreshing after deleting an index throws an error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/257</link><project id="" key="" /><description>Hiya
</description><key id="246527">257</key><summary>Refreshing after deleting an index throws an error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-13T20:42:08Z</created><updated>2010-07-15T14:40:18Z</updated><resolved>2010-07-15T14:40:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T20:16:58Z" id="312507">Anything else?
</comment><comment author="clintongormley" created="2010-07-15T14:40:17Z" id="313552">Pressed Enter too soon :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field Collapsing/Combining</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/256</link><project id="" key="" /><description>Ability to collapse on a field. For example, I want the most relevant result from all different report types. Or similarly, the most recent result of each report type. Or maybe, I want to de-dup on headline. 

So, the sort order would dictate which one from the group is returned. Similar to what is discussed here:
http://blog.jteam.nl/2009/10/20/result-grouping-field-collapsing-with-solr/

From my understanding, it seems that in order for field collapsing to be efficient, the result set must be relatively small.

This is also referred to as "Combine" on some other search products. 
</description><key id="246300">256</key><summary>Field Collapsing/Combining</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-07-13T17:02:32Z</created><updated>2015-06-14T01:24:50Z</updated><resolved>2014-05-23T14:08:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Omega359" created="2010-08-13T19:51:42Z" id="354160">Count this comment as a vote to have this feature added.
</comment><comment author="kwloafman" created="2010-09-05T17:46:50Z" id="389793">I could make good use of this feature.  Go for it!
</comment><comment author="Fiedzia" created="2010-09-30T11:12:46Z" id="439111">+1 vote for that
</comment><comment author="ekalyoncu" created="2010-10-29T13:02:34Z" id="502015">yes it's really cool feature.
</comment><comment author="ekalyoncu" created="2010-10-29T13:04:16Z" id="502020">In SOLR, grouping is not supported for distributed search. If it's implemented, it can be big plus for ElasticSearch
</comment><comment author="giorgiovinci" created="2010-10-29T13:04:30Z" id="502021">The only workaround is to "group" the results on the client side is correct?
+1 For this. To have the logic on the server is what we need!
</comment><comment author="jeroenr" created="2010-11-02T13:00:18Z" id="511558">+1 This sounds really useful
</comment><comment author="apatrida" created="2010-11-09T14:42:48Z" id="528989">This is probably a broader topic of collapsing (dropping dupes based on sort order although many times one field isn't enough to decide a good dedupe), or full rollups where you retain the individual documents within an aggregate replacement document ("5 books by this author").  

There are fun issues with each, such as do you try to satisfy the requested window results?  How does paging work when things are missing?  Does the total document count get adjusted (but is still wrong as you don't know what other pages hold)?  ...
</comment><comment author="Fiedzia" created="2010-11-09T15:23:05Z" id="529079">For me this should work like "select distinct" in sql - so i expect duplicates to be removed everywhere - including total document count, pagination and window result. 
</comment><comment author="apatrida" created="2010-11-09T15:37:17Z" id="529122">at that point, its a full group-by and in SQL you are getting aggregate values back in functions, and sometimes undefined if you ask for non-aggregate fields ... in the search engine how are the other fields besides the rollup key being treated?  Is it a grouping into a master aggregate document listing all the children, or at least the fact that there are children such as what Endeca does?  Of is it a deduping and the first one at highest relevancy wins even if many of the other fields differ outside of the key (you need compound keys then as deduping on a single field isn't enough to make that desirable)?
</comment><comment author="ppearcy" created="2010-12-14T20:38:26Z" id="611648">Hey,
  Just wanted to say that we are using our own poor man's version of this to satisfy some requirements by just requesting 10x the amount requested and collapsing down client side. Complete hack, but works 99% of the time.

We're now applying this and adding facets to it with a two phased approach. We first get the list of doc ids and then we pass them in as a term list and faceting on that query. 

Was curious if there was any more efficient method of doing this? 

Thanks,
Paul
</comment><comment author="dmartinpro" created="2011-04-04T10:03:21Z" id="953165">+1 vote for this issue too.
This is a really useful feature. Think about an e-commerce shop, indexing all sku. When looking at a product, a customer should have in his results list the products (and not the sku).
</comment><comment author="till" created="2011-05-10T14:19:34Z" id="1131143">_subscribe_
</comment><comment author="tfreitas" created="2011-05-10T16:58:54Z" id="1132112">+1
</comment><comment author="vincenttheeten" created="2011-05-13T11:03:03Z" id="1152259">plz don't make us switch to SOLR just for this feature
+1
</comment><comment author="kimchy" created="2011-05-13T11:54:53Z" id="1152418">Note that solr does not implment it for a distributed search (as far as I know) and the implementation is problematic (my view).
</comment><comment author="till" created="2011-05-13T18:13:18Z" id="1155218">Are you referring to the "field collapse patch" floating around in their Jira? I haven't checked if that made it into a recent release so I don't know how up to date my info is, I just noticed that queries using "field collapse patch" are by magnitude slower than queries without.
</comment><comment author="mikemccand" created="2011-05-18T18:52:42Z" id="1198926">Note that there is now (finally!) a new grouping module in Lucene -- see https://issues.apache.org/jira/browse/LUCENE-1421

It's been back-ported to 3.x, under lucene/contrib/grouping.

So in theory exposing this in ElasticSearch should be straightforward?  (And, if it's not, I'd really like to know about that so we can fix it!).

There is some performance hit but not as bad as I had expected.  See the 3 TermGroupXXX charts here: http://people.apache.org/~mikemccand/lucenebench -- it's ~ 2.3x-2.5X slower than the straight TermQuery, when grouping by a field with 100, 10K, 1M unique values (though, the sort and groupSort are relevance; maybe when sorting by other fields this is slower).  This should also be the worst-case slowdown since TermQuery is such an "easy" query; queries which are "hard" and don't produce many results should see less net impact from the grouping overhead, I expect.
</comment><comment author="kimchy" created="2011-05-19T21:02:37Z" id="1206368">Cool!, saw that a few days ago, will definitely have a look.
</comment><comment author="tfreitas" created="2011-06-03T22:53:46Z" id="1299942">Hi, with the release of Lucene 3.2, one of its features are:
"A new grouping module, under lucene / contrib / grouping, enable search results to Be group by single-valued indexed field "
http://wiki.apache.org/lucene-java/ReleaseNote32
</comment><comment author="darxriggs" created="2011-06-11T23:34:31Z" id="1353300">+1
</comment><comment author="aaronbinns" created="2011-06-13T18:55:10Z" id="1361015">+1
</comment><comment author="0xPIT" created="2011-06-13T20:48:04Z" id="1361741">++1
</comment><comment author="mkreidenweis" created="2011-06-14T07:26:32Z" id="1364290">+1
</comment><comment author="bbock" created="2011-06-14T07:56:12Z" id="1364394">+1
</comment><comment author="selaux" created="2011-06-14T08:14:31Z" id="1364494">+1
</comment><comment author="jmayr" created="2011-06-14T08:28:24Z" id="1364566">+1
</comment><comment author="mikemccand" created="2011-06-14T10:37:51Z" id="1365140">I'm also working on making it easy(ier) to distribute grouping, by adding static merge methods to TopDocs/TopGroups.  Ie, each shard can run the 1st pass collector, send top groups back to front end, front end merges the top groups (SearchGroup.merge) and issues request to all shards to run 2nd pass collector, gets results back, merges with TopGroups.merge.  This is all under https://issues.apache.org/jira/browse/LUCENE-3191
</comment><comment author="spinscale" created="2011-06-15T06:52:21Z" id="1371642">+1
</comment><comment author="stevencasey" created="2011-06-17T03:46:54Z" id="1385895">+1

any news on whether https://issues.apache.org/jira/browse/LUCENE-1421 as mentioned by mikemccand will work in elasticsearch?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Server Weighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/255</link><project id="" key="" /><description>Ability to weight servers in the cluster to receive different loads. Our newer h/w always seems to be 2x faster than older h/w and we'd like it to receive that load when placed into the cluster.

So, this would allow a per server Weight field that would preferably be a float to allow for arbitrary scaling of load. 

From an implementation perspective, it seems that this can be accomplished via shard allocation. 

Thanks!
</description><key id="246293">255</key><summary>Server Weighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2010-07-13T16:59:00Z</created><updated>2010-09-03T23:11:37Z</updated><resolved>2010-09-03T23:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2010-08-31T17:14:08Z" id="381155">I've been playing around more with the clustered set up and have a couple of more thoughts on this. 

I believe it would be ideal to track two pieces of information on each index, the query volume and the index volume. These two metrics can be used to identify the weight of a shard, probably biasing the search volume. 

Perhaps the shard sizes could also contribute.

In my current clustered setup, the load is not symmetrical. I believe that this is because shard allocation is not smart enough to know where to put each shard to avoid hot spotting. 
</comment><comment author="ppearcy" created="2010-09-03T23:11:37Z" id="388098">This ticket should be superseded by:
http://github.com/elasticsearch/elasticsearch/issues#issue/353
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make version number queryable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/254</link><project id="" key="" /><description>It'd be good to be able to retrieve the server version number from the running cluster.  That way, my API can provide workarounds when things change (eg the change from mapping being a JSON string to being actual JSON)

ta

clint
</description><key id="246157">254</key><summary>Make version number queryable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-07-13T14:55:39Z</created><updated>2010-07-14T18:33:32Z</updated><resolved>2010-07-14T19:02:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T10:46:21Z" id="311608">You can do a rest request against `/`, you will get the data.
</comment><comment author="clintongormley" created="2010-07-14T12:02:35Z" id="311666">Ah sweet  - thanks
</comment><comment author="kimchy" created="2010-07-14T18:33:32Z" id="312316">you get a nice quote while you are at it ;). closing the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make merging mappings smarter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/253</link><project id="" key="" /><description>Currently, if you put a mapping that is exactly the same as an existing mapping (with `ignore_conflicts` set to `false`) then ES throws an error.

Could this be changed to only throw an error if an actual conflict occurs?  (eg if trying to change the type of an existing field)
</description><key id="246058">253</key><summary>Make merging mappings smarter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-13T12:29:42Z</created><updated>2010-07-15T19:45:36Z</updated><resolved>2010-07-14T11:32:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T04:32:27Z" id="311282">Make merging mappings smarter, closed by 234455530af05b7a9958a310bda63a8e65b1ef29.
</comment><comment author="clintongormley" created="2010-07-15T14:42:12Z" id="313555">This works, but when there is a conflict, the error isn't very helpful: 

```
curl -XPUT 'http://127.0.0.1:9200/es_test_1,es_test_2/test/_mapping'  -d '
{
   "properties" : {
      "num" : {
         "type" : "string"
      },
      "text" : {
         "type" : "integer"
      }
   }
}
'
# {
#    "error" : "nested: "
# }
```
</comment><comment author="kimchy" created="2010-07-15T15:30:59Z" id="313628">I've tried to crete conflicts, and get proper exception message. Can you paste a full recreation for the fugly message?
</comment><comment author="clintongormley" created="2010-07-15T16:28:38Z" id="313701">It is not that the error is fugly, it is that it is too short, doesn't explain what the issue is:

```
curl -XPUT 'http://127.0.0.1:9200/es_test_1/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/test/_mapping'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/es_test_1/test/_mapping'  -d '
{
   "properties" : {
      "num" : {
         "type" : "string"
      },
      "text" : {
         "type" : "integer"
      }
   }
}
'
# {
#    "error" : "nested: "
# }
```

Also, it appears that `ignore_conflicts` is now being ignored:

```
curl -XPUT 'http://127.0.0.1:9200/es_test_1/test/_mapping?ignore_conflicts=true'  -d '
{
   "properties" : {
      "num" : {
         "type" : "string"
      },
      "text" : {
         "type" : "integer"
      }
   }
}
'
# {
#    "error" : "nested: "
# }
```
</comment><comment author="kimchy" created="2010-07-15T17:41:01Z" id="313815">ok ,just pushed a fix for this, can you check? ignore conflicts should work as well.
</comment><comment author="clintongormley" created="2010-07-15T18:01:30Z" id="313845">Error message much better.

However, the version with ignore_conflicts is now giving me this error:

```
ElasticSearchException[org.elasticsearch.index.mapper.xcontent.XContentStringFieldMapper cannot be cast to org.elasticsearch.index.mapper.xcontent.XContentNumberFieldMapper]; nested: ClassCastException[org.elasticsearch.index.mapper.xcontent.XContentStringFieldMapper cannot be cast to org.elasticsearch.index.mapper.xcontent.XContentNumberFieldMapper];
```
</comment><comment author="kimchy" created="2010-07-15T19:35:49Z" id="313981">ok, hope its fixed now, otherwise, I am testing something wrong, which I don't really undersand how...
</comment><comment author="clintongormley" created="2010-07-15T19:45:36Z" id="313997">Splendid. Fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put Mapping: Fail when an analyzer is specified that was not configured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/252</link><project id="" key="" /><description /><key id="245475">252</key><summary>Put Mapping: Fail when an analyzer is specified that was not configured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-07-12T20:17:32Z</created><updated>2010-07-12T20:18:37Z</updated><resolved>2010-07-12T20:18:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-12T20:18:37Z" id="309146">Put Mapping: Fail when an analyzer is specified that was not configured, closed by e6bd3f269359140f2b704aeb59f921585e54f4f8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE in NettyTransport when HTTP is enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/251</link><project id="" key="" /><description>I'm connecting to an Elastic Search server started in the same VM with my client. The server does not have http enabled. 

I am able to connect and index documents when I create a client using this approach:
    localNode = nodeBuilder().client(true).node();
    return localNode.client();

When I try to disable http on the client, I get the stacktrace shown at the end of this post:
    localNode = nodeBuilder().client(true).settings(settingsBuilder().put("http.enabled", false)).build();
    return localNode.client();
# Stacktrace

 WARN thread-1                    ElasticSearchListener: 138 - Failed to index [bucket][key]
org.elasticsearch.transport.SendRequestTransportException: [indices/createIndex]
    at org.elasticsearch.transport.RemoteTransportException.fillStack(RemoteTransportException.java:58)
    at org.elasticsearch.transport.SendRequestTransportException.fillInStackTrace(SendRequestTransportException.java:34)
    at java.lang.Throwable.&lt;init&gt;(Throwable.java:218)
    at java.lang.Exception.&lt;init&gt;(Exception.java:59)
    at java.lang.RuntimeException.&lt;init&gt;(RuntimeException.java:61)
    at org.elasticsearch.ElasticSearchException.&lt;init&gt;(ElasticSearchException.java:46)
    at org.elasticsearch.transport.TransportException.&lt;init&gt;(TransportException.java:34)
    at org.elasticsearch.transport.RemoteTransportException.&lt;init&gt;(RemoteTransportException.java:39)
    at org.elasticsearch.transport.SendRequestTransportException.&lt;init&gt;(SendRequestTransportException.java:30)
    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:197)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:487)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:383)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:183)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:171)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction.doExecute(TransportMasterNodeOperationAction.java:76)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction.doExecute(TransportMasterNodeOperationAction.java:37)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:54)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:85)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:56)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:54)
    at org.elasticsearch.client.node.NodeClient.index(NodeClient.java:115)
    at com.tracermedia.hazelcast.ElasticSearchListener.store(ElasticSearchListener.java:131)
    at com.tracermedia.hazelcast.ElasticSearchListenerContainer.store(ElasticSearchListenerContainer.java:148)
    at BridgeTest.testOnValueChangedAndRemoved(BridgeTest.java:65)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.testng.internal.MethodHelper.invokeMethod(MethodHelper.java:643)
    at org.testng.internal.MethodHelper$1.runTestMethod(MethodHelper.java:761)
    at org.springframework.test.context.testng.AbstractTestNGSpringContextTests.run(AbstractTestNGSpringContextTests.java:158)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.testng.internal.MethodHelper.invokeHookable(MethodHelper.java:769)
    at org.testng.internal.Invoker.invokeMethod(Invoker.java:552)
    at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:723)
    at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1027)
    at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:137)
    at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:121)
    at org.testng.TestRunner.runWorkers(TestRunner.java:1030)
    at org.testng.TestRunner.privateRun(TestRunner.java:709)
    at org.testng.TestRunner.run(TestRunner.java:579)
    at org.testng.SuiteRunner.runTest(SuiteRunner.java:331)
    at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:326)
    at org.testng.SuiteRunner.privateRun(SuiteRunner.java:288)
    at org.testng.SuiteRunner.run(SuiteRunner.java:193)
    at org.testng.TestNG.createAndRunSuiteRunners(TestNG.java:910)
    at org.testng.TestNG.runSuitesLocally(TestNG.java:879)
    at org.testng.TestNG.run(TestNG.java:787)
    at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:62)
    at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:141)
    at org.apache.maven.surefire.Surefire.run(Surefire.java:177)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:345)
    at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1009)
</description><key id="244565">251</key><summary>NPE in NettyTransport when HTTP is enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oravecz</reporter><labels /><created>2010-07-11T22:00:11Z</created><updated>2010-07-14T08:31:48Z</updated><resolved>2010-07-14T08:31:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T08:31:48Z" id="311482">Closing this, the problem was that the client was using a node that was not started.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shutdown API: Improve behavior when shutting down the whole cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/250</link><project id="" key="" /><description>When issuing a full shutdown of the whole cluster (for example, using REST `POST` to `/_cluster/nodes/_shutdown`), the request should issue a shutdown of all nodes without any allocation / re-routing happening while the nodes are being shutdown.
</description><key id="244459">250</key><summary>Shutdown API: Improve behavior when shutting down the whole cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-11T17:41:23Z</created><updated>2010-07-11T17:42:10Z</updated><resolved>2010-07-11T17:42:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-11T17:42:10Z" id="307749">Shutdown API: Improve behavior when shutting down the whole cluster, closed by 294f09a1d7a603f26ebd554be6c4e74a6f5ed76b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't create / use the work directory if not needed (for example, on client / non data) nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/249</link><project id="" key="" /><description /><key id="243744">249</key><summary>Don't create / use the work directory if not needed (for example, on client / non data) nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-10T11:29:10Z</created><updated>2010-07-10T11:29:49Z</updated><resolved>2010-07-10T11:29:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-10T11:29:49Z" id="306796">Don't create / use the work directory if not needed (for example, on client / non data) nodes, closed by a0ead0229970a0381c26522bbfd494f1e9e5e007.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: Control which nodes are allowed to become masters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/248</link><project id="" key="" /><description>Using the `discovery.zen.master` boolean setting, allow to control if the node can become a master or not. This allows to create a controlled groups of machines that will can become masters.

If a node looses connection to the masters, it will block operations until a master is discovered.
</description><key id="242722">248</key><summary>Zen Discovery: Control which nodes are allowed to become masters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-07-09T01:20:52Z</created><updated>2010-07-09T01:22:09Z</updated><resolved>2010-07-09T01:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-09T01:22:09Z" id="305078">Zen Discovery: Control which nodes are allowed to become masters, closed by b657ffc5e77b416139396cc02d1b0c11656b451f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: A node might get into an infinite state of trying to find a master (when client / non_master) nodes exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/247</link><project id="" key="" /><description /><key id="242597">247</key><summary>Zen Discovery: A node might get into an infinite state of trying to find a master (when client / non_master) nodes exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-07-08T21:42:40Z</created><updated>2010-07-08T21:43:19Z</updated><resolved>2010-07-08T21:43:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-08T21:43:19Z" id="304913">Zen Discovery: A node might get into an infinite state of trying to find a master (when client / non_master) nodes exists, closed by d531d82cfb33c97e546cfbaa1a754d05573d58b7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facets: Allow to specify a set of terms to exclude in the request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/246</link><project id="" key="" /><description>Allow to specify a set of terms that should be excluded from the terms facet request result. Here is an example:

```
"terms" : {
    "field" : "some_field",
    "exclude" : ["term1", "term2"]
}
```
</description><key id="240978">246</key><summary>Terms Facets: Allow to specify a set of terms to exclude in the request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-07-07T11:41:06Z</created><updated>2010-07-07T20:21:41Z</updated><resolved>2010-07-07T20:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-07T20:21:41Z" id="303350">Terms Facets: Allow to specify a set of terms to exclude in the request, closed by 9578ad3ef1e857391e01996a8fd9dccc2d569135.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type storage issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/244</link><project id="" key="" /><description>It looks like Elastic Search is overriding my default mapping values when it adds defaults dynamically.

I'm seeing ints being stored as longs and floats and double being stored as string when using
client.admin().indices().create(createRequest)

Dates appear fine.

Here is part of the value used in createRequest.settings(value):
"trxamount":{"type":"double","boost":1}

Here is what I see in the logs:
trxamount":{"type":"string","index_name":"trxamount","index":"analyzed","store":"no","term_vector"
:"no","boost":1.0,"omit_norms":false,"omit_term_freq_and_positions":false}

I initially create an index using mappings, I don't provide all the fields such term_vector omit_terms etc, so Elastic Search is dynamically adding them later when I perform an index.
It looks like it is overriding my mappings.
</description><key id="237582">244</key><summary>Type storage issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SamD</reporter><labels /><created>2010-07-03T00:22:25Z</created><updated>2011-11-17T00:39:29Z</updated><resolved>2011-11-17T00:39:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-06T07:47:22Z" id="301086">Is this really an issue? Did not get a response for the same question on the mailing list.
</comment><comment author="SamD" created="2011-11-17T00:39:29Z" id="2770545">hmm this was a long time ago
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move service wrapper support to its own repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/243</link><project id="" key="" /><description>Move the service wrapper support to its own repo at: http://github.com/elasticsearch/elasticsearch-servicewrapper.
</description><key id="235301">243</key><summary>Move service wrapper support to its own repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.09.0</label></labels><created>2010-06-30T18:48:57Z</created><updated>2010-06-30T18:51:40Z</updated><resolved>2010-06-30T18:51:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-30T18:51:40Z" id="294050">Move service wrapper support to its own repo, closed by dea8238ced7dd91e74e77a9959d7767a49190b50.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the terms API (replaced by facets in search)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/242</link><project id="" key="" /><description>Terms API can easily be replaced with search and facets (which are more powerful) and a size of 0 (i.e. don't get any hits back).
</description><key id="235185">242</key><summary>Remove the terms API (replaced by facets in search)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.09.0</label></labels><created>2010-06-30T16:48:21Z</created><updated>2010-06-30T16:49:51Z</updated><resolved>2010-06-30T16:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-30T16:49:51Z" id="293860">Remove the terms API (replaced by facets in search), closed by 66096e97f29e4f45bfdb6e67fd9bbaeac21bda6b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatic management of indexing buffer size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/241</link><project id="" key="" /><description>Currently, there is an option to set the `ram_buffer_size` on the robin engine, but this is set on a per shard level. So, it can be set to `64mb`, but it does not take into account the number fo total shards allocated on the nodes. For example, if there is a single shard, and we have plenty of memory, then we should set it to a higher value, and if there are a lot of shards, this should be set to a lower value.

A global management of the indexing buffer size should be maintained, with the ability to set the memory used for indexing as either a value or a percent of the allocated memory to the jvm, across all shards.

The setting is called `indices.memory.index_buffer_size`, and it defaults to `40%`. It can either be set to a percent value (with `%` suffix), or a byte size value (for example `1.5gb`).

The default of the setting is `40%`.
</description><key id="234247">241</key><summary>Automatic management of indexing buffer size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-29T19:54:22Z</created><updated>2010-06-29T19:55:01Z</updated><resolved>2010-06-29T19:55:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-29T19:55:01Z" id="292633">Automatic management of indexing buffer size, closed by fdb2eff998299b017247f4f9388fec32db1fb215.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed node with allocated primary shards do not cause backups to become primaries, but full gateway recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/240</link><project id="" key="" /><description>When a primary shard fails, a backup should become primary, and another backup should be created.
</description><key id="234051">240</key><summary>Failed node with allocated primary shards do not cause backups to become primaries, but full gateway recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-06-29T16:47:59Z</created><updated>2010-06-29T16:49:34Z</updated><resolved>2010-06-29T16:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-29T16:49:34Z" id="292377">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Block operation performed on cluster until it recovered from the gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/239</link><project id="" key="" /><description>This mainly applies to cases where `gateway.recover_after_time` and `gateway.recover_after_nodes` are used.
</description><key id="233221">239</key><summary>Block operation performed on cluster until it recovered from the gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-28T19:52:38Z</created><updated>2010-06-28T19:53:22Z</updated><resolved>2010-06-28T19:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-28T19:53:22Z" id="291131">Block operation performed on cluster until it recovered from the gateway, closed by 5f538b1ba39f939e6b596defd333d556295777c6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE if "content" is missing in mapper-attachment plugin mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/238</link><project id="" key="" /><description>Consider the following mapping being POSTed for use in mapper-attachment plugin:

```
{
    "my_attachment" : {
        "_content_type" : "application/pdf",
        "_name" : "resource/name/of/my.pdf",
    }
}
```

Note the missing `"content"`!

Results in NPE (something like the following):

```
java.lang.NullPointerException
at org.elasticsearch.common.io.FastByteArrayInputStream.&lt;init&gt;(FastByteArrayInputStream.java:90)
at org.elasticsearch.index.mapper.xcontent.XContentAttachmentMapper.parse(XContentAttachmentMapper.java:254)
at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.serializeObject(XContentObjectMapper.java:348)
at org.elasticsearch.index.mapper.xcontent.XContentObjectMapper.parse(XContentObjectMapper.java:320)
at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:320)
at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:272)
at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:268)
at org.elasticsearch.index.mapper.xcontent.SimpleAttachmentMapperTests.testMissingContentMapping(SimpleAttachmentMapperTests.java:107)
```

An ERROR response with reasonable error message would be better.

It should be clear that out of all `"content"`, `"_content_type"` and `"_name"` the first one is mandatory while the later two are just optional.
</description><key id="232862">238</key><summary>NPE if "content" is missing in mapper-attachment plugin mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-06-28T12:32:50Z</created><updated>2013-02-20T11:53:50Z</updated><resolved>2013-02-20T11:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2010-06-28T13:19:55Z" id="290558">There is a unit test for this [here](http://github.com/lukas-vlcek/elasticsearch/commit/f38f0223d86e88a9f5fa4bbb09538990c84f28fd) (check `testMissingContentMapping()` method)
</comment><comment author="dadoonet" created="2013-02-20T11:53:50Z" id="13828136">I moved this issue to https://github.com/elasticsearch/elasticsearch-mapper-attachments/issues/23
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document span queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/237</link><project id="" key="" /><description>The Query DSL documentation misses span queries and should be updated accordingly.
</description><key id="232809">237</key><summary>Document span queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">melix</reporter><labels /><created>2010-06-28T10:54:21Z</created><updated>2010-06-28T11:52:45Z</updated><resolved>2010-06-28T11:52:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-28T11:38:24Z" id="290467">I have documented it here: http://www.elasticsearch.com/docs/elasticsearch/rest_api/query_dsl/, is it enough?
</comment><comment author="melix" created="2010-06-28T11:39:09Z" id="290468">Perfect, thanks.
</comment><comment author="kimchy" created="2010-06-28T11:52:45Z" id="290486">documented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: field query does not take into account `allow_leading_wildcards`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/236</link><project id="" key="" /><description>Since it doesn't take into account `allow_leading_wildcard` parameter, which defaults to `true`, then its not allowed to place leading wildcards in queries.
</description><key id="232765">236</key><summary>Query DSL: field query does not take into account `allow_leading_wildcards`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-06-28T09:32:57Z</created><updated>2010-06-28T19:53:22Z</updated><resolved>2010-06-28T19:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-28T19:53:22Z" id="291130">Query DSL: field query does not take into account `allow_leading_wildcards`, closed by 63523f4525181ebb09304aea805344c994bd3365.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory Store / FS Memory: Create a node level memory store cache and allocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/235</link><project id="" key="" /><description>Currently, memory store is configured per shard, meaning that caching, for example, is done on a per shard level and configured on a per shard level. It means that configuring a cache of `20mb` is done on a per shard level, and is not scalable since the number of shards allocated to a node can vary.

There should be a global memory allocator and cache, that serves all different shards, and the configuration should be set on it. Here are the settings to set on it (note, these are node level settings):
- `cache.memory.buffer_size`: The buffer size, defaults to `100k`.
- `cache.memory.cache_size`: The size of cached buffers, defaults to `20mb`.
- `cache.memory.direct`: Should direct buffers be used (native memory) or not (heap). Defaults to `true`.
- `cache.memory.warm_cache`: Should the cache be warmed or not.

Note, all differnet memory level settings that were done on store level are no longer  applicable and replaced by the mentioned node level settings above.
</description><key id="231991">235</key><summary>Memory Store / FS Memory: Create a node level memory store cache and allocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-26T19:33:20Z</created><updated>2010-06-26T19:47:44Z</updated><resolved>2010-06-27T02:34:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-26T19:34:33Z" id="289187">Memory Store / FS Memory: Create a node level memory store cache and allocator, closed by 33d357dbb425de727ec834e6c59ce8c5a183cc2e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster State API: Allow to filter the state on nodes, routing_table, metadata, and indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/234</link><project id="" key="" /><description>Allow to filter the cluster state response using the following REST parameters:
- `filter_nodes`: set to `true` to filter out the `nodes` part of the response.
- `filter_routing_table`: set to `true` to filter out the `routing_table` part of the response.
- `filter_metadata`: set to `true` to filter out the `metadata` part of the response.
- `filter_indices`: When not filtering metadata, a comma separated list of indices to include in the response.
</description><key id="228373">234</key><summary>Cluster State API: Allow to filter the state on nodes, routing_table, metadata, and indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-22T13:18:46Z</created><updated>2014-03-05T19:00:57Z</updated><resolved>2010-06-23T03:01:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-22T20:01:27Z" id="284479">Cluster State API: Allow to filter the state on nodes, routing_table, metadata, and indices, closed by 8e62bb6934762e0f88b54ad995f4d81906740408.
</comment><comment author="clintongormley" created="2010-07-14T12:53:49Z" id="311727">Hiya

The cluster state filtering doesn't work:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/state?filter_indices=true&amp;filter_nodes=true&amp;filter_routing_table=true&amp;filter_metadata=true'
```

It looks like you added code to check for the parameters, but you aren't using the results anywhere
</comment><comment author="kimchy" created="2010-07-14T13:22:08Z" id="311763">right, will push a fix later today.
</comment><comment author="clintongormley" created="2010-07-15T14:40:38Z" id="313554">tested++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create index - Timing issue with index status report</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/233</link><project id="" key="" /><description>The following shell script gives the below output, note different index status output!

```
#!/bin/sh
echo
echo 'Creating index -----------------------------------------'
curl -XPUT 'http://localhost:9200/twitter/' -d '
    {index:{number_of_shards:3,number_of_replicas:1}}
'

echo
echo 'Index status -------------------------------------------'
curl -XGET 'http://localhost:9200/twitter/_status?pretty=true'

sleep 1

echo
echo 'Index status again -------------------------------------'
curl -XGET 'http://localhost:9200/twitter/_status?pretty=true'

echo
echo 'Delete index -------------------------------------------'
curl -XDELETE 'http://localhost:9200/twitter/'

echo
```

The following is output:

```
Creating index -----------------------------------------
{"ok":true,"acknowledged":true}

Index status -------------------------------------------
{
  "ok" : true,
  "_shards" : {
    "total" : 6,
    "successful" : 0,
    "failed" : 0
  },
  "indices" : {
  }
}

Index status again -------------------------------------
{
  "ok" : true,
  "_shards" : {
    "total" : 6,
    "successful" : 3,
    "failed" : 0
  },
  "indices" : {
    "twitter" : {
      "aliases" : [ ],
      "settings" : {
        "index.number_of_replicas" : "1",
        "index.number_of_shards" : "3"
      },
      "store_size" : "156",
      "store_size_in_bytes" : 156,
      "estimated_flushable_memory_size" : "0",
      "estimated_flushable_memory_size_in_bytes" : 0,
      "translog_operations" : 0,
      "docs" : {
        "num_docs" : 0,
        "max_doc" : 0,
        "deleted_docs" : 0
      },
      "shards" : {
        "0" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "028b7770-a38b-4df5-9ac4-ef3498e4fffc",
            "relocating_node" : null,
            "shard" : 0,
            "index" : "twitter"
          },
          "state" : "STARTED",
          "store_size" : "52",
          "store_size_in_bytes" : 52,
          "estimated_flushable_memory_size" : "0",
          "estimated_flushable_memory_size_in_bytes" : 0,
          "translog_id" : 0,
          "translog_operations" : 0,
          "docs" : {
            "num_docs" : 0,
            "max_doc" : 0,
            "deleted_docs" : 0
          }
        } ],
        "1" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "028b7770-a38b-4df5-9ac4-ef3498e4fffc",
            "relocating_node" : null,
            "shard" : 1,
            "index" : "twitter"
          },
          "state" : "STARTED",
          "store_size" : "52",
          "store_size_in_bytes" : 52,
          "estimated_flushable_memory_size" : "0",
          "estimated_flushable_memory_size_in_bytes" : 0,
          "translog_id" : 0,
          "translog_operations" : 0,
          "docs" : {
            "num_docs" : 0,
            "max_doc" : 0,
            "deleted_docs" : 0
          }
        } ],
        "2" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "028b7770-a38b-4df5-9ac4-ef3498e4fffc",
            "relocating_node" : null,
            "shard" : 2,
            "index" : "twitter"
          },
          "state" : "STARTED",
          "store_size" : "52",
          "store_size_in_bytes" : 52,
          "estimated_flushable_memory_size" : "0",
          "estimated_flushable_memory_size_in_bytes" : 0,
          "translog_id" : 0,
          "translog_operations" : 0,
          "docs" : {
            "num_docs" : 0,
            "max_doc" : 0,
            "deleted_docs" : 0
          }
        } ]
      }
    }
  }
}

Delete index -------------------------------------------
{"ok":true,"acknowledged":true}
```
</description><key id="228328">233</key><summary>Create index - Timing issue with index status report</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-06-22T12:17:45Z</created><updated>2013-04-04T18:13:24Z</updated><resolved>2013-04-04T18:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2010-06-22T12:18:44Z" id="283924">Is this a bug or not?
</comment><comment author="nervetattoo" created="2010-07-21T19:44:08Z" id="321511">Its the same for item indexes. I guess this is what is meant by ElasticsSearch being "near real time". 
</comment><comment author="lukas-vlcek" created="2010-07-21T20:10:03Z" id="321545">Not exactly. This is index creation and not adding a new document into existing index. In this particular case the issue was that the index was created but not all shards of that index have been allocated and started yet. That is a different problem as opposed to making newly indexed document available for search (aka. near real time search). Also it would be better to use cluster state API in this case.
</comment><comment author="clintongormley" created="2013-04-04T18:13:24Z" id="15913993">This has since been fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Internal refactoring, requires manual upgrade when using fs gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/232</link><project id="" key="" /><description>A comprehensive refactoring in the gateway module, allowing for easier implementation of other different types as well as more code reuse. Also, this is the first step at getting to reuse the work directory when possible.

When using the FS gateway, and upgrade to 0.9 requires location the matedata-XXX files, creating a directory called metadata in the same location, and moving the metadata-XXX files there.
</description><key id="227755">232</key><summary>Gateway: Internal refactoring, requires manual upgrade when using fs gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-21T19:39:27Z</created><updated>2010-06-22T08:57:15Z</updated><resolved>2010-06-22T02:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-21T19:40:23Z" id="283147">Gateway: Internal refactoring, requires manual upgrade when using fs gateway, closed by 7ed7c6db4e680b641c218aa4c8571d22af1bff8d.
</comment><comment author="clintongormley" created="2010-06-21T21:29:25Z" id="283259">kimchy - could you give a real example. It's not very clear from the description. btw, sounds like a real win!
</comment><comment author="kimchy" created="2010-06-22T08:57:15Z" id="283769">Sure, so, if you configure a gateway path to `/mnt/test`, you will find `metadata-` files within a sub directory named after the cluster name: `/mnt/test/[cluster_name]`. Upgrading to version `0.9` requires to take those `metadata-` files, and placing them under the `/mnt/test/[cluster_name]/metadata/`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene: Upgrade to latest 3.0.2 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/231</link><project id="" key="" /><description /><key id="226550">231</key><summary>Lucene: Upgrade to latest 3.0.2 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-19T22:04:13Z</created><updated>2010-06-19T22:04:44Z</updated><resolved>2010-06-19T22:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-19T22:04:44Z" id="281387">Lucene: Upgrade to latest 3.0.2 version, closed by e846ed6465e028b2d017d6739aa0365472c2d31d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: When specifying empty array for stopwords, use an empty list for stopwords</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/230</link><project id="" key="" /><description>Currently, when specifying empty array, it will use the default stopwords.
</description><key id="225176">230</key><summary>Analysis: When specifying empty array for stopwords, use an empty list for stopwords</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-17T21:33:33Z</created><updated>2010-06-17T21:34:26Z</updated><resolved>2010-06-17T21:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-17T21:34:26Z" id="279230">Analysis: When specifying empty array for stopwords, use an empty list for stopwords, closed by 22926f0026fdae9b51b8efd2f9a5e103653b2a28.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: fields that are `not_analyzed` should automatically default to keyword analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/229</link><project id="" key="" /><description>It does not make sense to have the default (standard) analyzer on fields that are `not_analyzed`, since when searching that analyzer will be used and there won't be any hits. It should default to `keyword` analyzer.
</description><key id="225135">229</key><summary>Analysis: fields that are `not_analyzed` should automatically default to keyword analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-17T20:23:15Z</created><updated>2010-06-17T20:24:06Z</updated><resolved>2010-06-17T20:24:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-17T20:24:06Z" id="279150">Analysis: fields that are `not_analyzed` should automatically default to keyword analyzer, closed by 8e4c139c8bc7757eba259ce76261f8e6d8eeeb6d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Script Histogram facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/228</link><project id="" key="" /><description>Allow to specify scripts for both key and value in histogram facets. For example:

```
{
    query : {
        "match_all" : {}
    },
    facets : {
        "histo1" : {
            "histogram" : {
                "key_script" : "doc['date'].date.minuteOfHour",
                "value_script" : "doc['num1']"
            }
        }
    }
}
```

`interval` can also be specified, which will further normalize the key value into buckets (by default, if its not set, the actual key value will be used). Also, `params` can be specified to the scripts.
</description><key id="223421">228</key><summary>Facets: Script Histogram facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-16T01:21:24Z</created><updated>2010-06-16T01:22:07Z</updated><resolved>2010-06-16T01:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-16T01:22:07Z" id="276808">Facets: Script Histogram facet, closed by 2d6f61b3c1377299ed461c1d26099ec31c26b9c2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facets: Script statistical facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/227</link><project id="" key="" /><description>Allow to define a script to be evaluated with statistical facets, for example:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "test" : {
            "statistical" : {
                "script" : "doc['num1'] + doc['num2']"
            },
            "global" : false
        }
    }
}
```

Scripts can also allow for parameters:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "test" : {
            "statistical" : {
                "script" : "(doc['num1'] + doc['num2']) * factor",
                "params" : {
                    "factor" : 2.2
                }
            },
            "global" : false
        }
    }
}
```
</description><key id="223350">227</key><summary>Facets: Script statistical facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-15T23:17:57Z</created><updated>2010-06-15T23:18:33Z</updated><resolved>2010-06-15T23:18:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-15T23:18:33Z" id="276694">Facets: Script statistical facets, closed by bb24b56c667b738ded7e809f5c806be1e94581f2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script Filter: Support providing a custom script as a filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/226</link><project id="" key="" /><description>A new filter, using the new script support (as used in `custom_score` and script fields). For example:

```
"filtered" : {
    "query" : {
        ...
    }, 
    "filter" : {
        "script" : {
            "script" : "doc['num1'].value &gt; 1"
        }
    }
}
```

Custom parameters can also be provided for it:

```
"filtered" : {
    "query" : {
        ...
    }, 
    "filter" : {
        "script" : {
            "script" : "doc['num1'].value &gt; param1"
            "params" : {
                "param1" : 5
            }
        }
    }
}
```
</description><key id="223318">226</key><summary>Script Filter: Support providing a custom script as a filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-15T22:31:32Z</created><updated>2010-06-15T22:32:11Z</updated><resolved>2010-06-15T22:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-15T22:32:11Z" id="276658">Script Filter: Support providing a custom script as a filter, closed by c2786038e2744a1830c675cd74d3b1c3483dde24.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tcp Transport: Reduce `transport.tcp.connection_per_node` from 5 to 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/225</link><project id="" key="" /><description>In general, since we use async IO, a single connection is enough, and should create less load on the system in terms of resources used. More connections might help get better network TPS, but thats on really beefy machines.
</description><key id="223203">225</key><summary>Tcp Transport: Reduce `transport.tcp.connection_per_node` from 5 to 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-15T20:36:06Z</created><updated>2010-06-15T20:36:39Z</updated><resolved>2010-06-15T20:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-15T20:36:39Z" id="276509">Tcp Transport: Reduce `transport.tcp.connection_per_node` from 5 to 1, closed by be3b779caaa4dff5943cc61030f080a310e70128.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: When a master node is forcefully killed, other nodes might not monitor the other elected master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/224</link><project id="" key="" /><description /><key id="223186">224</key><summary>Zen Discovery: When a master node is forcefully killed, other nodes might not monitor the other elected master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-06-15T20:08:09Z</created><updated>2010-06-15T20:08:59Z</updated><resolved>2010-06-15T20:08:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-15T20:08:59Z" id="276462">Zen Discovery: When a master node is forcefully killed, other nodes might not monitor the other elected master, closed by 65039ff21de11804a174c803ab863af177b07540
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Allow to configure a `recovery_after_time` and `recover_after_nodes`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/223</link><project id="" key="" /><description>Currently, when the first master starts up, it will recover the cluster state from the gateway and start to create indices immediately. This means that the first node (first master) will start allocating a lot of shards to itself (since other nodes might take time to join the cluster to share the load).

It would make sense to delay the recovery either by time or by cluster size. For example, to delay the recovery by time set:

```
gateway:
  recover_after_time: 1m
```

And to delay recovery after a certain cluster size has been reached:

```
gateway:
  recover_after_nodes: 5
```

Note, the two flags can be combined.
</description><key id="222428">223</key><summary>Gateway: Allow to configure a `recovery_after_time` and `recover_after_nodes`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-14T22:25:27Z</created><updated>2010-06-18T02:32:27Z</updated><resolved>2010-06-15T05:26:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-14T22:26:01Z" id="275324">Gateway: Allow to configure a delay till index creation from gateway will occur, closed by 98df1b3433ba079d2e14fea485769c8a1bd7b2b3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose the mappings of a given index or index/type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/222</link><project id="" key="" /><description>It would be nice to get the mappings of a given index or index/type pair by doing a GET.

For example, I would like to have something like:
- "GET /logs/_mapping" to get all the mappings defined in the "logs" index.
- "GET /logs/log/_mapping" to get the mappings of the "logs/log" type.
- "GET /_mapping" to get all the mappings.

Note that this is currently possible by doing a GET of "/_cluster/state" but that also obtains a bunch of non related information, and one still has to parse the json (the mappings come in a json encoded string).
</description><key id="222217">222</key><summary>Expose the mappings of a given index or index/type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rgl</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-14T17:42:05Z</created><updated>2010-06-17T15:19:25Z</updated><resolved>2010-06-17T15:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="grantr" created="2010-06-14T19:57:34Z" id="275153">+1
</comment><comment author="kimchy" created="2010-06-16T02:50:04Z" id="276866">pushed a change to the cluster state to return native json mapping (in an array of mappings) when possible. Its an oversight on my part, I had to enhance some aspects to be able to do it, but its already there, so its a simple fix.
</comment><comment author="lukas-vlcek" created="2010-06-17T11:38:15Z" id="278626">It would be practical to have an options to get mappings as a part of index REST API as well. (i.e. it would be possible to get more info about index via one REST request, no need to do two or more requests).
</comment><comment author="kimchy" created="2010-06-17T15:19:25Z" id="278801">Expose the mappings of a given index or index/type, closed by 7b145fd4e1d6035c5c5d0084453a208bfd3ec0a0. Added the ability to filter returned data from the cluster state API, and use it to expose REST GET endpoint for "/_mapping", "/{index}/_mapping", "/{index}/{type}/_mapping" ({index} can also be "_all").
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Allow to specify script fields to be loaded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/221</link><project id="" key="" /><description>Script fields are basically scripts that a evaluated and returned per hit, two nice benefits is the fact that the field does not have to be stored, and a custom value can be returned (note, they probably make sense on numeric fields and on not analyzed fields).

A sample:

```
{
    "query" : {
        ...
    },
    "script_fields" : {
        "test1" : {
            "script" : "doc['my_field_name'].value * 2"
        },
        "test2" : {
            "script" : "doc['my_field_name'].value * factor",
            "params" : {
                "factor"  : 2.0
            }
        }
    }
}
```
</description><key id="221832">221</key><summary>Search: Allow to specify script fields to be loaded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-14T09:49:04Z</created><updated>2010-06-14T21:32:59Z</updated><resolved>2010-06-15T01:27:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-14T18:27:36Z" id="275016">Search: Allow to specify script fields to be loaded, closed by 0a1bc874c351da1043342373a13b79303a99a03c.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: custom score (scripted) query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/220</link><project id="" key="" /><description>`custom_score` query allows to wrap another query and customize the scoring of it optionally with a computation derived from other field values in the doc (numeric ones). Here is a simple sample:

```
"custom_score" : {
    "query" : {
        ....
    },
    "script" : "score * doc['my_numeric_field'].value"
}
```

The `script` uses `mvel` (http://mvel.codehaus.org/) which gives a lot of flexibility in terms of what can be computed and is _really_ fast. Other options considered were building a Json based AST (which will be faster, and might be implemented in the future), or using javascript, which is not as fast on the JVM (rhino).

There are "extensions" to mvel provided, they include the `score` variable, which is the score of the internal query. The idea of providing the score and let the user use it is to allow for complete control over the scoring, without internally multiplying whatever the script produced with the score. This allows for the sub query score to be discarded if needed, or computed on a different "scale" than multiplying.

It also provides all the Math functions (will be listed in the final docs). The more interesting one is the `doc[field_name]`, which actually returns an object that has the following common properties:
- `doc['field_name'].value`: The native numeric value of the field. For example, if its a `short` type, it will be short.
- `doc['field_name'].values`: The native numeric array values of the field. For example, if its a `short` type, it will be short[]. Remember, a field can have several values within a single doc. Returns an empty array if the field has no values.
- `doc['field_name'].stringValue`: The string value of the field.
- `doc['field_name'].doubleValue`: The converted double of the field. Replace `double` with `int`, `long`, `float`, `short`, `byte`. Returns an empty array if the field has no values.
- `doc['field_name'].doubleValues`: A converted double values array.
- `doc['field_name'].date`: Applies only to date / long (timestamp) types, returns a `MutableDateTime` (http://joda-time.sourceforge.net/api-release/org/joda/time/MutableDateTime.html) allowing to get date / time specific data.
- `doc['field_name'].dates`: Applies only to date / long (timestamp) types, returns a `MutableDateTime` array (http://joda-time.sourceforge.net/api-release/org/joda/time/MutableDateTime.html) allowing to get date / time specific data.
- `doc['field_name'].empty`: A boolean indicating if the field has no values within the doc.
- `doc['field_name'].multiValued`: A boolean indicating that the field has several values within the corpus.

`custom_score` also allows to accept parameters to scripts (allowing to reuse scripts for better performance). Parameters are simple json object that can then be accessed in the script, for example:

```
"custom_score" : {
    "query" : {
        ....
    },
    "params" : {
        "param1" : 2,
        "param2" : 3.1
    }
    "script" : "score * doc['my_numeric_field'].value / pow(param1, param2)"
}
```
</description><key id="221598">220</key><summary>Query DSL: custom score (scripted) query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-14T00:14:31Z</created><updated>2010-06-14T01:12:33Z</updated><resolved>2010-06-14T07:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-14T00:15:34Z" id="274010">Query DSL: custom score (scripted) query, closed by 751c56f31ee4689a3df4b86ced46e2bd6fc29abf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Facets: Histogram Facet - allow to create buckets with one field name, and aggregate valued on another field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/219</link><project id="" key="" /><description>You can created buckets based on one field values (for example, timestamp), and total/count another field from the same doc.

Here is an example:

```
{
    query : {
        "match_all" : {}
    },
    facets : {
        "histo1" : {
            "histogram" : {
                "key_field" : "fieldX",
                "value_field" : "fieldY",
                "time_interval" : "5d"
            }
        }
    }
}
```
</description><key id="218880">219</key><summary>Search Facets: Histogram Facet - allow to create buckets with one field name, and aggregate valued on another field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-10T19:44:20Z</created><updated>2010-06-20T13:16:12Z</updated><resolved>2010-06-11T02:44:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-10T19:44:34Z" id="271137">implemented.
</comment><comment author="mootpointer" created="2010-06-18T07:52:30Z" id="279620">What if all I want is the bucketed facets, no aggregation?
</comment><comment author="kimchy" created="2010-06-19T21:05:59Z" id="281338">can you give an example?
</comment><comment author="mootpointer" created="2010-06-20T02:18:30Z" id="281544">So for example I want to know the the count of documents fitting into date buckets.

Eg. I have a document with a created_at field which is a date. I want to know how many have been found each day of the month for the last month, something akin to http://wiki.apache.org/solr/SimpleFacetParameters#Date_Faceting_Parameters
</comment><comment author="kimchy" created="2010-06-20T09:10:25Z" id="281692">The check out #228, it gives you the ability to work within the script on a date object, which has simple constructs to get, for example, the day of month as the key. In order to perform it in the last month, you will need to either execute a filter on the facet, or on the query, depends on the scope of your query (if its already filters by "last month filter).
</comment><comment author="kimchy" created="2010-06-20T13:16:12Z" id="281823">One more thing, you can also define `time_interval` which is in elasticsearch time format (for example `2d` or `1.5h`) as the interval. Since dates are represented as millis in the index, this will work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Cross-Origin resource in http/rest module </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/218</link><project id="" key="" /><description>There is a known issue with Firefox sending extra OPTIONS request first when XHR POST request is created and executed (http://stackoverflow.com/questions/1099787/jquery-ajax-post-sending-options-as-request-method-in-firefox/2865924#2865924). This is causing problems with all POST requests in ES REST API sent from FF.

Moreover, current implementation in ES is causing OPTIONS requests to fall back to GET requests (see NettyHttpRequest.method()) which results in "No handler found for uri [request.uri()] and method [GET]".

As far as I understand it is important to respond with "Access-Control-Allow-Origin: *" to OPTIONS requests.
</description><key id="217708">218</key><summary>Support Cross-Origin resource in http/rest module </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-09T12:08:57Z</created><updated>2010-07-19T17:46:56Z</updated><resolved>2010-07-19T17:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-19T17:46:56Z" id="318242">Support Cross-Origin resource in http/rest module, closed by 5f0470d68ba3760fb3096d28954cda101acfe6b1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Facets: Facets to allow to define filters on them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/217</link><project id="" key="" /><description>All facets should allow to define filter on them (same filters that can be used in queries). for example:

```
{
    query : {
        "match_all" : {}
    },
    facets : {
        "histo1" : {
            "histogram" : {
                "field" : "multi_num",
                "interval" : 100
            },
            "filter" : {
                "term" : ...
            }
        }
    }
}
```
</description><key id="217179">217</key><summary>Search Facets: Facets to allow to define filters on them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-08T19:29:48Z</created><updated>2010-06-09T08:19:36Z</updated><resolved>2010-06-09T08:19:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-09T08:19:36Z" id="269014">Search Facets: Facets to allow to define filters on them, closed by 5ca050ffcb5f87f9b43969dc2ef10c94bf8c4402.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add `and`, `or`, and `not` filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/216</link><project id="" key="" /><description>Add support for `and`, `or` and `not` filters. In general, they are more performant then the `bool` filter. Here are some exampled:

`and`:

```
{
    "filtered" : {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "and" : {
                "filters" : [
                    {
                        "term" : { "name.first" : "something" }
                    },
                    {
                        "term" : { "name.first" : "other" }
                    }
                ]
            }
        }
    }
}
```

`or`:

```
{
    "filtered" : {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "or" : {
                "filters" : [
                    {
                        "term" : { "name.first" : "something" }
                    },
                    {
                        "term" : { "name.first" : "other" }
                    }
                ]
            }
        }
    }
}
```

`not`:

```
{
    "filtered" : {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "not" : {
                "filter" :  {
                    "term" : { "name.first" : "someting" }
                }
            }
        }
    }
}
```
</description><key id="216915">216</key><summary>Query DSL: Add `and`, `or`, and `not` filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-08T15:04:04Z</created><updated>2015-04-03T04:27:33Z</updated><resolved>2010-06-08T15:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-08T15:04:39Z" id="267989">Query DSL: Add `and`, `or`, and `not` filters, closed by 6257b7824382d7ef3193828019e3e293a92417f4
</comment><comment author="bevankoopman" created="2015-03-09T03:45:27Z" id="77796294">Would it be possible to include an example of combining all three in one query; e.g., a search for `A and B not C or D`? Thanks.
</comment><comment author="sahild" created="2015-04-03T04:27:33Z" id="89158412">@bevankoopman +1 @kimchy @jaymode  Would it be possible to include an example of combining all three in one query; e.g., a search for A and B not C or D? Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Facets: Histogram Facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/215</link><project id="" key="" /><description>Support histogram facets for numeric data. The idea is that values are rounded into the closest bucket based on the interval provided (for example, for interval `100`, then `1055` will be rounded to `1000`) and then for each bucket the count, total, and mean values are provided.

Here is an example:

```
{
    query : {
        "match_all" : {}
    },
    facets : {
        "histo1" : {
            "histogram" : {
                "field" : "multi_num",
                "interval" : 100
            }
        }
    }
}
```
</description><key id="216836">215</key><summary>Search Facets: Histogram Facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-08T12:45:21Z</created><updated>2010-06-08T12:46:04Z</updated><resolved>2010-06-08T12:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-08T12:46:04Z" id="267864">Search Facets: Histogram Facets, closed by 47b3a81bec33a94380f9567af6346fda3c61346f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multicast fails with network interfaces instead of IP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/214</link><project id="" key="" /><description>Environment: two laptops on MacOSX 10.5 Leopard, in the same local network on Wifi (interface "en1"). The two laptops have virtual network devices (VMWare/Parallels/VirtualBox).

If don't give any configuration to elasticsearch, the two nodes do not discover each other.
If I set network.host to _en1_ on each host, I get these errors on the master http://pastie.org/995083
If I set network.host to the actual IPs, the two nodes go in the same cluster.
</description><key id="216176">214</key><summary>multicast fails with network interfaces instead of IP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nfo</reporter><labels /><created>2010-06-07T15:47:48Z</created><updated>2013-04-04T18:12:22Z</updated><resolved>2013-04-04T18:12:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-08T12:42:19Z" id="267861">How do you specify the `network.host` to be `en1`? It should be `_en1_`.
</comment><comment author="nfo" created="2010-06-08T13:04:56Z" id="267880">I set `network.host` to `_en1_`. (was a typo)
</comment><comment author="clintongormley" created="2013-04-04T18:12:22Z" id="15913937">No further reports of this in 3 years. Already fixed? Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Support fuzzy query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/213</link><project id="" key="" /><description>Here are some samples:

```
{
    "fuzzy" : { "name.first" : "sh" }
}
```

and:

```
{
    "fuzzy" : {
        "name.first" :  {
            "value" : "sh",
            "min_similarity" : 0.1,
            "prefix_length" : 1,
            "boost" : 2.0
        }
    }
}
```
</description><key id="216145">213</key><summary>Query DSL: Support fuzzy query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-07T15:12:55Z</created><updated>2010-06-07T15:14:15Z</updated><resolved>2010-06-07T15:14:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-07T15:14:14Z" id="266538">Query DSL: Support fuzzy query, closed by 26ed029d8a9c3faee8aaf2d2815171f9b0280249.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Facets: Numeric Statistical Facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/212</link><project id="" key="" /><description>Support getting numeric statistical data (min, max, total, count) for numeric fields:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "test" : {
            "statistical" : {
                "field" : "my_numeric_field"
            },
            "global" : false
        }
    }
}
```
</description><key id="216125">212</key><summary>Search Facets: Numeric Statistical Facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-07T14:45:48Z</created><updated>2010-06-07T14:46:10Z</updated><resolved>2010-06-07T14:46:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-07T14:46:09Z" id="266504">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>partial updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/211</link><project id="" key="" /><description>Given the following 'scheme'
    {
        post:"string",
        postId:1,
            comments: [{
            comment:"comment",
            commentId:1,
            author:"author name"
        }]
     }

If the author name changes across the board it would be nice to be able to tell elasticsearch "update posts where comments.author == "author name" set comments.author = "new author name" 

Taking it one step further, the abillity to link to a value in another document:
    {
        post:"string",
        postId:1,
            comments: [{
            comment:"comment",
            commentId:1,
            _author: "@@/myapp/users/3/fullName",
            authorId:32 //unlikely to change
        }]
     }

This would allow me to just update /myapp/users/3 and forget about dependencies all together. This is something I miss in all nosql/search data stores.
</description><key id="215293">211</key><summary>partial updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2010-06-06T09:51:32Z</created><updated>2011-08-17T07:34:14Z</updated><resolved>2010-11-09T22:49:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-06T10:36:38Z" id="265473">Not so simple to implement..., certainly not for something built on top of Lucene. Its usually inherent in how nosql solutions are structured. It would be nice to have it, but, to be perfectly honest, I am not going to invest time on this feature in the next couple of versions..., there are more basic features that needs to be implemented.
</comment><comment author="Mpdreamz" created="2010-06-07T18:14:10Z" id="266849">Too true, this is one of those things that would take it one step further. I can understand this not having top priority, as a long term goal it could/would really be a feature to stand out amongst the croud. 

Not sure if you are aware but lucene is definitly moving towards supporting it already:
http://wiki.apache.org/lucene-java/ParallelIncrementalIndexing
and
https://issues.apache.org/jira/browse/LUCENE-1879

As a sidenote: RavenDB has implemented partial updates last week.
</comment><comment author="kimchy" created="2010-06-09T15:28:02Z" id="269481">I am aware of the work done in Lucene to support it, of course, if its going to be built into Lucene, elasticsearch will be able to expose it easily.

Not too familiar with RavenDB, is that a search engine?
</comment><comment author="apatrida" created="2010-11-09T14:36:18Z" id="528964">Basically, ES might have to decide if it wants to also be the authoritative store or not for the documents.  If it is to be, then it has to have the base features to make that work-able; if it is not going to be then it can be clearly stated that the features around that will not be forthcoming (or not until a release where that decision changes).  

But there are a lot of factors around the "level" of being that store.  Transactions or not (I say no), partial updates (maybe), real-time retrieval by doc id (I vote yes), ...
</comment><comment author="Mpdreamz" created="2010-11-09T14:49:19Z" id="529006">My personal view on search/full text engines is that they should never be authoritative (provided that means leading datastore).  You should be free to optimize for maximum search throughput without taking data model requirements into account. 

Partial updates would still be a kick ass addition in the 1.0 releases though, whenever lucene implements it.
</comment><comment author="apatrida" created="2010-11-09T14:58:22Z" id="529028">&gt; My personal view on search/full text engines is that they should never be authoritative 

Unless you are creating a new hybrid that should be authoritative.

You can optimize for search through many means:  offline the blob storage for the doc out of the index; having a designation of query slaves that have allowable lag while indexer machines have realtime retrieval by ID and near-realtime by query, etc.

Right now ES is doing both in the same nodes at the same time.  It may want to allow this divide down the road so that you are not affecting query "slaves" with refresh requests or with the need for more accurate (in terms of time) document retrieval.

A little bit more movement of ES down this road and it COULD be the best of both worlds.
</comment><comment author="karmi" created="2011-08-17T07:34:14Z" id="1824123">MongoDB [has partial updates](http://www.mongodb.org/display/DOCS/Updating#Updating-ModifierOperations). With embedded documents, it is pushed as a superior feature. In practice, I think the usage is quite limited to a certain class of problems, and of course, the implications for durability may be horrible for you. AFAIK, Riak has a concept similar to presented by @mpdreamz with the _links_ feature?

I like the idea of "external field", as discussed [here](http://groups.google.com/group/elasticsearch/browse_thread/thread/c6f35567420f29dc), much better. If I am not mistaken, the [nested type](http://www.elasticsearch.org/guide/reference/mapping/nested-type.html) allows for splitting the document into couple of separate ones, thus making the update more performant?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flush API: Allow to provide `full` parameter for a complete clean</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/210</link><project id="" key="" /><description>When specifying full, it will completely clean the engines (basically, replacing IndexWriter).

Also, the memory monitor will perform it when getting into its full stage cleaning (cleaning the cache as well).
</description><key id="214682">210</key><summary>Flush API: Allow to provide `full` parameter for a complete clean</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-05T03:09:33Z</created><updated>2010-06-05T03:10:10Z</updated><resolved>2010-06-05T03:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-05T03:10:10Z" id="264663">Flush API: Allow to provide `full` parameter for a complete clean, closed by 5cdba0383b08b24e6d829975b308e60bc81fc459.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Search requests hangs when no indices exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/209</link><project id="" key="" /><description>It should basically throw an exception, return an error.
</description><key id="214593">209</key><summary>Search: Search requests hangs when no indices exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-06-04T22:35:37Z</created><updated>2010-06-04T22:36:30Z</updated><resolved>2010-06-04T22:36:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-04T22:36:30Z" id="264541">Search: Search requests hangs when no indices exists, closed by bcbc0dd7414b26fdb4479f0bc9ac269d46f335a6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Allow to easily plug a custom DSL query/filter parsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/208</link><project id="" key="" /><description>Within a plugin, allow to more easily plug in a custom `XContentQueryParser` and `XContentFilterParser`. It follows the same model as the analysis extension module, and should look something like this:

```
public class Customlugin extends AbstractPlugin {

    @Override public String name() {
        return "custom";
    }

    @Override public String description() {
        return "My custom plugin";
    }

    @Override public void processModule(Module module) {
        if (module instanceof IndexQueryParserModule) {
            IndexQueryParserModule queryParserModule = (IndexQueryParserModule) module;
            queryParserModule.addProcessor(new IndexQueryParserModule.QueryParsersProcessor() {
                @Override public void processXContentQueryParsers(XContentQueryParsersBindings bindings) {
                    bindings.processXContentQueryParser("my_name", MyNameQueryParser.class);
                }
            });
        }
    }
}
```
</description><key id="214577">208</key><summary>Plugins: Allow to easily plug a custom DSL query/filter parsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-04T22:04:32Z</created><updated>2010-06-04T22:07:19Z</updated><resolved>2010-06-04T22:07:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-04T22:07:19Z" id="264516">Plugins: Allow to easily plug a custom DSL query/filter parsers, closed by d0eb836c4a1ea0df3e7b7fb291ea9efbbc516e01.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Facets: Terms Facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/207</link><project id="" key="" /><description>Allow to specify field facets that return the N most frequent terms either globally or bounded by the search query. Sample search request:

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "test" : {
            "terms" : {
                "field" : "test",
                "size" : 10
            },
            "global" : false
        }
    }
}
```
</description><key id="213829">207</key><summary>Search Facets: Terms Facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-03T23:15:22Z</created><updated>2010-11-09T16:07:55Z</updated><resolved>2010-06-04T06:31:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-03T23:31:59Z" id="263436">Search Facets: Field Facets, closed by 66c9f2f8344bdf9e58e0bc8a9fb59a527ec547cf
</comment><comment author="bjon" created="2010-06-08T00:35:07Z" id="267285">Is it possible to implement "from" (offset)? So we can have both from/size as in other searches.
</comment><comment author="kimchy" created="2010-06-09T15:22:53Z" id="269469">not sure I understand the from part. Do you want to be able to filter out certain terms that construct the facet from the result?
</comment><comment author="bjon" created="2010-06-09T15:56:07Z" id="269527">No like this.

{
     "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "test" : {
            "terms" : {
                "field" : "test",
                "from" : 30,
                "size" : 10
            },
            "global" : false
        }
    }
 }

That would return the (position) 30-39 most frequent terms instead of 0-9
</comment><comment author="kimchy" created="2010-06-10T10:30:34Z" id="270510">Thats problematic to compute in a distributed system, what I would suggest is simply asking for 40, it will have a smaller overhead than trying to solve it properly in another way.
</comment><comment author="apatrida" created="2010-11-09T16:07:55Z" id="529195">It can be easily solved in the distributed system if you don't care about consistentcy between calls.  I have seen cases where you ask for 100, but need the next 100, then the next and it is outside what you would want to ask for in a single query, but have the chance to page.  

I do agree that for small counts, you should just ask for more rather than page the counts.

Otherwise you can allow it with no guarantee of consistency (not like scrolling which tries to maintain a snapshot)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Optimize Gateway recovery, recover only changed data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/206</link><project id="" key="" /><description>Gateway: Optimize Gateway recovery, recover only changed data. Basically, because of the write once nature of files, the local location of the index can be kept, and then only the diffs can be copied from the gateway.

Thats a bit tricky with how elasticsearch works now. The ability to start more than one node on a machine, and the fact that the data local data is deleted when the node is bought down. The local data can be kept around if there is a configured gateway, and deleted if there isn't.
</description><key id="213339">206</key><summary>Gateway: Optimize Gateway recovery, recover only changed data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-06-03T11:00:31Z</created><updated>2010-07-14T18:33:00Z</updated><resolved>2010-07-14T18:33:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-07-14T18:33:00Z" id="312314">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return the maxScore per search and score for each search hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/205</link><project id="" key="" /><description>Return `max_score` for each search request, and `_score` for each hit. Though they, most times, does not mean a lot...
</description><key id="212976">205</key><summary>Return the maxScore per search and score for each search hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mhorbul</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-02T23:46:30Z</created><updated>2010-06-19T21:41:41Z</updated><resolved>2010-06-20T04:23:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-19T21:23:40Z" id="281357">Return the maxScore per search and score for each search hit, closed by 384f8a4f4268b7261ac5f4807f91f2346abee495.
</comment><comment author="mhorbul" created="2010-06-19T21:41:41Z" id="281371">Thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>real time visibility of data for get requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/204</link><project id="" key="" /><description>Currently ElasticSearch has a near real-time engine which means when data is indexed, it is not immediately searchable/visible. This is a problem in some cases where data needs to be retrieved right after it is indexed. 
If get operation (or a version of it) can work in real time, it would be sufficient to meet some real time scenarios even without a fully real time engine.
</description><key id="212802">204</key><summary>real time visibility of data for get requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">berkay</reporter><labels /><created>2010-06-02T20:13:33Z</created><updated>2011-06-27T21:25:16Z</updated><resolved>2011-06-27T21:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talsalmona" created="2010-06-03T19:41:20Z" id="263175">Wouldn't a "refresh" of the index right before the search solve this?
</comment><comment author="berkay" created="2010-06-03T19:47:07Z" id="263183">yes, but calling refresh before ever index operation would have a very high performance penalty, hence not really a feasible solution when performance is a concern.
</comment><comment author="apatrida" created="2010-11-09T14:33:01Z" id="528953">Would it help to be able to retrieve by document ID immediately, but search be slightly delayed?  That might be more solvable without calling refresh.  So you can use it as an authoritative store for documents in realtime, but for search it is near-realtime.
</comment><comment author="karussell" created="2011-01-02T15:33:03Z" id="646570">@jaysonminard This would be a killer feature for ES. At the moment I'm using solr and an in memory cache to circumvent that issue.
</comment><comment author="karussell" created="2011-05-21T20:05:39Z" id="1215380">This can be closed because one can implement real time failure via #594 ?
</comment><comment author="kimchy" created="2011-05-22T04:21:00Z" id="1216311">Its not complete realtime for the actual source. One possible solution for get request can eb to also store the get requests in memory keyed by the id (until flushing / next refresh). Though, this can get tricky in terms of memory managemnet.
</comment><comment author="karussell" created="2011-06-27T13:27:57Z" id="1447279">Now, does #1060 solve this?
</comment><comment author="kimchy" created="2011-06-27T21:25:16Z" id="1450579">Yep, thanks for helping doing some issue cleanup, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API does not expose node-master status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/203</link><project id="" key="" /><description>There is no way how to learn which node is master via REST API. Cluster state or Nodes Info should expose this info?
</description><key id="212466">203</key><summary>REST API does not expose node-master status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-02T13:48:24Z</created><updated>2010-06-02T15:44:47Z</updated><resolved>2010-06-02T15:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-02T13:59:37Z" id="261382">Will add it to the cluster state API.
</comment><comment author="kimchy" created="2010-06-02T15:44:47Z" id="261553">REST API does not expose node-master status, closed by 6c8f49c37d03bc6f7ca66d6839b3d68106f9d84d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FS Gateway: Allow to configure is native file copying will be used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/202</link><project id="" key="" /><description>Using the `index.gateway.fs.native_copy` setting, native file copying can be turned on or off. By default, it is set to `true`. Set it to `false` to disable native file copying (and use buffers instead).
</description><key id="212312">202</key><summary>FS Gateway: Allow to configure is native file copying will be used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-06-02T09:06:20Z</created><updated>2010-07-25T20:03:57Z</updated><resolved>2010-06-02T22:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-02T15:44:47Z" id="261552">FS Gateway: Allow to configure is native file copying will be used, closed by c2d583064e95b1bb06f102a7d3610442a2ed8038.
</comment><comment author="kimchy" created="2010-07-25T20:03:57Z" id="326247">No longer relevant, always not using native copy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes Info API: Failed to generate REST response when node attributes are set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/201</link><project id="" key="" /><description>For example, when setting `node.data` set to `false`. 
</description><key id="211931">201</key><summary>Nodes Info API: Failed to generate REST response when node attributes are set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-06-01T22:10:10Z</created><updated>2010-06-01T22:10:48Z</updated><resolved>2010-06-01T22:10:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-01T22:10:48Z" id="260611">Nodes Info API: Failed to generate REST response when node attributes are set, closed by 5ef421e77969c60f326a83d5e246e67bec53b0ed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: ungraceful shutdown of the master and start of replacement node might cause the cluster not to elect a new master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/200</link><project id="" key="" /><description>If killing -9 the master, and then starting a new node right away on the same box might cause the cluster to get into a state where a new master is not elected. This is because the new node will use the same port and it will be considered as the failed node.
</description><key id="211789">200</key><summary>Zen Discovery: ungraceful shutdown of the master and start of replacement node might cause the cluster not to elect a new master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.09.0</label></labels><created>2010-06-01T19:29:08Z</created><updated>2010-06-01T19:29:48Z</updated><resolved>2010-06-01T19:29:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-01T19:29:48Z" id="260418">Zen Discovery: ungraceful shutdown of the master and start of replacement node might cause the cluster not to elect a new master, closed by a7ad295f63aaf3b6073cbbe6d0ca684031b84aab.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Simplify Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/199</link><project id="" key="" /><description>Add simplified execution model for all APIs that require less imports, more concise and simpler to use. For example: 

```
IndexResponse indexResponse = client1.prepareIndex()
    .setIndex("test")
    .setType("type1")
    .setId("1")
    .setSource(source("1", "test"))
    .execute().actionGet();

SearchResponse searchResponse = client.prepareSearch()
        .setIndices("test")
        .setSearchType(QUERY_THEN_FETCH)
        .setQuery(termQuery("_all", "test"))
        .setFrom(0).setSize(60)
        .addHighlightedField("_all").setHighlighterOrder("score").setHighlighterPreTags("&lt;xxx&gt;").setHighlighterPostTags("&lt;/xxx&gt;")
        .setScroll(timeValueMinutes(10))
        .execute().actionGet();
```
</description><key id="209492">199</key><summary>Java API: Simplify Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-05-29T19:23:18Z</created><updated>2010-05-29T19:23:45Z</updated><resolved>2010-05-29T19:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-29T19:23:45Z" id="257769">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Occassionally, queries return results from the wrong index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/198</link><project id="" key="" /><description>Hiya

See the query below - I query the alias `ia_object` which points to the index `ia_object_1274409377`, for objects of type `notice`, but the resuts returned include results from my other index `iannounce_object_1274399922` of types other than `notice`.

I'm wondering if one node has dispatched the query, then picked up the wrong resultset?

```
curl -XGET 'http://idb2:9200/ia_object/notice/_search?search_type=dfs_query_then_fetch'  -d '
{
   "sort" : [
      {
         "score" : "desc"
      }
   ],
   "fields" : [],
   "from" : 0,
   "query" : {
      "filtered" : {
         "filter" : {
            "bool" : {
               "must" : [
                  {
                     "term" : {
                        "sub_type" : "engagement"
                     }
                  },
                  {
                     "term" : {
                        "status" : "active"
                     }
                  },
                  {
                     "term" : {
                        "location_id" : "40"
                     }
                  }
               ]
            }
         },
         "query" : {
            "dis_max" : {
               "queries" : [
                  {
                     "query_string" : {
                        "fields" : [
                           "name"
                        ],
                        "boost" : 1,
                        "query" : "\"David Mcgeoch\"~7"
                     }
                  },
                  {
                     "filtered" : {
                        "filter" : {
                           "term" : {
                              "has_name" : "0"
                           }
                        },
                        "query" : {
                           "query_string" : {
                              "fields" : [
                                 "text"
                              ],
                              "boost" : 1.3,
                              "query" : "\"David Mcgeoch\"~7"
                           }
                        }
                     }
                  },
                  {
                     "query_string" : {
                        "boost" : 1,
                        "query" : "David Mcgeoch"
                     }
                  }
               ],
               "tie_breaker" : 0.7
            }
         }
      }
   },
   "size" : 300
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_index" : "iannounce_object_1274399922",
#             "_id" : "3686897",
#             "_type" : "memorial"
#          },
#          {
#             "_index" : "ia_object_1274409377",
#             "_id" : "1336845",
#             "_type" : "notice"
#          },
#          {
#             "_index" : "iannounce_object_1274399922",
#             "_id" : "2081719",
#             "_type" : "memorial"
#          },
#          {
#             "_index" : "iannounce_object_1274399922",
#             "_id" : "384424",
#             "_type" : "memorial"
#          },
#          {
#             "_index" : "iannounce_object_1274399922",
#             "_id" : "234160",
#             "_type" : "memorial"
#          },
#          {
#             "_index" : "iannounce_object_1274399922",
#             "_id" : "238852",
#             "_type" : "memorial"
#          },

[ SNIPPED ]

#       ],
#       "total" : 7390
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }
```

There were some errors in the ES log around that time:

```
[07:14:35,985][DEBUG][action.search.type       ] [Ringleader] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Hulkling][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [9785964], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[07:14:35,986][DEBUG][action.search.type       ] [Ringleader] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Hulkling][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [9785961], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[07:14:35,986][DEBUG][action.search.type       ] [Ringleader] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Hulkling][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [9785965], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[07:14:35,991][DEBUG][action.search.type       ] [Ringleader] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Hulkling][inet[/192.168.10.50:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [9785963], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[07:14:36,212][DEBUG][action.search.type       ] [Ringleader] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [4594371], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
```
</description><key id="208520">198</key><summary>Occassionally, queries return results from the wrong index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-05-28T08:36:24Z</created><updated>2011-07-27T16:59:15Z</updated><resolved>2011-07-27T16:59:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-28T15:08:54Z" id="256788">Hey,

  Very, very strange. Lets start with the second failures. Is there a chance that you get a long search requests or load that is higher than 5 minutes? The default timeout between a query phase and a fetch phase is 5 minutes (or controlled by setting the keep alive parameter).

Second, regarding the first problem. This is very very strange. I think your assumption is correct, and for some reason, requests got mixed up. I don't see where it can happen in elasticsearch, but I will investigate further. It must be in the HTTP layer, and thats pretty straightforward. It might also be on the http client side..., trying to think how best to nail down where and why this happens.... (well, it can also be in the logging layer :) )
</comment><comment author="clintongormley" created="2010-05-28T15:15:42Z" id="256792">Re long search requests - I don't honestly know.  I haven't seen that happening, but my app times out requests after 15 seconds, then retries, so it may well be hidden.

I'm certainly not seeing anything unusual regarding CPU or memory use in jvisualvm.

I think you'll need to add some more logging to nail it down.  Also, this is v 0.7.1 - I haven't upgraded yet.
</comment><comment author="kimchy" created="2010-05-28T20:34:12Z" id="257108">When I said client side, I meant the perl http client, which might mix messages up, if that was not obvious :). I am trying to think of the best logging to add. I can generate a unique id for each rest request when it is received, and log the request, and then, log the response sent under that unique id, what do you think?
</comment><comment author="clintongormley" created="2010-05-29T08:50:54Z" id="257423">I think it is highly unlikely that it is happening on the Perl side. I'm using a core module that has been stable for many years - something as obvious as this would have been noticed before.  Also, it is not an async request, so nothing to mix up there.

Re logging - you can add that if you like, but i think you're probably looking in the wrong place.  The reason I posted the errors was that they happened at the same time as the bad results (i can't be certain that they were generated by the same request or not).  I think it is likelier that the problem is node communication getting mixed up, while retrieving results from the other shards.

but what do I know :)
</comment><comment author="kimchy" created="2010-05-29T14:09:39Z" id="257593">I agree with you, the perl side is probably not to blame. 

So, some background. You execute a dfs query then fetch. Thats three phases, first, the dfs, that goes to all shards. Then, query, that goes to all shards with the dfs result. Last, the fetch, that goes to the relevant shards.

A certain shard communication might have got messes up, which I am trying to understand if can happen. In your response (if you still have all of it) does it look like some of the response are correct, while others are wrong?

If the response looks like simply a response for a another query, then its more high level then that, if its a mixed result, with some good and some bad, then its a messed up specific shard communication in the search.

When do you plan to upgrade to 0.8? It will be easier to help with that version.
</comment><comment author="clintongormley" created="2010-05-29T14:33:42Z" id="257606">I don't have the full results any more, but looking at the above, the second hit matches the query correctly, but all the others (i think it was all) were from the wrong index.

Hopefully I'll be on 0.8 next weekend. It won't be this weekend.
</comment><comment author="kimchy" created="2010-05-29T17:04:58Z" id="257702">Yea, missed the second hit. Ok, I will look into this more try and see why this might happen. Ping me when you upgrade to 0.8.
</comment><comment author="clintongormley" created="2010-06-11T09:11:55Z" id="271664">Bad news, kimchy - i'm still getting this error on v 0.8
</comment><comment author="kimchy" created="2010-06-11T19:57:32Z" id="272350">Do you still get the exceptions in the logs?
</comment><comment author="clintongormley" created="2010-06-12T11:33:31Z" id="272847">There are still exceptions in the logs, but the times don't coincide, so I'm not sure that they're related:

```
[02:20:53,165][DEBUG][action.search.type       ] [Fontanelle] [12798594] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [12798594], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
    at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[02:41:38,114][DEBUG][action.search.type       ] [Fontanelle] [6167654] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [6167654], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[02:41:38,114][DEBUG][action.search.type       ] [Fontanelle] [6167652] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [6167652], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[02:41:38,115][DEBUG][action.search.type       ] [Fontanelle] [6167653] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [6167653], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[02:41:38,160][DEBUG][action.search.type       ] [Fontanelle] [12820997] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [12820997], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
    at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[03:38:57,733][DEBUG][action.search.type       ] [Fontanelle] [12872042] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: java.lang.NullPointerException
[03:40:05,941][DEBUG][action.search.type       ] [Fontanelle] [6193567] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [iannounce_object_1275333493][0]: query[filtered((name:"pamela goody"~7 | filtered(short_text:"pamela goody"~7^1.3)-&gt;FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3e3f4f7) | (_all:pamela _all:goody))~0.7)-&gt;FilterCacheFilterWrapper(BooleanFilter( +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@33d4352b) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@40ea8cf6) +FilterCacheFilterWrapper(parent_id:[1 TO 1])))],from[0],size[300],sort[&lt;score&gt;]: Query Failed []
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:103)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:193)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:159)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:114)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term pamela not available
    at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
    at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.&lt;init&gt;(PhraseQuery.java:122)
    at org.apache.lucene.search.PhraseQuery.createWeight(PhraseQuery.java:264)
    at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.&lt;init&gt;(DisjunctionMaxQuery.java:107)
    at org.apache.lucene.search.DisjunctionMaxQuery.createWeight(DisjunctionMaxQuery.java:184)
    at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
    at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
    at org.apache.lucene.search.Query.weight(Query.java:101)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:78)
    at org.apache.lucene.search.Searcher.search(Searcher.java:49)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:97)
    ... 8 more
[03:52:03,125][DEBUG][action.search.type       ] [Fontanelle] [12881061] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: java.lang.ArrayIndexOutOfBoundsException
[03:52:03,142][DEBUG][action.search.type       ] [Fontanelle] [12881063] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [12881063], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
    at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:15:55,961][DEBUG][index.gateway            ] [Fontanelle][iannounce_object_1275333493][3] Snapshotting on close ...
[13:15:56,470][DEBUG][index.shard.recovery     ] [Fontanelle][iannounce_object_1275333493][3] Starting recovery from [Blue Bullet][b6b87b7e-89da-4644-b17a-2fb638b16041][inet[/192.168.10.41:9300]]
[13:16:50,518][DEBUG][index.shard.recovery     ] [Fontanelle][iannounce_object_1275333493][3] Recovery completed from [Blue Bullet][b6b87b7e-89da-4644-b17a-2fb638b16041][inet[/192.168.10.41:9300]], took[54s], throttling_wait [0s]
   Phase1: recovered [27] files with total size of [1.2g], took [53.5s], throttling_wait [0s]
   Phase2: recovered [1644] transaction log operations, took [390ms]
   Phase3: recovered [0] transaction log operations, took [65ms]
[22:34:33,612][DEBUG][action.search.type       ] [Fontanelle] [14161052] Failed to execute query phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/query/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [14161052], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:185)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:388)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:379)
    at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[22:34:33,630][DEBUG][action.search.type       ] [Fontanelle] [6837580] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [6837580], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[22:34:33,630][DEBUG][action.search.type       ] [Fontanelle] [6837581] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [6837581], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[22:34:33,631][DEBUG][action.search.type       ] [Fontanelle] [6837579] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [6837579], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:309)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:226)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[22:34:33,720][DEBUG][action.search.type       ] [Fontanelle] [14161053] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [14161053], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
    at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[22:34:33,722][DEBUG][action.search.type       ] [Fontanelle] [14161054] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Blue Bullet][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [14161054], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:274)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:444)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:435)
    at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:175)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
```
</comment><comment author="clintongormley" created="2011-05-20T14:26:04Z" id="1210291">In 0.16.2, an occurrence of this problem (ie ES returning the wrong results) coincided with this error in the logs:

Caught exception while handling client http traffic, closing connection
java.lang.IllegalStateException: cannot send more responses than requests

Full logs https://gist.github.com/85a707a6edf8e8534b4a
</comment><comment author="clintongormley" created="2011-07-27T16:59:15Z" id="1665551">Fixed in issue #1152
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud Discovery does not appear to work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/197</link><project id="" key="" /><description>https://gist.github.com/3d41aa8b3ee85f77392f
</description><key id="208310">197</key><summary>Cloud Discovery does not appear to work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fujin</reporter><labels /><created>2010-05-28T01:10:02Z</created><updated>2013-04-04T18:11:06Z</updated><resolved>2013-04-04T18:11:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-28T08:30:46Z" id="256455">From what I understand, it was a glitch in amazon failing on one of the APIs. can you give it another go?
</comment><comment author="idris" created="2010-06-20T19:02:34Z" id="281988">I'm getting the same message. Something like:
[WARN ][cloud.jclouds.aws.ec2.compute.functions.RunningInstanceToNodeMetadata] [Karkas] no tag parsed from i-XXXXXX's groups: [ElasticSearch]

Looks like a problem with jclouds perhaps?  Discovery is not working at all. The two instances just run as two separate master instances that know nothing about each other.
</comment><comment author="kimchy" created="2010-06-20T22:36:16Z" id="282196">Yea, its probably with jclouds. I am thinking to replace it in 0.9 with native calls to aws APIs... since I need so little from it. We'll see.
</comment><comment author="mootpointer" created="2010-07-12T00:16:04Z" id="307990">If you wait long enough, it does eventually work (or at least it does for me). This is on 0.8 and master.
</comment><comment author="clintongormley" created="2013-04-04T18:11:04Z" id="15913866">Jclouds has been replaced by EC2 discovery module. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replication Actions: Allow to control replication type - `async` or `sync`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/196</link><project id="" key="" /><description>Allow to control the replication type for index, delete, and delete_by_query operations. The replication type defaults to the node replication setting: `action.replication_type`, which in turn, defaults to `sync`.

The replication type can also be set on a per operation basis using the `replication` parameter accepting the `sync`, and  `async` values.
</description><key id="208302">196</key><summary>Replication Actions: Allow to control replication type - `async` or `sync`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.09.0</label></labels><created>2010-05-28T00:46:37Z</created><updated>2010-05-28T00:47:45Z</updated><resolved>2010-05-28T00:47:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-28T00:47:45Z" id="256161">Replication Actions: Allow to control replication type - `async` or `sync`, closed by 84a5c1eac82150a276ca65fba7daef60402f408f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: Improve Multicast Binding and Sending</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/195</link><project id="" key="" /><description>By default, bind on all network interfaces allowed, and send on all network interfaces allowed.
</description><key id="207322">195</key><summary>Zen Discovery: Improve Multicast Binding and Sending</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2010-05-26T23:04:33Z</created><updated>2013-07-16T08:32:00Z</updated><resolved>2013-07-16T08:32:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T08:32:00Z" id="21028166">this is long implemented.. closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Network Settings: Allow to use '_' as well as '#' for special host names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/194</link><project id="" key="" /><description>For example, allow to use `_local` as well as `#local#`. This make sense since in yaml, # is a comment :).
</description><key id="206844">194</key><summary>Network Settings: Allow to use '_' as well as '#' for special host names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-26T13:32:11Z</created><updated>2010-05-26T13:33:44Z</updated><resolved>2010-05-26T13:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-26T13:33:44Z" id="254280">Network Settings: Allow to use '_' as well as '#' for special host names, closed by 9433f4d6516fd91ea9fd560f08c9a818fbdd4e50.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to acquire lock</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/193</link><project id="" key="" /><description>Exception:
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [crawled][2]: Recovery failed from [ISAAC]
[...]
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/mnt/elasticsearch-0.7.1/work/elasticsearch/indices/cc4a7883-10b1-438d-a844-13930060205e/crawled/2/index/write.lock

happened while bulk-inserting data
</description><key id="206703">193</key><summary>Failed to acquire lock</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rb2k</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-26T09:04:59Z</created><updated>2010-05-26T13:33:44Z</updated><resolved>2010-05-26T13:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticsearch" created="2010-05-26T09:28:20Z" id="254066">I think this might have to do with AWS. I will change things so there will be, by default, no locking since always a single shard is using the index.
</comment><comment author="kimchy" created="2010-05-26T13:33:44Z" id="254279">Failed to acquire lock, closed by 26364afd7e9e377b8449ab0c2ad3ac301f3bebf6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Streamline single value with boost queries: Allow for both value and query name to specify the query value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/192</link><project id="" key="" /><description>As of now the prefix query accepts the following syntax:

```
{
    "prefix" : { "user" :  { "prefix" : "ki", "boost" : 2.0 } }
}
```

It would be natural to support value attribute as well. See the following example:

```
{
    "prefix" : { "user" :  { "value" : "ki", "boost" : 2.0 } }
}
```

This should be streamlined to also `term`, `wildcard`, and `span_term`, to allow for both `value` and the query name.
</description><key id="205394">192</key><summary>Streamline single value with boost queries: Allow for both value and query name to specify the query value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.09.0</label></labels><created>2010-05-25T07:32:26Z</created><updated>2010-05-28T09:23:58Z</updated><resolved>2010-05-28T09:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-28T09:23:58Z" id="256506">Streamline single value with boost queries: Allow for both value and query name to specify the query value, closed by 3406e77cec8aa3692e73957fa91d397e5c5698f5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway Storage: Improve it to support non breaking changes in the future</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/191</link><project id="" key="" /><description>Note, this is a breaking change, and requires reindexing of the data. Don't start elasticsearch after the upgrade against an existing gateway.
</description><key id="204802">191</key><summary>Gateway Storage: Improve it to support non breaking changes in the future</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.08.0</label></labels><created>2010-05-24T15:26:37Z</created><updated>2010-05-24T15:27:56Z</updated><resolved>2010-05-24T15:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-24T15:27:55Z" id="251713">Gateway Storage: Improve it to support non breaking changes in the future, closed by b7d11f1303f7cff45d85f242cf46038d46fb021f.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet Queries in count API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/190</link><project id="" key="" /><description>would be great Facets Queries in count API
</description><key id="204697">190</key><summary>Facet Queries in count API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tfreitas</reporter><labels /><created>2010-05-24T13:01:30Z</created><updated>2010-07-14T08:34:39Z</updated><resolved>2010-07-14T08:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tfreitas" created="2010-06-16T02:44:33Z" id="276858">In the same way
curl-XGET 'http://localhost:9200/twitter/tweet/_search'
have FACET QUERIES in
curl-XGET 'http://localhost:9200/twitter/tweet/_count'
</comment><comment author="kimchy" created="2010-07-14T08:34:39Z" id="311486">It would work exactly the same way as search with `size` 0. I will close this unless there is an objection to using the purposed solution.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hadoop Plugin: Use HDFS as gateway storage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/189</link><project id="" key="" /><description>A new plugin, called hadoop, to provide support for using HDFS as the gateway storage. Configuration is simple:
- `gateway.type` - set to `hdfs`.
- `gateway.hdfs.uri` - set to something like `hdfs://hostname:port`.
- `gateway.hdfs.path` - the path to store the data under.
</description><key id="204078">189</key><summary>Hadoop Plugin: Use HDFS as gateway storage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.08.0</label></labels><created>2010-05-23T14:01:36Z</created><updated>2010-05-23T14:02:15Z</updated><resolved>2010-05-23T14:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-23T14:02:15Z" id="250725">Hadoop Plugin: Use HDFS as gateway storage, closed by 28fa384b324b904d3e33d30e644f0971306b79da.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FS Gateway: Change indices location (Requires manual upgrade)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/188</link><project id="" key="" /><description>The current location of each index gateway when using file system based gateway is wrong. It should exists under an "indices" directory, and no on the same level as the metadata directory. An upgrade requires moving all the directories except for the metadata one into an `indices` directory.
</description><key id="203938">188</key><summary>FS Gateway: Change indices location (Requires manual upgrade)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-23T05:21:33Z</created><updated>2010-05-23T05:37:46Z</updated><resolved>2010-05-23T05:37:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-23T05:37:46Z" id="250515">FS Gateway: Change indices location (Requires manual upgrade), closed by 116cfce6f2b67ee5684b56a1ffec29722f8e4b05.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize API: Change flush and refresh to default to true and not false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/187</link><project id="" key="" /><description>Make more sense to do a flush and refresh after optimize by default then not.
</description><key id="202937">187</key><summary>Optimize API: Change flush and refresh to default to true and not false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-21T23:38:14Z</created><updated>2010-05-21T23:38:44Z</updated><resolved>2010-05-21T23:38:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-21T23:38:44Z" id="249044">Optimize API: Change flush and refresh to default to true and not false, closed by 6191bf9cdc81862026f42067c9e460b250389fc1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud Plugin: Cloud gateway default chunk size change to 1g</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/186</link><project id="" key="" /><description>For two reasons, the first is a bug in jclouds which should be fixed in the next version that effectively can't handle files larger than 2g.

The second reason is that chunks are now snapshotted in parallel, so chunking will make snapshot operation faster.
</description><key id="202909">186</key><summary>Cloud Plugin: Cloud gateway default chunk size change to 1g</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-21T22:08:11Z</created><updated>2010-05-21T22:08:50Z</updated><resolved>2010-05-21T22:08:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-21T22:08:50Z" id="248995">Cloud Plugin: Cloud gateway default chunk size change to 1g, closed by ecc74f225e8bf36f806450ffb97efee0082f7ec1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Network Settings: Default publish host to first non loopback (first ipv4 then ipv6)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/185</link><project id="" key="" /><description>Try and use a non loopback interface first, with first trying ipv4, then ipv6, and if none found, use the local address (which was the default before this change)
</description><key id="201990">185</key><summary>Network Settings: Default publish host to first non loopback (first ipv4 then ipv6)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-20T19:30:49Z</created><updated>2010-05-23T05:37:29Z</updated><resolved>2010-05-23T05:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-23T05:37:28Z" id="250512">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Network Settings: Add more #...# logical values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/184</link><project id="" key="" /><description>Add `non_loopback`, `non_loopback:ipv4` and `non_loopback:ipv6`. Chooses the first non loopback interface for different host type settings.
</description><key id="201988">184</key><summary>Network Settings: Add more #...# logical values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-20T19:29:16Z</created><updated>2010-05-20T19:29:28Z</updated><resolved>2010-05-20T19:29:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-20T19:29:28Z" id="247826">Implemented
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Killing a node during a query can lock up the cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/183</link><project id="" key="" /><description>To reproduce:
- start a node
- add an index and some data
- start another node
- wait for them to sync
- put the second node to sleep `^Z`
- issue a search against the first node
- `kill -9` the second node

The first node remains in a locked up state - it seems to still think that the first node exists. The only thing in the logs is:

```
[15:47:48,006][WARN ][transport                ] [Junta] Transport response handler timed out, action [discovery/zen/fd/ping], node [[Grandmaster][2738f9f3-4b5f-4206-a9ce-18968d879511][inet[/127.0.0.2:9301]]]
```

While this seems quite convoluted, it is relevant because the java service wrapper resorts to a `kill -9` if the server isn't responding
</description><key id="201691">183</key><summary>Killing a node during a query can lock up the cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-05-20T14:05:16Z</created><updated>2010-07-17T11:59:08Z</updated><resolved>2010-07-17T11:59:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-20T17:44:32Z" id="247696">Are you releasing the first node from its sleep? I understand you sue the service wrapper, and the service wrapper pings the jvm to see if its alive.
</comment><comment author="clintongormley" created="2010-05-20T17:53:20Z" id="247719">No, i'm killing it while it is asleep.  If the JVM isn't responding then  the service wrapper sends a `kill -9` as well, which (I think) replicates this situation.
</comment><comment author="kimchy" created="2010-05-20T17:55:50Z" id="247722">In this case, the socket might not have closed. It will do its retry pings (with timeout) and after they fail, the node will be considered dead. So, you are running now with pure elasticsearch script, no service wrapper?
</comment><comment author="clintongormley" created="2010-05-20T17:58:52Z" id="247726">In live I use the service wrapper. This test was just run directly.  How long should it take to timeout? Because I left it for a few minutes, and it didn't recover
</comment><comment author="kimchy" created="2010-05-20T19:08:00Z" id="247799">ok, so, after 1.5 minutes, it should detect it as disconnected. These values are configurable at: http://www.elasticsearch.com/docs/elasticsearch/modules/discovery/zen/. Note, I am still playing around with the best default values, you don't want to put it a too low  a value, since a node going through a long GC should usually not be considered disconnected (it will reconnect once its back if it has been).

But, there was a bug in this logic. So basically, the FD detection ended up only relying on socket close event. I have pushed a fix for this just now.
</comment><comment author="clintongormley" created="2010-07-17T11:59:07Z" id="316090">Fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter Cache: Improved Caching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/182</link><project id="" key="" /><description>Overhaul in how caching of filters is one (specifically with how Lucene NRT works). Also changed the default cache from Soft Cache to Weak Cache (garbage collection based cache, weak gets released more eagerly).
</description><key id="201009">182</key><summary>Filter Cache: Improved Caching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-19T18:02:02Z</created><updated>2010-05-19T18:05:51Z</updated><resolved>2010-05-19T18:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-19T18:05:51Z" id="246527">Filter Cache: Improved Caching, closed by d1acef1e09d59d5ac10b3f3f6f4e86e62f309fe7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Allow to control if filters should be cached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/181</link><project id="" key="" /><description>Add to the `contant_score`, `filtered` queries, as well to the `bool` filter field named `cache` allowing to control if filteres will be cached or not.
</description><key id="200974">181</key><summary>Query DSL: Allow to control if filters should be cached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-19T17:37:53Z</created><updated>2010-07-14T10:34:33Z</updated><resolved>2010-05-20T00:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-19T17:41:39Z" id="246507">Query DSL: Allow to control if filters should be cached, closed by bd6b89f7cab39acf5cd2b3d5b33adbcddf69c0d1.
</comment><comment author="clintongormley" created="2010-07-13T20:34:08Z" id="310766">Hi kimchy - there is nothing about named caches in the docs
</comment><comment author="kimchy" created="2010-07-14T10:34:33Z" id="311600">I meant a field named `cache`, for example, `constant_score` can have, on the same level as the `filter` field (object), a `cache` field with a value of `false` to indicate that it should not be cached.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud Plugin: Gateway should store meta data and indices under the same container</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/180</link><project id="" key="" /><description>Seems like the restriction I thought was in place in S3 (others don't have restriction) regarding the number of files in a bucket has been removed. There is a restriction for number of buckets though, so creating a bucket for metadata, and 2 buckets per shard does not make sense.

Everything should be stored under the same bucket. With the option, on the index leve, to store it on a different bucket if configured.

Sadly, this breaks backward... . So a reindex should be done.
</description><key id="198340">180</key><summary>Cloud Plugin: Gateway should store meta data and indices under the same container</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bjon</reporter><labels><label>feature</label><label>v0.08.0</label></labels><created>2010-05-18T14:52:36Z</created><updated>2010-05-18T20:39:03Z</updated><resolved>2010-05-18T20:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-18T20:39:03Z" id="245529">Cloud Plugin: Gateway should store meta data and indices under the same container, closed by c7075c16003b9a34c2e472361f08197159853711.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory Monitor: Remove explicit GC call, clear cached instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/179</link><project id="" key="" /><description>Currently, when there are memory problems for X amount of time, and things can't be flushed, GC is called. This should not happen. Instead, the caches should be explicitly cleared and GC should not be called. The new `clear_cache_threshold` replaces the `gc_threshold`.

Also, because of this change, there is no need for a scheduler to run on the soft/weak filter caches.
</description><key id="197493">179</key><summary>Memory Monitor: Remove explicit GC call, clear cached instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-17T15:43:07Z</created><updated>2010-05-17T20:27:37Z</updated><resolved>2010-05-17T20:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-17T20:27:37Z" id="244341">Memory Monitor: Remove explicit GC call, clear cached instead, closed by 8d3347bb5c4a508db16028d9764e74119fa815ab.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>uppercase first letter in property name breaks "_all" queries </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/178</link><project id="" key="" /><description>Using an uppercase letter first in a property name breaks queries against the "_all"-field.

For example, the code below doesn't match anything:
    curl -XPUT http://localhost:9200/sff/vehicle/1 -d '
    { 
        "make": "Jeep",
        "MODEL": "Jeep"
    }'

```
curl -XGET http://localhost:9200/sff/vehicle/_search?q=Jeep
```

But doing the following works:
    curl -XPUT http://localhost:9200/sff/vehicle/1 -d '
    { 
        "make": "Jeep",
        "model": "Jeep"
    }'

```
curl -XGET http://localhost:9200/sff/vehicle/_search?q=Jeep
```

The problem seems to be that the created mapping uses the property name (MODEL in this case) as "index_name". Manually creating a mapping with property name "MODEL" and "index_name" as "model" works. Using "mODEL" also works.

I'm using version v0.7.0.
</description><key id="197350">178</key><summary>uppercase first letter in property name breaks "_all" queries </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tweto</reporter><labels><label>bug</label><label>v0.08.0</label></labels><created>2010-05-17T12:01:52Z</created><updated>2010-05-17T20:27:37Z</updated><resolved>2010-05-17T20:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-17T14:24:10Z" id="243900">Ouch!. Found the problem, the case only relates to the internal ordering of the fields in Lucene... . Will push a fix later today.
</comment><comment author="tweto" created="2010-05-17T14:29:25Z" id="243908">Nice, that was fast! :-)
</comment><comment author="kimchy" created="2010-05-17T20:27:37Z" id="244340">uppercase first letter in property name breaks "_all" queries, closed by 23d2799d712838642ccffa70b83e09a2fc94cdd6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery: Increase Ping timeouts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/177</link><project id="" key="" /><description>Change ping timeout to 30 seconds, and retries to 3. Note, this does not mean that 1.5 minutes will pass till a node will be detected as failed, since it also detects connection close.
</description><key id="196985">177</key><summary>Zen Discovery: Increase Ping timeouts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.1</label></labels><created>2010-05-16T20:39:25Z</created><updated>2010-05-16T20:41:00Z</updated><resolved>2010-05-16T20:41:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-16T20:41:00Z" id="243237">Zen Discovery: Increase Ping timeouts, closed by ab57fa86af5698d60c4021d247994af64344a753.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throttling of recovery (both gateway recovery and peer node recovery)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/176</link><project id="" key="" /><description>There are two recoveries going on with elasticsearch. The first is when the first even primary is created, and is then recovery from the gateway, and the second is when a shard gets created / relocated into a another node.

This can be a taxing operation on the machine, especially when several shards are recovering at the same time. For this, we have two new settings that allow to throttle it:

`indices.recovery.throttler.concurrent_recoveries`: The number of concurrent recoveries allowed at the same time on a node. Defaults to the number of cores.

`indices.recovery.throttler.concurrent_streams`: The number of concurrent streams transfers allowed during recovery. A stream basically maps to an Lucene index file, which is a file that exists within specific shard index. Defaults to the number of cores.
</description><key id="196984">176</key><summary>Throttling of recovery (both gateway recovery and peer node recovery)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.1</label></labels><created>2010-05-16T20:36:36Z</created><updated>2010-05-16T20:38:05Z</updated><resolved>2010-05-16T20:38:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-16T20:38:05Z" id="243236">Throttling of recovery (both gateway recovery and peer node recovery), closed by 216dda3f9c00e3791011f681e251f453de1854dc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Request wrongly stores extra bytes as part of the document (trailing null bytes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/175</link><project id="" key="" /><description>Major bug introduced towards the end of 0.7 release. 0.7.1 release coming soon.
</description><key id="196777">175</key><summary>Index Request wrongly stores extra bytes as part of the document (trailing null bytes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.07.1</label></labels><created>2010-05-16T14:55:41Z</created><updated>2010-05-16T14:57:00Z</updated><resolved>2010-05-16T14:57:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-16T14:57:00Z" id="243000">Index Request wrongly stores extra bytes as part of the document (trailing null bytes), closed by 013e7699c34f84ae6fa899924002fdd59b054397.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consolidate network settings, common tcp settings, and generalized port settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/174</link><project id="" key="" /><description>Create common tcp settings for `network.tcp.no_delay`, `network.tcp.keep_alive`, `network.tcp.reuse_address`, `network.tcp.send_buffer_size`, and `network.tcp.receive_buffer_size`. Specific settings still exists (the netty ones per module of transport/http/memcached).

Also, create a more simplified setting for ports. Transport port can be set using `transport.tcp.port` (on top of the current `transport.netty.port`), HTTP port can be set using `http.port` (on top of `http.netty.port`), and memcached port can be set using `memcached.port` (on top of `memcached.netty.port`).
</description><key id="196282">174</key><summary>Consolidate network settings, common tcp settings, and generalized port settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.08.0</label></labels><created>2010-05-15T13:29:07Z</created><updated>2010-05-15T13:30:41Z</updated><resolved>2010-05-15T13:30:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-15T13:30:10Z" id="242348">Consolidate network settings, common tcp settings, and generalized port settings, closed by 723e47a54bb1a25901792c00decdf6006034b1b2.
</comment><comment author="kimchy" created="2010-05-15T13:30:41Z" id="242350">The docs should also be simplified and remove the netty sub module from it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Network Settings: Add `host` setting that automatically set both `bind_host` and `publish_host`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/173</link><project id="" key="" /><description>Currently, you need to set both `network.bind_host` and `network.publish_host` to completely set a node to use specific IP. The new `network.host` setting will automatically set both.
</description><key id="195695">173</key><summary>Network Settings: Add `host` setting that automatically set both `bind_host` and `publish_host`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-05-14T16:22:31Z</created><updated>2010-05-14T16:23:02Z</updated><resolved>2010-05-14T16:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-14T16:23:02Z" id="241695">Network Settings: Add `host` setting that automatically set both `bind_host` and `publish_host`, closed by c9be7bde529e60328d05871104e2447739e9fa20.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Doesn't bind to localhost only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/172</link><project id="" key="" /><description>Starting v 0.7 with this config:

```
network:
    bind_host:  127.0.0.1
```

It reports that it is bound to the non-localhost IP:

```
Starting ElasticSearch...Waiting for ElasticSearch.....[19:54:05,899][INFO ][node                     ] [Mary Zero] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T19:18:49}[7189]: Initializing ...
[19:54:05,903][INFO ][plugins                  ] [Mary Zero] Loaded []
.[19:54:06,877][INFO ][node                     ] [Mary Zero] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T19:18:49}[7189]: Initialized
[19:54:06,878][INFO ][node                     ] [Mary Zero] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T19:18:49}[7189]: Starting ...
[19:54:06,975][INFO ][transport                ] [Mary Zero] bound_address[inet[/127.0.0.1:9300]], publish_address[inet[lb2/192.168.10.30:9300]]

ElasticSearch started.

[19:54:10,022][INFO ][cluster.service          ] [Mary Zero] New Master [Mary Zero][c719f554-5693-4fb2-a451-1bdefd983c08][inet[lb2/192.168.10.30:9300]]
[19:54:10,082][WARN ][cluster.service          ] [Mary Zero] Failed to connect to node [[Mary Zero][c719f554-5693-4fb2-a451-1bdefd983c08][inet[lb2/192.168.10.30:9300]]]
org.elasticsearch.transport.ConnectTransportException: [Mary Zero][inet[lb2/192.168.10.30:9300]]
Caused by: org.elasticsearch.transport.ConnectTransportException: [Mary Zero][inet[lb2/192.168.10.30:9300]]
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
    at org.elasticsearch.util.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.connect(NioClientSocketPipelineSink.java:384)
    at org.elasticsearch.util.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:354)
    at org.elasticsearch.util.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:276)
    at org.elasticsearch.util.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.util.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[19:54:10,086][INFO ][discovery                ] [Mary Zero] iAnnounce-demo/c719f554-5693-4fb2-a451-1bdefd983c08
[19:54:10,098][INFO ][http                     ] [Mary Zero] bound_address[inet[/127.0.0.1:9200]], publish_address[inet[lb2/192.168.10.30:9200]]
[19:54:10,358][INFO ][jmx                      ] [Mary Zero] bound_address[service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publish_address[service:jmx:rmi:///jndi/rmi://192.168.10.30:9400/jmxrmi]
[19:54:10,359][INFO ][node                     ] [Mary Zero] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T19:18:49}[7189]: Started

lb2:/opt/apache/sites/iAnnounce # curl -XGET http://127.0.0.1:9200/_cluster/nodes?pretty=1
{
  "cluster_name" : "iAnnounce-demo",
  "nodes" : {
    "c719f554-5693-4fb2-a451-1bdefd983c08" : {
      "name" : "Mary Zero",
      "transport_address" : "inet[lb2/192.168.10.30:9300]",
      "attributes" : [ ],
      "http_address" : "inet[lb2/192.168.10.30:9200]",
      "os" : {
      },
      "process" : {
        "id" : 7189
      },
      "jvm" : {
        "pid" : 7189,
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "16.2-b04",
        "vm_vendor" : "Sun Microsystems Inc.",
        "start_time" : 1273780445151
      },
      "network" : {
      }
    }
  }
}
```
</description><key id="194972">172</key><summary>Doesn't bind to localhost only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-05-13T19:56:42Z</created><updated>2010-05-28T08:45:26Z</updated><resolved>2010-05-28T08:45:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-14T16:24:01Z" id="241697">Yo, the problem is that you need also to set `network.publish_host` to `127.0.0.1`. This is annoying, since you need to repeat the configuration..., so I created #173. 
</comment><comment author="kimchy" created="2010-05-28T08:45:26Z" id="256474">closing, now you can set `network.host`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node restart doesn't seem to work correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/171</link><project id="" key="" /><description>Hiya

I tried starting two nodes on my local machine (not using the java service wrapper, because that way I can only start one node), then issued a restart request to one of the nodes.

I get this in the logs:

```
[09:27:54,697][INFO ][action.admin.cluster.node.restart] [Silver Scorpion] Restarting in [1s]
[09:27:55,699][INFO ][action.admin.cluster.node.restart] [Silver Scorpion] Initiating requested restart
[09:27:55,700][INFO ][node                     ] [Silver Scorpion] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-12T12:56:28}[3538]: Stopping ...
[09:27:55,706][INFO ][cluster.service          ] [Invisible Girl] Removed {[Silver Scorpion][fba92d81-ef41-44cf-bead-308919a12e2f][inet[getafix.traveljury.com/127.0.0.2:9302]],}
[09:27:55,735][INFO ][cluster.service          ] [Answer] Removed {[Silver Scorpion][fba92d81-ef41-44cf-bead-308919a12e2f][inet[getafix.traveljury.com/127.0.0.2:9302]],}
[09:27:55,873][INFO ][node                     ] [Silver Scorpion] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-12T12:56:28}[3538]: Stopped
[09:27:55,873][INFO ][node                     ] [Silver Scorpion] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-12T12:56:28}[3538]: Starting ...
[09:27:55,875][INFO ][transport                ] [Silver Scorpion] bound_address[inet[/0.0.0.0:9302]], publish_address[inet[getafix.traveljury.com/127.0.0.2:9302]]
[09:27:58,922][INFO ][cluster.service          ] [Invisible Girl] Added {[Silver Scorpion][b038b644-5038-426b-a0bf-beefe8ebec77][inet[getafix.traveljury.com/127.0.0.2:9302]],}
[09:27:58,954][WARN ][discovery.zen            ] [Silver Scorpion] Received a cluster state from [[Invisible Girl][c71ce4ef-446e-4217-b240-f31b5e88becd][inet[getafix.traveljury.com/127.0.0.2:9300]]] and not part of the cluster, should not happen
[09:27:58,955][INFO ][cluster.service          ] [Answer] Added {[Silver Scorpion][b038b644-5038-426b-a0bf-beefe8ebec77][inet[getafix.traveljury.com/127.0.0.2:9302]],}
[09:28:00,938][WARN ][discovery.zen            ] [Silver Scorpion] Received a cluster state from [[Invisible Girl][c71ce4ef-446e-4217-b240-f31b5e88becd][inet[getafix.traveljury.com/127.0.0.2:9300]]] and not part of the cluster, should not happen
[09:28:28,930][WARN ][discovery                ] [Silver Scorpion] Waited for 30s and no initial state was set by the discovery
[09:28:28,933][INFO ][discovery                ] [Silver Scorpion] iAnnounce/b038b644-5038-426b-a0bf-beefe8ebec77
[09:28:28,940][INFO ][http                     ] [Silver Scorpion] bound_address[inet[/0.0.0.0:9202]], publish_address[inet[getafix.traveljury.com/127.0.0.2:9202]]
[09:28:28,968][INFO ][jmx                      ] [Silver Scorpion] bound_address[service:jmx:rmi:///jndi/rmi://:9403/jmxrmi], publish_address[service:jmx:rmi:///jndi/rmi://127.0.0.2:9403/jmxrmi]
[09:28:28,972][INFO ][node                     ] [Silver Scorpion] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-12T12:56:28}[3538]: Started
```
</description><key id="194471">171</key><summary>Node restart doesn't seem to work correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-05-13T07:30:03Z</created><updated>2010-07-17T11:55:43Z</updated><resolved>2010-07-17T11:55:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-13T12:40:46Z" id="240147">I just pushed a fix, can you try it. I don't really understand how it changed its name though (the [Name] just after the logger component.... . I just tested and it works for me...
</comment><comment author="clintongormley" created="2010-05-13T15:54:16Z" id="240417">OK, this throws a number of warnings, but appears to work:

```
[17:52:07,335][INFO ][action.admin.cluster.node.restart] [Terraxia] Restarting in [1s]
[17:52:07,335][INFO ][action.admin.cluster.node.restart] [Terraxia] Restarting in [1s]
[17:52:08,336][INFO ][action.admin.cluster.node.restart] [Terraxia] Initiating requested restart
[17:52:08,336][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Stopping ...
[17:52:08,357][INFO ][discovery.zen            ] [Crimson Cowl] Master [[Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]]] left, reason [shut_down]
[17:52:08,361][INFO ][cluster.service          ] [Crimson Cowl] Master {New [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], Previous [Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]]}, Removed {[Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]],}
[17:52:08,436][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Stopped
[17:52:08,436][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Starting ...
[17:52:08,438][INFO ][transport                ] [Terraxia] bound_address[inet[/0.0.0.0:9300]], publish_address[inet[getafix.traveljury.com/127.0.0.2:9300]]
[17:52:08,336][INFO ][action.admin.cluster.node.restart] [Terraxia] Initiating requested restart
[17:52:08,336][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Stopping ...
[17:52:08,357][INFO ][discovery.zen            ] [Crimson Cowl] Master [[Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]]] left, reason [shut_down]
[17:52:08,361][INFO ][cluster.service          ] [Crimson Cowl] Master {New [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], Previous [Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]]}, Removed {[Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]],}
[17:52:08,436][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Stopped
[17:52:08,436][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Starting ...
[17:52:08,438][INFO ][transport                ] [Terraxia] bound_address[inet[/0.0.0.0:9300]], publish_address[inet[getafix.traveljury.com/127.0.0.2:9300]]
[17:52:11,449][INFO ][cluster.service          ] [Crimson Cowl] Added {[Terraxia][61dbb9be-e707-438a-83f7-514924cbdb77][inet[getafix.traveljury.com/127.0.0.2:9300]],}
[17:52:11,472][INFO ][cluster.service          ] [Terraxia] Master {New [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], Previous [Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]]}, Removed {[Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]],}
[17:52:12,181][INFO ][discovery                ] [Terraxia] iAnnounce/61dbb9be-e707-438a-83f7-514924cbdb77
[17:52:11,449][INFO ][cluster.service          ] [Crimson Cowl] Added {[Terraxia][61dbb9be-e707-438a-83f7-514924cbdb77][inet[getafix.traveljury.com/127.0.0.2:9300]],}
[17:52:11,472][INFO ][cluster.service          ] [Terraxia] Master {New [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], Previous [Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]]}, Removed {[Terraxia][75afba3e-9ed6-4fe6-a730-ea43e55efda8][inet[getafix.traveljury.com/127.0.0.2:9300]],}
[17:52:12,181][INFO ][discovery                ] [Terraxia] iAnnounce/61dbb9be-e707-438a-83f7-514924cbdb77
[17:52:12,186][INFO ][http                     ] [Terraxia] bound_address[inet[/0.0.0.0:9200]], publish_address[inet[getafix.traveljury.com/127.0.0.2:9200]]
[17:52:12,197][INFO ][jmx                      ] [Terraxia] bound_address[service:jmx:rmi:///jndi/rmi://:9403/jmxrmi], publish_address[service:jmx:rmi:///jndi/rmi://127.0.0.2:9403/jmxrmi]
[17:52:12,201][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709
[17:52:12,201][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=ia_object_1273725818
[17:52:12,201][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Started
[17:52:12,186][INFO ][http                     ] [Terraxia] bound_address[inet[/0.0.0.0:9200]], publish_address[inet[getafix.traveljury.com/127.0.0.2:9200]]
[17:52:12,197][INFO ][jmx                      ] [Terraxia] bound_address[service:jmx:rmi:///jndi/rmi://:9403/jmxrmi], publish_address[service:jmx:rmi:///jndi/rmi://127.0.0.2:9403/jmxrmi]
[17:52:12,201][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709
[17:52:12,201][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=ia_object_1273725818
[17:52:12,201][INFO ][node                     ] [Terraxia] {ElasticSearch/0.7.0-SNAPSHOT/2010-05-13T15:40:14}[3337]: Started
[17:52:18,405][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=0
[17:52:18,407][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=0,shardType=store
[17:52:18,408][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][0] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:18,465][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=1
[17:52:18,466][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=1,shardType=store
[17:52:18,468][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][1] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:18,405][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=0
[17:52:18,407][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=0,shardType=store
[17:52:18,408][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][0] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:18,465][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=1
[17:52:18,466][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=1,shardType=store
[17:52:18,468][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][1] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:19,330][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=2
[17:52:19,331][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=2,shardType=store
[17:52:19,332][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][2] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:19,345][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=3
[17:52:19,346][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=3,shardType=store
[17:52:19,348][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][3] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:19,388][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=4
[17:52:19,891][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=4,shardType=store
[17:52:19,896][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][4] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,120][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][0] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:19,330][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=2
[17:52:19,331][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=2,shardType=store
[17:52:19,332][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][2] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:19,345][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=3
[17:52:19,346][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=3,shardType=store
[17:52:19,348][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][3] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:19,388][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=4
[17:52:19,891][WARN ][jmx                      ] [Terraxia] Could not register object with name: {elasticsearch}:type=Terraxia [iAnnounce_61dbb9be-e707-438a-83f7-514924cbdb77],service=indices,index=iannounce_object_1273723709,subService=shards,shard=4,shardType=store
[17:52:19,896][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][4] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,120][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][0] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,197][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][1] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,257][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][2] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,559][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][3] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,571][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][4] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,197][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][1] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,257][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][2] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,559][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][3] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:20,571][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][4] Starting recovery from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]
[17:52:21,781][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][0] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[1.6s]
   Phase1: recovered [1] files with total size of [32], took [2ms]
   Phase2: recovered [725] transaction log operations, took [1.2s]
   Phase3: recovered [0] transaction log operations, took [368ms]
[17:52:21,828][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][1] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[1.6s]
   Phase1: recovered [1] files with total size of [32], took [60ms]
   Phase2: recovered [725] transaction log operations, took [1.1s]
   Phase3: recovered [0] transaction log operations, took [426ms]
[17:52:22,069][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][1] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[3.6s]
   Phase1: recovered [1] files with total size of [32], took [1ms]
   Phase2: recovered [1271] transaction log operations, took [2.7s]
   Phase3: recovered [0] transaction log operations, took [839ms]
[17:52:21,781][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][0] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[1.6s]
   Phase1: recovered [1] files with total size of [32], took [2ms]
   Phase2: recovered [725] transaction log operations, took [1.2s]
   Phase3: recovered [0] transaction log operations, took [368ms]
[17:52:21,828][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][1] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[1.6s]
   Phase1: recovered [1] files with total size of [32], took [60ms]
   Phase2: recovered [725] transaction log operations, took [1.1s]
   Phase3: recovered [0] transaction log operations, took [426ms]
[17:52:22,069][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][1] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[3.6s]
   Phase1: recovered [1] files with total size of [32], took [1ms]
   Phase2: recovered [1271] transaction log operations, took [2.7s]
   Phase3: recovered [0] transaction log operations, took [839ms]
[17:52:22,303][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][3] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[1.7s]
   Phase1: recovered [1] files with total size of [32], took [4ms]
   Phase2: recovered [726] transaction log operations, took [1.4s]
   Phase3: recovered [0] transaction log operations, took [216ms]
[17:52:22,360][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][2] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[2.1s]
   Phase1: recovered [1] files with total size of [32], took [150ms]
   Phase2: recovered [726] transaction log operations, took [1.4s]
   Phase3: recovered [0] transaction log operations, took [246ms]
[17:52:22,612][INFO ][cluster.service          ] [Terraxia] Got old cluster state [51&lt;52] from source [zen-disco-receive(from [[Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]])], ignoring
[17:52:22,697][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][4] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[2.1s]
   Phase1: recovered [1] files with total size of [32], took [115ms]
   Phase2: recovered [725] transaction log operations, took [1.5s]
   Phase3: recovered [0] transaction log operations, took [367ms]
[17:52:22,812][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][2] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[3.4s]
   Phase1: recovered [1] files with total size of [32], took [2ms]
   Phase2: recovered [1285] transaction log operations, took [2.8s]
   Phase3: recovered [0] transaction log operations, took [574ms]
[17:52:22,864][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][3] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[3.5s]
   Phase1: recovered [1] files with total size of [32], took [37ms]
   Phase2: recovered [1294] transaction log operations, took [2.9s]
   Phase3: recovered [0] transaction log operations, took [475ms]
[17:52:22,865][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][4] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[2.9s]
   Phase1: recovered [1] files with total size of [32], took [244ms]
   Phase2: recovered [1280] transaction log operations, took [2.1s]
   Phase3: recovered [0] transaction log operations, took [507ms]
[17:52:22,871][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][0] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[4.4s]
   Phase1: recovered [1] files with total size of [32], took [97ms]
   Phase2: recovered [1336] transaction log operations, took [3.8s]
   Phase3: recovered [0] transaction log operations, took [457ms]
[17:52:22,303][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][3] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[1.7s]
   Phase1: recovered [1] files with total size of [32], took [4ms]
   Phase2: recovered [726] transaction log operations, took [1.4s]
   Phase3: recovered [0] transaction log operations, took [216ms]
[17:52:22,360][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][2] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[2.1s]
   Phase1: recovered [1] files with total size of [32], took [150ms]
   Phase2: recovered [726] transaction log operations, took [1.4s]
   Phase3: recovered [0] transaction log operations, took [246ms]
[17:52:22,612][INFO ][cluster.service          ] [Terraxia] Got old cluster state [51&lt;52] from source [zen-disco-receive(from [[Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]]])], ignoring
[17:52:22,697][DEBUG][index.shard.recovery     ] [Terraxia][ia_object_1273725818][4] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[2.1s]
   Phase1: recovered [1] files with total size of [32], took [115ms]
   Phase2: recovered [725] transaction log operations, took [1.5s]
   Phase3: recovered [0] transaction log operations, took [367ms]
[17:52:22,812][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][2] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[3.4s]
   Phase1: recovered [1] files with total size of [32], took [2ms]
   Phase2: recovered [1285] transaction log operations, took [2.8s]
   Phase3: recovered [0] transaction log operations, took [574ms]
[17:52:22,864][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][3] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[3.5s]
   Phase1: recovered [1] files with total size of [32], took [37ms]
   Phase2: recovered [1294] transaction log operations, took [2.9s]
   Phase3: recovered [0] transaction log operations, took [475ms]
[17:52:22,865][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][4] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[2.9s]
   Phase1: recovered [1] files with total size of [32], took [244ms]
   Phase2: recovered [1280] transaction log operations, took [2.1s]
   Phase3: recovered [0] transaction log operations, took [507ms]
[17:52:22,871][DEBUG][index.shard.recovery     ] [Terraxia][iannounce_object_1273723709][0] Recovery completed from [Crimson Cowl][f0fdde25-126e-455f-95a4-b7afb9dd4669][inet[getafix.traveljury.com/127.0.0.2:9301]], took[4.4s]
   Phase1: recovered [1] files with total size of [32], took [97ms]
   Phase2: recovered [1336] transaction log operations, took [3.8s]
   Phase3: recovered [0] transaction log operations, took [457ms]
```
</comment><comment author="kimchy" created="2010-05-13T17:12:18Z" id="240553">I see that you send more than one restart request concurrently, you shouldn't... . Let me fix so it will ignore that...
</comment><comment author="clintongormley" created="2010-07-17T11:55:43Z" id="316088">This looks to be fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: A failure to handle a response might cause the transport to stop working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/170</link><project id="" key="" /><description /><key id="193975">170</key><summary>Transport: A failure to handle a response might cause the transport to stop working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.07.0</label></labels><created>2010-05-12T18:58:01Z</created><updated>2010-05-14T20:51:03Z</updated><resolved>2010-05-14T20:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-12T18:58:29Z" id="239373">I will also add better logging when a timeout happens.
</comment><comment author="kimchy" created="2010-05-14T20:51:03Z" id="241908">fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Date format - allow for multiple formats using '||' separator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/169</link><project id="" key="" /><description>The format in the date format to allow for multiple date formats. This is handy for example to define a format similar to this: `yyyy/MM/dd HH:mm:ss||yyyy/MM/dd`. The "printing" from the millisecond back to a string will use the first format.
</description><key id="193292">169</key><summary>Mapper: Date format - allow for multiple formats using '||' separator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-05-12T00:31:42Z</created><updated>2010-05-12T12:44:53Z</updated><resolved>2010-05-12T00:32:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-12T00:32:31Z" id="238454">Mapper: Date format - allow for multiple formats using '||' separator, closed by fdf6eb9cb299f984099a2218e4fbcc075cf3a762.
</comment><comment author="clintongormley" created="2010-05-12T12:44:53Z" id="238939">Nice solution! Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NodeBuilder.local(false) sets local to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/168</link><project id="" key="" /><description>The method NodeBuilder.local(boolean local) always sets local to true regardless of input.
</description><key id="192843">168</key><summary>NodeBuilder.local(false) sets local to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tweto</reporter><labels><label>bug</label><label>v0.07.0</label></labels><created>2010-05-11T15:17:32Z</created><updated>2010-05-11T18:37:12Z</updated><resolved>2010-05-11T18:37:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-11T18:36:44Z" id="238075">Its fixed in master already ;)
</comment><comment author="kimchy" created="2010-05-11T18:37:11Z" id="238078">Closing... .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>starting first a non-data node and after that the data node will fail to allocate the index  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/167</link><project id="" key="" /><description>Thanks a lot for the great work!
Not sure if the following is really a small bug or intentional:
For now (elasticsearch &lt;=0.6) it is mandatory to start first a data-node and _after that_ the non-data-node if you want to have access to the index (else =&gt; {  "error" : "IndexMissingException[[twitter] missing]" }.

-o
</description><key id="192777">167</key><summary>starting first a non-data node and after that the data node will fail to allocate the index  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2010-05-11T14:10:55Z</created><updated>2010-05-14T15:37:24Z</updated><resolved>2010-05-14T08:58:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-11T14:25:48Z" id="237835">Can you explain more the process? Not sure I followed. For example, run this curl script, then start X, then stop, then restart, yada yada yada ;)
</comment><comment author="ghost" created="2010-05-14T08:58:46Z" id="241326">First, let me explain, I figured out what I was doing wrong - see at the bottom. But first here is my scenario:

# start nodes to index something

$ bin/elasticsearch -Des.gateway.type=fs -Des.gateway.fs.location=/home/lucene/elasticsearch/snapshot/gateway -Des.index.gateway.snapshot_interval=-1 -Des.index.engine.robin.refresh_interval=-1 -Des.http.enabled=false

$ bin/elasticsearch -f -Des.node.data=false

$ curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '
{ 
    "user": "kimchy", 
    "postDate": "2009-11-15T13:12:00", 
    "message": "Trying out Elastic Search, so far so good?" 
}'

$ curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=true'

{
  "_index" : "twitter",
  "_type" : "tweet",
  "_id" : "1", "_source" : 
{ 
    "user": "kimchy", 
    "postDate": "2009-11-15T13:12:00", 
    "message": "Trying out Elastic Search, so far so good?" 
}
}

# shutdown all nodes and start first data node and after that the non data node

$ curl -XPOST 'http://localhost:9200/_cluster/nodes/_all/_shutdown'

$ bin/elasticsearch -Des.gateway.type=fs -Des.gateway.fs.location=/home/lucene/elasticsearch/snapshot/gateway -Des.index.gateway.snapshot_interval=-1 -Des.index.engine.robin.refresh_interval=-1 -Des.http.enabled=false

$ bin/elasticsearch -f -Des.node.data=false

$ curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=true'

{
  "_index" : "twitter",
  "_type" : "tweet",
  "_id" : "1", "_source" : 
{ 
    "user": "kimchy", 
    "postDate": "2009-11-15T13:12:00", 
    "message": "Trying out Elastic Search, so far so good?" 
}
}

# shutdown all nodes and start first the non-data node and after that the data

$ curl -XPOST 'http://localhost:9200/_cluster/nodes/_all/_shutdown'

$ bin/elasticsearch -f -Des.node.data=false

$ bin/elasticsearch -Des.gateway.type=fs -Des.gateway.fs.location=/home/lucene/elasticsearch/snapshot/gateway -Des.index.gateway.snapshot_interval=-1 -Des.index.engine.robin.refresh_interval=-1 -Des.http.enabled=false

$ curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=true'

{
  "error" : "IndexMissingException[[twitter] missing]"

Btw. even when I now shutdown the non-data node and restart that non-data node,
these nodes donot "find" them (I guess) =&gt; cannot XGET a document.

But at least, I figured that when I start the non-data node with all the
parameters used for the data node, it works:

$  bin/elasticsearch -f -Des.node.data=false -Des.gateway.type=fs -Des.gateway.fs.location=/home/lucene/elasticsearch/snapshot/gateway -Des.index.gateway.snapshot_interval=-1 -Des.index.engine.robin.refresh_interval=-1

I was a little bit confused that it is necessary to parametrize a non-data node
with data-parameters,  but that will do it :)
</comment><comment author="kimchy" created="2010-05-14T15:30:41Z" id="241650">Ahh, I see what happened... . The problem is that the first node started will become the master and reload the cluster meta data (which includes indices created) from the gateway. This is why you need to configure the gateway on it as well.

What you can do, by the way, is set `node.client` to `true` (will automatically set it to be a non data node as well). In this case (this only works with the new zen discovery in upcoming 0.7), the client node will not become master, but wait till one will be started.

make sense?
</comment><comment author="ghost" created="2010-05-14T15:37:24Z" id="241654">excellent :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A flexible / dynamic JSON response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/166</link><project id="" key="" /><description>Allow json response can be dynamically calculated fields when searching.

Forum
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/ec551ae3645a87b5
</description><key id="190341">166</key><summary>A flexible / dynamic JSON response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tfreitas</reporter><labels /><created>2010-05-09T17:32:47Z</created><updated>2013-04-04T18:09:44Z</updated><resolved>2013-04-04T18:09:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:09:44Z" id="15913796">We support script_fields - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes Stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/165</link><project id="" key="" /><description>Add an API (similar in structure to node info) that return node level statistics. Currently, the statistics include Os, Process, JVM, and Network level stats. ElasticSearch relevant stats will be added later (for example, transport stats, and so on).
</description><key id="190081">165</key><summary>Nodes Stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-05-09T06:59:13Z</created><updated>2010-05-09T17:12:04Z</updated><resolved>2010-05-09T17:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-09T17:12:04Z" id="234489">Nodes Stats API, closed by 3e405c3ec76235145f81507f1189e92824d06b92,
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud Plugin: Auto Discovery on the Cloud (extending the Zen discovery)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/164</link><project id="" key="" /><description>Using the cloud API, the list of nodes that are currently running can be obtained. Using it, and using the special purpose zen discovery that was built to handle it, auto discovery of nodes can be implemented just using the cloud APIs.

This menas that no multicast is required, no elastic IPs, and no special gossip routers. All nodes are created equal!.

Here is a sample configuration:

```
cloud:
    account: &lt;Account ID Here&gt;
    key: &lt;Private Key Here&gt;
    compute:
        type: ec2 / cloudservers

discovery:
    type: cloud
```

The discovery by default has a `discovery.cloud.ports` range setting that is set to `9300-9302`. These are the default transport ports used by elasticsearch (its `9300`, but if its taken, the next one will be used). Make sure to enable communication between nodes in the cloud (internally, not externally) on at least port `9300` (usually, a single node per VM is enough).       
</description><key id="189409">164</key><summary>Cloud Plugin: Auto Discovery on the Cloud (extending the Zen discovery)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-05-07T23:53:00Z</created><updated>2010-05-08T00:15:07Z</updated><resolved>2010-05-07T23:53:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-07T23:53:27Z" id="233152">Implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud Plugin: Allow to use the cloud as gateway storage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/163</link><project id="" key="" /><description>Allow to use the cloud as gateway storage. This include Amazon S3, Rackspace CloudFiles, and Azure blob.

The configuration is simple:

```
cloud:
    account: &lt;Account ID Here&gt;
    key: &lt;Private Key Here&gt;
    blobstore:
        type: s3 / cloudfiles / azureblob

gateway:
    type: cloud
    cloud:
        container: YourContainerNameHere
```

There is also other properties that can be set on the gateway type, such as the location (east-1, west) that can be set.

A blob container (bucket in s3 terms) is created for the metadata, and for each index (both for the actual index files and the transaction log).
</description><key id="189407">163</key><summary>Cloud Plugin: Allow to use the cloud as gateway storage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-05-07T23:50:01Z</created><updated>2014-07-08T12:28:13Z</updated><resolved>2010-05-07T23:53:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-07T23:53:37Z" id="233153">Implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API: Allow to provide `case` parameter, with `camelCase` to return results in CamelCasing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/162</link><project id="" key="" /><description>All REST responses should allow for camel casing in field names within the JSON response. Note, this does not apply to the source document indexed / fetched, which is treated as is.
</description><key id="189289">162</key><summary>REST API: Allow to provide `case` parameter, with `camelCase` to return results in CamelCasing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-05-07T21:00:51Z</created><updated>2010-05-07T21:01:43Z</updated><resolved>2010-05-07T21:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-07T21:01:43Z" id="233045">REST API: Allow to provide `case` parameter, with `camelCase` to return results in CamelCasing, closed by 31d226188f75ac12affd336ec227da462c1137ce.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Custom Boost Factory Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/161</link><project id="" key="" /><description>A query called `custom_boost_factor` that applies a boost factor by simply multiplying it with the embedded query.

```
{
    "custom_boost_factor" : {
        "query" : {
            "term" : { "name.last" : "banon"}
        },
        "boost_factor" : 1.3
    }
}
```
</description><key id="188552">161</key><summary>Query DSL: Custom Boost Factory Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-05-06T22:44:52Z</created><updated>2010-05-06T22:45:39Z</updated><resolved>2010-05-06T22:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-06T22:45:39Z" id="232118">Query DSL: Custom Boost Factory Query, closed by c05e2ad1f1b5d6574be8fdc2483b91400e31dc59.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node uptime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/160</link><project id="" key="" /><description>It would be nice to be able to get info about how long individual node has been up and running.
</description><key id="187657">160</key><summary>Node uptime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-05-05T21:43:39Z</created><updated>2010-05-10T18:41:35Z</updated><resolved>2010-05-10T18:41:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-10T18:03:47Z" id="236742">The node stats API now provides that, if its good enough, you can close this issue.
</comment><comment author="lukas-vlcek" created="2010-05-10T18:41:35Z" id="236785">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Object mapper default 'date_formats' to also support 'yyyy/MM/dd HH:mm:ss' and 'yyyy/MM/dd'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/159</link><project id="" key="" /><description>Add support for this two formats for auto date detection for object mappers.
</description><key id="187456">159</key><summary>Mapper: Object mapper default 'date_formats' to also support 'yyyy/MM/dd HH:mm:ss' and 'yyyy/MM/dd'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-05-05T16:45:02Z</created><updated>2010-05-05T16:46:03Z</updated><resolved>2010-05-05T16:46:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-05T16:46:03Z" id="230588">Mapper: Object mapper default 'date_formats' to also support 'yyyy/MM/dd HH:mm:ss' and 'yyyy/MM/dd', closed by db4afc8750861bbc97d9dacd583d59ec3facb318.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clear pom.xml dependencies when using gradle install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/158</link><project id="" key="" /><description>gradle install produces pom.xml which contains compile time dependencies on log4j.1.2.15 (which has dependencies on javax.jms:jms, javax.mail:mail, com.sun.jmx:jmxri and com.sun.jdmk:jmxtools) and other unnecessary jars.
</description><key id="185838">158</key><summary>Clear pom.xml dependencies when using gradle install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2010-05-03T21:22:10Z</created><updated>2013-02-20T11:12:15Z</updated><resolved>2013-02-20T11:12:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2010-05-03T21:24:05Z" id="228625">For now the workaround is to use dependency exclusions:
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;0.7.0-SNAPSHOT&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;javax.jms&lt;/groupId&gt;
                    &lt;artifactId&gt;jms&lt;/artifactId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;javax.mail&lt;/groupId&gt;
                    &lt;artifactId&gt;mail&lt;/artifactId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.sun.jmx&lt;/groupId&gt;
                    &lt;artifactId&gt;jmxri&lt;/artifactId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.sun.jdmk&lt;/groupId&gt;
                    &lt;artifactId&gt;jmxtools&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
</comment><comment author="dadoonet" created="2013-02-20T11:12:15Z" id="13826794">I think we can close this one. We don't use Gradle anymore ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add online plugin repository and a 'plugin' command to download them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/157</link><project id="" key="" /><description>Add online plugin repository and a 'plugin' command to download them. The new scripts will support `plugin install mapper-attachments`. For now, the plugins must be of the same version as elasticsearch, and the base url is hardcoded.
</description><key id="185200">157</key><summary>Add online plugin repository and a 'plugin' command to download them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-05-03T07:49:28Z</created><updated>2010-11-09T15:47:16Z</updated><resolved>2010-05-04T18:43:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-04T18:43:31Z" id="229621">Add online plugin repository and a 'plugin' command to download them, closed by b0e1c58474dd5de39a2b26cca00ac85f72d8b55d
</comment><comment author="clintongormley" created="2010-05-12T12:53:25Z" id="238945">What about adding a `list` option to this command, to figure out what plugins are available?
</comment><comment author="kimchy" created="2010-05-12T13:31:08Z" id="238982">Yep, on my list of TODOs ;)
</comment><comment author="apatrida" created="2010-11-09T15:47:16Z" id="529156">Would add to this that the plugin version you install can be fixed to a specific version so that accidental updates aren't taken.  Having it be the same as the version number of ES is one thing, but there might be a patch that you do or do not want to that version as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script / Service: Add ES_MIN_MEM and ES_MAX_MEM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/156</link><project id="" key="" /><description>Allow to set the minimum and maximum memory of elasticsearch script and service using environment variables.
</description><key id="184384">156</key><summary>Script / Service: Add ES_MIN_MEM and ES_MAX_MEM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-05-01T12:55:24Z</created><updated>2010-05-01T12:55:57Z</updated><resolved>2010-05-01T12:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-01T12:55:57Z" id="226764">Script / Service: Add ES_MIN_MEM and ES_MAX_MEM, closed by c09877c2bde915cbf135927b13325203159d9338
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restart API: Allow to restart one or more nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/155</link><project id="" key="" /><description>The REST API follows the same rules as the shutdown API, simply replace shutdown with restart: `/_cluster/nodes/{nodeId}/_restart`, where `{nodeId}` can be a list of node ids, `_all`, or `_local`.

With the new service wrapper, if a restart is requested and the process is running using the service wrapper, a full restart of the JVM will be issued.
</description><key id="184152">155</key><summary>Restart API: Allow to restart one or more nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-04-30T23:59:04Z</created><updated>2013-07-03T17:19:47Z</updated><resolved>2010-05-01T00:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-01T00:00:15Z" id="226479">Restart API: Allow to restart one or more nodes, closed by ebded19dc1b41ac3841d44935cd7e8def2ae47de.
</comment><comment author="KimStebel" created="2013-01-02T08:53:52Z" id="11802350">This feature does not seem to be documented anywhere.
</comment><comment author="dadoonet" created="2013-01-02T17:32:19Z" id="11816367">@kimchy I tried the _restart API but without using a service wrapper and I get the following message:

``` sh
$ curl -XPOST 'http://localhost:9200/_cluster/nodes/_local/_restart'
```

``` javascript
{"error":"ElasticSearchIllegalStateException[restart is disabled (for now) ....]","status":500}
```

Does it mean that we can only use it with the service wrapper?
I'm on the way of creating the pull request to update the documentation so I need to understand if it's a windows restriction, a configuration restriction (but I can't see an option in the `elasticsearch.yml` file, or something else...

Thanks for your help

cc @KimStebel 
</comment><comment author="btiernay" created="2013-07-03T17:19:47Z" id="20431042">See https://github.com/elasticsearch/elasticsearch/issues/265
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Service Wrapper: A Java Service Wrapper integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/154</link><project id="" key="" /><description>Bundle and support Java Service Wrapper (http://wrapper.tanukisoftware.org) allowing to easily start/stop a service, as well as install it as a daemon. The scripts exists under `bin/service`.
</description><key id="184126">154</key><summary>Service Wrapper: A Java Service Wrapper integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-04-30T22:41:54Z</created><updated>2010-04-30T22:43:01Z</updated><resolved>2010-04-30T22:43:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-30T22:43:01Z" id="226431">Service Wrapper: A Java Service Wrapper integration, closed by 97958c3a66b16536ce59176eaa018e5d883fc5e8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Time Memory Leak: Search requests don't eagerly clean the search context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/153</link><project id="" key="" /><description>When executing a search request, they don't get eagerly cleaned properly, which means they will get cleaned after the default keep alive timeout (5 minutes). Under heavy search load, this might cause memory problems.
</description><key id="183490">153</key><summary>Time Memory Leak: Search requests don't eagerly clean the search context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.07.0</label></labels><created>2010-04-29T22:48:04Z</created><updated>2010-04-29T22:48:42Z</updated><resolved>2010-04-29T22:48:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-29T22:48:42Z" id="225520">Time Memory Leak: Search requests don't eagerly clean the search context, closed by 30aae506f3308e2b372f27633b48772e29188f13.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContent - An abstraction on top of content (JSON inspired)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/152</link><project id="" key="" /><description>An xcontent abstraction on top how elasticsearch handles data (both parsing and generation). Inspired by JSON format (actually, one by one to json) with two default implementations: JSON, and XSON. XSON (maps to Content-Type `application/xson`) is an optimized binary representation of JSON.

In general, content in elasticsearch is broken down into the following:
- Indexed content. In this case, the content type can be derived from the stream (thought can be set using `Content-Type`.
- Request: Search, Count (query), and other request body allow for xContent data now. Again, the content type can be derived from the stream.
- Response: The format of the response. Uses the `Content-Type` for this. If not available, and REST body is available, then tries to derive it from it. If not, defaults to JSON.

In Java, all custom builders (aside from index source) have been renamed. This include the query / filter builders. By default, they now generate xson format (the "user" is not aware of the format used, and xson is more optimized).
</description><key id="183392">152</key><summary>XContent - An abstraction on top of content (JSON inspired)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-04-29T20:09:47Z</created><updated>2010-04-29T20:10:59Z</updated><resolved>2010-04-29T20:10:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-29T20:10:59Z" id="225384">XContent - An abstraction on top of content (JSON inspired), closed by 34d99c39a55b542ce069b2bd4157707642e199f1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis ICU Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/151</link><project id="" key="" /><description>A plugin using ICU (http://icu-project.org/) to allow for unicode normalization, collation and folding. The plugin includes the following analysis token files:

ICU Normalization:

Normalizes characters as explained here: http://userguide.icu-project.org/transforms/normalization. it registeres itself by default under `icu_normalizer` or `icuNormalizer` using the default settings. Allows for the `name` parameter to be provided which can include the following values: `nfc`, `nfkc`, and `nfkc_cf`. 

Sample setting:

```
curl -XPUT 'http://localhost:9200/test_1/'  -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "collation" : {
                    "tokenizer" : "keyword",
                    "filter" : ["icu_normalizer"]
                }
            }
        }
    }
}
'
```

ICU Folding:

Folding of unicode characters based on UTR#30. It registeres itself under `icu_folding` and `icuFolding` names. Sample setting:

```
curl -XPUT 'http://localhost:9200/test_1/'  -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "collation" : {
                    "tokenizer" : "keyword",
                    "filter" : ["icu_folding"]
                }
            }
        }
    }
}
'
```

ICU Collation:

Uses collation token filter. Allows to either specify the rules for collation (defined here http://www.icu-project.org/userguide/Collate_Customization.html) using the `rules` parameter (can point to a location or expressed in the settings, location can be relative to config location), or using the `language` parameter (further specialized by `country` and `variant`). By default registers under `icu_collation` or `icuCollation` and uses the default locale.

Sample settings:

```
curl -XPUT 'http://localhost:9200/test_1/'  -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "collation" : {
                    "tokenizer" : "keyword",
                    "filter" : ["icu_collation"]
                }
            }
        }
    }
}
'
```

Sample custom collation:

```
curl -XPUT 'http://localhost:9200/test_1/'  -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "collation" : {
                    "tokenizer" : "keyword",
                    "filter" : ["icu_collation"]
                }
            },
            "filter" : {
                "myCollator" : {
                    type : "icu_collator",
                    language : "en"
                }
            }
        }
    }
}
'
```
</description><key id="181146">151</key><summary>Analysis ICU Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-04-27T20:53:55Z</created><updated>2010-04-27T20:54:38Z</updated><resolved>2010-04-27T20:54:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-27T20:54:38Z" id="222589">Analysis ICU Plugin, closed by 11e4ad9bd6ae360a2e7ef7d1cd78e8e7b4e86e02
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: Zen Discovery Module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/150</link><project id="" key="" /><description>An implementation of the discovery module built from the ground up to be optimized for elasticsearch. Supports both multicast and unicast discoveries with pluggable support (for simpler cloud integrations).

By default, the multicast discovery is enabled. Multicast settings:

```
# discovery.zen.ping.multicast.enabled: Is multicast discovery enabled. Defaults to `true`.
# discovery.zen.ping.multicast.address: The IP address to send the multicast packets on.
# discovery.zen.ping.multicast.port: The port to send the packets on. Defaults to `54328`.
# discovery.zen.ping.multicast.group: The multicast group address. Defaults to `224.2.2.4`.
# discovery.zen.ping.multicast.ttl: The packet time to live. Defaults to `3`.
```

The unicast discovery allows to perform discovery when multicast is disabled. Settings:

```
# discovery.zen.ping.unicast.hosts: Array of hosts to ping. By setting it, it will automatically enabled multicast.
```

Sample unicast setting:

```
discovery:
  zen.ping:
    multicast.enabled: false
    unicast.hosts:
      - 192.168.10.21:9300
      - 192.168.10.22:9301
      - 192.168.10.23:9302
      - 192.168.10.24:9303
```

There are more global settings that can be set, including:

```
# discovery.zen.master: Should this node be allowed to be a master in the cluster. Defaults to `true`. Note, when `node.client` is set to `true`, it will automatically set this setting to `false`.
# discovery.zen.initial_ping_timeout: The initial ping timeout to wait for discovery of other nodes. Defaults to `3s`.
```
</description><key id="179474">150</key><summary>Discovery: Zen Discovery Module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>v0.07.0</label></labels><created>2010-04-25T19:05:28Z</created><updated>2010-04-25T19:06:09Z</updated><resolved>2010-04-25T19:06:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-25T19:06:08Z" id="220163">implemented. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting broken when query is on `_all` field or with prefixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/149</link><project id="" key="" /><description>Hiya

Load this data:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : "2",
   "date" : "2010-04-2 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : "3",
   "date" : "2010-04-3 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : "4",
   "date" : "2010-04-4 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : "5",
   "date" : "2010-04-5 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : "6",
   "date" : "2010-04-6 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : "7",
   "date" : "2010-04-7 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : "8",
   "date" : "2010-04-8 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : "9",
   "date" : "2010-04-9 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : "10",
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : "11",
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : "12",
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : "13",
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : "14",
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : "15",
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : "16",
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : "17",
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : "18",
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : "19",
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : "20",
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : "21",
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : "22",
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : "23",
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : "24",
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : "25",
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : "26",
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : "27",
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : "28",
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : "29",
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
```

Then try these queries:

TERM QUERY:  WORKS

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "term" : {
         "text" : "foo"
      }
   },
   "highlight" : {
      "fields" : {
         "_all" : {}
      }
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "3",
#                "text" : "foo",
#                "date" : "2010-04-3 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          },
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

PREFIX QUERY: BROKEN

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "prefix" : {
         "text" : "foo"
      }
   },
   "highlight" : {
      "fields" : {
         "_all" : {}
      }
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : []
#             }
#          },
#          {
#             "_source" : {
#                "num" : "4",
#                "text" : "foo",
#                "date" : "2010-04-4 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1",
#             "highlight" : {
#                "_all" : []
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

QUERY STRING ON FIELD: WORKS

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "query_string" : {
         "query" : "text:foo"
      }
   },
   "highlight" : {
      "fields" : {
         "_all" : {}
      }
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "3",
#                "text" : "foo",
#                "date" : "2010-04-3 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          },
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

QUERY STRING ON ALL: BROKEN
    curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
    {
       "query" : {
          "query_string" : {
             "query" : "foo"
          }
       },
       "highlight" : {
          "fields" : {
             "_all" : {}
          }
       },
       "size" : 2
    }
    '
    # {
    #    "hits" : {
    #       "hits" : [
    #          {
    #             "_source" : {
    #                "num" : "3",
    #                "text" : "foo",
    #                "date" : "2010-04-3 00:00:00"
    #             },
    #             "_index" : "es_test_1",
    #             "_id" : "2",
    #             "_type" : "type_2",
    #             "highlight" : {
    #                "_all" : []
    #             }
    #          },
    #          {
    #             "_source" : {
    #                "num" : "6",
    #                "text" : "foo bar",
    #                "date" : "2010-04-6 00:00:00"
    #             },
    #             "_index" : "es_test_1",
    #             "_id" : "5",
    #             "_type" : "type_1",
    #             "highlight" : {
    #                "_all" : []
    #             }
    #          }
    #       ],
    #       "total" : 16
    #    },
    #    "_shards" : {
    #       "failed" : 0,
    #       "successful" : 10,
    #       "total" : 10
    #    }
    # }
</description><key id="176884">149</key><summary>Highlighting broken when query is on `_all` field or with prefixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-21T20:23:02Z</created><updated>2010-04-27T16:44:58Z</updated><resolved>2010-04-27T16:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-27T16:44:58Z" id="222257">Duplicate issue - closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting broken when query is on `_all` field or with prefixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/148</link><project id="" key="" /><description>Hiya

Load this data:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : "2",
   "date" : "2010-04-2 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : "3",
   "date" : "2010-04-3 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : "4",
   "date" : "2010-04-4 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : "5",
   "date" : "2010-04-5 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : "6",
   "date" : "2010-04-6 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : "7",
   "date" : "2010-04-7 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : "8",
   "date" : "2010-04-8 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : "9",
   "date" : "2010-04-9 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : "10",
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : "11",
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : "12",
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : "13",
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : "14",
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : "15",
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : "16",
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : "17",
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : "18",
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : "19",
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : "20",
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : "21",
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : "22",
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : "23",
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : "24",
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : "25",
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : "26",
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : "27",
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : "28",
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : "29",
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
```

Then try these queries:

TERM QUERY:  WORKS

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "term" : {
         "text" : "foo"
      }
   },
   "highlight" : {
      "fields" : {
         "_all" : {}
      }
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "3",
#                "text" : "foo",
#                "date" : "2010-04-3 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          },
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

PREFIX QUERY: BROKEN

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "prefix" : {
         "text" : "foo"
      }
   },
   "highlight" : {
      "fields" : {
         "_all" : {}
      }
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : []
#             }
#          },
#          {
#             "_source" : {
#                "num" : "4",
#                "text" : "foo",
#                "date" : "2010-04-4 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1",
#             "highlight" : {
#                "_all" : []
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

QUERY STRING ON FIELD: WORKS

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "query_string" : {
         "query" : "text:foo"
      }
   },
   "highlight" : {
      "fields" : {
         "_all" : {}
      }
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "3",
#                "text" : "foo",
#                "date" : "2010-04-3 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          },
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "highlight" : {
#                "_all" : [
#                   "00:00 &lt;em&gt;foo&lt;/em&gt; "
#                ]
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

QUERY STRING ON ALL: BROKEN
    curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
    {
       "query" : {
          "query_string" : {
             "query" : "foo"
          }
       },
       "highlight" : {
          "fields" : {
             "_all" : {}
          }
       },
       "size" : 2
    }
    '
    # {
    #    "hits" : {
    #       "hits" : [
    #          {
    #             "_source" : {
    #                "num" : "3",
    #                "text" : "foo",
    #                "date" : "2010-04-3 00:00:00"
    #             },
    #             "_index" : "es_test_1",
    #             "_id" : "2",
    #             "_type" : "type_2",
    #             "highlight" : {
    #                "_all" : []
    #             }
    #          },
    #          {
    #             "_source" : {
    #                "num" : "6",
    #                "text" : "foo bar",
    #                "date" : "2010-04-6 00:00:00"
    #             },
    #             "_index" : "es_test_1",
    #             "_id" : "5",
    #             "_type" : "type_1",
    #             "highlight" : {
    #                "_all" : []
    #             }
    #          }
    #       ],
    #       "total" : 16
    #    },
    #    "_shards" : {
    #       "failed" : 0,
    #       "successful" : 10,
    #       "total" : 10
    #    }
    # }
</description><key id="176883">148</key><summary>Highlighting broken when query is on `_all` field or with prefixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.07.0</label></labels><created>2010-04-21T20:22:52Z</created><updated>2010-04-25T17:30:11Z</updated><resolved>2010-04-25T17:30:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-25T11:28:33Z" id="219957">hey, found the reason for this, working on a fix. Is there a reason there are two issues for this?
</comment><comment author="kimchy" created="2010-04-25T17:30:11Z" id="220112">Highlighting broken when query is on `_all` field or with prefixes. Add also a flag to highlight to control if filters should be highlighted or not (called highlight_filters) which defaults to true. Closed by bf6cead984ffe9fbb13d6374e2dc7ffe1411f341.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write to the logs hourly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/147</link><project id="" key="" /><description>Hiya

The logs get rotated daily, but only if something is actually written to them.  So quite often, I'm tailing the log, and I don't see the expected outcome.  This is because the log I'm tailing is actually from yesterday, and when it writes the new info, it rotates the log from under me.

If you were to write (eg) `---MARK---` every hour, then this would ensure that the logs belong to the correct day.

Alternatively, just write to the logs at midnight

ta

clint
</description><key id="176364">147</key><summary>Write to the logs hourly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-21T10:04:00Z</created><updated>2010-09-23T11:12:57Z</updated><resolved>2010-09-23T11:12:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abh" created="2010-09-23T04:29:25Z" id="424502">... or when you read from the log use `tail -F some.log` :-)
</comment><comment author="clintongormley" created="2010-09-23T11:12:56Z" id="424987">Thanks - that I didn't know!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dependency on slf4j for logging, create own internal abstraction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/146</link><project id="" key="" /><description>Create an internal logger abstraction that works automatically with slf4j/log4j/jdk (in that order) for simpler usage when embedded.
</description><key id="176088">146</key><summary>Remove dependency on slf4j for logging, create own internal abstraction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-20T21:29:13Z</created><updated>2010-04-20T21:29:51Z</updated><resolved>2010-04-20T21:29:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-20T21:29:51Z" id="215261">Remove dependency on slf4j for logging, create own internal abstraction, closed by bda476eee846b674846a0ac6b3e52e11ef275365.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow tokenizers and filters to be added as plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/145</link><project id="" key="" /><description>I'm wanting to add a tokenizer for email addresses, eg http://stackoverflow.com/questions/19014/using-lucene-to-search-for-email-addresses

It'd be great if we could add our own tokenizers/filters as plugins

ta

clint
</description><key id="175798">145</key><summary>Allow tokenizers and filters to be added as plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-20T15:12:47Z</created><updated>2010-05-12T17:19:32Z</updated><resolved>2010-05-12T17:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-20T16:19:18Z" id="214914">That said - this is not a blocker for me. I've worked around it
</comment><comment author="kimchy" created="2010-04-20T18:01:49Z" id="215035">You can do that today (though in a clunky manner). I will add an API for that.
</comment><comment author="kimchy" created="2010-05-12T17:19:32Z" id="239272">The ICU plugin now uses this, so this can be done, though Java knowledge is required...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`analyzer` not being applied to search time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/144</link><project id="" key="" /><description>Hiya

It may be something else, but it looks like the `analyzer` defined in the mapping is being applied at index time, but not at search time:

I'm trying to add a custom analyzer which will lowercase terms, but preserve them as the whole term, so eg `ABC123` becomes the single term `abc123`

However, if I search for `ABC123` I would expect it to be converted to a search for `abc123`, and this isn't happening.  If I search for `abc123` then I get the correct results:

```
curl -XPUT 'http://127.0.0.2:9200/foo/'  -d '
{
   "index" : {
      "analysis" : {
         "analyzer" : {
            "lc" : {
               "filter" : [
                  "lowercase"
               ],
               "tokenizer" : "keyword"
            }
         }
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.2:9200/foo/bar/_mapping'  -d '
{
   "properties" : {
      "myfield" : {
         "type" : "string",
         "analyzer" : "lc"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPOST 'http://127.0.0.2:9200/foo/bar'  -d '
{
   "myfield" : "ABC123"
}
'
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "f413800e-69bf-4ca6-a9bd-484e08a371cf",
#    "_type" : "bar"
# }


curl -XGET 'http://127.0.0.2:9200/foo/bar/_search'  -d '
{
   "query" : {
      "term" : {
         "myfield" : "ABC123"
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 0
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }


curl -XGET 'http://127.0.0.2:9200/foo/bar/_search'  -d '
{
   "query" : {
      "term" : {
         "myfield" : "abc123"
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "myfield" : "ABC123"
#             },
#             "_index" : "foo",
#             "_id" : "f413800e-69bf-4ca6-a9bd-484e08a371cf",
#             "_type" : "bar"
#          }
#       ],
#       "total" : 1
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }
```
</description><key id="175745">144</key><summary>`analyzer` not being applied to search time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-20T14:04:08Z</created><updated>2010-04-26T08:07:23Z</updated><resolved>2010-04-20T18:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-20T18:26:12Z" id="215066">Doh of course - it would only be analyzed if passed in as a query string, not as a term.

closing
</comment><comment author="kimchy" created="2010-04-26T08:07:23Z" id="220590">Yea, but note that there is the `field` query as well, which make things simpler than using the `query_string` query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Indices Boost to apply a boost factor to each index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/143</link><project id="" key="" /><description>A boost factor is simply multiplying the score of the query by the boost factor to provide the final scoring of each document. This is more powerful then the way it works currently, by setting the boost on the top level query, since it makes little sense to do so... .
</description><key id="175010">143</key><summary>Search API: Indices Boost to apply a boost factor to each index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-19T16:14:50Z</created><updated>2010-04-19T17:45:55Z</updated><resolved>2010-04-19T16:15:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-19T16:15:31Z" id="213720">Search API: Indices Boost to apply a boost factor to each index, closed by 2fdc49c113dc4bd2d438ae96d1e20e32fda23187.
</comment><comment author="clintongormley" created="2010-04-19T17:45:55Z" id="213793">fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count / Delete By Query API: Add source parameter to pass the json body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/142</link><project id="" key="" /><description>Allow to pass the json string body as a REST parameter as well
</description><key id="173935">142</key><summary>Count / Delete By Query API: Add source parameter to pass the json body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-17T20:06:00Z</created><updated>2010-04-20T18:17:03Z</updated><resolved>2010-04-17T20:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-17T20:06:33Z" id="212328">Count / Delete By Query API: Add source parameter to pass the json body, closed by fbc138e38ccc64df9d05297d49bac881bbb978a0.
</comment><comment author="clintongormley" created="2010-04-20T06:37:26Z" id="214434">Also required for `mlt` | `more_like_this`
</comment><comment author="kimchy" created="2010-04-20T18:17:03Z" id="215052">Added `search_source` to `mlt`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memcache Plugin: Support memcached protocol</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/141</link><project id="" key="" /><description>Allow to use memcached protocol to interact with elasticsearch. Basically, the idea is to expose elasticsearch REST interface over memcached protocol. In general, the key part of the protocol is the REST uri used. For `set` command, the body is the REST body (http body). For `get` commands, the return payload is the body. `set` translates to `POST`, `get` translates to `GET`, and `delete` translates to `DELETE`. Note, this are the only commands implemented (and `quit`).

Note, memcached imposes restrictions compared to the REST protocol. `set` can not return a body for the response, and `get` can't accept body for the request (note, the uri can have query parameters).
</description><key id="173927">141</key><summary>Memcache Plugin: Support memcached protocol</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-04-17T19:41:00Z</created><updated>2010-04-17T19:41:26Z</updated><resolved>2010-04-17T19:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-17T19:41:26Z" id="212312">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Allow to pass the body JSON also as a parameter named 'source'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/140</link><project id="" key="" /><description>Have the option to pass the JSON search request as a parameter named source.
</description><key id="173715">140</key><summary>Search API: Allow to pass the body JSON also as a parameter named 'source'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-17T10:33:00Z</created><updated>2010-04-17T10:34:09Z</updated><resolved>2010-04-17T10:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-17T10:34:09Z" id="211419">Search API: Allow to pass the body JSON also as a parameter named 'source', closed by c6d13212b67074cedb2c3f6dd6a839aa4f081914.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/139</link><project id="" key="" /><description>Implemented a built in groovy client. Its implemented as a plugin, though not really loaded when elasticsearch is started. Within the zip file, the actual client jar file exists. It is also available under maven repo.

The code looks as follows:

```
import org.elasticsearch.groovy.node.GNode
import org.elasticsearch.groovy.node.GNodeBuilder

GNodeBuilder nodeBuilder = new GNodeBuilder()
nodeBuilder.settings {
    node {
        local = true
    }
}

def node = nodeBuilder.node()

def indexR = node.client.index {
    index "test"
    type "type1"
    id "1"
    source {
        test = "value"
        complex {
            value1 = "value1"
            value2 = "value2"
        }
    }
}
assertThat indexR.response.index, equalTo("test")
assertThat indexR.response.type, equalTo("type1")
assertThat indexR.response.id, equalTo("1")

def delete = node.client.delete {
    index "test"
    type "type1"
    id "1"
}
assertThat delete.response.index, equalTo("test")
assertThat delete.response.type, equalTo("type1")
assertThat delete.response.id, equalTo("1")

def refresh = node.client.admin.indices.refresh {}
assertThat refresh.response.failedShards, equalTo(0)

def get = node.client.get {
    index "test"
    type "type1"
    id "1"
}
assertThat get.response.exists, equalTo(false)

indexR = node.client.index {
    index "test"
    type "type1"
    id "1"
    source {
        test = "value"
        complex {
            value1 = "value1"
            value2 = "value2"
        }
    }
}
assertThat indexR.response.index, equalTo("test")
assertThat indexR.response.type, equalTo("type1")
assertThat indexR.response.id, equalTo("1")

refresh = node.client.admin.indices.refresh {}
assertThat refresh.response.failedShards, equalTo(0)

def count = node.client.count {
    indices "test"
    types "type1"
    query {
        term {
            test = "value"
        }
    }
}
assertThat count.response.failedShards, equalTo(0)
assertThat count.response.count, equalTo(1l)

def search = node.client.search {
    indices "test"
    types "type1"
    source {
        query {
            term(test: "value")
        }
    }
}
assertThat search.response.failedShards, equalTo(0)
assertThat search.response.hits.totalHits, equalTo(1l)
assertThat search.response.hits[0].source.test, equalTo("value")

def deleteByQuery = node.client.deleteByQuery {
    indices "test"
    query {
        term("test": "value")
    }
}
assertThat deleteByQuery.response.indices.test.failedShards, equalTo(0)

refresh = node.client.admin.indices.refresh {}
assertThat refresh.response.failedShards, equalTo(0)

get = node.client.get {
    index "test"
    type "type1"
    id "1"
}
assertThat get.response.exists, equalTo(false)
```
</description><key id="173396">139</key><summary>Groovy Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.07.0</label></labels><created>2010-04-16T19:17:15Z</created><updated>2010-04-16T19:17:52Z</updated><resolved>2010-04-16T19:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-16T19:17:49Z" id="208355">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Batch Submit API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/138</link><project id="" key="" /><description>In cases where a lot of existing data needs to be indexed, it would be helpful to support batching of documents in an attempt to reduce the message passing overhead associated with HTTP.

The goal would be to submit a batch of documents using a single HTTP call. Tradeoffs associated with such a feature are expected.
</description><key id="173373">138</key><summary>Batch Submit API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">egaumer</reporter><labels /><created>2010-04-16T18:02:24Z</created><updated>2011-01-23T12:16:26Z</updated><resolved>2011-01-23T12:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2010-05-11T15:31:17Z" id="237888">Try to use the java-api instead of curl. 
On my machine I was able to index 1 Mio documents a 4Kb in 50 minutes :)
Look at that FeederDemo.java:

http://pascal.selfip.org/jforum/posts/list/117.page

Make sure you already have a data-node running (bin/elasticsearch -f should do) and that you execute the FeederDemo.java on the same machine (or at least on a network where the Demo and the data-node see each other).

oo
</comment><comment author="kimchy" created="2010-05-12T00:41:08Z" id="238459">Yo, have not forgotten about this issue. Its a bit tricky to implement correctly. Will get 0.7 out of the way and see if I can get back to it.
</comment><comment author="kimchy" created="2011-01-23T12:16:26Z" id="700295">Closing as this has been implemented in the bulk api.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices_boost seems to have no effect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/137</link><project id="" key="" /><description>Hiya

Maybe I'm testing the wrong thing, but it doesn't look like `indices_boost` is having any effect:

Load this data:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : "2",
   "date" : "2010-04-2 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : "3",
   "date" : "2010-04-3 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : "4",
   "date" : "2010-04-4 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : "5",
   "date" : "2010-04-5 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : "6",
   "date" : "2010-04-6 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : "7",
   "date" : "2010-04-7 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : "8",
   "date" : "2010-04-8 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : "9",
   "date" : "2010-04-9 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : "10",
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : "11",
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : "12",
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : "13",
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : "14",
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : "15",
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : "16",
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : "17",
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : "18",
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : "19",
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : "20",
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : "21",
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : "22",
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : "23",
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : "24",
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : "25",
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : "26",
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : "27",
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : "28",
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : "29",
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
```

Then try these queries:

```
curl -XGET 'http://127.0.0.2:9200/_all/_search?pretty=true'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "field" : {
         "text" : "foo"
      }
   },
   "explain" : 0,
   "indices_boost" : {
      "es_test_2" : 0.1,
      "es_test_1" : 5000
   },
   "size" : 2
}
'
```

and

```
curl -XGET 'http://127.0.0.2:9200/_all/_search?pretty=true'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "field" : {
         "text" : "foo"
      }
   },
   "explain" : 0,
   "indices_boost" : {
      "es_test_2" : 5000,
      "es_test_1" : 0.1
   },
   "size" : 2
}
'
```

They both return:

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

I would have thought that they would have returned the same docs, but in the opposite order.  Instead, they both return the docs in the same order: (without the `fields` param):

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "3",
#                "text" : "foo",
#                "date" : "2010-04-3 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : "5",
#                "text" : "foo",
#                "date" : "2010-04-5 00:00:00"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```

When i do an explain, it seems to be taking the boost into account, at least for one index:

```
curl -XGET 'http://127.0.0.2:9200/_all/_search?pretty=true'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "field" : {
         "text" : "foo"
      }
   },
   "explain" : 1,
   "indices_boost" : {
      "es_test_2" : 5000,
      "es_test_1" : 0.1
   },
   "size" : 2
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_index" : "es_test_1",
#             "_id" : "2",
#             "_type" : "type_2",
#             "_explanation" : {
#                "value" : 1.287682,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "tf(termFreq(text:foo)=1)"
#                   },
#                   {
#                      "value" : 1.287682,
#                      "description" : "idf(docFreq=2, maxDocs=4)"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "fieldNorm(field=text, doc=0)"
#                   }
#                ],
#                "description" : "fieldWeight(text:foo in 0), product of:"
#             }
#          },
#          {
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2",
#             "_explanation" : {
#                "value" : 1.1823214,
#                "details" : [
#                   {
#                      "value" : 0.99999994,
#                      "details" : [
#                         {
#                            "value" : 5000,
#                            "description" : "boost"
#                         },
#                         {
#                            "value" : 1.1823215,
#                            "description" : "idf(docFreq=4, maxDocs=6)"
#                         },
#                         {
#                            "value" : 0.00016915871,
#                            "description" : "queryNorm"
#                         }
#                      ],
#                      "description" : "queryWeight(text:foo^5000.0), product of:"
#                   },
#                   {
#                      "value" : 1.1823215,
#                      "details" : [
#                         {
#                            "value" : 1,
#                            "description" : "tf(termFreq(text:foo)=1)"
#                         },
#                         {
#                            "value" : 1.1823215,
#                            "description" : "idf(docFreq=4, maxDocs=6)"
#                         },
#                         {
#                            "value" : 1,
#                            "description" : "fieldNorm(field=text,doc=0)"
#                         }
#                      ],
#                      "description" : "fieldWeight(text:foo in 0),product of:"
#                   }
#                ],
#                "description" : "weight(text:foo^5000.0 in 0), product of:"
#             }
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```
</description><key id="173304">137</key><summary>indices_boost seems to have no effect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-16T16:31:40Z</created><updated>2010-05-28T08:46:04Z</updated><resolved>2010-05-28T08:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-19T16:16:36Z" id="213721">I fixed it just now in #143, and now the boost per index is actually a boost factor (the explanation reflects that as well...). Can you verify that it works for you?
</comment><comment author="kimchy" created="2010-05-28T08:46:04Z" id="256475">ok, closing this, it should work (have tests). If not, reopen...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scrolling issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/136</link><project id="" key="" /><description>Hiya

A few things:
- a search with a `scroll` parameter, returns `scrollId`.  Shouldn't that be `scroll_id`
- the docs for scrolling http://www.elasticsearch.com/docs/elasticsearch/rest_api/search/#Scrolling don't actually tell you how it works
- from the code it looks like `/_search/scroll/{scroll_id}` should work, but it doesn't.  However, `_search/scroll?scroll_id={scroll_id}` does
- Apparently, scrolling returns `size x shards` results. This doesn't make sense to me, when the original search returns `size` results. Surely it should be the same?
- And finally, scrolling throws errors for me - see below

Load this data (I was running 3 nodes):

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : "2",
   "date" : "2010-04-2 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : "3",
   "date" : "2010-04-3 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : "4",
   "date" : "2010-04-4 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : "5",
   "date" : "2010-04-5 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : "6",
   "date" : "2010-04-6 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : "7",
   "date" : "2010-04-7 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : "8",
   "date" : "2010-04-8 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : "9",
   "date" : "2010-04-9 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : "10",
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : "11",
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : "12",
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : "13",
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : "14",
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : "15",
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : "16",
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : "17",
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : "18",
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : "19",
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : "20",
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : "21",
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : "22",
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : "23",
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : "24",
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : "25",
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : "26",
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : "27",
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : "28",
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : "29",
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
```

Then run these tests (you'll need to update the `scroll_id`):
    curl -XGET 'http://127.0.0.2:9201/es_test_1/_search?scroll=5m'  -d '
    {
       "sort" : [
          "num"
       ],
       "query" : {
          "match_all" : {}
       },
       "size" : 2
    }
    '
    # {
    #    "_scrollId" : "cXVlcnlUaGVuRmV0Y2g7MjpnZXRhZml4LTE1NDQ0OzI6Z2V
    # &gt;    0YWZpeC0zNTAzODszOmdldGFmaXgtMTU0NDQ7MTpnZXRhZml4LTE1NDQ0OzE
    # &gt;    6Z2V0YWZpeC0zNTAzODs=",
    #    "hits" : {
    #       "hits" : [
    #          {
    #             "_source" : {
    #                "num" : "2",
    #                "text" : "foo",
    #                "date" : "2010-04-2 00:00:00"
    #             },
    #             "_index" : "es_test_1",
    #             "_id" : "1",
    #             "_type" : "type_1"
    #          },
    #          {
    #             "_source" : {
    #                "num" : "3",
    #                "text" : "foo",
    #                "date" : "2010-04-3 00:00:00"
    #             },
    #             "_index" : "es_test_1",
    #             "_id" : "2",
    #             "_type" : "type_2"
    #          }
    #       ],
    #       "total" : 14
    #    },
    #    "_shards" : {
    #       "failed" : 0,
    #       "successful" : 5,
    #       "total" : 5
    #    }
    # }

```
curl -XGET 'http://127.0.0.2:9201/_search/scroll?scroll_id=cXVlcnlUaGVuRmV0Y2g7MjpnZXRhZml4LTE1NDQ0OzI6Z2V0YWZpeC0zNTAzODszOmdldGFmaXgtMTU0NDQ7MTpnZXRhZml4LTE1NDQ0OzE6Z2V0YWZpeC0zNTAzODs=' 
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : "6",
#                "text" : "foo bar",
#                "date" : "2010-04-6 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "5",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : "18",
#                "text" : "baz",
#                "date" : "2010-04-18 00:00:00"
#             },
#             "_index" : "es_test_1",
#             "_id" : "17",
#             "_type" : "type_1"
#          }
#       ],
#       "total" : 5
#    },
#    "_shards" : {
#       "failures" : [
#          {
#             "reason" : "SearchContextMissingException[No search c
# &gt;             ontext found for id [1], timed out]"
#          },
#          {
#             "reason" : "RemoteTransportException[[Alyssa Moy][ine
# &gt;             t[/127.0.0.2:9302]][search/phase/query/scroll]]; ne
# &gt;             sted: SearchContextMissingException[No search conte
# &gt;             xt found for id [1], timed out]; "
#          },
#          {
#             "reason" : "RemoteTransportException[[Alyssa Moy][ine
# &gt;             t[/127.0.0.2:9302]][search/phase/query/scroll]]; ne
# &gt;             sted: SearchContextMissingException[No search conte
# &gt;             xt found for id [3], timed out]; "
#          }
#       ],
#       "failed" : 3,
#       "successful" : 2,
#       "total" : 5
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_search/scroll?scroll_id=cXVlcnlUaGVuRmV0Y2g7MjpnZXRhZml4LTE1NDQ0OzI6Z2V0YWZpeC0zNTAzODszOmdldGFmaXgtMTU0NDQ7MTpnZXRhZml4LTE1NDQ0OzE6Z2V0YWZpeC0zNTAzODs=' 
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 1
#    },
#    "_shards" : {
#       "failures" : [
#          {
#             "reason" : "SearchContextMissingException[No search c
# &gt;             ontext found for id [1], timed out]"
#          },
#          {
#             "reason" : "RemoteTransportException[[Alyssa Moy][ine
# &gt;             t[/127.0.0.2:9302]][search/phase/query/scroll]]; ne
# &gt;             sted: SearchContextMissingException[No search conte
# &gt;             xt found for id [3], timed out]; "
#          },
#          {
#             "reason" : "RemoteTransportException[[Alyssa Moy][ine
# &gt;             t[/127.0.0.2:9302]][search/phase/query/scroll]]; ne
# &gt;             sted: SearchContextMissingException[No search conte
# &gt;             xt found for id [1], timed out]; "
#          },
#          {
#             "reason" : "RemoteTransportException[[Alyssa Moy][ine
# &gt;             t[/127.0.0.2:9302]][search/phase/query/scroll]]; ne
# &gt;             sted: SearchContextMissingException[No search conte
# &gt;             xt found for id [2], timed out]; "
#          }
#       ],
#       "failed" : 4,
#       "successful" : 1,
#       "total" : 5
#    }
# }
```
</description><key id="173242">136</key><summary>Scrolling issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.14.0</label></labels><created>2010-04-16T15:01:18Z</created><updated>2010-12-08T20:21:11Z</updated><resolved>2010-11-29T08:06:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mootpointer" created="2010-07-12T00:12:05Z" id="307984">I'm also seeing these issues using master. I have 30 shards for my index, and I'm only getting 10 successfully responding on the first request, and sometimes none as things continue. 
</comment><comment author="clintongormley" created="2010-07-17T11:28:52Z" id="316073">Yep - scrolling is still broken
</comment><comment author="otisg" created="2010-11-25T10:17:52Z" id="566243">Same here.  I'm wondering if Shay can give us a hint about the ETA for this?
</comment><comment author="kimchy" created="2010-11-28T23:32:48Z" id="571795">I am going to push for getting scrolling working properly for 0.14.
</comment><comment author="kimchy" created="2010-11-29T00:06:03Z" id="571841">Scrolling issues, fix releasing search context eagerly, they should not be released when scrolling, closed by 84f97e96a5e415a5af6ce452e3797f17242b994a.
</comment><comment author="kimchy" created="2010-12-08T18:58:25Z" id="597709">Hey, added exactly the same test to the test suite I have, and it seems to work: https://github.com/elasticsearch/elasticsearch/commit/e1495ff8be7c9c90ba13c330e15920aea6610ab5.

Obviously, you are getting this, trying to think of the best way to recreate the same thing you are getting. Maybe you can mail me the Java env/project test case you have? And I will run it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins not loaded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/135</link><project id="" key="" /><description>Hiya

I've created a plugins dir in the elasticsearch directory, which contains:
- elasticsearch-attachments-0.6.0.jar  
- elasticsearch-attachments-0.6.0-javadoc.jar
-  elasticsearch-attachments-0.6.0-sources.jar
-  tika-app-0.7.jar

However, when I start the server it says:
    [15:58:43,262][INFO ][node                     ] [Ezekiel] {ElasticSearch/0.6.0}: Initializing ...
    [15:58:43,266][INFO ][plugins                  ] [Ezekiel] Loaded []
    [15:58:45,536][INFO ][node                     ] [Ezekiel] {ElasticSearch/0.6.0}: Initialized
    [15:58:45,537][INFO ][node                     ] [Ezekiel] {ElasticSearch/0.6.0}: Starting ...

clint
</description><key id="172330">135</key><summary>Plugins not loaded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-15T14:00:17Z</created><updated>2010-04-15T14:23:41Z</updated><resolved>2010-04-15T14:23:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-15T14:23:40Z" id="188309">Ooops - just reread the docs - don't unzip the plugins :)

Works correctly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Allow for CamelCase field names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/134</link><project id="" key="" /><description /><key id="171334">134</key><summary>Query DSL: Allow for CamelCase field names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-14T08:48:06Z</created><updated>2010-04-14T08:48:47Z</updated><resolved>2010-04-14T08:48:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-14T08:48:46Z" id="186881">Query DSL: Allow for CamelCase field names, closed by fcb99b4d9bd432a467a521df95e9b2daf07a5550.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Allow to define CamelCase JSON fields in mapping definitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/133</link><project id="" key="" /><description>They will be converted to underscore case fields
</description><key id="171244">133</key><summary>Mapper: Allow to define CamelCase JSON fields in mapping definitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-14T05:53:22Z</created><updated>2010-04-14T05:54:58Z</updated><resolved>2010-04-14T05:54:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-14T05:54:58Z" id="186797">Mapper: Allow to define CamelCase JSON fields in mapping definitions, closed by 2d6de97069b2173ac800894cfd0f0dbb03e8b526.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reload indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/132</link><project id="" key="" /><description>Would it be possible to add a command which would instruct ES to reload the indices, like it does at startup?

What I'm thinking is that I could (for instance) make changes to my index on a different cluster, move the indices over to the live cluster, then issue this command, instead of having to bring the cluster down then start it up again.

Possible?

It'd need to make sure all of the current data is snapshot'ed before restarting I suppose
</description><key id="170659">132</key><summary>Reload indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-13T11:40:34Z</created><updated>2012-07-05T07:54:35Z</updated><resolved>2012-07-05T07:54:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-13T14:58:59Z" id="186006">A bit tricky, but should be possible. Let me think about it a bit, and ping me later to make sure I do (deep diving into some other features currently)
</comment><comment author="karussell" created="2011-01-02T15:31:09Z" id="646567">I'm not the owner of this bug ... but would this be possible?
</comment><comment author="clintongormley" created="2011-03-20T13:51:02Z" id="895179">Just pinging you about this feature, as requested
</comment><comment author="clintongormley" created="2012-07-05T07:54:35Z" id="6773171">Closed by issue #2067
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTP/Transport Modules: Default to not set the reuse_address setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/131</link><project id="" key="" /><description>Currently, it is set to `true`, default to not set it as it might cause problems (on windows specifically).
</description><key id="170337">131</key><summary>HTTP/Transport Modules: Default to not set the reuse_address setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-13T00:11:02Z</created><updated>2010-04-16T18:56:34Z</updated><resolved>2010-04-13T00:11:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-13T00:11:53Z" id="185429">Fixed.
</comment><comment author="tfreitas" created="2010-04-16T18:56:34Z" id="208345" /></comments><attachments /><subtasks /><customfields /></item><item><title>Support unicode collations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/130</link><project id="" key="" /><description>Hiya

Below is a test case which demonstrates that ES is not using a proper Unicode collation to sort.  "Jo&#227;o" should be sorted before "Jordania":

```
curl -XPUT 'http://127.0.0.2:9200/test_1/'  -d '
{}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.2:9200/test_1/foo/_mapping'  -d '
{
   "properties" : {
      "text" : {
         "type" : "string",
         "analyzer" : "keyword"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.2:9200/test_1/foo/1'  -d '
{
   "text" : "Jordania"
}
'
# {
#    "ok" : true,
#    "_index" : "test_1",
#    "_id" : "1",
#    "_type" : "foo"
# }


curl -XPUT 'http://127.0.0.2:9200/test_1/foo/2'  -d '
{
   "text" : "Jo&#227;o"
}
'
# {
#    "ok" : true,
#    "_index" : "test_1",
#    "_id" : "2",
#    "_type" : "foo"
# }


curl -XGET 'http://127.0.0.2:9200/test_1/_search'  -d '
{
   "sort" : [
      "text"
   ],
   "query" : {
      "match_all" : {}
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "text" : "Jordania"
#             },
#             "_index" : "test_1",
#             "_id" : "1",
#             "_type" : "foo"
#          },
#          {
#             "_source" : {
#                "text" : "Jo&#227;o"
#             },
#             "_index" : "test_1",
#             "_id" : "2",
#             "_type" : "foo"
#          }
#       ],
#       "total" : 2
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }
```

http://lucene.apache.org/java/3_0_0/api/all/org/apache/lucene/collation/package-summary.html

thanks

Clint
</description><key id="170120">130</key><summary>Support unicode collations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-12T18:53:06Z</created><updated>2010-05-28T08:35:41Z</updated><resolved>2010-05-28T08:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-28T08:35:41Z" id="256460">hey, with the icu plugin, I think its ok?, closing it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version tags should point to the official version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/129</link><project id="" key="" /><description>Hiya

When you make releases in the future, could you make the tag'ed commit include the real version number (eg `0.6.0` instead of `0.6.0-SNAPSHOT`)

Otherwise there is no way to compile the official release without changing the checked out code

ta

clint
</description><key id="170059">129</key><summary>Version tags should point to the official version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-12T17:04:31Z</created><updated>2010-05-28T09:24:26Z</updated><resolved>2010-05-28T09:24:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-12T23:54:43Z" id="185423">Yea, I was thinking about that. But then you can get the tag, make a change, compile, and it will be 0.6. I saw on other projects that they were tagging the SNAPSHOT version... .
</comment><comment author="clintongormley" created="2010-04-13T08:33:41Z" id="185714">I don't know what other projects are doing, but I think if somebody downloads the source and makes changes, they should change the version number as well.

Also, if they WANT to build version `0.6.0` with their own changes, there is nothing to stop them from editing the version string anyway, so it's not providing any real protection by keeping the `-SNAPSHOT` suffix in there
</comment><comment author="kimchy" created="2010-04-13T10:59:54Z" id="185830">Make sense, no problem. I do what my users want (you seem to occupy a big chunk of them ;) ). Will start doing it form the next release.
</comment><comment author="clintongormley" created="2010-04-13T11:38:07Z" id="185858">:)
</comment><comment author="kimchy" created="2010-05-28T09:24:25Z" id="256507">already implemented since 0.7, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a -v flag to output version information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/128</link><project id="" key="" /><description>Hiya

It'd be useful to have a `-version` flag so that we can get the actual version number from a binary (eg for my Alien::ElasticSearch module)

ta

clint
</description><key id="170056">128</key><summary>Add a -v flag to output version information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.07.0</label></labels><created>2010-04-12T17:00:43Z</created><updated>2010-04-26T08:06:03Z</updated><resolved>2010-04-26T08:06:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-12T23:53:35Z" id="185422">In which case it should just output the version and exit?
</comment><comment author="clintongormley" created="2010-04-15T14:04:41Z" id="188293">Yes, correct.
</comment><comment author="kimchy" created="2010-04-26T08:05:29Z" id="220586">I am going to add a `-v` option. This is mainly since getopt command seems to have problems with long parameters (not cross platform)
</comment><comment author="kimchy" created="2010-04-26T08:06:03Z" id="220587">Add a -v flag to output version information, closed by 2878ae7dd6070549aa7f17b75330500744603416
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Cassandra gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/127</link><project id="" key="" /><description>Create Cassandra gateway or some other nosql solution based in 0.7 maybe?
</description><key id="168021">127</key><summary>Create Cassandra gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hz</reporter><labels /><created>2010-04-09T08:56:48Z</created><updated>2010-12-11T13:23:47Z</updated><resolved>2010-12-11T13:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-09T11:24:38Z" id="182483">Sounds like a plan!, which other nosql were you thinking about?
</comment><comment author="hz" created="2010-04-09T11:44:54Z" id="182494">Because I see there is no issue about the nosql gateway plan. So create this one.
Cassandra I most prefer.
</comment><comment author="rb2k" created="2010-07-08T23:23:15Z" id="304994">riak might be worth looking into too... although they're supposedly rolling out their own lucene based search solution
</comment><comment author="kimchy" created="2010-07-09T00:11:45Z" id="305030">I am not sure that riak fits well to store blobs of data, which is what the gateway stores.
</comment><comment author="benoitc" created="2010-09-23T03:20:10Z" id="424404">hfs/hadoop ?
</comment><comment author="kimchy" created="2010-09-23T17:44:58Z" id="425792">There is already a gateway that stores the index in HDFS. The usage of "shared storage" gateways will need to be revisited once 0.11 is out which supports local gateway (no need for a shared one).
</comment><comment author="xeraa" created="2010-10-08T10:20:02Z" id="455920">Any news on a possible Cassandra integration?
There's Lucandra but ES looks sexier :)
</comment><comment author="kimchy" created="2010-10-08T10:48:06Z" id="455954">What type of integration are you after? The gateway aspect with cassandra is just an option, which you won't need when using local gateway (for example). Its a different feature than storing the terms in cassandra as Lucandara does.
</comment><comment author="xeraa" created="2010-10-08T12:54:28Z" id="456109">Thanks for the quick reply.
After taking another look at how ES uses gateways, I realized that I don't need them for my scenario. Sorry for the confusion.
</comment><comment author="hz" created="2010-12-11T13:23:47Z" id="604721">Yeah. No need to implement Cassandra gateway. Close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support REST API using the transport layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/126</link><project id="" key="" /><description>This will be a REST API exposed over TCP, which means that it will perform better to HTTP. Details on the schema of the request will be posted once I implement it :).
</description><key id="167984">126</key><summary>Support REST API using the transport layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>discuss</label></labels><created>2010-04-09T06:46:25Z</created><updated>2014-07-18T08:33:00Z</updated><resolved>2014-07-18T08:33:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T08:32:59Z" id="49407554">Discussed with Shay - He doesn't remember what this was about. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename Server (and all other Server related classes) to Node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/125</link><project id="" key="" /><description>It makes much more sense for Server and ServerBuilder to be called Node and NodeBuilder.
</description><key id="167955">125</key><summary>Rename Server (and all other Server related classes) to Node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-09T04:28:55Z</created><updated>2010-04-09T04:29:23Z</updated><resolved>2010-04-09T04:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-09T04:29:23Z" id="182264">Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>shutdown nodes gracefully</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/124</link><project id="" key="" /><description>Hiya

Thinking about how to shut down nodes gracefully. I'd say it should work something like this:
- i use the REST API to shutdown node X in 1 minute
- node X stops accepting new connections immediately
- node X is marked as 'shutting down', eg in `curl -XGET 'http://127.0.0.2:9200/_cluster/nodes'`
- node X continues to serve existing requests
- after 1 minute, node X starts its shutdown procedure (snapshotting etc)
</description><key id="167417">124</key><summary>shutdown nodes gracefully</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-08T09:31:56Z</created><updated>2011-03-20T13:50:12Z</updated><resolved>2011-03-20T13:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-03-20T13:50:12Z" id="895177">This seems to work correctly in recent releases
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi_field not returning for default field for certain type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/123</link><project id="" key="" /><description>Background to this is issue is discussed here

http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/3e20079f0225b531/978c64ecbb017adf#978c64ecbb017adf

What seems to be happening is that the for the default field in a multi_field type, doesn't seem to hit the appropriate JsonFieldMapper for the type in question (or so I what appears to be happening when attempting to breakpoint debug it).

So taking the following two mappings, for the date and string type:
- /integration-test-multifield-date-mapping.json

&lt;pre&gt;
    {
        "datetest" : {
            "properties" : {
                "date" : {
                    "type" : "multi_field",
                    "fields" : {
                        "date" : { type : "date", index : "analyzed" },            
                        "orig" : { type : "date", index : "analyzed" },                     
                        "untouched" : {type: "date",index : "not_analyzed"}
                    }
                }
            }
        }
    }
&lt;/pre&gt;

- /integration-test-multifield-string-mapping.json

&lt;pre&gt;
    {
        "stringtest" : {
            "properties" : {
                "string" : {
                    "type" : "multi_field",
                    "fields" : {
                        "string" : { type : "string", index : "analyzed" },            
                        "orig" : { type : "string", index : "analyzed" },                     
                        "untouched" : {type: "string", index : "not_analyzed"}
                    }
                }
            }
        }
    }
&lt;/pre&gt;


and the following code:

Setup of test (it's in junit):

&lt;pre&gt;
    private final Logger logger = Loggers.getLogger(getClass());

    private static Server server;

        @BeforeClass
    public static void setupServer() {
        server = serverBuilder().settings(
                settingsBuilder().put("node.local", true)).server();
    }

    @AfterClass
    public static void closeServer() {
        server.close();
    }

    @Before
    public void createIndex() {
        logger.info("creating index [test]");
        server.client().admin().indices().create(
                createIndexRequest("test").settings(
                        settingsBuilder().put("index.numberOfReplicas", 0).put(
                                "index.numberOfShards", 1))).actionGet();
        logger.info("Running Cluster Health");
        ClusterHealthResponse clusterHealth = server.client().admin().cluster()
                .health(clusterHealth().waitForGreenStatus()).actionGet();
        logger.info("Done Cluster Health, status " + clusterHealth.status());
        assertThat(clusterHealth.timedOut(), equalTo(false));
        assertThat(clusterHealth.status(), equalTo(ClusterHealthStatus.GREEN));
    }

    @After
    public void deleteIndex() {
        logger.info("deleting index [test]");
        server.client().admin().indices().delete(deleteIndexRequest("test"))
                .actionGet();
    }
    
&lt;/pre&gt;


The two test methods:
- String

&lt;pre&gt;
        @Test
    public void typeStringMultiFieldTest() throws IOException
    {
    
        String mapping = copyToStringFromClasspath("/integration-test-multifield-string-mapping.json");
        System.out.println("Mapping:");
        System.out.println("========");
        System.out.println(mapping);
        String val = "20100406";
        
        server.client().admin().indices().putMapping(putMappingRequest("test").mappingSource(mapping)).actionGet();

        server.client().index(indexRequest("test").type("stringtest").id("1")
                .source(jsonBuilder().startObject().field("string", val).endObject())).actionGet();
        
        server.client().admin().indices().refresh(refreshRequest()).actionGet();
       
        CountResponse countResponse1 = server.client().count(countRequest("test").querySource(fieldQuery("string", val))).actionGet();
        System.out.println("string:" + countResponse1.count());
            
        CountResponse countResponse2 = server.client().count(countRequest("test").querySource(fieldQuery("string.string", val))).actionGet();
        System.out.println("string.string:" + countResponse2.count());

        CountResponse countResponse4 = server.client().count(countRequest("test").querySource(fieldQuery("string.orig", val))).actionGet();
        System.out.println("string.orig:" + countResponse4.count());
        
        CountResponse countResponse3 = server.client().count(countRequest("test").querySource(fieldQuery("string.untouched", val))).actionGet();
        System.out.println("string.untouched:" + countResponse3.count());
            

        
    }
&lt;/pre&gt;

- Date test

&lt;pre&gt;
    @Test
    public void typeDateMultiFieldTest() throws IOException
    {
    
        String mapping = copyToStringFromClasspath("/integration-test-multifield-date-mapping.json");
        System.out.println("Mapping:");
        System.out.println("========");
        System.out.println(mapping);
        String val =  "20100406";
        
        server.client().admin().indices().putMapping(putMappingRequest("test").mappingSource(mapping)).actionGet();

        server.client().index(indexRequest("test").type("datetest").id("1")
                .source(jsonBuilder().startObject().field("date", val).endObject())).actionGet();
        
        server.client().admin().indices().refresh(refreshRequest()).actionGet();
       
        CountResponse countResponse4 = server.client().count(countRequest("test").querySource(fieldQuery("date", val))).actionGet();
        System.out.println("date:" + countResponse4.count());       
        
        CountResponse countResponse1 = server.client().count(countRequest("test").querySource(fieldQuery("date.date", val))).actionGet();
        System.out.println("date.date:" + countResponse1.count());
            
        CountResponse countResponse2 = server.client().count(countRequest("test").querySource(fieldQuery("date.orig", val))).actionGet();
        System.out.println("date.orig:" + countResponse2.count());

        CountResponse countResponse3 = server.client().count(countRequest("test").querySource(fieldQuery("date.untouched", val))).actionGet();
        System.out.println("date.untouched:" + countResponse3.count());
            

    }
    
&lt;/pre&gt;


String prints:

&lt;pre&gt;
    string:1
    string.string:0
    string.orig:1
    string.untouched:1
&lt;/pre&gt;


Date prints:

&lt;pre&gt;
    date:0
    date.date:0
    date.orig:1
    date.untouched:1
&lt;/pre&gt;


Showing that a fieldQuery on default "date" does not return, but does return for "date.orig".  
This is not the case for the string type, where field query on "string" does return, as does "string.orig"

This is with the latest 0.6.0-SNAPSHOT (pulling from maven), at the time of this message.

/dom
(Apologies if the formatting of this issue codes hey wire - I'll try to fix if so)

p.s. The entire test class is as follows:

&lt;pre&gt;
package org.elasticsearch.plugin.attachments.test;

import static org.elasticsearch.client.Requests.clusterHealth;
import static org.elasticsearch.client.Requests.countRequest;
import static org.elasticsearch.client.Requests.createIndexRequest;
import static org.elasticsearch.client.Requests.deleteIndexRequest;
import static org.elasticsearch.client.Requests.indexRequest;
import static org.elasticsearch.client.Requests.putMappingRequest;
import static org.elasticsearch.client.Requests.refreshRequest;
import static org.elasticsearch.index.query.json.JsonQueryBuilders.fieldQuery;
import static org.elasticsearch.server.ServerBuilder.serverBuilder;
import static org.elasticsearch.util.io.Streams.copyToBytesFromClasspath;
import static org.elasticsearch.util.io.Streams.copyToStringFromClasspath;
import static org.elasticsearch.util.json.JsonBuilder.jsonBuilder;
import static org.elasticsearch.util.settings.ImmutableSettings.settingsBuilder;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;

import java.io.IOException;

import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
import org.elasticsearch.action.count.CountResponse;
import org.elasticsearch.server.Server;
import org.elasticsearch.util.logging.Loggers;
import org.junit.After;
import org.junit.AfterClass;
import org.junit.Before;
import org.junit.BeforeClass;
import org.junit.Test;
import org.slf4j.Logger;

public class JsonFieldMapperTest {

    private final Logger logger = Loggers.getLogger(getClass());

    private static Server server;
    
    @BeforeClass
    public static void setupServer() {
        server = serverBuilder().settings(
                settingsBuilder().put("node.local", true)).server();
    }

    @AfterClass
    public static void closeServer() {
        server.close();
    }

    @Before
    public void createIndex() {
        logger.info("creating index [test]");
        server.client().admin().indices().create(
                createIndexRequest("test").settings(
                        settingsBuilder().put("index.numberOfReplicas", 0).put(
                                "index.numberOfShards", 1))).actionGet();
        logger.info("Running Cluster Health");
        ClusterHealthResponse clusterHealth = server.client().admin().cluster()
                .health(clusterHealth().waitForGreenStatus()).actionGet();
        logger.info("Done Cluster Health, status " + clusterHealth.status());
        assertThat(clusterHealth.timedOut(), equalTo(false));
        assertThat(clusterHealth.status(), equalTo(ClusterHealthStatus.GREEN));
    }

    @After
    public void deleteIndex() {
        logger.info("deleting index [test]");
        server.client().admin().indices().delete(deleteIndexRequest("test"))
                .actionGet();
    }

@Test
    public void typeDateMultiFieldTest() throws IOException
    {
    
        String mapping = copyToStringFromClasspath("/integration-test-multifield-date-mapping.json");
        System.out.println("Mapping:");
        System.out.println("========");
        System.out.println(mapping);
        String val =  "20100406";
        
        server.client().admin().indices().putMapping(putMappingRequest("test").mappingSource(mapping)).actionGet();

        server.client().index(indexRequest("test").type("datetest").id("1")
                .source(jsonBuilder().startObject().field("date", val).endObject())).actionGet();
        
        server.client().admin().indices().refresh(refreshRequest()).actionGet();
       
        CountResponse countResponse4 = server.client().count(countRequest("test").querySource(fieldQuery("date", val))).actionGet();
        System.out.println("date:" + countResponse4.count());       
        
        CountResponse countResponse1 = server.client().count(countRequest("test").querySource(fieldQuery("date.date", val))).actionGet();
        System.out.println("date.date:" + countResponse1.count());
            
        CountResponse countResponse2 = server.client().count(countRequest("test").querySource(fieldQuery("date.orig", val))).actionGet();
        System.out.println("date.orig:" + countResponse2.count());

        CountResponse countResponse3 = server.client().count(countRequest("test").querySource(fieldQuery("date.untouched", val))).actionGet();
        System.out.println("date.untouched:" + countResponse3.count());
            

    }
    
    @Test
    public void typeStringMultiFieldTest() throws IOException
    {
    
        String mapping = copyToStringFromClasspath("/integration-test-multifield-string-mapping.json");
        System.out.println("Mapping:");
        System.out.println("========");
        System.out.println(mapping);
        String val = "20100406";
        
        server.client().admin().indices().putMapping(putMappingRequest("test").mappingSource(mapping)).actionGet();

        server.client().index(indexRequest("test").type("stringtest").id("1")
                .source(jsonBuilder().startObject().field("string", val).endObject())).actionGet();
        
        server.client().admin().indices().refresh(refreshRequest()).actionGet();
       
        CountResponse countResponse1 = server.client().count(countRequest("test").querySource(fieldQuery("string", val))).actionGet();
        System.out.println("string:" + countResponse1.count());
            
        CountResponse countResponse2 = server.client().count(countRequest("test").querySource(fieldQuery("string.string", val))).actionGet();
        System.out.println("string.string:" + countResponse2.count());

        CountResponse countResponse4 = server.client().count(countRequest("test").querySource(fieldQuery("string.orig", val))).actionGet();
        System.out.println("string.orig:" + countResponse4.count());
        
        CountResponse countResponse3 = server.client().count(countRequest("test").querySource(fieldQuery("string.untouched", val))).actionGet();
        System.out.println("string.untouched:" + countResponse3.count());
            

        
    }
}
&lt;/pre&gt;
</description><key id="166657">123</key><summary>multi_field not returning for default field for certain type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tootedom</reporter><labels><label>bug</label><label>v0.07.0</label></labels><created>2010-04-07T11:20:40Z</created><updated>2010-04-07T14:11:17Z</updated><resolved>2010-04-07T14:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-07T14:11:17Z" id="180295">multi_field not returning for default field for certain type, closed by 5e4f7c77a2e2f7966e18222f6ee20f40bb421e02
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast example incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/122</link><project id="" key="" /><description>Hiya

The unicast example on http://www.elasticsearch.com/docs/elasticsearch/modules/discovery/jgroups/ should use port 7800, not 9700

clint
</description><key id="166131">122</key><summary>Unicast example incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-06T17:27:31Z</created><updated>2010-05-12T17:20:22Z</updated><resolved>2010-05-12T17:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-06T17:51:03Z" id="179415">The `bind_port` in the example is set to `9700`, so this should work... . It does not work for you?
</comment><comment author="kimchy" created="2010-04-07T10:38:38Z" id="180136">Just ran it as is on my machine, and the `bind_port` is taken into account properly (the sample works...). Maybe he was running an old version? The default `bind_port` in jgroups is `7800`, but the example also sets that.
</comment><comment author="kimchy" created="2010-05-12T17:20:21Z" id="239274">closing this, I think it was a misconfiguration. In any case, the zen discovery is much simpler ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"Failed to execute"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/121</link><project id="" key="" /><description>Hiya

Another example of errors I'm seeing regularly:

```
 [13:24:52,109][DEBUG][action.count             ] [Dragonwing] [ia_object_1270224642][2], Node[null], [B], S[UNASSIGNED]: Failed to execute [[[ia_object_1270224642]][[notice]], querySource[{
   "filtered" : {
      "filter" : {
         "term" : {
            "status" : "active"
         }
      },
      "query" : {
         "match_all" : {}
      }
   }
}
]]
org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException: [ia_object_1270224642][2] Not active
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:158)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:78)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:55)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:54)
    at org.elasticsearch.client.server.ServerClient.count(ServerClient.java:139)
    at org.elasticsearch.rest.action.count.RestCountAction.handleRequest(RestCountAction.java:85)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:82)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:104)
    at org.elasticsearch.http.HttpServer.access$000(HttpServer.java:37)
    at org.elasticsearch.http.HttpServer$1.dispatchRequest(HttpServer.java:67)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:234)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:40)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:101)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:391)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:506)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:490)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:427)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.elasticsearch.http.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:49)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:345)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:332)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:323)
    at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:275)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:196)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:24:52,110][DEBUG][action.count             ] [Dragonwing] [ia_object_1270224642][0], Node[null], [B], S[UNASSIGNED]: Failed to execute [[[ia_object_1270224642]][[notice]], querySource[{
   "filtered" : {
      "filter" : {
         "term" : {
            "status" : "active"
         }
      },
      "query" : {
         "match_all" : {}
      }
   }
}
]]
org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException: [ia_object_1270224642][0] Not active
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:158)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:78)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:55)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:54)
    at org.elasticsearch.client.server.ServerClient.count(ServerClient.java:139)
    at org.elasticsearch.rest.action.count.RestCountAction.handleRequest(RestCountAction.java:85)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:82)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:104)
    at org.elasticsearch.http.HttpServer.access$000(HttpServer.java:37)
    at org.elasticsearch.http.HttpServer$1.dispatchRequest(HttpServer.java:67)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:234)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:40)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:101)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:391)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:506)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:490)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:427)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.elasticsearch.http.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:49)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:345)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:332)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:323)
    at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:275)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:196)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:24:52,184][DEBUG][action.count             ] [Dragonwing] [ia_object_1270224642][3], Node[null], [B], S[UNASSIGNED]: Failed to execute [[[ia_object_1270224642]][[source]], querySource[{
   "filtered" : {
      "filter" : {
         "term" : {
            "status" : "active"
         }
      },
      "query" : {
         "match_all" : {}
      }
   }
}
]]
org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException: [ia_object_1270224642][3] Not active
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:158)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:78)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:55)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:54)
    at org.elasticsearch.client.server.ServerClient.count(ServerClient.java:139)
    at org.elasticsearch.rest.action.count.RestCountAction.handleRequest(RestCountAction.java:85)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:82)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:104)
    at org.elasticsearch.http.HttpServer.access$000(HttpServer.java:37)
    at org.elasticsearch.http.HttpServer$1.dispatchRequest(HttpServer.java:67)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:234)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:40)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:101)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:391)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:506)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:490)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:427)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.elasticsearch.http.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:49)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:345)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:332)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:323)
    at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:275)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:196)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:24:52,185][DEBUG][action.count             ] [Dragonwing] [ia_object_1270224642][1], Node[null], [B], S[UNASSIGNED]: Failed to execute [[[ia_object_1270224642]][[source]], querySource[{
   "filtered" : {
      "filter" : {
         "term" : {
            "status" : "active"
         }
      },
      "query" : {
         "match_all" : {}
      }
   }
}
]]
org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException: [ia_object_1270224642][1] Not active
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:158)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:78)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:55)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:54)
    at org.elasticsearch.client.server.ServerClient.count(ServerClient.java:139)
    at org.elasticsearch.rest.action.count.RestCountAction.handleRequest(RestCountAction.java:85)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:82)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:104)
    at org.elasticsearch.http.HttpServer.access$000(HttpServer.java:37)
    at org.elasticsearch.http.HttpServer$1.dispatchRequest(HttpServer.java:67)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:234)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:40)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:101)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:391)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:506)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:490)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:427)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
    at org.elasticsearch.http.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:49)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:345)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:332)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:323)
    at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:275)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:196)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
```
</description><key id="165947">121</key><summary>"Failed to execute"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-06T13:27:24Z</created><updated>2010-07-17T11:26:58Z</updated><resolved>2010-07-17T11:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-06T13:41:45Z" id="179147">Curiously, although I get these errors in the server log, it doesn't seem to result in errors in my application
</comment><comment author="clintongormley" created="2010-07-17T11:26:57Z" id="316071">Closed - jboss no longer in use
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Casting errors under load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/120</link><project id="" key="" /><description>Hiya

I am unable to replicate this, but I've seen this issue a lot, especially when the servers are under load.

I was in the process of indexing a lot of new docs. Sometimes the CPU would be running at 100% plus.

```
{
  request   =&gt; {
       cmd =&gt; "/ia_object/notice/_search?search_type=query_then_fetch",
       data =&gt; {
       fields =&gt; [],
       from   =&gt; 1332200,
       query  =&gt; {
           filtered =&gt; {
             filter =&gt; {
               bool =&gt; {
                 must =&gt; [
                   { term =&gt; { status =&gt; "active" } },
                   { term =&gt; { location_id =&gt; 23 } },
                 ],
                 },
               },
             query  =&gt; { match_all =&gt; {} },
           },
           },
       size   =&gt; 100,
       "sort" =&gt; [{ publish_date =&gt; { "reverse" =&gt; 1 } }],
       },
       method =&gt; "GET",
     },
  response  =&gt; {
       error =&gt; "ReduceSearchPhaseException[Failed to execute [query] [reduce] ]; nested: ClassCastException[java.lang.Float cannot be cast to java.lang.Long]; ",
     },
  server  =&gt; "http://192.168.10.41:9200",
  status_code =&gt; 500,
  status_msg  =&gt; "Internal Server Error",
}
```

Another:
    {
      request   =&gt; {
           cmd =&gt; "/ia_object/notice/_search?search_type=dfs_query_then_fetch",
           data =&gt; {
           fields =&gt; [],
           from   =&gt; 0,
           query  =&gt; {
               filtered =&gt; {
                 filter =&gt; {
                   bool =&gt; {
                     must =&gt; [
                       { term =&gt; { sub_type =&gt; "obit" } },
                       { term =&gt; { status =&gt; "active" } },
                       { term =&gt; { location_id =&gt; 28 } },
                     ],
                     },
                   },
                 query  =&gt; {
                   dis_max =&gt; {
                     queries =&gt; [
                       {
                       query_string =&gt; { boost =&gt; 1, fields =&gt; ["name"], query =&gt; "\"Sammy Brian Happer\"~8" },
                       },
                       {
                       filtered =&gt; {
                         filter =&gt; { term =&gt; { has_name =&gt; 0 } },
                         query  =&gt; {
                           query_string =&gt; { boost =&gt; "1.3", fields =&gt; ["text"], query =&gt; "\"Sammy Brian Happer\"~8" },
                           },
                       },
                       },
                       { query_string =&gt; { boost =&gt; 1, query =&gt; "Sammy Brian Happer" } },
                     ],
                     tie_breaker =&gt; "0.7",
                     },
                   },
               },
               },
           size   =&gt; 100,
           "sort" =&gt; ["score"],
           },
           method =&gt; "GET",
         },
      response  =&gt; {
           error =&gt; "ReduceSearchPhaseException[Failed to execute [query] [reduce] ]; nested: ClassCastException[java.lang.Float cannot be cast to java.lang.Long]; ",
         },
      server  =&gt; "http://192.168.10.41:9200",
      status_code =&gt; 500,
      status_msg  =&gt; "Internal Server Error",
    }
</description><key id="165928">120</key><summary>Casting errors under load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-06T12:53:01Z</created><updated>2011-03-20T13:49:40Z</updated><resolved>2011-03-20T13:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-06T13:05:33Z" id="179106">I enabled debugging, and put the server under load, and am seeing logs like this:

```
[12:57:56,059][DEBUG][action.search.type       ] [Dragonwing] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [ia_object_1270224642][0]: query[filtered((name:"ray woolston"~7 | filtered(text:"ray woolston"~7^1.3)-&gt;FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3e3f4f7) | (_all:ray _all:woolston))~0.7)-&gt;BooleanFilter( +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@8d6b4fd5) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@5d5d3c26) +publish_date:[1267833600000 TO *])],from[0],size[300],sort[&lt;score&gt;]: Query Failed []
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:98)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:156)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:128)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:113)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term ray not available
    at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
    at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.&lt;init&gt;(PhraseQuery.java:122)
    at org.apache.lucene.search.PhraseQuery.createWeight(PhraseQuery.java:264)
    at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.&lt;init&gt;(DisjunctionMaxQuery.java:107)
    at org.apache.lucene.search.DisjunctionMaxQuery.createWeight(DisjunctionMaxQuery.java:184)
    at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
    at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
    at org.apache.lucene.search.Query.weight(Query.java:101)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:64)
    at org.apache.lucene.search.Searcher.search(Searcher.java:49)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:92)
    ... 8 more
[12:57:56,060][DEBUG][action.search.type       ] [Dragonwing] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [ia_object_1270224642][4]: query[filtered((name:"ray woolston"~7 | filtered(text:"ray woolston"~7^1.3)-&gt;FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3e3f4f7) | (_all:ray _all:woolston))~0.7)-&gt;BooleanFilter( +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@8d6b4fd5) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@5d5d3c26) +publish_date:[1267833600000 TO *])],from[0],size[300],sort[&lt;score&gt;]: Query Failed []
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:98)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:156)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:128)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:113)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term ray not available
    at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
    at org.apache.lucene.search.Similarity.idfExplain(Similarity.java:769)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.&lt;init&gt;(PhraseQuery.java:122)
    at org.apache.lucene.search.PhraseQuery.createWeight(PhraseQuery.java:264)
    at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.&lt;init&gt;(DisjunctionMaxQuery.java:107)
    at org.apache.lucene.search.DisjunctionMaxQuery.createWeight(DisjunctionMaxQuery.java:184)
    at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
    at org.apache.lucene.search.FilteredQuery.createWeight(FilteredQuery.java:63)
    at org.apache.lucene.search.Query.weight(Query.java:101)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createWeight(ContextIndexSearcher.java:64)
    at org.apache.lucene.search.Searcher.search(Searcher.java:49)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:92)
    ... 8 more
[13:03:46,546][DEBUG][action.search.type       ] [Dragonwing] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [8144], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:212)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:150)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:128)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:113)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:03:46,546][DEBUG][action.search.type       ] [Dragonwing] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [8142], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:212)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:200)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:278)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:224)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:197)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:03:46,547][DEBUG][action.search.type       ] [Dragonwing] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [8145], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:212)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:200)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:278)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:224)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:197)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:03:46,547][DEBUG][action.search.type       ] [Dragonwing] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [8145], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:212)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:150)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:128)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:113)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[13:03:46,548][DEBUG][action.search.type       ] [Dragonwing] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [8144], timed out
    at org.elasticsearch.search.SearchService.findContext(SearchService.java:212)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:200)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:278)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:224)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:197)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
```
</comment><comment author="clintongormley" created="2010-04-19T12:37:16Z" id="213504">Just had another casting error:

```
ReduceSearchPhaseException[Failed to execute [query] [reduce] ]; nested: ClassCastException[java.lang.Float cannot be cast to java.lang.Long]; 
```

on this query:

```
curl -XGET 'http://192.168.10.42:9200/ia_object/notice/_search?search_type=dfs_query_then_fetch'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "from" : 0,
   "query" : {
      "filtered" : {
         "filter" : {
            "bool" : {
               "must" : [
                  {
                     "term" : {
                        "notice_type" : "all_memorial"
                     }
                  },
                  {
                     "term" : {
                        "status" : "active"
                     }
                  },
                  {
                     "term" : {
                        "location_id" : "30"
                     }
                  }
               ]
            }
         },
         "query" : {
            "dis_max" : {
               "queries" : [
                  {
                     "query_string" : {
                        "fields" : [
                           "name"
                        ],
                        "boost" : 1,
                        "query" : "\"Carmela Hitchings\"~7"
                     }
                  },
                  {
                     "filtered" : {
                        "filter" : {
                           "term" : {
                              "has_name" : "0"
                           }
                        },
                        "query" : {
                           "query_string" : {
                              "fields" : [
                                 "text"
                              ],
                              "boost" : "1.3",
                              "query" : "\"Carmela Hitchings\"~7"
                           }
                        }
                     }
                  },
                  {
                     "query_string" : {
                        "boost" : 1,
                        "query" : "Carmela Hitchings"
                     }
                  }
               ],
               "tie_breaker" : "0.7"
            }
         }
      }
   },
   "size" : "300"
}
'
```

with this in the logs of one node:

```
[11:54:07,863][DEBUG][action.search.type       ] [Longshot] [ia_object_1271065315][4], Node[idb2-11081], [P], S[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@54ed10ea]
org.elasticsearch.transport.RemoteTransportException: None remote transport exception
Caused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException: Failed to handler response
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:104)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:76)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:796)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:391)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:317)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:299)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:216)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:345)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:332)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:323)
        at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:275)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:196)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.NoSuchElementException: No active shard found
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable$IndexShardsIterator.nextActive(IndexShardRoutingTable.java:159)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:194)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$100(TransportSearchTypeAction.java:78)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:178)
        at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:92)
        at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:85)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:102)
        ... 21 more
[12:21:20,202][DEBUG][action.search.type       ] [Longshot] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [ia_object_1271065315][2]: query[filtered((name:"stephanie gregory"~7 | filtered(text:"stephanie gregory"~7^1.3)-&gt;FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3e3f4f7) | (_all:stephanie _all:gregory))~0.7)-&gt;BooleanFilter( +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@fdb5bf46) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@3a1bb115) +FilterCacheFilterWrapper(org.elasticsearch.util.lucene.search.TermFilter@8d6b4fd7))],from[0],size[300],sort[&lt;score&gt;]: Query Failed []
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:98)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:197)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:139)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:140)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$200(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$1.run(TransportSearchDfsQueryThenFetchAction.java:113)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: df for term stephanie not available
        at org.elasticsearch.search.dfs.CachedDfSource.docFreq(CachedDfSource.java:51)
...skipping...
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:289)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:227)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:198)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[12:23:42,706][DEBUG][action.search.type       ] [Longshot] Failed to execute fetch phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [348292], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:278)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:261)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:289)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchDfsQueryThenFetchAction.java:227)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.access$700(TransportSearchDfsQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$4.run(TransportSearchDfsQueryThenFetchAction.java:198)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[12:23:42,756][DEBUG][action.search.type       ] [Longshot] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Gemini][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [354851], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:278)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:261)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:424)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:415)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:155)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[12:27:53,074][DEBUG][action.search.type       ] [Longshot] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Gemini][inet[/192.168.10.41:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [355847], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:278)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:261)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:424)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:415)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:155)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
[12:27:53,148][DEBUG][action.search.type       ] [Longshot] Failed to execute fetch phase
org.elasticsearch.transport.RemoteTransportException: [Magnum, Moses][inet[/192.168.10.33:9300]][search/phase/fetch/id]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [342239], timed out
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:277)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:260)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:424)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchFetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:415)
        at org.elasticsearch.transport.netty.MessageChannelHandler$3.run(MessageChannelHandler.java:155)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
```
</comment><comment author="clintongormley" created="2010-04-19T12:41:47Z" id="213509">btw, these nodes were built from master on 2010-04-08, so not sure if you've made more changes that should have fixed this already
</comment><comment author="clintongormley" created="2011-03-20T13:49:40Z" id="895176">Seems to work in recent releases
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range filter gt is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/119</link><project id="" key="" /><description>Hiya

The `gt` term to the range filter is broken. `from` with `include_lower: false` works correctly, and the `gt` term works correctly for range queries:

```
curl -XDELETE 'http://127.0.0.2:9200/es_test_1/' 
curl -XDELETE 'http://127.0.0.2:9200/es_test_2/' 
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping'  -d '
{
   "_all" : {
      "store" : "yes",
      "term_vector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;wait_for_status=green' 
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1/_create'  -d '
{
   "num" : "2",
   "date" : "2010-04-2 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2/_create'  -d '
{
   "num" : "3",
   "date" : "2010-04-3 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3/_create'  -d '
{
   "num" : "4",
   "date" : "2010-04-4 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4/_create'  -d '
{
   "num" : "5",
   "date" : "2010-04-5 00:00:00",
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5/_create'  -d '
{
   "num" : "6",
   "date" : "2010-04-6 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6/_create'  -d '
{
   "num" : "7",
   "date" : "2010-04-7 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7/_create'  -d '
{
   "num" : "8",
   "date" : "2010-04-8 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8/_create'  -d '
{
   "num" : "9",
   "date" : "2010-04-9 00:00:00",
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9/_create'  -d '
{
   "num" : "10",
   "date" : "2010-04-10 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10/_create'  -d '
{
   "num" : "11",
   "date" : "2010-04-11 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11/_create'  -d '
{
   "num" : "12",
   "date" : "2010-04-12 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12/_create'  -d '
{
   "num" : "13",
   "date" : "2010-04-13 00:00:00",
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13/_create'  -d '
{
   "num" : "14",
   "date" : "2010-04-14 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14/_create'  -d '
{
   "num" : "15",
   "date" : "2010-04-15 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15/_create'  -d '
{
   "num" : "16",
   "date" : "2010-04-16 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16/_create'  -d '
{
   "num" : "17",
   "date" : "2010-04-17 00:00:00",
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17/_create'  -d '
{
   "num" : "18",
   "date" : "2010-04-18 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18/_create'  -d '
{
   "num" : "19",
   "date" : "2010-04-19 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19/_create'  -d '
{
   "num" : "20",
   "date" : "2010-04-20 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20/_create'  -d '
{
   "num" : "21",
   "date" : "2010-04-21 00:00:00",
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21/_create'  -d '
{
   "num" : "22",
   "date" : "2010-04-22 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22/_create'  -d '
{
   "num" : "23",
   "date" : "2010-04-23 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23/_create'  -d '
{
   "num" : "24",
   "date" : "2010-04-24 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24/_create'  -d '
{
   "num" : "25",
   "date" : "2010-04-25 00:00:00",
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25/_create'  -d '
{
   "num" : "26",
   "date" : "2010-04-26 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26/_create'  -d '
{
   "num" : "27",
   "date" : "2010-04-27 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27/_create'  -d '
{
   "num" : "28",
   "date" : "2010-04-28 00:00:00",
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28/_create'  -d '
{
   "num" : "29",
   "date" : "2010-04-29 00:00:00",
   "text" : "foo baz"
}
'
curl -XPOST 'http://127.0.0.2:9200/_refresh' 

echo "

RUNNING QUERIES:

RANGE FILTER GTE:
"

curl -XGET 'http://127.0.0.2:9200/_search?search_type=query_then_fetch&amp;pretty=true'  -d '
{
   "fields" : [],
   "size": 1,
   "query" : {
      "filtered" : {
         "filter" : {
            "range" : {
               "num" : {
                  "gte" : 10
               }
            }
         },
         "query" : {
            "match_all" : {}
         }
      }
   }
}
'

echo "

RANGE FILTER GT

"
curl -XGET 'http://127.0.0.2:9200/_search?search_type=query_then_fetch&amp;pretty=true'  -d '
{
   "fields" : [],
   "size": 1,
   "query" : {
      "filtered" : {
         "filter" : {
            "range" : {
               "num" : {
                  "gt" : 10
               }
            }
         },
         "query" : {
            "match_all" : {}
         }
      }
   }
}
'
echo "

RANGE FILTER FROM/INCLUDE_LOWER

"
curl -XGET 'http://127.0.0.2:9200/_search?search_type=query_then_fetch&amp;pretty=true'  -d '
{
   "fields" : [],
   "size": 1,
   "query" : {
      "filtered" : {
         "filter" : {
            "range" : {
               "num" : {
                  "from" : 10,
                  "include_lower": false
               }
            }
         },
         "query" : {
            "match_all" : {}
         }
      }
   }
}
'

echo "

RANGE QUERY FROM/INCLUDE_LOWER

"
curl -XGET 'http://127.0.0.2:9200/_search?search_type=query_then_fetch&amp;pretty=true'  -d '
{
   "fields" : [],
   "size": 1,
   "query" : {
        "range" : {
           "num" : {
              "gt" : 10
           }
        }
   }
}
'
```
</description><key id="165867">119</key><summary>Range filter gt is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.07.0</label></labels><created>2010-04-06T11:27:18Z</created><updated>2014-04-07T16:58:23Z</updated><resolved>2010-04-06T12:54:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-06T12:54:10Z" id="179096">Range filter gt is broken, closed by 1a9c5d6b15e1f2e6674fc6196778189beb8d4515.
</comment><comment author="clintongormley" created="2010-04-09T12:07:43Z" id="182511">Fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>httpAddress -&gt; http_address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/118</link><project id="" key="" /><description>httpAddress hasn't been renamed to http_address yet

```
curl -XGET 'http://192.168.10.41:9200/_cluster/nodes' 
# {
#    "nodes" : {
#       "idb2-38121" : {
#          "transport_address" : "inet[idb2/192.168.10.41:9300]",
#          "httpAddress" : "inet[/192.168.10.41:9200]",
#          "name" : "Torso",
#          "data_node" : true
#       }
#    },
#    "cluster_name" : "elasticsearch"
# }
```
</description><key id="165835">118</key><summary>httpAddress -&gt; http_address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-06T10:15:52Z</created><updated>2010-04-06T10:33:43Z</updated><resolved>2010-04-06T10:33:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-06T10:33:43Z" id="178978">httpAddress -&gt; http_address, closed by cc6e6eb3812247f2ee1329ba1fafcc13e92fb72b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type and property equals, will cause exception: ReplicationShardOperationFailedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/117</link><project id="" key="" /><description>curl -XPUT "http://localhost:9200/cms/blog/1" -d '
{
  "blog":55
}
'

 "error" : "ReplicationShardOperationFailedException[[cms][0] ]; nested: MapperException[Malformed json, after type is must start with an object]; "
</description><key id="165689">117</key><summary>Type and property equals, will cause exception: ReplicationShardOperationFailedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hz</reporter><labels /><created>2010-04-06T03:59:50Z</created><updated>2010-05-28T08:42:46Z</updated><resolved>2010-05-28T08:42:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-04-06T11:30:35Z" id="179015">You've run into the "gotcha"  (#112) where a field name is the same as your type name:
http://www.elasticsearch.com/docs/elasticsearch/rest_api/index/#Automatic_for_the_Index

try this instead:
    curl -XPUT "http://localhost:9200/cms/blog/1" -d '
    { "blog": { "blog":55 } }'
</comment><comment author="kimchy" created="2010-04-07T11:22:23Z" id="180166">Yea, this is how elasticsearch works... . I have fixed the message to be a bit more descriptive.
</comment><comment author="kimchy" created="2010-05-28T08:42:45Z" id="256470">by design, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move from CamelCase to '_' casing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/116</link><project id="" key="" /><description>Seems like most nosql and json based rest interfaces use '_' instead of CamelCase. Changes include:
1. All HTTP parameters move from CamelCase to '_' casing.
2. All JSON requests and responses move from CamelCase to '_' casing.
3. All settings now support both CamelCase as well as '_'.
</description><key id="164749">116</key><summary>Move from CamelCase to '_' casing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T14:17:33Z</created><updated>2014-07-07T13:47:59Z</updated><resolved>2010-04-04T14:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T14:18:34Z" id="177421">Move from CamelCase to '_' casing, closed by 3b5b4b4c3ab0c5fb4edf16832d4aaf09169bba5b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: search HTTP params vs JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/115</link><project id="" key="" /><description>Is there any reason in particular that you couldn't change all query string parameters to be allowable JSON params as well (or instead?)

So this:
    curl -XGET 'http://127.0.0.2:9200/ia_object/_search?searchType=query_then_fetch'  -d '
    {
       "fields" : [],
       "query" : {
          "matchAll" : {}
       }
    }
    '

would become this:

```
curl -XGET 'http://127.0.0.2:9200/ia_object/_search'  -d '
{
   "search_type" : "query_then_fetch",
   "fields" : [],
   "query" : {
      "matchAll" : {}
   }
}
'
```
</description><key id="164729">115</key><summary>API Change: search HTTP params vs JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-04T12:54:46Z</created><updated>2013-11-08T20:15:00Z</updated><resolved>2011-03-20T13:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T19:37:39Z" id="177542">Yes, since currently, I can use the HTTP params and then I don't need to parse the json body on the node that receives the requests, just on the nodes that execute the search. But, I agree, I need to find a nice way to support that (without parsing the json).
</comment><comment author="clintongormley" created="2011-03-20T13:48:56Z" id="895175">Won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Range - support lt/lte/gt/gte</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/114</link><project id="" key="" /><description /><key id="164726">114</key><summary>Query DSL: Range - support lt/lte/gt/gte</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T12:41:16Z</created><updated>2010-04-04T19:14:54Z</updated><resolved>2010-04-04T19:14:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T19:14:54Z" id="177526">Query DSL: Range - support lt/lte/gt/gte, closed by 5f7d0ce36e9bad1592e6b04f465c201a6a272c0a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: search sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/113</link><project id="" key="" /><description>Instead of specifying sort order like this:
    "sort" : [
      "name",
      {
         "last_modified" : {
            "reverse" : 1
         }
      }
    ],

What about like this?
    "sort" : [
      "name",
      {"last_modified" : "desc"}
    ],

or

```
"sort" : [
  {"name" : "asc"},
  {"last_modified" : "desc"}
],
```

When you sort on `score`, `reverse: true` implies `asc`, but for everything else, `reverse: true` implies `desc`. So why not make it unambiguous?
</description><key id="164724">113</key><summary>API Change: search sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T12:35:18Z</created><updated>2010-04-09T12:03:34Z</updated><resolved>2010-04-09T11:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-09T11:42:21Z" id="182488">Going to support it as well as the old way
</comment><comment author="kimchy" created="2010-04-09T11:42:54Z" id="182490">implemented
</comment><comment author="clintongormley" created="2010-04-09T12:03:34Z" id="182506">Tested++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: put_mapping and the optional type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/112</link><project id="" key="" /><description>I've brought this up before as ambiguous, and I've been bitten by it:

The optional ability to specify the type in the JSON, eg:
    $ curl -XPUT http://localhost:9200/twitter/tweet/_mapping -d \
    '
    {
        tweet : {
            properties : {
                message : {type : "string", store : "yes"}
            }
        }
    }
    '

... is redundant, and can cause problems.  You already specify the `type` in the URL, I'd drop it in the JSON
</description><key id="164721">112</key><summary>API Change: put_mapping and the optional type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-04T12:26:52Z</created><updated>2010-05-28T09:06:26Z</updated><resolved>2010-05-28T15:54:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-05-28T08:41:04Z" id="256468">One of the reasoning for this is that the mapping needs to be self sufficient in terms of describing what it maps. If I maintain that, then its easy to toss it somewhere as a single json, and then read it back.
</comment><comment author="clintongormley" created="2010-05-28T08:54:32Z" id="256483">ok.  btw the docs on http://www.elasticsearch.com/docs/elasticsearch/rest_api/admin/indices/put_mapping/ still say this, which is incorrect:

"The put mapping operation automatically creates an index if it has not been created before (check out the create index API for manually creating an index)."
</comment><comment author="kimchy" created="2010-05-28T09:06:26Z" id="256495">cool, fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: moreLikeThis and fuzzyLikeThis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/111</link><project id="" key="" /><description>What about providing aliases `mlt`, `mlt_field`, `flt` and `flt_field` for `more_like_this*` and `fuzzy_like_this*`
</description><key id="164720">111</key><summary>API Change: moreLikeThis and fuzzyLikeThis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T12:20:28Z</created><updated>2014-03-07T00:05:29Z</updated><resolved>2010-04-04T20:59:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T20:59:23Z" id="177598">API Change: moreLikeThis and fuzzyLikeThis, closed by 6480daff78dd928788ea25cf8ed1a7af58a3d637.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: put_mapping: change default ignore_conflicts to false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/110</link><project id="" key="" /><description>In keeping with my comment about having options default to `false`, and having to "enable" features with a `true` value, perhaps `ignoreConflicts` should be renamed to `warn_on_conflict`

Actually, thinking about it more, I think that `ignore_conflicts` is the right name, but that it should default to `false`.  Silently ignoring a problem seems like the wrong approach.  I'd rather have to take the decision to ignore it myself.
</description><key id="164717">110</key><summary>API Change: put_mapping: change default ignore_conflicts to false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-04T12:18:28Z</created><updated>2010-04-04T20:11:04Z</updated><resolved>2010-04-04T20:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T20:11:04Z" id="177571">API Change: put_mapping: change default ignore_conflicts to false, closed by 52f193c849df2d8d43b921d4a73eba6deebd1ee8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: moreLikeThis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/109</link><project id="" key="" /><description>Changes to mlt:

```
percentTermsToMatch  =&gt; pct_terms_to_match, pct_to_match?
minTermFrequency     =&gt; min_term_freq ( to match min_doc_freq)
maxQueryTerms        =&gt; max_terms?
```

Instead of having `boostTerms` and `boostTermsFactor`, why not
just have `boost_terms` which expects a `float`. So if specified
then boost the terms by that amount?
</description><key id="164716">109</key><summary>API Change: moreLikeThis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T12:10:12Z</created><updated>2010-04-04T20:36:58Z</updated><resolved>2010-04-04T20:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T20:36:23Z" id="177585">I will change the boost_terms (bot in mlt API, and in more_like_this and more_like_this_field queries).

Will change minTermFrequency to min_term_freq.

I will keep the percent and max_query_terms as is, I think its more descriptive.
</comment><comment author="kimchy" created="2010-04-04T20:36:58Z" id="177586">API Change: moreLikeThis, closed by 2d6c2d85861ed7dca25ace2bdd402acf8f409e88.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: Terms - Add support for gt/gte/lt/lte</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/108</link><project id="" key="" /><description>At the moment, you have `fromInclusive` and `toInclusive`, but defaulting to `true`.

It feels unintuitive to pass an option that is false ie if something isn't specified, then to me it is the same as false, but then you enable an action by passing a true.

So I'd change the options to `exclude_from` or `exclude_to`, which default to `false`

Alternatively, you could change the `from` and `to` options themselves, eg:

```
param              equivalent to:
-------------------------------------
from:              fromInclusive = true
to:                toInclusive   = true
greater_than:      fromInclusive = false
less_than:         toInclusive   = false
```

The Perl string comparison operators are `gt`, `lt`, `gte` (greater than or equals) and `lte`, but that may be a step too far :)
</description><key id="164712">108</key><summary>API Change: Terms - Add support for gt/gte/lt/lte</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T11:58:42Z</created><updated>2010-04-04T19:00:13Z</updated><resolved>2010-04-04T19:00:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T19:00:13Z" id="177516">API Change: Terms - Add support for gt/gte/lt/lte, closed by 33086fb98de56253797143794f9f51a3a7b33f96.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Change: opType=create </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/107</link><project id="" key="" /><description>What about changing this to:

```
curl -XPUT 'http://127.0.0.2:9200/foo/bar/123/_create'  -d '{...}'
```

or:

```
curl -XPUT 'http://127.0.0.2:9200/foo/bar/123?create=true'  -d '{...}'
```
</description><key id="164705">107</key><summary>API Change: opType=create </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T11:23:33Z</created><updated>2010-04-04T18:36:14Z</updated><resolved>2010-04-04T18:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T18:35:29Z" id="177500">Will ad support for the `_create` as well as `op_type`.
</comment><comment author="kimchy" created="2010-04-04T18:36:14Z" id="177501">API Change: opType=create, closed by d0a79223db6e9dc2b98d983e795d9031bc591acb
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Rename pathType to path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/106</link><project id="" key="" /><description /><key id="164680">106</key><summary>Mapper: Rename pathType to path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-04T08:59:44Z</created><updated>2010-04-04T09:02:43Z</updated><resolved>2010-04-04T09:02:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-04T09:02:43Z" id="177330">Mapper: Rename pathType to path, closed by 3f8acbd9948dfb448f5a529b7b4ddb1a74b776c1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Rename allFIeld to _all, sourceField to _source, idField to _id, and typeField to _type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/105</link><project id="" key="" /><description>I will maintain backward comp by being able to parse the Field suffix as well.
</description><key id="164507">105</key><summary>Mapper: Rename allFIeld to _all, sourceField to _source, idField to _id, and typeField to _type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-04-03T22:37:28Z</created><updated>2010-04-03T22:50:59Z</updated><resolved>2010-04-03T22:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-03T22:50:59Z" id="177115">Mapper: Rename allFIeld to _all, sourceField to _source, idField to _id, and typeField to _type, closed by 6bf19fcd930bcfdee25003a62f1c2d1098575eaf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTP Netty: Remove httpKeepAlive support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/104</link><project id="" key="" /><description>Until I figure out a nicer way to support this. Currently, it also closes ongoing requests.
</description><key id="163848">104</key><summary>HTTP Netty: Remove httpKeepAlive support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-04-02T15:42:26Z</created><updated>2010-04-02T15:45:23Z</updated><resolved>2010-04-02T15:45:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-02T15:45:23Z" id="176120">HTTP Netty: Remove httpKeepAlive support, closed by f2189e8176dc763e315ab573b3dd8e71db2fa7bf.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Fuzzy Like This</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/103</link><project id="" key="" /><description>Add another option to perform "like this" queries, this time using fuzzy queries. Here is the one that can execute on more than one field (defaults to the `_all` field):

```
{
    fuzzy_like_his : {
        fields : ["name.first", "name.last"],
        like_text : "something",
        max_query_terms : 12
    }
}
```

And here is one that can execute against a specific field:    

```
{
    fuzzy_like_this_field : {
        "name.first" : {
            like_text : "something",
            max_query_terms : 12
        }
    }
}
```
</description><key id="163340">103</key><summary>Query DSL: Fuzzy Like This</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-04-01T19:18:41Z</created><updated>2010-04-04T21:30:50Z</updated><resolved>2010-04-01T19:19:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-01T19:19:28Z" id="175507">Query DSL: Fuzzy Like This, closed by 118aa89614deeee79a75d5847ad8b61520e43182.
</comment><comment author="clintongormley" created="2010-04-02T17:03:58Z" id="176199">fuzzyLikeThis works, but fuzzyLikeThisField doesn't:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping?ignoreConflicts=true'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'

curl -XPOST 'http://127.0.0.2:9200/_refresh' 
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;waitForStatus=green' 


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1?opType=create'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'

curl -XGET 'http://127.0.0.2:9200/_all/_count?pretty=true'  -d '
{
   "fuzzyLikeThis" : {
      "fields":   ["text"],
      "likeText" : "fop"
   }
}
'

curl -XGET 'http://127.0.0.2:9200/_all/_count?pretty=true'  -d '
{
   "fuzzyLikeThisField" : {
      "text" : {
         "likeText" : "fop"
      }
   }
}
'
```

clint
</comment><comment author="kimchy" created="2010-04-02T18:55:50Z" id="176295">ok, fixed...
</comment><comment author="clintongormley" created="2010-04-03T10:22:34Z" id="176782">fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Starting multiple nodes at the same time can cause the cluster not to communicate properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/102</link><project id="" key="" /><description>Hiya

I've had a repeat occurrence of two issues (1) starting all 3 nodes at the same time means that the cluster doesn't communicate properly and (2) starting one node doesn't read the index in the gateway, but starting a second node does.

When I start a node, it puts a lot of load (loads of 30+) on the CPU.  Sometimes, if I start (eg) 3 nodes at the same time, they fail to communicate properly, possibly because the high CPU loads are causing timeouts?

That's what happened here - 2 nodes were talking, and the third wasn't.  I noticed at this stage that there were two metadata files in my gateway directory. 

I shut down all 3 nodes, then restarted the first node. It didn't read the existing index from the gateway.   There was nothing in the log about having found an index, and the work directory didn't grow in size (as it usually does while it builds the niofs caches).

I started the second node, and then it did find the index, and everything works correctly.

Below is the metadata file just before the second restart, plus the logs from all 3 servers:

METADATA:
    {
      "meta-data" : {
        "maxNumberOfShardsPerNode" : 100,
        "indices" : {
          "ia_object_1270054545" : {
            "settings" : {
              "index.analysis.analyzer.default.filter.1" : "lowercase",
              "index.analysis.analyzer.default.filter.2" : "stop",
              "index.numberOfReplicas" : "2",
              "index.analysis.analyzer.default.filter.0" : "standard",
              "index.analysis.analyzer.default.filter.3" : "asciifolding",
              "index.analysis.analyzer.default.tokenizer" : "standard",
              "index.numberOfShards" : "5",
              "index.aliases.0" : "ia_object"
            },
            "mappings" : {
              "site" : {
                "source" : "{\n  \"site\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "source" : {
                "source" : "{\n  \"source\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"name\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"url\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"url\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "scraper" : {
                "source" : "{\n  \"scraper\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"name\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "location" : {
                "source" : "{\n  \"location\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"name\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "image" : {
                "source" : "{\n  \"image\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"title\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"title\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "notice" : {
                "source" : "{\n  \"notice\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"has_name\" : {\n        \"type\" : \"boolean\",\n        \"indexName\" : \"has_name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"text\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"text\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"remote_last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"remote_last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"publish_date\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"publish_date\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"notice_type\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"notice_type\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"featured\" : {\n        \"type\" : \"boolean\",\n        \"indexName\" : \"featured\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"location_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"location_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"name\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.2,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"sub_type\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"sub_type\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "group" : {
                "source" : "{\n  \"group\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"name\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"text_id\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"text_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              },
              "user" : {
                "source" : "{\n  \"user\" : {\n    \"type\" : \"object\",\n    \"dynamic\" : true,\n    \"enabled\" : true,\n    \"pathType\" : \"full\",\n    \"dateFormats\" : [ \"dateOptionalTime\" ],\n    \"boostField\" : {\n      \"name\" : \"_boost\"\n    },\n    \"allField\" : {\n      \"enabled\" : true,\n      \"store\" : \"yes\",\n      \"termVector\" : \"with_positions_offsets\"\n    },\n    \"sourceField\" : {\n      \"name\" : \"_source\",\n      \"enabled\" : true\n    },\n    \"properties\" : {\n      \"id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"ancestor_ids\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"ancestor_ids\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      },\n      \"username\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"username\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"last_modified\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"last_modified\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"email\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"email\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"created\" : {\n        \"type\" : \"date\",\n        \"indexName\" : \"created\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"format\" : \"yyyy-MM-dd HH:mm:ss\"\n      },\n      \"status\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"status\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"name\" : {\n        \"type\" : \"string\",\n        \"indexName\" : \"name\",\n        \"index\" : \"analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : false,\n        \"omitTermFreqAndPositions\" : false\n      },\n      \"creator_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"creator_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4,\n        \"nullValue\" : 0\n      },\n      \"parent_id\" : {\n        \"type\" : \"long\",\n        \"indexName\" : \"parent_id\",\n        \"index\" : \"not_analyzed\",\n        \"store\" : \"no\",\n        \"termVector\" : \"no\",\n        \"boost\" : 1.0,\n        \"omitNorms\" : true,\n        \"omitTermFreqAndPositions\" : true,\n        \"precisionStep\" : 4\n      }\n    }\n  }\n}"
              }
            }
          }
        }
      }
    }

SERVER 1:
    [14:24:37,094][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Initializing ...
    [14:24:37,101][INFO ][plugins                  ] [Red Skull] Loaded []
    [14:24:42,197][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Initialized
    [14:24:42,197][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Starting ...
    [14:24:42,585][INFO ][transport                ] [Red Skull] boundAddress [inet[/10.224.103.159:9300]], publishAddress [inet[/10.224.103.159:9300]]
    [14:24:45,861][INFO ][cluster.service          ] [Red Skull] New Master [Red Skull][db1-13080][data][inet[/10.224.103.159:9300]]
    [14:24:45,976][INFO ][discovery                ] [Red Skull] iAnnounce/db1-13080
    [14:24:46,030][INFO ][http                     ] [Red Skull] boundAddress [inet[/10.224.103.159:9200]], publishAddress [inet[/10.224.103.159:9200]]
    [14:24:46,040][INFO ][cluster.metadata         ] [Red Skull] Creating Index [ia_object_1270054545], shards [5]/[2], mappings [site, source, scraper, location, image, notice, group, user]
    [14:24:46,664][INFO ][jmx                      ] [Red Skull] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://10.224.103.159:9400/jmxrmi]
    [14:24:46,664][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Started
    [14:25:06,637][WARN ][jgroups.pbcast.GMS       ] db1-13080: failed to collect all ACKs (expected=1) for view [db1-13080|1] [db1-13080, db2-14372] after 2000ms, missing ACKs from [db1-13080]
    [14:25:10,891][WARN ][jgroups.pbcast.GMS       ] db1-13080: failed to collect all ACKs (expected=1) for unicast view [db1-13080|1] [db1-13080, db2-14372] after 2000ms, missing ACKs from [db2-14372]
    [14:25:09,943][WARN ][jgroups.pbcast.NAKACK    ] db1-13080: dropped message from db3-20667 (not in xmit_table), keys are [db1-13080, db2-14372], view=[db1-13080|1] [db1-13080, db2-14372]
    [14:25:33,126][WARN ][jgroups.pbcast.NAKACK    ] db1-13080: dropped message from db3-20667 (not in xmit_table), keys are [db1-13080, db2-14372], view=[db1-13080|1] [db1-13080, db2-14372]
    [14:25:38,066][WARN ][jgroups.pbcast.NAKACK    ] db1-13080: dropped message from db3-20667 (not in xmit_table), keys are [db1-13080], view=[db1-13080|2] [db1-13080]
    [14:25:38,067][WARN ][jgroups.pbcast.GMS       ] db1-13080: failed to collect all ACKs (expected=1) for view [db1-13080|2] [db1-13080] after 2000ms, missing ACKs from [db1-13080]
    [14:25:58,253][WARN ][jgroups.pbcast.NAKACK    ] db1-13080: dropped message from db3-20667 (not in xmit_table), keys are [db1-13080], view=[db1-13080|2] [db1-13080]
    [14:25:58,254][WARN ][jgroups.pbcast.NAKACK    ] db1-13080: dropped message from db3-20667 (not in xmit_table), keys are [db1-13080], view=[db1-13080|2] [db1-13080]
    [14:26:07,173][WARN ][jgroups.pbcast.NAKACK    ] db1-13080: dropped message from db3-20667 (not in xmit_table), keys are [db1-13080], view=[db1-13080|2] [db1-13080]
    [14:26:11,311][INFO ][cluster.service          ] [Red Skull] Added {[][org.elasticsearch.util.transport.DummyTransportAddress@307b37df],}
    [14:26:11,539][WARN ][transport.netty          ] [Red Skull] Failed to connect to discovered node [[][org.elasticsearch.util.transport.DummyTransportAddress@307b37df]]
    java.lang.ClassCastException: org.elasticsearch.util.transport.DummyTransportAddress cannot be cast to org.elasticsearch.util.transport.InetSocketTransportAddress
            at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:437)
            at org.elasticsearch.transport.netty.NettyTransport.nodesAdded(NettyTransport.java:396)
            at org.elasticsearch.transport.TransportService.nodesAdded(TransportService.java:96)
            at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:185)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:619)
    [14:26:11,766][WARN ][jgroups.pbcast.GMS       ] db1-13080: failed to collect all ACKs (expected=1) for view MergeView::[db3-20667|3] [db3-20667, db1-13080], subgroups=[[db3-20667|2] [db3-20667], [db1-13080|2] [db1-13080]] after 2000ms, missing ACKs from [db1-13080]
    [14:26:39,751][INFO ][cluster.service          ] [Red Skull] Removed {[][org.elasticsearch.util.transport.DummyTransportAddress@307b37df],}
    [14:26:44,513][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Stopping ...
    [14:29:39,097][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Stopped
    [14:29:39,098][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Closing ...
    [14:29:39,161][INFO ][server                   ] [Red Skull] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Closed
    [14:33:44,049][INFO ][server                   ] [Blindspot] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Initializing ...
    [14:33:44,054][INFO ][plugins                  ] [Blindspot] Loaded []
    [14:33:46,857][INFO ][server                   ] [Blindspot] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Initialized
    [14:33:46,858][INFO ][server                   ] [Blindspot] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Starting ...
    [14:33:47,162][INFO ][transport                ] [Blindspot] boundAddress [inet[/10.224.103.159:9300]], publishAddress [inet[/10.224.103.159:9300]]
    [14:33:50,257][INFO ][cluster.service          ] [Blindspot] New Master [Blindspot][db1-32838][data][inet[/10.224.103.159:9300]]
    [14:33:50,321][INFO ][discovery                ] [Blindspot] iAnnounce/db1-32838
    [14:33:50,378][INFO ][http                     ] [Blindspot] boundAddress [inet[/10.224.103.159:9200]], publishAddress [inet[/10.224.103.159:9200]]
    [14:33:50,595][INFO ][jmx                      ] [Blindspot] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://10.224.103.159:9400/jmxrmi]
    [14:33:50,595][INFO ][server                   ] [Blindspot] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:10}: Started
    [14:35:00,834][INFO ][cluster.metadata         ] [Blindspot] Creating Index [ia_object_1270054545], shards [5]/[2], mappings [site, source, scraper, location, image, notice, group, user]
    [14:35:03,995][INFO ][cluster.service          ] [Blindspot] Added {[Chaka][db2-22084][data][inet[ip-10-224-103-48.eu-west-1.compute.internal/10.224.103.48:9300]],}
    [14:35:42,912][WARN ][jgroups.pbcast.GMS       ] db3-28837 already present; returning existing view [db1-32838|2] [db1-32838, db2-22084, db3-28837]
    [14:35:42,913][INFO ][cluster.service          ] [Blindspot] Added {[Death's Head I&amp;II][db3-28837][data][inet[ip-10-224-118-127.eu-west-1.compute.internal/10.224.118.127:9300]],}

SERVER 2:
    [14:24:52,318][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Initializing ...
    [14:24:52,323][INFO ][plugins                  ] [Ghost Dancer] Loaded []
    [14:24:55,746][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Initialized
    [14:24:55,746][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Starting ...
    [14:24:56,069][INFO ][transport                ] [Ghost Dancer] boundAddress [inet[/10.224.103.48:9300]], publishAddress [inet[/10.224.103.48:9300]]
    [14:25:01,035][WARN ][jgroups.pbcast.GMS       ] join(db2-14372) sent to db1-13080 timed out (after 3000 ms), retrying
    [14:25:01,071][INFO ][cluster.service          ] [Ghost Dancer] New Master [Ghost Dancer][db2-14372][data][inet[/10.224.103.48:9300]]
    [14:25:01,133][INFO ][discovery                ] [Ghost Dancer] iAnnounce/db2-14372
    [14:25:01,163][INFO ][http                     ] [Ghost Dancer] boundAddress [inet[/10.224.103.48:9200]], publishAddress [inet[/10.224.103.48:9200]]
    [14:25:01,532][INFO ][jmx                      ] [Ghost Dancer] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://10.224.103.48:9400/jmxrmi]
    [14:25:01,532][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Started
    [14:25:01,619][INFO ][cluster.metadata         ] [Ghost Dancer] Creating Index [ia_object_1270054545], shards [5]/[2], mappings [site, scraper, source, location, image, group, notice, user]
    [14:25:09,791][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:25:16,272][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::1 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:33,962][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::2 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:33,962][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::3 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:34,257][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::1 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:34,257][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::2 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:34,257][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::3 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:41,850][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::1 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:41,850][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::2 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:25:41,850][WARN ][jgroups.pbcast.NAKACK    ](requester=db1-13080, local_addr=db2-14372) message db2-14372::3 not found in retransmission table of db2-14372:
    [3 : 6 (6) (size=3, missing=0, highest stability=3)]
    [14:26:33,856][INFO ][cluster.service          ] [Ghost Dancer] Added {[War Machine][db3-20667][data][inet[ip-10-224-118-127.eu-west-1.compute.internal/10.224.118.127:9300]],}
    [14:26:42,309][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:27:04,703][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:27:04,703][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:27:14,552][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:27:15,125][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:27:33,301][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Stopping ...
    [14:28:04,529][WARN ][jgroups.pbcast.NAKACK    ] db2-14372: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:28:12,960][WARN ][jgroups.pbcast.GMS       ] db2-14372: failed to collect all ACKs (expected=1) for view [db3-20667|2] [db3-20667] after 2000ms, missing ACKs from [db3-20667]
    [14:28:20,577][ERROR][jgroups.UNICAST          ] db2-14372: sender window for db2-14372 not found
    [14:31:25,936][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Stopped
    [14:31:25,937][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Closing ...
    [14:31:25,955][INFO ][server                   ] [Ghost Dancer] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Closed
    [14:34:57,406][INFO ][server                   ] [Chaka] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Initializing ...
    [14:34:57,411][INFO ][plugins                  ] [Chaka] Loaded []
    [14:35:00,373][INFO ][server                   ] [Chaka] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Initialized
    [14:35:00,374][INFO ][server                   ] [Chaka] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Starting ...
    [14:35:00,703][INFO ][transport                ] [Chaka] boundAddress [inet[/10.224.103.48:9300]], publishAddress [inet[/10.224.103.48:9300]]
    [14:35:15,873][INFO ][cluster.service          ] [Chaka] Detected Master [Blindspot][db1-32838][data][inet[ip-10-224-103-159.eu-west-1.compute.internal/10.224.103.159:9300]], Added {[Blindspot][db1-32838][data][inet[ip-10-224-103-159.eu-west-1.compute.internal/10.224.103.159:9300]],}
    [14:35:16,774][INFO ][discovery                ] [Chaka] iAnnounce/db2-22084
    [14:35:16,801][INFO ][http                     ] [Chaka] boundAddress [inet[/10.224.103.48:9200]], publishAddress [inet[/10.224.103.48:9200]]
    [14:35:16,886][INFO ][jmx                      ] [Chaka] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://10.224.103.48:9400/jmxrmi]
    [14:35:16,887][INFO ][server                   ] [Chaka] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:13}: Started
    [14:35:44,296][INFO ][cluster.service          ] [Chaka] Added {[Death's Head I&amp;II][db3-28837][data][inet[ip-10-224-118-127.eu-west-1.compute.internal/10.224.118.127:9300]],}

SERVER 3:
    [14:24:55,498][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Initializing ...
    [14:24:55,515][INFO ][plugins                  ] [War Machine] Loaded []
    [14:24:58,908][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Initialized
    [14:24:58,908][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Starting ...
    [14:24:59,179][INFO ][transport                ] [War Machine] boundAddress [inet[/10.224.118.127:9300]], publishAddress [inet[/10.224.118.127:9300]]
    [14:25:08,919][WARN ][jgroups.pbcast.NAKACK    ] db3-20667: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667, db2-14372], view=[db2-14372|1] [db2-14372, db3-20667]
    [14:25:31,512][WARN ][discovery                ] [War Machine] Waited for 30s and no initial state was set by the discovery
    [14:25:31,512][INFO ][discovery                ] [War Machine] iAnnounce/db3-20667
    [14:25:31,524][INFO ][http                     ] [War Machine] boundAddress [inet[/10.224.118.127:9200]], publishAddress [inet[/10.224.118.127:9200]]
    [14:25:31,845][INFO ][jmx                      ] [War Machine] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://10.224.118.127:9400/jmxrmi]
    [14:25:31,845][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Started
    [14:25:33,407][INFO ][cluster.service          ] [War Machine] New Master [War Machine][db3-20667][data][inet[ip-10-224-118-127.eu-west-1.compute.internal/10.224.118.127:9300]]
    [14:25:35,048][WARN ][jgroups.pbcast.NAKACK    ] db3-20667: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667], view=[db3-20667|2] [db3-20667]
    [14:25:57,708][WARN ][jgroups.pbcast.NAKACK    ] db3-20667: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667], view=[db3-20667|2] [db3-20667]
    [14:25:59,258][WARN ][jgroups.pbcast.NAKACK    ] db3-20667: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667], view=[db3-20667|2] [db3-20667]
    [14:26:07,242][WARN ][jgroups.pbcast.NAKACK    ] db3-20667: dropped message from db1-13080 (not in xmit_table), keys are [db3-20667], view=[db3-20667|2] [db3-20667]
    [14:26:29,259][WARN ][jgroups.blocks.TCPConnectionMap] Could not read accept connection from peer java.net.SocketTimeoutException: Read timed out
    [14:26:36,756][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Stopping ...
    [14:26:37,382][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Stopped
    [14:26:37,382][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Closing ...
    [14:26:37,387][INFO ][server                   ] [War Machine] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Closed
    [14:35:33,336][INFO ][server                   ] [Death's Head I&amp;II] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Initializing ...
    [14:35:33,349][INFO ][plugins                  ] [Death's Head I&amp;II] Loaded []
    [14:35:35,510][INFO ][server                   ] [Death's Head I&amp;II] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Initialized
    [14:35:35,510][INFO ][server                   ] [Death's Head I&amp;II] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Starting ...
    [14:35:35,729][INFO ][transport                ] [Death's Head I&amp;II] boundAddress [inet[/10.224.118.127:9300]], publishAddress [inet[/10.224.118.127:9300]]
    [14:35:38,813][WARN ][jgroups.pbcast.GMS       ] join(db3-28837) sent to db1-32838 timed out (after 3000 ms), retrying
    [14:35:41,817][WARN ][jgroups.pbcast.GMS       ] join(db3-28837) sent to db1-32838 timed out (after 3000 ms), retrying
    [14:35:44,056][INFO ][cluster.service          ] [Death's Head I&amp;II] Detected Master [Blindspot][db1-32838][data][inet[ip-10-224-103-159.eu-west-1.compute.internal/10.224.103.159:9300]], Added {[Chaka][db2-22084][data][inet[ip-10-224-103-48.eu-west-1.compute.internal/10.224.103.48:9300]],[Blindspot][db1-32838][data][inet[ip-10-224-103-159.eu-west-1.compute.internal/10.224.103.159:9300]],}
    [14:35:44,907][INFO ][discovery                ] [Death's Head I&amp;II] iAnnounce/db3-28837
    [14:35:44,932][INFO ][http                     ] [Death's Head I&amp;II] boundAddress [inet[/10.224.118.127:9200]], publishAddress [inet[/10.224.118.127:9200]]
    [14:35:45,004][INFO ][jmx                      ] [Death's Head I&amp;II] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://10.224.118.127:9400/jmxrmi]
    [14:35:45,004][INFO ][server                   ] [Death's Head I&amp;II] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-31T14:31:22}: Started
</description><key id="163200">102</key><summary>Starting multiple nodes at the same time can cause the cluster not to communicate properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-04-01T14:52:10Z</created><updated>2010-05-28T08:42:11Z</updated><resolved>2010-05-28T08:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-01T20:27:26Z" id="175556">Hey,

Its a bit hard to nail this since you descrive two problems that I think are unrelated. It would be great to nail down one at a time.

First, what is your jgroups discovery module configuration? 

Second, regarding the single node starting and not recovering from the gateway, can you please set the `gateway.fs` logging to `TRACE` in the `logging.yml` file and post the log of that server (the part where it starts up and does not read it)?

Third, the other problems, of nodes not discovering each other. Thats a bit of a nasty one. Lets see what your jgroups configuration is and continue from there...
</comment><comment author="clintongormley" created="2010-04-01T20:52:42Z" id="175575">My jgroups config:

network:
       bindHost: 10.224.103.159
    discovery:
           jgroups:
                   config: tcp
                   tcpping:
                           initial_hosts: 10.224.103.159[7800],10.224.103.48[7800],10.224.118.127[7800]

I set my logging to:
    rootLogger: INFO, console, file
    logger:
         jgroups: WARN

```
gateway:
     fs:      TRACE
```

Or should that be this instead?
    gateway.fs:   TRACE

Btw, the issue of not seeing the index until two nodes have started only seems to happen after I've had a bad shutdown eg couldn't snapshot or something like that
</comment><comment author="kimchy" created="2010-04-01T20:57:01Z" id="175577">Configuration looks good. Regarding the logging, both option work, but note, this only applies to the single node starting and not recovering from the gateway.

Regarding the 3 notes started at the same time, I just pushed a more verbose logging into the jgroups discovery module, which I hope will help to nail down that problem. Set the `discovery.jgroups` to `DEBUG` and send me the 3 servers logs.

Lets try and nail these problems separately, otherwise I will get confused ;)
</comment><comment author="kimchy" created="2010-05-28T08:38:10Z" id="256465">Are we ok with this one, can I close this?
</comment><comment author="clintongormley" created="2010-05-28T08:42:11Z" id="256469">Yes - I think so.  In retrospect, the problem was probably timeouts caused by the unthrottled recovery
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clear Indices Cache API: Allow to clear indices cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/101</link><project id="" key="" /><description>Currently, there is only the filter cache (but there will be more...). The Rest API is either POST `/_cache/clear`, or POST `/{indices}/_cache/clear`.
</description><key id="163086">101</key><summary>Clear Indices Cache API: Allow to clear indices cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-04-01T09:25:20Z</created><updated>2010-04-01T09:26:01Z</updated><resolved>2010-04-01T09:26:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-04-01T09:26:01Z" id="175133">Clear Indices Cache API: Allow to clear indices cache, closed by fd574880fcf7cd7a14e25087029a47a7ae3aca1b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>locallucene plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/100</link><project id="" key="" /><description>Adding the ability to do geospatial indexing/searching using local lucene would be very useful, and a good candidate for a plugin.
</description><key id="162550">100</key><summary>locallucene plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2010-03-31T17:54:01Z</created><updated>2010-08-09T20:38:12Z</updated><resolved>2010-08-09T20:38:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-31T18:21:03Z" id="174461">Agreed, spatial search is something that I am going to work on. I don't even think it will be a plugin, but a core feature (attachments would have been core as well if it wasn't so big in terms of size).

Regarding the implementation itself, it seems like the spatial package is going through a lot of changes, so it will be delicate to create something that will survive the changes.
</comment><comment author="grantr" created="2010-05-19T18:03:21Z" id="246525">This may be useful in designing an api for spatial search: http://geojson.org/geojson-spec.html
</comment><comment author="kimchy" created="2010-05-19T18:42:52Z" id="246568">Interesting. I will certainly have a look at this once I tackle local lucene.
</comment><comment author="kimchy" created="2010-08-08T21:51:13Z" id="346305">Have been doing a lot of geo related work for the upcoming 0.9.1. Can you take a look and see if there is something missing? In any case, if there is, it would be great to raise it in the mailing list or another issue so we focus on specific features. What do you think, can this issue be closed?
</comment><comment author="grantr" created="2010-08-09T20:04:58Z" id="347664">The features we were looking for were:
radius (distance from point) search
bounding box search
polygon search
sort by distance from point

It looks like all of these have been implemented in 0.9.1 (I've been following commits closely). So I think this issue can safely be closed.

This is a really big deal for us! It finally allows us to drop sphinx for elasticsearch (sphinx supports geo distance sort), and we can add features that we couldn't with sphinx (thanks to polygon search)
</comment><comment author="kimchy" created="2010-08-09T20:38:12Z" id="347702">cool, I will close this issue. It would be a great help if you can take this features for a test drive.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search failing when cluster busy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/99</link><project id="" key="" /><description>Hiya Shay

It turns out that the issue I was having earlier with NFS was a red herring.  What seems to be happening is:

My process:
- i'm reindexing old_index to new_index
- i read 5000 docs from the old index, then create each one in the new index
- if there is an error, then i delete new_index

So:
- the cluster gets busy, and a search for the next 5,000 docs results in this error: `select failed: No child processes`.  
- This was triggering the cleanup in my script which deleted the index.  
- It appears the index has been deleted by one node, while another node is still trying to write snapshot info for the (now deleted) index, which results in these errors:
  
  [14:48:09,948][WARN ][index.gateway            ] [Nameless One][ia_object_1270046679][0] Failed to snapshot on close
  org.elasticsearch.index.gateway.IndexShardGatewaySnapshotFailedException: [ia_object_1270046679][0] Failed to append snapshot translog into [/opt/elasticsearch/data/iAnnounce/ia_object_1270046679/0/translog/translog-3]
      at org.elasticsearch.index.gateway.fs.FsIndexShardGateway.snapshot(FsIndexShardGateway.java:199)
      at org.elasticsearch.index.gateway.IndexShardGatewayService$1.snapshot(IndexShardGatewayService.java:154)
      at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:350)
      at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:369)
      at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:150)
      at org.elasticsearch.index.gateway.IndexShardGatewayService.close(IndexShardGatewayService.java:176)
      at org.elasticsearch.index.service.InternalIndexService.deleteShard(InternalIndexService.java:244)
      at org.elasticsearch.index.service.InternalIndexService.close(InternalIndexService.java:159)
      at org.elasticsearch.indices.InternalIndicesService.deleteIndex(InternalIndicesService.java:208)
      at org.elasticsearch.indices.InternalIndicesService.deleteIndex(InternalIndicesService.java:185)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
      at java.lang.reflect.Method.invoke(Method.java:597)
      at com.google.inject.internal.ConstructionContext$DelegatingInvocationHandler.invoke(ConstructionContext.java:108)
      at $Proxy19.deleteIndex(Unknown Source)
      at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:178)
      at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:193)
      at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
      at java.lang.Thread.run(Thread.java:619)
  Caused by: java.io.FileNotFoundException: /opt/elasticsearch/data/iAnnounce/ia_object_1270046679/0/translog/translog-3 (Stale NFS file handle)
      at java.io.RandomAccessFile.open(Native Method)
      at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:212)
      at org.elasticsearch.index.gateway.fs.FsIndexShardGateway.snapshot(FsIndexShardGateway.java:184)
      ... 20 more

Now, I'm catching the `select failed: No child processes` errors, sleeping for a few seconds, then trying again, and everything is working well.

ta

clint
</description><key id="162499">99</key><summary>Search failing when cluster busy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-31T16:26:07Z</created><updated>2011-07-27T17:37:19Z</updated><resolved>2011-07-27T17:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-31T18:18:23Z" id="174459">What is this select failed: No child process error? Very strange, there are no child processes in elasticsearch... . Is it an exception that you get as part of the search request that originates from elasticsearch? Do you see a stack trace? If not, can you set `action: DEBUG` on the logging.yml file and simulate it again, I would love to see the stack trace...
</comment><comment author="kimchy" created="2010-03-31T19:47:06Z" id="174530">Just committed a fix (I hope) for the snapshot/delete index thingy.
</comment><comment author="clintongormley" created="2010-04-01T15:52:40Z" id="175368">&#180;Fraid not :)

It logs that the index was deleted, but I see the directory growing for quite  a long time after the delete happened.
</comment><comment author="clintongormley" created="2010-04-01T15:59:53Z" id="175374">... then eventually I get this in the logs, and the dir size drops to 44kB (but continues to exist):

```
[WARN ][index.gateway            ] [Blade][ia_object_1270136239][4] Failed to snapshot on close
org.elasticsearch.index.gateway.IndexShardGatewaySnapshotFailedException: [ia_object_1270136239][4] Failed to finalize index snapshot into [/opt/elasticsearch/data/iAnnounce/ia_object_1270136239/4/index/segments_4]
    at org.elasticsearch.index.gateway.fs.FsIndexShardGateway.snapshot(FsIndexShardGateway.java:220)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.snapshot(IndexShardGatewayService.java:154)
    at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:350)
    at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:370)
    at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:150)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$SnapshotRunnable.run(IndexShardGatewayService.java:214)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: No such file or directory
    at java.io.UnixFileSystem.createFileExclusively(Native Method)
    at java.io.File.createNewFile(File.java:883)
    at org.elasticsearch.util.lucene.Directories.copyFromDirectory(Directories.java:86)
    at org.elasticsearch.index.gateway.fs.FsIndexShardGateway.snapshot(FsIndexShardGateway.java:209)
    ... 14 more
```
</comment><comment author="kimchy" created="2010-04-01T19:57:44Z" id="175531">Can you try it again, I just pushed a further validation that this will not happen. Just to make sure, I am trying to fix the delete index problem, that seems to collide with the scheduled snapshotting done for a shard.
</comment><comment author="clintongormley" created="2010-04-01T20:14:14Z" id="175547">Success+++  The index is deleted correctly, and no errors in the logs!

By the way, tt turns out that the `select failed: no child processes` was a spurious error caused by a fault in my code.  However, it was being thrown because of a real error: it looks like, when the cluster is busy, sometimes it doesn't return anything.

There is nothing in the logs, I just get this from the module doing the HTTP request:
`Server closed connection without sending any data back`

It takes quite a while (like 500,000 reindexed records) before I get this error, so not sure if it is doing it in the current build yet, but I'll rerun and let you know
</comment><comment author="kimchy" created="2010-04-01T20:23:41Z" id="175552">Do you know on which operation you get this exception? There is an http keep alive mechanism that closes connections after `30s` (which is configurable, see here: http://www.elasticsearch.com/docs/elasticsearch/modules/http/netty/, the `httpKeepAlive` option). I wonder maybe its too small a value, but it might be that the request is simply stuck on the server... . If so, I would love to understand first which request it is....
</comment><comment author="clintongormley" created="2010-04-01T20:49:10Z" id="175574">It's not that the keep-alive is timing out - the HTTP module I'm using handles that gracefully.  So it successfully makes the request, but then the ES server closes the connection.

The operation in question is a search:  

```
{ query: { matchAll: {}}, sort: ["id"], from: 500000, size: 5000 }
```

The 500,000 is an example.  If I wait a second or two, then the cluster responds correctly to the next request, although often there are a few such error messages close together.

There is nothing in the ES log
</comment><comment author="kimchy" created="2010-04-01T20:58:01Z" id="175578">The keepAlive thingy will close a connection even if a response was not sent (I need to double verify it). Can you time your requests, and the one that fails, can you print how long it took? If it's `30s`, then it might be it...
</comment><comment author="clintongormley" created="2010-04-02T13:42:31Z" id="176048">Your thought was correct - it is timing out after 30 seconds.  The request is as mentioned above, and as the nodes get busier, it takes 15 seconds plus to execute.

I'm comfortable with a timeout of 30 seconds - seems a reasonable setting to me. 

However, I'd consider changing the HTTP response code from 500 to 503 (Service unavailable -  http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.5.4 )

This kind of error is easy to catch and recover from, especially when you know to expect it (eg while hammering the cluster!)
</comment><comment author="kimchy" created="2010-04-02T13:56:22Z" id="176058">Actually, I need to think better on how to handle this properly. There might be requests that take more than 30s (for example, optimization) and I would not want to close the connection on the client calling it...

Can you verify that the invocation actually returns at the end by changing the timeout to a higher value, for example, set `http.netty.httpKeepAlive: 5m`.
</comment><comment author="clintongormley" created="2010-04-02T15:07:52Z" id="176091">Yeah, I can verify that it does eventually return, with results.

keep-alive is really meant for idle connections, rather than active requests.  you could have a request timeout set to much higher than the keep-alive
</comment><comment author="kimchy" created="2010-04-02T15:09:47Z" id="176094">Exactly. I will see what I can do to fix this..., cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>configuration export</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/98</link><project id="" key="" /><description>It would be useful to be able to dump elasticsearch configuration (indices, mapping, gateways, etc.) in YAML or JSON format.
This can be used for troubleshooting, to see what's created dynamically and also would help with in creation of configuration files and reduce syntactical errors
</description><key id="161690">98</key><summary>configuration export</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">berkay</reporter><labels /><created>2010-03-30T15:31:41Z</created><updated>2010-03-30T16:00:38Z</updated><resolved>2010-03-30T16:00:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-30T15:58:57Z" id="173431">There are two main APIs that provide you just that and much more. The indices status admin API, and the cluster state admin API.
</comment><comment author="berkay" created="2010-03-30T16:00:38Z" id="173434">perfect!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>moreLikeThis search query and count return different results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/97</link><project id="" key="" /><description>Why does a moreLikeThis query in count return half the results that the same query does in search?

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}
'


curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping?ignoreConflicts=true'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'


curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping?ignoreConflicts=true'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets"
   },
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'


curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;waitForStatus=green' 


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2'  -d '
{
   "num" : 3,
   "text" : "foo"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3'  -d '
{
   "num" : 4,
   "text" : "foo"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4'  -d '
{
   "num" : 5,
   "text" : "foo"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5'  -d '
{
   "num" : 6,
   "text" : "foo bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6'  -d '
{
   "num" : 7,
   "text" : "foo bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7'  -d '
{
   "num" : 8,
   "text" : "foo bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8'  -d '
{
   "num" : 9,
   "text" : "foo bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9'  -d '
{
   "num" : 10,
   "text" : "foo bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10'  -d '
{
   "num" : 11,
   "text" : "foo bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11'  -d '
{
   "num" : 12,
   "text" : "foo bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12'  -d '
{
   "num" : 13,
   "text" : "foo bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13'  -d '
{
   "num" : 14,
   "text" : "bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14'  -d '
{
   "num" : 15,
   "text" : "bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15'  -d '
{
   "num" : 16,
   "text" : "bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16'  -d '
{
   "num" : 17,
   "text" : "bar baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17'  -d '
{
   "num" : 18,
   "text" : "baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18'  -d '
{
   "num" : 19,
   "text" : "baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19'  -d '
{
   "num" : 20,
   "text" : "baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20'  -d '
{
   "num" : 21,
   "text" : "baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21'  -d '
{
   "num" : 22,
   "text" : "bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22'  -d '
{
   "num" : 23,
   "text" : "bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23'  -d '
{
   "num" : 24,
   "text" : "bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24'  -d '
{
   "num" : 25,
   "text" : "bar"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25'  -d '
{
   "num" : 26,
   "text" : "foo baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26'  -d '
{
   "num" : 27,
   "text" : "foo baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27'  -d '
{
   "num" : 28,
   "text" : "foo baz"
}
'


curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28'  -d '
{
   "num" : 29,
   "text" : "foo baz"
}
'


curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;waitForStatus=green' 

echo "

SEARCH
"

curl -XGET 'http://127.0.0.2:9200/_all/_search?pretty=true'  -d '
{
   "query" : {
      "moreLikeThis" : {
         "minTermFrequency" : 1,
         "likeText" : "foo bar baz"
      }
   }
}
'


echo "

COUNT
"

curl -XGET 'http://127.0.0.2:9200/_all/_count?pretty=true'  -d '
{
   "moreLikeThis" : {
      "minTermFrequency" : 1,
      "likeText" : "foo bar baz"
   }
}
'
```

ta

clint
</description><key id="160979">97</key><summary>moreLikeThis search query and count return different results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-29T18:10:11Z</created><updated>2010-04-02T16:13:41Z</updated><resolved>2010-04-02T16:13:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-31T21:49:02Z" id="174651">Hey,

  Can you check now, I pushed a fix that should resolve this. Count queries were being cached wrongly for this type of query (actually, the only type of query since mlt is a different beast).

  Note, mlt relies on document frequencies, and they are (as the case in the terms API) not exact until deletes are expunged (documents in Lucene are not deleted, just marked as deleted until they are merged out).

For cases like mlt, this should be good enough (the query will _not_ return a document that does not exists).
</comment><comment author="clintongormley" created="2010-04-02T16:13:40Z" id="176139">Fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fromInclusive and toInclusive default to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/96</link><project id="" key="" /><description>Hiya

In the docs for terms() ( http://www.elasticsearch.com/docs/elasticsearch/rest_api/terms/ ) you say that fromInclusve and toInclusive default to `true`, but in practice they default to `false`.

ta

clint
</description><key id="160883">96</key><summary>fromInclusive and toInclusive default to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-29T16:16:31Z</created><updated>2010-03-29T16:49:45Z</updated><resolved>2010-03-29T16:49:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-03-29T16:49:44Z" id="172349">Nope - I'm wrong. It was my own code messing it up. Sorry
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MoreLikeThis: Rename fields (rest/http parameter) to mltFields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/95</link><project id="" key="" /><description>As it collides with the fields you can pass in the body which relate to the search request fields.
</description><key id="160761">95</key><summary>MoreLikeThis: Rename fields (rest/http parameter) to mltFields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-29T13:42:50Z</created><updated>2010-03-29T13:43:33Z</updated><resolved>2010-03-29T13:43:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-29T13:43:33Z" id="172182">MoreLikeThis: Rename fields (rest/http parameter) to mltFields, closed by 81c6b9075c6c6e584c8029b52fa73f71b4597939.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The _moreLikeThis query fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/94</link><project id="" key="" /><description>A _moreLikeThis query fails, apparently because the query it constructs uses `like` instead of `likeText`

```
curl -XGET 'http://localhost:9200/ia_object/notice/754/_moreLikeThis?pretty=true'
{
  "error" : "SearchPhaseExecutionException[Failed to execute [query] total failure; shardFailures {[getafix-44509][ia_object_1269862021][1]: QueryPhaseExecutionException[[ia_object_1269862021][1]: query[like:all_birthday like:-9223372036854775808 like:-9223372036854775808 like:-9223372036854775808 like:active like:-292275055-05-16 16:47:04 like:wears like:-9223372036854775808 like:-9223372036854775808 like:-9223372036854775808 like:-292275055-05-16 16:47:04 like:-9223372036854775808 like:true like:-292275055-05-16 16:47:04 like:false like:birthday like:-292275055-05-16 16:47:04 like:happy 21st birthday katie nicholas\nWe wish you good health and happiness for your future.\nLove, Nan and Gran (Dad) XX.\nWill be loving you always. like:-9223372036854775808 like:-9223372036854775808 -_uid:notice#754],from[0],size[10]: Query Failed []]; nested: }{null: Unknown}{null: Unknown}{null: Unknown}{null: Unknown}]"
}
```
</description><key id="160677">94</key><summary>The _moreLikeThis query fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-29T11:37:48Z</created><updated>2011-03-20T14:58:37Z</updated><resolved>2011-03-20T14:58:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-29T13:34:07Z" id="172173">It constructs a likeText correctly. Is there a chance that you can post a test case with some data?
</comment><comment author="clintongormley" created="2010-03-29T18:32:49Z" id="172461">Hiya - this script reproduces the error:

```
curl -XPUT 'http://127.0.0.2:9200/ia_object/'  

curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/_mapping?ignoreConflicts=false'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets",
      "enabled" : 1
   },
   "properties" : {
      "notice_type" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "ancestor_ids" : {
         "type" : "long"
      },
      "status" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "location_ids" : {
         "type" : "long",
         "indexName" : "location_id"
      },
      "remote_last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "parent_id" : {
         "type" : "long"
      },
      "featured" : {
         "type" : "boolean"
      },
      "publish_date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "type" : "string"
      },
      "id" : {
         "type" : "long"
      },
      "creator_id" : {
         "nullValue" : 0,
         "type" : "long"
      },
      "last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "name" : {
         "boost" : "1.2",
         "type" : "string"
      },
      "has_name" : {
         "type" : "boolean"
      },
      "created" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "sub_type" : {
         "index" : "not_analyzed",
         "type" : "string"
      }
   }
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/754'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-03-25 18:33:14",
   "name" : "wears",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 1,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 21st birthday katie nicholas\nWe wish you good health and happiness for your future.\nLove, Nan and Gran (Dad) XX.\nWill be loving you always.",
   "id" : "754",
   "creator_id" : 0
}
'

curl -XGET 'http://127.0.0.2:9200/ia_object/notice/754/_moreLikeThis' 

# {
#    "error" : "SearchPhaseExecutionException[Failed to execute [qu
# &gt;    ery] total failure; shardFailures {[getafix-32775][ia_object
# &gt;    ][4]: QueryPhaseExecutionException[[ia_object][4]: query[lik
# &gt;    e:all_birthday like:-9223372036854775808 like:-9223372036854
# &gt;    775808 like:-9223372036854775808 like:active like:-292275055
# &gt;    -05-16 16:47:04 like:wears like:-9223372036854775808 like:-9
# &gt;    223372036854775808 like:-9223372036854775808 like:-292275055
# &gt;    -05-16 16:47:04 like:-9223372036854775808 like:true like:-29
# &gt;    2275055-05-16 16:47:04 like:false like:birthday like:-292275
# &gt;    055-05-16 16:47:04 like:happy 21st birthday katie nicholas\n
# &gt;    We wish you good health and happiness for your future.\nLove
# &gt;    , Nan and Gran (Dad) XX.\nWill be loving you always. like:-9
# &gt;    223372036854775808 like:-9223372036854775808 -_uid:notice#75
# &gt;    4],from[0],size[10]: Query Failed []]; nested: }{null: Unkno
# &gt;    wn}{null: Unknown}{null: Unknown}{null: Unknown}]"
# }
```
</comment><comment author="kimchy" created="2010-03-30T20:09:16Z" id="173671">I fixed the failure, but now, with the fix, when you just execute an mlt with no parameters, it will use the _source to generate a query, and note that it will ignore number based types.

One good option you may have is, when you store _all, is to list the `_all` in the `mltFileds`.
</comment><comment author="clintongormley" created="2010-04-02T16:28:51Z" id="176156">The query without and mltFields now works, however if i specify the `_all` field (or any unknown field), I get:

```
curl -XGET 'http://127.0.0.2:9200/es_test_1/type_1/1/_moreLikeThis?mltFields=_all&amp;minDocFreq=1&amp;minTermFrequency=1'  -d '
{}
'
# {
#    "error" : "NoShardAvailableActionException[[es_test_1][3] No s
# &gt;    hard available for [type_1#1]]; nested: ElasticSearchExcepti
# &gt;    on[No mapping for field [_all] in type [type_1]]; "
# }
```

And if I specify the `_source` field, I get:

```
curl -XGET 'http://127.0.0.2:9200/es_test_1/type_1/1/_moreLikeThis?mltFields=_source&amp;minDocFreq=1&amp;minTermFrequency=1'  -d '
{}
'
# {
#    "error" : "NullPointerException[null]"
# }
```

In my local test suite, specifying the `text` field works, but in my live setup (with more complex docs), I get the same NullPointerException if I specify any field name.
</comment><comment author="clintongormley" created="2010-04-02T16:36:28Z" id="176159">In fact, if you use my original test case in the first post in this issue, and just change the last line to:

```
curl -XGET 'http://127.0.0.2:9200/ia_object/notice/754/_moreLikeThis?mltFields=text'
```

.... you'll get a NullPointerException
</comment><comment author="kimchy" created="2010-04-02T19:23:30Z" id="176322">ok, hopefully its fixed now.
</comment><comment author="clintongormley" created="2010-04-03T10:21:57Z" id="176781">looks good.  One thing is if I specify (eg) a date field, instead of reporting "you can't to mlt on a date field" it says "No fields found to fetch likeText from" - could be a better error message.  I get the same message if specifying `_source`
</comment><comment author="clintongormley" created="2011-03-20T14:58:37Z" id="895311">Fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index aliases ignored for /.../_moreLikeThis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/93</link><project id="" key="" /><description>Hiya

Doing a moreLikeThis query on an index-alias instead of an index fails with:

```
{ "error" : "IndexMissingException[[my_alias] missing]"}
```

ta

Clint
</description><key id="160658">93</key><summary>Index aliases ignored for /.../_moreLikeThis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2010-03-29T11:03:42Z</created><updated>2010-03-29T11:27:46Z</updated><resolved>2010-03-29T11:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-29T11:14:30Z" id="172052">Index aliases ignored for /.../_moreLikeThis, closed by 8402738bbe4c955473a4eb810d2642e4c74dc78d.
</comment><comment author="clintongormley" created="2010-03-29T11:27:46Z" id="172057">Fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins::Attachments: Add an attachements plugin (support parsing various file formats)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/92</link><project id="" key="" /><description>Using the new plugins system, implement the `attachments` plugin, allow to add a mapping type called `attachment` which accepts a binary input (base64) of an attachment to index.

Installation is simple, just download the plugin zip file and place it under `plugins` directory within the installation. When building from source, the plugin will be under `build/distributions/plugins`. Once placed in the installation, the `attachment` mapper type will be automatically supported.

Using the `attachment` type is simple, in your mapping JSON, simply a certain JSON element as `attachment`, for example:

```
{
    person : {
        properties : {
            "myAttachment" : { type : "attachment" }
        }
    }
}
```

In this case, the JSON to index can be:

```
{
    myAttachment : "... base64 encoded attachment ..."
}
```

The `attachment` type not only indexes the content of the doc, but also automatically adds meta data on the attachment as well (when available). The metadata supported are: `date`, `title`, `author`, and `keywords`. They can be queries using the "dot notation", for example: `myAttachment.author`.

Both the meta data and the actual content are simple core type mappers (`string`, `date`, ...), thus, they can be controlled in the mappings. For example:

```
{
    person : {
        properties : {
            "file" : { 
                type : "attachment",
                fields : {
                    file : {index : "no"},
                    date : {store : "yes"},
                    author : {analyzer: "myAnalyzer"}
                }
            }
        }
    }
}
```

In the above example, the actual content indexed is mapped under `fields` name `file`, and we decide not to index it, so it will only be available in the `_all` field. The other `fields` map to their respective metadata names, but there is no need to specify the `type` (like `string` or `date`) since it is already known.

The plugin uses Apache Tika (http://lucene.apache.org/tika/) to parse it, so many formats are supported, listed here: http://lucene.apache.org/tika/0.6/formats.html.
</description><key id="160612">92</key><summary>Plugins::Attachments: Add an attachements plugin (support parsing various file formats)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-29T10:00:08Z</created><updated>2010-04-05T23:20:32Z</updated><resolved>2010-03-29T10:01:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-29T10:01:42Z" id="172010">Implemented.
</comment><comment author="lukas-vlcek" created="2010-04-05T21:55:58Z" id="178521">Not an Tika expert but it seems that Tika somehow supports for documents having nested documents (as of writing this is used when extracting content from archive files: zip, tar, ... etc). This could be also customized and used in other use cases (like parsing large mbox files, see http://markmail.org/message/h47lnpxtmdskmest ). Does ES integration take account on this? Note that in case of extracting data from archives individual documents are separated by DIV tags having specific class only. Looking at current ES implementation it seems that all nested documents are simply merged into one output document (parsedContent = tika().parseToString(new FastByteArrayInputStream(content), metadata)). Is there any way how this can be customized?
What I would love to see is an option to extract data from archive first, split into individual documents and then parse individual documents in parallel.
</comment><comment author="kimchy" created="2010-04-05T23:20:32Z" id="178587">Yea, archives are not really meant to be supported currently. This is for the simple reaons that archives are usually very large and it does not make sense to send them in a single HTTP request.

One option is to do the parsing on the client side, and feed elasticsearch with the documents. Another option is for the plugin to expose a streaming endpoint, that will parse and generate several documents out of the compound stream.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an S3 gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/91</link><project id="" key="" /><description>I think S3 would be a perfect place to store metadata and index snapshots.  It's reliable, scalable, cheap, and maintained by somebody else.  And if you're running on ec2, it's pretty fast as well.

An S3 gateway would make deployment of a persistent elasticsearch cluster much easier, because there's no external configuration required (of a shared filesystem).
</description><key id="159659">91</key><summary>Add an S3 gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2010-03-27T17:39:38Z</created><updated>2010-05-12T00:37:26Z</updated><resolved>2010-05-12T00:37:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-27T19:49:44Z" id="171027">Agreed, its certainly planned (not only S3, but others as well...), I plan to make the deployment on aws as simple as possible.
</comment><comment author="kimchy" created="2010-05-12T00:37:25Z" id="238457">Implemented in the cloud plugin, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Allow to have for a field to not be stored and not indexed, but still be included in `all`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/90</link><project id="" key="" /><description>By default, fields are indexed, not stored, and included in all. Sometimes, just having the index part of `_all` is enough. This can now be achieved by setting index to `no` (previously, this meant it will not be included in `_all`). In order to remove it from all as well, the `includeInAll` should be set to `false`.
</description><key id="158529">90</key><summary>Mapping: Allow to have for a field to not be stored and not indexed, but still be included in `all`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-25T18:56:21Z</created><updated>2010-03-25T18:56:52Z</updated><resolved>2010-03-25T18:56:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-25T18:56:52Z" id="169426">Mapping: Allow to have for a field to not be stored and not indexed, but still be included in `all`, closed by 9c0a37df3225393099ea801208af668030acce48.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices are no longer being auto-created with put_mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/89</link><project id="" key="" /><description>Start a node and do this:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1,es_test_2/test/_mapping?ignoreConflicts=true'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
```

Errors with: 
    {"error":"[es_test_1] missing"}
</description><key id="158257">89</key><summary>Indices are no longer being auto-created with put_mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-25T11:55:56Z</created><updated>2011-03-20T13:46:37Z</updated><resolved>2011-03-20T13:46:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-25T11:58:28Z" id="168999">Yea, had to remove this for now because of the aliases support. Need to think how to bring it back properly.
</comment><comment author="clintongormley" created="2011-03-20T13:46:37Z" id="895170">Won't be fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/88</link><project id="" key="" /><description>Allow to define aliases to indices, and have all APIs support that. Here is a simple way to define the alias using the REST API (`localhost:9200/_aliases`):

```
{
    actions : [
        { add : { index : "test1", alias : "alias1" } }
    ]
}
```

In the above, we define an `alias1` for index `test1`. In all operations, we can use `alias1` instead of `test1`. The API also allows to remove an alias from an index, and better yet remove and then add an alias for an index:

```
{
    actions : [
        { remove : { index : "test1", alias : "alias1" } },
        { add : { index : "test1", alias : "alias2" } }
    ]
}
```

In the above example, we simple remove `alias1` and add `alias2` to `test1` in a single, atomic operation.

Last, but not least, an alias can be assigned to more than one index. This applies to APIs that accept more than one index, such as search and count. Instead of specifying a list of indices, we can just alias them to the same alias, and use it. For example:

```
{
    actions : [
        { add : { index : "test1", alias : "alias1" } },
        { add : { index : "test1", alias : "alias2" } }
    ]
}
```

Ohh, and index aliases can be set when creating an index, here is a YAML example:

```
index:
    aliases : ["alias1", "alias2"]
```
</description><key id="158028">88</key><summary>Index Aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-24T23:59:53Z</created><updated>2013-03-26T13:51:03Z</updated><resolved>2010-03-25T00:01:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-25T00:01:05Z" id="168669">Index Aliases, closed by 8f324678e814432d32659bc7e48694759149ae74.
</comment><comment author="stoned7" created="2013-03-26T13:51:03Z" id="15459084">the yaml example, of creating alias during creation of index request, can you give me json post data of the same
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default Mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/87</link><project id="" key="" /><description>Allow for the creation of mappings that will be inherited when new indices are created, specifically when an index PUT request is made for an index that doesn't exist yet.
</description><key id="157907">87</key><summary>Default Mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BubDZombie</reporter><labels /><created>2010-03-24T20:25:49Z</created><updated>2013-01-10T15:36:24Z</updated><resolved>2013-01-10T15:36:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="saden1" created="2013-01-10T08:49:34Z" id="12085294">I just wanted to clarify if this issue still holds true and if there is any intention to fix it.

Say I have a default mappings file that specifies a field called "content" and I also create a mappings file for an index called "posts" that does not define a "content" property. Is true that if I index a document that contains the property "content" to the "posts" index the default mappings definition of this "content" property will be ignored? 
</comment><comment author="imotov" created="2013-01-10T15:31:25Z" id="12102195">@saden1 if you will define a property called "content" in the default mapping, this property will be added automatically for every mapping that doesn't define it.
</comment><comment author="imotov" created="2013-01-10T15:36:24Z" id="12102495">This issue is addressed with [Index](http://www.elasticsearch.org/guide/reference/api/admin-indices-templates.html) and [Dynamic](http://www.elasticsearch.org/guide/reference/mapping/root-object-type.html) Templates. Closing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boot-time Mapping Definitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/86</link><project id="" key="" /><description>Allow for mappings to be defined in a config file so that they will be pre-defined to the cluster when it starts up.
</description><key id="157905">86</key><summary>Boot-time Mapping Definitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BubDZombie</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-24T20:23:31Z</created><updated>2013-10-06T12:12:34Z</updated><resolved>2010-03-25T11:34:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-25T11:33:48Z" id="168978">I am going to allow for mappings to be defined under the config/mappings location. Each mapping should be in its own file, and the file name should be [mappingName].json. For example, under config/mappings you will have `person.json` that will have the `person` mappings.

Also, note that the mappings must exists on all nodes/servers in the cluster.
</comment><comment author="kimchy" created="2010-03-25T11:34:42Z" id="168980">Boot-time Mapping Definitions, closed by e5cd594503ef91f5b447a0489f12c5223eacf5be.
</comment><comment author="kimchy" created="2010-04-09T12:02:41Z" id="182503">Change the default location to `config/mappings/_default`, and also `config/mappings/[index_name]` can be used for specific indices.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Rename `filteredQuery` to `filtered`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/85</link><project id="" key="" /><description>The `filteredQuery` is badly named, as no other query is suffixed by `Query`. Rename it to `filtered`.
</description><key id="157221">85</key><summary>Query DSL: Rename `filteredQuery` to `filtered`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-23T22:36:08Z</created><updated>2010-03-23T22:38:09Z</updated><resolved>2010-03-23T22:38:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-23T22:38:09Z" id="167485">Query DSL: Rename `filteredQuery` to `filtered`, closed by f4c94a35d643629d2cd48d42874659070c737092.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Support for short type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/84</link><project id="" key="" /><description>Add support for numeric short type (2 bytes, -32,768 to 32,767). This type improves on index size when storing the number, and improve on memory utilization on certain operations.
</description><key id="156763">84</key><summary>Mapping: Support for short type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-23T12:17:50Z</created><updated>2010-03-23T12:21:07Z</updated><resolved>2010-03-23T12:21:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-23T12:21:07Z" id="166905">Mapping: Support for short type, closed by 621d222c9435e72cf3a15d9cd9cec35e1c359e0b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search with sort field of score fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/83</link><project id="" key="" /><description>Hiya

I've managed to replicate the RemoteTransportException that doesn't include the actual exception.

Start two nodes on localhost, then run this script:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/_mapping?ignoreConflicts=false'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets",
      "enabled" : 1
   },
   "properties" : {
      "notice_type" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "ancestor_ids" : {
         "type" : "long"
      },
      "status" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "location_ids" : {
         "type" : "long",
         "indexName" : "location_id"
      },
      "remote_last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "parent_id" : {
         "type" : "long"
      },
      "featured" : {
         "type" : "boolean"
      },
      "publish_date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "type" : "string"
      },
      "id" : {
         "type" : "long"
      },
      "creator_id" : {
         "nullValue" : 0,
         "type" : "long"
      },
      "last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "name" : {
         "boost" : "1.2",
         "type" : "string"
      },
      "has_name" : {
         "type" : "boolean"
      },
      "created" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "sub_type" : {
         "index" : "not_analyzed",
         "type" : "string"
      }
   }
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/754'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 21st birthday katie nicholas\nWe wish you good health and happiness for your future.\nLove, Nan and Gran (Dad) XX.\nWill be loving you always.",
   "id" : "754",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/755'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "id" : "755",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/756'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "HAPPY 18TH BIRTHDAY ROSHENE SMITH To a wonderful daughter. Enjoy your day. Lots of love. Mum and Dad XX.\nHappy Birthday Roshene. Legal at last!!\nNow the drinks are on you, not us.\nLove from Saffron and Grandma XX.",
   "id" : "756",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/757'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 18th birthday david and ashley donald Our precious babies all grown up now. We love you both so much. All our love. Mum, Dad, James, Lucy and Daisy XXXX.\nP.S. Congratulations to both of you for being accepted into The Camelia Botnar Foundation.\nWe are so proud of you.",
   "id" : "757",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/758'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 18th birthday ashley and david donald TWINS.\nHave a lovely birthday now it is all legal. Ha Ha. Lots of love and kisses. Nan XXXXX.",
   "id" : "758",
   "creator_id" : 0
}
'


curl -XGET 'http://127.0.0.2:9201/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.2:9200/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'
```
</description><key id="155972">83</key><summary>Search with sort field of score fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-22T15:32:46Z</created><updated>2010-03-23T11:58:27Z</updated><resolved>2010-03-23T11:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-03-22T16:03:38Z" id="165780">This seems to be the cause of the bad total values too - some shards fail, and the returned hits are less than estimated:

Try this script - i add various docs, do a search on both nodes, sleep, repeat the search, add another doc and repeat the search:

```
curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/_mapping?ignoreConflicts=false'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets",
      "enabled" : 1
   },
   "properties" : {
      "notice_type" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "ancestor_ids" : {
         "type" : "long"
      },
      "status" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "location_ids" : {
         "type" : "long",
         "indexName" : "location_id"
      },
      "remote_last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "parent_id" : {
         "type" : "long"
      },
      "featured" : {
         "type" : "boolean"
      },
      "publish_date" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "text" : {
         "type" : "string"
      },
      "id" : {
         "type" : "long"
      },
      "creator_id" : {
         "nullValue" : 0,
         "type" : "long"
      },
      "last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "name" : {
         "boost" : "1.2",
         "type" : "string"
      },
      "has_name" : {
         "type" : "boolean"
      },
      "created" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "sub_type" : {
         "index" : "not_analyzed",
         "type" : "string"
      }
   }
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/754'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 21st birthday katie nicholas\nWe wish you good health and happiness for your future.\nLove, Nan and Gran (Dad) XX.\nWill be loving you always.",
   "id" : "754",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/755'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "id" : "755",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/756'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "HAPPY 18TH BIRTHDAY ROSHENE SMITH To a wonderful daughter. Enjoy your day. Lots of love. Mum and Dad XX.\nHappy Birthday Roshene. Legal at last!!\nNow the drinks are on you, not us.\nLove from Saffron and Grandma XX.",
   "id" : "756",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/757'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 18th birthday david and ashley donald Our precious babies all grown up now. We love you both so much. All our love. Mum, Dad, James, Lucy and Daisy XXXX.\nP.S. Congratulations to both of you for being accepted into The Camelia Botnar Foundation.\nWe are so proud of you.",
   "id" : "757",
   "creator_id" : 0
}
'


curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/758'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 18th birthday ashley and david donald TWINS.\nHave a lovely birthday now it is all legal. Ha Ha. Lots of love and kisses. Nan XXXXX.",
   "id" : "758",
   "creator_id" : 0
}
'


curl -XGET 'http://127.0.0.2:9201/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.2:9200/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'

sleep 1;

curl -XGET 'http://127.0.0.2:9201/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.2:9200/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'

curl -XPUT 'http://127.0.0.2:9200/ia_object/notice/759'  -d '
{
   "notice_type" : "all_birthday",
   "ancestor_ids" : [
      "268",
      "23",
      "22"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:47:46",
   "name" : "",
   "location_ids" : [
      "23",
      "24",
      "30"
   ],
   "remote_last_modified" : "2007-02-14 18:10:00",
   "parent_id" : "268",
   "has_name" : 0,
   "created" : "2010-01-11 18:47:46",
   "featured" : 0,
   "sub_type" : "birthday",
   "publish_date" : "2007-02-13 00:00:00",
   "text" : "happy 21st birthday katie nicholas\nWe wish you good health and happiness for your future.\nLove, Nan and Gran (Dad) XX.\nWill be loving you always.",
   "id" : "754",
   "creator_id" : 0
}
'
curl -XGET 'http://127.0.0.2:9201/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.2:9200/ia_object/notice/_search?searchType=dfs_query_then_fetch&amp;pretty=1'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "status" : "active"
            }
         },
         "query" : {
            "queryString" : {
               "query" : "happy"
            }
         }
      }
   }
}
'
```
</comment><comment author="kimchy" created="2010-03-23T08:10:44Z" id="166729">Fixed, can you verify?
</comment><comment author="clintongormley" created="2010-03-23T11:58:26Z" id="166873">Fixed+++  Sheer genius :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index FS Store: Allow to cache (in memory) specific files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/82</link><project id="" key="" /><description>When using file system based storage, caching specific (lucene index) files make sense to speed up operations. The ability to specify which files to cache in memory (with all the parameters the memory store provides) based on their suffix should be provided. This allows, for example, to store term freq in memory, while keeping the field store on disk (which takes much more space).

The configuration should look something like this:

```
index:
    store:
        fs:
            memory:
                enabled: true
                extensions: ["", "del", "gen"]
```
</description><key id="155958">82</key><summary>Index FS Store: Allow to cache (in memory) specific files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-22T15:08:37Z</created><updated>2010-04-07T11:10:51Z</updated><resolved>2010-03-22T15:09:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-22T15:09:10Z" id="165709">Index FS Store: Allow to cache (in memory) specific files, closed by fa55c40c87a0571e8d54352f0f319cf45a19cbc3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reset ulimit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/81</link><project id="" key="" /><description>Hiya

I'm running ES with niofs, and after a while it hits the open file limit (default on my linux box is 1024).  Any chance you can add a `ulimit -n 4096` into the startup?  

Not sure if 4096 is the right number, but works for me :)

thanks

Cilnt
</description><key id="155934">81</key><summary>Reset ulimit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-22T14:53:27Z</created><updated>2010-03-30T17:00:15Z</updated><resolved>2010-03-30T17:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-22T18:45:30Z" id="166024">It should probably set it to a higher value, 16k or 32k. If I add the `ulimit` to the script, it might fail if there are no permissions to set it...
</comment><comment author="clintongormley" created="2010-03-22T19:28:33Z" id="166068">If ulimit fails, you could warn, no?
</comment><comment author="kimchy" created="2010-03-22T19:38:14Z" id="166088">Its just that I have never seen it being set explicitly in a startup script of any server product. Might be too intrusive. Have you seen it somewhere?
</comment><comment author="clintongormley" created="2010-03-22T19:54:13Z" id="166104">Not that I know of, I just thought that it goes along with the DoTheRightThing aspect of ES, but as you like :)
</comment><comment author="asereda" created="2010-03-29T21:14:07Z" id="172649">Hi Clint,
usually you set max open files limits in /etc/security/limits.conf (on RedHat flavours). Linux system will not allow a simple user to set anything higher than those limits.  You can define max open files per-user by adding the following line:

elasticsearch       hard    nofile          8096

Andrei.
</comment><comment author="clintongormley" created="2010-03-30T17:00:14Z" id="173482">Thanks Andrei - I wasn't aware of that

clint
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms query is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/80</link><project id="" key="" /><description>Hiya

Commit 93e025325e014693c813069091c7f0c213298abb breaks the terms query:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping?ignoreConflicts=true'  -d '
{
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'


curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2s&amp;waitForStatus=yellow' 


curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'

curl -XGET 'http://127.0.0.2:9200/_terms?fields=num&amp;toInclusive=true' 
# {"error":null}
```
</description><key id="155903">80</key><summary>Terms query is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-22T14:17:15Z</created><updated>2010-03-22T19:37:42Z</updated><resolved>2010-03-22T18:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-22T18:41:26Z" id="166018">Terms query is broken, closed by f9cac39024c32da016212fcb2d71c517e06de6af.
</comment><comment author="clintongormley" created="2010-03-22T19:37:42Z" id="166085">Fixed ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put mapping _all with no indices fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/79</link><project id="" key="" /><description>Hiya

If no indices exist, then put_mapping on `_all` fails:

```
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping?ignoreConflicts=true'  -d '
&gt; {
&gt;    "properties" : {
&gt;       "num" : {
&gt;          "store" : "yes",
&gt;          "type" : "integer"
&gt;       },
&gt;       "text" : {
&gt;          "store" : "yes",
&gt;          "type" : "string"
&gt;       }
&gt;    }
&gt; }
&gt; '
{
  "error" : "NoSuchElementException[null]"
}
```

Maybe this is correct?
</description><key id="155898">79</key><summary>Put mapping _all with no indices fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label></labels><created>2010-03-22T14:11:16Z</created><updated>2010-03-22T19:38:11Z</updated><resolved>2010-03-22T18:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-22T18:41:26Z" id="166019">better exception when trying to put mappings when no indices exists, closed by 89043b113a3b6c4bf9dbfb35f0d8759a149cb053.
</comment><comment author="clintongormley" created="2010-03-22T19:38:11Z" id="166087">fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms API: Support numbers/dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/78</link><project id="" key="" /><description>Terms API should support numbers and dates natively, and not treat them as strings. This means that from/to should support numbers/dates, and the terms returned should be in the number/date format and not in their internal format.

The convert parameter should be removed also since it is not required anymore.
</description><key id="155599">78</key><summary>Terms API: Support numbers/dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-22T00:33:27Z</created><updated>2010-03-22T00:34:52Z</updated><resolved>2010-03-22T00:34:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-22T00:34:52Z" id="165211">Terms API: Support numbers/dates, closed by 93e025325e014693c813069091c7f0c213298abb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/77</link><project id="" key="" /><description>When executing a search, a scroll parameter can be passed with a timeout value. When passed, a scrollId will be returned as part of the response. The scrollId can then be used to continue to scroll the results of the query. Each scroll request accepts (the same) scroll parameter with a timeout value to allow to continue scrolling.

For example:

```
curl -XGET localhost:9200/_search?q=content:something&amp;scroll=5m
```

Will execute a search with a scroll timeout of 5 minutes. The result `_scrollId` can be used to execute a scroll request:

```
curl -XGET localhost:9200/_search/scroll?scroll=5m&amp;scrollId=cXV...
```

The scroll operation is a point in time "snapshot" search. Any changes applied to the index, even after a refresh, will not be taken into account when scrolling.

Note, scrolling takes resources from each node they run on, since a state needs to be stored to continue scrolling.
</description><key id="155129">77</key><summary>Search API: Scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-20T23:14:10Z</created><updated>2010-04-03T23:03:32Z</updated><resolved>2010-03-20T23:15:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-20T23:15:04Z" id="164565">Search API: Scroll, closed by 1e455789d09295742ead819ad748886fad8e8a57.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Another truthy falsy issue, this time in the allField</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/76</link><project id="" key="" /><description>Hiya

This statement cause a null pointer exception, but succeeds when `enabled` is `true`:

```
curl -XPUT 'http://127.0.0.2:9200/ia_object/site/_mapping?ignoreConflicts=false'  -d '
{
   "allField" : {
      "store" : "yes",
      "termVector" : "with_positions_offsets",
      "enabled" : 1
   },
   "properties" : {
      "created" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "ancestor_ids" : {
         "type" : "long"
      },
      "status" : {
         "index" : "not_analyzed",
         "type" : "string"
      },
      "last_modified" : {
         "format" : "yyyy-MM-dd HH:mm:ss",
         "type" : "date"
      },
      "id" : {
         "type" : "long"
      },
      "parent_id" : {
         "type" : "long"
      },
      "creator_id" : {
         "nullValue" : 0,
         "type" : "long"
      }
   }
}
'
```
</description><key id="154859">76</key><summary>Another truthy falsy issue, this time in the allField</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-20T11:48:34Z</created><updated>2010-03-20T13:18:02Z</updated><resolved>2010-03-20T12:40:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-20T12:40:35Z" id="164201">parse correct enabled flag on all field when passed as number, closed by fa1071d0907b71e6e1aadc104aff619061bd2c1c
</comment><comment author="clintongormley" created="2010-03-20T13:18:02Z" id="164217">Fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ThreadPool: Rename dynamic to scaling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/75</link><project id="" key="" /><description>Better fitting name, especially with other thread pools I plan to play with
</description><key id="154644">75</key><summary>ThreadPool: Rename dynamic to scaling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-20T01:40:50Z</created><updated>2010-03-20T01:41:46Z</updated><resolved>2010-03-20T01:41:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-20T01:41:46Z" id="163950">ThreadPool: Rename dynamic to scaling, closed by 671bc4e734a0f900c24ff6103093e5550f1a0163.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ThreadPool: Add a blocking thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/74</link><project id="" key="" /><description>Add another thread pool, a blocking one, that will wait if the thread pool if busy on the execute level, and not add the task to a queue.
</description><key id="154638">74</key><summary>ThreadPool: Add a blocking thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-20T01:24:44Z</created><updated>2010-03-20T16:01:55Z</updated><resolved>2010-03-20T16:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-20T16:01:55Z" id="164293">closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tieBreakerMultiplier is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/73</link><project id="" key="" /><description>Hiya

The query below just hangs. Without the tieBreakerMultiplier it works fine:

```
curl -XGET 'http://127.0.0.2:9200/ia_object/notice/_search?searchType=dfs_query_then_fetch'  -d '
{
   "sort" : [
      "score"
   ],
   "fields" : [],
   "query" : {
      "tieBreakerMultiplier" : 0.7,
      "disMax" : {
         "queries" : [
            {
               "field" : {
                  "text" : {
                     "boost" : 1,
                     "query" : "andrew margaret"
                  }
               }
            },
            {
               "field" : {
                  "name" : {
                     "boost" : 1,
                     "query" : "andrew margaret"
                  }
               }
            }
         ]
      }
   },
   "from" : 0,
   "explain" : 1,
   "size" : 300
}
'
```

ta

clint
</description><key id="154469">73</key><summary>tieBreakerMultiplier is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-19T18:29:08Z</created><updated>2010-03-20T16:53:51Z</updated><resolved>2010-03-20T16:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-19T18:54:14Z" id="163674">can you add sample data that you used with this query?
</comment><comment author="clintongormley" created="2010-03-20T12:00:02Z" id="164185">here's a complete test case:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 
# {
#    "clusterName" : "iAnnounce",
#    "nodes" : {
#       "getafix-9780" : {
#          "httpAddress" : "inet[/127.0.0.2:9200]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9300]",
#          "name" : "Coldblood"
#       }
#    }
# }


curl -XPUT 'http://127.0.0.2:9200/ia_object/site/1'  -d '
{
   "created" : "2010-01-11 18:46:17",
   "ancestor_ids" : [],
   "status" : "active",
   "last_modified" : "2010-01-11 18:46:17",
   "id" : "1",
   "parent_id" : 0,
   "creator_id" : 0
}
'
# {
#    "ok" : true,
#    "_index" : "ia_object",
#    "_id" : "1",
#    "_type" : "site"
# }


curl -XPUT 'http://127.0.0.2:9200/ia_object/group/2'  -d '
{
   "ancestor_ids" : [],
   "status" : "active",
   "last_modified" : "2010-01-11 18:46:17",
   "name" : "All Scrapers",
   "parent_id" : 0,
   "text_id" : "all_scrapers",
   "created" : "2010-01-11 18:46:17",
   "id" : "2",
   "creator_id" : 0
}
'
# {
#    "ok" : true,
#    "_index" : "ia_object",
#    "_id" : "2",
#    "_type" : "group"
# }


curl -XPUT 'http://127.0.0.2:9200/ia_object/group/3'  -d '
{
   "ancestor_ids" : [
      "1"
   ],
   "status" : "active",
   "last_modified" : "2010-01-11 18:46:17",
   "name" : "All iAnnounce Scrapers",
   "parent_id" : "1",
   "text_id" : "iannounce_all_scrapers",
   "created" : "2010-01-11 18:46:17",
   "id" : "3",
   "creator_id" : 0
}
'
# {
#    "ok" : true,
#    "_index" : "ia_object",
#    "_id" : "3",
#    "_type" : "group"
# }


curl -XPUT 'http://127.0.0.2:9200/ia_object/group/4'  -d '
{
   "ancestor_ids" : [],
   "status" : "active",
   "last_modified" : "2010-01-11 18:46:17",
   "name" : "All Notices",
   "parent_id" : 0,
   "text_id" : "all_notices",
   "created" : "2010-01-11 18:46:17",
   "id" : "4",
   "creator_id" : 0
}
'
# {
#    "ok" : true,
#    "_index" : "ia_object",
#    "_id" : "4",
#    "_type" : "group"
# }

curl -XGET 'http://127.0.0.2:9200/ia_object/notice/_search?searchType=dfs_query_then_fetch'  -d '
{
   "query" : {
      "tieBreakerMultiplier" : 0.7,
      "disMax" : {
         "queries" : [
            {
               "field" : {
                  "text" : {
                     "boost" : 1.1,
                     "query" : "andrew margaret"
                  }
               }
            },
            {
               "field" : {
                  "name" : {
                     "boost" : 1,
                     "query" : "andrew margaret"
                  }
               }
            }
         ]
      }
   }
}
'
```
</comment><comment author="clintongormley" created="2010-03-20T16:53:51Z" id="164338">Fixed - tieBreakerMultiplier was used at the wrong level. Now the server throws an error instead of hanging
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Analysis: Add language analyzers and stemmers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/72</link><project id="" key="" /><description>Analyzers added: `arabic`, `brazilian`, `chinese`, `cjk`, `czech`, `dutch`, `french`, `german`, `greek`, `persian`, `russian`, `thai`.

Token Filters (stemmers): `arabicStem`, `brazilianStem`, `dutchStem`, `frenchStem`, `germanStem`, `russianStem`
</description><key id="154341">72</key><summary>Index Analysis: Add language analyzers and stemmers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-19T15:07:09Z</created><updated>2010-03-19T15:07:52Z</updated><resolved>2010-03-19T15:07:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-19T15:07:52Z" id="163489">Index Analysis: Add language analyzers and stemmers, closed by a344ebb1b34a40b7e2525d52d2abc87b6d276c24
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a lock_cluster command ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/71</link><project id="" key="" /><description>Just an idea, how about being able to lock the cluster to prevent writes and or reads.  

For instance, when the cluster is starting up and restoring from the gateway, it may take a few seconds to load all of the data.  It'd be nice to say:

```
lock the cluster until cluster_health is green
```

Also, when changing a mapping, (thus forcing a reindex), we could do something like:
- generate new index
- lock cluster
- rename the indices
- flush index
- unlock cluster when cluster_health is green

ta

clint
</description><key id="153751">71</key><summary>Add a lock_cluster command ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-18T16:04:53Z</created><updated>2011-03-20T13:44:39Z</updated><resolved>2011-03-20T13:44:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-19T23:06:31Z" id="163870">regarding the first point, the APIs will simply wait (until the timeout) if the cluster can't fullfil the requests yet.

An index level lock is interesting (mainly, I guess, to disallow dirty operations).
</comment><comment author="clintongormley" created="2011-03-20T13:44:39Z" id="895165">This seems to be handled nicely in recent releases
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory Store: change default bufferSize from 1k to 100k</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/70</link><project id="" key="" /><description /><key id="153729">70</key><summary>Memory Store: change default bufferSize from 1k to 100k</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-18T15:26:45Z</created><updated>2010-03-18T15:27:28Z</updated><resolved>2010-03-18T15:27:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-18T15:27:28Z" id="162542">Memory Store: change default bufferSize from 1k to 100k, closed by b1e5284a069d24b5f50c92c663136dbd941bf279
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Support highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/69</link><project id="" key="" /><description>Allow to highlight search results on one or more fields. The implementation uses the lucene fast-vector-highlighter. The search request body:

```
{
    query : {...},
    highlight : {
        fields : {
            "_all" : {}
        }
    }
}
```

In the above case, the `_all` field will be highlighted for each search hit (there will be another element in each search each, called `highlight`, which includes the highlighted fields and the highlighted fragments). 

Note, in order to highlight, the field in question must be stored _and_ have termVector of `with_positions_offsets`. In the above case, the mapping should include:

```
{
    typeName : {
        allField : {store : "yes", termVector : "with_positions_offsets"}
    }
}
```

The `_all` field can be an easy to use candidate for highlighting.

By default, the highlighting will wrap highlighted text in `&lt;em&gt;` and `&lt;/em&gt;`. This can be controlled by setting `preTags` and `postTags`, for example:

```
{
    query : {...},
    highlight : {
        preTags : ["&lt;tag1&gt;", "&lt;tag2&gt;"],
        postTags : ["&lt;/tag1&gt;", "&lt;/tag2&gt;"]
        fields : {
            "_all" : {}
        }
    }
}
```

There can be a single tag or more, and the "importance" is ordered. There are also built in "tag" schemas, with currently a single schema called `styled` with preTags of:

```
&lt;em class="hlt1"&gt;, &lt;em class="hlt2"&gt;, &lt;em class="hlt2"&gt;,
&lt;em class="hlt3"&gt;, &lt;em class="hlt4"&gt;, &lt;em class="hlt5"&gt;,
&lt;em class="hlt6"&gt;, &lt;em class="hlt7"&gt;, &lt;em class="hlt8"&gt;,
&lt;em class="hlt9"&gt;
```

And post tag of `&lt;/em&gt;`. If you think of more nice to have built in tag schemas, just commend on this issue or post another issue to create it. Here is an example of switching tag schemas:

```
{
    query : {...},
    highlight : {
        tagSchema : "styled",
        fields : {
            "_all" : {}
        }
    }
}
```

Each field highlighted can control the size of the highlighted fragment in characters (defaults to 100), and the maximum number of fragments to return (defaults to 5). For example:

```
{
    query : {...},
    highlight : {
        fields : {
            "_all" : {fragmentSize : 150, numberOfFragments : 3}
        }
    }
}
```
</description><key id="153682">69</key><summary>Search API: Support highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-18T14:02:49Z</created><updated>2012-04-15T12:30:11Z</updated><resolved>2010-03-18T14:05:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-18T14:05:35Z" id="162452">Search API: Support highlighting, closed by 28b0b5fc301fcfa7aa4ee1cb922d4098a182b086.
</comment><comment author="phoet" created="2012-04-10T14:11:03Z" id="5046717">is the `_all` field a special document attribute, or some other kind of special field that one can add to a document? is there further info on this?
</comment><comment author="kimchy" created="2012-04-15T12:30:11Z" id="5138890">@phoet see http://www.elasticsearch.org/guide/reference/mapping/all-field.html, questions are best asked on the mailing list, since issues are not monitored by the whole ES community.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerexception at org.elasticsearch.action.get.GetResponse.sourceAsString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/68</link><project id="" key="" /><description>When getting a response with null body,  org.elasticsearch.action.get.GetResponse.sourceAsString throws NullPointerException, here's the relevant call stack:

 at org.elasticsearch.util.Unicode.fromBytes(Unicode.java:70)
 at org.elasticsearch.action.get.GetResponse.sourceAsString(GetResponse.java:96)
</description><key id="153612">68</key><summary>NullPointerexception at org.elasticsearch.action.get.GetResponse.sourceAsString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sbtourist</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-18T11:23:07Z</created><updated>2010-03-18T12:31:57Z</updated><resolved>2010-03-18T12:08:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-18T12:08:39Z" id="162371">I will fix the NPE, which is nasty. But you will get a null back since there is no body. You should probably check the GetResponse#exists method first in any case.
</comment><comment author="kimchy" created="2010-03-18T12:08:54Z" id="162372">NullPointerexception at org.elasticsearch.action.get.GetResponse.sourceAsString, closed by fc3a805514842cacc821c64f1c04adf612965a53
</comment><comment author="sbtourist" created="2010-03-18T12:31:57Z" id="162387">That's absolutely fine.
Thanks much.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide API to lookup documents from external sources</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/67</link><project id="" key="" /><description>When the _source field is disabled, source document is not stored, nor returned in ES queries.

It would be great to provide an API to implement for looking up the source document from external sources, probably starting from index name and document id. 
This way, the document source could be used inside ES for search features that may need it, as well as transparently returned to the client inside the ES response.

Thoughts?
</description><key id="153578">67</key><summary>Provide API to lookup documents from external sources</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sbtourist</reporter><labels><label>discuss</label></labels><created>2010-03-18T09:45:03Z</created><updated>2014-07-18T07:39:56Z</updated><resolved>2014-07-18T07:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timrobertson100" created="2010-04-05T14:38:07Z" id="178102">This was my intended approach for using Lucene with HBase and I have done preliminary tests, but outgrew single machine capabilities of Lucene (e.g. indexes &gt;300G which led me to look at ES).  On single machine indices it worked really very fine, but I never got to test it properly when HBase performed region splits (I believe you might suffer up in the search layer in timeouts when HBase).

I'd be interested in looking at the HBase integration if I can find time.  I would do a MapReduce job for building the ES, and also implement the interface.  Taking it a bit further... would you think it wise to allow ES to push down content as well?  E.g. use the ES as the webservice to the storage layer? 
</comment><comment author="deinspanjer" created="2010-05-25T13:36:34Z" id="252906">I'm evaluating integrating ES with HBase for Mozilla's Socorro project.  We are storing billions of JSON objects, and we are sizing our Hadoop cluster to give us adequate storage of this data.  If _source were not disabled, it would mean that I need to account for an additional copy of the raw JSON in addition to the unavoidable overhead of the index data that is generated.

The most typical use cases for our search is going to be returning a list of "name" fields that would be hyperlinked to display the document data and returning a faceted set of results to allow users to drill in on subsets.
In both these cases, an API to allow transparent retrieval of the document from HBase would be a welcome addition.
</comment><comment author="imotov" created="2012-02-13T20:20:09Z" id="3948433">Shay,

We really need this feature. If you have no objections, I would like to implement it. My current thinking is to make external source provider plugable on the index level. By default, such source provider will store and retrieve sources in Lucene _source field the same way it's done today. I will implement a file-system based source provider as a simple (but not practical) example. However, most users will use it with their own custom source providers (using HBase, MySql, S3, etc. to retrieve source)

An external source provider will be configured on the index level similar to the way analyzers are configured today. I envision something like this:

``` json
{
    "tweet" : {
        "_source" : {
            "enabled" : true,
            "type" : "file",
            "root_directory" : "/data",
            "source_ref" : "filepath"
        }
    }
}
```

This configuration will instantiate a file based external source provider org.elasticsearch.index.source.file.FileSourceProvider that will retrieve source from files specified in the "filepath" field and located in the "/data" directory. 

External source providers will implement two methods:
- void parseSource(ParseContext context) that will be called by SourceFieldMapper.parse()
- byte[] extractSource(Document doc) that will be called by ShardGetService.extractSource() and FetchPhase.extractSource()

Proper caching of sources, connections and other resources will be responsibility of custom source providers. 

Does it make sense?
</comment><comment author="clintongormley" created="2014-07-18T07:39:56Z" id="49403696">After much discussion, we have decided we're against adding this feature.  It is very complex, has too many corner cases, and the latency will be terrible. While this could be implemented as a plugin, we highly recommend not trying to do this.  It is much better to store your data directly in Elasticsearch and use the native functionality.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper: Ability to disable storing the "source" field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/66</link><project id="" key="" /><description>The source field is used to store the actual json that was indexed. Have the ability to disable storing it.

Disclaimer: Some features in elasticsearch might, in the future, require accessing the source field. When disabled, this features will simply not work. It is preferable to keep it on.

Disabling the source field can be done using the following mapping:

```
{
    person : {
        dateFormats : ["yyyy-MM-dd", "dd-MM-yyyy"],
        sourceField : {enabled : false}
    }
}
```

It can be disabled for all dynamic mappings created using the shared dynamic mapping definition.
</description><key id="153172">66</key><summary>Mapper: Ability to disable storing the "source" field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-17T19:29:05Z</created><updated>2010-03-17T19:29:51Z</updated><resolved>2010-03-17T19:29:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-17T19:29:51Z" id="161690">Mapper: Ability to disable storing the "source" field, closed by 3a55998a3b2a733451c647599b5b714ce25ba6e5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get API: Allow to specify which fields to load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/65</link><project id="" key="" /><description>The Get API currently returns the _source. The API should allow to supply fields that will be returned (if stored). By default, the source would still be returned if no fields are specified.
</description><key id="153092">65</key><summary>Get API: Allow to specify which fields to load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-17T18:02:36Z</created><updated>2010-03-17T18:03:40Z</updated><resolved>2010-03-17T18:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-17T18:03:40Z" id="161610">Get API: Allow to specify which fields to load, closed by 6243f4f95b7a2034fd0d30cade3b90e19f7e8bf4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shutdown API: allow to shutdown node(s) or the whole cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/64</link><project id="" key="" /><description>Add a shutdown API allowing to shutdown a specific node, several nodes, or the whole cluster. REST Api to shutdown all cluster is `/_cluster/nodes/_shutdown`, and for specific node or nodes is `/_cluster/nodes/{nodeId}/_shutdown` (`nodeId` is a comma separated node ids).

By default, the node will shutdown after a `1s` delay. It can be configured by passing `delay` parameter.

The API itself can be disabled by setting `action.admin.cluster.node.shutdown.disabled` setting to `true`.
</description><key id="152549">64</key><summary>Shutdown API: allow to shutdown node(s) or the whole cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-16T22:28:54Z</created><updated>2010-03-16T22:29:52Z</updated><resolved>2010-03-16T22:29:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-16T22:29:52Z" id="160854">Shutdown API: allow to shutdown node(s) or the whole, closed by d8ef200b4b669c344d330aab820fca3f41954011.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/63</link><project id="" key="" /><description>Support `_all` field. The idea of an `_all` field is a field that includes the text of one or more other fields within the document indexed. It can come very handy especially for search request, where we want to execute a search query against the content of a document, without knowing which fields to search on. This comes at the expense of CPU cycles and index size.

The all fields can be completely disabled. Explicit field mapping and object mapping can be excluded / included in the all field. By default, all field is enabled and all fields are included in it for ease of use.

One of the nice features of the all field is that it takes into account specific fields boost levels. Meaning that if a `title` field is boosted more than `content`, the `title` (part) in the `_all` field will mean more than the `content` part in the `all` field.

With the `_all` field, certain APIs that require fields to be set can be simplified. They include: 
- The `queryString` query `defaultField` now defaults to the `_all` field. 
- The `terms` API `fields` now default to the `_all` field and not required. 
- More Like This query fields now default to `_all` if not set.

Here is a sample mapping:

```
{
    person : {
        allField : {enabled : true},
        properties : {
            name : {
                type : "object",
                dynamic : false,
                properties : {
                    first : {type : "string", store : "yes", includeInAll : false},
                    last : {type : "string", index : "not_analyzed"}
                }
            },
            address : {
                type : "object",
                includeInAll : false,
                properties : {
                    first : {
                        properties : {
                            location : {type : "string", store : "yes", indexName : "firstLocation"}
                        }
                    },
                    last : {
                        properties : {
                            location : {type : "string"}
                        }
                    }
                }
            },
            simple1 : {type : "long", includeInAll : true},
            simple2 : {type : "long", includeInAll : false}
        }
    }
}
```

The `allField` mapping can also be associated with an analyzer (index/search), a `termVector`, and the ability to `store` it (the concatenated text).
</description><key id="152474">63</key><summary>_all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.06.0</label></labels><created>2010-03-16T21:03:08Z</created><updated>2010-03-17T11:29:58Z</updated><resolved>2010-03-16T21:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-16T21:04:30Z" id="160790">_all field, closed by 1dd59978896e1c5637e492563100d08dc99211db.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indexName is lost after server restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/62</link><project id="" key="" /><description>Hiya

My server is set up to use `gateway.type=fs`

Put a mapping as follows:

```
curl -XPUT 'http://192.168.5.100:9200/foo/bar/_mapping?ignoreConflicts=false'  -d '
{
   "properties" : {
      "single" : {
         "type" : "long"
      },
      "multis" : {
         "type" : "long",
         "indexName" : "multi"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }
```

Set a record:

```
curl -XPOST 'http://192.168.5.100:9200/foo/bar'  -d '
{
   "single" : 123,
   "multis" : 123
}
'
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "fe82aef3-8af4-4e7b-8e83-46278481c4e4",
#    "_type" : "bar"
# }
```

Check the terms:

```
curl -XGET 'http://192.168.5.100:9200/_terms?fields=single,multi,multis&amp;toInclusive=true' 
# {
#    "fields" : {
#       "single" : {
#          "terms" : [
#             {
#                "docFreq" : 1,
#                "term" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000\u0000{"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "$\b\u0000\u0000\u0000\u0000\u0000\u0000\
# &gt;                u0000\u0007"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "(@\u0000\u0000\u0000\u0000\u0000\u0000\u
# &gt;                0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : ",\u0004\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "0 \u0000\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "4\u0002\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "8\u0010\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "&lt;\u0001\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "@\b\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "D@\u0000\u0000\u0000"
#             }
#          ]
#       },
#       "multis" : {
#          "terms" : [
#             {
#                "docFreq" : 1,
#                "term" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000\u0000{"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "$\b\u0000\u0000\u0000\u0000\u0000\u0000\
# &gt;                u0000\u0007"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "(@\u0000\u0000\u0000\u0000\u0000\u0000\u
# &gt;                0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : ",\u0004\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "0 \u0000\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "4\u0002\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "8\u0010\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "&lt;\u0001\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "@\b\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "D@\u0000\u0000\u0000"
#             }
#          ]
#       },
#       "multi" : {
#          "terms" : [
#             {
#                "docFreq" : 1,
#                "term" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000\u0000{"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "$\b\u0000\u0000\u0000\u0000\u0000\u0000\
# &gt;                u0000\u0007"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "(@\u0000\u0000\u0000\u0000\u0000\u0000\u
# &gt;                0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : ",\u0004\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "0 \u0000\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "4\u0002\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "8\u0010\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "&lt;\u0001\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "@\b\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "D@\u0000\u0000\u0000"
#             }
#          ]
#       }
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "docs" : {
#       "deletedDocs" : 0,
#       "numDocs" : 1,
#       "maxDoc" : 1
#    }
# }
```

Then restart the server.

```
curl -XGET 'http://192.168.5.100:9200/_terms?fields=single,multi,multis&amp;toInclusive=true' 
# {
#    "fields" : {
#       "single" : {
#          "terms" : [
#             {
#                "docFreq" : 1,
#                "term" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000\u0000{"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "$\b\u0000\u0000\u0000\u0000\u0000\u0000\
# &gt;                u0000\u0007"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "(@\u0000\u0000\u0000\u0000\u0000\u0000\u
# &gt;                0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : ",\u0004\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "0 \u0000\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "4\u0002\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "8\u0010\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "&lt;\u0001\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "@\b\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "D@\u0000\u0000\u0000"
#             }
#          ]
#       },
#       "multis" : {
#          "terms" : [
#             {
#                "docFreq" : 1,
#                "term" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000\u0000{"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "$\b\u0000\u0000\u0000\u0000\u0000\u0000\
# &gt;                u0000\u0007"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "(@\u0000\u0000\u0000\u0000\u0000\u0000\u
# &gt;                0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : ",\u0004\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "0 \u0000\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "4\u0002\u0000\u0000\u0000\u0000\u0000\u0
# &gt;                000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "8\u0010\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "&lt;\u0001\u0000\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "@\b\u0000\u0000\u0000\u0000"
#             },
#             {
#                "docFreq" : 1,
#                "term" : "D@\u0000\u0000\u0000"
#             }
#          ]
#       },
#       "multi" : {
#          "terms" : []
#       }
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "docs" : {
#       "deletedDocs" : 0,
#       "numDocs" : 1,
#       "maxDoc" : 1
#    }
# }
```

Note that now `multi` is `[]`, and search requests no longer understand the fieldname `multi`
</description><key id="150118">62</key><summary>indexName is lost after server restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-12T20:53:13Z</created><updated>2010-03-13T10:12:41Z</updated><resolved>2010-03-13T10:12:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-13T00:43:51Z" id="157860">I hope I sorted it out (seems like its working for me). It was a race condition with the index creation and mapping definitions (my previous fix was not good enough, test still passed because of timing). Can you please test it?
</comment><comment author="clintongormley" created="2010-03-13T10:12:41Z" id="158018">Fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query parse failures not being returned as server errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/61</link><project id="" key="" /><description>Hiya

If the search query doesn't parse correctly, the server should return a server error.  Currently, it returns the errors in the response, but the status code is 200:

```
curl -XGET 'http://192.168.5.100:9200/_all/_search'  -d '
{
   "query" : {}
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 0
#    },
#    "_shards" : {
#       "failures" : [
#          {
#             "index" : "ia_object",
#             "reason" : "SearchParseException[[ia_object][3]: quer
# &gt;             y[null],from[-1],size[-1]: Parse Failure [Failed to
# &gt;  parse [{\n   \"query\" : {}\n}\n]]]; nested: QueryParsingExcep
# &gt; tion[[ia_object] No json query parser registered for [query]]; 
# &gt; ",
#             "shardId" : 3
#          },
#          {
#             "index" : "ia_object",
#             "reason" : "SearchParseException[[ia_object][1]: quer
# &gt;             y[null],from[-1],size[-1]: Parse Failure [Failed to
# &gt;  parse [{\n   \"query\" : {}\n}\n]]]; nested: QueryParsingExcep
# &gt; tion[[ia_object] No json query parser registered for [query]]; 
# &gt; ",
#             "shardId" : 1
#          },
#          {
#             "index" : "ia_object",
#             "reason" : "SearchParseException[[ia_object][2]: quer
# &gt;             y[null],from[-1],size[-1]: Parse Failure [Failed to
# &gt;  parse [{\n   \"query\" : {}\n}\n]]]; nested: QueryParsingExcep
# &gt; tion[[ia_object] No json query parser registered for [query]]; 
# &gt; ",
#             "shardId" : 2
#          },
#          {
#             "reason" : "Unknown"
#          },
#          {
#             "reason" : "Unknown"
#          }
#       ],
#       "failed" : 5,
#       "successful" : 0,
#       "total" : 5
#    }
# }
```
</description><key id="149865">61</key><summary>Query parse failures not being returned as server errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-03-12T16:23:00Z</created><updated>2010-07-17T11:24:03Z</updated><resolved>2010-07-17T11:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-07-17T11:24:03Z" id="316065">Fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper : Schema less automatic date detection wrongly detects numbers as dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/60</link><project id="" key="" /><description>When passing { id : "1" }, the id will be wrongly detected as a string since it passes date parsing.
</description><key id="149852">60</key><summary>Mapper : Schema less automatic date detection wrongly detects numbers as dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-12T16:11:22Z</created><updated>2014-03-12T19:04:25Z</updated><resolved>2010-03-12T16:16:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-12T16:16:41Z" id="157418">Mapper : Schema less automatic date detection wrongly detects numbers as dates, closed by 65ed582a90e8ba9c6721ba26e61fdbf81927c1d1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boolean field type does not handle number/string properly when searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/59</link><project id="" key="" /><description>Hiya

Here is another place where eg '1' is not being accepted as 'true':

```
curl -XPUT 'http://127.0.0.1:9200/foo/bar/_mapping?ignoreConflicts=true'  -d '
{
   "properties" : {
      "bool_val" : {
         "type" : "boolean"
      }
   }
}
'
# {
#    "ok" : true,
#    "acknowledged" : true
# }


curl -XPUT 'http://127.0.0.1:9200/foo/bar/1'  -d '
{
   "bool_val" : 0
}
'
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "1",
#    "_type" : "bar"
# }



curl -XGET 'http://127.0.0.1:9200/foo/bar/_search'  -d '
{
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "bool_val" : 1
            }
         },
         "query" : {
            "matchAll" : {}
         }
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "bool_val" : 0
#             },
#             "_index" : "foo",
#             "_id" : "1",
#             "_type" : "bar"
#          }
#       ],
#       "total" : 1
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }



curl -XGET 'http://127.0.0.1:9200/foo/bar/_search'  -d '
{
   "query" : {
      "filteredQuery" : {
         "filter" : {
            "term" : {
               "bool_val" : true
            }
         },
         "query" : {
            "matchAll" : {}
         }
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 0
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }
```
</description><key id="149745">59</key><summary>Boolean field type does not handle number/string properly when searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-12T13:56:08Z</created><updated>2013-12-09T20:35:19Z</updated><resolved>2010-03-12T14:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-12T14:53:16Z" id="157302">Boolean field type does not handle number/string properly when searching, closed by 47c11aa5382601586e00ab7fdfb612f34fb01b95.
</comment><comment author="clintongormley" created="2010-03-12T15:38:42Z" id="157367">fix confirmed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Expose source as Map (in GetResponse, SearchHit), allow to index a Map</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/58</link><project id="" key="" /><description>Provide support for Map both when indexing (using the IndexRequest) and when getting source back (GetResponse, SearchHit).
</description><key id="149076">58</key><summary>Java API: Expose source as Map (in GetResponse, SearchHit), allow to index a Map</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-11T14:38:34Z</created><updated>2010-03-11T14:39:39Z</updated><resolved>2010-03-11T14:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-11T14:39:39Z" id="156380">Java API: Expose source as Map (in GetResponse, SearchHit), allow to index a Map, closed by 86c3a406c69e59b9db6ffe638ea539bdefd741fa.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping metadata not restored from gateway on server restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/57</link><project id="" key="" /><description>Hiya
- start a server with a fs gateway
- put  a mapping
- index some docs
- check cluster state -&gt; can see mapping metadata
- optionally snapshot the gateway
- close down the server
- restart the server
- check cluster state -&gt; no mapping metadata
- indexed docs still exist, and the mapping still applies

I have my config set up as follows:

```
cluster:
    name:               iAnnounce

network:
    bindHost:           192.168.5.100
    publishHost:        192.168.5.100

node:
    data:               true

http:
    enabled:            true


gateway:
    type:               fs
    fs:             
        location:       /opt/elasticsearch/data

index:
    store:
        type:           memory
        memory:
            cacheSize:  20m
            bufferSize: 10k
```

thanks

Clint
</description><key id="149060">57</key><summary>Mapping metadata not restored from gateway on server restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-11T14:15:07Z</created><updated>2010-03-13T10:12:48Z</updated><resolved>2010-03-12T16:22:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-03-11T14:17:07Z" id="156362">Oh, and the mapping metadata is successfully saved to disk in `/opt/elasticsearch/data/iAnnounce/metadata-8`
</comment><comment author="kimchy" created="2010-03-12T16:22:11Z" id="157423">A fix is making its way through the github tubes
</comment><comment author="kimchy" created="2010-03-12T16:22:25Z" id="157424">Mapping metadata not restored from gateway on server restart, closed by 4e1a1f3437df2c139fea3698ef9b1c523ec54e59
</comment><comment author="clintongormley" created="2010-03-12T16:28:18Z" id="157432">This doesn't fix it I'm afraid.  I get:

```
[17:26:00,279][INFO ][cluster.service          ] [Moonstone] New Master [Moonstone][getafix-18870][data][inet[/192.168.5.100:9300]]
[17:26:00,461][INFO ][discovery                ] [Moonstone] iAnnounce/getafix-18870
[17:26:00,496][INFO ][cluster.metadata         ] [Moonstone] Creating Index [ia_object], shards [5]/[1]
[17:26:00,645][INFO ][http                     ] [Moonstone] boundAddress [inet[/192.168.5.100:9200]], publishAddress [inet[/192.168.5.100:9200]]
[17:26:00,976][ERROR][gateway                  ] [Moonstone] Failed to put mapping [3] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,979][ERROR][gateway                  ] [Moonstone] Failed to put mapping [2] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,979][ERROR][gateway                  ] [Moonstone] Failed to put mapping [10] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,980][ERROR][gateway                  ] [Moonstone] Failed to put mapping [1] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,980][ERROR][gateway                  ] [Moonstone] Failed to put mapping [8] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,980][ERROR][gateway                  ] [Moonstone] Failed to put mapping [13] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,980][ERROR][gateway                  ] [Moonstone] Failed to put mapping [11] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:00,981][ERROR][gateway                  ] [Moonstone] Failed to put mapping [12] for index [ia_object]
org.elasticsearch.indices.IndexMissingException: [ia_object] missing
    at org.elasticsearch.cluster.metadata.MetaDataService.putMapping(MetaDataService.java:268)
    at org.elasticsearch.gateway.GatewayService$2$1$1.run(GatewayService.java:172)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
[17:26:01,070][INFO ][jmx                      ] [Moonstone] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://192.168.5.100:9400/jmxrmi]
[17:26:01,070][INFO ][server                   ] [Moonstone] {ElasticSearch/0.6.0-SNAPSHOT/2010-03-12T16:23:36}: Started
```
</comment><comment author="clintongormley" created="2010-03-12T16:30:42Z" id="157438">... and the metadata is still missing from cluster_state.

My config file is as follows:

```
cluster:
    name:               iAnnounce

network:
    bindHost:           192.168.5.100
    publishHost:        192.168.5.100

node:
    data:               true

http:
    enabled:            true


gateway:
    type:               fs
    fs:             
        location:       /opt/elasticsearch/data

index:
    store:
        type:           memory
        memory:
            cacheSize:  20m
            bufferSize: 10k
```
</comment><comment author="kimchy" created="2010-03-13T00:46:08Z" id="157861">Can you try again? Hope I fixed it now (it relates to the other issue).
</comment><comment author="clintongormley" created="2010-03-13T10:12:48Z" id="158019">Fixed ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Another numbers-as-strings error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/56</link><project id="" key="" /><description>Hiya

```
curl -XGET 'http://127.0.0.1:9200/_all/_search'  -d '
{
   "fields" : [],
   "query" : {
      "field" : {
         "text" : {
            "boost" : "1",
            "query" : "foo"
         }
      }
   }
}
'
```

Returns:

```
SearchParseException[[ia_object][4]: query[null],from[-1],size[-1]: Parse Failure [Failed to parse [{
   \"fields\" : [],
   \"query\" : {
      \"field\" : {
         \"text\" : {
            \"boost\" : \"1\",
            \"query\" : \"foo\"
         }
      }
   }
}
]]]; nested: QueryParsingException[[ia_object] Failed to parse]; nested: JsonParseException[Current token (VALUE_STRING) not numeric, can not use numeric value accessors
 at [Source: [B\@3256e684; line: 6, column: 24]]; 
```
</description><key id="149035">56</key><summary>Another numbers-as-strings error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>v0.06.0</label></labels><created>2010-03-11T13:40:34Z</created><updated>2010-03-12T15:37:02Z</updated><resolved>2010-03-12T14:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-12T14:37:51Z" id="157287">Hey, fixed this for all queries / filters json.
</comment><comment author="clintongormley" created="2010-03-12T15:37:02Z" id="157366">fix confirmed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A search with an empty fields param causes a NullPointerException or a runaway process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/55</link><project id="" key="" /><description>Hiya

An empty fields[] array either returns a NullPointerException, or causes a runaway process.  Try this:

```
curl -XGET 'http://192.168.5.100:9200/_all/_search'  -d '
{
   "fields" : [],
   "query" : {
      "matchAll" : {}
   }
}
'
```

thanks

Clint
</description><key id="148321">55</key><summary>A search with an empty fields param causes a NullPointerException or a runaway process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.06.0</label></labels><created>2010-03-10T14:56:49Z</created><updated>2010-03-10T17:12:54Z</updated><resolved>2010-03-10T15:49:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-10T15:49:52Z" id="155399">A search with an empty fields param causes a NullPointerException or a runaway process. Changed logic for an empty fields array, where it won't return the source in this case. Closed by 4b04db903075a84df34c1b847951a58bff058b6d.
</comment><comment author="clintongormley" created="2010-03-10T17:12:54Z" id="155467">fixed++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support parsing Strings with numeric types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/54</link><project id="" key="" /><description>Hiya

In dynamic languages like Perl, numbers and strings can be interchangeable, so 5 and "5" are equivalent.

As a result, it is difficult for JSON modules to output the correct type - it often guesses wrong.

Is there any chance that you could add implicit typing, so that if you're expecting a number and get a string instead, but the string looks like a number, then it's ok?

eg:

```
curl -XPUT 'http://192.168.5.100:9200/ia_object/1/_mapping?ignoreConflicts=false'  -d '
{
   "properties" : {
      "id" : {
         "type" : "long"
      }
   }
}
'

curl -XPUT 'http://192.168.5.100:9200/ia_object/1/1'  -d '
{
   "id" : "1"
}
'
{
  "error" : "ReplicationShardOperationFailedException[[ia_object][0] ]; nested: MapperParsingException[Failed to parse]; nested: JsonParseException[Current token (VALUE_STRING) not numeric, can not use numeric value accessors\n at [Source: [B@2ac02d83; line: 7, column: 12]]; "
}
```

thanks

Clint
</description><key id="147761">54</key><summary>Support parsing Strings with numeric types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-09T20:22:59Z</created><updated>2010-03-10T10:52:50Z</updated><resolved>2010-03-09T22:40:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-09T22:40:16Z" id="154795">Support parsing Strings with numeric types, closed by ae5bcb6d2ed06ec13430594c5f62ed84b2f22ed5
</comment><comment author="kimchy" created="2010-03-09T22:40:51Z" id="154797">Fixed, float/double/long/integer/boost types support strings as well as numbers.
</comment><comment author="clintongormley" created="2010-03-10T10:52:50Z" id="155204">Hiya - confirming that this is fixed - many thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping - Support specifying string as number values (for example, for boost)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/53</link><project id="" key="" /><description>Hiya

Try this: 

```
curl -XPUT 'http://127.0.0.1:9200/foo/bar/_mapping?ignoreConflicts=false'  -d '
{
   "properties" : {
      "name" : {
         "boost" : "2.0",
         "type" : "string"
      }
   }
}
'
```

Results in:
    {
      "error" : "NullPointerException[null]"
    }
</description><key id="147720">53</key><summary>Mapping - Support specifying string as number values (for example, for boost)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.06.0</label></labels><created>2010-03-09T19:49:45Z</created><updated>2010-03-10T09:23:01Z</updated><resolved>2010-03-10T09:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-10T09:07:24Z" id="155123">Bad exception... . But basically, it expects the boost to be a float, and not a string. If you change that, it will work. Let me do the same fix as the number types to support passing string values for numbers here as well...
</comment><comment author="kimchy" created="2010-03-10T09:23:01Z" id="155128">Mapping - Support specifying string as number values (for example, for boost), closed by 7c684897582d40e236e50352c74ca9a03b0ecfd6.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_count requests broken as a of commit 7bf0f1ffca589df6e626d61182689bde005ce649 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/52</link><project id="" key="" /><description>The server now hangs with `_count` queries. A bisect shows:

7bf0f1ffca589df6e626d61182689bde005ce649 is the first bad commit
commit 7bf0f1ffca589df6e626d61182689bde005ce649
Author: kimchy kimchy@gmail.com
Date:   Fri Mar 5 01:39:04 2010 +0200

```
refactor client api, remove execXXX, and simple remain with the actual operation name as the method name, one that returns a future, and one that accepts a listener
```

:040000 040000 941417e6df82b84dbd3ac28576aa589916d398ea c2c27160d53974c1c78a3b9108ce8f3ca16c665e M  modules

Test script:

```
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2'  -d '
{
   "num" : 3,
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3'  -d '
{
   "num" : 4,
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4'  -d '
{
   "num" : 5,
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5'  -d '
{
   "num" : 6,
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6'  -d '
{
   "num" : 7,
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7'  -d '
{
   "num" : 8,
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8'  -d '
{
   "num" : 9,
   "text" : "foo bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9'  -d '
{
   "num" : 10,
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10'  -d '
{
   "num" : 11,
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11'  -d '
{
   "num" : 12,
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12'  -d '
{
   "num" : 13,
   "text" : "foo bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13'  -d '
{
   "num" : 14,
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14'  -d '
{
   "num" : 15,
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15'  -d '
{
   "num" : 16,
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16'  -d '
{
   "num" : 17,
   "text" : "bar baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17'  -d '
{
   "num" : 18,
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18'  -d '
{
   "num" : 19,
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19'  -d '
{
   "num" : 20,
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20'  -d '
{
   "num" : 21,
   "text" : "baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21'  -d '
{
   "num" : 22,
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22'  -d '
{
   "num" : 23,
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23'  -d '
{
   "num" : 24,
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24'  -d '
{
   "num" : 25,
   "text" : "bar"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25'  -d '
{
   "num" : 26,
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26'  -d '
{
   "num" : 27,
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27'  -d '
{
   "num" : 28,
   "text" : "foo baz"
}
'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28'  -d '
{
   "num" : 29,
   "text" : "foo baz"
}
'
curl -XGET 'http://127.0.0.2:9200/_cluster/health?timeout=2&amp;waitForStatus=green' 
curl -XGET 'http://127.0.0.2:9200/_all/_count'  -d '
{
   "term" : {
      "text" : "foo"
   }
}
'
```
</description><key id="146914">52</key><summary>_count requests broken as a of commit 7bf0f1ffca589df6e626d61182689bde005ce649 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.1</label></labels><created>2010-03-08T19:12:00Z</created><updated>2010-03-09T11:37:54Z</updated><resolved>2010-03-09T11:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-09T11:25:24Z" id="154218">_count hangs, closed by 83a78b39798700269c19eb9a410017ba163a73cc
</comment><comment author="kimchy" created="2010-03-09T11:28:31Z" id="154221">Fixed, my bad!. By the way, int the health API, you specify a timeout of 2, which is 2 milliseconds..., you should set it to something like `2s` (so its 2 seconds).
</comment><comment author="kimchy" created="2010-03-09T11:37:42Z" id="154228">Releasing 0.5.1 to fix this...
</comment><comment author="clintongormley" created="2010-03-09T11:37:54Z" id="154229">Fix confirmed. And thanks for the note about the timeout
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping - MultiField Mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/51</link><project id="" key="" /><description>Add another type `multi_field` for mapping to support mapping more than one core type. This is handy, for example, to map the same string type, once `analyzed` and one `not_analyzed`. For example:

```
{
    person : {
        properties : {
            "name" : {
                type : "multi_field",
                "fields" : {
                    "name" : {type: "string", index : "analyzed", store : "yes"},
                    "not_analyzed" : {type: "string", index : "not_analyzed"},
                }
            }
        }
    }
}
```

The above example, the `name` mapping is a multi field mapping. The `name` within the `multi_field` mapping is the "default" mapping (and used as if it was mapped regularly). The `not_analyzed` name is accessed through `name.not_analyzed`, and it won't be analyzed.

A special case is when merging, upgrading core type mapping into multi field mapping if the new mapping set for this property is now multi mapping.
</description><key id="146301">51</key><summary>Mapping - MultiField Mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.1</label></labels><created>2010-03-07T18:10:57Z</created><updated>2010-03-07T18:14:09Z</updated><resolved>2010-03-07T18:14:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-07T18:14:09Z" id="152803">Mapping - MultiField Mapping, closed by b13f6b1bddb29f47f01a660b7f6e369cc8251e9d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Query Facet - Add global flag to control if the facet is bounded to the search query or not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/50</link><project id="" key="" /><description>Sometimes, within the same search request, some facets should be global and not bounded by the search query. Here is how this should be done:

```
facets : {
    facet1: {
        query : { ... },
        global : true
    }
}
```
</description><key id="145294">50</key><summary>Search API: Query Facet - Add global flag to control if the facet is bounded to the search query or not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-03-05T14:01:37Z</created><updated>2011-12-14T22:40:51Z</updated><resolved>2010-03-05T14:05:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-05T14:05:16Z" id="151473">Search API: Query Facet - Add global flag to control if the facet is bounded to the search query or not, closed by f4f26d2118b1bc6f0d895ba990ede22b02e731b4.
</comment><comment author="allochi" created="2011-12-14T21:51:41Z" id="3151198">Hi,

Well, tried everything, in ES 0.18.5, global doesn't do anything

{
  "query": {
    "query_string": {
      "query": "T*"
    }
  },
  "facets": {
    "tags": {
      "terms": {
        "field": "tags",
        "global": true
      }
    }
  }
}

is the same with global or without, the only way I get facet to work globally is to remove the query. If I want to have a facet to further filter my search it would be fine, but what if I want to compare? like I want 2 facets, one for the current search and one global on all tags, so that I can say that there is 6/32, could you please help or redirect me? I tried docs and forumes and I can't find what's wrong.

Thanks fo ES, it's great!
</comment><comment author="kimchy" created="2011-12-14T21:59:50Z" id="3151430">@allochi move the global another level up (under the facet name)
</comment><comment author="allochi" created="2011-12-14T22:40:51Z" id="3153180">Thanks a million!!! And sorry, I should have noticed, I'm still learning ES and my eyes are geting crossed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JGroups: Upgrade to 2.10 (check sys proper preferIpX)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/49</link><project id="" key="" /><description>When upgrading to 2.10, check if we can remove the system property of preferIpv4. Consider simulating the logic for choosing first non loopback ip in es (for transport, http, and jgroups bind_addr)
</description><key id="143082">49</key><summary>JGroups: Upgrade to 2.10 (check sys proper preferIpX)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2010-03-02T13:56:23Z</created><updated>2010-06-29T19:48:49Z</updated><resolved>2010-06-29T19:48:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-06-29T19:48:49Z" id="292624">not going to be added since the jgroups plugin was removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: queryString - allow to run against multiple fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/48</link><project id="" key="" /><description>The current queryString requires to define the defaultField in order to run the query against a default field. Running the same query string against several "default" fields can be done by combining them either with bool or disMax queries, but its cumbersome.

queryString should support providing a "fields" field, where a list of fields the query will run against will be provided. An option to choose if the query will be combined using disMax or bool should be a simple flag. A tieBreaker field allowed when using disMax.

Sample 1:

```
{
    queryString : {
        fields : ["content", "name"],
        useDisMax : false,
        query: "test"
    }
}
```

Sample 2:

```
{
    queryString : {
        fields : ["content", "name"],
        useDisMax : true,
        query: "test"
    }
}
```

Boosting per field should be allowed, for example:

```
{
    queryString : {
        fields : ["content^1.4", "name"],
        query: "test"
    }
}
```
</description><key id="142736">48</key><summary>Query DSL: queryString - allow to run against multiple fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-03-01T22:25:44Z</created><updated>2010-03-01T22:43:01Z</updated><resolved>2010-03-01T22:43:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-01T22:43:01Z" id="148110">Query DSL: queryString - allow to run against multiple fields, closed by fdd221e8ea303d569b0d6ac77bcb283dd4c1f294.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Field Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/47</link><project id="" key="" /><description>A new query, field query, which is very similar to the queryString query, except it works on a single field. It has the same parameters as the queryString (except for the defaultField). The idea is to make querying against a single field using the query string sytax (fuzzy, boolean, phrase, and so on).

Sample 1:

```
{
    field : { age : 34 }
}
```

Sample 2:

```
{
    field : { "name.first" : "+shay -kimchy" }
}
```

Sample 3:    

```
{
    field : { 
        "name.first" : { 
            query : "+shay -kimchy", 
            boost : 2.0, 
            enablePositionIncrements : false 
        } 
    }
}
```
</description><key id="142524">47</key><summary>Query DSL: Field Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-03-01T18:00:58Z</created><updated>2010-03-01T18:10:48Z</updated><resolved>2010-03-01T18:10:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-01T18:10:48Z" id="147871">Query DSL: Field Query, closed by 4dbc1679668bbe0482d55a4dd5cbea705f0a22be.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JSON Implementation is not standard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/46</link><project id="" key="" /><description>Among other problems, I have noticed the following:
1. Floating points in quotes, such as "1.0" causes null pointer exceptions when sent as document fields. These should just be treated as a regular string. Here is the error message:
       "error" : "Index[...] Shard[1] ; nested: Failed to parse; nested: Current token (VALUE_STRING) not numeric, can not use numeric value accessors\n at [Source: {..., \"f\": \"1.0\",...}; line: 1, column: 32]; "
2. Integers in status response are placed in strings. This is more an annoying detail, than a bug.

I tested this with json generated in the python standard standard package, which is confirmed to be compatible with in-browser json and php json implementations. From the tracebacks I have seen, the problem is either in the JsonObectMapper or possibly the flexjson package, but I haven't looked into it in detail.

Let me know if you need some more examples.

Thanks for the great project!
</description><key id="141925">46</key><summary>JSON Implementation is not standard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stevvooe</reporter><labels /><created>2010-03-01T03:14:14Z</created><updated>2010-03-09T22:42:03Z</updated><resolved>2010-03-09T19:05:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-01T09:10:10Z" id="147448">Hi,
1. Can you paste an example of an API where you use it? It might be API (or rather, parsing of an API) specific.
2. Which status response? Can you post an example?
</comment><comment author="stevvooe" created="2010-03-01T21:35:15Z" id="148064">1.  I haven't been able to reproduce this consistently. I use a python script
   like this (although I have seen it with curl):
       import json
       import httplib2
   
   ```
   c = httplib2.Http()
   
   url = "http://localhost:9200/twitter/tweet/123413241234"
   body = json.dumps({"an_id": 123413241234, "f": "1.0", "stuff": 'a' * 100})
   headers = {'Content-Type': 'application/json; charset=UTF-8'}
   
   response, content = c.request(url, 'PUT', body=body, headers=headers)
   print content
   if response.status != 200:
       print "response:", response
       print "content:", content
       print "BODY:", repr(body)
       print "==================="
   ```
   
   It seems like the problem might lie in changing a field's type, but it
   doesn't happen until I have done a large number of updates. Currently, I am
   running a test to see if this is true. I'll get back to you on this.
2.  I made a mistake here. I was printing a httplib2 library response dictionary.
   It seems to have not used integers, my apologies.

Sorry about the shoddy bug report; I wrote this when I was way too tired.
Usually I am much more thorough.
</comment><comment author="kimchy" created="2010-03-01T22:18:11Z" id="148089">Fist of all, I prefer "shoddy" bug reports than no bug reports :). They usually uncover something that can be simplified is possible. Regarding your first point, which version are you running against, is is vanilla 0.4 or master?
</comment><comment author="stevvooe" created="2010-03-01T22:59:21Z" id="148135">I have high standards for my bug reports ;). But I do suspect something odd is
going on here.

I am running against 0.4.0. I'll look into upgrading to the master. Do you have
the link to a build process?

These are probably unrelated, but here are various snippets of information that
might help:
1.  I can only get this issue after having the server run for a very long time. I
   don't know if this is based on time or number of nature of operations. I
   suspect the latter.
2.  At times, I can only reproduce the issue with python's httplib client and not
   using curl, although, I have seen it with curl. A tcpdump of packets shows that
   the python requests are valid and going out as suspected.
3.  Python sends the body and headers of a put request in different packets,
   whereas curl send them together. This is the default behaviour of the
   python standard httplib module, so urllib, urllib2 and httplib2 will all
   behave in the same way. I thought this might have been part of the problem,
   but I no longer see the exception after a full restart.
4.  I am using Ubuntu 9.10 with the interpreter under sun-java6-jdk. The
   executeable is being run in console mode (-f). The first time I saw the
   problem was with logging level debug, although, I think that I have seen in
   it with logging set to info.
5.  I have increased the socket buffers with the following command:
       sudo sysctl net.core.rmem_max=$((26_1024_1024)) net.core.wmem_max=$((25_1024_1024))
</comment><comment author="kimchy" created="2010-03-01T23:34:36Z" id="148172">If you can, it would be great if you can download master and check. The download section has a link to master, and instructions on how to build it. Some changes done in master include much improved support for mappings, support for chunked http requests (though, for performance reasons, you should probably do without).

(Vizzini said) go back to the beginning. Do you define explicit mapping with type string, and then try and index a float number to it? or maybe rely on the first indexable document to set the field to type string, and then get the float parsing exception?
</comment><comment author="stevvooe" created="2010-03-01T23:50:28Z" id="148187">Will do on the master version.

No mappings were defined. Its possible that the field type changed (ie I indexed it one way then changed), but I do not get a float parse exception.
</comment><comment author="stevvooe" created="2010-03-09T19:05:50Z" id="154568">So, I took a little time to test the master and I definitely don't see this issue. We'll go ahead and close this and I will keep an eye out.

Thanks for the time and kepp up the good work.
</comment><comment author="kimchy" created="2010-03-09T22:42:03Z" id="154798">cool!, thanks for the effort.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MoreLikeThis API: Search documents that are "like" the specified document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/45</link><project id="" key="" /><description>URI is: {index}/{type}/{id}/_moreLikeThis
Method: GET and POST

It translates into a search request with an mlt query (using the mlt query DSL). All mlt parameters are extracted from the http parameters.

The body of the request can optionally include the typical search request body (facets, from, size, ...).
</description><key id="140911">45</key><summary>MoreLikeThis API: Search documents that are "like" the specified document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-27T01:20:38Z</created><updated>2010-02-27T01:57:42Z</updated><resolved>2010-02-27T01:57:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-27T01:57:42Z" id="146049">MoreLikeThis API: Search documents that are "like" the specified document, closed by 8b36281d60350727ac2475f54b32769a84ed863b.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query: support negative queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/44</link><project id="" key="" /><description>Allow negative queries such as all the documents that do not match something. This can be expressed by a boolean query with a mustNot clause or in the query string.
</description><key id="140485">44</key><summary>Query: support negative queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-26T12:43:42Z</created><updated>2012-03-26T16:03:22Z</updated><resolved>2010-02-26T12:44:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-26T12:44:22Z" id="145502">Query: support negative queries, closed by e0c3bb588360cba6f98e39c7e0d0790e1673b22f.
</comment><comment author="folke" created="2012-03-26T16:03:22Z" id="4697336">The current support for negative queries is only working for very simple use cases, but not for all possible queries containing possible negative subqueries.

As an example, see:
- `-foo`: this will work as expected
- `-foo OR bar`: will yield 0 results

Currently we preprocess the query before sending it to ES, by changing it to something like `(*:* -foo) OR bar`

Apart from the examples above, `*:* AND (-foo OR bar)` is also not yielding any results.

I would reopen this issue, but since I do not have permission :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: moreLikeThis &amp; moreLikeThisField</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/43</link><project id="" key="" /><description>Add support for moreLikeThis (MLT) queries. More like this allows to find documents that "look" like the provided text when matches against one or more field.

The moreLikeThis query looks as follows:

```
{
    moreLikeThis : {
        fields : ["name.first", "name.last"],
        likeText : "something",
        minTermFrequency : 1,
        maxQueryTerms : 12
    }
}
```

The moreLikeThisField looks as follows:

```
{
    moreLikeThisField : {
        "name.first" : {
            likeText : "something",
            minTermFrequency : 1,
            maxQueryTerms : 12
        }
    }
}
```

There are much more parameters that are supported, will be documented in the ref docs for the next release. They basically follow the MLT support in Lucene.

The difference between moreLikeThisField and moreLikeThis is that in moreLikeThisField only a single field can be provided, and it will supported "typed" fields (will automatically filter based on the type).
</description><key id="140254">43</key><summary>Query DSL: moreLikeThis &amp; moreLikeThisField</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-26T01:28:02Z</created><updated>2010-02-26T01:28:49Z</updated><resolved>2010-02-26T01:28:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-26T01:28:49Z" id="145171">Query DSL: moreLikeThis &amp; moreLikeThisField, closed by 0ebb74dd9e11e416ede16a84e397cb3c80d5a3d1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Health API: A simple way to know if the cluster is healthy or not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/42</link><project id="" key="" /><description>Using a simple API, without a lot of information (which is provided both with _clsuter/state and _indices/status) provide a simple mechanism to know if the cluster is healthy or not. Also, the api should allow to "wait" till the cluster reaches a healthy state.
</description><key id="139330">42</key><summary>Cluster Health API: A simple way to know if the cluster is healthy or not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-24T21:14:51Z</created><updated>2010-03-04T14:20:27Z</updated><resolved>2010-02-25T18:13:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-25T18:13:40Z" id="144851">API implemented. Can be accessed through `/_cluster/health` or `/_cluster/health/{index}` (index can be a list indices as well, or _all). Parameters include:
- _level_: can be one of `cluster`, `indices` or `shards`, controls the details level of the health information returned. Defaults to `cluster`.
- _waitForStatus_: one of `green`, `yellow`, or `red`. Will wait (until the timeout provided) until the status of the cluster changes to the one provided. By default, will not wait for any status.
- _waitForRelocatingShards_: A number controlling to how many relocating shards to wait. Usually will be 0 to indicate to wait till all relocation have happened. Defaults to not wait.
- _timeout_: A time based parameter controlling how long to wait if one of the waitForXXX are provided. Defaults to `30s`.

The status can be one of green, yellow or red. On the shard level, a red status indicates that the shard nothing allocated in the cluster, yellow means that the primary shard is allocated, and green means that all shards are allocated. The index level status is the controlled by the worst shard status. The cluster status is controlled by the worst index status.
</comment><comment author="clintongormley" created="2010-03-02T17:28:05Z" id="148754">Ahh, the URL is `/_cluster/health` or `/_cluster/health/$indices`
</comment><comment author="kimchy" created="2010-03-02T18:09:27Z" id="148788">Yes..., markdown formatting... . Fixed.
</comment><comment author="clintongormley" created="2010-03-02T19:41:28Z" id="148889">Hmm, in running my test suite, I add a `flush_index(); sleep 1` every now and again, to allow the cluster to catch up.

I've tried replacing it with a `waitForStatus: "green"`, and i get various errors. It seems that waiting for green isn't sufficient to ensure that actions like deleting or creating indices have settled down sufficiently to allow another similar action.
</comment><comment author="kimchy" created="2010-03-02T20:01:03Z" id="148912">Can you paste a sample script that recreates this?
</comment><comment author="clintongormley" created="2010-03-04T12:58:21Z" id="150391">Start three servers and then run this script

```
#!/bin/bash
curl -XPUT 'http://127.0.0.2:9200/es_test_1/test/1?pretty=true'  -d '
{
   "num" : "foo",
   "text" : "123"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "1",
#    "_type" : "test"
# }


curl -XGET 'http://127.0.0.2:9200/_cluster/health?waitForStatus=green&amp;pretty=true' 
# {
#    "activeShards" : 20,
#    "activePrimaryShards" : 10,
#    "relocatingShards" : 0,
#    "status" : "green",
#    "timedOut" : false
# }


curl -XGET 'http://127.0.0.2:9200/es_test_1/test/1?pretty=true' 
# {
#    "error" : "NullPointerException[null]"
# }
```

The second time you run it, it works correctly, presumably because the cluster has had time to create the index properly
</comment><comment author="clintongormley" created="2010-03-04T13:00:44Z" id="150392">Also, it'd be nice if `waitForStatus='yellow'` would return if the status is `green`.  ie wait-until-status-is-at-least
</comment><comment author="kimchy" created="2010-03-04T13:09:46Z" id="150396">Fixed the waitForStatus to wait for at least the given status. Will check on the script soon.
</comment><comment author="kimchy" created="2010-03-04T13:36:03Z" id="150414">pushed a fix for the null pointer exception, it works well now. Note, because of the near real time search, you will get a NOT FOUND http response when you run it, it will get after ~1 second.
</comment><comment author="clintongormley" created="2010-03-04T14:20:27Z" id="150439">Both of those fixes work ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: queryString - allow to escape the string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/41</link><project id="" key="" /><description>Add an option to the queryString DSL to escape the query string, which should be turned _off_ by default.
</description><key id="138990">41</key><summary>Query DSL: queryString - allow to escape the string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-24T12:27:03Z</created><updated>2010-03-02T18:18:02Z</updated><resolved>2010-02-24T20:00:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-24T20:00:07Z" id="143994">Query DSL: queryString - allow to escape the string (should be on by default), closed by c7389df8e049724b25d30910ffd71510ae792666.
</comment><comment author="clintongormley" created="2010-03-02T17:16:49Z" id="148742">Could you give an example of how and why you would use escape?

ta
</comment><comment author="kimchy" created="2010-03-02T18:07:15Z" id="148786">actually, the only reason is when knowing you are not going to use any reserve query parser keywords. Its not that common to enable escaping, I just added it as an option.
</comment><comment author="clintongormley" created="2010-03-02T18:14:05Z" id="148796">and it is used as `{...., escape: true }` ?
</comment><comment author="kimchy" created="2010-03-02T18:18:02Z" id="148802">Yep. or `escape : 1` :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception in clusters with embedded and standalone nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/40</link><project id="" key="" /><description>When I run a cluster made up of embedded and standalone nodes, former ones seems not to be able to connect to latter ones, and I get the following log messages on embedded nodes:

OOB-1,elasticsearch,caffeine.local-16363 -
jgroups.pbcast.NAKACK - caffeine.local-16363: dropped message from
caffeine.local-17690 (not in xmit_table), keys are
[caffeine.local-16363], view=[caffeine.local-16363|0]
[caffeine.local-16363]

The problem seems to be related to JGroups which, on IPv6-enabled machines, prefers IPv6 addresses over IPv4 ones, but embedded instances aren't able to guess a proper IPv6 address.

That can be fixed by:
Specifying java.net.preferIPv4Stack=true on the embedded node, forcing the use of IPv4.
_or_
Specifying java.net.preferIPv4Stack=false and java.net.preferIPv6Stack=false on the standalone node, apparently forcing JGroups to take the proper default.
</description><key id="138470">40</key><summary>Exception in clusters with embedded and standalone nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sbtourist</reporter><labels><label>bug</label></labels><created>2010-02-23T18:01:12Z</created><updated>2010-08-18T21:51:42Z</updated><resolved>2010-08-18T21:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-23T20:37:26Z" id="142978">The elasticsearch script currently set the preferIPv4Stack to true because of problems I had with using the default in Java (which defaults to the machine). This menas that you need to either change the flag on the script, or set the flag on the embedded client side.

I need to revisit this decision to set the ipv4 flag to true and use the JVM default, but it needs to work well out of the box, without needing to set it explicitly. See this thread for why I default to ipv4: http://old.nabble.com/Suspect-warning-in-jgroups-2.8.0-CR6-to26560364.html#a26560364
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creating a duplicate mapping throws the whole cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/39</link><project id="" key="" /><description>Hiya

I'm starting 3 nodes, then creating a mapping, then creating a duplicate (with ignoreDuplicate=false).

It is sufficient to throw the whole cluster out.  It doesn't seem to recover.

Run this script a few times, and watch the server logs:

```
#!/bin/bash
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 
curl -XDELETE 'http://127.0.0.2:9202/es_test_1/' 
curl -XPUT 'http://127.0.0.2:9202/es_test_1,es_test_2/test/_mapping?ignoreDuplicates=false'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9202/es_test_1,es_test_2/test/_mapping?ignoreDuplicates=false'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9202/es_test_1,es_test_2/test_2/_mapping?ignoreDuplicates=false'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
curl -XDELETE 'http://127.0.0.2:9202/es_test_1/' 
curl -XDELETE 'http://127.0.0.2:9202/es_test_2/' 
curl -XPUT 'http://127.0.0.2:9202/es_test_1/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9202/es_test_2/'  -d '
{}
'
curl -XPUT 'http://127.0.0.2:9202/_all/type_1/_mapping?ignoreDuplicates=false'  -d '
{
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPUT 'http://127.0.0.2:9202/_all/type_2/_mapping?ignoreDuplicates=false'  -d '
{
   "properties" : {
      "num" : {
         "store" : "yes",
         "type" : "integer"
      },
      "text" : {
         "store" : "yes",
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9202/_flush?refresh=true' 
```
</description><key id="137972">39</key><summary>Creating a duplicate mapping throws the whole cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-22T21:20:51Z</created><updated>2010-03-02T17:33:04Z</updated><resolved>2010-03-02T17:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-23T22:10:50Z" id="143079">Fixed one issue, where the proper exception was not raised (actually serialized) since you have ignoreDuplicates to false. Trying to recreate the problem you posted.
</comment><comment author="kimchy" created="2010-02-28T09:16:55Z" id="146744">Hopefully, its fixed now. Since the allocation of shards is async, the create/delete/create/delete cycle caused the delete api to hand in the ack phase. Can you give it a go? (note, I added a cluster health API where you can wait for the cluster to get to a certain state)
</comment><comment author="clintongormley" created="2010-03-02T17:33:03Z" id="148760">This looks like it has been fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms results differs between one node and multiple</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/38</link><project id="" key="" /><description>hiya

When I run 'terms' queries against multiple nodes, I get incorrect results with these shard failures:

```
  "reason" : "BroadcastShardOperationFailedException[[es_test_2][2] ]; nested: RemoteTransportException[[Thumb, Tom][inet[/127.0.0.2:9302]][indices/terms/shard]]; nested: ArrayIndexOutOfBoundsException[23]; "
```

Start one server, then run this script.  It pauses to allow you to stop the server, then to start 3 nodes, then it shows the diff between the two runs:

```
#!/bin/bash
curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping?ignoreDuplicates=false'  -d '
{"properties":{"num":{"store":"yes","type":"integer"},"text":{"store":"yes","type":"string"}}}'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping?ignoreDuplicates=false'  -d '
{"properties":{"num":{"store":"yes","type":"integer"},"text":{"store":"yes","type":"string"}}}'
curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
sleep 2;
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1'  -d '
{"num":2,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2'  -d '
{"num":3,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3'  -d '
{"num":4,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4'  -d '
{"num":5,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5'  -d '
{"num":6,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6'  -d '
{"num":7,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7'  -d '
{"num":8,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8'  -d '
{"num":9,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9'  -d '
{"num":10,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10'  -d '
{"num":11,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11'  -d '
{"num":12,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12'  -d '
{"num":13,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13'  -d '
{"num":14,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14'  -d '
{"num":15,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15'  -d '
{"num":16,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16'  -d '
{"num":17,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17'  -d '
{"num":18,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18'  -d '
{"num":19,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19'  -d '
{"num":20,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20'  -d '
{"num":21,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21'  -d '
{"num":22,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22'  -d '
{"num":23,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23'  -d '
{"num":24,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24'  -d '
{"num":25,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25'  -d '
{"num":26,"text":"foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26'  -d '
{"num":27,"text":"foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27'  -d '
{"num":28,"text":"foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28'  -d '
{"num":29,"text":"foo baz"}'
curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
sleep 2
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/30'  -d '
{
   "text" : "foo"
}
'

curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true'" &gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true' &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/es_test_1/_terms?pretty=true&amp;fields=text&amp;toInclusive=true'" &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/es_test_1/_terms?pretty=true&amp;fields=text&amp;toInclusive=true' &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;minFreq=17'" &gt;&gt; log_1 
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;minFreq=17' &gt;&gt; log_1 

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;maxFreq=16&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;maxFreq=16&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;size=2&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;size=2&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;sort=freq&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;sort=freq&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz&amp;fromInclusive=false'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz&amp;fromInclusive=false'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=false'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=false'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;prefix=ba'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;prefix=ba'  &gt;&gt; log_1

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;regexp=foo|baz'"  &gt;&gt; log_1
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;regexp=foo|baz'  &gt;&gt; log_1
  #########################################################################


echo "

Now kill the current server, and start 3 nodes, then press Enter

"

read

curl -XPUT 'http://127.0.0.2:9200/es_test_1/'  -d '
{}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'  -d '
{}'
curl -XPUT 'http://127.0.0.2:9200/_all/type_1/_mapping?ignoreDuplicates=false'  -d '
{"properties":{"num":{"store":"yes","type":"integer"},"text":{"store":"yes","type":"string"}}}'
curl -XPUT 'http://127.0.0.2:9200/_all/type_2/_mapping?ignoreDuplicates=false'  -d '
{"properties":{"num":{"store":"yes","type":"integer"},"text":{"store":"yes","type":"string"}}}'
curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
sleep 2;
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/1'  -d '
{"num":2,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/2'  -d '
{"num":3,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3'  -d '
{"num":4,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4'  -d '
{"num":5,"text":"foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/5'  -d '
{"num":6,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/6'  -d '
{"num":7,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7'  -d '
{"num":8,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8'  -d '
{"num":9,"text":"foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/9'  -d '
{"num":10,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/10'  -d '
{"num":11,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11'  -d '
{"num":12,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12'  -d '
{"num":13,"text":"foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/13'  -d '
{"num":14,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/14'  -d '
{"num":15,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15'  -d '
{"num":16,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16'  -d '
{"num":17,"text":"bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/17'  -d '
{"num":18,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/18'  -d '
{"num":19,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19'  -d '
{"num":20,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20'  -d '
{"num":21,"text":"baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/21'  -d '
{"num":22,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/22'  -d '
{"num":23,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23'  -d '
{"num":24,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24'  -d '
{"num":25,"text":"bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/25'  -d '
{"num":26,"text":"foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_2/26'  -d '
{"num":27,"text":"foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27'  -d '
{"num":28,"text":"foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28'  -d '
{"num":29,"text":"foo baz"}'
curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
sleep 2
curl -XPUT 'http://127.0.0.2:9200/es_test_1/type_1/30'  -d '
{
   "text" : "foo"
}
'

curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true'" &gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true' &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/es_test_1/_terms?pretty=true&amp;fields=text&amp;toInclusive=true'" &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/es_test_1/_terms?pretty=true&amp;fields=text&amp;toInclusive=true' &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;minFreq=17'" &gt;&gt; log_2 
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;minFreq=17' &gt;&gt; log_2 

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;maxFreq=16&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;maxFreq=16&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;size=2&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;size=2&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;sort=freq&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;sort=freq&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=true'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=true'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz&amp;fromInclusive=false'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;from=baz&amp;fromInclusive=false'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=false'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;to=baz&amp;fields=text&amp;toInclusive=false'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;prefix=ba'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;prefix=ba'  &gt;&gt; log_2

echo "
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;regexp=foo|baz'"  &gt;&gt; log_2
curl -XGET 'http://127.0.0.2:9200/_terms?pretty=true&amp;fields=text&amp;toInclusive=true&amp;regexp=foo|baz'  &gt;&gt; log_2

echo "




Showing diff:
"

diff -y --left-column log_1 log_2
```
</description><key id="137941">38</key><summary>Terms results differs between one node and multiple</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-22T21:02:37Z</created><updated>2010-03-02T17:17:18Z</updated><resolved>2010-02-25T19:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-25T19:36:01Z" id="144913">Just pushed a fix for this, the array index exception is fixed. Can you check? 
</comment><comment author="clintongormley" created="2010-02-25T19:45:27Z" id="144926">That's fixed++ 

I'm away for the weekend, but I'll be back on Tuesday to find more bugs :)
</comment><comment author="kimchy" created="2010-02-25T21:17:09Z" id="145004">great stuff!, I really appreciate your effort into making elasticsearch better
</comment><comment author="clintongormley" created="2010-03-02T17:17:18Z" id="148744">likewise :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms with filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/37</link><project id="" key="" /><description>For auto-suggest, it would be nice to be able to ask for (eg) all terms with prefix 'sch' that occur in the same document as 'arnold'
</description><key id="137870">37</key><summary>Terms with filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-02-22T19:05:36Z</created><updated>2010-07-15T15:37:35Z</updated><resolved>2010-07-15T15:37:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-22T19:39:17Z" id="141843">Can you give a more concrete example? When you say document arnold, is that the id of the document, or actually a query?

I do plan to add a more like this option, where (in one case) you would give the document id, and all the documents that are like the document will be returned.
</comment><comment author="clintongormley" created="2010-02-22T19:50:48Z" id="141859">That's nice too, but not what I mean.

Think about google's auto suggest.

The user starts typing "ar", possible matches are 'arnold' 'arnaud', 'argentinia' etc

If the user starts typing 'sch', possible matches are 'schwab', 'schweitz', 'schwarzenegger'

But if the user types "arnold sch" then the likeliest match is "schwarzenegger"

So it'd be nice to say:  give me all the terms starting with $prefix, in documents that contain [$token_1, $token_n]
</comment><comment author="kimchy" created="2010-02-22T20:25:57Z" id="141895">Yes, now I understand what you mean. Thats a more complex autosuggest, which can be done in several ways, not just the one you mentioned (for example, google uses the queries users enter for auto suggest, which I think is far better).
</comment><comment author="clintongormley" created="2010-02-22T21:04:29Z" id="141943">sure - agreed
</comment><comment author="clintongormley" created="2010-07-15T15:37:34Z" id="313632">Now implemented with search facets in 0.9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: Store cluster meta data in JSON (and not binary)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/36</link><project id="" key="" /><description>Makes things much easier to debug, tinker with, and support for backward compatibility. Sadly, this will break backward compatibility.
</description><key id="137830">36</key><summary>Gateway: Store cluster meta data in JSON (and not binary)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-22T18:07:56Z</created><updated>2010-02-22T18:08:40Z</updated><resolved>2010-02-22T18:08:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-22T18:08:40Z" id="141756">Gateway: Store cluster meta data in JSON (and not binary), closed by 67d86de7eafbb78b2d5a1faa046bb13290de12ed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only storing one mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/35</link><project id="" key="" /><description>Hiya

Something weird is going on with put_mapping, eg:
- index /foo/one/ -d { "xxx": "text"}
- &gt; mappings for index 'foo' shows the correct mapping for 'one'
- index /foo/two -d {"yyy": "text"}
- &gt; mappings for index 'foo'  shows the mapping for 'one' but not 'two'

or
- index /foo/one/ -d { "xxx": "text"}
- &gt; mappings for index 'foo' shows the correct mapping for 'one'
- put_mapping /foo/two -d { properties: {"yyy": { type: "string"}}}
- &gt; mappings for index 'foo'  shows the mapping for 'one' but not 'two'

or
- put_mapping /foo/one -d { properties: {"xxx": { type: "string"}}}
- &gt; mappings for index 'foo' shows the correct mapping for 'one'
- put_mapping /foo/two -d { properties: {"yyy": { type: "string"}}}
- &gt; mappings for index 'foo'  shows the mapping for 'two' but not for 'one'

Example below:

```
curl -XPOST 'http://127.0.0.2:9200/foo/one'  -d '
{
   "xxx" : "text"
}
'
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "340e1d15-3dfd-4a8e-95ec-1c29fb8e9182",
#    "_type" : "one"
# }

Server log:
-----------
    Index [foo]: Update mapping [one] (dynamic) with source [{
      "one" : {
        "type" : "object",
        "dynamic" : true,
        "enabled" : true,
        "pathType" : "full",
        "dateFormats" : [ "dateOptionalTime" ],
        "boostField" : {
          "name" : "_boost"
        },
        "properties" : {
          "xxx" : {
            "type" : "string",
            "indexName" : "xxx",
            "index" : "analyzed",
            "store" : "no",
            "termVector" : "no",
            "boost" : 1.0,
            "omitNorms" : false,
            "omitTermFreqAndPositions" : false
          }
        }
      }
    }]


Cluster-state: indices.foo.mappings.mapping.value:
--------------------------------------------------
    '{
      "one" : {
        "type" : "object",
        "dynamic" : true,
        "enabled" : true,
        "pathType" : "full",
        "dateFormats" : [ "dateOptionalTime" ],
        "boostField" : {
          "name" : "_boost"
        },
        "properties" : {
          "xxx" : {
            "type" : "string",
            "indexName" : "xxx",
            "index" : "analyzed",
            "store" : "no",
            "termVector" : "no",
            "boost" : 1.0,
            "omitNorms" : false,
            "omitTermFreqAndPositions" : false
          }
        }
      }
    }'

curl -XPOST 'http://127.0.0.2:9200/foo/two'  -d '
{
   "yyy" : "text"
}
'
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "c57b2421-943e-4623-be80-8168211fca5d",
#    "_type" : "two"
# }

Server log:
-----------
    Update mapping [two] (dynamic) with source [{
      "two" : {
        "type" : "object",
        "dynamic" : true,
        "enabled" : true,
        "pathType" : "full",
        "dateFormats" : [ "dateOptionalTime" ],
        "boostField" : {
          "name" : "_boost"
        },
        "properties" : {
          "yyy" : {
            "type" : "string",
            "indexName" : "yyy",
            "index" : "analyzed",
            "store" : "no",
            "termVector" : "no",
            "boost" : 1.0,
            "omitNorms" : false,
            "omitTermFreqAndPositions" : false
          }
        }
      }
    }]

Cluster-state: indices.foo.mappings.mapping.value:
--------------------------------------------------
    '{
      "one" : {
        "type" : "object",
        "dynamic" : true,
        "enabled" : true,
        "pathType" : "full",
        "dateFormats" : [ "dateOptionalTime" ],
        "boostField" : {
          "name" : "_boost"
        },
        "properties" : {
          "xxx" : {
            "type" : "string",
            "indexName" : "xxx",
            "index" : "analyzed",
            "store" : "no",
            "termVector" : "no",
            "boost" : 1.0,
            "omitNorms" : false,
            "omitTermFreqAndPositions" : false
          }
        }
      }
    }'
```
</description><key id="137760">35</key><summary>Only storing one mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-02-22T16:21:53Z</created><updated>2010-02-22T20:55:59Z</updated><resolved>2010-02-22T20:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-02-22T18:46:02Z" id="141783">Btw, it looks like the mapping is still enforced, you're just not merging the JSON metadata correctly
</comment><comment author="kimchy" created="2010-02-22T19:36:18Z" id="141835">I am not sure I understand. I indexed into type one, and indexed into type two, and then ran the _cluster/state and I do see the two mappings.

By the way, the mappings returned were wrong in terms of JSON response, since I returned the same object name (mapping) for each one. I fixed it. Maybe that was the reason?
</comment><comment author="clintongormley" created="2010-02-22T20:03:22Z" id="141878">yep, the old "json properties SHOULD..." issue - your commit fixed it.

thanks
</comment><comment author="kimchy" created="2010-02-22T20:27:12Z" id="141897">No problem. I thought I went over all the places and fixed them, guess I missed this one.
</comment><comment author="kimchy" created="2010-02-22T20:55:59Z" id="141934">cool, I will close the issue then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping Overhaul - More user friendly, cluster aware</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/34</link><project id="" key="" /><description>1. When adding mapping definitions, they are now merged with the current mapping definitions. Duplicates are silently ignore unless specified in the put mapping API (HTTP param ignoreDuplicates set to false). Note, duplicates refer only to field mapping, object mapping are recursively checked.
2. Mappings definitions are now clustered. Up until now, when creating mapping, the mappings were not merged (see point 1) but they were broadcasted to the cluster. But, when a document indexed resulted in updated mapping, this fact was lost. Now, this changed mapping are updated on the master and merged, and broadcasted to the whole cluster, which means all the different nodes will know about the new types introduced almost immediately. 
</description><key id="137255">34</key><summary>Mapping Overhaul - More user friendly, cluster aware</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-21T20:56:27Z</created><updated>2010-03-02T18:17:24Z</updated><resolved>2010-02-21T20:57:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-21T20:57:19Z" id="141033">Note, I renamed create mapping to be named put mapping since it reflects the API better. The rest API is the same, the site docs will be updated.
</comment><comment author="kimchy" created="2010-02-21T20:57:48Z" id="141035">Mapping Overhaul - More user friendly, cluster aware, closed by 6d52065db30aaa2c6c2161dabdbbf2345b88a8f4.
</comment><comment author="kimchy" created="2010-03-01T11:45:00Z" id="147521">Renamed ignoreDuplicates to ignoreConflicts.
</comment><comment author="clintongormley" created="2010-03-02T16:41:22Z" id="148708">The current version on github still uses ignoreDuplicates, not ignoreConflicts
</comment><comment author="kimchy" created="2010-03-02T18:03:50Z" id="148784">You sure? Http request parameter parsed is ignoreConflicts, and defaults to true.
</comment><comment author="clintongormley" created="2010-03-02T18:13:01Z" id="148793">Apologies - my update failed because you changed 'devRelease' to 'release' and I didn't see it.

it does work now ++
</comment><comment author="kimchy" created="2010-03-02T18:17:24Z" id="148801">Yea, sorry about that. Just run gradlew without anything, it will default to building things properly always.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boolean Type: Support also cases when a number/string value are passed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/33</link><project id="" key="" /><description>Even though there is an explicit boolean type in JSON, support also cases when a number or string are passed. 0 meaning false, any other meaning true. "false" string meaning false, any other string meaning true.

Note, the boolean type will have to be explicitly defined, otherwise, a number will be defined as a number, and a string as a string.
</description><key id="136316">33</key><summary>Boolean Type: Support also cases when a number/string value are passed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-19T23:50:24Z</created><updated>2010-02-19T23:50:57Z</updated><resolved>2010-02-19T23:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-19T23:50:57Z" id="139917">Boolean Type: Support also cases when a number/string value are passed, closed by 4806df426ab838ca12bab22fd53965643dbab951.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting on a text field hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/32</link><project id="" key="" /><description>{ sort: { text_field: {} }} just hangs - no response.

Doesn't matter if I create an explicit mapping or not, or search on one index/type combination or _all

Test script:
    curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 
    curl -XDELETE 'http://127.0.0.2:9200/es_test/' 
    curl -XDELETE 'http://127.0.0.2:9200/es_test_2/' 
    curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
    curl -XPUT 'http://127.0.0.2:9200/es_test/' 
    curl -XPUT 'http://127.0.0.2:9200/es_test_2/'
    curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 

```
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/1?opType=create'  -d '{
   "num" : 2,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/2?opType=create'  -d '{
   "num" : 3,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3?opType=create'  -d '{
   "num" : 4,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4?opType=create'  -d '{
   "num" : 5,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/5?opType=create'  -d '{
   "num" : 6,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/6?opType=create'  -d '{
   "num" : 7,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7?opType=create'  -d '{
   "num" : 8,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8?opType=create'  -d '{
   "num" : 9,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/9?opType=create'  -d '{
   "num" : 10,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/10?opType=create'  -d '{
   "num" : 11,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11?opType=create'  -d '{
   "num" : 12,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12?opType=create'  -d '{
   "num" : 13,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/13?opType=create'  -d '{
   "num" : 14,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/14?opType=create'  -d '{
   "num" : 15,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15?opType=create'  -d '{
   "num" : 16,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16?opType=create'  -d '{
   "num" : 17,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/17?opType=create'  -d '{
   "num" : 18,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/18?opType=create'  -d '{
   "num" : 19,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19?opType=create'  -d '{
   "num" : 20,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20?opType=create'  -d '{
   "num" : 21,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/21?opType=create'  -d '{
   "num" : 22,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/22?opType=create'  -d '{
   "num" : 23,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23?opType=create'  -d '{
   "num" : 24,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24?opType=create'  -d '{
   "num" : 25,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/25?opType=create'  -d '{
   "num" : 26,   "text" : "foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/26?opType=create'  -d '{
   "num" : 27,   "text" : "foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27?opType=create'  -d '{
   "num" : 28,   "text" : "foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28?opType=create'  -d '{
   "num" : 29,   "text" : "foo baz"}'


curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 


curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "sort" : {
      "text" : {}
   },
   "query" : {
      "matchAll" : {}
   }
}
'
```
</description><key id="136203">32</key><summary>Sorting on a text field hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-19T20:14:28Z</created><updated>2010-03-02T17:30:45Z</updated><resolved>2010-03-02T17:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-20T02:08:33Z" id="140019">Looked at it and found a problem in search "map reduce" logic where if just certain shards fail, the action will not finish. I have just pushed a fix.

The problem you have is the fact that when you sort on string types, you must have them not analyzed (in the type mapping). Lucene might still perform the sort when they are analyzed, but it will throw a runtime exception when it can't (and this is what happens on some of the shards).

As a side note, I am going to spend time the next days to return proper failure messages from all operations, and not just counters on shards that failed.

If you can verify on your end that it is solved, I can close this issue.
</comment><comment author="clintongormley" created="2010-03-02T17:30:45Z" id="148757">Fixed ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting on a field explicitly mapped as an integer fails when not all types mapped as well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/31</link><project id="" key="" /><description>- create two indices 'es_test' and 'es_test_2'
- create a mapping for 'type_1' which includes { num: { type: "integer"}
- store documents as /es_test|es_test_2/type_1|type_2 with integer values in num
- search on all indices and types, sorting by num
- server hangs

Test script:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 
curl -XDELETE 'http://127.0.0.2:9200/es_test/' 
curl -XDELETE 'http://127.0.0.2:9200/es_test_2/' 
curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
curl -XPUT 'http://127.0.0.2:9200/es_test/' 
curl -XPUT 'http://127.0.0.2:9200/es_test_2/'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/_mapping'  -d '
{
   "properties" : {
      "num" : {
         "type" : "integer"
      },
      "text" : {
         "type" : "string"
      }
   }
}
'
curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/1?opType=create'  -d '{
   "num" : 2,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/2?opType=create'  -d '{
   "num" : 3,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3?opType=create'  -d '{
   "num" : 4,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4?opType=create'  -d '{
   "num" : 5,   "text" : "foo"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/5?opType=create'  -d '{
   "num" : 6,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/6?opType=create'  -d '{
   "num" : 7,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7?opType=create'  -d '{
   "num" : 8,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8?opType=create'  -d '{
   "num" : 9,   "text" : "foo bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/9?opType=create'  -d '{
   "num" : 10,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/10?opType=create'  -d '{
   "num" : 11,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11?opType=create'  -d '{
   "num" : 12,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12?opType=create'  -d '{
   "num" : 13,   "text" : "foo bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/13?opType=create'  -d '{
   "num" : 14,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/14?opType=create'  -d '{
   "num" : 15,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15?opType=create'  -d '{
   "num" : 16,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16?opType=create'  -d '{
   "num" : 17,   "text" : "bar baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/17?opType=create'  -d '{
   "num" : 18,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/18?opType=create'  -d '{
   "num" : 19,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19?opType=create'  -d '{
   "num" : 20,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20?opType=create'  -d '{
   "num" : 21,   "text" : "baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/21?opType=create'  -d '{
   "num" : 22,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/22?opType=create'  -d '{
   "num" : 23,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23?opType=create'  -d '{
   "num" : 24,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24?opType=create'  -d '{
   "num" : 25,   "text" : "bar"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/25?opType=create'  -d '{
   "num" : 26,   "text" : "foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test/type_2/26?opType=create'  -d '{
   "num" : 27,   "text" : "foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27?opType=create'  -d '{
   "num" : 28,   "text" : "foo baz"}'
curl -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28?opType=create'  -d '{
   "num" : 29,   "text" : "foo baz"}'


curl -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 


curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "sort" : {
      "num" : {}
   },
   "query" : {
      "matchAll" : {}
   }
}
'
```
</description><key id="136200">31</key><summary>Sorting on a field explicitly mapped as an integer fails when not all types mapped as well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-19T20:09:27Z</created><updated>2010-03-02T17:36:18Z</updated><resolved>2010-03-02T17:36:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-02-19T20:10:47Z" id="139773">This sort works correctly if all /index/type combinations have num mapped as integer
</comment><comment author="kimchy" created="2010-02-22T08:21:01Z" id="141352">Well, I tracked down the problem... . First, the search operation will not hang now (I just pushed a fix for it). The reason this fails is because when there is no mapping for a number, it is treated as a long, and when trying to compare long to ints the reduce phase fails. Now, you will get a failed response because of that. I still need to think how to support this (we should...).
</comment><comment author="clintongormley" created="2010-03-02T17:36:18Z" id="148765">It now throws an exception correctly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query.sort should be an array, not an object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/30</link><project id="" key="" /><description>Because JSON doesn't take order into account, sort should be an array, not an object.

For instance:
    sort : {
        postDate : {reverse : true},
        user : { },
        score : { }
    }

is the equivalent of:

```
sort : {
    score : { }
    user : { },
    postDate : {reverse : true},
}
```

Instead, perhaps this syntax:

```
sort : [
    "score",
    "user",
    { "postDate": { "reverse": true}} 
]
```

clint
</description><key id="136172">30</key><summary>query.sort should be an array, not an object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-19T19:08:11Z</created><updated>2010-02-20T15:35:32Z</updated><resolved>2010-02-20T00:48:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-20T00:47:38Z" id="139959">Have you seem somewhere that JSON doesn't take order into account? All the parsers that I have seen, for example, do create a linked hash map to represent an object, which means order is maintained. Of course, I don't convert the JSON into "nodes" or maps, but use pull parsing instead, but I still take order into account.

In any case, I agree that the format you mentioned is more coherent. I will add support for that (and the old format as well :) ).
</comment><comment author="kimchy" created="2010-02-20T00:48:32Z" id="139962">Allow for the sort search element to speciy the sort fields in an array, closed by 1aa8e011846fae20d3333b73dcf463f9bc99538b.
</comment><comment author="clintongormley" created="2010-02-20T01:49:57Z" id="140007">To be honest, no I hadn't - but I've found this post which discusses the issue http://pmuellr.blogspot.com/2009/04/ordered-javascript-properties.html.

In Perl, hashes (the equivalent structure) are unordered, so I'm glad you're happy to add support for the array form :)
</comment><comment author="clintongormley" created="2010-02-20T15:35:32Z" id="140254">Excellent - I can confirm that this is working
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New nodes not joining the cluster properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/29</link><project id="" key="" /><description>I start one node, insert various documents, run some queries - I get the correct results.

I start a new node, and wait for it to settle (even running optimize/flush/refresh) 

When rerunning the same queries, I get different totals and fewer hits returned, eg instead of the default 10, I may get 4 or 5

Killing the other nodes and rerunning the queries returns the correct results

ta

clint
</description><key id="136161">29</key><summary>New nodes not joining the cluster properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-19T18:51:14Z</created><updated>2010-03-02T16:56:29Z</updated><resolved>2010-03-02T16:56:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-02-19T18:53:16Z" id="139705">Similarly, if I start with (eg) 3 nodes, then the queries return the correct results until I remove one or more nodes.
</comment><comment author="kimchy" created="2010-02-20T00:17:15Z" id="139940">Is there a chance for a bash script that index documents (with curl) on a single node, then execute the query in question, pauses (for me to start the second node), and then runs the query again?
</comment><comment author="clintongormley" created="2010-02-20T02:41:54Z" id="140033">Emailed to you
</comment><comment author="kimchy" created="2010-02-28T09:20:46Z" id="146746">Can you run the test again? Fixed bunch of stuff which hopefully will resolve this (and if there are errors, you will now see then in the result).
</comment><comment author="clintongormley" created="2010-03-02T16:56:29Z" id="148719">Looks like this is working now ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet query crashes the cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/28</link><project id="" key="" /><description>Hiya

In my test script, I create two indices, then add 28 documents, then try searching on those.

When I get to the final facets query, the cluster never responds, and then remains unresponsive to all further queries.  One of the nodes balloons to 631MB of resident memory - I presume this is some sort of max allowed by java.

Then when closing down the nodes, two close down fine, and the third throws exceptions like the ones listed below.

Test script:  (see bottom for facets query)

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 
# {
#    "clusterName" : "elasticsearch",
#    "nodes" : {
#       "getafix-10912" : {
#          "httpAddress" : "inet[/127.0.0.2:9200]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9300]",
#          "name" : "Ameridroid"
#       },
#       "getafix-62342" : {
#          "httpAddress" : "inet[/127.0.0.2:9201]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9302]",
#          "name" : "Super Sabre"
#       },
#       "getafix-27084" : {
#          "httpAddress" : "inet[/127.0.0.2:9202]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9301]",
#          "name" : "Shadow Slasher"
#       }
#    }
# }

curl -XPUT 'http://127.0.0.2:9201/es_test/'  -d '
{}
'
# {
#    "ok" : true
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/'  -d '
{}
'
# {
#    "ok" : true
# }


curl -XPOST 'http://127.0.0.2:9201/_flush?refresh=true' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 20,
#       "total" : 20
#    }
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/1?opType=create'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "1",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/2?opType=create'  -d '
{
   "num" : 3,
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "2",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/3?opType=create'  -d '
{
   "num" : 4,
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "3",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/4?opType=create'  -d '
{
   "num" : 5,
   "text" : "foo"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "4",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/5?opType=create'  -d '
{
   "num" : 6,
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "5",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/6?opType=create'  -d '
{
   "num" : 7,
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "6",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/7?opType=create'  -d '
{
   "num" : 8,
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "7",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/8?opType=create'  -d '
{
   "num" : 9,
   "text" : "foo bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "8",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/9?opType=create'  -d '
{
   "num" : 10,
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "9",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/10?opType=create'  -d '
{
   "num" : 11,
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "10",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/11?opType=create'  -d '
{
   "num" : 12,
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "11",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/12?opType=create'  -d '
{
   "num" : 13,
   "text" : "foo bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "12",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/13?opType=create'  -d '
{
   "num" : 14,
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "13",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/14?opType=create'  -d '
{
   "num" : 15,
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "14",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/15?opType=create'  -d '
{
   "num" : 16,
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "15",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/16?opType=create'  -d '
{
   "num" : 17,
   "text" : "bar baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "16",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/17?opType=create'  -d '
{
   "num" : 18,
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "17",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/18?opType=create'  -d '
{
   "num" : 19,
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "18",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/19?opType=create'  -d '
{
   "num" : 20,
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "19",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/20?opType=create'  -d '
{
   "num" : 21,
   "text" : "baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "20",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/21?opType=create'  -d '
{
   "num" : 22,
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "21",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/22?opType=create'  -d '
{
   "num" : 23,
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "22",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/23?opType=create'  -d '
{
   "num" : 24,
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "23",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/24?opType=create'  -d '
{
   "num" : 25,
   "text" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "24",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_1/25?opType=create'  -d '
{
   "num" : 26,
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "25",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/type_2/26?opType=create'  -d '
{
   "num" : 27,
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test",
#    "_id" : "26",
#    "_type" : "type_2"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_1/27?opType=create'  -d '
{
   "num" : 28,
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "27",
#    "_type" : "type_1"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/type_2/28?opType=create'  -d '
{
   "num" : 29,
   "text" : "foo baz"
}
'
# {
#    "ok" : true,
#    "_index" : "es_test_2",
#    "_id" : "28",
#    "_type" : "type_2"
# }


curl -XPOST 'http://127.0.0.2:9201/_flush?refresh=true' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 20,
#       "total" : 20
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search'  -d '
{
   "query" : {
      "matchAll" : {}
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 4,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 5,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 9,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "8",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 21,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "20",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 3,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 7,
#                "text" : "foo bar"
#             },
#             "_index" : "es_test",
#             "_id" : "6",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 2,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "1",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 11,
#                "text" : "foo bar baz"
#             },
#             "_index" : "es_test",
#             "_id" : "10",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 22,
#                "text" : "bar"
#             },
#             "_index" : "es_test",
#             "_id" : "21",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 10,
#                "text" : "foo bar baz"
#             },
#             "_index" : "es_test",
#             "_id" : "9",
#             "_type" : "type_1"
#          }
#       ],
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search'  -d '
{
   "query" : {
      "matchAll" : {}
   },
   "size" : 100
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 5,
#                "text" : "foo"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 7,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "6",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 3,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 11,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "10",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 4,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 9,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "8",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 21,
#                "text" : "baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "20",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 2,
#                "text" : "foo"
#             },
#             "_index" : "es_test",
#             "_id" : "1",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 6,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "5",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 8,
#                "text" : "foo bar"
#             },
#             "_index" : "es_test_2",
#             "_id" : "7",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 12,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "11",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 10,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "9",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 20,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "19",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 22,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "21",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 16,
#                "text" : "bar baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "15",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 23,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "22",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 14,
#                "text" : "bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "13",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 13,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "12",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 18,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "17",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 24,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "23",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 17,
#                "text" : "bar baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "16",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 15,
#                "text" : "bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "14",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 19,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "18",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 25,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "24",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 26,
#                "text" : "foo baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "25",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 28,
#                "text" : "foo baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "27",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 27,
#                "text" : "foo baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "26",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 29,
#                "text" : "foo baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "28",
#             "_type" : "type_2"
#          }
#       ],
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search?searchType=query_then_fetch'  -d '
{
   "query" : {
      "matchAll" : {}
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 4,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 5,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 9,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "8",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 21,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "20",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 3,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 7,
#                "text" : "foo bar"
#             },
#             "_index" : "es_test",
#             "_id" : "6",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 2,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "1",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 11,
#                "text" : "foo bar baz"
#             },
#             "_index" : "es_test",
#             "_id" : "10",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 22,
#                "text" : "bar"
#             },
#             "_index" : "es_test",
#             "_id" : "21",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 10,
#                "text" : "foo bar baz"
#             },
#             "_index" : "es_test",
#             "_id" : "9",
#             "_type" : "type_1"
#          }
#       ],
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search?searchType=query_and_fetch'  -d '
{
   "query" : {
      "matchAll" : {}
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 5,
#                "text" : "foo"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 7,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "6",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 3,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 11,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "10",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 4,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 9,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "8",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 21,
#                "text" : "baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "20",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 2,
#                "text" : "foo"
#             },
#             "_index" : "es_test",
#             "_id" : "1",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 6,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "5",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 8,
#                "text" : "foo bar"
#             },
#             "_index" : "es_test_2",
#             "_id" : "7",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 12,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "11",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 10,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "9",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 20,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "19",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 22,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "21",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 16,
#                "text" : "bar baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "15",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 23,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "22",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 14,
#                "text" : "bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "13",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 13,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "12",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 18,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "17",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 24,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "23",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 17,
#                "text" : "bar baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "16",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 15,
#                "text" : "bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "14",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 19,
#                "text" : "baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "18",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 25,
#                "text" : "bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "24",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 26,
#                "text" : "foo baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "25",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 28,
#                "text" : "foo baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "27",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 27,
#                "text" : "foo baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "26",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 29,
#                "text" : "foo baz"
#             },
#             "_index" : "es_test_2",
#             "_id" : "28",
#             "_type" : "type_2"
#          }
#       ],
#       "total" : 28
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search'  -d '
{
   "query" : {
      "term" : {
         "text" : "foo"
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 3,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 5,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 4,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "3",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 6,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "5",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 9,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "8",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 7,
#                "text" : "foo bar"
#             },
#             "_index" : "es_test",
#             "_id" : "6",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 8,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "7",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 26,
#                "text" : "foo baz"
#             },
#             "_index" : "es_test",
#             "_id" : "25",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 28,
#                "text" : "foo baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "27",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 27,
#                "text" : "foo baz"
#             },
#             "_index" : "es_test",
#             "_id" : "26",
#             "_type" : "type_2"
#          }
#       ],
#       "total" : 16
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search'  -d '
{
   "query" : {
      "queryString" : {
         "defaultField" : "text",
         "query" : "foo OR bar"
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 6,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "5",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 8,
#                "text" : "foo bar"
#             },
#             "_index" : "es_test_2",
#             "_id" : "7",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 7,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "6",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 9,
#                "text" : "foo bar"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "8",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 10,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "9",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 11,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "10",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 12,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "11",
#             "_type" : "type_1"
#          },
#          {
#             "_source" : {
#                "num" : 13,
#                "text" : "foo bar baz"
#             },
#             "fields" : {},
#             "_index" : "es_test_2",
#             "_id" : "12",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 3,
#                "text" : "foo"
#             },
#             "fields" : {},
#             "_index" : "es_test",
#             "_id" : "2",
#             "_type" : "type_2"
#          },
#          {
#             "_source" : {
#                "num" : 5,
#                "text" : "foo"
#             },
#             "_index" : "es_test_2",
#             "_id" : "4",
#             "_type" : "type_2"
#          }
#       ],
#       "total" : 24
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }


curl -XGET 'http://127.0.0.2:9201/_all/_search'  -d '
{
   "query" : {
      "queryString" : {
         "defaultField" : "text",
         "query" : "foo OR bar"
      }
   },
   "facets" : {
      "barFacet" : {
         "query" : {
            "term" : {
               "text" : "bar"
            }
         }
      },
      "bazFacet" : {
         "query" : {
            "term" : {
               "text" : "baz"
            }
         }
      }
   }
}
'
#500 Server closed connection without sending any data back
```

Node errors:

```
[18:51:56,987][INFO ][server                   ] [Super Sabre] {ElasticSearch/0.5.0/2010-02-19T12:32:15/dev}: Closing ...
[18:52:06,995][INFO ][server                   ] [Super Sabre] {ElasticSearch/0.5.0/2010-02-19T12:32:15/dev}: Closed
[18:52:06,998][WARN ][indices.cluster          ] [Super Sabre] Failed to start shard for index [es_test_2] and shard id [0]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test_2][0]: Recovery failed from [Shadow Slasher][getafix-27084][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Super Sabre][getafix-62342][data][inet[getafix.traveljury.com/127.0.0.2:9302]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[18:52:06,999][WARN ][indices.cluster          ] [Super Sabre] Failed to start shard for index [es_test] and shard id [2]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test][2]: Recovery failed from [Shadow Slasher][getafix-27084][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Super Sabre][getafix-62342][data][inet[getafix.traveljury.com/127.0.0.2:9302]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[18:52:06,998][WARN ][indices.cluster          ] [Super Sabre] Failed to start shard for index [es_test_2] and shard id [3]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test_2][3]: Recovery failed from [Shadow Slasher][getafix-27084][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Super Sabre][getafix-62342][data][inet[getafix.traveljury.com/127.0.0.2:9302]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[18:52:07,003][WARN ][cluster.action.shard     ] [Super Sabre] Sending failed shard for [es_test_2][3], Node[getafix-62342], [B], S[INITIALIZING]
[18:52:07,001][WARN ][cluster.action.shard     ] [Super Sabre] Sending failed shard for [es_test_2][0], Node[getafix-62342], [B], S[INITIALIZING]
[18:52:07,004][WARN ][indices.cluster          ] [Super Sabre] Failed to mark shard as failed after a failed start for index [es_test_2] and shard id [0]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test_2][0]: Recovery failed from [Shadow Slasher][getafix-27084][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Super Sabre][getafix-62342][data][inet[getafix.traveljury.com/127.0.0.2:9302]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[18:52:07,002][WARN ][cluster.action.shard     ] [Super Sabre] Sending failed shard for [es_test][2], Node[getafix-62342], [B], S[INITIALIZING]
[18:52:07,005][WARN ][indices.cluster          ] [Super Sabre] Failed to mark shard as failed after a failed start for index [es_test] and shard id [2]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test][2]: Recovery failed from [Shadow Slasher][getafix-27084][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Super Sabre][getafix-62342][data][inet[getafix.traveljury.com/127.0.0.2:9302]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[18:52:07,004][WARN ][indices.cluster          ] [Super Sabre] Failed to mark shard as failed after a failed start for index [es_test_2] and shard id [3]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test_2][3]: Recovery failed from [Shadow Slasher][getafix-27084][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Super Sabre][getafix-62342][data][inet[getafix.traveljury.com/127.0.0.2:9302]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
```
</description><key id="136112">28</key><summary>Facet query crashes the cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-19T17:56:37Z</created><updated>2010-02-20T15:32:03Z</updated><resolved>2010-02-20T13:21:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2010-02-19T18:29:55Z" id="139681">It seems to be related to running more than one node - if I use just one node and run the query below, then it works correctly:

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "term" : {
         "text" : "foo"
      }
   },
   "facets" : {
      "bazFacet" : {
         "query" : {
            "term" : {
               "text" : "baz"
            }
         }
      }
   }
}
'
```

However, multiple facets are broken:

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{
   "query" : {
      "term" : {
         "text" : "foo"
      }
   },
   "facets" : [
      {
         "bazFacet" : {
            "query" : {
               "term" : {
                  "text" : "baz"
               }
            }
         }
      },
      {
         "barFacet" : {
            "query" : {
               "term" : {
                  "text" : "baz"
               }
            }
         }
      }
   ]
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 0
#    },
#    "_shards" : {
#       "failed" : 10,
#       "successful" : 0,
#       "total" : 10
#    }
# }
```
</comment><comment author="kimchy" created="2010-02-20T13:21:09Z" id="140210">Facet query crashes the cluster. Wrong serialzation of facets caused for construction of a rough sized array list. closed by 008b00f51a073765c46441e2ad299bf0a6f95646.
</comment><comment author="clintongormley" created="2010-02-20T15:32:03Z" id="140249">Sorry - reposting in the issue:

I'm afraid your facet fix still doesn't work. I can now check for multiple facets (which if I remember correctly wasn't working yesterday) when running with one node, but with a second node, a facet search still crashes the cluster.

Test script below:

```
#!/bin/bash
set -o verbose
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/1'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/2'  -d '
{
   "num" : 3,
   "text" : "foo"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/3'  -d '
{
   "num" : 4,
   "text" : "foo"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/4'  -d '
{
   "num" : 5,
   "text" : "foo"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/5'  -d '
{
   "num" : 6,
   "text" : "foo bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/6'  -d '
{
   "num" : 7,
   "text" : "foo bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/7'  -d '
{
   "num" : 8,
   "text" : "foo bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/8'  -d '
{
   "num" : 9,
   "text" : "foo bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/9'  -d '
{
   "num" : 10,
   "text" : "foo bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/10'  -d '
{
   "num" : 11,
   "text" : "foo bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/11'  -d '
{
   "num" : 12,
   "text" : "foo bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/12'  -d '
{
   "num" : 13,
   "text" : "foo bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/13'  -d '
{
   "num" : 14,
   "text" : "bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/14'  -d '
{
   "num" : 15,
   "text" : "bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/15'  -d '
{
   "num" : 16,
   "text" : "bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/16'  -d '
{
   "num" : 17,
   "text" : "bar baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/17'  -d '
{
   "num" : 18,
   "text" : "baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/18'  -d '
{
   "num" : 19,
   "text" : "baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/19'  -d '
{
   "num" : 20,
   "text" : "baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/20'  -d '
{
   "num" : 21,
   "text" : "baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/21'  -d '
{
   "num" : 22,
   "text" : "bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/22'  -d '
{
   "num" : 23,
   "text" : "bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/23'  -d '
{
   "num" : 24,
   "text" : "bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/24'  -d '
{
   "num" : 25,
   "text" : "bar"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_1/25'  -d '
{
   "num" : 26,
   "text" : "foo baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test/type_2/26'  -d '
{
   "num" : 27,
   "text" : "foo baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_1/27'  -d '
{
   "num" : 28,
   "text" : "foo baz"
}
'
curl -s -XPUT 'http://127.0.0.2:9200/es_test_2/type_2/28'  -d '
{
   "num" : 29,
   "text" : "foo baz"
}
'
curl -s -XPOST 'http://127.0.0.2:9200/_flush?refresh=true' 
curl -XGET 'http://127.0.0.2:9200/_all/_search?pretty=true'  -d '
{
   "query" : {
      "term" : {
         "text" : "foo"
      }
   },
   "facets" : {
      "bazFacet" : {
         "query" : {
            "term" : {
               "text" : "baz"
            }
         }
      }
   }
}
'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JSON object properties are not positional</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/27</link><project id="" key="" /><description>Hiya

It looks like you are parsing JSON as a stream, where position matters, instead of parsing the whole objects before analysing them.  For instance, this works:

```
curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
{ query: 
    {
       "filteredQuery" : {
          "query" : {
             "term" : {
                "text" : "foo"
             }
          },
          "filter" : {
             "range" : {
                "num" : {
                   "from" : 10,
                   "to" : 20
                }
             }
          }
       }
   }
}
'
```

But this fails:
    curl -XGET 'http://127.0.0.2:9200/_all/_search'  -d '
    { query: 
        {
           "filteredQuery" : {
              "filter" : {
                 "range" : {
                    "num" : {
                       "to" : 20,
                       "from" : 10
                    }
                 }
              },
              "query" : {
                 "term" : {
                    "text" : "foo"
                 }
              }
           }
       }
    }
    '

A JSON parser should consider these two structures to be identical, which is also the thing that rules out having non-unique property names.

thanks

Clint

(Edited to correct JSON)
</description><key id="136066">27</key><summary>JSON object properties are not positional</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-19T16:11:43Z</created><updated>2010-02-20T02:11:47Z</updated><resolved>2010-02-20T02:11:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-19T23:43:42Z" id="139906">In all the places where JSON is handled, location is not relevant. This problem is not actually related to the location of the filter and the query (there is support for placing them where you want) but with the fact the range query did not behaved correctly when used as inner query.

I pushed a fix for it, can you give it a go?
</comment><comment author="clintongormley" created="2010-02-20T01:58:02Z" id="140011">Perfect! that works - thanks
</comment><comment author="kimchy" created="2010-02-20T02:11:47Z" id="140024">Great, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accept 1 / 0 as true/false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/26</link><project id="" key="" /><description>Hiya

When you have boolean parameters, eg { "explain": true }, please could you accept any truthy value (eg 1) as for dynamic languages, the user has to go to great lengths to force a  boolean true in JSON

thanks

clint
</description><key id="135406">26</key><summary>Accept 1 / 0 as true/false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-18T16:44:02Z</created><updated>2010-02-20T00:13:13Z</updated><resolved>2010-02-20T00:13:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-18T21:20:18Z" id="138848">In JSON there is a specific type for boolean, its funny that its hard to set it. So, how do you imaging this, as {"explain" : "1"}  or {"explain" : 1}?
</comment><comment author="clintongormley" created="2010-02-19T01:26:48Z" id="139074">well, in dynamic languages that don't have a boolean type (eg Perl), false is
- 0
- undef / null
- ""
- "0"

and everything else is true.

Typically, we'd pass in 1 and 0 to represent true and false, but users could pass in anything.  The one gotcha could be if they pass in 'true' or 'false', which would become "true" or "false", but at least in Perl users would be unlikely to do that.
</comment><comment author="kimchy" created="2010-02-19T08:01:13Z" id="139253">Ok, make sense. So, places where I parse the JSON and know what to expect, I can take care of that. But, how to you index a JSON document, are the boolean fields there used, or do you use numbers?
</comment><comment author="clintongormley" created="2010-02-19T11:46:50Z" id="139394">Hmm, that's an interesting one...  

Typically, most Perl users would use 1 / 0 (so Int).  The module I'm using to encode the JSON provides the facility to represent a real JSON 'true' or 'false' by using \1 or \0  (which actually means "a reference to the scalar value 1")

Are you able to convert truthy/falsey values to real boolean values when indexing a field mapped as type: "boolean" ?  If so, you may not want to enable it by default, so that users of typed languages don't complain.  

But allow dynamic languages to pass a flag, perhaps?

And I'd put a a big GOTCHA warning in my docs explaining the need to use mappings before indexing.
</comment><comment author="kimchy" created="2010-02-19T23:45:50Z" id="139907">I do perform optimization when it comes to boolean fields. People can use 1 and 0, but they will be treated automatically as numbers when not explicitly defined in mapping. Even when explicitly defined in the mappings as boolean, only a boolean type is supported. I will add support for numbers as well.
</comment><comment author="kimchy" created="2010-02-19T23:51:14Z" id="139918">Opened #33 for the boolean type in JSON documents indexed.
</comment><comment author="kimchy" created="2010-02-20T00:13:13Z" id="139936">Accept also 0 int number to indicate false, and any other number to indicate true (on top of accepting json boolean type), closed by 3f045dee1fac51edd6cf754be616fd9aa95fedd5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerExceptions when flushing an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/25</link><project id="" key="" /><description>Hiya

I'm running a test suite against a local server, started as:

```
./bin/elasticsearch -f
```

I'm getting NullPointerExceptions if I create an index, flush the indices, then delete the index, without sleeping after the flush.  Shouldn't flush/refresh/optimize etc block until the action is complete?

Or is there some way of asking the cluster: are you ready now?

ta

clint
    &gt; ./bin/elasticsearch -f
    [15:04:00,872][INFO ][server                   ] [Doe, John] {ElasticSearch/0.5.0/2010-02-18T13:42:47/dev}: Initializing ...
    [15:04:02,663][INFO ][server                   ] [Doe, John] {ElasticSearch/0.5.0/2010-02-18T13:42:47/dev}: Initialized
    [15:04:02,663][INFO ][server                   ] [Doe, John] {ElasticSearch/0.5.0/2010-02-18T13:42:47/dev}: Starting ...
    [15:04:02,755][INFO ][transport                ] [Doe, John] boundAddress [inet[/0.0.0.0:9300]], publishAddress [inet[/127.0.0.2:9300]]
    [15:04:02,771][WARN ][jgroups.UDP              ] send buffer of socket java.net.DatagramSocket@1faac07d was set to 640KB, but the OS only allocated 131.07KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
    [15:04:02,771][WARN ][jgroups.UDP              ] receive buffer of socket java.net.DatagramSocket@1faac07d was set to 20MB, but the OS only allocated 131.07KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
    [15:04:02,771][WARN ][jgroups.UDP              ] send buffer of socket java.net.MulticastSocket@2259a735 was set to 640KB, but the OS only allocated 131.07KB. This might lead to performance problems. Please set your max send buffer in the OS correctly (e.g. net.core.wmem_max on Linux)
    [15:04:02,771][WARN ][jgroups.UDP              ] receive buffer of socket java.net.MulticastSocket@2259a735 was set to 25MB, but the OS only allocated 131.07KB. This might lead to performance problems. Please set your max receive buffer in the OS correctly (e.g. net.core.rmem_max on Linux)
    [15:04:04,814][INFO ][cluster                  ] [Doe, John] New Master [Doe, John][getafix-2590][data][inet[/127.0.0.2:9300]]
    [15:04:04,882][INFO ][discovery                ] [Doe, John] elasticsearch/getafix-2590
    [15:04:04,901][INFO ][http                     ] [Doe, John] boundAddress [inet[/0.0.0.0:9200]], publishAddress [inet[/127.0.0.2:9200]]
    [15:04:05,140][INFO ][jmx                      ] [Doe, John] boundAddress [service:jmx:rmi:///jndi/rmi://:9400/jmxrmi], publishAddress [service:jmx:rmi:///jndi/rmi://127.0.0.2:9400/jmxrmi]
    [15:04:05,140][INFO ][server                   ] [Doe, John] {ElasticSearch/0.5.0/2010-02-18T13:42:47/dev}: Started
    [15:04:06,516][INFO ][cluster.metadata         ] [Doe, John] Creating Index [es_test_2], shards [3]/[1]
    [15:04:06,992][INFO ][cluster.metadata         ] [Doe, John] Deleting index [es_test_2]
    Exception in thread "elasticsearch[Doe, John]clusterService#updateTask-pool-6-thread-1" java.lang.NullPointerException
        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:129)
        at org.elasticsearch.cluster.DefaultClusterService$2.run(DefaultClusterService.java:161)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
    Exception in thread "elasticsearch[Doe, John]clusterService#updateTask-pool-6-thread-2" java.lang.NullPointerException
        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:129)
        at org.elasticsearch.cluster.DefaultClusterService$2.run(DefaultClusterService.java:161)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
    Exception in thread "elasticsearch[Doe, John]clusterService#updateTask-pool-6-thread-3" java.lang.NullPointerException
        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:129)
        at org.elasticsearch.cluster.DefaultClusterService$2.run(DefaultClusterService.java:161)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

Test script:

```
curl -XGET http://127.0.0.1:9200/_cluster/nodes
# {
#    "clusterName" : "elasticsearch",
#    "nodes" : {
#       "getafix-2590" : {
#          "httpAddress" : "inet[/127.0.0.2:9200]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;         0.2:9300]",
#          "name" : "Doe, John"
#       }
#    }
# }


curl -XPUT http://127.0.0.2:9200/es_test_2/ -d '{
   "index" : {
      "numberOfReplicas" : 1,
      "numberOfShards" : 3
   }
}
'
# {
#    "ok" : true
# }


curl -XPOST http://127.0.0.2:9200/_flush
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 6,
#       "successful" : 0,
#       "total" : 6
#    }
# }


curl -XDELETE http://127.0.0.2:9200/es_test_2/
# {
#    "ok" : true
# }


curl -XGET http://127.0.0.2:9200/es_test_2/_status
# {
#    "debug" : {
#       "at" : {
#          "className" : "java.lang.Thread",
#          "methodName" : "run",
#          "fileName" : "Thread.java",
#          "lineNumber" : 619
#       },
#       "message" : "Index[es_test_2] missing"
#    },
#    "error" : "Index[es_test_2] missing"
# }
```
</description><key id="135304">25</key><summary>NullPointerExceptions when flushing an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-18T14:17:52Z</created><updated>2010-03-02T16:59:21Z</updated><resolved>2010-03-02T16:59:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-18T21:47:06Z" id="138881">The exception on the server side is a bug (well, actually, nothing bad happened, but it should not be printed out).

I agree regarding the ability to "wait" for an index to get instantiated on the nodes it can. It might be a flag on the create index, or another API (localhost:9200/test/_wait or something).

One last question, I added the debug element to APIs that fail. I know, I have a whole lot of work at getting exception proper and all, but for now, do you think that by default, should it return the debug element, or just the message (9 out of 10 times, the message is enough)?
</comment><comment author="clintongormley" created="2010-02-19T01:29:13Z" id="139077">"nothing bad happened"... hmm, my cluster would hang for ages - maybe it recovers in the end? i didn't wait.

re debug element vs message - perhaps have a server startup flag?  the debug messages are only really helpful to you, so to have them enabled by default would just give more crud to the user.  but you want to be able to ask them for the debug output, so it should be easy to switch that on.
</comment><comment author="kimchy" created="2010-02-19T07:51:39Z" id="139247">Really, it got stuck? Strange, I will have a look at why it might happen. Have you tested it with the new version and it is fixed?

I will add a flag that is called traceError that will print it. By default, I will turn it off.
</comment><comment author="clintongormley" created="2010-02-19T13:34:06Z" id="139441">Here's an example of how it hangs (latest version):

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes' 
# {
#    "clusterName" : "elasticsearch",
#    "nodes" : {
#       "getafix-55022" : {
#          "httpAddress" : "inet[/127.0.0.2:9201]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9301]",
#          "name" : "Talbot, Glenn"
#       },
#       "getafix-15896" : {
#          "httpAddress" : "inet[/127.0.0.2:9200]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9300]",
#          "name" : "Monroe, Trip"
#       },
#       "getafix-61392" : {
#          "httpAddress" : "inet[/127.0.0.2:9202]",
#          "dataNode" : true,
#          "transportAddress" : "inet[getafix.traveljury.com/127.0.
# &gt;          0.2:9302]",
#          "name" : "Puck"
#       }
#    }
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/'  -d '
{}
'
# {
#    "ok" : true
# }


curl -XPUT 'http://127.0.0.2:9201/es_test/'  -d '
{}
'
# {
#    "error" : "Index[es_test] Already exists"
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/'  -d '
{
   "index" : {
      "numberOfReplicas" : 1,
      "numberOfShards" : 3
   }
}
'
# {
#    "ok" : true
# }


curl -XPOST 'http://127.0.0.2:9201/_flush?refresh=true' 
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 15,
#       "successful" : 1,
#       "total" : 16
#    }
# }


curl -XDELETE 'http://127.0.0.2:9201/es_test_2/' 
# {
#    "ok" : true
# }


curl -XPUT 'http://127.0.0.2:9201/es_test_2/'  -d '
{}
'
# 500 Server closed connection without sending any data back
```

Then, when I shutdown the servers, I get:

```
[14:31:52,707][INFO ][server                   ] [Talbot, Glenn] {ElasticSearch/0.5.0/2010-02-19T12:32:15/dev}: Closing ...
[14:32:02,714][INFO ][server                   ] [Talbot, Glenn] {ElasticSearch/0.5.0/2010-02-19T12:32:15/dev}: Closed
[14:32:02,736][WARN ][indices.cluster          ] [Talbot, Glenn] Failed to start shard for index [es_test_2] and shard id [0]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test_2][0]: Recovery failed from [Monroe, Trip][getafix-15896][data][inet[getafix.traveljury.com/127.0.0.2:9300]] into [Talbot, Glenn][getafix-55022][data][inet[getafix.traveljury.com/127.0.0.2:9301]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[14:32:02,741][WARN ][cluster.action.shard     ] [Talbot, Glenn] Sending failed shard for [es_test_2][0], Node[getafix-55022], [B], S[INITIALIZING]
[14:32:02,742][WARN ][indices.cluster          ] [Talbot, Glenn] Failed to mark shard as failed after a failed start for index [es_test_2] and shard id [0]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test_2][0]: Recovery failed from [Monroe, Trip][getafix-15896][data][inet[getafix.traveljury.com/127.0.0.2:9300]] into [Talbot, Glenn][getafix-55022][data][inet[getafix.traveljury.com/127.0.0.2:9301]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
```

And...

```
[14:31:45,972][INFO ][server                   ] [Monroe, Trip] {ElasticSearch/0.5.0/2010-02-19T12:32:15/dev}: Closing ...
[14:31:55,981][INFO ][server                   ] [Monroe, Trip] {ElasticSearch/0.5.0/2010-02-19T12:32:15/dev}: Closed
[14:31:55,985][WARN ][indices.cluster          ] [Monroe, Trip] Failed to start shard for index [es_test] and shard id [4]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test][4]: Recovery failed from [Talbot, Glenn][getafix-55022][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Monroe, Trip][getafix-15896][data][inet[getafix.traveljury.com/127.0.0.2:9300]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[14:31:55,988][WARN ][cluster.action.shard     ] [Monroe, Trip] Sending failed shard for [es_test][4], Node[getafix-15896], [B], S[INITIALIZING]
[14:31:55,989][WARN ][indices.cluster          ] [Monroe, Trip] Failed to mark shard as failed after a failed start for index [es_test] and shard id [4]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test][4]: Recovery failed from [Talbot, Glenn][getafix-55022][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Monroe, Trip][getafix-15896][data][inet[getafix.traveljury.com/127.0.0.2:9300]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[14:31:55,985][WARN ][indices.cluster          ] [Monroe, Trip] Failed to start shard for index [es_test] and shard id [1]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test][1]: Recovery failed from [Talbot, Glenn][getafix-55022][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Monroe, Trip][getafix-15896][data][inet[getafix.traveljury.com/127.0.0.2:9300]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
[14:31:55,991][WARN ][cluster.action.shard     ] [Monroe, Trip] Sending failed shard for [es_test][1], Node[getafix-15896], [B], S[INITIALIZING]
[14:31:55,991][WARN ][indices.cluster          ] [Monroe, Trip] Failed to mark shard as failed after a failed start for index [es_test] and shard id [1]
org.elasticsearch.index.shard.recovery.RecoveryFailedException: Index Shard [es_test][1]: Recovery failed from [Talbot, Glenn][getafix-55022][data][inet[getafix.traveljury.com/127.0.0.2:9301]] into [Monroe, Trip][getafix-15896][data][inet[getafix.traveljury.com/127.0.0.2:9300]]
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:154)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService$3.run(IndicesClusterStateService.java:325)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: org.elasticsearch.ElasticSearchInterruptedException
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:97)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:34)
    at org.elasticsearch.index.shard.recovery.RecoveryAction.startRecovery(RecoveryAction.java:124)
    ... 4 more
```
</comment><comment author="kimchy" created="2010-02-28T09:18:46Z" id="146745">can you give this a go again? I fixed some things relating to that and I think this will be fixed as well.
</comment><comment author="clintongormley" created="2010-03-02T16:59:21Z" id="148724">Fixed ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changing field type with create_mapping just hides the error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/24</link><project id="" key="" /><description>Easier to give an example than to explain:
    - on  a new cluster  (ie no indices, no mappings)
    - index a document with eg { foo: 123 }  # sets type of foo to 'int'
    - index a doc with { foo : "bar" }               # throws an error
    - create a mapping and set foo's type to 'string'
    - index the doc with { foo: "bar"}   # ok
    - search for {term: { foo: 123}}   # 1 hit
    - search for {term: { foo: "bar"}}   # no hits

So setting the mapping doesn't change the type of 'foo', it just hides the error message later on.

This seems inconsistent to me - it should either change the type of 'foo' going forward, or throw an error when you try to change the type with create_mapping.

Log file follows:

```
curl -XPUT http://127.0.0.2:9200/twitter/tweet/1 -d '{
   "foo" : 123
}
'
# {
#    "ok" : true,
#    "_index" : "twitter",
#    "_id" : "1",
#    "_type" : "tweet"
# }


curl -XPUT http://127.0.0.2:9200/twitter/tweet/2 -d '{
   "foo" : "bar"
}
'
# {
#    "debug" : {
#       "at" : {
#          "className" : "java.lang.Thread",
#          "methodName" : "run",
#          "fileName" : "Thread.java",
#          "lineNumber" : 619
#       },
#       "cause" : {
#          "at" : {
#             "className" : "java.lang.Thread",
#             "methodName" : "run",
#             "fileName" : "Thread.java",
#             "lineNumber" : 619
#          },
#          "message" : "Current token (VALUE_STRING) not numeric, c
# &gt;         an not use numeric value accessors\n at [Source: {\n   
#       },
#       "message" : "Index[twitter] Shard[1] "
#    },
#    "error" : "Index[twitter] Shard[1] ; nested: Failed to parse; 
# &gt;   nested: Current token (VALUE_STRING) not numeric, can not use
# }


curl -XPUT http://127.0.0.2:9200/_all/tweet/_mapping -d '{
   "properties" : {
      "foo" : {
         "type" : "string"
      }
   }
}
'
# {
#    "ok" : true
# }


curl -XPUT http://127.0.0.2:9200/twitter/tweet/2 -d '{
   "foo" : "bar"
}
'
# {
#    "ok" : true,
#    "_index" : "twitter",
#    "_id" : "2",
#    "_type" : "tweet"
# }


curl -XGET http://127.0.0.2:9200/twitter/tweet/_search -d '{
   "query" : {
      "term" : {
         "foo" : "123"
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 0
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    }
# }


curl -XGET http://127.0.0.2:9200/twitter/tweet/_search -d '{
   "query" : {
      "term" : {
         "foo" : "bar"
      }
   }
}
'
# {
#    "hits" : {
#       "hits" : [],
#       "total" : 0
#    },
#    "_shards" : {
#       "failed" : 5,
#       "successful" : 0,
#       "total" : 5
#    }
# }
```
</description><key id="135256">24</key><summary>Changing field type with create_mapping just hides the error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-18T12:05:16Z</created><updated>2010-03-02T18:05:44Z</updated><resolved>2010-03-02T17:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-03-01T12:28:54Z" id="147545">part of the mapper overhaul, this scenario should now work as expected. Basically, putting the new mapping will ignore conflicts so no exception on it
</comment><comment author="clintongormley" created="2010-03-02T17:06:43Z" id="148731">This works now.  I note that searching on the wrong type (eg field set to type integer, and search for 'foo') now throws this error:

 "SearchParseException[[twitter][0]: query[null],from[-1],size[-1]: Parse Failure [Failed to parse [[B@7e054643]]]; nested: NumberFormatException[For input string: \"bar\"]; "

May be worth tidying that up :)
</comment><comment author="kimchy" created="2010-03-02T18:05:44Z" id="148785">There is so much more work on getting proper exceptions out :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Set different boost for indices when searching across indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/23</link><project id="" key="" /><description>Have the ability to set different boost values per index when searching across indices. This comes in handy for example, when each twitter user has an index, and his friends count more than the rest of the indices.

The parameter is a url parameter called indicesBoost, and for example: indicesBoost=indexName1:2,indexName2:3.1
</description><key id="134773">23</key><summary>Search API: Set different boost for indices when searching across indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-17T17:48:51Z</created><updated>2015-11-26T00:57:40Z</updated><resolved>2010-02-17T17:49:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-17T17:49:54Z" id="137574">Search API: Set different boost for indices when searching across indices, closed by 8a5a44c1c39e83f6f90d657d6a3d91239d508da4.
</comment><comment author="thiagolocatelli" created="2015-11-26T00:57:40Z" id="159769402">Which of the available query builders provide this capability? I cant find a way to add indices_boost to my query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge bytebuffer and memory stores into a single memory store options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/22</link><project id="" key="" /><description>The bytebuffer store name is really bad. It basically exposes the user to the internals of Java on how a direct memory allocation (outside the JVM heap) is done.

Instead, there should be a single memory store, with the option to choose its "location", which can be either "heap" or "direct", with "direct" being the default.

This does mean that if someone was configuring to use the bytebuffer, things will break and they will need to change to memory type.
</description><key id="134566">22</key><summary>Merge bytebuffer and memory stores into a single memory store options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-17T11:08:50Z</created><updated>2010-02-17T11:09:28Z</updated><resolved>2010-02-17T11:09:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-17T11:09:28Z" id="137229">Merge bytebuffer and memory stores into a single memory store options, closed by 872781536d631cf1b849ace49ccc31e262bbd820.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms API: Allow to get terms for one or more field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/21</link><project id="" key="" /><description>Getting terms (from one or more indices) and their document frequency (the number of time those terms appear in a document) is very handy. For example, implementing tag clouds, or providing basic auto suggest search box.

There should be several options for this API, including sorting by term (lex) or doc freq, bounding size, from/to (inclusive or not), min/max freq, prefix and regexp filtering.

The rest api should be: /{index}/_terms
</description><key id="134137">21</key><summary>Terms API: Allow to get terms for one or more field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-16T19:47:05Z</created><updated>2014-02-12T01:34:19Z</updated><resolved>2010-02-16T19:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-16T19:48:04Z" id="136685">Terms API: Allow to get terms for one or more field. Closed by 5d781961a07368ae458126e4fad0a8db566637da.
</comment><comment author="clintongormley" created="2010-02-18T16:53:21Z" id="138572">Please could you provide the docs for the usage of terms, so that I can add it to ElasticSearch.pm

thanks

clint
</comment><comment author="kimchy" created="2010-02-21T23:38:47Z" id="141152">The terms API accepts the following uris:
- GET /_terms
- GET /{index}/_terms (where {index} can be one or more indices, with _all support)

The http parameters are (fields or field must be set):
- fields: The fields to search on, comma separated.
- field: The field to search on, can be multiple HTTP field parameters.
- from: The lower bound (lex) term from which the iteration will start.  Defaults to start from the first.
- to: The upper bound (lex) term to which the iteration will end. Defaults to unbound (null).
- fromInclusive: Should the first from (if set) be inclusive or not. Defaults to false.
- toInclusive: Should the last to (if set) be inclusive or not. Defaults to true.
- prefix: An optional prefix from which the terms iteration will start (in lex order).
- regexp: An optional regular expression to filter out terms (only the ones that match the regexp will return).
- minFreq: An optional minimum document frequency to filter out terms.
- maxFreq: An optional maximum document frequency to filter out terms.
- size: The number of term / doc freq pairs to return per field. Defaults to 10.
- sort: The type of sorting for term / doc freq. Can either be "term" or "freq". Defaults to term.

The field names support for indexName based lookup, and full path lookup (can have a type prefix or not).

The results basically include a docs header, and then a object named based on the field name, and the term and document frequency for each.

The only thing that I am not sure about is that currently, the term value is the JSON object name, and I wonder if it make sense to create generic JSON object, with a term field inside with its value, what do you think?
</comment><comment author="kimchy" created="2010-02-21T23:52:27Z" id="141165">Regarding my previous question, I simply added another http boolean parameter called termsAsArray. It defaults to true, which means you will get an array of JSON objects, with term and docFreq as fields. This will also maintain the order for parsers that are not order aware (since you can sort). If set to false, it will return JSON object names with the term itself.
</comment><comment author="clintongormley" created="2010-02-22T17:44:25Z" id="141733">&gt; fromInclusive: Should the first from (if set) be inclusive or not. Defaults to false.
&gt; toInclusive: Should the last to (if set) be inclusive or not. Defaults to true

You mean fromInclusive defaults to TRUE.  I've renamed these `exclude_from` and `exclude_to` so that the default (unspecified) is false.
</comment><comment author="clintongormley" created="2010-02-22T17:56:25Z" id="141742">What do you mean by this:

&gt; The field names support for indexName based lookup, and full path lookup (can have 
&gt; a type prefix or not).

Can you give me an example of the format?
</comment><comment author="clintongormley" created="2010-02-22T19:36:09Z" id="141834">&gt; fromInclusive: Should the first from (if set) be inclusive or not. Defaults to false. 
&gt; toInclusive: Should the last to (if set) be inclusive or not. Defaults to true

Actually, these are both incorrect.  Currently `fromInclusive` is `true` and `toInclusive` is `false`.

Why do you have these as different values?  From the naming of `from` and `to`, I'd expect them to be inclusive, and only to exclude them if specified.
</comment><comment author="kimchy" created="2010-02-22T20:20:30Z" id="141891">The idea of fromInclusive and toInclusive is to follow the usually convention of writing a for loop, something like for (i=0;i&lt;10;i++), in this case, the from (0) is inclusive, and to to is not. In any case, I suggest that you follow the same wording and parameters elasticsearch uses, so you won't confuse users. We can talk about if it make sense to change this, but while I suggest keeping it the same.

Regarding the field name, it is exaplined a bit here (http://www.elasticsearch.com/docs/elasticsearch/mapping/object_type/#pathType), though I should add a page that explains it explicitly. For example, if you have (person is the type of the mapping):

```
{ person : { name : { firstName : "...", lastName : "..." } } }
```

then the field name (that will match) will be either person.name.firstName, or name.firstName. If you add explicit mapping for the name object (or person), you can control the pathType.
</comment><comment author="clintongormley" created="2010-02-22T20:38:30Z" id="141906">&gt; The idea of fromInclusive and toInclusive is to follow the usually convention of writing
&gt; a for loop, something like for (i=0;i&lt;10;i++), 

OK - I didn't get that. I would say then they should be called `from` and `until`, rather than `to`.

In Perl (and some other dynamic languages), loops can be written more succinctly, like:

```
for (1..5) {  }
foreach my $name (@names) 
```

... both of which are inclusive.  To my mind, basing the default values of `fromInclusive` and `toInclusive` on a `for` loop exposes implementation, rather than representing how a user might think in natural language.

&gt; Regarding the field name....

OK, I have two mappings: type_1 and type_2. Both have a field 'text', but i ask for terms on field 'text' or 'type_1.text', I get the same results, which doesn't seem to be what I'm asking.

Is this what it is supposed to do?
</comment><comment author="kimchy" created="2010-02-22T20:55:27Z" id="141932">No problem, make sense, I will change the toInclusive to true.

Regarding the field name, yea, its not filtered by type if you prefix it by type (which is different than if you use the typed field in search queries for example). It can be implemented, but its more difficult and will be _much_ more expensive to perform, so for now, I did not implement it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>flush_index returns both success and failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/20</link><project id="" key="" /><description>while running the default server, just started with

```
./bin/elasticsearch -f
```

I create an index, then flush it, and it returns:

```
'{
   "ok" : true,
   "_shards" : {
      "failed" : 5,
      "successful" : 5,
      "total" : 10
   }
}
'
```

Why do 5 fail?
</description><key id="134068">20</key><summary>flush_index returns both success and failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2010-02-16T17:29:45Z</created><updated>2010-02-24T20:00:45Z</updated><resolved>2010-02-18T21:18:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-16T19:50:04Z" id="136688">Its because, by default, an index is created with 5 shards, each with one replica. When you start a single node, only 5 shards are allocated on that node, when you start another one, the other 5 replicas will be allocated. The failed ones refer to the 5 shards that have not been started yet.
</comment><comment author="clintongormley" created="2010-02-17T10:03:07Z" id="137198">Thanks for the explanation
</comment><comment author="kimchy" created="2010-02-18T21:18:55Z" id="138847">Closing this. Later on, I will add explicit explanation in the response as to why each shard failed...
</comment><comment author="kimchy" created="2010-02-24T20:00:45Z" id="143995">Just an update, responses now return failure reasons which will make things a bit more usable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>flush_index hangs when no indices exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/19</link><project id="" key="" /><description>flush_index hangs when no indices exist, then eventually just closes the connection 
</description><key id="134067">19</key><summary>flush_index hangs when no indices exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.05.0</label></labels><created>2010-02-16T17:25:30Z</created><updated>2010-02-16T19:56:29Z</updated><resolved>2010-02-16T19:56:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-16T19:56:15Z" id="136693">Yep, its a bug. A fix is traveling on the intertubes as I type...
</comment><comment author="kimchy" created="2010-02-16T19:56:29Z" id="136696">flush_index hangs when no indices exist, closed by 1299f203645d1b4b72abfedc1d65991b05042361.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation error: Object Type mapping JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/18</link><project id="" key="" /><description>In 
http://www.elasticsearch.com/docs/elasticsearch/mapping/object_type/
there are a few occurrences of
type = "object"
which should be
type: "object",
</description><key id="133878">18</key><summary>Documentation error: Object Type mapping JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hubgit</reporter><labels><label>docs</label></labels><created>2010-02-16T13:23:02Z</created><updated>2010-02-16T14:31:31Z</updated><resolved>2010-02-16T14:31:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-16T14:31:30Z" id="136392">Fixed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count API: Also accepts /_count uri to search over all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17</link><project id="" key="" /><description>You can do it now with /_all/_count, but its nicer to just do /_count
</description><key id="133719">17</key><summary>Count API: Also accepts /_count uri to search over all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-16T08:17:46Z</created><updated>2010-02-16T08:18:53Z</updated><resolved>2010-02-16T08:18:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-16T08:18:53Z" id="136136">Count API: Also accepts /_count uri to search over all indices. Closed by 06cbc0a95bf26471f0e76182d829f17b3ff01ab1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Also accepts /_search uri to search over all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16</link><project id="" key="" /><description>You can do it now with /_all/_search, but it would be nice to just do /_search
</description><key id="133717">16</key><summary>Search API: Also accepts /_search uri to search over all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-16T08:15:13Z</created><updated>2010-02-16T08:16:41Z</updated><resolved>2010-02-16T08:16:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-16T08:16:41Z" id="136134">Search API: Also accepts /_search uri to search over all indices. Closed by 854fc21a706e8df59bec77c781af43706be57e58.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize API: Add onlyExpungeDeletes, flush and refresh parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15</link><project id="" key="" /><description>- _onlyExpungeDeletes_: Performs lightweight optimization by only expunging pending deltes. Defaults to false.
- _flush_: Should a flush be performed after the optimization. Defaults to false.
- _refresh_: Should a refresh be performed after the optimization. Defaults to false.
</description><key id="133469">15</key><summary>Optimize API: Add onlyExpungeDeletes, flush and refresh parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-15T21:54:16Z</created><updated>2010-02-15T21:55:18Z</updated><resolved>2010-02-15T21:55:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-15T21:55:18Z" id="135780">Optimize API: Add onlyExpungeDeletes, flush and refresh parameters. Closed by 66b86a7a034eb9ccc7d391713016f30f9448737e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flush API: Add refresh flag (refresh after flush)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14</link><project id="" key="" /><description>Add a refresh flag (HTTP refresh boolean parameter) controlling if a refresh will be done after flush or not. Defaults to false, meaning no refresh will be done.
</description><key id="133460">14</key><summary>Flush API: Add refresh flag (refresh after flush)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-15T21:35:10Z</created><updated>2010-02-15T21:36:00Z</updated><resolved>2010-02-15T21:36:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-15T21:36:00Z" id="135761">Flush API: Add refresh flag (refresh after flush). Closed by 9633108ad619bad4cab5244eee530fdf833bd06a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search API: Fields listed should exists within a "fields" Json object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13</link><project id="" key="" /><description>The current search response is not very friendly since the fields are listed (by name) without a wrapper JSON object. The "fields" JSON object should wrap them.
</description><key id="133067">13</key><summary>Search API: Fields listed should exists within a "fields" Json object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-15T08:26:46Z</created><updated>2010-02-15T08:27:46Z</updated><resolved>2010-02-15T08:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-15T08:27:46Z" id="135279">Search API: Fields listed should exists within a "fields" Json object. Closed by e768f67fa0375441048b430e6ad8929909a7ad77.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Mapping API: Automatically create indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/12</link><project id="" key="" /><description>In a similar manner that the index operation automatically create the index, the mapping API should do the same. This can be disable using the setting 'action.autoCreateIndex'.
</description><key id="133064">12</key><summary>Create Mapping API: Automatically create indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-15T08:23:23Z</created><updated>2015-03-17T19:58:44Z</updated><resolved>2010-02-15T08:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-15T08:23:59Z" id="135276">Create Mapping API: Automatically create indices. Closed by 9a9ce99364fcf7d77d81b5300530fac2ee435f37.
</comment><comment author="rectalogic" created="2015-03-17T19:04:20Z" id="82534670">Automatically creating the index doesn't seem to work with elasticsearch 1.4.4

```
curl -XPUT http://localhost:9200/twitter/tweet/_mapping -d '
&gt; {
&gt;     tweet : {
&gt;         properties : {
&gt;             message : {type : "string", store : "yes"}
&gt;         }
&gt;     }
&gt; }
&gt; '
{"error":"IndexMissingException[[twitter] missing]","status":404}
```
</comment><comment author="markwalkom" created="2015-03-17T19:58:44Z" id="82571887">This is an issue that was closed 5 years ago!

If you have problems then I'd suggest trying the Elasticsearch mailing list - https://groups.google.com/forum/#!forum/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/11</link><project id="" key="" /><description>Hiya

Following the examples in your docs, create-mapping does not seem to work, eg:

```
curl -XPUT http://localhost:9200/twitter/tweet -d '
{
    tweet : {
        properties : {
            message : {type : "string", store : "yes"}
        }
    }
}
'
```

&gt; No handler found for uri [/twitter/tweet] and method [PUT]

I tried creating the index first, but same thing.

Also, the JSON format for specifying the mapping type to use when indexing a document is ambiguous, eg:

```
curl -XPUT http://localhost:9200/twitter/tweet/1 -d \
'
{
     tweet : {
        user : "kimchy",
        postDate : "2009-11-15T14:12:12",
        message : "trying out Elastic Search"
    }
  }
  '
```

Does that mean  that the document has mapping type 'tweet', or that there is no mapping type specified, and it has a single top level key called 'tweet'.

And one thing i'm not sure about? Is a mapping the same thing as a type? So you would never have type 'foo' and mapping 'bar'?

thanks

Clint
</description><key id="132905">11</key><summary>Mapping not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label></labels><created>2010-02-14T23:10:38Z</created><updated>2010-02-15T11:41:58Z</updated><resolved>2010-02-15T08:15:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-15T08:15:38Z" id="135273">There is a bug in the docs (which I just fixed), the url for it should be http://localhost:9200/twitter/tweet/_mapping. Here is the example:

```
curl -XPUT http://localhost:9200/twitter/tweet/_mapping -d '
{
    tweet : {
        properties : {
            message : {type : "string", store : "yes"}
        }
    }
}
'
```

&gt; I tried creating the index first, but same thing.

You first need to create the index explicitly to add mappings. I will add another issue so the index will be created automatically in this case.

&gt; Also, the JSON format for specifying the mapping type to use when indexing a document is ambiguous

Not sure that I understand why its ambiguous? The indexable content can have the type as the first level JSON field, but its optional (since the type already exists in the url and I can derive it from that).

&gt; And one thing i'm not sure about? Is a mapping the same thing as a type? So you would never have type 'foo' and mapping 'bar'?

Mappings are basically meta data on how to map the indexable JSON content of a type into the search engine. You can have more than one type, and each type can optionally have mapping defined for it.
</comment><comment author="kimchy" created="2010-02-15T08:24:40Z" id="135277">Create mapping now will automatically create the indices by default, see #12.
</comment><comment author="clintongormley" created="2010-02-15T10:26:02Z" id="135326">OK, so there is one mapping per index+type.  Gotcha.

The reason that including the mapping name in the JSON is ambiguous is because of object type mappings.  Without a predefined mapping, you don't know if the top level 'tweet' key is a mapping name, or the first key in the object being stored.

So either you have to always specify the mapping, so the second case would look like:

```
curl -XPUT http://localhost:9200/twitter/tweet/1 -d \
'
{
    tweet : {
        tweet : {
            user : "kimchy",
            postDate : "2009-11-15T14:12:12",
            message : "trying out Elastic Search"
        }
    }
}
'
```

or never specify the mapping in the JSON, and just use the 'type' from the URL as the mapping name (which would be my preference)
</comment><comment author="kimchy" created="2010-02-15T11:30:10Z" id="135349">Yes, you are correct about the possible problem there. The reason I added the support for that is to simplify working json converters in different languages, which usually add the "type" as the outermost JSON object. Do you think it was a mistake?
</comment><comment author="clintongormley" created="2010-02-15T11:36:04Z" id="135351">I think that having it as either-or is a mistake, yes.  

Not sure which is the better interface though. My feeling (having just written a Perl interface to ElasticSearch) is that you have to generate the URL and the JSON anyway, and it looks more consistent to me to specify the type in the URL, and the data in the JSON.

But as I say, it makes little difference to me which version you settle on.
</comment><comment author="kimchy" created="2010-02-15T11:41:58Z" id="135354">Let me think about it a bit more. The only case where it will break is if the case you noted,  (JSON object with the type name, and another JSON _object_ with the same type name) and I am not sure if people will ever generate a JSON like that. I like the ability to try and support both...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query http listeners</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/10</link><project id="" key="" /><description>In the same way as you can find out if a node is a data node or not, it'd be good to tell if a node has http enabled or not.

One of the things I'd like to be able to do, is to query one node about
the other http enabled nodes in the cluster (in the same was as you can
find out which nodes are data nodes)

In other words, one of my clients starts up, queries the 'main' node
about which listeners are available, then randomly selects one of those
nodes.

The idea is to spread the load between the nodes, and also, if a node
goes down, then my client already has a list of other nodes that it can
try connecting to.

thanks

Clint
</description><key id="132806">10</key><summary>Query http listeners</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-14T19:15:10Z</created><updated>2010-02-22T16:03:08Z</updated><resolved>2010-02-14T19:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-14T19:28:41Z" id="134956">Make sense. This will be part of the admin cluster node info API (REST is: http://localhost:9200/_cluster/nodes?pretty=true).

The json will be:

```
{
  "clusterName" : "elasticsearch",
  "nodes" : {
    "mackimchy-45484" : {
      "name" : "Commander Kraken",
      "transportAddress" : "inet[10.0.0.1/10.0.0.1:9300]",
      "dataNode" : true,
      "httpAddress" : "inet[/10.0.0.1:9200]"
    },
    "mackimchy-13357" : {
      "name" : "Ramshot",
      "transportAddress" : "inet[/10.0.0.1:9301]",
      "dataNode" : true,
      "httpAddress" : "inet[/10.0.0.1:9201]"
    }
  }
}
```

Note that it will now wrap the nodes in the "nodes" element for simpler usage.
</comment><comment author="kimchy" created="2010-02-14T19:29:21Z" id="134957">Query http listeners. Closed by b5f3fc9ae1a68a9114acf1ef2bc9bc4d90ad1bea.
</comment><comment author="kimchy" created="2010-02-14T19:36:21Z" id="134960">Ohh, and I added an http parameter to include node settings, just use: http://localhost:9200/_cluster/nodes?settings=true
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize API: Allow to optimize index/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/9</link><project id="" key="" /><description>Provide the ability to optimize an index or indices down to a provided number of segments.
</description><key id="132800">9</key><summary>Optimize API: Allow to optimize index/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-14T18:32:46Z</created><updated>2010-02-20T17:07:42Z</updated><resolved>2010-02-14T18:33:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-14T18:33:37Z" id="134926">Optimize API: Allow to optimize index/indices. Closed by cfafb52bebbd5bb50b4fc74b1aebc121a9e91548.
</comment><comment author="clintongormley" created="2010-02-18T16:52:10Z" id="138569">is this the correct usage for the options?

```
curl -XPOST http://127.0.0.2:9200/_optimize?onlyExpungeDeletes=1&amp;flush=1&amp;refresh=1
```

ta

clint
</comment><comment author="kimchy" created="2010-02-18T21:10:11Z" id="138838">Yes, except that the boolean parameters expect true, and not 1. Ohh wait, just pushed support for 1 as well for http parameters to indicate true :)
</comment><comment author="kimchy" created="2010-02-18T21:18:07Z" id="138846">By the way, I think I am also going to add support to provide all this parameters in the body of the request as a JSON (instead in the query string), what do you think?
</comment><comment author="clintongormley" created="2010-02-19T01:21:27Z" id="139072">about you added support for true / 1 - please can you do that for the JSON as well.  For instance:

```
{ query: {
        filteredQuery: {
                query: {............},
                explain: true      # should accept 1 here as well
}}
```

re adding the params to the JSON instead:

yes, that would make sense to me - for a wrapper like ElasticSearch.pm, it makes little difference having the params in one place or the other - it'd probably be easier to just deal with the JSON.
</comment><comment author="kimchy" created="2010-02-20T12:42:12Z" id="140200">Added support for 0 to represent false in all places where I parse JSON or http parameters. Will work later on getting support for JSON body as well.
</comment><comment author="clintongormley" created="2010-02-20T15:40:40Z" id="140258">Hiya

Just had another thought about this:

&gt; By the way, I think I am also going to add support to provide all this 
&gt; parameters in the body of the request as a JSON (instead in the 
&gt; query string), what do you think?"

One request that wouldn't work so well is:

```
curl -XPUT 'http://127.0.0.2:9200/es_test/type_1/1?opType=create'  -d '
{
   "num" : 2,
   "text" : "foo"
}
'
```

because the entire JSON document represents the document being indexed, so slipping opType=create in there wouldn't work

clint
</comment><comment author="kimchy" created="2010-02-20T17:07:42Z" id="140309">Yes, I am referring currently only to requests that don't have a body. The one mentioned as the index/delete/create and search operations do not fall into this category. I need to think what a solution (if needed) for them will be.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTP: Rest API should support receiving HTTP chunks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8</link><project id="" key="" /><description>For large messages, certain HTTP clients will chunk the requests. Though it is probably better to disable this if possible on the client side, we should still support chunked HTTP messages.
</description><key id="132532">8</key><summary>HTTP: Rest API should support receiving HTTP chunks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-13T21:43:11Z</created><updated>2010-02-13T21:44:43Z</updated><resolved>2010-02-13T21:44:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-13T21:44:43Z" id="134470">HTTP: Rest API should support receiving HTTP. Closed by 5ac51ee93feab6c75fcbe979b9bb338962622c2e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery/Jgroups: Upgrade to 2.9.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7</link><project id="" key="" /><description>Upgrade to latest JGroups release (2.9.0) from the current (2.8.0)
</description><key id="131696">7</key><summary>Discovery/Jgroups: Upgrade to 2.9.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-12T16:08:42Z</created><updated>2010-02-12T16:09:21Z</updated><resolved>2010-02-12T16:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-12T16:09:21Z" id="133535">Discovery/Jgroups: Upgrade to 2.9.0. Closed by 78eaacccefae44e9cabc44216b26175e830cdcf8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Bool query/filter to be valid JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6</link><project id="" key="" /><description>Currently, the bool query is not a valid Javassctipt (still valid JSON though...) since indicating two must clauses uses the same field name for a JSON object. The old way should still be supported, but, we should also allow for something like this:

Currently, the bool query is not a valid Javassctipt (still valid JSON though...) since indicating two must clauses uses the same field name for a JSON object. The old way should still be supported, but, we should also allow for something like this:

```
{
    bool : {
        must : [
            {
                queryString : {
                    defaultField : "content",
                    query : "test1"
                }
            },
            {
                queryString : {
                    defaultField : "content",
                    query : "test4"
                }
            }
        ],
        mustNot: {
            queryString : {
                defaultField : "content",
                query : "test2"
            }
        },
        should: {
            queryString : {
                defaultField : "content",
                query : "test3"
            }
        }
    }
}
```
</description><key id="131616">6</key><summary>Query DSL: Bool query/filter to be valid JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-12T13:20:07Z</created><updated>2010-02-20T00:10:58Z</updated><resolved>2010-02-12T13:24:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="simonw" created="2010-02-12T13:57:04Z" id="133432">It's definitely still invalid JSON - the JSON spec on http://www.json.org/ is clear that double quotes are required around string values, including object keys. In practice it's not a huge problem that elasticsearch accepts invalid JSON for the queries though, provided it also accepts valid JSON. The JSON output by elasticsearch uses quotes in the right places and is absolutely fine.

Python's JSON parser is strict by default, and throws the following exception if I feed in the above example:

ValueError: Expecting property name: line 2 column 5 (char 6)
</comment><comment author="kimchy" created="2010-02-12T14:04:52Z" id="133436">The invalid part I was talking about is the usage of the same field name twice within an object. 

Regarding the quotes on field names, you are absolutely correct. ES does accept field names that are either quoted or not, for two reasons:
1. Less text on the wire / simplifies writing examples :).
2. Makes direct Javascript usage simpler.
</comment><comment author="clintongormley" created="2010-02-19T01:51:33Z" id="139089">Actually, non-unique key names are invalid JSON.  See section 2.2 in http://www.ietf.org/rfc/rfc4627.txt?number=4627

So whenever you have repeatable items, you should provide both:
    { key: value } 
    { keys: [ value_1, value_n ] }
</comment><comment author="clintongormley" created="2010-02-19T13:39:40Z" id="139446">or:

```
{ key: value }  | { key: [ value_1, value_n] }
```
</comment><comment author="kimchy" created="2010-02-20T00:10:58Z" id="139935">They say SHOULD not must :). In any case, I will make sure in the future that the SHOULD, with all its uppercase glory, is maintained :).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Any plans to support faceting?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5</link><project id="" key="" /><description>One of the most useful features of Solr is faced searching - is that likely to be added to elasticsearch at any point?
</description><key id="130952">5</key><summary>Any plans to support faceting?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simonw</reporter><labels /><created>2010-02-11T17:52:36Z</created><updated>2014-02-25T20:22:36Z</updated><resolved>2010-02-12T13:18:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-11T17:57:21Z" id="132525">Can you please send questions to the mailing list? This is not a feature request format, its a question that might lead to a discussion which will end up as a feature request. ES has basic support for facet queries, other types of faces are on the road map.
</comment><comment author="simonw" created="2010-02-11T23:15:35Z" id="132904">Sure, I'll use the mailing list for this kind of thing in the future. Thanks for the replies.
</comment><comment author="kimchy" created="2010-02-12T06:00:37Z" id="133121">Sure, no problem. To be honest, I don't know where best to conduct this. I now see that others actually maintain a mini forums in the issues, I am simply used to mailing lists to have discussions, and issue trackers to have discussions about open bugs or features implementations. But, maybe I am wrong, and this is a better place, what do you think?
</comment><comment author="kimchy" created="2010-02-12T13:18:04Z" id="133383">Closing for now. Will create issues for each new facet support that I plan.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Any plans to add geospatial search?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4</link><project id="" key="" /><description>It would be incredibly useful to be able to index documents with latitude/longitude positions and run searches that return results ordered by distance from a specific latitude/longitude point.
</description><key id="130951">4</key><summary>Any plans to add geospatial search?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simonw</reporter><labels /><created>2010-02-11T17:51:37Z</created><updated>2014-02-25T20:22:36Z</updated><resolved>2010-02-12T13:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-11T17:55:30Z" id="132521">Yes, very much. There is a Lucene spatial module that I need to look at. There are other solutions running around. When I support that, I want that to be very closely integrated in terms that you will have a type: location (or something similar).

Not sure it will make it into 0.5, but its certainly on the short list for 0.6.0.
</comment><comment author="simonw" created="2010-02-11T23:15:59Z" id="132905">Thanks - it's a very exciting project, thanks for releasing it!
</comment><comment author="kimchy" created="2010-02-12T13:17:45Z" id="133382">I will close this for now, I will add an issue that says "Spatial Support" later on 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: Support local (JVM level) transport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3</link><project id="" key="" /><description>Allow to have a JVM (well, actually class loader) level transport for simple testing / embedding of a single node (which, potentially exists with other nodes in the same class loader).

Enable it using:

```
transport:
    type: local
```

Or using:

```
node:
    local: true
```

(which will also enable other modules to be local, such as the discovery)
</description><key id="130939">3</key><summary>Transport: Support local (JVM level) transport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-11T17:28:41Z</created><updated>2010-02-11T17:29:31Z</updated><resolved>2010-02-11T17:29:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-11T17:29:31Z" id="132485">Transport: Support local (JVM level) transport. Closed by 847db717c66509a817e1b965226ee1e44c08918d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: Support local (JVM level) discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2</link><project id="" key="" /><description>Allow to have a JVM (well, actually class loader) level discovery for simple testing / embedding of a single node (which, potentially exists with other nodes in the same class loader).

Enable it using:

```
discovery:
    type: local
```

Or using:

```
node:
    local: true
```

(which will also enable other modules to be local, such as the transport - once we have that...)
</description><key id="130149">2</key><summary>Discovery: Support local (JVM level) discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.05.0</label></labels><created>2010-02-10T22:10:55Z</created><updated>2014-08-08T07:12:52Z</updated><resolved>2010-02-10T22:12:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-10T22:12:58Z" id="131427">Discovery: Support local (JVM level) discovery. Closed by b61964a2b8a4f6465928efcb5b1b434dbbb6b1a5.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Terms Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1</link><project id="" key="" /><description>Support terms filter which allows to configure more than one term for a specific field. For example:

```
{
    filteredQuery : {
        query : {
            term : { "name.first" : "shay" }
        },
        filter : {
            terms : {
                "name.last" : ["banon", "kimchy"]
            }
        }
    }
}
```
</description><key id="129156">1</key><summary>Query DSL: Terms Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.05.0</label></labels><created>2010-02-09T20:08:08Z</created><updated>2014-08-08T07:12:52Z</updated><resolved>2010-02-09T20:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2010-02-09T20:19:00Z" id="130104">Support terms filter, closed by bd2b0a632bfc5aabb408e7f47cfaa52a7d1b2b50
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>