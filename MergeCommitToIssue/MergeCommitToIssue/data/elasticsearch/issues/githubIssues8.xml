<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title> Add fromXContent method to HighlightBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15157</link><project id="" key="" /><description>For the search refactoring the HighlightBuilder needs a way to create new instances by parsing xContent. For bwc this PR start by moving over and slightly modifying the parsing from HighlighterParseElement and keeps parsing for top level highlighter and field options separate. Also adding tests for roundtrip of random builder (rendering it to xContent and parsing it and making sure the original builder properties are preserved).
</description><key id="119766308">15157</key><summary> Add fromXContent method to HighlightBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T17:22:06Z</created><updated>2015-12-08T18:39:01Z</updated><resolved>2015-12-08T18:39:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-01T17:24:45Z" id="161039536">There's some weirdness going on with the first two commits I see, the diff looks okay though. Will try to squash this somehow.
Edit: looks better now.
</comment><comment author="colings86" created="2015-12-07T11:14:27Z" id="162487889">@cbuescher This is looking good so far, I left a few comments
</comment><comment author="cbuescher" created="2015-12-07T16:16:28Z" id="162573319">@colings86 thanks for the review, I added commits that switch to using ParseFields in HighlightBuilder and added checks for unknown field names in the parser code (only in HighlightBuilder since the HighlightParseElement will be removed at some point in the future as far as I understand). Would be glad if you could take a second look.
</comment><comment author="cbuescher" created="2015-12-08T11:28:29Z" id="162855566">Hi @colings86, anything else to add here after ParseFields and additional exceptions?
</comment><comment author="colings86" created="2015-12-08T14:49:24Z" id="162904709">Left a couple of comments but otherwise LGTM
</comment><comment author="cbuescher" created="2015-12-08T15:01:59Z" id="162908172">Thanks for the review, added the checks for unexpected token while parsing, will merge in a bit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't treat _default_ as a regular type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15156</link><project id="" key="" /><description>This adds safety that you can't index into the `_default_` type (it was possible
before), and can't add default mappers to the field type lookups (was not
happening in tests but I think this is still a good check).

Also MapperService.types() now excludes `_default` so that eg. the `ids` query
does not try to search on this type anymore.

Related to #15049
</description><key id="119763375">15156</key><summary>Don't treat _default_ as a regular type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T17:07:57Z</created><updated>2015-12-02T10:15:59Z</updated><resolved>2015-12-01T17:14:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-01T17:12:43Z" id="161036358">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use doc_values for streaming _uid / _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15155</link><project id="" key="" /><description>This issue relates heavily to #11887.

In many use cases, there is frequently the need to stream all (or many) `_id`s from Elasticsearch to munge them together with some other data set. In these cases, Elasticsearch is often the actual search platform, but something else is acting as the source of "truth" or a more complete representation of the data (as oppose to just what is indexed to make search work). For example imagine a scroll request that disables `_source`:

``` json
{
  "_source" : false,
  "query" : { ... }
}
```

For these use cases, it's not uncommon to want to stream literally more than 50K+ document IDs per second (aka as fast as possible). However, in practice, there is a bottleneck on streaming `_id`s due to the need to fetch the _stored_ `_uid` field, decompress it, split it into `_id`, then finally serialize it as part of the response. If the aforementioned issue is merged, then we can use doc_values in order to stream these values from disk more efficiently in this use case.

Note: It may be worthwhile to consider this for other use cases where source filtering is enabled and _all_ of the selected fields exist in doc values, especially if the user supplies the list of fields using `fielddata_fields`.
</description><key id="119757031">15155</key><summary>Use doc_values for streaming _uid / _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Scroll</label><label>discuss</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-12-01T16:39:10Z</created><updated>2015-12-11T10:46:44Z</updated><resolved>2015-12-11T10:46:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T22:02:51Z" id="161447755">&gt; Note: It may be worthwhile to consider this for other use cases where source filtering is enabled and all of the selected fields exist in doc values, especially if the user supplies the list of fields using fielddata_fields.

For the record, this optimization to go to doc values is only safe if there is a single field to fetch. Otherwise it could be much slower than going to stored fields if the index size is much larger than the amount of free RAM on the server running elasticsearch.
</comment><comment author="clintongormley" created="2015-12-11T10:46:44Z" id="163906828">This streaming use can can be implemented client side today by using another field to contain the ID and setting it to use doc values.  The question of whether _uid should have doc values and what format they should be in is a different one, which I think we should continue discussing on https://github.com/elastic/elasticsearch/issues/11887
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>changing REST integration test command line.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15154</link><project id="" key="" /><description>fixed #15124.
</description><key id="119756287">15154</key><summary>changing REST integration test command line.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels><label>docs</label></labels><created>2015-12-01T16:36:04Z</created><updated>2015-12-02T10:16:38Z</updated><resolved>2015-12-01T17:19:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-01T16:49:40Z" id="161028305">Left a small comment, otherwise LGTM.
</comment><comment author="andrejserafim" created="2015-12-01T17:04:48Z" id="161033423">Changed.
</comment><comment author="jasontedor" created="2015-12-01T17:20:08Z" id="161038360">Integrated to master in 9015d0ca73d8b7cfff6c839b643ab3367e411531. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate the `missing` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15153</link><project id="" key="" /><description>Deprecate the missing filter in favour of using an exists filter, which works also in a nested context, where the user can place the negation where appropriate.
Fixes #14112
</description><key id="119755657">15153</key><summary>Deprecate the `missing` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>review</label><label>v2.2.0</label></labels><created>2015-12-01T16:33:16Z</created><updated>2016-02-08T09:08:51Z</updated><resolved>2015-12-07T13:22:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T09:58:40Z" id="161243718">This looks good to me.

@clintongormley question for you: do you think we are soon enough in the 2.x release cycle so that the `missing` query gets removed in 3.0, or should we keep it in 3.0 and only remove in 4.0?
</comment><comment author="rjernst" created="2015-12-02T14:56:18Z" id="161322845">I don't think where we are within a major release cycle should matter. Making breaking changes in a major release has a to be possible, otherwise changes have to be delayed, as master is the only branch that such changes can be made on or wedont have a branch for the next major release after master).
</comment><comment author="jpountz" created="2015-12-02T14:57:46Z" id="161323591">@clintongormley told me as well he'd like to get rid of it in 3.0 but also suggested that we make use of the deprecation logger to warn users about the deprecation so that it is less likely to come as a surprise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable copy_to in multi_fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15152</link><project id="" key="" /><description>This is just a poc for #14946 . I am opening this pull request so that we can discuss better. 

I tried re enabling `copy_to` functionality for `multi_fields` and it seems to me this is just a matter of shifting three lines. I have not thought about it much though yet and might be missing something. However, tests with this branch pass.
</description><key id="119753516">15152</key><summary>Enable copy_to in multi_fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>:Plugin Mapper Attachment</label><label>discuss</label></labels><created>2015-12-01T16:23:39Z</created><updated>2015-12-02T13:39:20Z</updated><resolved>2015-12-02T13:39:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-02T13:39:20Z" id="161292207">Closing. See https://github.com/elastic/elasticsearch/issues/14946#issuecomment-161263824
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support wildcards for getting repositories and snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15151</link><project id="" key="" /><description>Supports the following:

```
GET /_snapshot/*
GET /_snapshot/repo*
GET /_snapshot/repo/*
GET /_snapshot/repo/prefix*
```

What is not supported:

```
GET /_snapshot/*/snap
GET /_snapshot/repo*/snap*
```

Relates to #4758
</description><key id="119738002">15151</key><summary>Support wildcards for getting repositories and snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T15:16:25Z</created><updated>2015-12-11T09:36:28Z</updated><resolved>2015-12-11T09:36:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-01T15:28:51Z" id="161001615">Wondering if it requires some documentation changes?
</comment><comment author="ywelsch" created="2015-12-01T15:37:34Z" id="161003939">you're right, I'll add some docs.
</comment><comment author="ywelsch" created="2015-12-01T16:00:13Z" id="161012770">@dadoonet I've added documentation. Could you review this PR?
</comment><comment author="dadoonet" created="2015-12-01T16:08:11Z" id="161014957">Left a very small comment about test. It looks good to me. I'd prefer that @bleskes double checks though.
</comment><comment author="imotov" created="2015-12-10T21:50:07Z" id="163759525">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move IndicesService.canDeleteShardContent to use IndexSettings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15150</link><project id="" key="" /><description>Just a minor cleanup/simplification

Closes #15059
</description><key id="119732075">15150</key><summary>Move IndicesService.canDeleteShardContent to use IndexSettings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T14:50:53Z</created><updated>2015-12-03T09:09:40Z</updated><resolved>2015-12-03T09:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T10:00:03Z" id="161244009">LGTM
</comment><comment author="ywelsch" created="2015-12-02T14:33:32Z" id="161316814">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>compile against compact3 profile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15149</link><project id="" key="" /><description>This cuts the size of the JRE in half (see usages like https://github.com/delitescere/docker-zulu)

Users will probably just expect this works and file bugs if it doesn't
</description><key id="119730514">15149</key><summary>compile against compact3 profile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-12-01T14:42:49Z</created><updated>2015-12-01T19:07:34Z</updated><resolved>2015-12-01T19:07:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-01T14:50:43Z" id="160988876">LGTM. The [JEP 161](http://openjdk.java.net/jeps/161) and [Oracle's overview](http://www.oracle.com/technetwork/java/embedded/resources/tech/compact-profiles-overview-2157132.html) are also useful resources, and this should be revisited after Project Jigsaw.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API will not responding if the analyzer or the tokenizer is not exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15148</link><project id="" key="" /><description>elasticsearch v2.1.0

`http://127.0.0.1:9200/_analyze?text=13ab2abcd&amp;analyzer=keyword1`
`http://127.0.0.1:9200/_analyze?text=13ab2abcd&amp;tokenizer=keyword1`
`http://127.0.0.1:9200/_analyze?text=13ab2abcd&amp;tokenizer=keyword&amp;token_filters=keyword1`

the `keyword1` is not exist,if you use them for testing,the server will not responding.
error in the console is:

`
[2015-12-01 21:10:05,347][ERROR][transport                ] [Crossbones] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@64daa69a]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-12-01 21:11:06,503][ERROR][transport                ] [Crossbones] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@5560501]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-12-01 21:12:04,471][ERROR][transport                ] [Crossbones] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@26a7dd07]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
`
</description><key id="119715534">15148</key><summary>Analyze API will not responding if the analyzer or the tokenizer is not exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">medcl</reporter><labels><label>:Analysis</label><label>adoptme</label><label>bug</label></labels><created>2015-12-01T13:19:20Z</created><updated>2016-01-22T18:29:43Z</updated><resolved>2015-12-22T01:45:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="PhaedrusTheGreek" created="2015-12-08T19:11:32Z" id="162985242">This also happens if the analyzer is attached to a specific index, and the API is called out of context:

This works:

```
GET /my_index/_analyze
{
  "analyzer" : "some_analyzer",
  "text" : "3 bedroom"
}
```

This hangs, and throws a java.lang.NullPointerException

```
GET /_analyze
{
  "analyzer" : "some_analyzer",
  "text" : "3 bedroom"
}
```
</comment><comment author="yangaoquan" created="2015-12-09T10:40:33Z" id="163184436">@PhaedrusTheGreek answer was proven correct.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Bucket Selector Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15147</link><project id="" key="" /><description /><key id="119712745">15147</key><summary>Aggregations Refactor: Refactor Bucket Selector Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-01T13:00:30Z</created><updated>2015-12-07T11:00:19Z</updated><resolved>2015-12-07T10:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-01T14:14:26Z" id="160980480">Looks good to me, left one question and a general remark concerning the way reading/writing objects is spread across abstract classes and implementations, but this is something than can be revisited later. 
</comment><comment author="jpountz" created="2015-12-02T10:02:49Z" id="161245137">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistency: keyAsString for terms aggregation on boolean field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15146</link><project id="" key="" /><description>Just a minor thing, but still...
When performing a terms aggregation on a boolean field via sense, I get something like this for the `false` bucket

```
    ...
    "key": 0,
    "key_as_string": "false",
    ...
```

When using Java, `bucket.getKey()` returns `0L` (I would rather expect a Boolean than a Long as the returned Object - something that is true for both APIs), but at least it is the same.
However, `bucket.getKeyAsString()` returns `"0"` instead of `"false"` (as the Long is converted and not a Boolean).

In my Opinion, `"key_as_string"` and `getKeyAsString()` should return the same result.
</description><key id="119709284">15146</key><summary>Inconsistency: keyAsString for terms aggregation on boolean field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DavidHauger</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-12-01T12:43:01Z</created><updated>2016-11-02T07:29:15Z</updated><resolved>2016-01-28T12:26:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-01T12:48:43Z" id="160957607">@colings86 what do you think?
</comment><comment author="colings86" created="2015-12-01T13:07:51Z" id="160964700">@clintongormley I agree that `"key_as_string"` and `getKeyAsString()` should be returning the same result. Maybe we should properly support Boolean fields in the Terms Aggregation and have is return `true` and `false` for `"key"` and `getKey()` and `"true"` and `"false"` for `"key_as_string"` and `getKeyAsString()`? /cc @jpountz 
</comment><comment author="jpountz" created="2015-12-02T22:08:41Z" id="161449150">+1 on returning "true"/"false" in getKeyAsString. However I think getKey should still return 0/1. This is consistent with how we handle other numeric-based types such as ipv4 addresses or dates.
</comment><comment author="colings86" created="2015-12-11T10:52:31Z" id="163907776">Discussed in FixItFriday and decided that we should fix the inconsistency here, and maybe we will add support for Boolean later so the key is returned as a Boolean rather than 0/1
</comment><comment author="jimczi" created="2015-12-11T14:47:27Z" id="163953051">The same reasoning can be made for date field where key_as_string contains the date and getKeyAsString returns the number of milliseconds since epoch in a string. The downside (for both date and boolean) is that getBucketByKey relies on getKeyAsString so any change here would affect this function.
</comment><comment author="jimczi" created="2016-01-28T12:26:26Z" id="176155173">Closed via https://github.com/elastic/elasticsearch/pull/15393
</comment><comment author="clintongormley" created="2016-01-28T12:27:56Z" id="176155711">@jimferenczi does that PR also deal with dates?
</comment><comment author="jimczi" created="2016-01-28T12:32:13Z" id="176157012">@clintongormley booleans and dates yes.
</comment><comment author="clintongormley" created="2016-01-28T12:33:44Z" id="176157439">thanks @jimferenczi 
</comment><comment author="qiuranke" created="2016-11-02T07:26:56Z" id="257791343">date type not fixed on 2.4.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchFieldsTests.testLoadMetadata fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15145</link><project id="" key="" /><description>SearchFieldsTests.testLoadMetadata fails in current master with the following error. It reproduced only once for me, tried running the test multiple times with no luck. I am thinking it may be related to recent mapping changes, also wondering if #15142 might fix this issue. Seems an actual problem though rather than a test bug. @jpountz @jasontedor can either of you please have a look and assign this?

```
java.lang.AssertionError: 
Expected: an empty iterable
     but: [&lt;java.lang.IllegalArgumentException: can't add a _parent field that points to an already existing type&gt;]
    at __randomizedtesting.SeedInfo.seed([2723CD59178F56E2:30964EA8729EAE09]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1433)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1353)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1337)
    at org.elasticsearch.test.ESIntegTestCase.indexRandom(ESIntegTestCase.java:1313)
    at org.elasticsearch.messy.tests.SearchFieldsTests.testLoadMetadata(SearchFieldsTests.java:663)
```

http://build-us-00.elastic.co/job/es_g1gc_master_metal/25993/testReport/junit/org.elasticsearch.messy.tests/SearchFieldsTests/testLoadMetadata/
</description><key id="119698282">15145</key><summary>SearchFieldsTests.testLoadMetadata fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">javanna</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-12-01T11:33:19Z</created><updated>2015-12-23T18:25:36Z</updated><resolved>2015-12-23T18:25:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-01T11:35:39Z" id="160942300">Same failure, but different test (ChildQuerySearchIT.testScoreForParentChildQueriesWithFunctionScore): http://build-us-00.elastic.co/job/es_core_master_window-2012/2083/testReport/junit/org.elasticsearch.search.child/ChildQuerySearchIT/testScoreForParentChildQueriesWithFunctionScore/
</comment><comment author="javanna" created="2015-12-01T11:36:14Z" id="160942399">And another one (ChildQuerySearchIT.testCachingBugWithFqueryFilter): http://build-us-00.elastic.co/job/es_core_master_suse/2614/testReport/junit/org.elasticsearch.search.child/ChildQuerySearchIT/testCachingBugWithFqueryFilter/
</comment><comment author="jpountz" created="2015-12-01T12:34:39Z" id="160955309">I'm in the process of doing significant refactorings of how mappings are validated so I'll take this one.
</comment><comment author="jpountz" created="2015-12-23T18:25:36Z" id="166963587">These failures have not reproduced recently.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't ignore mapping merge failures.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15144</link><project id="" key="" /><description>Related to #15049
</description><key id="119696717">15144</key><summary>Don't ignore mapping merge failures.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T11:23:31Z</created><updated>2015-12-02T10:11:58Z</updated><resolved>2015-12-01T15:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-01T15:35:49Z" id="161003482">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.net.SocketException: Protocol family unavailable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15143</link><project id="" key="" /><description>I am trying to upgrade elastic search 1.x to 2.1 , getting below error.

i tried all these settings but it's still failing.

To bind to all IPv6/IPv4 addresses, you can use

network.bind_host: "0"
network.bind_host: "::"

To bind to IPv4 loopback (localhost) only

network.bind_host: "0.0.0.0"
together with JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses"

ERROR :

[2015-11-29 11:55:55,866][WARN ][transport.netty ] [Node1] exception caught on transport layer [[id: 0xfa429912]], closing connection
java.net.SocketException: Protocol family unavailable
at sun.nio.ch.Net.connect0(Native Method)
at sun.nio.ch.Net.connect(Net.java:435)
at sun.nio.ch.Net.connect(Net.java:427)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:643)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
at org.jboss.netty.channel.Channels.connect(Channels.java:634)
at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:216)
at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:913)
at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:880)
at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:852)
at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:250)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:395)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
[2015-11-29 11:55:57,361][WARN ][transport.netty ] [Node1] exception caught on transport layer [[id: 0xe44a1084]], closing connection
java.net.SocketException: Protocol family unavailable
at sun.nio.ch.Net.connect0(Native Method)
at sun.nio.ch.Net.connect(Net.java:435)
at sun.nio.ch.Net.connect(Net.java:427)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:643)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
at org.jboss.netty.channel.Channels.connect(Channels.java:634)
at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:216)
at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:913)
at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:880)
at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:852)
at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:250)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:395)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
[2015-11-29 11:55:57,836][INFO ][node ] [Node1] stopping ...
[2015-11-29 11:55:57,857][INFO ][node ] [Node1] stopped
[2015-11-29 11:55:57,857][INFO ][node ] [Node1] closing ...
[2015-11-29 11:55:57,861][INFO ][node ] [Node1] closed
</description><key id="119696534">15143</key><summary>java.net.SocketException: Protocol family unavailable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shashidharrao</reporter><labels><label>:Network</label><label>feedback_needed</label></labels><created>2015-12-01T11:22:24Z</created><updated>2016-10-06T11:21:06Z</updated><resolved>2015-12-01T20:32:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-01T11:53:34Z" id="160945522">I get the above exception when I set `bind_host` to `::` but specify `-Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses`.  With the preferIPv4, I can set `bind_host` to `0` or  `0.0.0.0` and it works.

By default, without setting anything, Elasticsearch will bind to localhost on IPv4 and IPv6 (whatever is available). But it won't bind to a public IP address unless you specifically tell it to.

What is it that you are trying to do? What OS are you on? Do you have IPv4? Do you have IPv6?
</comment><comment author="shashidharrao" created="2015-12-01T15:08:58Z" id="160995674">I am using IPv4 and linux 64 bit . I am trying to configure 3 ES nodes on one physical box. 
the old version of ES works fine without any issues. now i am upgrading it to latest ES. 

$ uname -a
Linux **********l 2.6.18-371.9.1.el5 #1 SMP Tue May 13 06:52:49 EDT 2014 x86_64 x86_64 x86_64 GNU/Linux

$ cat /etc/redhat-release
Red Hat Enterprise Linux Server release 5.10 (Tikanga)
</comment><comment author="shashidharrao" created="2015-12-01T15:23:59Z" id="161000133">I exported the java options in the elasticserach startup script like below, JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses"
export JAVA_OPTIONS

and tried three options each time, but no luck facing the same issue.  

#network.host: 0
#network.bind_host: 0.0.0.0
#network.bind_host: 0 
</comment><comment author="rmuir" created="2015-12-01T15:24:59Z" id="161000441">FYI: java.net.preferIPv4Addresses does absolutely nothing. this is not a recognized property by java.
</comment><comment author="shashidharrao" created="2015-12-01T16:45:58Z" id="161027250">I added in the 

# Force the JVM to use IPv4 stack

if [ "x$ES_USE_IPV4" != "x" ]; then
  JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses"
fi
</comment><comment author="rmuir" created="2015-12-01T16:55:39Z" id="161029983">Like i said, `-Djava.net.preferIPv4Addresses` does nothing, never did do anything. Maybe you mean `-Djava.net.preferIPv6Addresses`, which actually does something.

See https://docs.oracle.com/javase/7/docs/api/java/net/doc-files/net-properties.html
</comment><comment author="shashidharrao" created="2015-12-01T17:07:48Z" id="161034739">both options are not helping here, still getting the same error.

[2015-12-01 09:06:04,277][WARN ][transport.netty          ] [Node1] exception caught on transport layer [[id: 0x686bdc61]], closing connection
java.net.SocketException: Protocol family unavailable
        at sun.nio.ch.Net.connect0(Native Method)
        at sun.nio.ch.Net.connect(Net.java:435)
        at sun.nio.ch.Net.connect(Net.java:427)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:643)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
        at org.jboss.netty.channel.Channels.connect(Channels.java:634)
        at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:216)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:913)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:880)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:852)
        at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:250)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:395)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-01 09:06:05,769][WARN ][transport.netty          ] [Node1] exception caught on transport layer [[id: 0x79b2e8d7]], closing connection
java.net.SocketException: Protocol family unavailable
        at sun.nio.ch.Net.connect0(Native Method)
        at sun.nio.ch.Net.connect(Net.java:435)
        at sun.nio.ch.Net.connect(Net.java:427)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:643)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
        at org.jboss.netty.channel.Channels.connect(Channels.java:634)
        at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:216)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:913)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:880)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:852)
        at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:250)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:395)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-01 09:06:07,079][INFO ][node             
</comment><comment author="shashidharrao" created="2015-12-01T20:32:22Z" id="161087058">closing the ticket. Some miss configuration from my side causing this issue. thank you. 
</comment><comment author="ewpeters" created="2016-03-02T02:38:52Z" id="191024755">What did you do to fix it?????
</comment><comment author="stardust85" created="2016-10-06T11:21:05Z" id="251934852">I got a similar message with cassandra when changing something in /etc/hosts, /etc/sysconfig/network and /etc/resolv.conf
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Treat mappings at an index-level feature.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15142</link><project id="" key="" /><description>Today we try to have type-level granularity when dealing with mappings. This
does not play well with the cross-type validations that we are adding. For
instance we prevent the `_parent` field to point to an existing type. This
validation would be skipped today in the case of dedicated master nodes, since
those master nodes would only create the type that is being updated when
updating a mapping.
</description><key id="119692691">15142</key><summary>Treat mappings at an index-level feature.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T10:56:56Z</created><updated>2015-12-03T13:11:37Z</updated><resolved>2015-12-02T15:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-01T13:45:26Z" id="160974212">left some comments.
</comment><comment author="bleskes" created="2015-12-02T11:13:15Z" id="161263845">LGTM. Thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support global `repositories.azure.` settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15141</link><project id="" key="" /><description>All those repository settings can also be defined globally in `elasticsearch.yml` file using prefix `repositories.azure.`. For example:

``` yml
repositories.azure:
    container: backup-container
    base_path: backups
    chunk_size: 32m
    compress": true
```

Closes #13776.
</description><key id="119689012">15141</key><summary>Support global `repositories.azure.` settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T10:36:38Z</created><updated>2015-12-29T09:46:31Z</updated><resolved>2015-12-29T09:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-01T10:37:04Z" id="160929387">@imotov or @tlrx Wanna review it please?
</comment><comment author="imotov" created="2015-12-10T21:59:49Z" id="163761736">Left one comment about filtering out the key setting. Otherwise, LGTM.
</comment><comment author="dadoonet" created="2015-12-16T08:09:40Z" id="165027778">@imotov I added a comment. Not sure you saw it so I'm pinging you. :D 
</comment><comment author="dadoonet" created="2015-12-18T13:59:30Z" id="165784239">@imotov I added another commit. Could you review please? Just to make sure it's correct before I push it.
</comment><comment author="imotov" created="2015-12-22T17:17:40Z" id="166678384">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kibana not showing indices screen after new install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15140</link><project id="" key="" /><description>I'm new to ELK. 
I installed ELK on Ubuntu. All seems to be up and running (ES, nginx, Kibana, Logstash) 
But, when I browse to Kibana, after authenticating on nginx, the browser displays only the upper header: Kibana logo + Discover / Visualize / Dashboard / Settings , without the indices info (sse attachment.)
Any idea what went wrong?

![kibana ui](https://cloud.githubusercontent.com/assets/16098342/11498031/8b99d7b8-9825-11e5-9af3-0e2f501a27f8.jpg)
</description><key id="119686533">15140</key><summary>Kibana not showing indices screen after new install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haium</reporter><labels /><created>2015-12-01T10:20:33Z</created><updated>2015-12-01T10:30:47Z</updated><resolved>2015-12-01T10:30:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-01T10:30:47Z" id="160928205">Please join us on discuss.elastic.co. We can help you there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Processing pending deletes should rely on index UUID instead of index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15139</link><project id="" key="" /><description>Relates to #14932 which shows that processing pending deletes can block shard initialization for 30 minutes. The core issue is that processing pending deletes (which is an asynchronous operation) is based on index name instead of UUID. If a new index with same name has been recreated in the meantime, we run into some issues.

As a fix, I suggest the following changes:
- Pass index UUID when requesting ShardLock. If uuid do not match with uuid of currently held ShardLock for that shard, throw LockObtainFailedException. We use the special wildcard "null" to match any uuid. This is useful if we don't know what the index UUID is (fallback to the old behavior).
- pendingDeletes map is indexed by index uuid, not index name anymore. This means that we process only the pending deletes for the specific UUID.
- processPendingDeletes should not request all, but as many ShardLocks as possible. If only subset is locked, then index directory should NOT be deleted. Same as before, it should only delete directories for which it has lock.

Note that it is still unsafe in the following case: Let's assume we recreate index with more shards. If the given machine only gets the shard with number higher than any number in old index, then processPendingDeletes might still delete the index directory.
</description><key id="119677343">15139</key><summary>Processing pending deletes should rely on index UUID instead of index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Store</label><label>discuss</label><label>WIP</label></labels><created>2015-12-01T09:37:57Z</created><updated>2015-12-01T10:53:29Z</updated><resolved>2015-12-01T10:53:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-01T10:53:29Z" id="160933660">After discussion with @bleskes and @s1monw we decided to fix this as part of a larger refactoring of the store/locking infrastructure. Closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using dynamic_date_format across multiple types causes errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15138</link><project id="" key="" /><description>First reported on discuss forum: https://discuss.elastic.co/t/mapping-problem-with-dynamic-date-formats-es-2-0/35955

If you set `dynamic_date_formats` in the mappings and then try to index two documents in separate types and error is raised on the second index request even though the date format on the second document is the same so no mapping update should be necessary.

```
$ curl -XPUT http://localhost:9200/myIndex -d '{"mappings": {"_default_": 
    { "dynamic_date_formats": ["yyyy-MM-dd"]}
}}'

$ curl -XPOST http://localhost:9200/myIndex/type1 -d '{"myDate":"2015-01-01"}'

$ curl -XPOST http://localhost:9200/myIndex/type2 -d '{"myDate":"2015-01-02"}'
```

Stack Trace:

```
[2015-12-01 08:54:41,115][INFO ][rest.suppressed          ] /my-index/type2 Params: {update_all_types=, index=my-index, type=type2}
java.lang.IllegalArgumentException: Mapper for [myDate] conflicts with existing mapping in other types:
[mapper [myDate] is used by multiple types. Set update_all_types to true to update [format] across all types.]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:109)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:317)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:268)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:214)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:334)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:252)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

```
</description><key id="119670829">15138</key><summary>Using dynamic_date_format across multiple types causes errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2015-12-01T09:01:26Z</created><updated>2016-01-10T20:06:17Z</updated><resolved>2015-12-21T12:32:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T12:32:56Z" id="166294812">I could reproduce the bug. This is due to how we handle dynamic mapping updates when a field already exists in another type. Closing in favour of #15568
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bad formatting at compression section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15137</link><project id="" key="" /><description>This [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#_static_index_settings) has some weird formatting happening.

What it looks like;
![screen shot 2015-12-01 at 17 24 33 pm](https://cloud.githubusercontent.com/assets/3184718/11493797/7195e408-9850-11e5-87bb-d085c65f58f3.jpg)

And when I mouse over the `[experimental]` bit;
![screen shot 2015-12-01 at 17 24 37 pm](https://cloud.githubusercontent.com/assets/3184718/11493801/7671190c-9850-11e5-9ac3-b8691522f72c.jpg)
</description><key id="119651053">15137</key><summary>Bad formatting at compression section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/palecur/following{/other_user}', u'events_url': u'https://api.github.com/users/palecur/events{/privacy}', u'organizations_url': u'https://api.github.com/users/palecur/orgs', u'url': u'https://api.github.com/users/palecur', u'gists_url': u'https://api.github.com/users/palecur/gists{/gist_id}', u'html_url': u'https://github.com/palecur', u'subscriptions_url': u'https://api.github.com/users/palecur/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1779279?v=4', u'repos_url': u'https://api.github.com/users/palecur/repos', u'received_events_url': u'https://api.github.com/users/palecur/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/palecur/starred{/owner}{/repo}', u'site_admin': False, u'login': u'palecur', u'type': u'User', u'id': 1779279, u'followers_url': u'https://api.github.com/users/palecur/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-12-01T06:25:24Z</created><updated>2015-12-02T12:25:01Z</updated><resolved>2015-12-02T12:25:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added a new scripting language (PlanA)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15136</link><project id="" key="" /><description>The initial commit into ElasticSearch for the PlanA language.

Uses ANTLR to do lexing/parsing of a simple, limited subset of groovy-like scripts and converts them into Java byte code using ASM.

Initial features include the following:
- Types including byte, short, char, int, long, float, double, Byte, Short, Character, Integer, Long, Float, Double, basic support for Lists and Maps
- Dynamic Type including def
- Ability to cast
- Mathematical operators including +, -, /, *, %
- Bitwise operators including ~, &amp;, |, ^, &lt;&lt;, &gt;&gt;, &gt;&gt;&gt;
- Boolean operators including !, &amp;&amp;, ||, ^
- Comparison operators including &lt;, &gt;, &lt;=, &gt;=, == (does a safe .equals comparison), !=, === (does a reference comparison), !==
- Assignments and compound assignments
- Control flow including if/else, while, do/while, for, return
- Shortcuts for lists/maps using both the . operator and the [] operator
- Automatic return for last value on the stack

TODO:
1) Documentation
2) More Tests
3) Fill out the API with more basic classes/methods.

Closes #13084
</description><key id="119649846">15136</key><summary>Added a new scripting language (PlanA)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>feature</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T06:12:19Z</created><updated>2016-02-01T21:03:57Z</updated><resolved>2015-12-10T01:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-09T19:27:05Z" id="163364965">@jdconrad et.al. I am so damn impressed by this it's not funny. I can start going over this but really I won't comment much but nit picks and bikeshedding. I think you guys spend a lot of time getting this done and the fact that you opened this PR is more like a "we are ready to merge". There will be bugs for sure but we should try to get our hands dirty and get it into master. I am looking forward to get users going on this even if it's experimental.
</comment><comment author="nik9000" created="2015-12-09T19:30:51Z" id="163365937">&gt; I am looking forward to get users going on this even if it's experimental.

Yeah. Honestly if the build passes I'm pretty happy to get it in as an experimental plugin and find problems later. You've obviously done a lot with it already. I'm happy to review the code soonish if you really want another set of eyes on it before merging but I'm not likely to propose anything huge. And if I do it'd probably not be something that'd block merging, just something to do in the future.
</comment><comment author="rmuir" created="2015-12-09T19:40:29Z" id="163368563">A practical matter of having this stuff in the repo here is more eyes on it. I'm currently a big bottleneck for @jdconrad as far as code reviews and so on go in the existing external repository, its just too isolated and we all have too much going on.

As a plugin (even in an experimental state), I think it will get more attention.
</comment><comment author="jdconrad" created="2015-12-09T19:43:43Z" id="163369348">@simonw @nik9000 Thank you both for the kind feedback.  I would be very happy getting this in as is, as long as it's marked experimental.  It's going to take a while to stabilize, and I'm sure there will need to be code clean up, a lot more tests, documentation, etc., but these are all things that are an ongoing process and don't necessarily have to be tied to this specific review.
</comment><comment author="nik9000" created="2015-12-09T19:50:11Z" id="163371071">&gt; as long as it's marked experimental.

If you are pushing it to master only then it is experimental for a few months by default. I imagine there are some missing docs to be added that should mention it as experimental but you can send another PR for that if you want. Or bundle them into this one.
</comment><comment author="jdconrad" created="2015-12-10T00:27:38Z" id="163446369">@nik9000 I'm going to opt to add the docs in a second PR in a day or two (I'm giving a demo tomorrow, so I may get delayed with that).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce distance in distance query is &gt; 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15135</link><project id="" key="" /><description>We got a test failure when the randomly generated distance was 0 for a geo distance query:
http://build-us-00.elastic.co/job/es_g1gc_master_metal/25980/testReport/junit/org.elasticsearch.messy.tests/GeoDistanceTests/testDuelOptimizations/

In addition to fixing the test, I think we should validate the distance passed into the query builder is &gt; 0, so that the error message can reference the `distance` parameter, instead of the current error message which may be confusing since "radiusMeters" is an inner parameter to a libarary the user does not know about.
</description><key id="119646202">15135</key><summary>Enforce distance in distance query is &gt; 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Geo</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T05:40:38Z</created><updated>2015-12-01T13:07:59Z</updated><resolved>2015-12-01T13:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>primary shard of stable index become unassigned when other indices trigger TooManyOpenFiles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15134</link><project id="" key="" /><description>in ES version 1.7.3, when cluster is green and another index is created, then triggres too many open files error, some primary shards of online indices which were stable become unassigned, and then those shards become disabled and cluster reroute API also does not work for those shards.
</description><key id="119641862">15134</key><summary>primary shard of stable index become unassigned when other indices trigger TooManyOpenFiles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2015-12-01T04:54:59Z</created><updated>2015-12-01T11:18:46Z</updated><resolved>2015-12-01T11:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sweetest" created="2015-12-01T05:24:15Z" id="160857431">And also I'd like to ask you if it's possible to recover those disabled shards without losing data.
</comment><comment author="sweetest" created="2015-12-01T05:39:30Z" id="160858881">I tried to reroute that shard to where the actual data exists, with allow_primary option set to true, but resulted in ES creating a new empty shards and deleting all existing data.

```
POST /_cluster/reroute
{
   "commands": [
      {
         "allocate": {
            "index": "data-2015-12-01",
            "shard": 1,
            "node": "node03",
            "allow_primary": true
         }
      }
   ]
}
```
</comment><comment author="sweetest" created="2015-12-01T06:28:06Z" id="160868592">this is the result I got from POST /_cluster/reroute?explain&amp;pretty

```
   "explanations": [
      {
         "command": "allocate",
         "parameters": {
            "index": "data-2015-12-01",
            "shard": 1,
            "node": "node03",
            "allow_primary": false
         },
         "decisions": [
            {
               "decider": "allocate_allocation_command",
               "decision": "NO",
               "explanation": "trying to allocate a primary shard [data-2015-12-01][1], which is disabled"
            }
         ]
      }
```
</comment><comment author="clintongormley" created="2015-12-01T11:18:46Z" id="160938539">@sweetest the problem is too few file handles, which is a config option that should be set up before elasticsearch starts.  We can't work around this, it is like having a full disk.  Once you fix that problem (and restart Elasticsearch) the existing primaries should recover.  It may be that you have corrupted transaction logs which prevent recovery (this has been fixed in 2.0) but you can delete those and allow recovery to continue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Have processors operate one field at time.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15133</link><project id="" key="" /><description>To simplify both error handling and specialization of each processor definition, we can operate on one field only. If one wishes to operate on a list of fields, further processors can be defined and repeated within a pipeline.
</description><key id="119624921">15133</key><summary>[Ingest] Have processors operate one field at time.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-01T02:07:26Z</created><updated>2015-12-07T16:31:11Z</updated><resolved>2015-12-07T16:31:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-01T09:38:30Z" id="160910253">I left some comments, I like the simplification. Tests need to be adapted though, they won't compile at the moment, thus there are some missing bits in prod code too (e.g. missing pkg private getters).
</comment><comment author="talevy" created="2015-12-01T16:26:33Z" id="161021606">@javanna thanks for the comments! I hoped you wouldn't do a full review just yet :) but appreciated! Wanted to first put it out there to share with you to see if you are a fan of the simplification proposed... looks like you are!

I'll go ahead and complete the PR
</comment><comment author="javanna" created="2015-12-01T17:25:50Z" id="161039789">thanks @talevy btw don't forget to update the docs as well ;)
</comment><comment author="talevy" created="2015-12-01T23:40:59Z" id="161132729">```
gradle test -Dtests.class=org.elasticsearch.ingest.PipelineTests -Dtests.method="testProcessorSettingsRemainUntouched"
```

is failing, still figuring out why. I may not be understanding the intention here
</comment><comment author="martijnvg" created="2015-12-02T09:44:17Z" id="161240163">@talevy that test was added because of the deep copy change. Processor settings could be modified by a document running through the pipeline. For example if one a field in the set processor holds a map, a remove processor could alter that map. This tests verifies that this doesn't happen. 
</comment><comment author="talevy" created="2015-12-03T16:21:35Z" id="161702195">@martijnvg thanks for the fix! tests pass now.

~~just need to resolve this open design question: https://github.com/elastic/elasticsearch/pull/15133#discussion_r46443264~~

##### EDIT: resolved.
</comment><comment author="javanna" created="2015-12-04T08:22:57Z" id="161906701">I left a few comments, mainly around testing. It's very close though, looks good.
</comment><comment author="talevy" created="2015-12-04T20:59:45Z" id="162080542">updated
</comment><comment author="martijnvg" created="2015-12-07T14:20:14Z" id="162535725">LGTM
</comment><comment author="javanna" created="2015-12-07T14:48:36Z" id="162545674">thanks a lot for all the tests @talevy . I left two minor comments (which I had already left last time around), LGTM otherwise.
</comment><comment author="talevy" created="2015-12-07T16:30:33Z" id="162580055">@javanna thanks, updated.

I now agree with your latest comment around the `containsKey`. I suppose I was thinking of treating them differently for configuration as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] No match for grok</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15132</link><project id="" key="" /><description>Throw exception when grok expression does not match
</description><key id="119612722">15132</key><summary>[Ingest] No match for grok</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-12-01T00:23:28Z</created><updated>2015-12-01T17:13:13Z</updated><resolved>2015-12-01T17:13:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-01T09:15:54Z" id="160903164">left some comments around testing, LGTM otherwise
</comment><comment author="martijnvg" created="2015-12-01T16:42:04Z" id="161026214">LGTM2
</comment><comment author="talevy" created="2015-12-01T16:59:19Z" id="161031208">updated to reflect comments
</comment><comment author="javanna" created="2015-12-01T17:12:05Z" id="161036163">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] No match for grok</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15131</link><project id="" key="" /><description>Throw exception when grok expression does not match
</description><key id="119612654">15131</key><summary>[Ingest] No match for grok</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-12-01T00:22:48Z</created><updated>2015-12-01T00:22:59Z</updated><resolved>2015-12-01T00:22:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Preserve existing mappings on batch mapping updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15130</link><project id="" key="" /><description>This commit addresses an issues introduced in #14899 to apply mapping
updates in batches. The issue is that an existing mapping for a type
could be lost if that type came in a batch that already contained a
mapping update for another type on the same index. The underlying issue
was that the existing mapping would not be merged in because the merging
logic was only tripped once per index, rather than for all types seeing
updates for each index. Resolving this issue is simply a matter of
ensuring that all existing types seeing updates are merged in.

Closes #15129
</description><key id="119610626">15130</key><summary>Preserve existing mappings on batch mapping updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T00:05:07Z</created><updated>2015-12-01T12:53:25Z</updated><resolved>2015-12-01T12:53:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-01T08:16:10Z" id="160890942">LGTM
</comment><comment author="bleskes" created="2015-12-01T08:26:15Z" id="160892520">good catch. Left some minor comments
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Batched mapping updates can overwrite existing mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15129</link><project id="" key="" /><description>The introduction in #14899 of applying mapping updates in batches brought with it a bug that could cause existing mappings to be lost during an update. In particular, in a scenario in which a batch contained updates for at least two distinct existing types on the same index, the existing mappings for all but the first existing type on the index would be lost. This arises because the workflow for applying mapping updates in batches now looks like:
1. [if](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L234) any of the indices in the request batch do not exist on master, [create](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L236) them for the purpose of applying the mapping update and [merge](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L244) in the existing index mappings from the cluster state
2. [apply](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L250-L257) the mapping updates
3. [if](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L261-L263) any indices were created, [delete](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L262) them from master

The code for step 1. [looks like](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L229-L249):

``` java
for (PutMappingClusterStateUpdateRequest request : tasks) {
  // failures here mean something is broken with our cluster state - fail all tasks by letting exceptions bubble up
  for (String index : request.indices()) {
      if (currentState.metaData().hasIndex(index)) {
          // if we don't have the index, we will throw exceptions later;
          if (indicesService.hasIndex(index) == false) {
              final IndexMetaData indexMetaData = currentState.metaData().index(index);
              IndexService indexService = indicesService.createIndex(nodeServicesProvider, indexMetaData, Collections.EMPTY_LIST);
              indicesToClose.add(indexMetaData.getIndex());
              // make sure to add custom default mapping if exists
              if (indexMetaData.getMappings().containsKey(MapperService.DEFAULT_MAPPING)) {
                  indexService.mapperService().merge(MapperService.DEFAULT_MAPPING, indexMetaData.getMappings().get(MapperService.DEFAULT_MAPPING).source(), false, request.updateAllTypes());
              }
              // only add the current relevant mapping (if exists)
              if (indexMetaData.getMappings().containsKey(request.type())) {
                  indexService.mapperService().merge(request.type(), indexMetaData.getMappings().get(request.type()).source(), false, request.updateAllTypes());
              }
          }
      }
  }
}
```

The flaw is that on the second distinct existing type for the same index, [`indicesService.hasIndex(index) == false`](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L234) will evaluate to false (because the [index was created](https://github.com/elastic/elasticsearch/blob/c4a229819406deb4407d8401d698453d936186cf/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L236) for the first existing type for the index) and the existing mapping will never get merged in. This will then cause the mapping update to overwrite the existing mapping so losing it.
</description><key id="119610398">15129</key><summary>Batched mapping updates can overwrite existing mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T00:03:05Z</created><updated>2015-12-01T12:53:23Z</updated><resolved>2015-12-01T12:53:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch data corruption after disk expention</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15128</link><project id="" key="" /><description>When expanding an existing disk on a machine, (ec2) elasticsearch does not work correctly with the allocated space.

It does recognize that the new space is available and allocates shards to the node BUT once the shards get there there the below error is reported:

[2015-11-30 00:01:49,502][WARN ][cluster.action.shard     ] [ES01] [indexname][18] received shard failed for [indexname][18], node[xvIIzsByRt-Ku-t-wLSKyA], relocating [5QhU1g2rQDK-AqTyUMmYMw], [P], s[INITIALIZING], indexUUID [rrS6H6hHTkixYnmTv3th2Q], reason [shard failure [failed recovery][RecoveryFailedException[[indexname][18]: Recovery failed from [ES08][5QhU1g2rQDK-AqTyUMmYMw][PreProd-ElasticSearch-m4.xl-ES08][inet[/XXX.XX.X.XXX:9300]]{master=true} into [ES12][xvIIzsByRt-Ku-t-wLSKyA][ip-XXX-XX-X-XXX][inet[/XXX.XX.XX.XXX:9300]]{master=true}]; nested: RemoteTransportException[[ES08][inet[/XXX.XX.X.XXX:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[indexname][18] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[indexname][18] Failed to transfer [127] files with total size of [5gb]]; nested: RemoteTransportException[[ES12][inet[/XXX.XX.X.XXX:9300]][internal:index/shard/recovery/file_chunk]]; nested: FileNotFoundException[/es_data/ebs/PreProd/nodes/0/indices/indexname/18/index/recovery.1448841041953.segments_1x (No space left on device)]; ]

The error is reported although there is lots of it and it is updated in in node stats as well.

The issue is resolved in most cases by rebooting the node, this updates ES on the new space in a more complete way and the allocation succeeds.

The big problem here is that in most cases we had corruption on some of the data.

10x 
</description><key id="119601015">15128</key><summary>Elasticsearch data corruption after disk expention</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Idofriedman</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2015-11-30T23:00:38Z</created><updated>2016-02-14T18:25:56Z</updated><resolved>2016-02-14T18:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-01T11:08:48Z" id="160936892">What version of Elasticsearch are you on? Are you using multiple data paths per node? Also, this exception looks like it comes from the OS, not from Elasticsearch:

```
No space left on device
```
</comment><comment author="Idofriedman" created="2015-12-01T11:12:51Z" id="160937595">ES v1.7.3
One data path 
</comment><comment author="clintongormley" created="2016-02-14T18:25:56Z" id="183944590">No further info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix unit tests to bind to port 0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15127</link><project id="" key="" /><description>I will followup with ITs and other modules. By fixing this, these tests become more reliable (will never sporatically fail due to other stuff on your machine: ports are assigned by the OS), and it allows us to move forward with gradle parallel builds, in my tests this is a nice speedup, but we can't do it until tests are cleaned up
</description><key id="119595166">15127</key><summary>Fix unit tests to bind to port 0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T22:24:58Z</created><updated>2015-12-01T11:09:24Z</updated><resolved>2015-11-30T23:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-30T22:34:50Z" id="160783506">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increase the number of failed tests shown in test summary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15126</link><project id="" key="" /><description>We had increased this in maven, but it was lost in the transition to
gradle. This change adds it as a configurable setting the the logger for
randomized testing and bumps it to 25.
</description><key id="119587398">15126</key><summary>Increase the number of failed tests shown in test summary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T21:42:10Z</created><updated>2015-11-30T21:47:15Z</updated><resolved>2015-11-30T21:47:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-30T21:45:43Z" id="160771604">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Basic reindex implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15125</link><project id="" key="" /><description>Note: this has been edited from my first comment.

This creates basic reindex and update-by-query implementations that work like delete-by-query. They scroll documents and then turn around and issue bulk requests for each.

This isn't the last word on reindex. Before we merge the feature branch down to master we'll need to fix it to retry on bulk rejection and integrate it with the task management api.

For those of you reading the issues, realize that all the discussion about APIs is discussion. The API we're going to merge is the result of those discussions. You can read about it by reading the asciidoc files in the PR.
</description><key id="119577137">15125</key><summary>Basic reindex implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label></labels><created>2015-11-30T20:43:51Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-01-13T14:39:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-30T20:45:40Z" id="160756454">@tlrx this is my start. I'm intentionally not copying things from delete-by-query because I'm trying to understand why it has the pieces it has.
</comment><comment author="clintongormley" created="2015-12-01T12:45:31Z" id="160956970">Some thoughts...

Initially I was a fan of specifying the index and type in the URL, but now I think it is confusing.  For instance, what if the source index is in a remote cluster? I'm leaning more towards specifying the source and destination explicitly, eg:

```
POST _index_by_search
{
  "src": {
    "index": ["index_1", "index_2"],
    "query": {
      "match": {
        "status": "published"
      }
    }
  },
  "dest": {
    "index": "new_index"
  }
}
```

Essentially, the `src` would accept anything that a search request would accept, and `dest` would be used to configure the bulk request.

Routing could be changed or removed as follows:

```
POST _index_by_search
{
  "src": {
    "index": ["index_1", "index_2"],
    "routing": ["foo", "bar"],
    "query": {
      "match": {
        "status": "published"
      }
    }
  },
  "dest": {
    "index": "new_index",
    "routing": null
  }
}
```

The `version_type` parameter could be used as follows:
- `external`: set the new version to the same value as the existing version (and fail if the document already exists)
- `internal`: update the document only if it exists and is the same version as retrieved by the scroll request
- `none`: write the new document regardless of whether it exists or not, or whether it has already been updated or not
</comment><comment author="nik9000" created="2015-12-01T20:14:37Z" id="161082710">&gt; Initially I was a fan of specifying the index and type in the URL, but now I think it is confusing.

I think I'm with you. I'll move it to the single endpoint.

&gt; `none`: write the new document regardless of whether it exists or not, or whether it has already been updated or not

We actually have `force` for that now.

&gt; "routing": null

Right now it copies routing values if they are set. I figure that should be the default. Maybe call it `"routing": "keep"` and make `"routing": "discard"` cause it to just throw the routing away.

&gt; `external`: set the new version to the same value as the existing version (and fail if the document already exists)

Can we follow the usual `external` semantics and write the document only if the version is newer?

&gt; `internal`: update the document only if it exists and is the same version as retrieved by the scroll request

I figured `internal` would just work like internal versioning works in bulk requests and write the document with version 1 if it doesn't exist.

One thing that comes up when I play with this is that its simpler to just support "index" requests in the bulk, at least for now. I'll think about it some more because update requests would be useful because they'd let you use scripts for free. You could probably get away with only returning smaller portions of the document during the query too.
</comment><comment author="clintongormley" created="2015-12-02T11:26:22Z" id="161266175">&gt; &gt; `none`: write the new document regardless of whether it exists or not, or whether it has already been updated or not
&gt; &gt; We actually have force for that now.

`force` is a slightly different, in that it will forcibly set the `version` to the specified value (which could cause the `version` number to drop), while what I was envisaging was just incrementing the `version` number, the same way we would if a `version` hadn't been specified.

&gt; &gt; `"routing": null`
&gt; &gt; Right now it copies routing values if they are set. I figure that should be the default. Maybe call it "routing": "keep" and make "routing": "discard" cause it to just throw the routing away.

The nice thing about `routing: null` is that you could also set it to `routing: foo`.  

&gt; Can we follow the usual `external` semantics and write the document only if the version is newer?

Not if you're indexing into the same index - nothing would be updated.  The use case I'm seeing here is: we use external versioning so eg a db is the source of the version number, now we want to add some multi-field with a new mapping and backfill existing docs, so we reindex but keep the version number the same.  

We could use `force` to index the doc regardless and to keep the same version number, but that wouldn't allow for skipping newer docs... not sure if this is an issue.

&gt; I figured internal would just work like internal versioning works in bulk requests and write the document with version 1 if it doesn't exist.

This isn't how it works.  Try this:

```
PUT t/t/1?version=4
{}
```

You'll get a conflict exception because the document doesn't exist.  Instead we could use `none` (which should just delete the `version` number before indexing).

This versioning thing is complex.  We should probably have a brainstorming session to make sure that we nail it down correctly.
</comment><comment author="bleskes" created="2015-12-02T12:10:48Z" id="161273537">when re-indexing into the same index, internal versioning is what I think we need, i.e., only re-index if it has changed.

For another index it's tricky. If people use external versioning in general, then it's a good fit. If they don't and we concurrently index into a remote while other processes also index it then external version semantics doesn't really make sense . It doesn't mean much that the version in the source index is higher than the one in the target index. In that case I think the only two options are either create (i.e., only reindex if the document doesn't exist) or override (i.e., always index, no guarantees over which copy survives). 

In all cases I don't think force makes sense. It can be used as a measure of last resort where people use one index as a source of truth to another _and_ want to override existing docs, but it's super expert and dangerous.
</comment><comment author="clintongormley" created="2015-12-02T13:06:26Z" id="161285492">&gt; when re-indexing into the same index, internal versioning is what I think we need, i.e., only re-index if it has changed.

We probably also need the ability to specify what should happen if there is a version conflict: ignore and continue, or throw an exception.
</comment><comment author="nik9000" created="2015-12-02T15:13:19Z" id="161328250">&gt; The nice thing about `routing: null` is that you could also set it to `routing: foo`.

I implemented `routing: keep`, `routing: discard` and `routing: =foo` last night. It feels a bit better than relying `unset` being different from `null`. I'm happy to iterate on it. Its simple to change.

&gt; when re-indexing into the same index, internal versioning is what I think we need, i.e., only re-index if it has changed.

But that'll bump the version number which isn't what someone using external versioning wants. I wonder if those folks are just out of luck here?

&gt; either create (i.e., only reindex if the document doesn't exist) or override (i.e., always index, no guarantees over which copy survives).

I think this'd work fine for now. The big use case for writing to an other index is when someone wants to change analysis or sharding.
</comment><comment author="bleskes" created="2015-12-02T18:33:57Z" id="161391744">&gt; &gt; when re-indexing into the same index, internal versioning is what I think we need, i.e., only re-index if it has changed.
&gt; 
&gt; But that'll bump the version number which isn't what someone using external versioning wants. I wonder if those folks are just out of luck here?

Good point and the first valid use for `EXTERNAL_GTE` I heard :) .In general I was talking about the defaults for the operations that make sense. In general I think we should just allow people to specify what ever versioning support they want.
</comment><comment author="nik9000" created="2015-12-02T20:02:53Z" id="161416488">Rebased so I could remove the file. I figure rebase is ok here because no one is actively reviewing the code.
</comment><comment author="rjernst" created="2015-12-02T21:44:07Z" id="161443161">&gt; Initially I was a fan of specifying the index and type in the URL, but now I think it is confusing

@clintongormley Is this supposed to operate from the destination index? If so, I don't see why this should appear different than a normal indexing command, except instead of the content of the request being a document, it is the specification for where/how to pull documents (index/types/routing/etc).
</comment><comment author="nik9000" created="2015-12-02T21:54:31Z" id="161445684">&gt; @clintongormley Is this supposed to operate from the destination index? If so, I don't see why this should appear different than a normal indexing command, except instead of the content of the request being a document, it is the specification for where/how to pull documents (index/types/routing/etc).

That is certainly one way to do it. We could do something like:

```
curl -XPOST 'http://localhost:9200/twitter/_index_by_search' -d '{
  "source": {
    ...
  }
}'
```

In that case we could move all the parsing for the index request from the body to the url which is where it is for index requests.

I was originally thinking of rooting the request at the source index like it was a search. And then I thought about copying the index from the source in the url to the index request. And then it started getting complicated. But rooting the request at the destination would make it simpler.
</comment><comment author="clintongormley" created="2015-12-03T14:16:09Z" id="161652369">You may well want to use this API to import an index from an older version of Elasticsearch, which doesn't have the `_index_by_search` API.  So putting the source into the URL would be tricky as there is nowhere to put the remote cluster details.  

Putting the destination into the URL would work in this situation.  But what if we decide that this API should also be able to export to a remote cluster?

These were the questions that led me to having an explicit `src` and `dest` section in the body.
</comment><comment author="nik9000" created="2015-12-03T18:56:12Z" id="161746718">So I had a think about version_type and what it should default to and I made a table of what I think they should be:

&lt;table&gt;
  &lt;tr&gt; &lt;th&gt;                                      &lt;/th&gt;&lt;th&gt;Internal Versioning&lt;/th&gt;&lt;th&gt;External Versioning&lt;/th&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;Destination is same     &lt;/th&gt;&lt;td&gt;internal           &lt;/td&gt;&lt;td&gt;external_exact?!?  &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;Destination is different&lt;/th&gt;&lt;td&gt;external!!         &lt;/td&gt;&lt;td&gt;external           &lt;/td&gt; &lt;/tr&gt;
&lt;/table&gt;

Here are the situations and what I think the defaults should do:

&lt;table&gt;
  &lt;tr&gt; &lt;th align="right"&gt;Destination is same     &lt;/th&gt;&lt;td&gt;Index if versions equal. Do not index if doesn't exist.&lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;Destination is different&lt;/th&gt;&lt;td&gt;Index if &lt;code&gt;dest.version&lt;/code&gt; &lt; &lt;code&gt;src.version&lt;/code&gt;. Index if doesn't exist.&lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;Internal versioning     &lt;/th&gt;&lt;td&gt;Version number after write doesn't matter.&lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;External versioning     &lt;/th&gt;&lt;td&gt;Version number must be copied.&lt;/td&gt; &lt;/tr&gt;
&lt;/table&gt;

And here are the relevant version_types:

&lt;table&gt;
  &lt;tr&gt; &lt;th align="right"&gt;internal      &lt;/th&gt;&lt;td&gt;Index if the version == provided. Do not index if doesn't exist. Any version.&lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;external_exact&lt;/th&gt;&lt;td&gt;Index if the version == provided. Do not index if doesn't exist. Preserves version. Doesn't exist yet.&lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;external_gte  &lt;/th&gt;&lt;td&gt;Index if the version &lt;= provided or is not found. Preserves version. Almost good enough to replace external_exact but it'd index on document not found.&lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;th align="right"&gt;external      &lt;/th&gt;&lt;td&gt;Index if the version &lt; provided or is not found. Preserves version.&lt;/tr&gt; &lt;/tr&gt;
&lt;/table&gt;

So I guess I propose creating another version_type, `external_exact` that works just like `internal` but preserves the version rather than increments it.

I propose using `external` versioning when whenever the request is copying to another index. That'll preserve the version for all documents which I think is useful because it'll minimize writes in the quick and dirty DR use case.

Does that make sense?
</comment><comment author="bleskes" created="2015-12-04T12:37:28Z" id="161956976">re different destination + internal versioning. current table says use external versioning. I don't think this makes sense unless you concurrently run re-indexing operations. Concurrently indexing from the API to the target index will have no version relationship to the source index and thus we have no guarantees. Imho we should default to _create and not have any other check. People should be able to choose to just index, over riding what ever document they have there.

For the same index, external version use case - I think external_gte works fine with the exception of deletes (as we forget them after 60s). We can introduce what you call external_equal (or replace) but I'm not sure this is worth the trouble? if someone uses an external database and keeps indexing to ES and wants to do an inline reindex concurrently, I think it's a fair ask to say that they should just reindex from source (i.e., the DB). Note that the deletes is also an issue with different index, external versioning. 
</comment><comment author="nik9000" created="2015-12-04T13:53:59Z" id="161973058">&gt; if someone uses an external database and keeps indexing to ES and wants to do an inline reindex concurrently, I think it's a fair ask to say that they should just reindex from source (i.e., the DB). Note that the deletes is also an issue with different index, external versioning.

I've worked on a project where rebuilding the document sent to Elasticsearch takes much much too long to reindex from the source consistently. I don't think its that much trouble to create the extra version_type. I wish version types weren't created for read as well as write though. This isn't useful for read.

&gt; Imho we should default to _create and not have any other check.

I think of copying from one index to another as "refresh changes to documents made in the source into the destination." Its kind of a silly way to think about it because it doesn't include deletes, but that is how I got to `external` because it'd work well in the situation that you just want to dump updates across the wire.

OTOH `op_type: create` would be super simple for internally versioned docs. As would `version_type: force`. One issue with `op_type: create` is that right now it doesn't work with `version_type: external` but I expect I can fix that.

One advantage to what I have proposed is that the defaults can be consistent for internally and externally versioned objects: `external` and `external_exact`. This is nice because you can't tell at all if the objects are internally or externally versioned. That information is ephemeral to the index requests.

I suppose the `op_type: create` solution could use the same defaults for internal versioning and external versioning too....
</comment><comment author="nik9000" created="2015-12-04T15:11:21Z" id="161989950">I suppose the upshot of all this is that the docs will need contain a sort of version cookbook for how to get it to write things appropriately.
</comment><comment author="nik9000" created="2015-12-04T16:40:53Z" id="162015450">Another idea is to have no default when destination != source and just have a kind of cookbook of options available.
</comment><comment author="nik9000" created="2015-12-04T19:14:57Z" id="162056882">&gt; OTOH op_type: create would be super simple for internally versioned docs.

I'm going with `op_type: create` being the default when you copy to another index. Its the simplest thing to think about. Anything else requires some explanation. Thus, I'm going to write a cookbook for how you can set up versioning.
</comment><comment author="clintongormley" created="2015-12-05T14:31:05Z" id="162195078">Some thoughts about versioning.  I think these are the scenarios we need to handle:

(Note: `version` refers to the version retrieved from the scroll request, while `current` refers to the version of the document in the index.)

## Internal versioning:
1. Always index - increment the version
2. Index unless already exists (_create)
3. Update in place: index only if version == current
4. Index unless document exists and version &lt; current

## External versioning:
1. Always index - set version explicitly
2. Index unless document exists and version &lt; current

The main difference between internal and external is that external should maintain the external version number, while with internal we don't really care.

I think these scenarios can be supported with the following logic:

&lt;table&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;code&gt;op_type&lt;/code&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;code&gt;version&lt;/code&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;code&gt;version_type&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Internal&lt;/th&gt;
&lt;th align="left"&gt;Always index&lt;/th&gt;
&lt;td&gt;&lt;code&gt;index&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td`&gt;
&lt;td&gt;&lt;code&gt;internal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th align="left"&gt;External&lt;/th&gt;
&lt;th align="left"&gt;Always index&lt;/th&gt;
&lt;td&gt;&lt;code&gt;index&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;$version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;force&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Internal&lt;/th&gt;
&lt;th align="left"&gt;Index unless exists&lt;/th&gt;
&lt;td&gt;&lt;code&gt;create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;internal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Internal&lt;/th&gt;
&lt;th align="left"&gt;Update in place&lt;/th&gt;
&lt;td&gt;&lt;code&gt;index&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;$version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;internal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Internal&lt;/th&gt;
&lt;th align="left"&gt;Index unless newer exists&lt;/th&gt;
&lt;td&gt;&lt;code&gt;index&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;$version++&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;external_gt&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th align="left"&gt;External&lt;/th&gt;
&lt;th align="left"&gt;Index unless newer exists&lt;/th&gt;
&lt;td&gt;&lt;code&gt;index&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;$version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;external_gte&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

While the above is what we should do internally, it'd be good to expose a simpler interface to the user.  Perhaps something like:
- `version_type: internal`, `action: index | create | update | index_unless_newer`
- `version_type: external`, `action: index | index_unless_newer`

The default should be `index_unless_newer`.

On top of that we need to control what to do in case of version conflicts. Perhaps:
- `on_conflict: ignore | report | abort`
</comment><comment author="nik9000" created="2015-12-09T15:40:44Z" id="163296435">@bleskes, @imotov, @clintongormley, and I had another voice chat about versions and came to the conclusion that this is hard! Its fairly easy to write one of these update by query based scripts for a specific use case but to build something that is relatively safe for a huge swath of use cases is hard! So I think we end up with (at least) two APIs. One is `POST /$indexes/($types/)?_reindex` and it supports touch-style semantics I'll describe in another comment. The other is `POST /_index_by_search` and looks like the things we've talked about above. I'll describe it in yet another comment. Sorry for splitting the updates into multiple comments
</comment><comment author="nik9000" created="2015-12-09T16:35:20Z" id="163317067"># `POST $indexes/($types/)?_reindex`

As the name implies, this is for reindexing a document. It handles two use cases:
1.  Picks up in place mapping changes like adding a new multi-field value or picking up a new property when `"dynamic": false`.
2.  Update-by-query

The API looks like:

``` js
POST $indexes/($types/)?_reindex
{
  "query": { // &lt;---- This is just an example
    "match": {
      "state": "published"
    }
  },
  "script": {},               // Just a regular script element for transforming the source.
                              // Doesn't allow changing ctx.op to anything other than noop.
  "ingest": "",               // Should work like a script but defined in #14049
  "version_type": "internal", // or "reindex"
  "conflicts": "abort"        // Or "proceed"
}
```

You can change the document's `_source` but you can't change things like `_routing` or `_parent` or `type`.

`"version_type": "internal"` reuses Elasticsearch's standard internal versioning semantics for optimistic concurrency control so the document will only be reindexed if they match the version number ("touch" semantics) _and_ the version number will be incremented. `"version_type": "reindex"` will work exactly like internal _but_ it won't increment the version number. As such it'll only be safe to use if the operation doesn't change the `_source` or if all incoming changes will include this change implicitly.

`"conflicts": "abort"` will abort the operation if there are any version conflicts, returning a list of them. `"conflicts": "proceed"` will simply count the conflicts.

If no `script` or `ingest` is defined then the `version_type` will default to `reindex` and `conflicts` will default to `proceed`. This should make it "just do the right thing" to pick up in place mapping changes. Because it preserves the version number it is safe to use for indexes that use `external` versioning.

If a `script` or `ingest` is defined then the `version_type` will default to `internal` and `conflicts` will default to `abort`. This should make it "just do the right thing" for update by query. The version number will be incremented so it'll play nice with folks using internal versioning and optimistic concurrency control. It won't "just do the right thing" if the index uses external versioning.

Making external versioning work with update-by-query is hard! You have to make sure that all incoming changes will include the changes made by the update implicitly. **You**. The client. Elasticsearch can't do it for you. And then you manually set `"version_type": "reindex", "conflicts": "proceed"`. The reason you need to do this is because of the way Elasticsearch handles concurrent updates and replication. The upshot is: if you update by query with `"version_type":`reindex`" then sometimes the reindex operation will be squashed by concurrent index operations.

Any errors other than version conflicts will cause an abort. Some caveats:
- Abort isn't rollback. This request isn't atomic.
- These requests aren't done one by one. We'll probably use Elasticsearch's `BulkProcessor` and its async bulk actions. Its nice because it'll get retries on bulk queue full and things like that. The upshot is that you might end up with more than one error. The current plan is to just abort the async stuff and return all the errors we've accumulated.

We talked about having something like `"conflict": "retry"` but rejected for now because its more complex to implement and because on update the document may no longer match the query. Its technically possible to recheck it the query but this is complex enough.
</comment><comment author="nik9000" created="2015-12-09T16:48:31Z" id="163320681">&gt; `POST /$indexes/($types/)?_reindex`

Another point: If `$indexes` is multiple indexes then this process just queries multiple indexes. It still writes the documents back to the indexes and the type that they came from.
</comment><comment author="dakrone" created="2015-12-09T16:49:56Z" id="163321037">&gt; You can change the document's `_source` but you can't change things like `_routing` or `_parent` or `type`.

Being able to change the `_type` seems like one of the major reasons to re-index, so you can put documents into a different type (let's say you need to fix the `field.name` dot problem due to a breaking mapping change)
</comment><comment author="nik9000" created="2015-12-09T16:51:07Z" id="163321378">&gt; Being able to change the _type seems like one of the major reasons to re-index, so you can put documents into a different type (let's say you need to fix the field.name dot problem due to a breaking mapping change)

You move them into a new type and not a new index?
</comment><comment author="dakrone" created="2015-12-09T16:56:00Z" id="163323207">&gt; You move them into a new type and not a new index?

How if you can't change the type when reindexing?
</comment><comment author="nik9000" created="2015-12-09T18:27:41Z" id="163349685">&gt; How if you can't change the type when reindexing?

You can change all kinds of stuff with the other API, the one that I've not yet written a comment about but mentioned above, `POST /_index_by_search`. I admit that it might be more clear to call it `POST /_reindex` or something like that. I'm going to make a comment to describe that API now. Can you let me know if you think its good enough?
</comment><comment author="nik9000" created="2015-12-09T19:02:27Z" id="163358532"># `POST _index_by_search`

This API is the other half of the `POST /$indexes/($types/)?_reindex` API, in that the source and destination indexes can be different. In fact they have to be different. But we'll get to that. Example time:

``` js
POST _index_by_search
{
  "src": {                                // This is just a search. Its literally the scroll query we execute.
    "index": ["index_1", "index_2"],
    "routing": ["foo", "bar"],
    "query": {
      "match": {
        "status": "published"
      }
    }                                     // If you don't add a scroll we'll add one for your with reasonable defaults.
  },
  "dest": {
    "index": "new_index",                 // Required. Only supports a single index.
    "type": "new type",                   // Optional. If not provided objects keep their old type. If two objects with
                                          // the same ID are pushed into the same type then one will win and its hard
                                          // to predict which so just don't do that. OK?
    "routing": "keep"                     // or "discard" or "=&lt;some new value&gt;"
  },
  "size": 1000,                           // Total number of documents to create/dump/refresh. Size in the src is the size passed to the scroll request.
  "script": {},                           // These next three are our friends from the POST $indexes/($types/)?_reindex api
  "ingest": "",
  "conflicts": "abort",
  "op_type": "overwrite"                  // or "create" or "refresh"
}
```

The reason only these three `op_type` are supported is that Elasticsearch uses versioning to handle concurrent updates. So while it'd be possible to make `overwrite` keep the version number while still overwriting documents with a higher version number it'd move the version number backwards which would be prone to being dropped during shard relocation with concurrent indexing.

`src` is just a search request. Instead of putting the index, type, routing, etc, into the request we put it in the body so this request isn't confused with the other one.

`dest` is just the prototype for the index request.

Most of the other properties are shared with the other request. I've not spent a lot of time thinking about what the defaults should be but that is something we can work through later.
</comment><comment author="martijnvg" created="2015-12-09T22:18:02Z" id="163417111">I think the `ingest` option should be a string? It should hold the id of the desired pipeline for reindexing.
</comment><comment author="nik9000" created="2015-12-09T23:07:38Z" id="163430892">&gt; I think the ingest option should be a string? It should hold the id of the desired pipeline for reindexing.

Sure!
</comment><comment author="bleskes" created="2015-12-10T07:50:16Z" id="163526430">Thanks @nik9000 . Two minor comments:

Re reindex

&gt; "version_type": "external" will work exactly like internal but it won't increment the version number.

Do you mean version_type reindex?

Re index_by_searhch:

&gt; ""version_type": "overwrite"

I'm not sure what you meant here. If you really meant version type, I think we the parameter value should be one of the version types (i.e., internal, external etc.) . O.w. it will be confusing.  If you mean to control things like index vs create, I think we need another parameter. maybe just `op_type`?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What's the best way to run REST integration tests?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15124</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc suggests 

```
gradle integTest -Dtests.filter="@Rest"
```

but that matches no tests, I've had some success with 

```
gradle integTest -Dtests.class="*.Rest*IT"
```

Is that the recommended way to run these?
</description><key id="119561119">15124</key><summary>What's the best way to run REST integration tests?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels /><created>2015-11-30T19:21:32Z</created><updated>2015-11-30T19:38:53Z</updated><resolved>2015-11-30T19:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-30T19:36:50Z" id="160735595">You can do something like this:

```
gradle :distribution:tar:integTest \
  -Dtests.class=org.elasticsearch.test.rest.RestIT \
  -Dtests.method="test {p0=cat.shards/10_basic/Help}"
```

to run, for example, the REST test named ["Help"](https://github.com/elastic/elasticsearch/blob/cc627e41cc927446ceb06ff2c7d676591fe6b28f/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards/10_basic.yaml#L2) in the [`10_shards.yaml` file](https://github.com/elastic/elasticsearch/blob/cc627e41cc927446ceb06ff2c7d676591fe6b28f/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards/10_basic.yaml) under [`cat.shards`](https://github.com/elastic/elasticsearch/tree/cc627e41cc927446ceb06ff2c7d676591fe6b28f/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check mapping compatibility up-front.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15123</link><project id="" key="" /><description>We have a bug when updating an existing mapping and the mapping update conflicts
with another type. In such a case, the mapping is already partially applied
before we raise the exception, which can lead to all sorts of bugs.

This commit moves the compatibility checks up-front so that we don't start
applying a mapping before we are sure that it is not only applicable to the
current type but also to other types.

Note that this might break compatibility for some users: we had some tests
that checked that fields that did not have a conflict were added even though
other fields from the same mapping update were not compatible. This does not
work anymore.

MapperService.merge has been refactored to take a map of mapping updates so
that we can perform some cross-type validation (necessary for _parent) that
was previously spread in-between MapperService and MetaDataMappingService.

We now also verify that fields are defined only once in a given mapping. It
was previously possible to have a field twice either by reusing the name of
a metadata mapper or by using subfields both explicitly in a plugin and in
the code of a field mapper.

The check that _parent can't point to an existing type would only be actually
applied if the master node has mappings locally. So for instance in case of
dedicated master nodes, it would be ignored. This is fixed now by adding
mappings from all types when an index does not exist locally, instead of only
mappings from the types that are being updated.
</description><key id="119542527">15123</key><summary>Check mapping compatibility up-front.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-11-30T17:39:28Z</created><updated>2015-12-01T10:57:47Z</updated><resolved>2015-12-01T10:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-30T20:26:15Z" id="160751711">This looks great! Just left a couple questions, but as is this change looks good to me to push.
</comment><comment author="jpountz" created="2015-12-01T10:19:10Z" id="160925936">As per @s1monw 's and @bleskes 's request, I will split this PR into smaller PRs.
</comment><comment author="jpountz" created="2015-12-01T10:57:47Z" id="160934598">Here is a PR to fix an issue with cross-type validation for `_parent`: #15142
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade fields with dot character to 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15122</link><project id="" key="" /><description>Hi,

I have a small number of fields with "." character in my 1.5 elasticsearch stack which gets data from logstash 1.4.2. I take snapshots to S3 daily.

The problem is that I can't start a 2.0.0 elasticsearch and use the snapshot to restore because it complains about "." character. I checked the fields with "." character using  curl -XGET 'http://localhost:9200/_all/_mapping . I know about the logstash de-dot filter but this does not help me as it cannot go back and fix existing data.

How do I restore my snapshot? If the option is to delete the offending data then I'm ok with it. Could anyone let me know how?
</description><key id="119537134">15122</key><summary>Upgrade fields with dot character to 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkr1234</reporter><labels /><created>2015-11-30T17:12:41Z</created><updated>2015-12-02T13:23:56Z</updated><resolved>2015-11-30T17:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T17:51:28Z" id="160704893">Hi @pkr1234 

Dots are no longer allowed because they introduce an ambiguity in field lookup that is impossible to work around.  So your options are:
- reindex the data (in 1.x) into a new index without dots in the field names
- delete the old data
- keep a small 1.x cluster around to look at the old indices when you need to
</comment><comment author="pkr1234" created="2015-12-01T13:48:33Z" id="160974925">Thanks Clinton.

I went for the second option and deleted the data using delete by query and taken another snapshot. But the _restore still fails. I checked the _mapping and the mapping still exists for the offending fields. Is this causing the problem now for restore?

How do I get around this?
</comment><comment author="clintongormley" created="2015-12-01T13:58:01Z" id="160976971">You have to delete the index itself.
</comment><comment author="pkr1234" created="2015-12-01T14:19:30Z" id="160981560">That's not an option for me. I keep 45-60 days of logs pushed in by logstash for display by Kibana. These field names appear in all of the indexes logstash-yyyy.mm.dd . 

Is there a script that renames the existing fields and reindexes anywhere that I can use? I'm stuck. 
</comment><comment author="clintongormley" created="2015-12-01T14:31:44Z" id="160984302">The Perl and Python clients provide helpers for reindexing data, eg see:
- http://elasticsearch-py.readthedocs.org/en/master/helpers.html#reindex
- https://metacpan.org/pod/Search::Elasticsearch::Bulk#REINDEXING-DOCUMENTS
</comment><comment author="pkr1234" created="2015-12-02T12:37:47Z" id="161279448">Thanks Clinton. The python library is quite easy to install. I can't say the same about the perl library which fails to install using cpan. It complains about 0.20 Hijk. Anyway, I have worked out a way and here are my steps:

A. Open all indexes
`curl -XPOST http://localhost:9200/logstash-*/_open`

B. Identify my dodgy fields with a "." character
`curl -XGET http://localhost:9200/_all/_mapping?pretty |grep "\."`
Grab all fields with dodgy character

C. Search (count) my records with dodgy field

```
 curl -XGET 'http://localhost:9200/_all/_count' -d '{
   "query" : {
     "filtered" : {
         "filter" : {
            "exists" : { "field" : "DODGY.FIELD.NAME"}
         }
     }
   }
}'
```

D. Once we know how many records will be affected and we are OK with purging them. I'm ok with purge but others may not be. I don;t know how to rename it. Maybe you could help us rename it. Anyway, to delete those records.

```
 curl -XDELETE  'http://localhost:9200/_all/_query' -d '{
   "query" : {
     "filtered" : {
         "filter" : {
            "exists" : { "field" : "DODGY.FIELD.NAME"}
         }
     }
   }
}'

```

E. Now the records are deleted but the mapping still exists and snapshot restore will still fail. So we need to reindex. So install python library:

```
yum install python-pip
pip install elasticsearch
```

I have written a script that reindexes the old index to a new name. It works on 1.5.x version. Older versions of elasticsearch may have a problem if indexes are closed as cat did not list closed indexes. For older versions, simply open all indexes using curl. First argument is elasticsearch host and the second one is the indexname that we want to reindex. The target is indexname + 'a'. It will close the old index after reindex.

```
#!/usr/bin/python

#########################
#pkr1234
# A script to reindex. It takes an index name and reindexes to indexname followed by a character 'a' i.e. logstash-2015.12.02a
#########################

import sys
import traceback
import time
from elasticsearch import Elasticsearch
from elasticsearch.helpers import reindex

def isempty(name, value):
    if (value == None):
        print '[' + name + '] must be defined!'
        return 1
    if (len(value.strip()) == 0):
        print '[' + name + '] cannot be empty!'
        return 1

    return 0

def printexit(msg, exitcode):
    print "\n" + msg + "\n"
    sys.exit(exitcode)

def existsindex(indexlist, indexname):
    # Expects a python list of indexes as in cat
    found = None
    for item in indexlist:
        itemsplitted = item.split()
        if ('close' in item):
            indname = itemsplitted[1]
            indstatus = itemsplitted[0]
        else:
            indname = itemsplitted[2]
            indstatus = itemsplitted[1]
        if (indname == indexname):
            found = [indname, indstatus]
            break
    return found



if (len(sys.argv) &lt;&gt; 3):
    printexit("You must supply two arguments: elasticsearch host as first argument and index name as second argument!", 1)


elhost = sys.argv[1]
elindex = sys.argv[2]
elnewindex = sys.argv[2] + 'a'

isempty('elhost', elhost)
isempty('elindex', elindex)

es = None
esindiceslist = None

try:
    es = Elasticsearch(host=elhost)
    esindiceslist = es.cat.indices().splitlines()
except:
    traceback.print_exc()
    printexit("Unable to connect to elasticsearch and list indices! [" + elhost + "]", 1)

nelind = existsindex(esindiceslist, elnewindex)
if (nelind):
    printexit("New index [" + elnewindex + "] already exists with status [" + nelind[1] + ". Cannot create!", 1)

elind = existsindex(esindiceslist, elindex)
if (elind == None):
    printexit("Index specified [" + elindex + "] does not exist. Cannot reindex!", 1)


if (elind[1] == "close"):
   # We have to open it for reindex to proceed
   print "Now opening source [" + elindex + "] and sleeping 120 sec.."
   es.indices.open(elindex)
   time.sleep(120)

print "Now indexing source [" + elindex + "] to target [" + elnewindex + "] .."
reindex(client=es,source_index=elindex,target_index=elnewindex)

# Close old index
es.indices.close(elindex)

# Open new index
es.indices.open(elnewindex)


```

```
reindex.py 'localhost'  'logstash-2015.12.02' 
```

This will result in logstash-2015.12.02 being closed and a new index created called logstash-2015.12.02a

F. Call the above script for each and every index that contains dodgy field. Once the reindex is complete, list the indexes:

```
curl http://localhost:9200/_cat/indices?pretty
```

It will show both old and new indexes (as in 1.5.0). For older elasticsearch, all indexes will have to be opened with a wildcard in a curl command.

G. Once satisfied, delete the old indices (ones not ending in 'a')

```
curl -XDELETE http://localhost:9200/[OLDINDEXNAME]

```

H. Once the new indices are created and old ones deleted, take the snapshot using the snapshot API. Take it over to the new 2.0.0 cluster and it should import as it does not have the fields with "." character. 

Hope this helps someone. Ideally, a migration script should've been provided. Reindex is a slow process. I have about 45 days worth of logs (each 1 GB) - it will take a couple of overnight jobs to do it.

For new events, de-dot filter on logstash does the job fine on the fly renaming dodgy fiedls to underscores.

For rename - I don't know how to do it. My process is based upon deleting the offending records.
</comment><comment author="clintongormley" created="2015-12-02T13:23:56Z" id="161288408">&gt; Hope this helps someone. Ideally, a migration script should've been provided. Reindex is a slow process. 

Won't help you now, but we're working on making this better: https://github.com/elastic/elasticsearch/pull/15125
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Cleanup Processor.Factory interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15121</link><project id="" key="" /><description>1) It no longer extends from Closeable.
2) Removed the config directory setter. Implementation that relied on it, now get the location to the config dir via their constructors.
</description><key id="119537100">15121</key><summary>[Ingest] Cleanup Processor.Factory interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-30T17:12:26Z</created><updated>2015-12-01T16:36:58Z</updated><resolved>2015-12-01T16:36:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-12-01T12:28:33Z" id="160954137">&gt; This would be the preferred way, but because the ingest processor code needs to be ES agnostic I'm unable to add Environment class as constructor parameter.

aaah... I c... that explains it well.. thx @martijnvg 
</comment><comment author="martijnvg" created="2015-12-01T13:00:28Z" id="160963324">@uboness I've updated the PR and added a comment on why the `ProcessorFactoryProvider` interface is needed.
</comment><comment author="uboness" created="2015-12-01T13:19:47Z" id="160966839">left one comment, OTT LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding no source to index request in bulk causes NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15120</link><project id="" key="" /><description>Found this while coding some tests for #7361 

``` java
BulkRequest bulkRequest = new BulkRequest();
bulkRequest.add(new IndexRequest("index", "type", "id"));
```

Causes:

```
java.lang.NullPointerException
    at __randomizedtesting.SeedInfo.seed([C59AA8EEBA35115D:D7DEB6DADB17F03]:0)
    at org.elasticsearch.action.bulk.BulkRequest.internalAdd(BulkRequest.java:137)
    at org.elasticsearch.action.bulk.BulkRequest.add(BulkRequest.java:127)
```
</description><key id="119530237">15120</key><summary>Adding no source to index request in bulk causes NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Bulk</label><label>:Java API</label><label>bug</label></labels><created>2015-11-30T16:40:25Z</created><updated>2015-12-01T11:46:36Z</updated><resolved>2015-12-01T11:46:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support * wildcard to retrieve stored fields in the 'fields' option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15119</link><project id="" key="" /><description>Skips #14489 in order to preserve backward compatibility in 2.x.
Fixes #10783
  Support \* wildcard to retrieve stored fields when using the `fields` option.
  Supported pattern styles are "xxx_", "_xxx", "_xxx_" and "xxx*yyy".
  Add docs.
</description><key id="119528579">15119</key><summary>Support * wildcard to retrieve stored fields in the 'fields' option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-11-30T16:32:42Z</created><updated>2015-12-01T11:00:27Z</updated><resolved>2015-12-01T11:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-30T16:43:49Z" id="160684505">Just left one comment about indentation, otherwise it looks good to me!
</comment><comment author="jimczi" created="2015-11-30T17:25:50Z" id="160696113">Thanks @jpountz.
</comment><comment author="jpountz" created="2015-11-30T17:28:38Z" id="160697175">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi field names may not contain dots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15118</link><project id="" key="" /><description>related to #14957
</description><key id="119524040">15118</key><summary>Multi field names may not contain dots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label></labels><created>2015-11-30T16:11:50Z</created><updated>2015-12-01T16:31:06Z</updated><resolved>2015-12-01T10:11:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-11-30T16:13:18Z" id="160674474">@rjernst can you take a look?
</comment><comment author="rjernst" created="2015-11-30T16:44:53Z" id="160684974">Looks good, a few suggestions on the exception and test. 
</comment><comment author="brwe" created="2015-11-30T17:41:59Z" id="160702573">@rjernst Thanks for reviewing so quickly! I changed according to your suggestions.
</comment><comment author="rjernst" created="2015-11-30T17:44:39Z" id="160703200">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Task Management</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15117</link><project id="" key="" /><description>We have identified several potential features of elasticsearch that can spawn long running tasks and therefore require a common management mechanism for this tasks. This issue will introduce task management API that will provide a mechanism for communicating with and controlling currently running tasks. The task management API will be based on the top of existing TransportAction framework, which will allow any transport action to become a task. 

The tasks will maintain parent/child relationship between tasks running on the coordinating nodes and subtasks that are spawn by the coordinating node on other nodes.

The task management will be introduced in several iterations. The first phase will be back-ported to 2.x and the second phase will be only available in 5.0.
Phase I
- [X] #15347 Create framework for registering and communicating with tasks. 
- [X] #16033 Make the Task object available to the action caller
- [X] #15931 Establish parent/child relationship for master node, replication and broadcast actions
- [X] #16320 Add support for task cancellation
- [X] #16356 Add a task status field to the task info object
- [X] #16586 Add wait for task registration to testCanFetchIndexStatus
- [X] #16744 Combine node name and task id into single string task id
- [X] #16829 Store and report task start time and duration

Phase II
- [X] #17383 Add Task Management API Documentation
- [X] #17551 Add cat API version of task management APIs
- [X] #17928 Provide a generic mechanism for storing results of long running tasks
- [x] Add ability for a task to survive restart of coordinating node
</description><key id="119514773">15117</key><summary>Task Management</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>feature</label><label>Meta</label><label>v6.0.0</label></labels><created>2015-11-30T15:28:45Z</created><updated>2017-07-19T12:50:00Z</updated><resolved>2017-07-19T12:50:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-02T15:52:06Z" id="161341746">I wonder if we need a way to store the results of a task until they are fetched? I'm thinking of something like update-by-query which would be a task because it is long running, cancelable, etc. But it wants to return counts of how many documents it updated and things like that. Maybe just write them to an index? Maybe with a ttl?
</comment><comment author="raf64flo" created="2015-12-02T16:08:43Z" id="161347394">Nice remark of @nik9000 about long task results availability after its end, as it is already done for snapshots.
But I'd prefer a TTL or/and a dedicated query to drop the result instead of only drop on fetch, which could be problematic in my opinion.
</comment><comment author="nik9000" created="2015-12-02T16:10:52Z" id="161348433">&gt; But I'd prefer a TTL or/and a dedicated query to drop the result instead of only drop on fetch, which could be problematic in my opinion.

Yeah - drop on fetch would be rough.

Not all tasks will want to do this but I think some would like it.
</comment><comment author="imotov" created="2015-12-02T17:29:19Z" id="161372493">@nik9000 is the goal to make results available after the task finished? 
</comment><comment author="nik9000" created="2015-12-02T17:53:41Z" id="161379934">&gt; @nik9000 is the goal to make results available after the task finished?

Yeah. In the case of update-by-query it'd be just to make the status available. The most "convenient" way to do it seems like write it to an index with a ttl - but I think I'm just stuck on that idea because it came to me. The point is that after the task is done you'll want to see what its results were for some period of time. You'd want some place you could fetch the results by task id, some way to clear out results when you've finished with them, some way for them to clear themselves out if you don't read them back soon enough.

I don't think it needs to come at iteration 1, but at some point it'd be nice.

Look at delete-by-query, it makes some effort to build a nice results object. Once it becomes a "task" it'll have nothing to do with the fancy result object.

Another thing that might be useful is to make an API that'd block until the task was finished and return the result of it. Or just fetch the result if it was already finished. This'd be super useful in general but kind of required for the REST tests because they don't have loops and things.
</comment><comment author="imotov" created="2015-12-02T19:54:23Z" id="161414119">I think traditionally we do that in two places - 1) log files for per-operation level and 2) in stats as combined metric. I can see how we might want to have a third way, but I think the biggest question here is lifecycle of this result. Persistence (even temporary persistence) of results is very unclear to me unless the result is associated with some persistent object (such as snapshot). So, I would rather make it an option to block and get result if you are interested in the result.
</comment><comment author="nik9000" created="2015-12-02T20:22:04Z" id="161422248">&gt; So, I would rather make it an option to block and get result if you are interested in the result

I don't know if that'll be enough in the end though. Imagine the delete-by-query operation that takes 30 minutes too complete. Its too long for any blocking to be reliable - all kinds of http equipment will time you out after 5 minutes and something is bound to sneak in and get you a `connection reset by peer`.

So you'd have to build in a retry to the blocking. But if results aren't persisted, at least for a little while, then there is always the possibility that the job will finish between one request timing out and the next one starting. A low possibility but an icky one.

Something like a TTL on the result with explicit commands to read the result and delete it would work. These results wouldn't be huge documents so we could probably keep them in memory, certainly if they were serialized xContent or implemented Accountable or something.

Its complicated but I can't think of how else to report on tasks that are "do a thing" rather than "make a thing".
</comment><comment author="niemyjski" created="2015-12-03T15:24:22Z" id="161674043">+1
</comment><comment author="clintongormley" created="2015-12-05T19:42:16Z" id="162240407">Another user of task mgmt: the forced merge API
</comment><comment author="nik9000" created="2015-12-07T19:28:56Z" id="162633511">&gt; Another user of task mgmt: the forced merge API

I wonder if we should add a list of users to the top, like, below the requirements. I'm happy to work on retrofitting some of our long running requests to make their status more fetch-able and to make them more cancel-able but we should should make a list/tag the old issues.
</comment><comment author="jprante" created="2015-12-21T23:44:47Z" id="166460652">Will it be possible to suspend/resume tasks by API? For perpetual tasks? Or to schedule tasks by a cron-like specification? It seems not, since the task design discusses TransportAction only, which means the lifetime of a task is "one-shot", corresponding to a request/response roundtrip of an action executed by a user?
</comment><comment author="imotov" created="2015-12-22T16:39:31Z" id="166668545">@jprante at the moment we are targeting use cases where tasks have clear start, stop and finite running time. We might extend it to perpetual tasks in the future, but this is not on the immediate road map.
</comment><comment author="nik9000" created="2015-12-22T16:53:56Z" id="166672189">&gt; Will it be possible to suspend/resume tasks by API?

I suspect that'll be opt-in in the same way that cancel will be opt-in. Reindex will probably opt in because it'll want to have API controlled throttling. So you could set the throttle to 0 and it'd just stop.  The bulk request powering it would timeout pretty soon, making the whole thing fail though.
</comment><comment author="wuranbo" created="2016-01-13T03:19:06Z" id="171148961">We will has the internal result of a long-term-running task? For example:
The query will retrieve total 100 shards on 10 elastic node, each node has 10 shards. We can get the result immediately when the first shard is done. Then the second shard is done, we reduce the result to the former, send notify to the API, so the user can update their view. When all the shards are done, send notify that the task is done.
So we can execute this long-term-running task in background with less thread one shard by another, release the CPU resources for the high-priority task. And the user can update the views of long-term-running task frequently, got a better user experience.
</comment><comment author="imotov" created="2016-01-14T02:48:53Z" id="171514150">@wuranbo you don't need task manager to do that. You can do it already by simply sending 100 requests with different [shards preferences](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/search-request-preference.html) and then keep track of the responses on the client side. When you get 100th response - the "task" is done. Since sort keys are returned with every result you can easily merge sort the results as they are coming in. Personally, I don't think that the complexity of such solution and overhead of communication will make it worthwhile but all technical capability are already there, so you can give it a shot.
</comment><comment author="wuranbo" created="2016-01-14T03:24:31Z" id="171518542">@imotov&#12288;thanks very much for your reply. You are right! This can pleasure most needs I want. I have checked out the shards preferences API. The shards preferences looks only set the shards (lucene's index) where the query retrieve from. The only API which can not be merged simply is cardinality, unless with a huge communication cost. Am I right? For the others, the cost looks like fine.
</comment><comment author="imotov" created="2016-01-14T03:46:56Z" id="171520991">@wuranbo it depends. In some use cases the communication and retrieval overhead can be overwhelming. Let's say you would like to show 25 results on the page. If you let elasticsearch to do its thing, it will only have to pull 25 results from the disk thanks to the [query_then_fetch execution](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/search-request-search-type.html#query-then-fetch) strategy. However, if you will execute a request against each shard separately, you will get first 25 results that you can display as soon as the first shard finishes, but eventually elasticsearch will have to pull 2500 results from the disk since each shard doesn't know which records to pull and has to pull all top 25 in case one of them should be displayed in the top 25. So, unless your intention is to retrieve everything, the overhead of populating top N results can be quite big. 

Since it turns into a search strategy discussion, let's move it to https://discuss.elastic.co/ if you have any other questions. I would like to keep the discussion here focused on Task Management. 
</comment><comment author="wuranbo" created="2016-01-14T04:43:16Z" id="171528320">@imotov got it , I have moved to https://discuss.elastic.co/t/how-get-the-just-enough-information-when-manually-merge-the-search-result-by-shards/39184/1 . Thank you very much again.
</comment><comment author="clintongormley" created="2016-02-11T11:02:53Z" id="182810662">@imotov would be good to report how long a task has been running in the tasks API as well.
</comment><comment author="imotov" created="2016-02-11T14:22:24Z" id="182884490">@clintongormley good idea, I have added it to the description 
</comment><comment author="nik9000" created="2017-02-14T14:35:42Z" id="279723462">@imotov, is this done now? I think we've decided not to do the "task can survive restart" thing, right?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle is still hiding errors when commands fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15116</link><project id="" key="" /><description>You have no idea what failed when a commandline execution fails, it just says "failed". at info level it gives more information, but this is wrong to do.

if we are executing a command, it is important, we need to see all the output, always, just like integ tests.
</description><key id="119510411">15116</key><summary>gradle is still hiding errors when commands fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-11-30T15:05:24Z</created><updated>2017-06-16T16:42:29Z</updated><resolved>2017-06-16T16:42:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-30T15:08:35Z" id="160655940">The worst is gradle hiding the IOException from Runtime.exec(), that is most critical to debugging things.
</comment><comment author="clintongormley" created="2016-02-14T18:13:49Z" id="183942439">@rjernst can this be closed yet?
</comment><comment author="rjernst" created="2016-02-14T18:18:54Z" id="183942722">I'm not sure. The situation was improved with LoggedExecTask, but I dont remember the circumstances of the IOException being hidden. 
</comment><comment author="clintongormley" created="2016-02-15T12:28:59Z" id="184186635">@rmuir do you know if this can be closed?
</comment><comment author="javanna" created="2017-06-16T16:42:29Z" id="309075268">I would go for closing this for now, we can always reopen whenever we find the same problem.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion suggester fields don't error out when given multi-fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15115</link><project id="" key="" /><description>**Edit:** As it turns out, the issue I originally had was expected behavior, but it would be better if mappings like this would be rejected when someone tries to set them, so I'm leaving the original issue text below:

I'm trying to specify a field like this in 2.1.0:

```
...
"city": {
    "type": "completion",
    "preserve_separators": false,
    "payloads": true,
    "fields": {
        "analyzed": {
            "type": "completion",
            "analyzer": "standard",
            "payloads": true,
        }
    }
}
...
```

Indexing seems to be okay, but when searching, I get an error saying that `"Field [city.analyzed] is not a completion suggest field"`. Am I wrong to expect this to work?
</description><key id="119508746">15115</key><summary>Completion suggester fields don't error out when given multi-fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">underyx</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-30T14:57:25Z</created><updated>2016-02-14T18:13:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T15:46:39Z" id="160666876">Hi @underyx 

Completion fields shouldn't support multi-fields, because a completion field can accept a structured object which wouldn't work with (eg) a `string` subfield.  I've marked this as a bug because we should complain about this at mapping time.
</comment><comment author="underyx" created="2015-11-30T15:48:53Z" id="160667716">I see, thank you, @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for path_style_access</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15114</link><project id="" key="" /><description>From https://github.com/elastic/elasticsearch-cloud-aws/pull/159

Add a new option `path_style_access` for S3 buckets. It adds support for path style access for [virtual hosting of buckets](http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html).
Defaults to `false`.

Closes https://github.com/elastic/elasticsearch-cloud-aws/issues/124.
</description><key id="119508490">15114</key><summary>Add support for path_style_access</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>feature</label><label>v5.0.0-alpha5</label></labels><created>2015-11-30T14:55:57Z</created><updated>2016-07-20T08:51:19Z</updated><resolved>2016-07-19T13:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-30T14:58:54Z" id="160653510">@tlrx Could you review please?

Note that I can't find a simple way to unit test this as it's super hard to build easily a `S3Repository` instance.
</comment><comment author="dadoonet" created="2015-12-10T08:02:13Z" id="163528270">@tlrx I rebased my changes on latest master version. 
</comment><comment author="dakrone" created="2016-04-06T20:43:21Z" id="206557750">@dadoonet looks like this is a bit stale, maybe you can rebase and @tlrx can review?
</comment><comment author="awislowski" created="2016-04-26T11:47:43Z" id="214714893">@dadoonet @dakrone 
I have made PR https://github.com/elastic/elasticsearch/pull/17985#issuecomment-214713131
that address same feature, but changes InternalAwsS3Service class api less.
</comment><comment author="dadoonet" created="2016-04-29T13:40:49Z" id="215715081">@tlrx I rebased the code onto master. Could you review please?
</comment><comment author="awislowski" created="2016-04-29T14:08:56Z" id="215727813">Could you rebase it on cloud-aws plugin for version 2.3.x also?

Current clients (f.e. me :) could use production plugin version without changes on my cluster, then.
</comment><comment author="dadoonet" created="2016-04-29T14:13:25Z" id="215729692">@awislowski I'm unsure we will backport it.

@clintongormley WDYT?
</comment><comment author="clintongormley" created="2016-04-29T16:37:43Z" id="215798772">@dadoonet it looks like we will do a 2.4, so i'd backport to the 2.x branch as well
</comment><comment author="awislowski" created="2016-04-29T19:12:47Z" id="215851966">@clintongormley thanks.

Will it be delivered as in the master as repository-s3 plugin or cloud-aws as in previous 2.x?
</comment><comment author="dadoonet" created="2016-04-29T20:41:52Z" id="215876057">cloud-aws for 2.x
</comment><comment author="tlrx" created="2016-05-02T08:14:30Z" id="216131832">Left minor comment. Do you know if there's a way to test that the setting is correctly propagated to the final request to the S3 service?
</comment><comment author="koolhead17" created="2016-05-06T07:06:07Z" id="217365585">:+1: - would love to get this merged for us @Minio to validate our integration with elasticsearch :-) 
Thanks.
</comment><comment author="dadoonet" created="2016-07-11T21:56:41Z" id="231878141">@tlrx I guess we have to trust what the AWS S3 client is doing when we call this:

``` java
client.setS3ClientOptions(new S3ClientOptions().withPathStyleAccess(pathStyleAccess));
```

Not sure how we can test it does it effectively and if we really need that.
Definitely this is something we should test once we have real integration tests with S3.
</comment><comment author="harshavardhana" created="2016-07-15T07:28:41Z" id="232882602">Tested this locally for some reason the set option is not reflected and elasticsearch complains, building elasticsearch from @dadoonet branch. 

```
[2016-07-15 00:24:11,361][WARN ][bootstrap                ] [Armageddon] uncaught exception in thread [main]
org.elasticsearch.bootstrap.StartupError: java.lang.IllegalArgumentException: unknown setting [repositories.s3.path_style_access] please check that any required plugins are installed, or check the breaking changes documentation for removed settings
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:105)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:96)
        at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:54)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:75)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:68)
Caused by: java.lang.IllegalArgumentException: unknown setting [repositories.s3.path_style_access] please check that any required plugins are installed, or check the breaking changes documentation for removed settings
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:270)
        at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:238)
        at org.elasticsearch.common.settings.SettingsModule.&lt;init&gt;(SettingsModule.java:138)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:259)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:192)
        at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:174)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:174)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:255)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:101)
        ... 6 more
(END)
```

Anything that i am missing?
</comment><comment author="harshavardhana" created="2016-07-15T07:35:10Z" id="232883636">&gt; @tlrx Could you review please?
&gt; 
&gt; Note that I can't find a simple way to unit test this as it's super hard to build easily a S3Repository instance.

You could use [Minio](https://github.com/minio/minio) with its FS backend locally, its a single binary. Should be simple enough to test this plugin as well.

Disclaimer: I work for Minio. 
</comment><comment author="dadoonet" created="2016-07-15T07:39:48Z" id="232884424">Ah! Good catch ! I forgot to register the setting probably!
</comment><comment author="harshavardhana" created="2016-07-15T08:15:31Z" id="232890725">Found the problem - this is needed for the setting to work, it is working fine locally now. 

```
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java b/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
index 0d6fac2..e695b0b 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/plugin/repository/s3/S3RepositoryPlugin.java
@@ -111,6 +111,7 @@ public class S3RepositoryPlugin extends Plugin implements RepositoryPlugin {
         S3Repository.Repositories.CHUNK_SIZE_SETTING,
         S3Repository.Repositories.COMPRESS_SETTING,
         S3Repository.Repositories.STORAGE_CLASS_SETTING,
+       S3Repository.Repositories.PATH_STYLE_ACCESS_SETTING,
         S3Repository.Repositories.CANNED_ACL_SETTING,
         S3Repository.Repositories.BASE_PATH_SETTING,
         S3Repository.Repositories.USE_THROTTLE_RETRIES_SETTING,
@@ -128,6 +129,7 @@ public class S3RepositoryPlugin extends Plugin implements RepositoryPlugin {
         S3Repository.Repository.CHUNK_SIZE_SETTING,
         S3Repository.Repository.COMPRESS_SETTING,
         S3Repository.Repository.STORAGE_CLASS_SETTING,
+       S3Repository.Repository.PATH_STYLE_ACCESS_SETTING,      
         S3Repository.Repository.CANNED_ACL_SETTING,
         S3Repository.Repository.BASE_PATH_SETTING);
     }
```
</comment><comment author="dadoonet" created="2016-07-15T08:22:38Z" id="232891988">Exactly! So everything is fine after this change?
</comment><comment author="harshavardhana" created="2016-07-15T08:33:59Z" id="232894055">&gt; Exactly! So everything is fine after this change?

Yes the request is going through, some signature related issue looking into it. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make HighlighterBuilder implement Writable, equals and hashCode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15113</link><project id="" key="" /><description>This change adds serialization by implementing Writeable to HighlighterBuilder. It also pulls out sixteen common fields and setters shared by HighlighterBuilder and its nested Field class into a new class AbstractHighlighterBuilder superclass which also gets it own equals() and hashCode() method and methods to serialize the common fields. Also adding tests for serialization roundtrip, hashCode and equals().
</description><key id="119507441">15113</key><summary>Make HighlighterBuilder implement Writable, equals and hashCode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T14:50:31Z</created><updated>2016-03-10T18:52:41Z</updated><resolved>2015-12-01T14:28:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-30T15:57:56Z" id="160670295">LGTM
</comment><comment author="nik9000" created="2015-11-30T15:58:08Z" id="160670350">Though I'm not an expert in these conversions. The highlighting part is sane.
</comment><comment author="cbuescher" created="2015-11-30T18:30:52Z" id="160715886">@nik9000 thanks for the review, I made a few minor updates including adressing your review comment, hold the merge for a bit longer in case you want to take another look.
</comment><comment author="nik9000" created="2015-11-30T19:42:12Z" id="160737071">It looks fine to me.
</comment><comment author="MaineC" created="2015-12-01T12:07:11Z" id="160948836">Left some minor thoughts. Overall LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add the ability to create multiple documents from one ingest document.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15112</link><project id="" key="" /><description>Could be useful to add the ability to create multiple documents from a single ingest document. Denormalization for instance would be a good candidate though we would need something to create unique ids for the extra documents.
Example:

```
 public Cursor&lt;IngestDocument&gt; execute(IngestDocument ingestDocument) throws Exception;
```
</description><key id="119500244">15112</key><summary>[Ingest] Add the ability to create multiple documents from one ingest document.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-30T14:13:42Z</created><updated>2016-02-14T18:11:40Z</updated><resolved>2016-02-14T18:11:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-01T16:56:07Z" id="161030134">Based on what rules should a document be split? Also this is going to be tricky when it comes to represent these additional documents back in the response, because ingest intercepts requests. Index response can only hold the response of one document and with bulk if the order of the responses don't match with the order of the request items that the bulk response becomes useless.
</comment><comment author="clintongormley" created="2016-02-14T18:11:40Z" id="183942293">Given the reasons pointed out by @martijnvg, this sounds like something better handled client side.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce Local checkpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15111</link><project id="" key="" /><description>This PR introduces the notion of a local checkpoint on the shard level. A local check point is defined as a the highest sequence number for which all previous operations (i.e. with a lower seq#) have been processed. 

The current implementation is based on a fixed in memory bit array which is used in a round robin fashion. This introduces a limit to the spread between inflight indexing operation. We are still discussing options to work around this, but I think we should move forward toward a working system and optimize from there (and either remove this limitation or better understand it's implications).

relates to #10708 
</description><key id="119498635">15111</key><summary>Introduce Local checkpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label><label>review</label></labels><created>2015-11-30T14:04:58Z</created><updated>2015-12-11T13:14:27Z</updated><resolved>2015-12-11T13:14:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-11T08:48:03Z" id="163876962">@jasontedor  I pushed a new approach. Can you take another look?
</comment><comment author="bleskes" created="2015-12-11T13:14:27Z" id="163934039">closing... github made this a mess.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix toXContent() for mapper attachments field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15110</link><project id="" key="" /><description>We must use simpleName() instead of name() because otherwise when the mapping
is generated as a string the field name will be the full path with dots
and that is illegal from es 2.0 on.

closes https://github.com/elastic/elasticsearch-mapper-attachments/issues/169
</description><key id="119497467">15110</key><summary>Fix toXContent() for mapper attachments field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Plugin Mapper Attachment</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T13:59:37Z</created><updated>2015-12-01T10:19:14Z</updated><resolved>2015-11-30T14:16:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-30T14:09:14Z" id="160640640">It looks good to me. 

Can you also push it to master and in elasticsearch-mapper-attachments project in `es-2.1` and `es-2.0` branches?
</comment><comment author="bleskes" created="2015-11-30T14:11:50Z" id="160641217">good catch.
</comment><comment author="brwe" created="2015-11-30T14:14:50Z" id="160641864">@dadoonet thanks for the quick review! will add it to elasticsearch-mapper-attachments project too.
</comment><comment author="rjernst" created="2015-11-30T18:36:56Z" id="160717459">Thanks @brwe!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] IngestDocument#setFieldValue(...) should deep copy maps and lists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15109</link><project id="" key="" /><description>If no deep copy of map/lists are being made then this can lead to pipelines being corrupted by index and bulk requests
</description><key id="119495751">15109</key><summary>[Ingest] IngestDocument#setFieldValue(...) should deep copy maps and lists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-30T13:52:00Z</created><updated>2015-12-01T15:02:32Z</updated><resolved>2015-12-01T15:02:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-30T14:00:27Z" id="160637835">left one small comment, LGTM otherwise
</comment><comment author="martijnvg" created="2015-11-30T14:22:51Z" id="160643541">@javanna I'v updated this PR after your additional comments.
</comment><comment author="martijnvg" created="2015-12-01T13:49:46Z" id="160975199">@javanna I've updated the PR.
</comment><comment author="javanna" created="2015-12-01T14:10:10Z" id="160979589">left a minor comment, no need for another round of review once you addressed that. LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add simple EditorConfig</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15108</link><project id="" key="" /><description>The [EditorConfig](http://editorconfig.org/) file applies the formatting rules described in `CONTRIBUTING.md`.

Many IDEs and text editors support the EditorConfig standard to apply formatting rules, e. g. IntelliJ IDEA: http://editorconfig.org/#download
</description><key id="119493331">15108</key><summary>Add simple EditorConfig</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joschi</reporter><labels><label>Meta</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T13:39:59Z</created><updated>2015-11-30T16:20:31Z</updated><resolved>2015-11-30T16:16:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-30T16:15:57Z" id="160675189">Looks good, I'll merge this in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow adding classpath plugins via NodeBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15107</link><project id="" key="" /><description>Analogous to `TransportClient.Builder#addPlugin()` allowing to add classpath plugins to a `TransportClient`, `NodeBuilder` now allows adding classpath plugins to a `Node` which wasn't possible before because the constructor taking a collection of plugin classes is package protected.

Refs #13055, closes #13212.
</description><key id="119491343">15107</key><summary>Allow adding classpath plugins via NodeBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joschi</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label></labels><created>2015-11-30T13:27:22Z</created><updated>2015-12-03T10:11:15Z</updated><resolved>2015-12-02T13:45:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T14:07:46Z" id="160640371">@rjernst could you take a look at this please
</comment><comment author="bdbogjoe" created="2015-12-01T14:01:36Z" id="160977719">i have same issue, now with 2.1.x not possible to add plugin from classpath, before i was using plugin.types setting

Is there any workaround ?
</comment><comment author="clintongormley" created="2015-12-02T13:45:08Z" id="161294126">Hi @joschi 

i chatted to @rjernst about this.  This change was made as part of locking Elasticsearch down with the Java security manager. Loading plugins from the classpath is no longer supported, and this change is intentional.  Plugins should be loaded via the plugin mechanism (ie added to the plugin directory, and be provided with a `plugin-descriptor.properties` file), in which case their privileges are locked down and they are subject to JarHell checks. 
</comment><comment author="bdbogjoe" created="2015-12-02T14:20:14Z" id="161311173">Pity, I'm using elasticsearch inside one Java webapp, and when the war is deployed, elasticsearch is started, and join cluster if any, or as local node. 
I don't know all details why you remove feature to add manually a plugin from classpath, but in my case how to solve my issue ?

For now cannot upgrade to 2.1.0 because of this
</comment><comment author="joschi" created="2015-12-02T14:45:51Z" id="161319844">@clintongormley @rjernst The problem with creating a separate plugin being loaded into Elasticsearch is, that it couldn't communicate internally with the hosting application anymore (in my case, we're using an in-process Guava [`EventBus`](https://google.github.io/guava/releases/18.0/api/docs/com/google/common/eventbus/EventBus.html) for this).

I totally get the intention of locking down what plugins can do when being loaded in Elasticsearch, but the Java Security Manager isn't being used in our application anyway and that's probably the case for most applications embedding an Elasticsearch (client) node.
</comment><comment author="bdbogjoe" created="2015-12-02T15:05:50Z" id="161325999">i tried to use protected constructor of Node and passing list of plugins classes, and actually works fine, but i prefer to have nicer solution.
I did not catch how to solve this issue with EventBus

Note : i'm loading mapper-attachment plugin
</comment><comment author="rmuir" created="2015-12-02T15:12:37Z" id="161328027">why would a client need mapper-attachments? Note: this is the absolute worst plugin to shove in the classpath, as it will add like 90 jars to your classpath.

That is why this mechanism is not supported: because PluginService loads plugins into their own classloader, means those 90 jars won't cause jar hell with whatever else is happening.
</comment><comment author="bdbogjoe" created="2015-12-02T16:14:19Z" id="161349459">i need this plugin because my app can run as local node (without any cluster)
</comment><comment author="rjernst" created="2015-12-02T16:16:34Z" id="161350137">Why don't you run an actual elasticsearch process (single node) and connect to it locally with a client?
</comment><comment author="bdbogjoe" created="2015-12-02T16:34:51Z" id="161357275">this is one solution, but in in dev mode (i'm using grails) i don't want to start in separate JVM another process
</comment><comment author="joschi" created="2015-12-02T16:50:21Z" id="161361737">&gt; Why don't you run an actual elasticsearch process (single node) and connect to it locally with a client?

This also adds the burden of having to manage an additional process on the system. For small data sets an embedded Elasticsearch node might just be more practical.

Additionally, HTTP clients or a `TransportClient` don't receive cluster state updates like client nodes do, but would have to poll for that information on a regular basis.
</comment><comment author="rjernst" created="2015-12-02T16:51:14Z" id="161361977">&gt; i don't want to start in separate JVM another process

I understand that JVMs can be heavyweight, but ES is already heavyweight, regardless of whether it is run in process or not. By running in a separate JVM, you will have more control over ES, and you will be running a setup that is actually tested. IMO the overhead of another JVM is tiny compared to the resources ES will use with a normal size index. If you have a tiny index (since you always have a single node, I assume you are not concerned with the data growing beyond a single node), the overhead may be a larger portion of the total resources. But having an isolated process that can manage itself and is well tested is better than injecting an application within a container, which it was not designed for.

Also, if you absolutely must run within a container, you can still install plugins outside of the container using normal mechanisms (ie bin/plugin). It is already required that you must set path.home in this embedded case.
</comment><comment author="rjernst" created="2015-12-02T16:53:02Z" id="161362476">&gt; Additionally, HTTP clients or a TransportClient don't receive cluster state updates like client nodes do, but would have to poll for that information on a regular basis.

TransportClient has a "sniff" mode that tries to be smart about this. But also, running in a single local node case would not matter to know the cluster state as there is only one node! 
</comment><comment author="joschi" created="2015-12-02T16:59:05Z" id="161364177">&gt; But also, running in a single local node case would not matter to know the cluster state as there is only one node!

That's completely correct, I've probably mixed up the use cases of @bdbogjoe and myself a bit. My use-case is being able to get notified about cluster state changes as fast as possible (nodes joined/left, indices created/closed/deleted).

I'll take a look into the "sniff" mode of `TransportClient`. Thanks for pointing me at it!
</comment><comment author="bdbogjoe" created="2015-12-02T17:14:51Z" id="161368714">Thanks @rjernst i will copy plugin in ES plugins folder before starting node
</comment><comment author="joschi" created="2015-12-02T17:21:17Z" id="161370389">@rjernst Correct me if I'm wrong, but it doesn't seem possible to create a `ClusterStateListener` in a plugin used in a `TransportClient`, is it?
</comment><comment author="rjernst" created="2015-12-02T18:39:36Z" id="161393110">&gt; it doesn't seem possible to create a ClusterStateListener in a plugin used in a TransportClient

I believe that is correct. I suggest we move this discussion to http://discuss.elastic.co and we can help understand what your use case is for needing the cluster state client side.
</comment><comment author="bdbogjoe" created="2015-12-03T09:41:56Z" id="161568944">so before starting my app i copy the plugin zip content in plugins folder but i have this issue : 

```
| Error java.lang.IllegalStateException: jar hell!
class: org.apache.tools.ant.taskdefs.optional.TraXLiaison
jar1: /home/eric/.sdkman/candidates/grails/2.5.3/lib/org.apache.ant/ant/jars/ant-1.9.4.jar
jar2: /home/eric/.sdkman/candidates/grails/2.5.3/lib/org.apache.ant/ant-trax/jars/ant-trax-1.7.1.jar
| Error         at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:280)
| Error         at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:186)
| Error         at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:336)
| Error         at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
| Error         at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
| Error         at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
| Error         at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
| Error         at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)
| Error         at org.elasticsearch.node.NodeBuilder$node$1.call(Unknown Source)
...
```

Any idea ??
</comment><comment author="dadoonet" created="2015-12-03T09:47:00Z" id="161570284">@bdbogjoe You have to fix that. The same Class is available twice in the classloader which is wrong.
</comment><comment author="bdbogjoe" created="2015-12-03T09:52:00Z" id="161571191">yes, seems grails issue, i fixed by doing this : 

```
String grailsHome = System.getProperty('grails.home')
String pathSeparator = System.getProperty("path.separator");
String classPath = System.getProperty('java.class.path')
classPath = classPath.split(pathSeparator).findAll {!it.isEmpty() &amp;&amp; (!grailsHome || !it.startsWith(grailsHome)) }.join(pathSeparator);
System.setProperty('java.class.path', classPath)
```

little bit tricky...
</comment><comment author="joschi" created="2015-12-03T10:11:14Z" id="161580900">@rjernst Moved to the discussion forums: https://discuss.elastic.co/t/add-plugins-from-classpath-in-embedded-elasticsearch-client-node/36269
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loads mapping from backup/temp files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15106</link><project id="" key="" /><description>I had a mapping file named test.json under 
**config&gt;mappings&gt;_default&gt;test.json**
Now I changed some of the properties in my json file and my editor creates a backfile named test.json~
**config&gt;mappings&gt;_default&gt;test.json~**

Now when I restart elastic search it loads both files and when I dump documents with type _"test"_ unfortunately the mapping under _"test.json~"_ is used.
</description><key id="119485784">15106</key><summary>Loads mapping from backup/temp files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vijicools</reporter><labels /><created>2015-11-30T12:50:10Z</created><updated>2015-11-30T14:06:23Z</updated><resolved>2015-11-30T14:06:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T14:06:23Z" id="160640035">File based mappings are no longer supported in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] org.elasticsearch.search.simple.SimpleSearchIT.testQueryNumericFieldWithRegex fails on Jenkins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15105</link><project id="" key="" /><description>Build URL:
http://build-us-00.elastic.co/job/es_feature_ingest/2843

reproduce command:

```
gradle :core:integTest -Dtests.seed=7945A7170257BF98 -Dtests.class=org.elasticsearch.search.simple.SimpleSearchIT -Dtests.method="testQueryNumericFieldWithRegex" -Des.logger.level=DEBUG -Dtests.assertion.disabled=org.elasticsearch -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseConcMarkSweepGC -XX:+UseCompressedOops" -Dtests.locale=hr_HR -Dtests.timezone=Africa/Accra
```

Error:

```
Expected: "Cannot use regular expression to filter numeric field [num]"      but: was "CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]"
```

Stack Trace:

```
java.lang.AssertionError: 
Expected: "Cannot use regular expression to filter numeric field [num]"
     but: was "CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]"
    at __randomizedtesting.SeedInfo.seed([7945A7170257BF98:655DB686FD9C0083]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.search.simple.SimpleSearchIT.testQueryNumericFieldWithRegex(SimpleSearchIT.java:343)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="119475279">15105</key><summary>[TEST] org.elasticsearch.search.simple.SimpleSearchIT.testQueryNumericFieldWithRegex fails on Jenkins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-11-30T11:39:30Z</created><updated>2015-12-01T11:13:33Z</updated><resolved>2015-12-01T11:13:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>allow to reboot cluster without reallocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15104</link><project id="" key="" /><description>While rebooting entire cluster to upgrade elasticsearch version or so, even though I use the same configuration, stopped data flow for a while, and reboot the exactly same nodes, it takes several hours to become 'green' after cluster reboot, reallocating every shard I have.

It might help ensuring data consistency, but as an end user I would be glad if there is a way to somehow 'finalize' es cluster before reboot, and then elasticsearch quickly gets 'green' after restart, using the same data of shards in the same nodes.
</description><key id="119470871">15104</key><summary>allow to reboot cluster without reallocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2015-11-30T11:11:17Z</created><updated>2015-11-30T11:16:05Z</updated><resolved>2015-11-30T11:16:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T11:16:05Z" id="160601566">Fixed by https://github.com/elastic/elasticsearch/pull/14652
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add java-api doc about shading / embedding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15103</link><project id="" key="" /><description>Two new sections added
- Dealing with JAR dependency conflicts
- Embedding jar with dependencies

Closes #15071.
</description><key id="119467075">15103</key><summary>add java-api doc about shading / embedding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T10:47:55Z</created><updated>2015-11-30T13:28:36Z</updated><resolved>2015-11-30T13:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-30T10:48:26Z" id="160595581">@clintongormley or @rmuir Wanna review this doc please?
</comment><comment author="rmuir" created="2015-11-30T13:20:29Z" id="160625093">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename "default" similarity into "classic" and map default module to classic.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15102</link><project id="" key="" /><description>To avoid default =&gt; default definition.
To prepare for the BM25 default switch in Lucene 6.0.
</description><key id="119465407">15102</key><summary>Rename "default" similarity into "classic" and map default module to classic.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>adoptme</label><label>breaking</label></labels><created>2015-11-30T10:38:15Z</created><updated>2015-12-30T16:42:40Z</updated><resolved>2015-12-30T16:42:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-11T10:58:57Z" id="163908806">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk queue size appears much bigger than the configured queue size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15101</link><project id="" key="" /><description>1. es version: 1.7.0
2. cluster: 3 dedicated master nodes, 10 data nodes
3. bulk.threadpool type: fix, thread size: 32, queue size: 30

under heavy write, marvel tell that bulk queue size is over 1.5k. why is it over my setting?
</description><key id="119460586">15101</key><summary>Bulk queue size appears much bigger than the configured queue size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Stats</label><label>feedback_needed</label></labels><created>2015-11-30T10:12:16Z</created><updated>2016-01-26T12:28:01Z</updated><resolved>2015-12-04T09:19:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T10:17:44Z" id="160587927">@makeyang could you paste the output of these requests (when under heavy write load):

```
GET _nodes/thread_pool
GET _nodes/stats/thread_pool
```

This will help us to see whether it is Elasticsearch or Marvel at fault.
</comment><comment author="makeyang" created="2015-11-30T10:36:01Z" id="160591360">@clintongormley 
GET _nodes/thread_pool:
https://github.com/makeyang/grocery/blob/master/_nodes-thread_pool.txt
GET _nodes/stats/thread_pool:
https://github.com/makeyang/grocery/blob/master/_nodes-stats-thread_pool.txt
</comment><comment author="clintongormley" created="2015-11-30T11:09:32Z" id="160599721">@makeyang currently your nodes stats are showing nothing in the bulk queues, just a number of previous rejections.  Please repost these stats when you are seeing active rejections. 
</comment><comment author="makeyang" created="2015-12-02T10:36:50Z" id="161253299">@clintongormley  I updated files
GET _nodes/thread_pool:
https://github.com/makeyang/grocery/blob/master/_nodes-thread_pool.txt
GET _nodes/stats/thread_pool:
https://github.com/makeyang/grocery/blob/master/_nodes-stats-thread_pool.txt

u can see that set  thread pool size is 30 and I posted stats file, there are some nodes pool size is over 30.
</comment><comment author="clintongormley" created="2015-12-02T12:22:37Z" id="161276215">I still don't understand.  According to the data you've posted, your bulk threadpools are configured as follows:

```
        "bulk": {
           "type": "fixed",
           "min": 48,
           "max": 48,
           "queue_size": "30"
        },
```

And the bulk queue is currently empty, although you have had many rejections:

```
        "bulk": {
           "threads": 48,
           "queue": 0,
           "active": 30,
           "rejected": 7352,
           "largest": 48,
           "completed": 145157
        },
```

Are you not confusing rejections with the queue size?

Could you paste a screenshot of Marvel showing what you think is an error?
</comment><comment author="makeyang" created="2015-12-03T01:20:46Z" id="161485797">@clintongormley 
look at the node nRDJU5LjRRi3U2TK1ndJ3g 
 "bulk": {
               "threads": 48,
               "queue": 57,
               "active": 48,
               "rejected": 24547,
               "largest": 48,
               "completed": 140952
            },
the queue is 57
</comment><comment author="makeyang" created="2015-12-04T01:19:41Z" id="161844219">anybody can take a look at this issue?
</comment><comment author="bleskes" created="2015-12-04T09:19:36Z" id="161916979">@makeyang the bulk queues sizes are intended to back pressure incoming operations on the primary (and thus to the client). Once an operation gets in the primary, we have to make sure it is also indexed on the replicas. As such, we insert these operations to the bulk queues on nodes with replicas, even if the queue is "over capacity". 

I also noted that the thread size is not what you asked.  I think you use `thread_size` and not just `size` in your config, but that's just guess work.

Closing for now as it is "by design". Let me know if you have any questions. 
</comment><comment author="makeyang" created="2015-12-07T01:30:57Z" id="162385990">if it is by desing, then it is design by simple, not design be reasonable.
with the queue over capacity, it is easy to cause memory issue, GC issue, cluster state unstaybility.
Hope u guys can make progress on this spot ASAP.
</comment><comment author="sunyonggang" created="2016-01-26T06:10:03Z" id="174850250">i'm sorry, but i met the same problem.
1: es version 1.4.4, with 5 nodes. use the default bulk queue_size(50)
when i upload about 2k message, ES can only get 1.8k, after detection, i get the bulk response with every message, like this:

```
{"index":{"_index":"name@2015-12-12","_type":"vn","_id":"b01c65c7dab6094b4cb099451c47c474b86228ef9b0d53f8b90f625c42e485da","status":429,"error":"RemoteTransportException[[ali-tsi-0b-svg-p-01-svm-a-009][inet[/10.144.210.10:9300]][indices:data/write/bulk[s]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 50) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1@23deb51c]; "}}
```

so i want to find a solution to it. so how to configure the specified number of the bulk queue_size? can i just choose 500 or 1000?
these will be helpful

```
"bulk" : {
          "type" : "fixed",
          "min" : 4,
          "max" : 4,
          "queue_size" : "50"
        },

"bulk" : {
          "threads" : 4,
          "queue" : 0,
          "active" : 0,
          "rejected" : 1137,
          "largest" : 4,
          "completed" : 14785507
        },
```

&#128123;
</comment><comment author="clintongormley" created="2016-01-26T12:28:01Z" id="174980614">@sunyonggang the right place to ask questions like these is the forum: http://discuss.elastic.co/

Short answer: you either need to slow down on how fast you're sending data, or increase the indexing capacity of the structure by adding more nodes or better disks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.1.0 wouldn't start on OSX 10.10.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15100</link><project id="" key="" /><description>Hi!
Using Elasticsearch 2.1.0 on OSX 10.10.5 Yosemite.
When starting it, getting:

```
/usr/local/bin/elasticsearch                                                                                                                                                                                                                           
[2015-11-29 22:11:09,591][INFO ][node                     ] [Samuel Guthrie] version[2.1.0], pid[59155], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-11-29 22:11:09,591][INFO ][node                     ] [Samuel Guthrie] initializing ...
[2015-11-29 22:11:09,651][INFO ][plugins                  ] [Samuel Guthrie] loaded [], sites []
[2015-11-29 22:11:09,668][INFO ][env                      ] [Samuel Guthrie] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [63.2gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2015-11-29 22:11:11,352][ERROR][gateway                  ] [Samuel Guthrie] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [.kibana], reason: [Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:335)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
    ... 48 more
[2015-11-29 22:11:11,416][ERROR][gateway                  ] [Samuel Guthrie] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [.kibana], reason: [Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:335)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
    ... 39 more
[2015-11-29 22:11:11,554][ERROR][gateway                  ] [Samuel Guthrie] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [.kibana], reason: [Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:335)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
    ... 30 more
Exception in thread "main" java.lang.IllegalStateException: unable to upgrade the mappings for the index [.kibana], reason: [Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]]
Likely root cause: java.lang.IllegalArgumentException: Mapper for [version] conflicts with existing mapping in other types:
[mapper [version] cannot be changed from type [long] to [int]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Please help!
Thank you!
</description><key id="119425789">15100</key><summary>Elasticsearch 2.1.0 wouldn't start on OSX 10.10.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Drewshg312</reporter><labels /><created>2015-11-30T06:14:22Z</created><updated>2015-12-04T06:10:18Z</updated><resolved>2015-11-30T08:28:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T08:28:31Z" id="160553614">It appears you have an old `.kibana` index which has conflicting mappings, which are no longer allowed in 2.0.  I suggest deleting that index and trying again.
</comment><comment author="Drewshg312" created="2015-12-04T03:47:31Z" id="161866362">but how can I do this without running elasticsearch server?
</comment><comment author="dadoonet" created="2015-12-04T06:08:39Z" id="161886123">You can potentially remove it manually. In data dir...
</comment><comment author="Drewshg312" created="2015-12-04T06:10:18Z" id="161886264">done. thanx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Elasticsearch 2.1.0 wouldn't start on OSX 10.10.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15099</link><project id="" key="" /><description>Hi!
Using Elasticsearch 2.1.0 on OSX 10.10.5 Yosemite.
When starting it getting:

```
$ /usr/local/bin/elasticsearch                                                                                                                                                                                                                           
[2015-11-29 17:45:28,554][INFO ][node                     ] [Bison] version[2.1.0], pid[56327], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-11-29 17:45:28,554][INFO ][node                     ] [Bison] initializing ...
Exception in thread "main" java.lang.IllegalStateException: Unable to initialize plugins
Likely root cause: java.nio.file.NoSuchFileException: /usr/local/var/lib/elasticsearch/plugins/bigdesk/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.Files.newByteChannel(Files.java:407)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
    at java.nio.file.Files.newInputStream(Files.java:152)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:302)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:108)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

Please help!
Thank you!
</description><key id="119417585">15099</key><summary> Elasticsearch 2.1.0 wouldn't start on OSX 10.10.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Drewshg312</reporter><labels /><created>2015-11-30T04:40:25Z</created><updated>2015-11-30T09:10:16Z</updated><resolved>2015-11-30T06:06:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-11-30T05:31:47Z" id="160519377">This looks like a plugin issue. Can you remove the `bigdesk` plugin and retry? Seems that this particular plugin is not ready yet for 2.x
</comment><comment author="Drewshg312" created="2015-11-30T06:01:31Z" id="160526220">how can I remove the plugin?
</comment><comment author="spinscale" created="2015-11-30T06:06:01Z" id="160526593">You can run `plugin list` and check if the bigdesk plugin is listed, then you can run `plugin remove bigdesk`. Alternatively delete the `/usr/local/var/lib/elasticsearch/plugins/bigdesk` directory.

Closing this as it is not an elasticsearch issue, but expected behaviour. Feel free to reopen, if anything goes wrong after removing too old plugins.
</comment><comment author="Drewshg312" created="2015-11-30T06:08:36Z" id="160526804">Thank you! Does that mean that bigdesk is not supported anymore? It was pretty handy... Is there anything else instead?
</comment><comment author="spinscale" created="2015-11-30T06:17:09Z" id="160528696">The bigdesk maintainers need to upgrade the plugin. You can take a look at a couple of other plugins like [marvel](https://www.elastic.co/products/marvel) or those listed [in the official docs](https://www.elastic.co/guide/en/elasticsearch/plugins/current/management.html)
</comment><comment author="Drewshg312" created="2015-11-30T06:19:55Z" id="160530497">Thank you very much! By the way, I've opened the issue again, because removing the plugin did not actually let me to run it successfully((( https://github.com/elastic/elasticsearch/issues/15100
</comment><comment author="lukas-vlcek" created="2015-11-30T09:10:16Z" id="160564328">@Drewshg312 I am sorry Bigdesk does not support ES 2 and above ATM. Once it does I will drop a note to forums.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve output when integ test fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15098</link><project id="" key="" /><description>This outputs a lot more info when integ tests fail to start, as well as
(should) fix windows (at least in my VM run) integ tests.
</description><key id="119413963">15098</key><summary>Improve output when integ test fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-30T03:46:40Z</created><updated>2015-11-30T19:24:37Z</updated><resolved>2015-11-30T03:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-30T03:52:42Z" id="160509410">looks good, thanks for adding this. should make debugging jenkins much easier.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add workaround for JDK-6427854</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15097</link><project id="" key="" /><description>See e.g. http://build-us-00.elastic.co/job/es_feature_ingest/2831/consoleFull

The bug can still happen, so we should let netty do this workaround
</description><key id="119388733">15097</key><summary>Add workaround for JDK-6427854</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-11-29T21:13:38Z</created><updated>2016-03-10T18:53:56Z</updated><resolved>2015-11-29T22:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-29T22:16:14Z" id="160475928">LGTM
</comment><comment author="martijnvg" created="2015-11-29T22:19:35Z" id="160476089">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation incorrect - reference/2.0/modules-scripting.html</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15096</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-scripting.html

Documentation example below causes "script_score query does not support [file]" error

```
     "script_score": {
        "lang": "groovy",
        "file": "calculate-score",
        "params": {
          "my_modifier": 8
        }
      }
```

Should be 

"script_file": "calculate-score"
</description><key id="119374859">15096</key><summary>Documentation incorrect - reference/2.0/modules-scripting.html</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nicksanders</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-11-29T18:12:11Z</created><updated>2015-11-30T10:13:15Z</updated><resolved>2015-11-30T08:25:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-30T08:21:53Z" id="160552702">The `script_score` in function score still uses the old scripting syntax.  Must have been missed in the script cleanup in https://github.com/elastic/elasticsearch/pull/11164

@colings86 could you take a look please?
</comment><comment author="clintongormley" created="2015-11-30T08:25:17Z" id="160553145">whoops - i tested the wrong syntax, `script_score` works as intended.

@nicksanders presumably you're not using Elasticsearch 2.0.  You should look at the documentation for the version you're using
</comment><comment author="nicksanders" created="2015-11-30T09:24:23Z" id="160568754">@clintongormley I was using Elasticsearch 2.0 and got the following error

script_score query does not support [file]

It is possible I was doing something else wrong but when I changed "file" to "script_file" it worked ;)
</comment><comment author="clintongormley" created="2015-11-30T09:53:37Z" id="160580100">@nicksanders you're missing one layer (and so you're using the undocumented bwc syntax). Should be:

```
"script_score": {
  "script": {
    "lang": "groovy",
    "file": "calculate-score",
    "params": {
      "my_modifier": 8
    }
  }
}
```

See https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-script-score
</comment><comment author="nicksanders" created="2015-11-30T10:02:19Z" id="160582688">@clintongormley the example on https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-scripting.html is missing one layer too ;)
</comment><comment author="clintongormley" created="2015-11-30T10:13:14Z" id="160586598">thanks @nicksanders - fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mapper-attachments platform-dependent newline is harmful</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15095</link><project id="" key="" /><description>Mapper-attachments extraction (somewhere in our code, or tika itself, who knows), uses the newline of the operating system running elasticsearch during extraction.

This means extracted bytes differ on windows vs unix, which at first sounds harmless, but its really not, because it can have a snowball effect, causing a different character set to be detected and so on down the road.

We should see if there is a way to fix this... for now I disabled exact bytes checks in the tests (see #15094)
</description><key id="119370417">15095</key><summary>mapper-attachments platform-dependent newline is harmful</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Ingest Attachment</label><label>adoptme</label><label>bug</label></labels><created>2015-11-29T16:58:34Z</created><updated>2016-09-27T16:21:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T18:10:26Z" id="183942229">@spinscale could you please check if this is still the same in the new ingest attachment plugin?
</comment><comment author="dakrone" created="2016-09-27T16:21:37Z" id="249917232">@spinscale ping on this, if it's still present in 5.0 for ingest node we should probably document it somewhere prior to the release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>do not assert charset or exact lengths for mapper-attachments tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15094</link><project id="" key="" /><description>Its enough to test the content type for what we are testing.

Currently tests are flaky if charset is detected as e.g. windows-1252 vs iso-8859-1 and so on.
In fact, they fail on windows 100% of the time.

We are not trying to test charset detection heuristics (which might be different even due to newlines in tests or other things).
If we want to do test that, we should test it separately.

We have the same issue with testing for exact content lengths. Currently when extracting from a PDF, this will differ (unfortunately) depending on whether ES is running on windows vs unix. If we want to fix this, we should fix it in tika, but for now, its platform dependent.
</description><key id="119370147">15094</key><summary>do not assert charset or exact lengths for mapper-attachments tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-29T16:52:05Z</created><updated>2015-11-29T17:12:18Z</updated><resolved>2015-11-29T17:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-29T16:59:05Z" id="160432025">By the way, i don't like this either, i opened a bug to follow up: https://github.com/elastic/elasticsearch/issues/15095

But i think we should stop the constant jenkins failures on windows, since the problem is known.
</comment><comment author="dadoonet" created="2015-11-29T17:09:34Z" id="160432486">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix integration tests and 'gradle run' to work on windoze</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15093</link><project id="" key="" /><description>`esArgs` was being completely ignored, meaning we were just running `cmd.exe` with no parameters. This is why things would just hang.

I know jenkins is failing, but I really do not want to push this fix until #15092 at least is addressed, ideally #15091 too. It was hellacious to debug and I do not wish to go through the pain again.

Closes #15084
</description><key id="119365855">15093</key><summary>Fix integration tests and 'gradle run' to work on windoze</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-29T15:42:04Z</created><updated>2015-11-29T17:15:30Z</updated><resolved>2015-11-29T17:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-29T17:15:06Z" id="160432741">This fix looks good. I will work on improving the output.
</comment><comment author="rmuir" created="2015-11-29T17:15:27Z" id="160432758">thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle needs to print out every single commandline it executes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15092</link><project id="" key="" /><description>The maven build does this, it prints ENV and parameters for every commandline. This makes it easy to reproduce problems and debug issues.

But gradle is too succinct, it makes it impossible to debug e.g. why integration tests don't work on windows.

It really needs to print every command being executed.
</description><key id="119363393">15092</key><summary>gradle needs to print out every single commandline it executes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-29T14:49:19Z</created><updated>2016-02-14T18:16:43Z</updated><resolved>2016-02-14T18:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T18:09:13Z" id="183942117">@rjernst can this be closed yet?
</comment><comment author="rjernst" created="2016-02-14T18:16:43Z" id="183942592">We added LoggedExecTask for this purpose and use it, so I think so. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default gradle build to --debug --stacktrace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15091</link><project id="" key="" /><description>The gradle build in master "hides" too much, yet it is not stabilized (e.g. integ tests dont work on windows).

We should default to full output (verbose as possible) until everything is working. This idea of super-succinct output is not good until everything is stabilized: it just discourages anyone from fixing problems because they cannot see what is wrong.
</description><key id="119362448">15091</key><summary>default gradle build to --debug --stacktrace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-29T14:25:32Z</created><updated>2016-02-14T18:15:43Z</updated><resolved>2016-02-14T18:15:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T18:08:52Z" id="183942083">@rjernst can this be closed yet?
</comment><comment author="rjernst" created="2016-02-14T18:15:43Z" id="183942539">In CI, we run gradle clean --stacktrace as a first step to the build, so that we catch any configuration errors with the stacktrace. Execution is different, since gradle uses exceptions to signal build failure, but does not distinguish unknown exceptions from a normal build failure. If there was a way to default both stacktrace and debug, it would be extremely noisy, so from that alone I dont think we would want this, but there is also the fact there is simply no way to enable these by default. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pull Fields instance once from LeafReader in completion stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15090</link><project id="" key="" /><description>Closes #6593
</description><key id="119353403">15090</key><summary>Pull Fields instance once from LeafReader in completion stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-29T11:14:06Z</created><updated>2015-12-02T18:17:01Z</updated><resolved>2015-11-29T11:17:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-29T11:16:42Z" id="160403294">LGTM
</comment><comment author="areek" created="2015-12-02T18:17:01Z" id="161387295">FYI @mikemccand, I backported this change to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix delayed extra config file checks to be right before copy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15089</link><project id="" key="" /><description>The current delay waits until later after normal configuration, but
still just after resolution (ie when paths would be known for
dependencies but not actual execution). This delays the checks further
to be done right before we actually execute the copy task.

closes #15068
</description><key id="119330824">15089</key><summary>Fix delayed extra config file checks to be right before copy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-29T02:07:16Z</created><updated>2015-11-29T03:44:20Z</updated><resolved>2015-11-29T03:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-29T03:42:42Z" id="160365499">Lgtm
On Nov 28, 2015 9:07 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; The current delay waits until later after normal configuration, but
&gt; still just after resolution (ie when paths would be known for
&gt; dependencies but not actual execution). This delays the checks further
&gt; to be done right before we actually execute the copy task.
&gt; 
&gt; ## closes #15068 https://github.com/elastic/elasticsearch/issues/15068
&gt; 
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;   https://github.com/elastic/elasticsearch/pull/15089
&gt; Commit Summary
&gt; - Build: Fix delayed extra config file checks to be right before copy
&gt; 
&gt; File Changes
&gt; - _M_
&gt;   buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterFormationTasks.groovy
&gt;   https://github.com/elastic/elasticsearch/pull/15089/files#diff-0 (8)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elastic/elasticsearch/pull/15089.patch
&gt; - https://github.com/elastic/elasticsearch/pull/15089.diff
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/15089.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move debug options setting to before command line is logged</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15088</link><project id="" key="" /><description>Currently if running with --info, the command line for ES, along with
env vars, are logged before they may be ammended to add debug options.
This moves the adding JAVA_OPTS to before we print the command.
</description><key id="119328076">15088</key><summary>Move debug options setting to before command line is logged</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-29T01:07:53Z</created><updated>2015-11-29T01:19:40Z</updated><resolved>2015-11-29T01:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-29T01:13:02Z" id="160353447">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setup standalone tests to compile in intellij</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15087</link><project id="" key="" /><description>This adds the standalone tests so they will compile (and thus can be
modified with import completion) within IntelliJ. It also explicitly
sets up buildSrc as a module.

Note that this does _not_ mean eg evil-tests can be run from intellij.
These are special tests that require special settings (eg disabling
security manager). They need to be run from the command line.

closes #15075
</description><key id="119327882">15087</key><summary>Setup standalone tests to compile in intellij</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-29T00:58:27Z</created><updated>2015-12-08T00:02:55Z</updated><resolved>2015-12-08T00:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix rpm and deb generated poms to have correct artifactId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15086</link><project id="" key="" /><description>For all our distributions, we use "elasticsearch" as the artifactId.
This fixes the id for the generated poms of rpm and deb distributions.
</description><key id="119314261">15086</key><summary>Fix rpm and deb generated poms to have correct artifactId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-28T19:32:06Z</created><updated>2015-11-28T22:06:26Z</updated><resolved>2015-11-28T22:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-28T21:41:19Z" id="160338278">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove a trailing comma from an example data of JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15085</link><project id="" key="" /><description>This PR fixes a JSON-formatted fault in [post-filter.asciidoc](https://github.com/elastic/elasticsearch/blob/148265bd164cd5a614cd020fb480d5974f523d81/docs/reference/search/request/post-filter.asciidoc) due to a trailing comma.
</description><key id="119306999">15085</key><summary>Remove a trailing comma from an example data of JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kaneshin</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-11-28T17:02:12Z</created><updated>2015-11-30T07:05:46Z</updated><resolved>2015-11-30T07:05:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T17:51:53Z" id="160324253">Hi @kaneshin 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="kaneshin" created="2015-11-28T18:15:40Z" id="160325624">Hi @clintongormley 

I signed the CLA.

Thanks.
</comment><comment author="clintongormley" created="2015-11-30T07:05:46Z" id="160541468">thanks @kaneshin - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>3.0.0 tar tests having lots of trouble on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15084</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_window-2012/2067/console

Not sure if this is a windows issue or what.
</description><key id="119299305">15084</key><summary>3.0.0 tar tests having lots of trouble on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-28T14:49:22Z</created><updated>2015-11-29T17:15:30Z</updated><resolved>2015-11-29T17:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-28T14:51:59Z" id="160306925">I don't think these tests have ever worked on windows since we moved to gradle.
</comment><comment author="rmuir" created="2015-11-29T14:39:52Z" id="160419776">ES is definitely not getting even close to starting up. I can reproduce the fail easily, there is nothing in logs/ folder or even a logs/ at all, so I don't think bin/elasticsearch.bat is ever invoked.

Maybe its a bug with spaces/windows integ logic in the build. I'll try to dig.
</comment><comment author="rmuir" created="2015-11-29T15:17:55Z" id="160422010">This code is just super broken and messy. E.g. 'esArgs' is not even being used.

Time to ditch all the useless closures and just have a simple if (Windows) in one place that does the right thing there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update all "official" plugins with a single command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15083</link><project id="" key="" /><description>Follow up for #15072 

It could be super handy to update all plugins with a single command.
Basically as when you upgrade elasticsearch you must upgrade all plugins, it could make sense to support a command like:

``` sh
bin/plugin update _all
```

This would internally:
- list existing plugins
- for each plugin
  - remove it
  - install it
</description><key id="119297485">15083</key><summary>Update all "official" plugins with a single command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>discuss</label></labels><created>2015-11-28T14:25:00Z</created><updated>2015-12-01T11:24:51Z</updated><resolved>2015-12-01T11:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mfn" created="2015-11-28T15:57:42Z" id="160314427">Naming suggestion: `update-all` instead of some magic `_all` "package?
</comment><comment author="dadoonet" created="2015-11-28T17:22:03Z" id="160321928">Yeah I thought about this as well. I was wondering to support _all for remove as well but as there is almost no interest doing this, using update-all is definitely better.
</comment><comment author="dadoonet" created="2015-12-01T11:24:51Z" id="160939477">As https://github.com/elastic/elasticsearch/issues/15000 won't be merged, this issue is not relevant anymore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reject refresh usage in bulk items when using and fix NPE when no source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15082</link><project id="" key="" /><description>The REST bulk API rejects use of `refresh` at the item level. But the Java API lets the user setting it.

We need to have the same behavior and don't let think the user he can define `refresh` per bulk item.

Note that the user can still define `refresh` on the bulk itself.

Closes #7361.
</description><key id="119296862">15082</key><summary>Reject refresh usage in bulk items when using and fix NPE when no source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>bug</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-28T14:16:36Z</created><updated>2015-12-30T17:03:24Z</updated><resolved>2015-12-01T11:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-28T14:18:03Z" id="160302369">@bleskes Could you review it please?
</comment><comment author="bleskes" created="2015-11-30T15:26:28Z" id="160660656">left a minor comment. Thanks for picking this up?
</comment><comment author="dadoonet" created="2015-11-30T16:47:21Z" id="160685815">@bleskes Updated with another commit. BTW I found a NPE in another part of the test while coding tests.
</comment><comment author="bleskes" created="2015-12-01T09:13:25Z" id="160901995">LGTM. Left two minor suggestions.
</comment><comment author="dadoonet" created="2015-12-01T10:50:16Z" id="160932589">@bleskes I added another commit and I'd prefer you double check before pushing!

Thanks! 
</comment><comment author="bleskes" created="2015-12-01T10:52:01Z" id="160933111">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `_index` field mapping does not accept `enabled`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15081</link><project id="" key="" /><description>According to #9870, the `_index` field mapping should accept the `enabled` parameter, but it throws an exception:

```
PUT trial
{
  "mappings": {
    "doc": {
      "_index": {
        "enabled": true
      }
    }
  }
}
```

Returns:

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "Mapping definition for [_index] has unsupported parameters:  [enabled : true]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "mapping [doc]",
      "caused_by": {
         "type": "mapper_parsing_exception",
         "reason": "Mapping definition for [_index] has unsupported parameters:  [enabled : true]"
      }
   },
   "status": 400
}
```
</description><key id="119291093">15081</key><summary>The `_index` field mapping does not accept `enabled`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-28T12:34:18Z</created><updated>2015-11-30T07:44:00Z</updated><resolved>2015-11-30T07:43:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-28T22:50:27Z" id="160343745">We removed this setting, as it is no longer necessary: #12329.

I think we just need to update the docs?
</comment><comment author="clintongormley" created="2015-11-30T07:44:00Z" id="160545991">@rjernst I'd completely forgotten about that.  Thanks for the reminder. I've updated the docs accordingly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeout settings (default to 5 minutes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15080</link><project id="" key="" /><description>By default, azure does not timeout. This commit adds support for a timeout settings which defaults to 5 minutes.
It's a timeout **per request** not a global timeout for a snapshot request.

It can be defined globally, per account or both. Defaults to `5m`.

``` yml
cloud:
    azure:
        storage:
            timeout: 10s
            my_account1:
                account: your_azure_storage_account1
                key: your_azure_storage_key1
                default: true
            my_account2:
                account: your_azure_storage_account2
                key: your_azure_storage_key2
                timeout: 30s
```

In this example, timeout will be 10s for `my_account1` and 30s for `my_account2`.

Closes #14277.
</description><key id="119289842">15080</key><summary>Add timeout settings (default to 5 minutes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-28T12:00:08Z</created><updated>2015-12-29T11:07:04Z</updated><resolved>2015-12-29T11:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-28T12:00:24Z" id="160287644">@tlrx Wanna review this?
</comment><comment author="dadoonet" created="2015-11-28T12:00:57Z" id="160287674">cc @craigwi feel free to comment on this PR :D 
</comment><comment author="craigwi" created="2015-11-30T22:27:39Z" id="160781877">LGTM.  Thanks @dadoonet.
</comment><comment author="dadoonet" created="2015-12-10T08:11:48Z" id="163529591">If no one objects, I'd like to merge this PR hopefully tomorrow morning CET.
@bleskes agreed?
</comment><comment author="tlrx" created="2015-12-10T09:03:55Z" id="163538537">Left a minor comment, other than that LGTM (and sorry for the time it took me to review it)
</comment><comment author="dadoonet" created="2015-12-11T21:55:12Z" id="164059810">@tlrx I added a new commit. Just want to double check that is what you were talking about. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES2.1.0 "no handler for type [murmur3] declared on field [hash]"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15079</link><project id="" key="" /><description>Maybe related to https://github.com/elastic/elasticsearch/issues/14828 but this happens when I put mapping, not when I start ES.

I try to use murmur3 and have this error when putting mapping in ES2.1.0

Here is the final error

```
{
    "traces_v1": {
        "aliases": {
            "traces": {}
        }
    }
} {
    "error": {
        "root_cause": [{
            "type": "mapper_parsing_exception",
            "reason": "no handler for type [murmur3] declared on field [hash]"
        }],
        "type": "mapper_parsing_exception",
        "reason": "no handler for type [murmur3] declared on field [hash]"
    },
    "status": 400
}
```

And here is my bash file. It is for reseting/reindexing an indice via logstash.

```
#!/bin/bash

curl -XDELETE 'http://xxx:9200/traces_v1'

curl -XPUT 'http://xxx:9200/traces_v1' -d '
index :
    number_of_shards : 3
'

curl -XPOST 'http://xxx:9200/_aliases' -d'
{
    "actions": [
        { "add": {
            "alias": "traces",
            "index": "traces_v1"
        }}
    ]
}'

curl -XGET 'http://xxx:9200/_aliases?v'

curl -XPUT http://xxx:9200/traces_v1/_mapping/website' -d '
{
  "properties": {
    "loc": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string"
        },
        "coordinates":{
          "type": "geo_point",
          "geohash":true,
          "geohash_prefix":true,
          "lat_lon":true,
          "fielddata" : {
              "format" : "compressed",
              "precision" : "1cm"
          }
        }
      }
    },
    "trackerId":{
      "type": "string",
      "index" : "not_analyzed"
    },
    "geoip": {
      "type": "boolean"
    },
    "date": {
      "type": "date"
    },
    "visitor": {
      "type": "object",
      "properties": {
        "returning": {
          "type": "boolean"
        },
        "uuid":{
          "type" : "string",
          "fields": {
            "hash": {
              "type": "murmur3" 
            }
          }
        },
        "navigator":{
          "type" : "object",
          "properties": {
            "appCodeName":{
              "type" : "string"
            },
            "appName":{
              "type" : "string"
            },
            "appVersion":{
              "type" : "string"
            },
            "language":{
              "type" : "string"
            },
            "platform":{
              "type" : "string"
            },
            "userAgent":{
              "type" : "string"
            }
          }
        }
      }
    },
    "usage":{
      "type" : "object",
      "properties": {
        "actualPage":{
          "type" : "string",
          "fields": {
            "hash": {
              "type": "murmur3" 
            }
          }
        },
        "referrer":{
          "type" : "string",
          "fields": {
            "hash": {
              "type": "murmur3" 
            }
          }
        }
      }
    }
  }
}'
```
</description><key id="119289024">15079</key><summary>ES2.1.0 "no handler for type [murmur3] declared on field [hash]"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dagatsoin</reporter><labels /><created>2015-11-28T11:48:12Z</created><updated>2017-05-21T10:30:33Z</updated><resolved>2015-11-28T12:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-28T11:49:32Z" id="160284395">Can you print the result of `GET _cat/plugins?v` please?
</comment><comment author="dagatsoin" created="2015-11-28T11:56:00Z" id="160286588">You right, murmur 3 is not loaded.

```
name        component version type url            
Honey Lemon head      master  s    /_plugin/head/ 
```

However I have done `bin/plugin install mapper-murmur3` to install it (and I just retried and it warn me that directory already exists). What do I miss?
</comment><comment author="dadoonet" created="2015-11-28T11:56:40Z" id="160286846">Restart?
</comment><comment author="dagatsoin" created="2015-11-28T12:00:03Z" id="160287627">Yep, works well now. Thx you @dadoonet :+1: 
</comment><comment author="morkrispil" created="2017-05-21T09:20:32Z" id="302925124">"Restart needed", should be added to the official guide:
https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-murmur3.html
</comment><comment author="dadoonet" created="2017-05-21T10:30:33Z" id="302928206">I think it's there:

&gt; The plugin must be installed on every node in the cluster, and each node must be restarted after installation.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IngestDocument to support accessing and modifying list items</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15078</link><project id="" key="" /><description>When reading, through #getFieldValue and #hasField, and a list is encountered, the next element in the path is treated as the index of the item that the path points to (e.g. `list.0.key`). If the index is not a number or out of bounds, an exception gets thrown.
# removeField supports now removing a specific element of a list, by specifying its index. (e.g. field.list.0)
# setFieldValue supports now lists while resolving the path same as described above for reading and remove. It also now allows to add a value to a list at a specific position by doing `setFieldValue("field.list.1", value);`. That said when the last element in the path is a list, e.g. field.list, the set will replace the list with the new value.

Added #appendFieldValue method that has the same behaviour as setFieldValue, but when a list is the last element in the path, instead of replacing the whole list it will simply add a new element to the existing list. This method is currently unused, we have to decide whether the set processor or a new processor should use it.

A few other changes made:
- Renamed hasFieldValue to hasField, as this method is not really about values but only keys. It will return true if a key is there but its value is null, while it returns false only when a field is not there at all.
- Changed null semantic in getFieldValue. null gets returned only when it was an actual value in the source, an exception is thrown otherwise when trying to access a non existing field, so that null != field not present.
- Made getFieldValue stricter about the input path. Throw error if null or empty rather than returning null.
- Made remove stricter about non existing fields. Throws error when trying to remove a non existing field. This is more consistent with the other methods in IngestDocument which are strict about fields that are not present.

Relates to #14324
</description><key id="119287742">15078</key><summary>IngestDocument to support accessing and modifying list items</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-28T11:35:26Z</created><updated>2015-11-30T13:02:45Z</updated><resolved>2015-11-30T13:02:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-30T12:15:17Z" id="160612931">LGTM. great to see these ingest document tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated `/{index}/{type}/_mapping` endpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15077</link><project id="" key="" /><description>Since 1.0, the preferred URL format for mappings has been: `/{index}/_mapping/{type}`, but the old `/{index}/{type}/_mapping` format is still supported for bwc.

i think it is time to remove the bwc support.
</description><key id="119286841">15077</key><summary>Remove deprecated `/{index}/{type}/_mapping` endpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label></labels><created>2015-11-28T11:19:43Z</created><updated>2016-10-18T08:12:28Z</updated><resolved>2016-10-18T08:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-28T22:44:31Z" id="160343544">Why is `/{index}/_mapping/{type}` the "preferred" way? `/{index}/{type}/_mapping` matches with other endpoints we have, eg `/{index}/{type}/_search`
</comment><comment author="dakrone" created="2015-11-29T07:09:04Z" id="160388189">@clintongormley yeah, where did we decide the preferred way? I agree with @rjernst that `/{index}/{type}/_search` matches our other APIs better (it's my preference)
</comment><comment author="clintongormley" created="2015-11-30T07:56:32Z" id="160548347">We decided this in v1.0.0.RC1 https://github.com/elastic/elasticsearch/issues/4071
</comment><comment author="clintongormley" created="2016-10-18T08:12:23Z" id="254437654">Superseded by #15613
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle does not define `-ea` for tests </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15076</link><project id="" key="" /><description>Not sure where this is coming from.

Tests in IntelliJ are not working anymore as they were when using Maven. It sounds like that a default option `-ea` is not set by default.

```
Assertions mismatch: -ea was not specified but -Dtests.asserts=true

java.lang.Exception: Assertions mismatch: -ea was not specified but -Dtests.asserts=true
    at __randomizedtesting.SeedInfo.seed([1691FDB8DE3F3A10]:0)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:48)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)

REPRODUCE WITH: gradle null -Dtests.seed=1691FDB8DE3F3A10 -Dtests.class=org.elasticsearch.cloud.azure.storage.AzureStorageServiceTest -Dtests.locale=fr_FR -Dtests.timezone=Europe/Paris
NOTE: test params are: codec=null, sim=null, locale=null, timezone=(null)
NOTE: Mac OS X 10.11.1 x86_64/Oracle Corporation 1.8.0_60 (64-bit)/cpus=4,threads=1,free=102234880,total=128974848
NOTE: All tests run in this JVM: [AzureStorageServiceTest]

Process finished with exit code 255
```

Workaround: add manually `-ea` as a VM Option for the test. Would be fantastic to have that out of the box.
</description><key id="119286831">15076</key><summary>Gradle does not define `-ea` for tests </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>test</label></labels><created>2015-11-28T11:19:27Z</created><updated>2016-02-14T18:05:40Z</updated><resolved>2016-02-14T18:05:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-28T22:43:03Z" id="160343495">For me this is a default that I set up in IntelliJ long ago (vm options for JUnit). I don't think there was something special that did this for Maven.
</comment><comment author="dadoonet" created="2015-11-30T13:19:20Z" id="160624807">I just ran a test from scratch:
- git clone in a new dir
- checkout 2.x
- import in IntelliJ (maven based version): `-ea` is added as a VM option
- checkout master branch
- run `gradle idea`
- import in IntelliJ (gradle based version): no default `-ea` VM option.

If I then reimport the 2.x project, `-ea` option is not anymore here.
If I remove `elasticsearch.ipr` file and import again the maven 2.x project, `-ea` is back again as a default option.

So my first guess is that this file `.ipr` which is only generated when using gradle, introduce something which breaks the "default" behavior.

Once again, not a big deal as there is a workaround. So feel free to close it if you think we can't really fix that.
</comment><comment author="clintongormley" created="2016-02-14T18:00:43Z" id="183938911">@rjernst can this be closed?
</comment><comment author="rjernst" created="2016-02-14T18:05:40Z" id="183940535">I don't know of any settings for a project which can set the default. Anyone using intellij should set this as the default for the entire IDE. Closing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>qa/evil-tests is not correctly imported in IntelliJ</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15075</link><project id="" key="" /><description>Reproduction:
- git clone the project (not intellij file)
- run `gradle idea`
- open the project in IntelliJ

The `qa/evil-tests` project is not imported as a Java project. `src/test/java` is not defined as a test resource.
You can't compile the code from the IDE or launch tests.

![pluginmanagertests java - elasticsearch - -documents-elasticsearch-dev-es-gradle-elasticsearch intellij idea aujourd hui at 11 08 08](https://cloud.githubusercontent.com/assets/274222/11451304/c57c2942-95c1-11e5-990c-f05b5d802a4d.png)
</description><key id="119284168">15075</key><summary>qa/evil-tests is not correctly imported in IntelliJ</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-28T10:18:56Z</created><updated>2015-12-08T00:02:49Z</updated><resolved>2015-12-08T00:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-28T10:27:37Z" id="160271726">If I mark the source dir as test source, it does not compile.

![pluginmanagertests java - elasticsearch - -documents-elasticsearch-dev-es-gradle-elasticsearch intellij idea aujourd hui at 11 26 13](https://cloud.githubusercontent.com/assets/274222/11451341/0458c55c-95c3-11e5-9c09-4a967168e028.png)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is there any way to de-duplicated documents based on the field?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15074</link><project id="" key="" /><description>The following were the use cases
1. http://stackoverflow.com/questions/25448186/remove-duplicate-documents-from-a-search-in-elasticsearch
2. http://stackoverflow.com/questions/29886477/how-to-remove-duplicate-search-result-in-elasticsearch

I need to de-duplicate the documents based on a specified field. Meanwhile I noticed the post filter which could filter the search results on hits, while it seems doesn't make any sense to my case.

Could anyone could offer me some suggestions on it? or Elasticsearch plan to provide some solutions on that?
</description><key id="119280850">15074</key><summary>Is there any way to de-duplicated documents based on the field?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jkryanchou</reporter><labels /><created>2015-11-28T08:45:47Z</created><updated>2015-11-28T14:59:49Z</updated><resolved>2015-11-28T13:45:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T13:45:55Z" id="160296730">Hi @ryanchou1991 

The best place to ask questions like these is in the forums: https://discuss.elastic.co/
</comment><comment author="jkryanchou" created="2015-11-28T14:59:49Z" id="160307242">OK
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kubernetes discovery plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15073</link><project id="" key="" /><description>Is it worth including [this](https://github.com/fabric8io/elasticsearch-cloud-kubernetes) [here](https://www.elastic.co/guide/en/elasticsearch/plugins/current/discovery.html#_community_contributed_discovery_plugins)?

It was mentioned in [this](https://discuss.elastic.co/t/elasticsearch-2-0-unicast-discovery-via-load-balancer/35210/5) forum post from @jimmidyson and I can see that @dadoonet has also contributed to the plugin.
</description><key id="119271639">15073</key><summary>Kubernetes discovery plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-11-28T05:18:26Z</created><updated>2015-11-28T07:06:40Z</updated><resolved>2015-11-28T07:06:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimmidyson" created="2015-11-28T06:51:42Z" id="160253859">I added it to master docs in #15013. There are versions for the plug plugin supporting es back to 1.5 so would be good to add docs for all versions.
</comment><comment author="markwalkom" created="2015-11-28T07:06:35Z" id="160254733">Ahh nice work :D
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add update parameter to bin/plugin, for easier updating of plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15072</link><project id="" key="" /><description>With the frequent release cycle (and the need to update every elastic plugin), it would be nice to add an `update` parameter to `bin/plugin`, that does `bin/plugin remove ...` and `bin/plugin install ...` in one step.

Closes #15000.
</description><key id="119247882">15072</key><summary>Add update parameter to bin/plugin, for easier updating of plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>discuss</label><label>feature</label><label>review</label></labels><created>2015-11-27T21:25:48Z</created><updated>2015-12-02T16:00:14Z</updated><resolved>2015-12-01T11:24:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-01T11:24:01Z" id="160939347">See discussion in https://github.com/elastic/elasticsearch/issues/15000

Won't be merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error when upgrading the server to 2.1 without upgrading java client at the same time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15071</link><project id="" key="" /><description>Here is what I did:

I install a fresh new version of elasticsearch 2.1.0. And starts it (note that I'm running it with `license` and `marvel-agent`).

```
$ bin/elasticsearch
[2015-11-27 20:59:15,909][INFO ][node                     ] [Ajak] version[2.1.0], pid[74419], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-11-27 20:59:15,910][INFO ][node                     ] [Ajak] initializing ...
[2015-11-27 20:59:16,341][INFO ][plugins                  ] [Ajak] loaded [license, marvel-agent], sites []
[2015-11-27 20:59:16,387][INFO ][env                      ] [Ajak] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [36.6gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2015-11-27 20:59:19,519][INFO ][node                     ] [Ajak] initialized
[2015-11-27 20:59:19,519][INFO ][node                     ] [Ajak] starting ...
[2015-11-27 20:59:19,657][INFO ][transport                ] [Ajak] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2015-11-27 20:59:19,669][INFO ][discovery                ] [Ajak] workshop/K8cI0q5zTU-J-iuxKIBcIg
[2015-11-27 20:59:22,697][INFO ][cluster.service          ] [Ajak] new_master {Ajak}{K8cI0q5zTU-J-iuxKIBcIg}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-11-27 20:59:22,712][INFO ][http                     ] [Ajak] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}
[2015-11-27 20:59:22,713][INFO ][node                     ] [Ajak] started
```

I have a Java application using elasticsearch 2.0.0 jar as a client.

``` java
Client client = TransportClient.builder().build()
        .addTransportAddress(new InetSocketTransportAddress(new InetSocketAddress("127.0.0.1", 9300)));

if (!client.admin().indices().prepareExists("person").execute().actionGet().isExists()) {
    String indexSettings = readFileInClasspath("/settings.json");
    client.admin().indices().prepareCreate("person").setSettings(indexSettings).execute().actionGet();
}
```

When running this, I get on the client side:

```
nov. 27, 2015 9:03:03 PM org.elasticsearch.plugins.PluginsService &lt;init&gt;
INFOS: [James Sanders] loaded [], sites []
nov. 27, 2015 9:03:03 PM org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler doSample
INFOS: [James Sanders] failed to get node info for {#transport#-1}{127.0.0.1}{127.0.0.1:9300}, disconnecting...
RemoteTransportException[[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]]; nested: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]; nested: ExceptionInInitializerError; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]];
Caused by: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]; nested: ExceptionInInitializerError; nested: IllegalArgumentException[An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]];
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:179)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:138)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ExceptionInInitializerError
        at org.elasticsearch.Version.fromId(Version.java:500)
        at org.elasticsearch.Version.readVersion(Version.java:276)
        at org.elasticsearch.cluster.node.DiscoveryNode.readFrom(DiscoveryNode.java:326)
        at org.elasticsearch.cluster.node.DiscoveryNode.readNode(DiscoveryNode.java:309)
        at org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse.readFrom(LivenessResponse.java:52)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:177)
        ... 23 more
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.PostingsFormat.forName(PostingsFormat.java:112)
        at org.elasticsearch.common.lucene.Lucene.&lt;clinit&gt;(Lucene.java:103)
        ... 29 more

nov. 27, 2015 9:03:03 PM org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler doSample
INFOS: [James Sanders] failed to get node info for {#transport#-1}{127.0.0.1}{127.0.0.1:9300}, disconnecting...
RemoteTransportException[[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]]; nested: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]; nested: NoClassDefFoundError[Could not initialize class org.elasticsearch.common.lucene.Lucene];
Caused by: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse]]; nested: NoClassDefFoundError[Could not initialize class org.elasticsearch.common.lucene.Lucene];
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:179)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:138)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.lucene.Lucene
        at org.elasticsearch.Version.fromId(Version.java:500)
        at org.elasticsearch.Version.readVersion(Version.java:276)
        at org.elasticsearch.cluster.node.DiscoveryNode.readFrom(DiscoveryNode.java:326)
        at org.elasticsearch.cluster.node.DiscoveryNode.readNode(DiscoveryNode.java:309)
        at org.elasticsearch.action.admin.cluster.node.liveness.LivenessResponse.readFrom(LivenessResponse.java:52)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:177)
        ... 23 more

Exception in thread "main" NoNodeAvailableException[None of the configured nodes are available: []]
        at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:280)
        at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:197)
        at org.elasticsearch.client.transport.support.TransportProxyClient.execute(TransportProxyClient.java:55)
        at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:272)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1177)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
        at org.elasticsearch.demo.workshop.injector.runner.Generate.main(Generate.java:103)
```

If I update the client to 2.1.0, the exact same code now works perfectly.
</description><key id="119242089">15071</key><summary>Error when upgrading the server to 2.1 without upgrading java client at the same time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2015-11-27T20:06:14Z</created><updated>2016-11-28T02:31:36Z</updated><resolved>2015-11-30T13:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-28T13:25:02Z" id="160295956">Also tested without license and marvel. Same issue.

```
[2015-11-28 14:23:24,643][INFO ][node                     ] [Native] version[2.1.0], pid[11128], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-11-28 14:23:24,644][INFO ][node                     ] [Native] initializing ...
[2015-11-28 14:23:24,721][INFO ][plugins                  ] [Native] loaded [], sites []
```
</comment><comment author="rmuir" created="2015-11-28T14:57:41Z" id="160307157">Looks like you don't have lucene jars configured correctly on the client side:

```
Caused by: java.lang.IllegalArgumentException: An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath.  The current classpath supports the following names: [es090, completion090, XBloomFilter]
        at org.apache.lucene.util.NamedSPILoader.lookup(NamedSPILoader.java:109)
        at org.apache.lucene.codecs.PostingsFormat.forName(PostingsFormat.java:112)
        at org.elasticsearch.common.lucene.Lucene.&lt;clinit&gt;(Lucene.java:103)
        ... 29 more
```

This happens easily with improper shading. Lucene uses SPI and it is important to configure things to merge META-INF/ services.
</comment><comment author="dadoonet" created="2015-11-28T15:05:28Z" id="160308007">Here is what I'm doing.

Client side `pom.xml`:

``` xml
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;2.0.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.3&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.7&lt;/source&gt;
                    &lt;target&gt;1.7&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.5.4&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;finalName&gt;injector-${elasticsearch.version}&lt;/finalName&gt;
                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;mainClass&gt;org.elasticsearch.demo.workshop.injector.runner.Generate&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
```

So is this happening because I'm building a jar with all deps? `jar-with-dependencies`
Is it what you called "shading"? Note that I'm not moving any class to another package.
</comment><comment author="rmuir" created="2015-11-28T15:10:24Z" id="160309396">Right, shading, uber-jar, whatever it is. Its not properly merging META-INF/ entries, creating a broken lucene installation. Hence lucene can'f find services that should be there.
</comment><comment author="dadoonet" created="2015-11-28T15:11:58Z" id="160309752">Ok thanks Robert. I guess we can close it then.
</comment><comment author="rmuir" created="2015-11-28T15:13:35Z" id="160309810">AFAIK, the shade plugin is the best way to create an uber jar (even if you arent relocating packages). It has the ability to properly merge META-INF/services across the different jars into a combined one that will work, but you may have to pass some parameters to do that.
</comment><comment author="rmuir" created="2015-11-28T15:15:22Z" id="160309888">See https://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer

It is really necessary if you want to shade/uberjar lucene at all.
</comment><comment author="dadoonet" created="2015-11-28T15:16:38Z" id="160309939">Thank you Robert. I think I need to add that to the Java documentation.
Reopening as "doc"
</comment><comment author="dadoonet" created="2015-11-30T10:35:23Z" id="160591047">I ran some more tests this morning. Actually maven assembly plugin sucks when you have to deal with `META-INF/services`.

So instead of calling `maven-assembly-plugin`, it's better to use `maven-shade-plugin` with the option @rmuir gave.

At the end, this is working well:

``` xml
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.4.1&lt;/version&gt;
                &lt;executions&gt;
                    &lt;!-- Run shade goal on package phase --&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;finalName&gt;injector-${elasticsearch.version}&lt;/finalName&gt;
                            &lt;transformers&gt;
                                &lt;!-- add Main-Class to manifest file --&gt;
                                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt;
                                    &lt;mainClass&gt;org.elasticsearch.demo.workshop.injector.runner.Generate&lt;/mainClass&gt;
                                &lt;/transformer&gt;
                                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/&gt;
                            &lt;/transformers&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
```
</comment><comment author="kdubezerra" created="2016-08-30T07:48:36Z" id="243360150">The solution you guys posted is for maven-based projects. Do you know an equivalent solution for gradle projects? FYI, the problem I'm facing is more similar to https://github.com/elastic/elasticsearch/issues/18908, but perhaps this one here has the same solution.
</comment><comment author="rjernst" created="2016-08-31T20:58:57Z" id="243899552">@kdubezerra It looks like the [fat jar plugin](https://github.com/musketyr/gradle-fatjar-plugin) is able to handle properly merging `META-INF/services` as was described in this issue.
</comment><comment author="ayalaio" created="2016-11-28T02:31:36Z" id="263170040">@dadoonet 
Thanks, that conf works!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regexp on the _index does not work in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15070</link><project id="" key="" /><description>Currently, some queries work on the special `_index` field, as an example:
- Input

```
POST trial/doc
{
  "text": "this is text"
}

GET trial/_search
{
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "_index": "trial"
        }
      }
    }
  }
}
```
- Response

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "trial",
        "_type": "doc",
        "_id": "AVFKXPURmqplIqGCgb_z",
        "_score": 1,
        "_source": {
          "text": "this is text"
        }
      }
    ]
  }
}
```

**However**, wildcards are not supported for querying this field:
- Input

```
GET trial/_search
{
  "query": {
    "filtered": {
      "filter": {
        "regexp": {
          "_index": "tri*"
        }
      }
    }
  }
}
```
- Response (No results)

```
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": null,
    "hits": []
  }
}
```
</description><key id="119237992">15070</key><summary>Regexp on the _index does not work in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Query DSL</label><label>docs</label></labels><created>2015-11-27T19:14:54Z</created><updated>2015-11-30T07:44:36Z</updated><resolved>2015-11-30T07:43:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T12:36:24Z" id="160289902">The `_index` field is not indexed - it is exposed in queries as a fake field (https://github.com/elastic/elasticsearch/pull/12027), which means that it can only work with term/match queries, not with wildcards, fuzzy, regexp, etc.

It should be possible to set the `_index` field to `enabled: true` to index the index name with each doc, in which case all queries would work normally, but this appears to be broken currently (#15081).

Once #15081 is fixed, the docs should be updated to explain the above.
</comment><comment author="clintongormley" created="2015-11-30T07:44:36Z" id="160546064">Actually, the ability to enable the index field was removed in #12329.

I've updated the docs accordingly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_index not accessible inside external script file?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15069</link><project id="" key="" /><description>I am able to run the following query successfully 

```
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "stadt": {
      "sum": {
        "script": "_index['title']['energy'].tf()"
      }
    }
  },
  "size": 0
}
```

But I dont want to enable `dynamic scripting` so I put the content of script into external file inside `es/config/scripts/tfscript.groovy`

Now the following query throws exception

```
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "stadt": {
      "sum": {
        "script": {
          "file": "tfscript"
        }
      }
    }
  },
  "size": 0
}
```

The exception is 

```
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:747)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:572)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:544)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:385)
    at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
    at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.search.SearchParseException: [enerknol][0]: query[ConstantScore(*:*)],from[-1],size[0]: Parse Failure [Unexpected token START_OBJECT in [stadt].]
    at org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser.parse(NumericValuesSourceMetricsAggregatorParser.java:61)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:148)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:78)
    at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:731)
```

I searched for `Unexpected token START_OBJECT in [stadt]` but could not figure out anything.
Am I missing something basic here?
</description><key id="119233559">15069</key><summary>_index not accessible inside external script file?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chintan-shah-25</reporter><labels /><created>2015-11-27T18:28:29Z</created><updated>2015-11-28T12:09:50Z</updated><resolved>2015-11-28T12:09:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chintan-shah-25" created="2015-11-27T18:29:32Z" id="160186259">I am using ES 1.7.2
</comment><comment author="clintongormley" created="2015-11-28T12:09:50Z" id="160288046">The syntax for scripting in 1.7.2 was different: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-aggregations-metrics-sum-aggregation.html#_script_3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix extraConfigFile to not check for the file, until runtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15068</link><project id="" key="" /><description>Currently gradle checks for this at compile-time, but this prevents you from generating configuration files in the build itself. Can we move the check to runtime?
</description><key id="119224552">15068</key><summary>fix extraConfigFile to not check for the file, until runtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-27T16:55:13Z</created><updated>2015-11-29T03:44:16Z</updated><resolved>2015-11-29T03:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-27T21:36:45Z" id="160213697">I already attempted to do this here:
https://github.com/elastic/elasticsearch/commit/52f31ee14a8c07ab1e2592abd855dc0dc05497d4

And it works for me locally (with a separate clean before running check). Not sure why its not working on jenkins...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename `add` processor to `set` processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15067</link><project id="" key="" /><description>This name makes more sense, because if a field already exists it overwrites it.
</description><key id="119214667">15067</key><summary>Rename `add` processor to `set` processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-27T15:40:44Z</created><updated>2015-11-30T14:05:54Z</updated><resolved>2015-11-30T14:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-11-27T16:33:18Z" id="160171329">if we remove the `add`, how do we append values to a multi-valued field? One option is to have another processor just for that... another option is to support the following syntax in this `set` processor:

``` js
{
  "set" : {
    "field_1" : 42, // default behaviour is to override or add
    "field_2" : { "value" : 42, "append_if_exists" : false } // same as above
    "field_3" : { "value" : 42, "append_if_exists" : true } // this will append the value to the existing one/s
  }
}
```
</comment><comment author="javanna" created="2015-11-27T16:54:50Z" id="160174702">@uboness your comment on arrays makes me think of #14324 which I am working on. Add/set don't really support adding values to an array at the moment. Maybe we can move the discussion to that other issue and do the right thing there?
</comment><comment author="uboness" created="2015-11-27T16:58:48Z" id="160175405">@javanna oh... didn't see that one... will take a look. thx!
</comment><comment author="javanna" created="2015-11-30T13:15:04Z" id="160624091">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Lifecycle#stop()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15066</link><project id="" key="" /><description>We don't allow stopping services anyway so all this code is confusing and
just wrong. We eventually once we remove guice can simplify this even further
towards a simple closeable but for now all the stop code can just go into close()
</description><key id="119202167">15066</key><summary>Remove Lifecycle#stop()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>review</label></labels><created>2015-11-27T14:11:44Z</created><updated>2016-10-16T18:46:15Z</updated><resolved>2016-10-12T14:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-30T10:01:16Z" id="160582135">LGTM, nice stats and I'm looking forward to simply using closable!
</comment><comment author="s1monw" created="2015-12-01T10:07:05Z" id="160922139">I get some test failure crazyness related to discovery here. there is too much magic in here I don't really get. I don't think I will have the patience to fix that but maybe somebody with more disco knowledge can chime in @bleskes @martijnvg 
</comment><comment author="bleskes" created="2015-12-01T10:08:07Z" id="160922502">Would love to. Can you give an example of the failures?

&gt; On 01 Dec 2015, at 11:07, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; I get some test failure crazyness related to discovery here. there is too much magic in here I don't really get. I don't think I will have the patience to fix that but maybe somebody with more disco knowledge can chime in @bleskes @martijnvg
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2015-12-01T10:32:27Z" id="160928521">`DiscoveryWithServiceDisruptionsIT.testVerifyApiBlocksDuringPartition` for example
</comment><comment author="martijnvg" created="2015-12-01T14:43:39Z" id="160986983">@s1monw @bleskes the discovery tests failures are caused by the fact the in places we used to call stop() on MasterFaultDetection and NodesFaultDetection we now call close() which does more than just stop() did before.

I added back the stop() methods in MasterFaultDetection and NodesFaultDetection and updated ZenDiscovery to use them when we rejoin or detected that we lost the master, then the discovery tests pass. Maybe we can maybe rename the stop() method to interrupt() or pause() or something like that.
</comment><comment author="s1monw" created="2015-12-01T14:46:50Z" id="160987780">&gt; I added back the stop() methods in MasterFaultDetection and NodesFaultDetection and updated ZenDiscovery to use them when we rejoin or detected that we lost the master, then the discovery tests pass. Maybe we can maybe rename the stop() method to interrupt() or pause() or something like that.

I am ok with adding it back on both of these classes as long as the interface is clean. Can you share your code?
</comment><comment author="bleskes" created="2015-12-01T15:03:27Z" id="160993226">+1 on leaving them in the classes. We do use the "stop/start" semantics here. I think it's OK (the FaultDetection class doesn't even inherit from LifecycleComponent ).
</comment><comment author="martijnvg" created="2015-12-01T15:16:55Z" id="160998232">Sure, the changes:

``` patch
Index: core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
&lt;+&gt;UTF-8
===================================================================
--- core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java (date 1448630180000)
+++ core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java (revision )
@@ -129,7 +129,7 @@
         threadPool.schedule(pingInterval, ThreadPool.Names.SAME, masterPinger);
     }

-    private void stop(String reason) {
+    public void stop(String reason) {
         synchronized (masterNodeMutex) {
             if (masterNode != null) {
                 if (logger.isDebugEnabled()) {
Index: core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
&lt;+&gt;UTF-8
===================================================================
--- core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java  (date 1448630180000)
+++ core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java  (revision )
@@ -106,10 +106,14 @@
         }
     }

+    public void stop() {
+        nodesFD.clear();
+    }
+
     @Override
     public void close() {
         super.close();
-        nodesFD.clear();
+        stop();
     }

     @Override
Index: core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
&lt;+&gt;UTF-8
===================================================================
--- core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java    (date 1448630180000)
+++ core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java    (revision )
@@ -662,7 +662,7 @@
                     return newState;

                 } else {
-                    nodesFD.close();
+                    nodesFD.stop();
                     if (electedMaster != null) {
                         discoveryNodes = DiscoveryNodes.builder(discoveryNodes).masterNodeId(electedMaster.id()).build();
                         masterFD.restart(electedMaster, "possible elected master since master left (reason = " + reason + ")");
@@ -951,8 +951,8 @@
         assert Thread.currentThread().getName().contains(InternalClusterService.UPDATE_THREAD_NAME);

         logger.warn(reason + ", current nodes: {}", clusterState.nodes());
-        nodesFD.close();
-        masterFD.close(reason);
+        nodesFD.stop();
+        masterFD.stop(reason);


         ClusterBlocks clusterBlocks = ClusterBlocks.builder().blocks(clusterState.blocks())

```
</comment><comment author="clintongormley" created="2016-03-10T12:51:29Z" id="194829376">@s1monw still want to get this in?
</comment><comment author="dakrone" created="2016-09-12T21:21:46Z" id="246498015">@s1monw is this still useful? Re-ping as the code freeze for 5.0 is approaching soon :)
</comment><comment author="s1monw" created="2016-10-12T14:14:22Z" id="253225256">closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bad breaker log doesn't allow understanding the problematic query/field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15065</link><project id="" key="" /><description>There is no way to understand from this log, what is actually problematic, or to debug it:

```
[REQUEST] New used memory 12804659728 [11.9gb] from field [&lt;reused_arrays&gt;] would be larger than configured breaker: 12804659609 [11.9gb], breaking
```

I think the minimum is either to print the query/aggregation or at least the problematic field.
</description><key id="119194006">15065</key><summary>Bad breaker log doesn't allow understanding the problematic query/field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AdallomRoy</reporter><labels><label>:Circuit Breakers</label><label>discuss</label></labels><created>2015-11-27T13:11:34Z</created><updated>2015-12-18T02:00:36Z</updated><resolved>2015-11-28T11:26:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T11:25:33Z" id="160278143">The problem is that the aggregation at fault may just be the straw that broke the camel's back. It could well be that another much bigger aggregation is the one that used up most of the memory, but it didn't hit the limits.
</comment><comment author="clintongormley" created="2015-11-28T11:26:23Z" id="160278163">Duplicate of #11145 
</comment><comment author="rayward" created="2015-12-17T22:22:36Z" id="165599727">Is that actually a duplicate? I don't necessarily need the whole query logged if I still don't know which agg or field was the problem.

`&lt;reused_arrays&gt;` is just very vague. I'm curious as to why sometimes you can get an actual description, eg `Data too large, data for field [request.count]` but on this occasion there isn't. 
</comment><comment author="rayward" created="2015-12-18T02:00:36Z" id="165640676">Btw we had two nodes that were constantly hitting this exception. We ended up restarting both of those nodes and the issue has seemed to resolve itself. Perhaps whatever this `reuse_arrays` structure is was not freeing up memory correctly?

We're on ES 1.7.3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API Spec preferred URL paths does not match documentation preferred URL paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15064</link><project id="" key="" /><description>In the REST API Spec, for exmpale `indices.put_mapping.json` the URL path in the JSON is:

```
"path": "/{index}/{type}/_mapping",
```

and all flavours are present in the `paths` as:

```
"paths": ["/{index}/{type}/_mapping", 
              "/{index}/_mapping/{type}", 
              "/_mapping/{type}", 
              "/{index}/{type}/_mappings", 
              "/{index}/_mappings/{type}", 
              "/_mappings/{type}"],
```

But the preferred form is not obvious.  In the documentation it is `/{index}/_mapping/{type}` (https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) 

Shouldn't the REST API Spec make it clear the order of preference?   Is the `url.path` element supposed to mark the preferred item from `url.paths`?  It doesn't in this case either. 

Are all the forms in `url.paths` really valid?  Some are not documented.  Are they old deprecated forms?  If so, marking them deprecated would be useful as well.  
</description><key id="119191922">15064</key><summary>REST API Spec preferred URL paths does not match documentation preferred URL paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2015-11-27T12:55:07Z</created><updated>2015-11-28T11:23:08Z</updated><resolved>2015-11-28T11:23:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2015-11-27T12:58:08Z" id="160134258">I think that the REST API spec should have the paths in priority order, you take the first one that matches the available path segment variables you have on hand, and validate that all required fields are present and any in the path before them must also be present.
</comment><comment author="clintongormley" created="2015-11-28T11:23:08Z" id="160278027">Those are indeed deprecated forms and I've just opened an issue to get them removed (https://github.com/elastic/elasticsearch/issues/15077).  

Yes, it'd be nice to list the URLs in order of priority but given that this is manually maintained, I think it is quite likely that errors will creep in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API Spec inconsistent use of url parts required field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15063</link><project id="" key="" /><description>In some cases like `indices.put_mappings.json` the url.parts[].required field is set to `true` ... in this example only for `type` part.  But why wouldn't `index` part also have `required: true` in this case?

Is there some implied rule when required is assumed `true`?
</description><key id="119187069">15063</key><summary>REST API Spec inconsistent use of url parts required field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2015-11-27T12:16:50Z</created><updated>2015-11-28T11:15:22Z</updated><resolved>2015-11-28T11:14:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T11:14:59Z" id="160277786">@apatrida the `put_mapping` API also accepts this format `/_mapping/{type}`, so `index` isn't required.
</comment><comment author="clintongormley" created="2015-11-28T11:15:22Z" id="160277797">to clarify: in this case the put mapping request applies to all indices
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin policy not being applied on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15062</link><project id="" key="" /><description>Due to a particularity in dealing with drive letters, the security policy files for plugins are read but never matched on Windows.

After much hairy pulling, the issue was discovered to be caused in `Security.java#getPluginPermissions()` in particular
`codebases.add(jar.toRealPath().toUri().toURL());`

For the given `Path` which points to `e:\tmp\repository-hdfs\elasticsearch-3.0.0-SNAPSHOT\plugins\lang-groovy\groovy-all-2.4.4-indy.jar`, we get the following behaviour:

`jar.toAbsolutePath()` -&gt; `e:\tmp\repository-hdfs\elasticsearch-3.0.0-SNAPSHOT\plugins\lang-groovy\groovy-all-2.4.4-indy.jar`

`jar.toRealPath()` -&gt; `E:\tmp\repository-hdfs\elasticsearch-3.0.0-SNAPSHOT\plugins\lang-groovy\groovy-all-2.4.4-indy.jar`

Thus the policy is keyed under a upper case `E` but the `ProtectionDomain`/`CodeBase` is using the lower case version:

```
access: domain that failed ProtectionDomain  (file:/e:/tmp/repository-hdfs/elasticsearch-3.0.0-SNAPSHOT/plugins/lang-groovy/groovy-all-2.4.4-indy.jar &lt;no signer certificates&gt;)
 java.net.FactoryURLClassLoader@52af26ee
 &lt;no principals&gt;
 java.security.Permissions@7d30007d (
 ("java.io.FilePermission" "\e:\tmp\repository-hdfs\elasticsearch-3.0.0-SNAPSHOT\plugins\lang-groovy\groovy-all-2.4.4-indy.jar" "read")
)
```

Unfortunately the `URLClassLoader` relies on sun specific classes in retrieving the URL but it looks like it does not rely on the `java.nio` package. Potential solutions would be using `absolutePath` instead though that does not handle symlinks or adding a special check for Windows (which likely might actually work only on Oracle/Sun JDK as it is platform and implementation specific) that lowercases the drive letter.
I wonder who this behaves with things like SMB or other crazy URLs.
</description><key id="119182663">15062</key><summary>Plugin policy not being applied on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>:Plugins</label><label>bug</label><label>non-issue</label></labels><created>2015-11-27T11:42:48Z</created><updated>2016-01-10T09:01:23Z</updated><resolved>2016-01-08T21:10:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-27T13:59:25Z" id="160145352">Thanks Costin. Unfortunately we need symlink resolution... We got eg jenkins boxes with linked .m2. And despite the trouble i think simple string lookup is important. My current thinking is .toFile then getCanonical... But i have not looked yet.
</comment><comment author="rmuir" created="2015-11-27T14:22:45Z" id="160149548">Something like this is my idea (needs testing):

```
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -141,7 +141,9 @@ final class Security {
                         List&lt;URL&gt; codebases = new ArrayList&lt;&gt;();
                         try (DirectoryStream&lt;Path&gt; jarStream = Files.newDirectoryStream(plugin, "*.jar")) {
                             for (Path jar : jarStream) {
-                                codebases.add(jar.toRealPath().toUri().toURL());
+                                // codebase normalization in java is crazy, use java.io.File on purpose,
+                                // since that is what is being used by classloaders.
+                                codebases.add(jar.toFile().getCanonicalFile().toURI().toURL());
                             }
                         }
```
</comment><comment author="costin" created="2015-12-02T17:30:43Z" id="161373180">@rmuir if it's okay, I'll try to fix this one myself.
</comment><comment author="rmuir" created="2015-12-02T18:05:30Z" id="161383487">Thanks. My only concern is nobody can repro this right now. Jenkins is happy on windows systems it tests... So it would be great to have some explanation why that is.
</comment><comment author="costin" created="2015-12-03T11:19:44Z" id="161602607">Got to the root of the problem and figured out why the tests don't fail on Jenkins.
In my case ES and Eclipse are started many times from a file explorer (TotalCommander in this case). Which in turn changes the drive letter in the spawn consoles (like `cmd`) to be lowercase.
Windows on the other hand likes upper case drives.
ES starts, picks up the environment and thus thinks it's under `e:\...` drive (in `path.home`). Everything else that is relative to this will use the `e:\` in its path. However when calling `toRealPath`, the file-system and not the environment is checked and the upper case drive letter is used instead (`E:\`).

After stopping the explorer to change the drive letter and use the upper case one instead, things worked out fine out of the box.
So in short, this is not a bug but rather an error in the user environment. It looks to be Windows specific, maybe there's a way to do some sanity checks in the `.bat` file? /cc @gmarz @Mpdreamz 
</comment><comment author="costin" created="2016-01-08T21:10:58Z" id="170124410">Closing it down.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index stays in UNASSIGNED state, if cluster was shutdown while a process of changing the replicas was running.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15061</link><project id="" key="" /><description>If a cluster is shutdown (all nodes at once), while there is a process running to increase the number of replicas, the index stays in a `UNASSIGNED` state with no messages in the logs.
If the number of replica is changed again the index comes back.

Steps to reproduce:
1. spin up a cluster with at least 3 nodes
2. Index some data (should be enough data for the replicas assignment to take some time) 
3. Change replicas to 0 `curl -XPUT 'localhost:9200/_settings' -d '{ "index" : {"number_of_replicas" : 0 }}'`
4. Raise number of replicas `curl -XPUT 'localhost:9200/_settings' -d '{ "index" : {"number_of_replicas" : 1 }}'` and shut down all nodes before the replicas have been fully assigned.
5. Start all nodes again
6.  The Index will stay in an `UNASSIGNED` state, but there is nothing in the logs on why this is the case.
7. Changing the number of replicas to 0 again, will bring back the index into a `STARTED` state.

Reproduced on 2.0 and 2.1.
</description><key id="119180054">15061</key><summary>Index stays in UNASSIGNED state, if cluster was shutdown while a process of changing the replicas was running.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jakommo</reporter><labels><label>:Recovery</label><label>bug</label></labels><created>2015-11-27T11:21:56Z</created><updated>2015-12-16T14:34:55Z</updated><resolved>2015-12-01T13:49:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T11:11:30Z" id="160277690">@bleskes could you take a look at this please
</comment><comment author="bleskes" created="2015-11-30T15:41:11Z" id="160664719">yeah, sadly this is how things work today. When we recover a primary, we reach out to all the nodes in the cluster and select the best copy we find (in order to make sure we don't select stale copies). In order to make this effective we require to see at least a quorum of the possible copies (i.e., number of replicas +1 ) with a special case for 2.  Since the number of replicas is increased and the cluster is shut down before ES is able to make more copies, a new primary is not elected. 

This will be fixed once we complete the work on #14739 . I'm closing this for now, as a duplicate of #14739 where github nicely refers back to this. Feel free to reopen if there is anything else.
</comment><comment author="bleskes" created="2015-11-30T15:49:11Z" id="160667827">sorry, mis read the ticket and thought you had more then 1 replica. Re-opening for further investigation. 
</comment><comment author="bleskes" created="2015-12-01T12:14:56Z" id="160950402">I tried to reproduce but I couldn't, @jakommo are you sure you used just one replica in your example in # 4 ? if it was 2 what I said above would explain it.
</comment><comment author="jakommo" created="2015-12-01T13:46:45Z" id="160974583">@bleskes you are right, sorry. I initially tried it with 4 nodes and num of replicas set to 3.
Just tried it again with 3 nodes and it doesn't happen with replicas set to 1, but with replicas 2.
</comment><comment author="bleskes" created="2015-12-01T13:49:13Z" id="160975064">OK. Good.  See my explanation above.  Closing again :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing classpath plugins in a node has become cumbersome in ES 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15060</link><project id="" key="" /><description>Testing classpath plugins in a single temporary node has become cumbersome in ES 2.1.

In ES 2.0 it was easy to use a NodeBuilder. The node started and picked up the plugins. 

With ES 2.1, the NodeBuilder has been rendered useless, since it does no longer provides
the necessary control to load plugins from classpath.

Even worse, plugin authors have to write an auxilliary class in package org.elasticsearch.node
to be able to use the new Node class constructor to pass the classpath plugins to be tested.
The new class constructor has to be used in addition to the `plugin.types` setting in 
the node settings or the plugins do not load. 

These wrinkles make code longer and hard to understand/maintain for plugin authors.

I suggest to enhance NodeBuilder to be able to test plugins from classpath easily, without 
the auxilliary `MockNode` class, and without double declaration of plugins, as it was the case in ES 2.0

ES 2.0

```
Settings nodeSettings = Settings.settingsBuilder()
        .put("path.home", System.getProperty("path.home"))
        .put("plugin.types", MyPlugin.class.getName())
        .put("index.number_of_shards", 1)
        .put("index.number_of_replica", 0)
        .build();
Node node = NodeBuilder.nodeBuilder().settings(nodeSettings).local(true).build().start();
node.start();
return node;
```

ES 2.1

```
Settings nodeSettings = Settings.settingsBuilder()
        .put("path.home", System.getProperty("path.home"))
        .put("plugin.types", MyPlugin.class.getName())
        .put("index.number_of_shards", 1)
        .put("index.number_of_replica", 0)
        .build();
// ES 2.1 renders NodeBuilder as useless
//Node node = NodeBuilder.nodeBuilder().settings(nodeSettings).local(true).build().start();
Set&lt;Class&lt;? extends Plugin&gt;&gt; plugins = new HashSet&lt;&gt;();
plugins.add(MyPlugin.class);
Node node = new MockNode(nodeSettings, plugins);
node.start();
return node;
```

```
package org.elasticsearch.node;

import org.elasticsearch.Version;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.plugins.Plugin;

import java.util.Collection;

public class MockNode extends Node {

    public MockNode(Settings settings, Collection&lt;Class&lt;? extends Plugin&gt;&gt; classpathPlugins) {
        super(settings, Version.CURRENT, classpathPlugins);
    }

}
```
</description><key id="119177323">15060</key><summary>Testing classpath plugins in a node has become cumbersome in ES 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2015-11-27T11:03:26Z</created><updated>2015-11-28T11:16:51Z</updated><resolved>2015-11-28T11:16:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-27T11:08:49Z" id="160115861">why are you starting a node yourself? Can't u use the testing framework?
</comment><comment author="jprante" created="2015-11-27T11:19:23Z" id="160118056">I just want to write 8 lines of code to start a node with a classpath plugin. Testing framework is nice, but not an option, too heavy for my test code.
</comment><comment author="jprante" created="2015-11-27T11:34:20Z" id="160120364">My NodeBuilder suggestion is already brought in by https://github.com/elastic/elasticsearch/issues/13212
</comment><comment author="s1monw" created="2015-11-27T12:45:48Z" id="160132399">&gt; I just want to write 8 lines of code to start a node with a classpath plugin. Testing framework is nice, but not an option, too heavy for my test code.

that is the supported way to test things with es. I don't see why this is too heavy.
</comment><comment author="clintongormley" created="2015-11-28T11:16:51Z" id="160277837">Closing in favour of #13212
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndicesService.canDeleteShardContent() takes a lot of CPU and time because of regex that is executed against the Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15059</link><project id="" key="" /><description>When analyzing some high CPU that we encountered when working with many indices on an Elasticsearch 2.0 clusters. We tried to use 600 indexes with 12 shards here to see where we max out the system.

We saw that quite some time is spent running regex-matches in the Settings-Builder on the IndicesStore.clusterChanged() event. 

It seems this is executed many times during cluster-change events, as this is run in a loop over all entries in the ShardRoutingsTable:

```
    for (IndexRoutingTable indexRoutingTable : event.state().routingTable()) {
        for (IndexShardRoutingTable indexShardRoutingTable : indexRoutingTable) {
            if (shardCanBeDeleted(event.state(), indexShardRoutingTable)) {
```

See the following extract when using Dynatrace to do some performance analysis, out of the 19 seconds spent in the clusterChanged() method, nearly 6 are needed just for constructing the IndexSettings again and again:

&lt;img width="774" alt="elasticsearch_candeleteshardcontent_highcpu_2015-11-27_09-05-42" src="https://cloud.githubusercontent.com/assets/548322/11438382/bffdbcd8-94f4-11e5-831f-7f3a9dc305a9.png"&gt;

I checked the code to ensure this is still the same on newer branches for 2.1, 2.x and master.
</description><key id="119171200">15059</key><summary>IndicesService.canDeleteShardContent() takes a lot of CPU and time because of regex that is executed against the Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">centic9</reporter><labels><label>:Internal</label><label>:Settings</label><label>discuss</label><label>enhancement</label></labels><created>2015-11-27T10:22:21Z</created><updated>2015-12-03T09:04:20Z</updated><resolved>2015-12-03T09:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-01T11:23:07Z" id="160939223">I just looked and we can optimize some minor things, like only construct the setting once per index and not once per shard as we do now. That said, even 7200 operations shouldn't take 6s. How big are your settings?
</comment><comment author="centic9" created="2015-12-01T12:29:30Z" id="160954309">The cluster was in a bit of stress at that time, that might have added some lag to the overall processing. Unfortunately I do not see how many times it was actually called with this type of CPU analysis. But I will do a more detailed sampling to get better numbers to rule out that other threads did steal most of the time here actually...
</comment><comment author="centic9" created="2015-12-01T12:56:05Z" id="160961706">Ok, the overall time is not that high on a healthy cluster, but the percentage is still quite high. In a quick test the 6 calls of a cluster-state-change to the 6 cluster-nodes with take 3795 ms combined (aprox 630ms on each node), the re-building of the Settings takes 1091ms or 182ms on each node.

This is with 620 indices and 6978 primary as well as replica shards (not productive, just testing where ES breaks :) )
</comment><comment author="bleskes" created="2015-12-01T14:51:30Z" id="160989061">@centic9 OK, fair enough. I've opened a PR with the small optimization (more of simplification in my mind). I think we're good here.
</comment><comment author="centic9" created="2015-12-01T17:13:12Z" id="161036492">Great, looks nice, that should reduce the number of calls from 14000 to 700 in my case, which should provide quite a noticeable improvement. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Serial Differencing Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15058</link><project id="" key="" /><description /><key id="119168715">15058</key><summary>Aggregations Refactor: Refactor Serial Differencing Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-27T10:05:16Z</created><updated>2015-11-30T08:38:33Z</updated><resolved>2015-11-30T08:38:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-27T22:26:20Z" id="160220491">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping `properties._id` causes conflicts and shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15057</link><project id="" key="" /><description>Recreation on 2.1.0:

```
PUT t
{
  "mappings": {
    "one": {
      "properties": {
        "_id": {
          "type": "string"
        }
      }
    },
    "two": {}
  }
}
```

This returns OK, but in the logs:

```
[2015-11-27 10:36:51,848][WARN ][indices.cluster          ] [Primevil] [t] failed to add mapping [one], source [{"one":{"properties":{"_id":{"type":"string"}}}}]
java.lang.IllegalArgumentException: Mapper for [_id] conflicts with existing mapping in other types:
[mapper [_id] cannot be changed from type [_id] to [string]]
  at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
  at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:364)
  at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:315)
  at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:261)
  at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:418)
  at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:372)
  at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:177)
  at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
[2015-11-27 10:36:51,849][WARN ][indices.cluster          ] [Primevil] [[t][4]] marking and sending shard failed due to [master [{Primevil}{FZZ2xKQATOSVT6qMODFe2w}{127.0.0.1}{127.0.0.1:9300}] marked shard as started, but shard has not been created, mark shard as failed]
[2015-11-27 10:36:51,849][WARN ][cluster.action.shard     ] [Primevil] [t][4] received shard failed for [t][4], node[FZZ2xKQATOSVT6qMODFe2w], [P], v[2], s[STARTED], a[id=SfSzHTRlSAawEvHtC7cMLg], indexUUID [bcTOLPxjTM2cB95wPE0sJQ], message [master [{Primevil}{FZZ2xKQATOSVT6qMODFe2w}{127.0.0.1}{127.0.0.1:9300}] marked shard as started, but shard has not been created, mark shard as failed], failure [Unknown]
```
</description><key id="119163367">15057</key><summary>Mapping `properties._id` causes conflicts and shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v2.2.0</label></labels><created>2015-11-27T09:38:31Z</created><updated>2016-03-02T01:33:55Z</updated><resolved>2015-12-15T09:38:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ardevd" created="2015-12-04T08:16:54Z" id="161905846">Im experiencing this issue. Is there a known workaround? How do I prevent data loss?
</comment><comment author="clintongormley" created="2015-12-04T09:03:38Z" id="161912815">We're working on a bug fix.  There isn't a workaround that I'm aware of.  You will need to reindex this data to a new index.
</comment><comment author="jpountz" created="2015-12-08T16:12:36Z" id="162931817">Delaying to 2.2.0: I'm on the fence to put such a change in a bugfix release.
</comment><comment author="krisb78" created="2016-01-02T12:44:39Z" id="168388514">Shouldn't dynamic templates recognise these meta-properties (i.e., "_all", "_id", "_parent", "_routing", "_timestamp", "_ttl") and handle them properly?

I have the following template:

```
{
  "shop_order_v1" : {
    "order" : 0,
    "template" : "shop_order_v1*",
    "settings" : { },
    "mappings" : {
      "shop_order" : {
        "dynamic_templates" : [ {
          "store_generic" : {
            "mapping" : {
              "index" : "not_analyzed",
              "store" : true,
              "doc_values" : true
            },
            "match" : "*"
          }
        } ],
        "properties" : {
          "timestamp" : {
            "store" : true,
            "format" : "yyyy-MM-dd HH:mm:ss",
            "doc_values" : true,
            "type" : "date"
          },
          "started" : {
            "store" : true,
            "format" : "yyyy-MM-dd HH:mm:ss",
            "doc_values" : true,
            "type" : "date"
          },
          "geo_point" : {
            "doc_values" : true,
            "type" : "geo_point"
          }
        }
      }
    },
    "aliases" : {
      "shop_order_v1" : { }
    }
  }
}
```

I'm hitting this error when I'm trying to index a document that contains the "_id" property, e.g.

```
{
  "_id": "foo"
}
```

At the moment I'm working around this by removing the "_id" property from the docs before indexing (bulk), but this used to work ok on 1.7.4.
</comment><comment author="clintongormley" created="2016-01-10T17:42:51Z" id="170373750">@krisb78 the ability to embed metafields in the source of a document has been removed.  It required double parsing of the document (once on the coordinating node and once on the data node)
</comment><comment author="krisb78" created="2016-01-10T21:41:44Z" id="170396790">Thought so - thanks, I'll amend my code.
</comment><comment author="vladan-me" created="2016-03-02T01:33:55Z" id="191010441">This is a breaking change that is not mentioned in your documentation...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch does not respect the mapping types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15056</link><project id="" key="" /><description>HI,
 I am working with Elasticsearch 2.1.0.
I have a requirement to specify the schema for my documents.
So i used the ElasticSearch Mapping functionality like this:

curl -XPUT 'http://localhost:9200/customer/external/_mapping' -d ' { "external" : { "properties" : { "example" : { "properties" : { "custom_property" : { "type" : "byte" } } } } } } '

It gets reflected in the schema mapping as well:

``` json
{
  "customer" : {
    "mappings" : {
      "external" : {
        "properties" : {
          "first" : {
            "type" : "long"
          },
          "example" : {
            "properties" : {
              "custom_property" : {
                "type" : "byte"
              },
              "last" : {
                "type" : "long"
              }
            }
          }
        }
      }
    }
  }
}
```

I then added the value for this field like this:

curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '{"example": {"custom_property": 12345}}'

And to my surprise it was saved successfully. Here is the output : 

``` json
{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "1",
  "_version" : 3,
  "found" : true,
  "_source":{"example": {"custom_property": 12345}}
}
```

I did some more investigation and found that the value is treated as short and not as byte. 

There is a similar issue with float dataType as well. Can someone explain what may be causing this and how can this be rectified?

Thanks,
Anuj Kumar
</description><key id="119158516">15056</key><summary>ElasticSearch does not respect the mapping types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anujgandharv</reporter><labels /><created>2015-11-27T09:16:24Z</created><updated>2015-11-28T11:01:14Z</updated><resolved>2015-11-28T11:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T11:01:14Z" id="160276779">Duplicate of https://github.com/elastic/elasticsearch/issues/11513
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>set ActiveProcessLimit=1 on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15055</link><project id="" key="" /><description>This blocks process creation, similar to the approach for BSD.
</description><key id="119142141">15055</key><summary>set ActiveProcessLimit=1 on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-27T06:54:55Z</created><updated>2015-11-28T12:54:29Z</updated><resolved>2015-11-27T21:57:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-27T07:13:04Z" id="160057124">Awesome! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>active directory auth ignoring url setting, possibly?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15054</link><project id="" key="" /><description>Hi,

Could you please confirm the active directory auth process?

I have the following config...

```
  active_directory:
    type: active_directory
    order: 0
    domain_name: moon.corp
    url: ldaps://adldaps.moon.corp:636
```

We have a few domain controllers sitting behind a load balancer which resolves to adldaps.moon.corp and I would have expected shield to authenticate users to one of these load balanced domain controllers however it appears that shield is looking up ldap services in DNS instead.

Is this expected behavior?
</description><key id="119129634">15054</key><summary>active directory auth ignoring url setting, possibly?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels /><created>2015-11-27T04:19:34Z</created><updated>2015-11-28T10:48:26Z</updated><resolved>2015-11-28T10:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T10:48:26Z" id="160274417">Hi @robertsmarty 

The correct place to ask about Shield is here: https://discuss.elastic.co/c/shield
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk update with doc reindexes document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15053</link><project id="" key="" /><description>When running a bulk request with update actions, even if `doc` is sent in the source, the document is replaced with the update source, instead of being merged. Tested on ES 2.0.0.

Create the document

```
POST my_index/my_type/1
{
  "field1": "value1"
}
```

Update the document without bulk

```
POST my_index/my_type/1/_update
{
  "doc": {
    "field2": "value2"
  }
}
```

Document is now:

```
{
  "field1": "value1",
  "field2": "value2"
}
```

Bulk update the document

```
$ cat requests
{ "update" : { "_index" : "my_index", "_type" : "my_type", "_id" : "1" } }
{ "doc": { "field1" : "aNewValue" } }
$ curl -s -XPOST localhost:9200/_bulk --data-binary "@requests"; echo
{"took":7,"errors":false,"items":[{"create":{"_index":"my_index","_type":"my_type","_id":"1","_version":3, "status":200}}]}
```

Fetch the document

```
GET my_index/my_type/1
```

Output

```
{
  "field1": "aNewValue"
}
```
</description><key id="119112273">15053</key><summary>Bulk update with doc reindexes document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yoitsro</reporter><labels /><created>2015-11-27T00:00:00Z</created><updated>2015-12-09T01:23:50Z</updated><resolved>2015-11-28T10:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T10:46:06Z" id="160273524">@yoitsro this is a known issue when running with Shield enabled, because of field level security.  2.0.1 and 2.1.0 have been released with field level security disabled by default (making the bulk updates work correctly) and disabling bulk updates if FLS is enabled.  You should upgrade your cluster.

We're working on a fix that will allow FLS to work with bulk updates in the future.
</comment><comment author="yoitsro" created="2015-12-09T01:22:42Z" id="163076694">@clintongormley Our cluster is running 2.1.0 on Found, and this is where I initially noticed the problem. Is there anything else I might be missing and why this doesn't work on Found?

Update: actually, I haven't tested on 2.1.0 yet. Apologies. Let me see if we're still experiencing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JarHell does not check whether a given path entry exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15052</link><project id="" key="" /><description>When embedding ES in Websphere Application Server (don't know the app server version right now), it is breaking boot because of:

&gt; Caused by: java.lang.IllegalStateException: failed to load bundle [] due to jar hell
&gt;   at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:342)
&gt;   at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:113)
&gt;   at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:144)
&gt;   at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
&gt;   at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)
&gt;   ... 97 more
&gt; Caused by: java.io.FileNotFoundException: C:\Program Files (x86)\IBM\WebSphere\AppServer\lib\jsf-nls.jar 
&gt; (The system cannot find the file specified)
&gt;   at java.util.zip.ZipFile.open(Native Method)
&gt;   at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:231)
&gt;   at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:161)
&gt;   at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:169)
&gt;   at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:106)
&gt;   at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:173)
&gt;   at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:340)
&gt;   ... 104 more

The correction is as simple as an if in [JarHell](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/bootstrap/JarHell.java#L174).
</description><key id="119087687">15052</key><summary>JarHell does not check whether a given path entry exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tberne</reporter><labels /><created>2015-11-26T18:03:00Z</created><updated>2015-11-30T12:50:56Z</updated><resolved>2015-11-30T12:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-26T19:12:34Z" id="159978682">The classpath should not contain non-existent elements though, that is just indicative of a bug.
</comment><comment author="tberne" created="2015-11-26T19:42:47Z" id="159984547">I couldn't disagree, but still it is a commonly used application server and because of the assumption that the jar file exists I am unable to deploy ES on Websphere.

I already implemented a workaround (for instance, I transformed JarHell.checkJarHell(URL[]) in a dummy method via Javassist).

I just think that a simple if could solve my problem (and possibly others). And there's no simple workaround for that, since Websphere's classpath is not under my control.
</comment><comment author="rmuir" created="2015-11-26T19:46:45Z" id="159985282">It destroys the entire purpose of this class. The purpose is to fail when the codebases are incorrect, not to be lenient like javas classloading and deliver strange errors later.
</comment><comment author="tberne" created="2015-11-26T19:52:59Z" id="159985911">Agreed. But I need to deploy ES on Websphere. I have no control on its classpath. What should I do?
I also think CheckJarHell is a good thing, but I have no other option since I have no control on generated classpath.
I think that many others may reach this very same problem.
</comment><comment author="s1monw" created="2015-11-26T21:45:12Z" id="159997006">why don't you add the missing jar file instead?
</comment><comment author="tberne" created="2015-11-27T01:10:56Z" id="160013509">@s1monw, I'm going to check whether this install is ok. But, once more, if the install is ok and it is an issue of Websphere, how could I deploy ES?
</comment><comment author="s1monw" created="2015-11-27T19:54:49Z" id="160196230">&gt; @s1monw, I'm going to check whether this install is ok. But, once more, if the install is ok and it is an issue of Websphere, how could I deploy ES?

well I guess if you can't fix it you need to wait until it's fixed in on the websphere end?
</comment><comment author="tberne" created="2015-11-27T20:27:25Z" id="160200426">Well, this is not an option at all. Given that CheckJarHell is susceptible to this kind of classpath failure and I can't fix the classpath, there's only one way: completely disable CheckJarHell through Javassist.

BTW, in [Java SE Documentation](https://docs.oracle.com/javase/7/docs/technotes/tools/windows/classpath.html), it is said that "Classpath entries that are neither directories nor archives (.zip or .jar files) nor \* are ignored.".

This makes me think the CheckJarHell should have the same behavior.
</comment><comment author="nik9000" created="2015-11-28T14:56:48Z" id="160307119">&gt; This makes me think the CheckJarHell should have the same behavior.

I think JarHell is intentionally being more strict that the spec.

Are you trying to embed the Elasticsearch server in websphere or just the client?
</comment><comment author="rmuir" created="2015-11-28T15:02:15Z" id="160307325">&gt; I think JarHell is intentionally being more strict that the spec.

Exactly, as already explained above. Adding any leniency like classpath elements that don't exist will only hide bugs, e.g. hiding errors of wrong file permissions and so on, and instead delivering confusing NoClassDefFoundError later. 

The problem is, anytime someone has jar hell, they want leniency for their specific case: if we were to do this, it would already be a no-op check and be useless.
</comment><comment author="tberne" created="2015-11-28T20:08:26Z" id="160332681">@nik9000, we embed Elasticsearch. Depending on the client configuration, it will act as a client or as a server. In our product we do not recommend to use the embedded ES as a server for production, but it is configurable.

@rmuir, I don't think a no-op check (like I did with Javassist) is the only option here. There could be a blacklist of classpath entries (and/or classes maybe?) or something so this strange scenario could be solved without making CheckJarHell useless. This way the specific (ugly) problem could be fixed and CJH would still play along.
</comment><comment author="rjernst" created="2015-11-28T21:00:39Z" id="160335253">&gt; There could be a blacklist of classpath entries (and/or classes maybe?) or something so this strange scenario could be solved without making CheckJarHell useless.

That is exactly the leniency that would make it useless, because you could then configure JarHell to essentially do nothing (by blacklisting every jar). JarHell is an upfront check for potentially buggy behavior. As @rmuir explained above, a file "not existing" could be an error in our security policy which does not grant access to the jar.
</comment><comment author="s1monw" created="2015-11-30T09:15:38Z" id="160566184">I agree we can't really loosen the strictness here. This is something that IMO should be build-in to Java but it isn't. We can't add workarounds for broken 3rd party installations, those installations need to be fixed instead. We try to prevent hunting any ghosts because of class file clashes or broken installations. If we make this check lenient or add a flag it's entire purpose vanishes.
</comment><comment author="tberne" created="2015-11-30T12:23:29Z" id="160615550">Guys, you forgot we live in a real world, where not always we will have complete control over everything. If you decide to let CJH as it is, I will understand. But in a real world those things happen. And whenever they happen, someone will have to do dirty things (like Javassisting and make CJH no-op).
</comment><comment author="rmuir" created="2015-11-30T12:42:51Z" id="160618598">Its not really that. Its just that jarhell gets _SO MUCH HATE_ because its the first thing that will fail for you.

But really, if you run inside any container, you are in for a world of pain, because such a configuration is completely untested.

The point of jarhell is to fail early and hard on stupid problems, hence users whine about it. But it is not the issue, the problem is your broken configuration.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>body attribute was at wrong nesting level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15051</link><project id="" key="" /><description /><key id="119083916">15051</key><summary>body attribute was at wrong nesting level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>:REST</label><label>non-issue</label></labels><created>2015-11-26T17:34:12Z</created><updated>2016-01-13T20:35:25Z</updated><resolved>2015-11-28T10:31:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2015-11-26T17:39:59Z" id="159966051">Signed contributor agreement just now.
</comment><comment author="clintongormley" created="2015-11-28T10:31:54Z" id="160271838">thanks @apatrida - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Another number of cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15050</link><project id="" key="" /><description>PipelineStore no longer is a lifecycle component
Client in PipelineStore gets provided via a guice provider
Processor and Factory throw Exception instead of IOException
Removed PipelineExecutionService.Listener with ActionListener
</description><key id="119081618">15050</key><summary>[Ingest] Another number of cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-26T17:13:49Z</created><updated>2015-11-27T12:00:36Z</updated><resolved>2015-11-27T12:00:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-27T10:45:46Z" id="160111075">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API allows conflicting mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15049</link><project id="" key="" /><description>The update API allows conflicting field mappings to be introduced which can then bring a cluster down as it keeps trying and failing to sync the mappings.  Recreation:

```
PUT t
{
  "mappings": {
    "one": {
      "properties": {
        "foo": {
          "type": "nested",
          "properties": {
            "bar": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}

PUT t/two/1
{}

POST t/two/1/_update
{
  "doc": {
    "foo": {
      "bar": 5
    }
  }
}

GET t/_mapping
```

returns:

```
{
  "t": {
    "mappings": {
      "one": {
        "properties": {
          "foo": {
            "type": "nested",
            "properties": {
              "bar": {
                "type": "string"
              }
            }
          }
        }
      },
      "two": {
        "properties": {
          "foo": {
            "properties": {
              "bar": {
                "type": "long"
              }
            }
          }
        }
      }
    }
  }
}
```

From: https://discuss.elastic.co/t/cluster-keeps-becoming-unresponsive/35677
</description><key id="119079645">15049</key><summary>Update API allows conflicting mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label></labels><created>2015-11-26T16:56:48Z</created><updated>2015-12-05T12:06:24Z</updated><resolved>2015-12-04T13:46:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-27T13:19:16Z" id="160137790">This is a more general issue of our mapping API. I haven't figured out the cause yet, but here is a minimal recreation that reproduces the issue all the time:

```
PUT t
{
  "mappings": {
    "one": {
      "properties": {
        "foo": {
          "type": "string"
        }
      }
    }
  }
}

PUT t/two/_mapping
{
  "two": {

  }
}

PUT t/two/_mapping
{
  "two": {
    "properties": {
      "foo": {
        "type": "long"
      }
    }
  }
}

PUT t/two/_mapping
{
  "two": {
    "properties": {
      "foo": {
        "type": "long"
      }
    }
  }
}

GET t/_mapping
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REPRODUCE command has double quotes around jvm options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15048</link><project id="" key="" /><description>See for example http://build-us-00.elastic.co/job/es_core_2x_window-2008/265/consoleFull

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.qa.backwards:2.0.0 -Dtests.seed=9772A1E19F8B6B15 -Dtests.class=org.elasticsearch.search.aggregations.bucket.SignificantTermsBackwardCompatibilityIT -Dtests.method="testBucketStreaming" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Des.node.mode=network -Dtests.jvm.argline=""-server -XX:+UseSerialGC -XX:+UseCompressedOops -Djava.net.preferIPv4Stack=true"" -Dtests.locale=fr -Dtests.timezone=America/Cuiaba

```

but should be 

```
... -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:+UseCompressedOops -Djava.net.preferIPv4Stack=true" ...
```
</description><key id="119056310">15048</key><summary>REPRODUCE command has double quotes around jvm options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-11-26T14:38:04Z</created><updated>2016-02-13T22:43:20Z</updated><resolved>2016-02-13T22:43:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jobin-jose" created="2015-12-01T05:42:36Z" id="160859300">This fixes the additional double quotes in jvm argline which appears in REPRODUCE WITH
</comment><comment author="jobin-jose" created="2015-12-01T05:43:39Z" id="160859420">https://github.com/jobin-jose/elasticsearch/commit/6863d93ac38f28919ac3fd047583ba59b5ccd071
</comment><comment author="clintongormley" created="2016-02-13T22:43:19Z" id="183766995">Closed by #16407
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CRUD: Allow to get and set ttl as a time value/string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15047</link><project id="" key="" /><description>Specifying the ttl as date math is possible only through the REST layer at the moment. Same should be possible using the java api, namely `IndexRequest`, which should keep track of the time value as a `TimeValue` object instead. This makes it possible for plugins like ingest to intercept an index request, eventually retrieve the `_ttl`, which can then be retrieved as a string, and modify/set it back to the index request, again as a string which will get parsed into a time value expression.
</description><key id="119052839">15047</key><summary>CRUD: Allow to get and set ttl as a time value/string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:CRUD</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T14:14:48Z</created><updated>2015-12-04T11:46:08Z</updated><resolved>2015-12-04T11:46:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-26T16:50:00Z" id="159958656">I think this extra setter isn't enough and that `ttl` should be a string field in index request. Plugins that introspect an index requests now and like to modify ttl, don't know if the original ttl was 1w, 7d or 3864h. 

Inside the transport action we can convert the time value to the amount millis.
</comment><comment author="javanna" created="2015-11-30T17:06:43Z" id="160691439">@martijnvg I updated the PR, this PR is meant for master only, I will put up another PR for backporting later so we don't have to deal with bw comp here.
</comment><comment author="martijnvg" created="2015-12-01T11:49:58Z" id="160944566">Maybe also label this issue with internal and CRUD? LGTM
</comment><comment author="javanna" created="2015-12-01T12:05:55Z" id="160948257">@bleskes could you please have a look at this too?
</comment><comment author="bleskes" created="2015-12-01T15:33:53Z" id="161002992">LGTM. Left some very minor comments 
</comment><comment author="javanna" created="2015-12-01T18:54:14Z" id="161061564">@bleskes I pushed a new commit and left a comment about using readOptionalStreamable. thanks for your review!
</comment><comment author="bleskes" created="2015-12-02T11:10:30Z" id="161263330">Thx @javanna . LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error while using the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15046</link><project id="" key="" /><description>I started a new Java project. I'm using the elasticsearch-2.0.0.jar file, included it in my project, and the guava-18.0.jar file. I did not do any other configurations. 

I'm using Elasticsearch 2.0.0 and it is running on localhost port 9200. The following is my Java code, all I'm trying to do is create a client:

```
      public void createClient(String Username) throws UnknownHostException {
               Client client = TransportClient.builder().build()
                              .addTransportAddress(new InetSocketTransportAddress(InetAddress.getLocalHost(), 9200))
                              .addTransportAddress(new InetSocketTransportAddress(InetAddress.getLocalHost(), 9200));

}
```

However, it is giving me the following error:

```
     Nov 26, 2015 2:39:42 PM org.elasticsearch.plugins.PluginsService &lt;init&gt;
     INFO: [Hydro] loaded [], sites []
     Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/lucene/util/Version
         at org.elasticsearch.Version.&lt;clinit&gt;(Version.java:44)
         at org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:117)
         ......
     Caused by: java.lang.ClassNotFoundException: org.apache.lucene.util.Version
         at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
         at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
         at java.security.AccessController.doPrivileged(Native Method)
         at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         ... 4 more
     Java Result: 1
```
</description><key id="119038089">15046</key><summary>Error while using the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Israaaaa</reporter><labels /><created>2015-11-26T12:45:53Z</created><updated>2015-11-26T13:57:44Z</updated><resolved>2015-11-26T13:23:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Israaaaa" created="2015-11-26T12:47:10Z" id="159903692">Do I need to add more jar files? Lucene files, perhaps? Because I already tried that, and it still gave me errors.. 
</comment><comment author="javanna" created="2015-11-26T13:23:51Z" id="159911038">Hi @Israaaaa can you please join us on https://discuss.elastic.co/ so we can help you out there? We use github for bug reports and tasks rather than user questions.
</comment><comment author="dadoonet" created="2015-11-26T13:24:49Z" id="159911191">Yes. You need to add all transitive dependencies.
Using maven or Gradle could simplify that a lot.

Not an issue here so closing.
</comment><comment author="Israaaaa" created="2015-11-26T13:29:05Z" id="159912049">@dadoonet Thank you for your reply. I don't have any experience with maven, so I can't use it on such short time. Is there a way to find a list of all the jar dependencies that I should add? Because Ive tried to add all the Lucene jar files, and it just kept giving me errors. So maybe I was adding the wrong jar files. 
</comment><comment author="dadoonet" created="2015-11-26T13:42:21Z" id="159916733">@Israaaaa here is what `mvn dependency:tree` gives on the 2.0.0 project (I removed test scoped libs):

```
[INFO] org.elasticsearch:elasticsearch:jar:2.0.0
[INFO] +- org.apache.lucene:lucene-core:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-queries:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-memory:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile
[INFO] |  \- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile
[INFO] |  \- org.apache.lucene:lucene-misc:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-join:jar:5.2.1:compile
[INFO] |  \- org.apache.lucene:lucene-grouping:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile
[INFO] +- org.apache.lucene:lucene-expressions:jar:5.2.1:compile
[INFO] |  +- org.antlr:antlr-runtime:jar:3.5:compile
[INFO] |  +- org.ow2.asm:asm:jar:4.1:compile
[INFO] |  \- org.ow2.asm:asm-commons:jar:4.1:compile
[INFO] +- com.spatial4j:spatial4j:jar:0.4.1:compile
[INFO] +- com.vividsolutions:jts:jar:1.13:compile
[INFO] +- com.github.spullara.mustache.java:compiler:jar:0.8.13:compile
[INFO] +- com.google.guava:guava:jar:18.0:compile
[INFO] +- com.carrotsearch:hppc:jar:0.7.1:compile
[INFO] +- joda-time:joda-time:jar:2.8.2:compile
[INFO] +- org.joda:joda-convert:jar:1.2:compile
[INFO] +- com.fasterxml.jackson.core:jackson-core:jar:2.5.3:compile
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:compile
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:compile
[INFO] |  \- org.yaml:snakeyaml:jar:1.12:compile
[INFO] +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:compile
[INFO] +- io.netty:netty:jar:3.10.5.Final:compile
[INFO] +- com.ning:compress-lzf:jar:1.0.2:compile
[INFO] +- com.tdunning:t-digest:jar:3.0:compile
[INFO] +- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile
[INFO] +- commons-cli:commons-cli:jar:1.3.1:compile
[INFO] +- org.codehaus.groovy:groovy-all:jar:indy:2.4.4:compile
[INFO] +- log4j:log4j:jar:1.2.17:compile
[INFO] +- log4j:apache-log4j-extras:jar:1.2.17:compile
[INFO] +- org.slf4j:slf4j-api:jar:1.6.2:compile
[INFO] +- net.java.dev.jna:jna:jar:4.1.0:compile
[INFO] \- com.twitter:jsr166e:jar:1.1.0:compile
```

Good luck for adding that manually!

If you have any other question, please follow up on discuss as explained by @javanna!
</comment><comment author="Israaaaa" created="2015-11-26T13:57:44Z" id="159922055">Okay thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards mapping inconsistency </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15045</link><project id="" key="" /><description>Hi guys, 

I'm using elasticsearch 1.7.3 and encountered a strange case where 2 data nodes return a different result set for the exact same query. The queried index have 1 primary shard and 1 replica shard.

Debugging it I see that the mapping of that same index is different in the 2 relevant nodes. one of the nodes have more fields in the mapping, although both have the same doc count.

Can you please help? How can this happen? is there any indexing config I can apply to prevent these sort of thing? I've scanned the issues here and couldn't find a relevant issue.
- Not sure it is related, but the elasticsearch cluster I'm using have many indices/shards, but this is the first time it happens.

Thanks!
</description><key id="119035168">15045</key><summary>Shards mapping inconsistency </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">segalziv</reporter><labels /><created>2015-11-26T12:24:06Z</created><updated>2015-11-26T16:41:48Z</updated><resolved>2015-11-26T16:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-26T15:50:52Z" id="159945784">Are you using dynamic mappings?
</comment><comment author="segalziv" created="2015-11-26T15:55:47Z" id="159946866">hi @rjernst, thanks. yes I'm using dynamic mapping with doc-values by default.
</comment><comment author="rjernst" created="2015-11-26T16:16:27Z" id="159951791">Thanks @segalziv. This was fixed in 2.0. Dynamic mapping updates were previously sent to the master asynchronously, so updates could get out of sync on failures or conflicts across nodes. We now send the update to the master before applying the update. 
</comment><comment author="segalziv" created="2015-11-26T16:41:48Z" id="159956741">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make HighlightBuilder implement Writable </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15044</link><project id="" key="" /><description>In the current state of the search refactoring the SearchSourceBuilder still holds the highlighter as BytesReference which needs to be parsed later on the shard. This was done to speed up the query refactoring but in order to parse it on the coordinating, we need to make HighlightBuilder and its nested structures (mainly Field) to implement Writeable and override equals and hashcode for testing.

Relates to #10217 
</description><key id="119030108">15044</key><summary>Make HighlightBuilder implement Writable </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T11:49:39Z</created><updated>2015-12-15T16:44:15Z</updated><resolved>2015-12-15T16:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-30T13:57:20Z" id="160636052">Current plan is to split this in three different PRs:
- [x] make HighlightBuilder implement Writable, add equals(), hashCode() and tests (#15113)
- [x] add fromXContent method to HighlightBuilder, using same parsing logic as in the current SearchContextHighlighter (#15157)
- [x] create conversion method from HighlightBuilder to SearchContextHighlight, since that is what we still use in the SearchContext (#15324)

EDIT: 
There are some additional issues that should be adressed before this issue is closed:
- [x] Make parsing of global highlight parameters and field-level overwrites more consistent (#15368)
- [x] use HighlightBuilder in SearchSourceBuilder  (#15376)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.1 occasional failed engine [refresh failed] due to NullPointerException while evicting query cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15043</link><project id="" key="" /><description>Hello all.

I am running 4-node ES cluster with logstash as data producer. Recently I've upgraded ES from 1.6 to 2.1. After upgrade recent indexs' shards are continuously being marked as failed and re-initialized. If failed shard is primary shard then indexing stucks.

One node:

```
2015-11-26_07:01:21.22958 [2015-11-26 07:01:21,229][WARN ][index.engine             ] [John Falsworth] [logstash-2015.11.26][2] failed engine [refresh failed]
2015-11-26_07:01:21.22961 java.lang.NullPointerException
2015-11-26_07:01:21.23006 [2015-11-26 07:01:21,229][WARN ][index.shard              ] [John Falsworth] [logstash-2015.11.26][2] Failed to perform scheduled engine refresh
2015-11-26_07:01:21.23008 [logstash-2015.11.26][[logstash-2015.11.26][2]] RefreshFailedEngineException[Refresh failed]; nested: NullPointerException;
2015-11-26_07:01:21.23008       at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:686)
2015-11-26_07:01:21.23008       at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
2015-11-26_07:01:21.23009       at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
2015-11-26_07:01:21.23009       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
2015-11-26_07:01:21.23009       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
2015-11-26_07:01:21.23010       at java.lang.Thread.run(Thread.java:745)
2015-11-26_07:01:21.23010 Caused by: java.lang.NullPointerException
2015-11-26_07:01:21.23044 [2015-11-26 07:01:21,230][WARN ][indices.cluster          ] [John Falsworth] [[logstash-2015.11.26][2]] marking and sending shard failed due to [eng
ine failure, reason [refresh failed]]
2015-11-26_07:01:21.23046 java.lang.NullPointerException
2015-11-26_07:04:18.05160 [2015-11-26 07:04:18,050][WARN ][index.translog           ] [John Falsworth] [logstash-2015.11.26][2] failed to delete temp file /var/lib/elasticsea
rch/dss2es/nodes/0/indices/logstash-2015.11.26/2/translog/translog-8708119625210250383.tlog
2015-11-26_07:04:18.05162 java.nio.file.NoSuchFileException: /var/lib/elasticsearch/dss2es/nodes/0/indices/logstash-2015.11.26/2/translog/translog-8708119625210250383.tlog
2015-11-26_07:04:18.05163       at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
2015-11-26_07:04:18.05163       at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
2015-11-26_07:04:18.05163       at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
2015-11-26_07:04:18.05164       at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
2015-11-26_07:04:18.05164       at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
2015-11-26_07:04:18.05164       at java.nio.file.Files.delete(Files.java:1126)
2015-11-26_07:04:18.05165       at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
2015-11-26_07:04:18.05165       at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
2015-11-26_07:04:18.05165       at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
2015-11-26_07:04:18.05166       at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
2015-11-26_07:04:18.05166       at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
2015-11-26_07:04:18.05167       at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
2015-11-26_07:04:18.05167       at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
2015-11-26_07:04:18.05167       at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
2015-11-26_07:04:18.05168       at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
2015-11-26_07:04:18.05168       at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
2015-11-26_07:04:18.05168       at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
2015-11-26_07:04:18.05169       at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
2015-11-26_07:04:18.05169       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
2015-11-26_07:04:18.05169       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
2015-11-26_07:04:18.05171       at java.lang.Thread.run(Thread.java:745)
```

Another node:

```
2015-11-26_07:04:16.53521 [2015-11-26 07:04:16,534][WARN ][index.engine             ] [Turner D. Century] [logstash-2015.11.26][2] failed engine [refresh failed]
2015-11-26_07:04:16.53524 java.lang.NullPointerException
2015-11-26_07:04:16.53525       at org.elasticsearch.indices.cache.query.IndicesQueryCache$1.onDocIdSetEviction(IndicesQueryCache.java:158)
2015-11-26_07:04:16.53525       at org.apache.lucene.search.LRUQueryCache.clearCoreCacheKey(LRUQueryCache.java:313)
2015-11-26_07:04:16.53525       at org.apache.lucene.search.LRUQueryCache$1.onClose(LRUQueryCache.java:276)
2015-11-26_07:04:16.53526       at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:168)
2015-11-26_07:04:16.53526       at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:157)
2015-11-26_07:04:16.53526       at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:175)
2015-11-26_07:04:16.53527       at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
2015-11-26_07:04:16.53527       at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:359)
2015-11-26_07:04:16.53527       at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
2015-11-26_07:04:16.53528       at org.apache.lucene.index.IndexReader.close(IndexReader.java:403)
2015-11-26_07:04:16.53528       at org.apache.lucene.index.FilterDirectoryReader.doClose(FilterDirectoryReader.java:134)
2015-11-26_07:04:16.53529       at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
2015-11-26_07:04:16.53529       at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:130)
2015-11-26_07:04:16.53530       at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
2015-11-26_07:04:16.53531       at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:274)
2015-11-26_07:04:16.53531       at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:189)
2015-11-26_07:04:16.53531       at org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)
2015-11-26_07:04:16.53532       at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:678)
2015-11-26_07:04:16.53532       at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
2015-11-26_07:04:16.53532       at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
2015-11-26_07:04:16.53532       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
2015-11-26_07:04:16.53533       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
2015-11-26_07:04:16.53533       at java.lang.Thread.run(Thread.java:745)
```
</description><key id="119028543">15043</key><summary>ES 2.1 occasional failed engine [refresh failed] due to NullPointerException while evicting query cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">vs-adm</reporter><labels><label>:Cache</label><label>bug</label><label>v2.1.1</label></labels><created>2015-11-26T11:39:30Z</created><updated>2015-12-03T08:38:08Z</updated><resolved>2015-12-03T08:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vs-adm" created="2015-11-27T14:29:32Z" id="160150602">Anyone? I can handle this by moving failed shard to another node by the way.
</comment><comment author="vs-adm" created="2015-11-27T23:18:43Z" id="160226379">It seems like NPE is in cache eviction method which can't aquire stats counters using `stats2`

``` java
protected void onDocIdSetEviction(Object readerCoreKey, int numEntries, long sumRamBytesUsed) {
                assert Thread.holdsLock(this);
                super.onDocIdSetEviction(readerCoreKey, numEntries, sumRamBytesUsed);
                // We can't use ShardCoreKeyMap here because its core closed
                // listener is called before the listener of the cache which
                // triggers this eviction. So instead we use use stats2 that
                // we only evict when nothing is cached anymore on the segment
                // instead of relying on close listeners
                final StatsAndCount statsAndCount = stats2.get(readerCoreKey);
                final Stats shardStats = statsAndCount.stats;
                shardStats.cacheSize -= numEntries;
                shardStats.ramBytesUsed -= sumRamBytesUsed;
                statsAndCount.count -= numEntries;
                if (statsAndCount.count == 0) {
                    stats2.remove(readerCoreKey);
                }
            }
```

What is the preferable way to fix this? Ignoring stats update if `statsAndCount` is `null` leads to incorrect statistics.
</comment><comment author="vs-adm" created="2015-11-27T23:26:05Z" id="160226692">Also, API call `POST /_cache/clear` episodically gives the same error

``` json
{
  "_shards": {
    "total": 6830,
    "successful": 6827,
    "failed": 3,
    "failures": [
      {
        "shard": 2,
        "index": "logstash-2015.11.26",
        "status": "INTERNAL_SERVER_ERROR",
        "reason": {
          "type": "null_pointer_exception",
          "reason": null
        }
      }
    ]
  }
}
```
</comment><comment author="clintongormley" created="2015-11-28T13:41:48Z" id="160296550">@s1monw Is this issue fixed by https://github.com/elastic/elasticsearch/pull/15012 ?
</comment><comment author="bleskes" created="2015-11-30T15:34:40Z" id="160662777">@clintongormley I think this is something else, relating to evictions in the new query cache. Maybe @jpountz can take a look?
</comment><comment author="IngaFeick" created="2015-12-02T10:00:04Z" id="161244015">Same story here. Updated to 2.1 a couple of days ago also, and ever since we have the same error message that some temporary files could not be deleted:

```
[2015-12-02 09:31:41,802][WARN ][index.translog           ] [Bacon] [.kibana][0] failed to delete temp file /appdata/elasticsearch/data/elasticsearch/nodes/0/indices/.kibana/0/translog/translog-4247269950871871367.tlog
```

The mentioned files never exist. 
Also some of our shards are stuck in a translog state. When this first occured a couple of days ago, they would recover from it after some time, but now that's not working anymore and the number of affected shards is growing. Restarting elasticsearch does not help (anymore). The indices also cannot be closed or deleted via the http api anymore. Indexing is stuck, too.
</comment><comment author="jpountz" created="2015-12-02T10:39:34Z" id="161254367">@IngaFeick This warning can be safely ignored. It will be fixed in 2.1.1, see https://github.com/elastic/elasticsearch/pull/14872.
</comment><comment author="jpountz" created="2015-12-02T10:53:41Z" id="161257310">To be clear, only this warning that elasticsearch "failed to delete" a temp file will be fixed. The NullPointerException mentioned above is an actual bug however.
</comment><comment author="IngaFeick" created="2015-12-02T10:57:42Z" id="161258522">Thanks @jpountz , unfortunately we get that NPE also:

```
java.lang.NullPointerException
        at org.elasticsearch.indices.cache.query.IndicesQueryCache$1.onDocIdSetEviction(IndicesQueryCache.java:158)
        at org.apache.lucene.search.LRUQueryCache.clearCoreCacheKey(LRUQueryCache.java:313)
        at org.apache.lucene.search.LRUQueryCache$1.onClose(LRUQueryCache.java:276)
        at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:168)
        at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:157)
        at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:175)
        at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
        at org.apache.lucene.index.ReadersAndUpdates.dropReaders(ReadersAndUpdates.java:182)
        at org.apache.lucene.index.IndexWriter$ReaderPool.dropAll(IndexWriter.java:603)
        at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2075)
```
</comment><comment author="vs-adm" created="2015-12-02T10:58:44Z" id="161259092">As a temporary and dirty workaround I'm running `POST /cache/clear` every ten minutes in cronjob. This slows down search requests but prevents shards format failing.
</comment><comment author="IngaFeick" created="2015-12-02T11:00:39Z" id="161260205">@vs-adm we tried that also, to no noticable effect, but I'll give that cronjob a try to prevent additional failures. Thanks!
</comment><comment author="jpountz" created="2015-12-02T21:10:12Z" id="161434814">I managed to find a case in which I'm able to reproduce the above NullPointerException, see #15202. Hopefully this is the same case as this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES: 2.1.0 Cloud-Azure-Plugin Not able to create Repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15042</link><project id="" key="" /><description>With latest elasticsearch version and lastet cloud-azure plugin installed with

```
bin/plugin install cloud-azure
```

renders into an error:

```
{"error":{"root_cause":[{"type":"repository_exception","reason":"[backup_azure] failed to create repository"}],"type":"repository_exception","reason":"[backup_azure] failed to create repository","caused_by":{"type":"illegal_argument_exception","reason":"Unknown [repository] type [azure]"}},"status":500}
```
</description><key id="119012234">15042</key><summary>ES: 2.1.0 Cloud-Azure-Plugin Not able to create Repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkonrad</reporter><labels /><created>2015-11-26T10:13:27Z</created><updated>2015-11-26T13:33:23Z</updated><resolved>2015-11-26T13:33:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-26T12:03:09Z" id="159896409">Can you share your elasticsearch.yml file? 

You can anonymize it!
</comment><comment author="rkonrad" created="2015-11-26T12:16:33Z" id="159898861">yes, of course, i cleaned it up a little bit

```
# ======================== Elasticsearch Configuration =========================
cluster.name: XXX

node.name: node-2

path.repo: /esshare

bootstrap.mlockall: true

network.host: 10.1.1.2

http.port: 9200

discovery.zen.ping.unicast.hosts: ["10.1.1.1", "10.1.1.2", "10.1.1.3"]
discovery.zen.minimum_master_nodes: 3

action.destructive_requires_name: true

http.cors.enabled: true
http.cors.allow-origin: "XXX"

cloud.azure.storage.storage_account: "XXX"
cloud.azure.storage.storage_key: "XXX"
```
</comment><comment author="dadoonet" created="2015-11-26T13:22:04Z" id="159910707">Settings names changed and were deprecated some months ago. Old names have been removed in 2.0.

See https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-azure-repository.html#cloud-azure-repository
</comment><comment author="rkonrad" created="2015-11-26T13:28:06Z" id="159911676">you're right... i totally missed it. sorry for interrupting you.
</comment><comment author="dadoonet" created="2015-11-26T13:33:23Z" id="159913855">No problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Grok discover api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15041</link><project id="" key="" /><description>We should add an api to the ingest plugin that allows to provide a log line and get back what the corresponding grok pattern would look like. This would make dealing with grok expressions much easier.
</description><key id="119011426">15041</key><summary>[Ingest] Grok discover api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>feature</label></labels><created>2015-11-26T10:10:36Z</created><updated>2016-03-23T17:59:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>SharedClusterSnapshotRestoreIT.changeSettingsOnRestoreTest times out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15040</link><project id="" key="" /><description>See 
http://build-us-00.elastic.co/job/es_core_21_metal/264
</description><key id="119003147">15040</key><summary>SharedClusterSnapshotRestoreIT.changeSettingsOnRestoreTest times out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-11-26T09:20:52Z</created><updated>2016-03-25T08:41:03Z</updated><resolved>2016-03-25T08:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-26T11:13:00Z" id="159884876">Looks to me like filesystem error.

We have threads hanging in sun.nio.ch.FileDispatcherImpl.force0(Native Method).

Interesting bits in log:

```
[2015-11-26 01:54:03,961][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,961][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,961][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,964][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,964][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,964][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,965][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,965][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,965][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,965][DEBUG][netty.channel.socket.nio.AbstractNioSelector] Selector.select() returned prematurely because the I/O thread has been interrupted. Use shutdown() to shut the NioSelector down.
[2015-11-26 01:54:03,965][WARN ][gateway                  ] [node_s0] [_global]: failed to write global state
org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:406)
    at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:133)
    at org.elasticsearch.gateway.MetaStateService.writeGlobalState(MetaStateService.java:149)
    at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:148)
    at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Suppressed: java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:391)
        at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:396)
        ... 10 more
Caused by: java.lang.InterruptedException: sleep interrupted
    at java.lang.Thread.sleep(Native Method)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:404)
    ... 10 more
```

Comment in Netty code regarding the `Selector.select()` warning: http://netty.io/3.9/xref/org/jboss/netty/channel/socket/nio/AbstractNioSelector.html

```
// Thread was interrupted but NioSelector was not shutdown.
239                                 // As this is most likely a bug in the handler of the user or it's client
240                                 // library we will log it.
241                                 //
242                                 // See https://github.com/netty/netty/issues/2426
```

Interesting threads in stack dump:

```
    elasticsearch[node_s2][snapshot][T#1]" ID=7457 RUNNABLE
    at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
    at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
    at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:387)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:396)
    at org.elasticsearch.common.blobstore.fs.FsBlobContainer.writeBlob(FsBlobContainer.java:99)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:659)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:604)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:191)
    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:340)
    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:76)
    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:296)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Locked synchronizers:
    - java.util.concurrent.ThreadPoolExecutor$Worker@3d035ed2

"elasticsearch[node_s1][snapshot][T#1]" ID=7456 RUNNABLE
    at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
    at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
    at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:387)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:396)
    at org.elasticsearch.common.blobstore.fs.FsBlobContainer.writeBlob(FsBlobContainer.java:98)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:659)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:604)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:191)
    at org.elasticsearch.snapshots.SnapshotShardsService.snapshot(SnapshotShardsService.java:340)
    at org.elasticsearch.snapshots.SnapshotShardsService.access$200(SnapshotShardsService.java:76)
    at org.elasticsearch.snapshots.SnapshotShardsService$1.doRun(SnapshotShardsService.java:296)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Locked synchronizers:
    - java.util.concurrent.ThreadPoolExecutor$Worker@4a066cec
```
</comment><comment author="ywelsch" created="2015-11-26T11:13:23Z" id="159884955">@s1monw Looking at org.apache.lucene.util.IOUtils.fsync, I'm wondering though why the force operation is retried in case of a ClosedChannelException (of which the thrown ClosedByInterruptException is one).
</comment><comment author="ywelsch" created="2015-11-26T11:34:30Z" id="159890641">might be related to this:

https://issues.apache.org/jira/browse/LUCENE-6902
</comment><comment author="clintongormley" created="2016-02-14T17:59:02Z" id="183938832">@ywelsch can this be closed?
</comment><comment author="ywelsch" created="2016-03-25T08:41:03Z" id="201202308">With the fix in Lucene an no further reoccurrences I'm aware of, I'm closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>set RLIMIT_NPROC = 0 on bsd/os X systems.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15039</link><project id="" key="" /><description>This BSD-specific limit prevents child process creation.

Windows has something that appears similar, I will look into it.

Tested on OS X, FreeBSD, and OpenBSD.
</description><key id="119003140">15039</key><summary>set RLIMIT_NPROC = 0 on bsd/os X systems.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T09:20:47Z</created><updated>2015-11-28T10:47:21Z</updated><resolved>2015-11-27T02:18:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-26T15:39:03Z" id="159943065">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give enough ram so javac won't OOM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15038</link><project id="" key="" /><description>Today its based on the automatic -Xmx, but this
will fail in some environments (e.g. vagrant).
</description><key id="119002804">15038</key><summary>Give enough ram so javac won't OOM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T09:18:24Z</created><updated>2015-11-27T02:18:16Z</updated><resolved>2015-11-27T02:18:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-26T09:20:20Z" id="159855619">LGTM
</comment><comment author="rjernst" created="2015-11-26T15:33:11Z" id="159940040">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexStatsIT.throttleStats failure: index throttling didn't kick in </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15037</link><project id="" key="" /><description>See http://build-us-00.elastic.co/job/es_core_21_metal/265/testReport/junit/org.elasticsearch.indices.stats/IndexStatsIT/throttleStats/

```
java.lang.AssertionError: index throttling didn't kick in after 5 minutes of intense merging
    at __randomizedtesting.SeedInfo.seed([1EEE4D54B95D30BC:A3ED42A92929FD08]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.elasticsearch.indices.stats.IndexStatsIT.throttleStats(IndexStatsIT.java:352)
```

Sadly, it doesn't reproduce. @mikemccand this is classically your baby. Feel free to reassign.  
</description><key id="119002323">15037</key><summary>IndexStatsIT.throttleStats failure: index throttling didn't kick in </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T09:14:55Z</created><updated>2015-12-16T22:57:31Z</updated><resolved>2015-12-16T22:57:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-27T14:34:55Z" id="160151471">LOL my baby :)  I will dig ...
</comment><comment author="mikemccand" created="2015-12-16T22:18:25Z" id="165270642">This looks fixed now by e3628ecad244f15f74badca63146d548d0869dd4 where I tried switching to async translog, since we are doing individual indexing ops.

It's odd this was necessary only on 2.1.x though, I can't explain that.

But I'll port forward to 2.2 and 3.0 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Support for ingest transient metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15036</link><project id="" key="" /><description>We should add support for transient metadata fields that get added by ingest to each document that gets processed, which can later be referred to by any processor. For instance the ingest timestamp would be a good one to be available all the time; it should be possible to add a field through the add processor and set it to that value. Same for information about the error when a processor fails, so that the on failure processor can grab it and write it to the desired field using the add processor.

Yet to be defined is the syntax to refer to these fields, we have to be careful and try to prevent confusion with elasticsearch metadata fields.

Relates to #14990 .
</description><key id="118999211">15036</key><summary>[Ingest] Support for ingest transient metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-26T08:53:48Z</created><updated>2015-12-09T21:09:14Z</updated><resolved>2015-12-09T21:09:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-09T21:09:14Z" id="163392402">Implemented in #15283. The timestamp is already available, the failure object will be exposed as part of #14548.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify MonitorService construction and detach from guice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15035</link><project id="" key="" /><description>This is a pretty trivial change that moves most of the monitor service related
object creation from guice into the monitor service. This is a babystep towards removing
guice on the node level as well. Instead of opening huge PRs I try to do this in baby-steps
that are easier to digest.
</description><key id="118999182">15035</key><summary>Simplify MonitorService construction and detach from guice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T08:53:34Z</created><updated>2015-11-28T17:48:42Z</updated><resolved>2015-11-26T13:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-11-26T09:03:13Z" id="159852501">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>testUpgradeOldMapping200 fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15034</link><project id="" key="" /><description>See: http://build-us-00.elastic.co/job/es_core_21_strong/363/

```
Failed to execute phase [query_fetch], all shards failed
    at __randomizedtesting.SeedInfo.seed([13BE8512880D9B9B:3332E128222610B]:0)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:228)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Sadly this doesn't reproduce locally for me. It may be related to #14896, so @jpountz can you please take a look? 
</description><key id="118990828">15034</key><summary>testUpgradeOldMapping200 fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2015-11-26T07:44:55Z</created><updated>2015-12-23T18:28:09Z</updated><resolved>2015-12-23T18:28:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-26T07:46:12Z" id="159838429">another example, if it helps: http://build-us-00.elastic.co/job/es_core_2.x_oel_6/1202
</comment><comment author="jpountz" created="2015-12-23T18:28:07Z" id="166964010">Fixed by @s1monw  (thanks)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong properties is used in document.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15033</link><project id="" key="" /><description>`ignore_script` is not right. `ignored_script' is right.

See org.elasticsearch.index.analysis.CJKBigramFilterFactory
</description><key id="118974135">15033</key><summary>Wrong properties is used in document.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yokotaso</reporter><labels><label>docs</label></labels><created>2015-11-26T05:24:30Z</created><updated>2015-11-27T13:13:48Z</updated><resolved>2015-11-27T13:13:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-27T13:13:48Z" id="160137127">thanks @yokotaso 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>problems with Marvel plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15032</link><project id="" key="" /><description>Hi,

i've problems with marvel and reverse proxy.

My apache conf for this terms:

  ProxyPass /app/marvel  http://*********_:5601/app/marvel
  ProxyPassReverse /app/marvel http://**_*******:5601/app/marvel

  ProxyPass http://*******_:5601/api/marvel
  ProxyPassReverse http://**_*****:5601/api/marvel

I see the cluster info correct:

![image](https://cloud.githubusercontent.com/assets/13537268/11414645/1c461fec-93f8-11e5-9676-1017a166366d.png)

But the nodes or other options give me errors:

![image](https://cloud.githubusercontent.com/assets/13537268/11414714/e94488a8-93f8-11e5-8ca6-d518ac3c71e2.png)

I try with the rewrite options but i did not lucky, any ideas?

Thx for the support
</description><key id="118966498">15032</key><summary>problems with Marvel plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dfaropennetwork</reporter><labels /><created>2015-11-26T03:46:30Z</created><updated>2015-11-26T23:31:02Z</updated><resolved>2015-11-26T09:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-11-26T09:22:04Z" id="159855902">Hi,

I saw you already asked your question on https://discuss.elastic.co/ [here](https://discuss.elastic.co/t/problems-with-apache-and-marvel-plugin/35623), which is the right place for such questions. We try to keep Github issues for real issues. 

Please feel free to reopen (or I will) if it ends up being a real bug.
</comment><comment author="uboness" created="2015-11-26T09:26:22Z" id="159856640">https://discuss.elastic.co/c/marvel is the right place for this.. also for bugs
</comment><comment author="dfaropennetwork" created="2015-11-26T23:31:02Z" id="160004525">Ah ok,  sorry tlex :D
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix deb and rpm tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15031</link><project id="" key="" /><description>Gradle is causing so much trouble here! Its too cute for its own good.
</description><key id="118947395">15031</key><summary>Fix deb and rpm tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-26T00:29:17Z</created><updated>2015-11-29T01:11:54Z</updated><resolved>2015-11-29T01:10:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-26T00:31:27Z" id="159765483">@bleskes, @imotov, @rjernst, this should fix the deb and rpm package tests.

I haven't been too careful with it because I'm angry at it now so it might be dangerous to merge as is. But it gets them passing on my mac. After I `brew install rpm` and `brew install dpkg`. It'll probably break everyone who has yet to install those.
</comment><comment author="rjernst" created="2015-11-26T00:55:43Z" id="159769206">This generally looks good. I also think we should add configuration to skip if dpkg or rpm is not available? I'm personally unsure if we really need these integration tests long term since we are doing this all in bats tests right? I think we could make a separate module to run the rest tests (just against the zip). But that is separate from this work, the changes I noted should make this work without doing extra work at configuration (which would slow down all commands, not just when running these tests).
</comment><comment author="rjernst" created="2015-11-26T00:56:48Z" id="159769314">&gt; I also think we should add configuration to skip if dpkg or rpm is not available?

Or at least we should fail early (not at execution time) and tell the user they need to install dpkg and rpm.
</comment><comment author="nik9000" created="2015-11-26T15:57:50Z" id="159947437">&gt; Or at least we should fail early (not at execution time) and tell the user they need to install dpkg and rpm.

We skipped the tests in maven days so I'm skipping them in gradle. We can fail early if we'd prefer but I don't think that is needed. Its important that these tests run in CI but not all developer machines need to run them. It looks like the deb ones were always skipped on mac because they didn't check for homebrew's rpm location in 2.x. I fixed that, but that never caused an issue so I suspect we're ok here.
</comment><comment author="nik9000" created="2015-11-26T15:59:08Z" id="159948196">&gt; We skipped the tests in maven days so I'm skipping them in gradle.

I should be clear here: I've pushed a new commit to this PR that skips them if the tools around available. It uses file exists checks rather than `which` style checks because they fit more easily into gradle's model and they were what we did in maven anyway.
</comment><comment author="nik9000" created="2015-11-28T14:45:55Z" id="160306694">@rjernst, I pushed a new commit. I dropped the TODO around the test and left a comment instead. I fixed the toStrings.
</comment><comment author="rjernst" created="2015-11-28T19:47:18Z" id="160331893">This looks good, but we should wait for my PR to fix the artifactId for deb and rpm, so that the change here to remove packaging from the distro dependency can be undone.
</comment><comment author="nik9000" created="2015-11-29T00:46:22Z" id="160352683">Pushed another revision that cleans up some more things including undoing removing the file types. Also rebased to make that work.
</comment><comment author="rjernst" created="2015-11-29T01:03:27Z" id="160353160">Looks good, thanks @nik9000!
</comment><comment author="nik9000" created="2015-11-29T01:11:54Z" id="160353404">&gt; Looks good, thanks @nik9000!

Thanks so much for the reviews! I still feel pretty bumbling around gradle/groovy but I'm more and more comfortable and I think its important that there are a few folks with experience with it. One day I will be good with it. One day.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indexed script with terms aggregation not working.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15030</link><project id="" key="" /><description>hi peoble, i` m have problems with terms aggregation in scripts, indexed scripts..

Here is a ES website link about this use:

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-script

It is saying that all script types are accepted, and shows an example for inline and file script. Under the file example it explains if want use indexed script need change key "file" in json by "id" key.

inline example

``` javascript
{
    "aggs" : {
        "genders" : {
            "terms" : {
                "script" : "doc['gender'].value"
            }
        }
    }
}
```

file example

``` javascript
{
    "aggs" : {
        "genders" : {
            "terms" : {
                "script" : {
                    "file": "my_script",
                    "params": {
                        "field": "gender"
                    }
                }
            }
        }
    }
}
```

and tip under file example in website explains if must need change key "file" in json by "id" key to use indexed script.

ok...all fine here.

i have a groovy indexed script named "test_field", for test.
it code is

``` groovy
 return "test field";
```

well...then not are any doubt about script error becouse the simplicity of it.

when in a query i use this script in script_fields it works perfect

``` javascript
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "seller_id": 1
          }
        }
      ]
    }
  },
  "script_fields": {
    "statuses": {
      "id": "test_field",
      "params": {
        "concurrent_ids": [
          1,
          2,
          3,
          4
        ]
      }
    }
  }
}
```

when i use the same script in aggs how describe in use of indexed scripts

``` javascript
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "seller_id": 1
          }
        }
      ]
    }
  },
  "aggs": {
    "statuses": {
      "terms": {
        "script": {
          "id": "test_field",
          "params": {
            "concurrent_ids": [
              1,
              2,
              3,
              4
            ]
          }
        }
      }
    }
  }
}
```

receive the error: 

``` javascript
{
  "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[5j_srZt-TDatRuMytEMuQA][4ninjas_products_development][0]: SearchParseException[[4ninjas_products_development][0]: query[+ConstantScore(seller_id: \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"term\": {\n            \"seller_id\": 1\n          }\n        }\n      ]\n    }\n  },\n  \"aggs\": {\n    \"statuses\": {\n      \"terms\": {\n        \"script\": {\n          \"id\": \"test_field\",\n          \"params\": {\n            \"concurrent_ids\": [\n              1,\n              2,\n              3,\n              4\n            ]\n          }\n        }\n      }\n    }\n  }\n}]]]; nested: SearchParseException[[4ninjas_products_development][0]: query[+ConstantScore(seller_id: \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0001)],from[-1],size[-1]: Parse Failure [Unknown key for a START_OBJECT in [statuses]: [script].]]; }]",
  "status": 400
}
```

when i change to inline script

``` javascript
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "seller_id": 1
          }
        }
      ]
    }
  },
  "aggs": {
    "statuses": {
      "terms": {
        "script": "return \"test status\";",
        "params": {
          "concurrent_ids": [
            1,
            2,
            3,
            4
          ]
        }
      }
    }
  }
}
```

it works perfect...

what gone wrong ?
- Indexed scripts is activated
- sandboad for indexed scripts is "off"
- Indexed scripts works perfect in filters and script_fields
- in aggregation scripts using "file" make the same error too... 

the version of Elasticseach is:

``` javascript
version: {
    number: "1.7.1",
    build_hash: "b88f43fc40b0bcd7f173a1f9ee2e97816de80b19",
    build_timestamp: "2015-07-29T09:54:16Z",
    build_snapshot: false,
    lucene_version: "4.10.4"
}
```

I search in internet, google, discus...and no find any similar example. Can be a bug. Please can you help me if not ? I had much work to explain exactly the point end formating this question me better display, and better understanding in github format.
</description><key id="118946603">15030</key><summary>indexed script with terms aggregation not working.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luizamboni</reporter><labels /><created>2015-11-26T00:19:42Z</created><updated>2016-11-05T14:21:36Z</updated><resolved>2015-11-28T17:46:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="luizamboni" created="2015-11-26T01:51:22Z" id="159778090">I search about this a little more, it is a problem of diferente versions with diferente APIs....

https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-aggregations-bucket-terms-aggregation.html

i use the instructions here and works... 
"god ! give me patience, becouse if you give me power, i will broke alll"
</comment><comment author="thomasmodeneis" created="2016-11-04T19:52:34Z" id="258531272">Always bad documentation + bad examples. With breaking changes every major release (5.0). Here we go on Elasticsearch purgatory &#128078; 
</comment><comment author="clintongormley" created="2016-11-05T14:21:36Z" id="258614586">@thomasmodeneis thanks for the encouraging words.  I notice that you haven't sent any PRs to improve the situation https://github.com/elastic/elasticsearch/pulls/thomasmodeneis

This is an open source project, feel free to get involved to improve the experience for all.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Plugin.onIndexService.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15029</link><project id="" key="" /><description>Now that field mappers are registered at the node level (#14896), this method
is not necessary anymore.
</description><key id="118897776">15029</key><summary>Remove Plugin.onIndexService.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T18:35:58Z</created><updated>2015-12-03T16:07:18Z</updated><resolved>2015-12-03T08:46:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-25T19:13:17Z" id="159704512">I don't think this is needed for 2.2 this doesn't exist on the 2.x branch
</comment><comment author="rjernst" created="2015-11-26T01:01:10Z" id="159769850">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write log entries of external nodes to console</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15028</link><project id="" key="" /><description>Before external nodes wrote logs only to a file which makes it tricky to debug
test failures.

As an alternative to this we also discussed to 
- set `inheritIO()` on the process that starts es (this actually prints the log to console too) and try to get the pid and port from the log file. We would then have to make sure than that each node name is only used once in each test run because otherwise we would have several pids and ports in each log file and would have to chose.
- set `inheritIO()` and write the pid to a file and then read pid from the file get port from the cluster.

This here just seemed most convenient to implement.
</description><key id="118891338">15028</key><summary>Write log entries of external nodes to console</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-11-25T17:57:14Z</created><updated>2015-12-10T13:22:51Z</updated><resolved>2015-12-10T13:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-12-07T18:29:59Z" id="162615893">This seems messy and after a discussion with @nik9000 I decided to try the `inheritIO` anyway in #15290. I'll close this here once we made  decision. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable rpm and deb integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15027</link><project id="" key="" /><description>They are broken and we didn't know it.
</description><key id="118890382">15027</key><summary>Disable rpm and deb integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T17:50:55Z</created><updated>2015-11-25T18:11:49Z</updated><resolved>2015-11-25T18:11:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-25T17:51:00Z" id="159686524">@rjernst  for you!
</comment><comment author="rjernst" created="2015-11-25T18:06:56Z" id="159690108">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Fix toString() in GeoDistanceRangeQuery and GeoPolygonQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15026</link><project id="" key="" /><description>Minor typo in the two queries toString() method. They were former filters the toString() seems to have been forgotten while renaming.
</description><key id="118889401">15026</key><summary>Geo: Fix toString() in GeoDistanceRangeQuery and GeoPolygonQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T17:44:44Z</created><updated>2015-11-25T20:58:33Z</updated><resolved>2015-11-25T20:53:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-25T18:41:27Z" id="159697538">LGTM
</comment><comment author="cbuescher" created="2015-11-25T20:58:33Z" id="159730137">Also pushed to 2.x (3787baf) and 2.2 (5b8cfeb)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some ingest cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15025</link><project id="" key="" /><description>- Inlined PipelineStoreClient class into the PipelineStore class
- Moved PipelineReference to a top level class and named it PipelineDefinition
- Pulled some logic from the crud transport classes to the PipelineStore
- Use IOUtils#close(...) where appropriate

Based on the review #15007 and https://github.com/s1monw/elasticsearch/commit/042ef8c20476459f92ea1c2ea12bc1950211e693
</description><key id="118886916">15025</key><summary>Some ingest cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-25T17:30:24Z</created><updated>2015-11-26T13:47:03Z</updated><resolved>2015-11-26T13:46:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-25T17:50:22Z" id="159686392">I like it, left some comments but looks good
</comment><comment author="martijnvg" created="2015-11-26T11:37:52Z" id="159891127">@javanna I've updated the PR and make use of Writable instead of Streamable and moved the scroll search iterator to core as helper class.
</comment><comment author="javanna" created="2015-11-26T12:17:50Z" id="159899050">left a super small comment on testing, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simulate api improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15024</link><project id="" key="" /><description>Move ParsedSimulateRequest to SimulatePipelineRequest and remove Parser class in favor of static parse methods.
Simplified execute methods in SimulateExecutionService.
</description><key id="118881285">15024</key><summary>Simulate api improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-25T17:04:27Z</created><updated>2015-11-26T12:27:45Z</updated><resolved>2015-11-26T12:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-26T12:22:03Z" id="159899660">LGTM
</comment><comment author="javanna" created="2015-11-26T12:27:45Z" id="159900618">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use general cluster state batching mechanism for shard started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15023</link><project id="" key="" /><description>This commit modifies the handling of shard started cluster state updates
to use the general cluster state batching mechanism. An advantage of
this approach is we now get correct per-listener notification on
failures.

Relates #14899, relates #14725
</description><key id="118875144">15023</key><summary>Use general cluster state batching mechanism for shard started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T16:36:18Z</created><updated>2015-12-14T22:35:20Z</updated><resolved>2015-12-03T19:10:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-25T16:37:53Z" id="159665596">@bleskes I'll rebase this pull request on master when #14899 is reintegrated there. The salient commit for this review is thus 8f7194d52650180c08d3a9e32acc26e47f6061f7 pending #14899 (all the changes for that commit are in [ShardStateAction.java](https://github.com/elastic/elasticsearch/pull/15023/files#diff-3a4f751f360271846c44772fb64294b1L20) and some minor modifications in [AllocationService.java](https://github.com/elastic/elasticsearch/pull/15023/files#diff-792ac9952eb61524ecac9d045d6eae4bL25)).
</comment><comment author="jasontedor" created="2015-11-26T14:56:10Z" id="159932641">@bleskes I've rebased this pull request on the latest changes in #14899.
</comment><comment author="jasontedor" created="2015-12-01T15:28:39Z" id="161001559">@bleskes This pull request has been rebased on master since #14899 has been integrated there.
</comment><comment author="bleskes" created="2015-12-03T15:04:05Z" id="161667725">LGTM. Left a more general comment about how we report failures with the new batching.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>when filtering after a 'terms_stats' sort query isnt applied v1.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15022</link><project id="" key="" /><description>Hi All

I am having an issue with using terms_stats in Kibana, I am not on current version due to problems with the new Kibana interface. 

This is the first time I have come across this, I am trying to get back a very small section of data however only the date filters seem to get applied before the data is returned. this means a large response is returned but the data is null. I have tried changing "should" to "Must" and it still doesnt work.

I have tried

Has this already been addressed?

does anyone know a fix for this ?

Many thanks 

Barry

{
    "facets": {
        "terms": {
            "terms_stats": {
                "value_field": "AverageSeconds.raw",
                "key_field": "Machine.raw",
                "size": 2,
                "order": "term"
            },
            "facet_filter": {
                "fquery": {
                    "query": {
                        "filtered": {
                            "query": {
                                "bool": {
                                    "should": [{
                                        "query_string": {
                                            "query": "type:S_Agent AND SourceName.raw:S_Averages"
                                        }
                                    }]
                                }
                            },
                            "filter": {
                                "bool": {
                                    "must": [{
                                        "range": {
                                            "@timestamp": {
                                                "from": 1447860850443,
                                                "to": 1448465650443
                                            }
                                        }
                                    }]
                                }
                            }
                        }
                    }
                }
            }
        }
    },
    "size": 0
}'
</description><key id="118871227">15022</key><summary>when filtering after a 'terms_stats' sort query isnt applied v1.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BarryGr</reporter><labels /><created>2015-11-25T16:16:52Z</created><updated>2015-11-28T17:30:33Z</updated><resolved>2015-11-28T17:30:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T17:30:33Z" id="160322790">Hi @BarryGr 

Facets have been removed.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.1 - Failed to delete temp translog file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15021</link><project id="" key="" /><description>After upgrading to 2.1, when starting my 1-node dev cluster, I often see the error below from each index that had recent writes before the previous graceful (ctrl-c) shutdown. It seems that these files really do not exist. 

I have Marvel installed, but the issue occurs for non-marvel indexes as well. Here's the log from my most recent start. 

```
[2015-11-25 10:09:00,073][INFO ][node                     ] [Cyclone] version[2.1.0], pid[21953], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-11-25 10:09:00,074][INFO ][node                     ] [Cyclone] initializing ...
[2015-11-25 10:09:00,416][INFO ][plugins                  ] [Cyclone] loaded [marvel-agent, license], sites []
[2015-11-25 10:09:00,443][INFO ][env                      ] [Cyclone] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [81.9gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2015-11-25 10:09:03,358][INFO ][node                     ] [Cyclone] initialized
[2015-11-25 10:09:03,358][INFO ][node                     ] [Cyclone] starting ...
[2015-11-25 10:09:03,687][INFO ][transport                ] [Cyclone] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[fe80::1]:9300}, {[::1]:9300}
[2015-11-25 10:09:03,701][INFO ][discovery                ] [Cyclone] elasticsearch/FEmp7CE8QNuHT_0JujYmrQ
[2015-11-25 10:09:06,743][INFO ][cluster.service          ] [Cyclone] new_master {Cyclone}{FEmp7CE8QNuHT_0JujYmrQ}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-11-25 10:09:06,763][INFO ][http                     ] [Cyclone] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[fe80::1]:9200}, {[::1]:9200}
[2015-11-25 10:09:06,764][INFO ][node                     ] [Cyclone] started
[2015-11-25 10:09:07,088][INFO ][license.plugin.core      ] [Cyclone] license [cf52acc6-1817-4e1c-b97b-0170927a54b2] - valid
[2015-11-25 10:09:07,094][ERROR][license.plugin.core      ] [Cyclone] 
#
# License will expire on [Friday, December 25, 2015]. If you have a new license, please update it.
# Otherwise, please reach out to your support contact.
# 
# Commercial plugins operate with reduced functionality on license expiration:
# - marvel
#  - The agent will stop collecting cluster and indices metrics
[2015-11-25 10:09:07,169][INFO ][gateway                  ] [Cyclone] recovered [3] indices into cluster_state
[2015-11-25 10:09:07,827][WARN ][index.translog           ] [Cyclone] [.marvel-es-data][0] failed to delete temp file /skearns/es/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/.marvel-es-data/0/translog/translog-5891737687262078999.tlog
java.nio.file.NoSuchFileException: /skearns/es/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/.marvel-es-data/0/translog/translog-5891737687262078999.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1079)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-25 10:09:07,838][WARN ][index.translog           ] [Cyclone] [.kibana][0] failed to delete temp file /skearns/es/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/.kibana/0/translog/translog-222310247107863220.tlog
java.nio.file.NoSuchFileException: /skearns/es/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/.kibana/0/translog/translog-222310247107863220.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1079)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-25 10:09:07,919][WARN ][index.translog           ] [Cyclone] [.marvel-es-2015.11.25][0] failed to delete temp file /skearns/es/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/.marvel-es-2015.11.25/0/translog/translog-6094222780037808919.tlog
java.nio.file.NoSuchFileException: /skearns/es/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/.marvel-es-2015.11.25/0/translog/translog-6094222780037808919.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1079)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

```
</description><key id="118860031">15021</key><summary>ES 2.1 - Failed to delete temp translog file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skearns64</reporter><labels /><created>2015-11-25T15:28:07Z</created><updated>2015-11-25T15:34:19Z</updated><resolved>2015-11-25T15:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-25T15:33:08Z" id="159642414">Duplicates #14989, fixed in #14872.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace property with field in IngestDocument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15020</link><project id="" key="" /><description>getPropertyValue =&gt; getFieldValue
hasPropertyValue =&gt; hasFieldValue
setPropertyValue =&gt; setFieldValue
removeProperty =&gt; removeField

Also use MetaData enum for metadata field names in IngestDocument.
</description><key id="118856239">15020</key><summary>Replace property with field in IngestDocument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-25T15:09:29Z</created><updated>2015-11-25T17:12:55Z</updated><resolved>2015-11-25T17:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-25T16:07:26Z" id="159656339">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date formats: use a function instead of our own interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15019</link><project id="" key="" /><description>Also turn the different date formats into an enum.
</description><key id="118850504">15019</key><summary>date formats: use a function instead of our own interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-25T14:42:03Z</created><updated>2015-11-25T15:17:39Z</updated><resolved>2015-11-25T15:17:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-25T15:10:45Z" id="159636133">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle daemon is a demon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15018</link><project id="" key="" /><description>This commit adds a property that will prevent the Gradle daemon from
being used for builds (even if one is running). This is to avoid some
nasty issues (e.g., SIGBUS faults and other mmap disasters) that result
from class loaders not being closed properly.
</description><key id="118848730">15018</key><summary>Gradle daemon is a demon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T14:34:03Z</created><updated>2016-01-13T13:55:53Z</updated><resolved>2015-11-25T15:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-25T14:49:13Z" id="159630035">Fine by me. I don't use it anyway because it eats characters. Maybe its worth filing issues upstream?
</comment><comment author="uschindler" created="2015-11-25T14:56:35Z" id="159632032">+1 :-)
</comment><comment author="rmuir" created="2015-11-25T14:59:35Z" id="159633085">+1
</comment><comment author="dakrone" created="2015-11-25T15:12:17Z" id="159636547">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refuse to load fields from _source when using the `fields` option and support wildcards. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15017</link><project id="" key="" /><description>Fixes #14489  
  Do not to load fields from _source when using the `fields` option.
  Non stored (non existing) fields are ignored by the fields visitor when using the `fields` option.

Fixes #10783
  Support \* wildcard to retrieve stored fields when using the `fields` option.
  Supported pattern styles are "xxx_", "_xxx", "_xxx_" and "xxx*yyy".
</description><key id="118841903">15017</key><summary>Refuse to load fields from _source when using the `fields` option and support wildcards. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T13:59:58Z</created><updated>2015-11-30T17:58:26Z</updated><resolved>2015-11-30T10:01:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-25T17:54:50Z" id="159687327">Thanks Jim, I'm happily surprised that there were not too many tests using the `fields` option to get fields from the `_source`! Could you add a note to `docs/reference/migration/migrate_3_0.asciidoc` to let users know that in 3.0 the `fields` option won't be able to load fields from `_source` anymore?
</comment><comment author="jpountz" created="2015-11-26T18:02:31Z" id="159970293">Thanks @jimferenczi this looks good to me. I just left a comment about two REST tests that I think we should remove. @bleskes would you mind having a quick look at this PR before it gets merged?
</comment><comment author="bleskes" created="2015-11-26T20:50:37Z" id="159992787">This looks good to me. Thanks @jimferenczi . My only question is whether we should resolve wild cards in the field visitor. Feels more natural for me to resolve it based on the mappings and get a concrete list to the visitor. I think we have the support we need in the MapperService (or will be easy to build). I'm worried that a wild card on the lucene level may expose system/hidden fields users didn't mean.. I don't think it's a problem in practice though. @jpountz wdyt?
</comment><comment author="jpountz" created="2015-11-27T08:07:50Z" id="160064696">@bleskes Good question. We already support fields=\* in master (see mentions of `loadAllStoredFields` in the code) and I'm not aware of problems, so I _think_ we should be fine.
</comment><comment author="jpountz" created="2015-11-30T09:02:15Z" id="160561483">@jimferenczi Let's merge this one?
</comment><comment author="clintongormley" created="2015-11-30T11:11:09Z" id="160600259">@jimferenczi please add version labels for each branch that you merged this PR into
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use general cluster state batching mechanism for shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15016</link><project id="" key="" /><description>This commit modifies the handling of shard failure cluster state updates
to use the general cluster state batching mechanism. An advantage of
this approach is we now get correct per-listener notification on
failures.

Relates #14899, relates #14725
</description><key id="118841759">15016</key><summary>Use general cluster state batching mechanism for shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T13:59:03Z</created><updated>2015-12-14T22:36:06Z</updated><resolved>2015-12-03T19:02:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-25T14:01:21Z" id="159615135">@bleskes I'll rebase this pull request on master when #14899 is reintegrated there. The salient commit for this review is thus ccc89c3666780d0bf7b2425f9e8c76dbe6316187 pending #14899 (all the changes for that commit are in [ShardStateAction.java](https://github.com/elastic/elasticsearch/pull/15016/files#diff-3a4f751f360271846c44772fb64294b1L20) and some minor modifications in [AllocationService.java](https://github.com/elastic/elasticsearch/pull/15016/files#diff-792ac9952eb61524ecac9d045d6eae4bL104)).
</comment><comment author="jasontedor" created="2015-11-26T14:31:54Z" id="159928224">@bleskes I've rebased this pull request on the latest changes in #14899.
</comment><comment author="jasontedor" created="2015-12-01T15:20:44Z" id="160999200">@bleskes This pull request has been rebased on master since #14899 has been integrated there.
</comment><comment author="bleskes" created="2015-12-01T16:19:59Z" id="161019351">looking good. Left some comments.
</comment><comment author="bleskes" created="2015-12-03T14:57:04Z" id="161663954">LGTM. Left trivial comments.
</comment><comment author="bleskes" created="2015-12-03T19:10:57Z" id="161750475">One note - the comment from the shard started pr about error reporting holds for this one as well. Is the plan to do a followup? As with the shard started, we can just let the exception bubble up
</comment><comment author="jasontedor" created="2015-12-03T19:14:17Z" id="161751299">&gt;  Is the plan to do a followup?

@bleskes Yeah. :)
</comment><comment author="jasontedor" created="2015-12-14T22:36:05Z" id="164581903">&gt; &gt; Is the plan to do a followup?
&gt; 
&gt; Yeah. :)

I opened #15428.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Cumulative Sum Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15015</link><project id="" key="" /><description /><key id="118837088">15015</key><summary>Aggregations Refactor: Refactor Cumulative Sum Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-25T13:31:13Z</created><updated>2015-11-26T09:51:50Z</updated><resolved>2015-11-26T09:33:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-25T19:14:15Z" id="159704717">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Bucket Script Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15014</link><project id="" key="" /><description /><key id="118834238">15014</key><summary>Aggregations Refactor: Refactor Bucket Script Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-25T13:14:40Z</created><updated>2015-12-07T10:29:00Z</updated><resolved>2015-12-07T10:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-30T14:41:27Z" id="160648883">@jpountz updated with your comments
</comment><comment author="jpountz" created="2015-12-03T18:07:35Z" id="161734681">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Add Kubernetes discovery community plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15013</link><project id="" key="" /><description /><key id="118832790">15013</key><summary>Docs: Add Kubernetes discovery community plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmidyson</reporter><labels><label>docs</label></labels><created>2015-11-25T13:05:55Z</created><updated>2015-11-25T15:26:42Z</updated><resolved>2015-11-25T15:26:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-25T15:26:42Z" id="159640623">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent writing to closed channel if translog is already closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15012</link><project id="" key="" /><description>We handle AlreadyClosedExceptions gracefully wherever IndexShard / Engine
is used. In some cases, instead of throwing the appropriate exception we
bubble up ChannelClosedException instead which causes shard failures etc.
Today, it seems like that this can only happen if the engine is closed without
acquireing the lock which means that the shard has failed already so the impact is really
just a confusing log message. Yet, this change enforces throwing the right exception
if the translog is already closed.

Closes #14866
</description><key id="118832720">15012</key><summary>Prevent writing to closed channel if translog is already closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>PITA</label><label>review</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T13:05:30Z</created><updated>2015-11-26T15:51:11Z</updated><resolved>2015-11-26T13:47:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-25T13:47:45Z" id="159612232">LGTM
</comment><comment author="s1monw" created="2015-11-26T09:16:55Z" id="159855068">@bleskes I reviewed all the code again and moved ensureOpen calls under lock where necessary / possible
</comment><comment author="bleskes" created="2015-11-26T10:33:36Z" id="159871939">awesome. Thanks @s1monw .
</comment><comment author="bleskes" created="2015-11-26T13:02:44Z" id="159907293">i.e., LGTM^2
</comment><comment author="s1monw" created="2015-11-26T15:51:11Z" id="159945844">I backported this to 2.0.2, 2.1.1, and 2.2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Polish doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15011</link><project id="" key="" /><description /><key id="118825897">15011</key><summary>Polish doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">izeye</reporter><labels><label>docs</label></labels><created>2015-11-25T12:24:36Z</created><updated>2015-11-28T17:26:15Z</updated><resolved>2015-11-28T17:26:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T17:25:57Z" id="160322451">thanks @izeye - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make remaining ShapeBuilders implement Writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15010</link><project id="" key="" /><description>This is the second part of making ShapeBuilders implement Writeable.

This PR add serialization, equality and hashCode to all remaining ShapeBuilders that don't implement it yet. Also adds writing and reading ShapeBuilders to streams via NamedWriteable and adds registering the ShapeBuilders with the NamedWriteableRegistry. In addition to this, switched GeoShapeQueryBuilder from representing shape as byte array (parsed later) to ShapeBuilders internally, so now the whole query can be parsed and streamed as Writeable. 

Relates to #14416
</description><key id="118824853">15010</key><summary>Make remaining ShapeBuilders implement Writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T12:17:23Z</created><updated>2015-12-18T10:11:34Z</updated><resolved>2015-12-18T10:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-25T12:24:54Z" id="159592488">@nknize this is the follow up to #14933, there are some norelease comment in here that will vanish once #14969 is in. 

I have an additional question regarding the use of "orientation" in GeometryCollection. So far the parser seems to disregard the "orientation" parameter for GeometryCollection. If it should use it, the next question would be if it should be passed along to all the shapes in the collection that make use of it (only Polygon, MultiPolygon and Envelope as far as I can tell, but correct me if I'm wrong)
</comment><comment author="cbuescher" created="2015-12-04T19:41:09Z" id="162062860">@nknize after talking we decided that really only PolygonBuilder seems in need of the `orientation` parameter, since it is the only shape that actually seems to regard it when building the Geometry, so we decided to remove the orientation field from ShapeBuilder and move it down to PolygonBuilder only. 

A MultiPolygon specified as xContent will still be able to utilize the "orientation" setting, during parsing this will still be passed on to all the contained polygons.

However, I'm still unsure if MultiPolygonBuilder should have the "orientation" field itself, otherwise people using the Java-API to generate REST requests will not be able to render a MultiPolygon shape as xContent with this option. 

On the other hand, when using MultiPolygonBuilder and adding PolygonBuilder instances with different orientation than the MultiPolygonBuilder itself, those "orientation" settings will not be rendered to xContent (they will all be rendered to one big `coordinates` array), so after parsing the shape back this information is lost. Ideally we would be able to somehow make sure that when a PolygonBuilder is added via MultiPolygonBuilder#polygon(), either we reject polygons that don't match the MultiPolygonBuilders `orientation` e.g. via exception or we silently need to change that `orientation` setting for the polygon to the setting of the multipolygon it gets added to (and document that properly). I'd be in favour of the first approach (throw error), which would only happen for users of the Java-API. 
@nknize I'm undecided here, wdyt?
</comment><comment author="cbuescher" created="2015-12-07T17:16:58Z" id="162597392">@nknize I removed the `orientation` field from EnvelopeBuilder and GeometryCollectionBuilder, so only PolygonBuilder and MultiPolygonBuilder have constructors accepting it and render it to xContent now. There's still the problem that you can add polygons with different orientations to MultiPolygonBuilder via Java-API, which won't survive a toXContent/parsing roundtrip. At the moment I think it's better to apply the orientation setting of the parent MultiPolygon to the polis added to the builder, since that reflects what happens in GeoJSON when adding coordinate arrays to a MultiPolygon.
</comment><comment author="nknize" created="2015-12-08T15:43:15Z" id="162922059">@cbuescher that sounds like a plan. The more I've been thinking about this the more I'm leaning towards deprecating `orientation` altogether. It was originally added to handle the plethora of ambiguity issues by adding strict compliance with the OGC specification, but enough leniency to minimize integration disruptions. As we've seen other places leniency tends to be a source of evil - and the spec strictly requires the right-hand rule. I think 3.0 would be a good time to remove the crutch.

Lets keep as is for now and I'll open a separate issue to discuss deprecating the parameter. I'll also finish reviewing this PR today.
</comment><comment author="nknize" created="2015-12-14T22:58:34Z" id="164587143">I love the added testing!! Orientation removal and refactor LGTM. I really only have 2 concerns that I think we can probably address in separate PR(s)? 
</comment><comment author="cbuescher" created="2015-12-15T09:33:10Z" id="164699770">@nknize thanks for the review, I looked into your comments, left some questions there and added a test checking that an empty GeometryCollectionBuilder is valid in the parser and in the real request. I'm going ahead and merge this in a while if you have no further objections to my replies.
</comment><comment author="cbuescher" created="2015-12-17T18:00:45Z" id="165532136">@nknize I added additional checks to ensure presence of jts and spatial4j jars when registering geo-related parsers and shapes, so it does not throw errors on nodes where those libraries are not present. Could you have another quick look just at that additional commit before I merge? Will open another follow-up PR for adding the validation to the shape builders as we discussed yesterday.
</comment><comment author="nknize" created="2015-12-17T23:01:28Z" id="165609600">Wonderful! Thanks for creating the separate issues. This LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Avg Bucket, Min Bucket, Max Bucket, Sum Bucket, Percentiles Bucket, Stats Bucket and Extended Stats Bucket Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15009</link><project id="" key="" /><description /><key id="118818780">15009</key><summary>Aggregations Refactor: Refactor Avg Bucket, Min Bucket, Max Bucket, Sum Bucket, Percentiles Bucket, Stats Bucket and Extended Stats Bucket Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-25T11:41:43Z</created><updated>2015-12-01T11:44:29Z</updated><resolved>2015-12-01T11:44:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-30T18:20:57Z" id="160713530">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Children Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15008</link><project id="" key="" /><description /><key id="118801880">15008</key><summary>Aggregations Refactor: Refactor Children Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-25T10:08:41Z</created><updated>2015-11-26T09:51:47Z</updated><resolved>2015-11-26T09:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-25T19:16:52Z" id="159705318">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>code review</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15007</link><project id="" key="" /><description /><key id="118801072">15007</key><summary>code review</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Ingest</label></labels><created>2015-11-25T10:05:00Z</created><updated>2015-12-01T16:57:02Z</updated><resolved>2015-12-01T16:57:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-11-25T10:19:35Z" id="159562752">@martijnvg @javanna 
</comment><comment author="martijnvg" created="2015-12-01T16:57:02Z" id="161030419">All the comments in this review have been addressed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Nested and Reverse Nested Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15006</link><project id="" key="" /><description /><key id="118798825">15006</key><summary>Aggregations Refactor: Refactor Nested and Reverse Nested Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-25T09:54:17Z</created><updated>2015-11-26T09:51:44Z</updated><resolved>2015-11-26T09:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-25T11:49:59Z" id="159584653">Looks good to me, left one question but maybe @jpountz should also have a final look.
</comment><comment author="jpountz" created="2015-11-25T19:17:48Z" id="159705509">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>aggregation filter does not work for child aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15005</link><project id="" key="" /><description>Hi, 
   I have a parent-child structure index, where I want to do filtering on child type documents while doing child-aggregation. 
   For example, the child type has a field of date, and I want to set the range of the date field in the child aggregation.
   Is that possible even in the latest version (2.0)?
   Thank you guies very much.
</description><key id="118798692">15005</key><summary>aggregation filter does not work for child aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuralf</reporter><labels><label>:Aggregations</label><label>:Parent/Child</label><label>bug</label><label>feedback_needed</label></labels><created>2015-11-25T09:53:41Z</created><updated>2016-02-14T17:57:08Z</updated><resolved>2016-02-14T17:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-25T09:56:26Z" id="159555344">Can you post a recreation of what you are doing today and that doesn't work?
</comment><comment author="saai" created="2015-11-25T10:05:40Z" id="159558788">hi jpountz,  I am xuralf's colleague, and here is the problem.

the querying json is as followed:

```
{
  "query" : {
    "filtered" : {
      "query" : {
        "bool" : {
          "must" : {
            "has_child" : {
              "query" : {
                "bool" : { }
              },
              "child_type" : "hotel-level-lose-stat"
            }
          }
        }
      },
      "filter" : {
        "bool" : {
          "must" : {
            "has_child" : {
              "filter" : {
                "bool" : {
                  "must" : {
                    "range" : {
                      "report_date" : {
                        "from" : "2015-11-01",
                        "to" : "2015-11-03",
                        "include_lower" : true,
                        "include_upper" : true
                      }
                    }
                  }
                }
              },
              "child_type" : "hotel-level-lose-stat"
            }
          }
        }
      }
    }
  },
  "aggregations" : {
    "sales_center" : {
      "terms" : {
        "field" : "sales_center",
        "size" : 0
      },
      "aggregations" : {
        "region" : {
          "terms" : {
            "field" : "region",
            "size" : 0
          },
          "aggregations" : {
            "sales_area" : {
              "terms" : {
                "field" : "sales_area",
                "size" : 0
              },
              "aggregations" : {
                "secondary_area" : {
                  "terms" : {
                    "field" : "secondary_area",
                    "size" : 0
                  },
                  "aggregations" : {
                    "city_name.raw" : {
                      "terms" : {
                        "field" : "city_name.raw",
                        "size" : 0
                      },
                      "aggregations" : {
                        "manager_name.raw" : {
                          "terms" : {
                            "field" : "manager_name.raw",
                            "size" : 0
                          },
                          "aggregations" : {
                            "trading_area.raw" : {
                              "terms" : {
                                "field" : "trading_area.raw",
                                "size" : 0
                              },
                              "aggregations" : {
                                "competitor_price" : {
                                  "children" : {
                                    "type" : "hotel-level-lose-stat"
                                  },
                                  "aggregations" : {
                                    "competitor_price" : {
                                      "date_histogram" : {
                                        "field" : "report_date",
                                        "interval" : "1d",
                                        "min_doc_count" : 0,
                                        "format" : "yyyy-MM-dd",
                                        "extended_bounds" : {
                                          "min" : "2015-11-01",
                                          "max" : "2015-11-03"
                                        }
                                      },
                                      "aggregations" : {
                                        "competitor_price" : {
                                          "filter" : {
                                            "exists" : {
                                              "field" : "competitor_price"
                                            }
                                          }
                                        }
                                      }
                                    }
                                  }
                                },
                                "lose_result" : {
                                  "children" : {
                                    "type" : "hotel-level-lose-stat"
                                  },
                                  "aggregations" : {
                                    "lose_result" : {
                                      "date_histogram" : {
                                        "field" : "report_date",
                                        "interval" : "1d",
                                        "min_doc_count" : 0,
                                        "format" : "yyyy-MM-dd",
                                        "extended_bounds" : {
                                          "min" : "2015-11-01",
                                          "max" : "2015-11-03"
                                        }
                                      },
                                      "aggregations" : {
                                        "lose_result" : {
                                          "filter" : {
                                            "bool" : {
                                              "must" : {
                                                "range" : {
                                                  "report_date" : {
                                                    "from" : "2015-11-01",
                                                    "to" : "2015-11-03",
                                                    "include_lower" : true,
                                                    "include_upper" : true
                                                  }
                                                }
                                              }
                                            }
                                          },
                                          "aggregations" : {
                                            "lose_result" : {
                                              "terms" : {
                                                "field" : "lose_result"
                                              }
                                            }
                                          }
                                        }
                                      }
                                    }
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}


and the returning json is 

{
  "took" : 300,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "sales_center" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "&#39640;&#26143;&#37202;&#24215;",
        "doc_count" : 1,
        "region" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "&#39640;&#26143;&#37202;&#24215;",
            "doc_count" : 1,
            "sales_area" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ {
                "key" : "&#35199;&#21335;&#21306;&#22495;",
                "doc_count" : 1,
                "secondary_area" : {
                  "doc_count_error_upper_bound" : 0,
                  "sum_other_doc_count" : 0,
                  "buckets" : [ {
                    "key" : "&#22235;&#24029;",
                    "doc_count" : 1,
                    "city_name.raw" : {
                      "doc_count_error_upper_bound" : 0,
                      "sum_other_doc_count" : 0,
                      "buckets" : [ {
                        "key" : "&#38463;&#22365;",
                        "doc_count" : 1,
                        "manager_name.raw" : {
                          "doc_count_error_upper_bound" : 0,
                          "sum_other_doc_count" : 0,
                          "buckets" : [ {
                            "key" : "&#24429;&#27905;",
                            "doc_count" : 1,
                            "trading_area.raw" : {
                              "doc_count_error_upper_bound" : 0,
                              "sum_other_doc_count" : 0,
                              "buckets" : [ {
                                "key" : "&#26410;&#30830;&#23450;",
                                "doc_count" : 1,
                                "lose_result" : {
                                  "doc_count" : 2,
                                  "lose_result" : {
                                    "buckets" : [ {
                                      "key_as_string" : "2015-11-01",
                                      "key" : 1446336000000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-02",
                                      "key" : 1446422400000,
                                      "doc_count" : 1,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-03",
                                      "key" : 1446508800000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-04",
                                      "key" : 1446595200000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-05",
                                      "key" : 1446681600000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-06",
                                      "key" : 1446768000000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-07",
                                      "key" : 1446854400000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-08",
                                      "key" : 1446940800000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-09",
                                      "key" : 1447027200000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-10",
                                      "key" : 1447113600000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-11",
                                      "key" : 1447200000000,
                                      "doc_count" : 0,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-12",
                                      "key" : 1447286400000,
                                      "doc_count" : 1,
                                      "lose_result" : {
                                        "doc_count" : 0,
                                        "lose_result" : {
                                          "doc_count_error_upper_bound" : 0,
                                          "sum_other_doc_count" : 0,
                                          "buckets" : [ ]
                                        }
                                      }
                                    } ]
                                  }
                                },
                                "competitor_price" : {
                                  "doc_count" : 2,
                                  "competitor_price" : {
                                    "buckets" : [ {
                                      "key_as_string" : "2015-11-01",
                                      "key" : 1446336000000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-02",
                                      "key" : 1446422400000,
                                      "doc_count" : 1,
                                      "competitor_price" : {
                                        "doc_count" : 1
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-03",
                                      "key" : 1446508800000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-04",
                                      "key" : 1446595200000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-05",
                                      "key" : 1446681600000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-06",
                                      "key" : 1446768000000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-07",
                                      "key" : 1446854400000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-08",
                                      "key" : 1446940800000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-09",
                                      "key" : 1447027200000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-10",
                                      "key" : 1447113600000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-11",
                                      "key" : 1447200000000,
                                      "doc_count" : 0,
                                      "competitor_price" : {
                                        "doc_count" : 0
                                      }
                                    }, {
                                      "key_as_string" : "2015-11-12",
                                      "key" : 1447286400000,
                                      "doc_count" : 1,
                                      "competitor_price" : {
                                        "doc_count" : 1
                                      }
                                    } ]
                                  }
                                }
                              } ]
                            }
                          } ]
                        }
                      } ]
                    }
                  } ]
                }
              } ]
            }
          } ]
        }
      } ]
    }
  }
}
```

the returning date is out of [2015-11-01,2015-11-03] , we can see the data of 2015-11-04 etc,

 it seems that the aggregation filter doesn't work, please help me !!
</comment><comment author="saai" created="2015-11-25T10:26:13Z" id="159564049">the returned report_date value is exceeding the range filter , which is from 2015-11-01 to 2015-11-03, 

is the child aggregation filter doesn't work ? or did I wrongly set the filter?
</comment><comment author="saai" created="2015-11-25T11:42:21Z" id="159581179">I did another test, the json is much easier,

the query json 

```
{
  "aggregations" : {
    "sales_center" : {
      "terms" : {
        "field" : "sales_center"
      },
      "aggregations" : {
        "region" : {
          "terms" : {
            "field" : "region"
          },
          "aggregations" : {
            "sales_area" : {
              "terms" : {
                "field" : "sales_area"
              },
              "aggregations" : {
                "secondary_area" : {
                  "terms" : {
                    "field" : "secondary_area"
                  },
                  "aggregations" : {
                    "city_name.raw" : {
                      "terms" : {
                        "field" : "city_name.raw"
                      },
                      "aggregations" : {
                        "manager_name.raw" : {
                          "terms" : {
                            "field" : "manager_name.raw"
                          },
                          "aggregations" : {
                            "lose_stat" : {
                              "children" : {
                                "type" : "hotel-level-lose-stat"
                              },
                              "aggregations" : {
                                "qunar_scope_filter" : {
                                  "filter" : {
                                    "terms" : {
                                      "quanr_scope" : [ "ZHI_XIAO" ]
                                    }
                                  }
                                },
                                "qunar_scope" : {
                                  "terms" : {
                                    "field" : "qunar_scope"
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}


the returned results
{
  "took" : 190,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "sales_center" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "&#39640;&#26143;&#37202;&#24215;",
        "doc_count" : 2,
        "region" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "&#39640;&#26143;&#37202;&#24215;",
            "doc_count" : 2,
            "sales_area" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ {
                "key" : "&#35199;&#21335;&#21306;&#22495;",
                "doc_count" : 2,
                "secondary_area" : {
                  "doc_count_error_upper_bound" : 0,
                  "sum_other_doc_count" : 0,
                  "buckets" : [ {
                    "key" : "&#22235;&#24029;",
                    "doc_count" : 2,
                    "city_name.raw" : {
                      "doc_count_error_upper_bound" : 0,
                      "sum_other_doc_count" : 0,
                      "buckets" : [ {
                        "key" : "&#38463;&#22365;",
                        "doc_count" : 2,
                        "manager_name.raw" : {
                          "doc_count_error_upper_bound" : 0,
                          "sum_other_doc_count" : 0,
                          "buckets" : [ {
                            "key" : "&#21521;&#29840;",
                            "doc_count" : 1,
                            "lose_stat" : {
                              "doc_count" : 1,
                              "qunar_scope_filter" : {
                                "doc_count" : 0
                              },
                              "qunar_scope" : {
                                "doc_count_error_upper_bound" : 0,
                                "sum_other_doc_count" : 0,
                                "buckets" : [ {
                                  "key" : "ZHI_XIAO_AND_AGENTS",
                                  "doc_count" : 1
                                } ]
                              }
                            }
                          }, {
                            "key" : "&#24429;&#27905;",
                            "doc_count" : 1,
                            "lose_stat" : {
                              "doc_count" : 2,
                              "qunar_scope_filter" : {
                                "doc_count" : 0
                              },
                              "qunar_scope" : {
                                "doc_count_error_upper_bound" : 0,
                                "sum_other_doc_count" : 0,
                                "buckets" : [ {
                                  "key" : "ZHI_XIAO",
                                  "doc_count" : 2
                                } ]
                              }
                            }
                          } ]
                        }
                      } ]
                    }
                  } ]
                }
              } ]
            }
          } ]
        }
      }, {
        "key" : "&#21333;&#20307;&#30452;&#38144;&#20013;&#24515;",
        "doc_count" : 1,
        "region" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "&#21326;&#20013;&#22823;&#21306;",
            "doc_count" : 1,
            "sales_area" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ {
                "key" : "&#21326;&#35199;&#21335;",
                "doc_count" : 1,
                "secondary_area" : {
                  "doc_count_error_upper_bound" : 0,
                  "sum_other_doc_count" : 0,
                  "buckets" : [ {
                    "key" : "&#24029;&#36149;",
                    "doc_count" : 1,
                    "city_name.raw" : {
                      "doc_count_error_upper_bound" : 0,
                      "sum_other_doc_count" : 0,
                      "buckets" : [ {
                        "key" : "&#38463;&#22365;",
                        "doc_count" : 1,
                        "manager_name.raw" : {
                          "doc_count_error_upper_bound" : 0,
                          "sum_other_doc_count" : 0,
                          "buckets" : [ {
                            "key" : "&#40644;&#26195;&#38745;",
                            "doc_count" : 1,
                            "lose_stat" : {
                              "doc_count" : 0,
                              "qunar_scope_filter" : {
                                "doc_count" : 0
                              },
                              "qunar_scope" : {
                                "doc_count_error_upper_bound" : 0,
                                "sum_other_doc_count" : 0,
                                "buckets" : [ ]
                              }
                            }
                          } ]
                        }
                      } ]
                    }
                  } ]
                }
              } ]
            }
          } ]
        }
      } ]
    }
  }
}
```

we can see that the "qunar_scope_filter" aggregation  DID NOT  hit any doc which equals "ZHI_XIAO" , while qunar_scope aggregation has hits 2 documents of "ZHI_XIAO", they are both the sub aggregation of child type, so the filtered aggregation inside child aggregation does not work properly.  
</comment><comment author="jpountz" created="2015-11-25T18:01:30Z" id="159688715">You seem to have a typo in your example:

```
                                    "terms" : {
                                      "quanr_scope" : [ "ZHI_XIAO" ]
                                    }
```

Should it be "qunar_scope" rather than "quanr_scope"? Could it be the cause of your problem?
</comment><comment author="saai" created="2015-11-26T02:49:40Z" id="159787903">@jpountz  Yes you are right , I passed the wrong field name for qunar_scope. But , here is another strange thing, when I pass the date type field "report_date" for filter, I got wrong result , the json is as followed.

the query json:
{
  "aggregations" : {
    "sales_center" : {
      "terms" : {
        "field" : "sales_center"
      },
      "aggregations" : {
        "lose_stat" : {
          "children" : {
            "type" : "hotel-level-lose-stat"
          },
          "aggregations" : {
            "qunar_scope" : {
              "terms" : {
                "field" : "qunar_scope"
              }
            },
            "qunar_scope_filter" : {
              "filter" : {
                "terms" : {
                  "qunar_scope" : [ "ZHI_XIAO" ]
                }
              },
              "aggregations" : {
                "lose_result" : {
                  "terms" : {
                    "field" : "lose_result"
                  }
                }
              }
            },
            "report_date_filter" : {
              "filter" : {
                "range" : {
                  "report_date" : {
                    "from" : "2015-11-01",
                    "to" : "2015-11-03",
                    "include_lower" : true,
                    "include_upper" : true
                  },
                  "_name" : "report_date"
                }
              },
              "aggregations" : {
                "lose_result" : {
                  "terms" : {
                    "field" : "lose_result"
                  }
                }
              }
            },
            "report_date" : {
              "terms" : {
                "field" : "report_date"
              }
            },
            "lose_result" : {
              "terms" : {
                "field" : "lose_result"
              }
            }
          }
        }
      }
    }
  }
}

the result json:
{
  "took" : 267,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "sales_center" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "&#39640;&#26143;&#37202;&#24215;",
        "doc_count" : 2,
        "lose_stat" : {
          "doc_count" : 3,
          "report_date" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : 1446422400000,
              "key_as_string" : "2015-11-02",
              "doc_count" : 1
            }, {
              "key" : 1447286400000,
              "key_as_string" : "2015-11-12",
              "doc_count" : 1
            }, {
              "key" : 1448236800000,
              "key_as_string" : "2015-11-23",
              "doc_count" : 1
            } ]
          },
          "qunar_scope_filter" : {
            "doc_count" : 2,
            "lose_result" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ {
                "key" : "PRICE_LOSE",
                "doc_count" : 2
              } ]
            }
          },
          "qunar_scope" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : "ZHI_XIAO",
              "doc_count" : 2
            }, {
              "key" : "ZHI_XIAO_AND_AGENTS",
              "doc_count" : 1
            } ]
          },
          "lose_result" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : "PRICE_LOSE",
              "doc_count" : 3
            } ]
          },
          "report_date_filter" : {
            "doc_count" : 0,
            "lose_result" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ ]
            }
          }
        }
      }, {
        "key" : "&#21333;&#20307;&#30452;&#38144;&#20013;&#24515;",
        "doc_count" : 1,
        "lose_stat" : {
          "doc_count" : 0,
          "report_date" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ ]
          },
          "qunar_scope_filter" : {
            "doc_count" : 0,
            "lose_result" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ ]
            }
          },
          "qunar_scope" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ ]
          },
          "lose_result" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ ]
          },
          "report_date_filter" : {
            "doc_count" : 0,
            "lose_result" : {
              "doc_count_error_upper_bound" : 0,
              "sum_other_doc_count" : 0,
              "buckets" : [ ]
            }
          }
        }
      } ]
    }
  }
}

the report_date_filter did not hit any doc, since the range is [2015-11-01, 2015-11-03],  the doc 2015-11-02 should be hit, did I wrongly set  the date filter ? Thanks in advance!!
</comment><comment author="clintongormley" created="2015-11-27T13:07:16Z" id="160136053">Hi @saai 

As @jpountz said before, please could you upload a full (minimal) recreation, ie show how you create the index, index the docs, then run the query.  Otherwise we're just guessing.
</comment><comment author="saai" created="2015-12-09T02:57:11Z" id="163090273">Hello @jpountz and @clintongormley , Here I am again... 
I have made a simple example to show the issue more explicitly.
An index, which has two types , hotel which is the parent , and order which is the child. 
hotel has 3 fields: hotel_seq (id), hotel_name, hotel_level ,
order has 4 fields: report_date (date type), hotel_seq, selling_type (indicates whether the order is from the website or mobile apps), price (the price of the order).
I use java to create the index, and here are the XContentBuilder of hotel and order:

&lt;pre&gt;&lt;code&gt;
XContentBuilder parentJson = XContentFactory.jsonBuilder()
                .startObject()
                .startObject("properties")
                .startObject("hotel_seq").field("type", "string").field("index", "not_analyzed").field("doc_values", true).endObject()
                .startObject("hotel_name").field("type", "string").field("analyzer", "cjk").startObject("fields").startObject("raw").field("type", "string").field("index", "not_analyzed").field("doc_values", true).endObject().endObject().endObject()
                .startObject("hotel_level").field("type", "string").field("index", "not_analyzed").field("doc_values", true).endObject()
                .endObject()
                .endObject()
                ;
XContentBuilder childJson = XContentFactory.jsonBuilder()
                .startObject()
                .startObject("_parent").field("type", parentType).endObject()
                .startObject("properties")
                .startObject("report_date").field("type", "date").field("format", "yyyy-MM-dd").endObject()
                .startObject("hotel_seq").field("type", "string").field("index", "not_analyzed").field("doc_values", true).endObject()
                .startObject("selling_type").field("type", "string").field("index", "not_analyzed").field("doc_values", true).endObject()
                .startObject("price").field("type", "double").field("index", "not_analyzed").field("doc_values", true).endObject()
                .endObject()
                .endObject()
                ; 
&lt;/code&gt;&lt;/pre&gt;

then , I put 2 hotels and 3 orders data into the index, here are the json of the data

&lt;pre&gt;&lt;code&gt;
XContentBuilder hotel1 = XContentFactory.jsonBuilder()
                .startObject()
                .field("hotel_seq", "aba_201")
                .field("hotel_name","holidays inn")
                .field("hotel_level", "B")
                .endObject();
        XContentBuilder hotel2 = XContentFactory.jsonBuilder()
                .startObject()
                .field("hotel_seq", "aba_202")
                .field("hotel_name", "four seasons")
                .field("hotel_level", "A")
                .endObject();
        XContentBuilder order1 = XContentFactory.jsonBuilder()
                .startObject()
                .field("report_date", "2015-11-01")
                .field("hotel_seq", "aba_201")
                .field("selling_type", "WEBSITE")
                .field("price", 766.0)
                .endObject();
        XContentBuilder order2 = XContentFactory.jsonBuilder()
                .startObject()
                .field("report_date", "2015-11-03")
                .field("hotel_seq", "aba_201")
                .field("selling_type", "APP")
                .field("price", 766.0)
                .endObject();
        XContentBuilder order3 = XContentFactory.jsonBuilder()
                .startObject()
                .field("report_date","2015-11-02")
                .field("hotel_seq", "aba_202")
                .field("selling_type", "WEBSITE")
                .field("price", 1666.0)
                .endObject();
&lt;/code&gt;&lt;/pre&gt;

Then I try to query the incomes of selling_type "WEBSITE" and the incomes from report_date 2015-11-01 to 2015-11-03, aggregated by hotel_level in parent fields . In order to compare, I also added two child aggregation of report_date terms and selling_type terms.

the request json is as followed:
&lt;pre&gt;&lt;code&gt;
{
  "aggregations" : {
    "hotel_level" : {
      "terms" : {
        "field" : "hotel_level"
      },
      "aggregations" : {
        "order" : {
          "children" : {
            "type" : "child-order"
          },
          "aggregations" : {
            "selling_type_terms" : {
              "terms" : {
                "field" : "selling_type"
              }
            },
            "report_date_terms" : {
              "terms" : {
                "field" : "report_date"
              }
            },
            "selling_type_WEBSITE" : {
              "filter" : {
                "terms" : {
                  "selling_type" : [ "WEBSITE" ]
                }
              },
              "aggregations" : {
                "income" : {
                  "sum" : {
                    "field" : "price"
                  }
                }
              }
            },
            "report_date_20151101_20151103" : {
              "filter" : {
                "range" : {
                  "report_date" : {
                    "from" : "2015-11-01",
                    "to" : "2015-11-03",
                    "include_lower" : true,
                    "include_upper" : true
                  }
                }
              },
              "aggregations" : {
                "income" : {
                  "sum" : {
                    "field" : "price"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
&lt;/pre&gt;&lt;/code&gt;

and the result is 

&lt;pre&gt;&lt;code&gt;
{
  "took" : 1373,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "hotel_level" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "A",
        "doc_count" : 1,
        "order" : {
          "doc_count" : 1,
          "report_date_20151101_20151103" : {
            "doc_count" : 0,
            "income" : {
              "value" : 0.0
            }
          },
          "selling_type_WEBSITE" : {
            "doc_count" : 1,
            "income" : {
              "value" : 1666.0
            }
          },
          "selling_type_terms" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : "WEBSITE",
              "doc_count" : 1
            } ]
          },
          "report_date_terms" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : 1446422400000,
              "key_as_string" : "2015-11-02",
              "doc_count" : 1
            } ]
          }
        }
      }, {
        "key" : "B",
        "doc_count" : 1,
        "order" : {
          "doc_count" : 2,
          "report_date_20151101_20151103" : {
            "doc_count" : 0,
            "income" : {
              "value" : 0.0
            }
          },
          "selling_type_WEBSITE" : {
            "doc_count" : 1,
            "income" : {
              "value" : 766.0
            }
          },
          "selling_type_terms" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : "APP",
              "doc_count" : 1
            }, {
              "key" : "WEBSITE",
              "doc_count" : 1
            } ]
          },
          "report_date_terms" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [ {
              "key" : 1446336000000,
              "key_as_string" : "2015-11-01",
              "doc_count" : 1
            }, {
              "key" : 1446508800000,
              "key_as_string" : "2015-11-03",
              "doc_count" : 1
            } ]
          }
        }
      } ]
    }
  }
}
&lt;/pre&gt;&lt;/code&gt;

In hotel_level bucket "A", I got 1666.0 for the total incomes of WEBSITE ,which is right,  but 0 for total incomes from 20151101 to 20151103, which should be 1666.0,  so I guess that the date range filter is not working in child aggregations.
</comment><comment author="clintongormley" created="2015-12-14T20:31:30Z" id="164550609">@martijnvg Please could you have a look at this?
</comment><comment author="clintongormley" created="2015-12-14T20:32:53Z" id="164550905">Related to https://github.com/elastic/elasticsearch/issues/15413 ?
</comment><comment author="martijnvg" created="2015-12-15T20:59:58Z" id="164895037">@saai I tried to make a reproduction based on what you have shared in your latest comment:

Mappings and documents:

```
DELETE /_all

PUT /test
{
  "mappings": {
    "parentType" : {
      "properties": {
        "hotel_name": {"type": "string"},
        "hotel_level" : {"type": "string", "index": "not_analyzed"}
      }
    },
    "childType": {
      "_parent": {
        "type": "parentType"
      },
      "properties": {
        "selling_type" : {"type": "string", "index": "not_analyzed"},
        "price" : {"type": "double", "index": "not_analyzed"},
        "report_date": {"type": "date", "format": "yyyy-MM-dd"}
      }
    }
  }
}

PUT /test/parentType/1
{
  "hotel_name":"holidays inn",
  "hotel_level": "B"
}

PUT /test/parentType/2
{
  "hotel_name": "four seasons",
  "hotel_level": "A"
}

PUT /test/childType/order1?parent=1
{
  "report_date": "2015-11-01",
  "selling_type": "WEBSITE",
  "price": 766.0
}

PUT /test/childType/order2?parent=1
{
  "report_date": "2015-11-03",
  "selling_type": "APP",
  "price": 766.0  
}

PUT /test/childType/order3?parent=2
{
  "report_date": "2015-11-02",
  "selling_type": "WEBSITE",
  "price": 1666.0
}
```

Search request:

```
GET /test/_search
{
  "size": 0, 
  "aggregations" : {
    "hotel_level" : {
      "terms" : {
        "field" : "hotel_level"
      },
      "aggregations" : {
        "order" : {
          "children" : {
            "type" : "childType"
          },
          "aggregations" : {
            "selling_type_terms" : {
              "terms" : {
                "field" : "selling_type"
              }
            },
            "report_date_terms" : {
              "terms" : {
                "field" : "report_date"
              }
            },
            "selling_type_WEBSITE" : {
              "filter" : {
                "terms" : {
                  "selling_type" : [ "WEBSITE" ]
                }
              },
              "aggregations" : {
                "income" : {
                  "sum" : {
                    "field" : "price"
                  }
                }
              }
            },
            "report_date_20151101_20151103" : {
              "filter" : {
                "range" : {
                  "report_date" : {
                    "from" : "2015-11-01",
                    "to" : "2015-11-03",
                    "include_lower" : true,
                    "include_upper" : true
                  }
                }
              },
              "aggregations" : {
                "income" : {
                  "sum" : {
                    "field" : "price"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

When running this I do get results for the `report_date_20151101_20151103`(income is 1666). So this looks good here, but maybe my reproduction isn't exactly the same? (something got lost between translating the java snippets to curl requests)

Some questions:
- Can you try and run the reproduction I have shared here? 
- What ES version are you using exactly? 
- Did you run your reproduction on already existing index?
</comment><comment author="saai" created="2016-01-04T04:17:04Z" id="168576989">@martijnvg Thanks for the reply, your reproduction is working fine. Since I am using the version 1.7.1, I guess this is an old bug for "date" type,  it will take some time for my project to update to the version 2.0 +, I will let you know if I found the same issue after the update...

Again thanks very much for your conscientious reply !!!
</comment><comment author="clintongormley" created="2016-02-14T17:57:07Z" id="183938731">thanks @saai - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ZenFaultDetectionTests:testMasterFaultDetectionConnectOnDisconnect doesn't check throwable causing failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15004</link><project id="" key="" /><description>This is a followup to #14827 - after adding the Throwable causing the failure as an input parameter to MasterFaultDetection.Listener:onMasterFailure in addition to checking failure node and exception message information we should also check the correct Throwable was thrown.

https://github.com/elastic/elasticsearch/blob/473b19400f5f379c7c33e5afb70a3a8e1e258680/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java#L196 is the method that now gets more information than before.
</description><key id="118798613">15004</key><summary>ZenFaultDetectionTests:testMasterFaultDetectionConnectOnDisconnect doesn't check throwable causing failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-11-25T09:53:19Z</created><updated>2016-02-15T10:43:56Z</updated><resolved>2016-02-15T10:43:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-25T13:28:55Z" id="159607139">@MaineC can you elaborate on the plan ? right now we check this (or similar) before calling the listener. See https://github.com/elastic/elasticsearch/blob/473b19400f5f379c7c33e5afb70a3a8e1e258680/core/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java#L257
</comment><comment author="MaineC" created="2015-11-25T13:53:00Z" id="159613224">@bleskes No concrete plan, just a potential left over of #14827 where I introduced the additional parameter to the interface but in this particular test didn't do anything with it.

Happy to close this issue as "no change needed - the checking we have in place is sufficient". I'm just not familiar enough with this particular test to make the judgement call myself.
</comment><comment author="bleskes" created="2015-11-25T14:33:38Z" id="159624214">why did you add the parameter to the throwable?
</comment><comment author="MaineC" created="2015-11-26T08:41:44Z" id="159848705">To be able to add it to log messages we create in non-test implementations of the same interface. See also full changes here: #14827 
</comment><comment author="clintongormley" created="2016-02-14T17:54:56Z" id="183938619">@bleskes @MaineC anything further to do here?
</comment><comment author="bleskes" created="2016-02-15T10:43:55Z" id="184162482">I think we can close this (and do so ;) ). @MaineC if you still see something here, please feel free to reopen... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node leaving the cluster does not trigger 'recover', unassigned shards are not re-assigned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15003</link><project id="" key="" /><description>Dear people,

After upgrading to ES 2.0.0 I noticed that sometimes shards stay unassigned after a node leaves the cluster. We have been looking at all the usual suspects like 

```
curl -XPUT localhost:9200/_cluster/settings -d '
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}'
```

but, the only thing that we found working in this situation was a manual reroute or enabling the allocation (which is already enabled). Later it became clear that it only happens on stable clusters. If there are relocations going on the unassigned shards are assigned fine.

Yesterday we applied the 2.1.0 update. But we still have the same issue.

As far as we can see this is an issue (for us certainly). If it is a feature my apologies for abusing this area, and I will move the discussion to the forums.

Groet,
Jurg.
</description><key id="118797803">15003</key><summary>Node leaving the cluster does not trigger 'recover', unassigned shards are not re-assigned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">truthtrap</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2015-11-25T09:48:38Z</created><updated>2016-02-28T19:17:13Z</updated><resolved>2016-02-28T19:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-25T10:22:35Z" id="159563321">A manual reroute should not be needed. This sounds like a bug. Can you share information about the size of the cluster and the settings used, especially those affecting shard allocation (https://www.elastic.co/guide/en/elasticsearch/reference/2.1/index-modules-allocation.html)?
</comment><comment author="truthtrap" created="2015-11-25T12:01:17Z" id="159588769">Cluster has been fluctuating between 6 and 3 nodes, to test the scaling. It holds 12 indexes, with 3 shards and 1 replica. It holds 75m documents, in total 100G of data. (Kind of augmented logstash setup ingesting a bit more than 3 documents per second, sustained.)

It runs on EC2. Normally we use 3 t2.medium instances (it is staging) but we have been rotating up to m4.10xlarge because we had to restore a month.

elasticsearch.json:

```
{
    "_comment": "this configuration is auto-generated (from userdata)",
    "bootstrap": {
        "mlockall": true
    },
    "cloud": {
        "aws": {
            "access_key": "...",
            "region": "eu-west-1",
            "secret_key": "..."
        }
    },
    "cluster": {
        "name": "staging.elasticsearch"
    },
    "discovery": {
        "ec2": {
            "groups": "staging-elasticsearch-30mhz-com",
            "host_type": "public_dns"
        },
        "type": "ec2",
        "zen": {
            "minimum_master_nodes": 2
        }
    },
    "gateway": {
        "expected_nodes": 3,
        "recover_after_nodes": 2
    },
    "indices": {
        "fielddata": {
            "cache": {
                "size": "50%"
            }
        },
        "recovery": {
            "concurrent_streams": 5,
            "max_bytes_per_sec": "50mb"
        }
    },
    "network.host": "_ec2:publicDns_",
    "node": {
        "data": true,
        "master": true
    }
}
```
</comment><comment author="bleskes" created="2015-11-25T13:01:47Z" id="159601097">Just double checking - this is not by any chance a case of delayed assignment? - see https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html#delayed-allocation    
</comment><comment author="truthtrap" created="2015-11-25T13:27:32Z" id="159606792">(hi boaz :) )

no. i tried to put that in the config, to force an automatic explicit trigger :) (couldn't find how to do that.)
</comment><comment author="bqk-" created="2015-11-25T13:32:07Z" id="159608310">I have a similar problem in a 8 nodes cluster, 7 data nodes and 1 non-data.
After closing a first node, shards get reassigned nicely on the other nodes, but when closing a second one later, shards stay unassigned. 
Both nodes have the exact same configuration and I cannot see why it would have a difference behavior on the two nodes.

This is using ES 2.0 also.
</comment><comment author="bqk-" created="2015-11-25T14:14:48Z" id="159618780">Somehow it allocated the shards after a few hours, although delayed_unassigned_shards showed 0 the whole time.
Is there any action that triggers it ?
</comment><comment author="bleskes" created="2015-12-09T16:56:14Z" id="163323314">I tried to to reproduce this and couldn't - I started a 4 nodes cluster, added 12 indices and your settings. Shutting down a node (kill -9) resulted in the expected 60s wait for it to come back, followed by allocation of the missing shards and back to green. @truthtrap can you supply more info/work on a small and reliable reproduction? 
</comment><comment author="truthtrap" created="2015-12-09T19:33:38Z" id="163366597">@bleskes thanks for trying. can you try an 'orderly' shutdown with the init.d script that is part of the package (rpm we use)?

i'll try to set up a completely standard cluster, populate with some indices, and try to reproduce.
</comment><comment author="clintongormley" created="2016-02-14T17:53:17Z" id="183938521">@truthtrap is this something you're still seeing on 2.2?
</comment><comment author="truthtrap" created="2016-02-15T10:13:38Z" id="184145413">@clinton didn't try yet. will upgrade to 2.2.0, and let you know...

On Sun, Feb 14, 2016 at 6:54 PM, Clinton Gormley notifications@github.com
wrote:

&gt; @truthtrap https://github.com/truthtrap is this something you're still
&gt; seeing on 2.2?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/15003#issuecomment-183938521
&gt; .
</comment><comment author="truthtrap" created="2016-02-17T06:58:45Z" id="185060697">@clinton looks good so far. upgraded (and rotated) our staging cluster. it
honors the 60s timeout, and then it start re-assigning shards again. if we
are ready to upgrade production i'll let you know how that goes.

(our staging cluster has 125G, 14 indices, each 3 shards with 1 replica.)

On Mon, Feb 15, 2016 at 11:13 AM, Jurg van Vliet jurg@truthtrap.com wrote:

&gt; @clinton didn't try yet. will upgrade to 2.2.0, and let you know...
&gt; 
&gt; On Sun, Feb 14, 2016 at 6:54 PM, Clinton Gormley &lt;notifications@github.com
&gt; 
&gt; &gt; wrote:
&gt; &gt; 
&gt; &gt; @truthtrap https://github.com/truthtrap is this something you're still
&gt; &gt; seeing on 2.2?
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elastic/elasticsearch/issues/15003#issuecomment-183938521
&gt; &gt; .
</comment><comment author="truthtrap" created="2016-02-21T10:55:04Z" id="186797551">@clinton rotating production elasticsearch cluster was ok as well. no
manual intervention necessary to start assigning shards.

thanks!!

On Wed, Feb 17, 2016 at 7:58 AM, Jurg van Vliet jurg@truthtrap.com wrote:

&gt; @clinton looks good so far. upgraded (and rotated) our staging cluster. it
&gt; honors the 60s timeout, and then it start re-assigning shards again. if we
&gt; are ready to upgrade production i'll let you know how that goes.
&gt; 
&gt; (our staging cluster has 125G, 14 indices, each 3 shards with 1 replica.)
&gt; 
&gt; On Mon, Feb 15, 2016 at 11:13 AM, Jurg van Vliet jurg@truthtrap.com
&gt; wrote:
&gt; 
&gt; &gt; @clinton didn't try yet. will upgrade to 2.2.0, and let you know...
&gt; &gt; 
&gt; &gt; On Sun, Feb 14, 2016 at 6:54 PM, Clinton Gormley &lt;
&gt; &gt; notifications@github.com&gt; wrote:
&gt; &gt; 
&gt; &gt; &gt; @truthtrap https://github.com/truthtrap is this something you're
&gt; &gt; &gt; still seeing on 2.2?
&gt; &gt; &gt; 
&gt; &gt; &gt; &#8212;
&gt; &gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; &gt; https://github.com/elastic/elasticsearch/issues/15003#issuecomment-183938521
&gt; &gt; &gt; .
</comment><comment author="clintongormley" created="2016-02-28T19:17:13Z" id="189924553">thanks @truthtrap - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Geo Centroid Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15002</link><project id="" key="" /><description /><key id="118793462">15002</key><summary>Aggregations Refactor: Refactor Geo Centroid Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-25T09:23:55Z</created><updated>2015-11-26T09:52:56Z</updated><resolved>2015-11-26T09:44:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-25T15:37:52Z" id="159643594">@colings86 left two small comments, otherwise looks good. Again, not sure if maybe also @jpountz should take a brief look?
</comment><comment author="jpountz" created="2015-11-25T19:16:08Z" id="159705163">LGTM, I just seconded @cbuescher 's request to have a bit more of javadocs.
</comment><comment author="colings86" created="2015-11-26T09:52:53Z" id="159862926">Despite what Github may think, this was successfully merged into the agg refactoring branch https://github.com/elastic/elasticsearch/commit/68f18e927b4023db7bf093ed8bfbf82711d1583d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] define index template for .ingest index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15001</link><project id="" key="" /><description>We use the .ingest index to store pipeline configuration, we should make sure to apply proper defaults to it via a specific index template. This index should have a single shard, also the mapping is important, we should disable the whole pipeline configuration object, so that nothing gets indexed there.
</description><key id="118792697">15001</key><summary>[Ingest] define index template for .ingest index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-25T09:18:32Z</created><updated>2015-12-24T14:20:07Z</updated><resolved>2015-12-24T14:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-24T14:20:07Z" id="167117557">Added via: 1936d6118df79fa63e18014675facfc905877d5b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add update parameter to bin/plugin, for easier updating of plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15000</link><project id="" key="" /><description>With the frequent release cycle (and the need to update every elastic plugin), it would be nice to add an `update` parameter to `bin/plugin`, that does `bin/plugin remove ...` and `bin/plugin install ...` in one step. 
</description><key id="118791014">15000</key><summary>Add update parameter to bin/plugin, for easier updating of plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jakommo</reporter><labels><label>:Plugins</label><label>discuss</label><label>enhancement</label></labels><created>2015-11-25T09:08:42Z</created><updated>2015-12-11T11:04:19Z</updated><resolved>2015-12-11T11:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-11-27T23:48:34Z" id="160227570">Personally I'm -1 on this one... I really want to discourage ppl from blindly updating plugins and hoping everything "just works". I much rather force ppl to read upgrade docs that will take them step by step through the upgrade process.
</comment><comment author="ppf2" created="2015-12-01T00:10:08Z" id="160804835">+1 from additional users to have an update plugin option.

Also +1 on @uboness 's comment on discouraging people from blindly updating plugins.  Perhaps we can build some intelligence into the update plugin script.  For example, in addition to verifying that the plugin version is compatible with the ES version (a given), perhaps we can have a way to tag a new plugin version with a "breaking" flag (for plugin updates that have its own breaking changes), and when the user attempts to update to this plugin, we can prompt them to acknowledge it (with a link to the breaking change documentation for the plugin) before allowing the actual update to run.  But this also means that we will need to start creating a breaking changes section in our documentation for each plugin, etc.. just a thought ...
</comment><comment author="dadoonet" created="2015-12-01T02:59:21Z" id="160832997">@ppf2 FYI we do have such doc in breaking changes. For example in 3.0: https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_30_plugins.html
</comment><comment author="dadoonet" created="2015-12-01T03:03:13Z" id="160833409">Also IMO we should not have breaking changes between minor versions (second digit in X.Y.Z).
It means that we should always deprecate but keep the compatibility with old settings, old file formats.
Like we try to do in elasticsearch itself.

For sure, updating from 2.x to 3.x should not be allowed in general.
</comment><comment author="clintongormley" created="2015-12-01T11:15:53Z" id="160938085">I'm -1 on this as well.  Besides the reasons stated by @uboness above, we also have to consider that plugins come from the community as well, which can come from github, maven, and http URL, wherever.  Plugin locations may change, be renamed, be discontinued etc.  While this seems like a simple change to implement, it comes with a boatload of complexity.
</comment><comment author="clintongormley" created="2015-12-11T11:04:19Z" id="163910060">We're not going to add the update option, but we should document the need to upgrade plugins during the rolling upgrade process. Closing in favour of https://github.com/elastic/elasticsearch/issues/15389
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight not working in elasticsearch 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14999</link><project id="" key="" /><description>When I was upgraded es from 2.0 to 2.1, highlight feature stop working.

I got this error message:

```
{
  "shard": 0,
  "index": "4odevelop_4o",
  "node": "XiZaRocLQGuBwZPg58Naqw",
  "reason": {
    "type": "illegal_state_exception",
    "reason": "can't load global ordinals for reader of type: class org.apache.lucene.search.highlight.WeightedSpanTermExtractor$DelegatingLeafReader must be a DirectoryReader"
  }
}
```
</description><key id="118789369">14999</key><summary>Highlight not working in elasticsearch 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">lukapor</reporter><labels><label>:Highlighting</label><label>:Parent/Child</label><label>bug</label><label>v2.4.0</label></labels><created>2015-11-25T09:00:15Z</created><updated>2016-07-29T21:56:12Z</updated><resolved>2016-07-29T11:01:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukapor" created="2015-11-25T09:24:29Z" id="159546068">Problem occurs when I using next search query

```
{
  "from": 0,
  "size": 20,
  "sort": [
    {
      "_score": {
        "missing": "_last",
        "order": "desc"
      }
    }
  ],
  "highlight": {
    "pre_tags": [
      "&lt;lukituki&gt;"
    ],
    "post_tags": [
      "&lt;/lukituki&gt;"
    ],
    "fields": {
      "searchText": { }
    }
  },
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "authorizationToken": {
              "value": "1859"
            }
          }
        },
        {
          "term": {
            "deleted": {
              "value": false
            }
          }
        }
      ],
      "should": [
        {
          "has_child": {
            "type": "stream1boost",
            "score_mode": "sum",
            "query": {
              "function_score": {
                "query": {
                  "bool": {
                    "must": [
                      {
                        "term": {
                          "userAccountId": {
                            "boost": 0,
                            "value": 1859
                          }
                        }
                      }
                    ]
                  }
                },
                "score_mode": "sum",
                "boost_mode": "max",
                "script_score": {
                  "script": "doc['searchBoost'].value"
                }
              }
            }
          }
        }
      ],
      "minimum_should_match": "0"
    }
  }
}
```
</comment><comment author="nik9000" created="2015-11-25T14:17:18Z" id="159619480">@lukapor, I edited your messages to apply code formatting.

This certainly looks like a bug. It'd be super helpful if you could make a gist that recreates this against an empty index using [curl](https://www.elastic.co/help). It'd make reproducing the issue locally super easy.
</comment><comment author="lukapor" created="2015-11-25T14:50:01Z" id="159630202">Hello,
unfortunately I do not have script for the generation the index, whereas it creates programming. An easy way to tell just where the problem is.
I have stream1 object that has a child object stream1boost.
When I search from stream1 one of sort is by most used. There i use child function score. If this is used ("should": [{"has_child": { ...) the problem occours otherwise not.

I will try to generate script  to reproduce the bug
</comment><comment author="lukapor" created="2015-11-25T21:59:45Z" id="159741091">I attached script to create index mapping, insert data and then query them. On query request will
[insertData.txt](https://github.com/elastic/elasticsearch/files/44438/insertData.txt)
[mapping.txt](https://github.com/elastic/elasticsearch/files/44436/mapping.txt)
[query.txt](https://github.com/elastic/elasticsearch/files/44437/query.txt)

 fail.
</comment><comment author="clintongormley" created="2015-11-28T17:45:02Z" id="160323849">Reduced to the following minimal test case:

```
PUT /test1
{
  "mappings": {
    "stream1": {
    },
    "stream1boost": {
      "_parent": {
        "type": "stream1"
      }
    }
  }
}

PUT /test1/stream1/1
{
    "searchText": "stream1"
}


PUT /test1/stream1boost/1?parent=1
{
    "searchText": "stream1",
    "searchBoost": 1,
    "userId": 1
}


POST /test1/stream1/_search
{
  "from": 0,
  "size": 20,
  "highlight": {
    "fields": {
      "searchText": { }
    }
  },
  "query": {
    "bool": {
      "should": [
        {
          "has_child": {
            "type": "stream1boost",
            "query": {
              "match_all": {}
            }
          }
        }
      ]
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-11-28T17:45:35Z" id="160323990">@martijnvg could you take a look please
</comment><comment author="martijnvg" created="2016-01-27T13:39:37Z" id="175632334">The problem here is that the parent/child queries since 2.0 require a top level reader is used. During highlighting we re-execute the query for each hit using the leaf reader the hit was found in. The parent/child queries refuse to work with this now. Before 2.0 highlighting wouldn't have worked all the time with parent/child queries as the child hit maybe in a different leaf reader (segment) then the parent hit.

The right way for highlighting in this case would be to use `inner_hits` and move the highlighting part from the top level to the inner hits part:

```
POST /test1/stream1/_search
{
  "from": 0,
  "size": 20,
  "query": {
    "bool": {
      "should": [
        {
          "has_child": {
            "type": "stream1boost",
            "query": {
              "match": {
                "searchText": "stream1"
              }
            },
            "inner_hits": {
              "highlight": {
                "fields": {
                  "searchText": {}
                }
              }
            }
          }
        }
      ]
    }
  }
}
```

I think that instead of throwing an error highlighting shouldn't try extract terms from `has_child` or `has_parent`, so that if these queries just happen to be part of a bigger query other highlights do get returned in the response.
</comment><comment author="lukapor" created="2016-01-27T14:37:34Z" id="175659759">I can tell you only that the query works in es2.0, it stop working with version 2.1

My use case is next
stream1 holds searchText propertie
stream1boost is user boost for specific stream (stream1 has multiple stream1boost or none), stream1boost has only searchBoost propertie (weight)

So I am searching for stream with some prefix query with different sorts (by name, by most used, ..). Results that I want are stream1 and their highlights. When most used sort is selected I use has_child should query with function_score, that calculate score of current streams. 
</comment><comment author="clintongormley" created="2016-01-28T10:08:09Z" id="176100998">&gt; I think that instead of throwing an error highlighting shouldn't try extract terms from has_child or has_parent, so that if these queries just happen to be part of a bigger query other highlights do get returned in the response.

Makes sense.  Without inner hits, you wouldn't expect docs matching a has_child or has_parent query to be returned anyway, so there shouldn't be any highlighting on these docs.
</comment><comment author="rpedela" created="2016-02-19T00:12:40Z" id="185988506">I have the same problem with 2.2. In my case, I need highlighting for the parent and the child. If I remove the top-level highlighting then it works fine.

Does not work

``` js
{
    query: {
        bool: {
            should: [
                {
                    query_string: 'google'
                },
                {
                    has_child: {
                        type: 'child_doc',
                        score_mode: 'max',
                        query: {
                            query_string: 'google'
                        },
                        inner_hits: {
                            highlight: {
                                order: 'score',
                                fields: {
                                    title: { number_of_fragments: 0 },
                                    body: { number_of_fragments: 3 }
                                }
                            },
                            from: 0,
                            size: 1
                        }
                    }
                }
            ]
        }
    },
    highlight: {
        order: 'score',
        fields: {
            description: { number_of_fragments: 0 }
        }
    }
}
```

Does work, but no highlighting on parent document

``` js
{
    query: {
        bool: {
            should: [
                {
                    query_string: 'google'
                },
                {
                    has_child: {
                        type: 'child_doc',
                        score_mode: 'max',
                        query: {
                            query_string: 'google'
                        },
                        inner_hits: {
                            highlight: {
                                order: 'score',
                                fields: {
                                    title: { number_of_fragments: 0 },
                                    body: { number_of_fragments: 3 }
                                }
                            },
                            from: 0,
                            size: 1
                        }
                    }
                }
            ]
        }
    }
}
```
</comment><comment author="borjakhet" created="2016-03-04T11:01:38Z" id="192234177">I'm having the same results as @rpedela (using ES 2.1). Any fixes?
</comment><comment author="nik9000" created="2016-03-04T13:35:26Z" id="192286246">&gt; I'm having the same results as @rpedela (using ES 2.1). Any fixes?

I'd try using a highlight_query element that doesn't include parent/child.
</comment><comment author="nik9000" created="2016-03-04T13:35:46Z" id="192286305">This thing: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html#_highlight_query
</comment><comment author="martijnvg" created="2016-03-04T13:38:23Z" id="192287005">So the search request body should look like this:

```
{
    query: {
        bool: {
            should: [
                {
                    query_string: 'google'
                },
                {
                    has_child: {
                        type: 'child_doc',
                        score_mode: 'max',
                        query: {
                            query_string: 'google'
                        },
                        inner_hits: {
                            highlight: {
                                order: 'score',
                                fields: {
                                    title: { number_of_fragments: 0 },
                                    body: { number_of_fragments: 3 }
                                }
                            },
                            from: 0,
                            size: 1
                        }
                    }
                }
            ]
        }
    },
    highlight: {
        order: 'score',
        fields: {
            description: { number_of_fragments: 0 }
        },
        highlight_query: {
            bool: {
            should: [
                {
                    query_string: 'google'
                }
            ]
        }
    }
}
```
</comment><comment author="borjakhet" created="2016-03-04T14:05:43Z" id="192294989">It's still not working for me.

Original code without highlights, working ok.

```
        {
            query:
            {
                bool:
                {
                    must:
                    [
                        {
                            query_string:
                            {
                                fields: [ 'title', 'body'],
                                query: pUserInput,

                            }
                        },
                        {
                           term: {
                            source_id: pSourceIdInput
                           }
                        },
                        {
                            has_child:
                            {
                                type: "user_item_relation",
                                query:
                                {
                                    bool:
                                    {
                                        must:
                                        [
                                            {
                                                term:
                                                {
                                                    user_id: pUserIdInput
                                                }
                                            }
                                            /*{
                                                term:
                                                {
                                                    favorite: 1
                                                }
                                            }*/

                                        ]
                                    }
                                }
                            }
                        }
                    ]
                }
            }
        }
```

Code with highlights, query works ok but no highlights are shown.

```
        {
            query:
            {
                bool:
                {
                    must:
                    [
                        {
                            query_string:
                            {
                                fields: [ 'title', 'body'],
                                query: pUserInput,

                            }
                        },
                        {
                           term: {
                            source_id: pSourceIdInput
                           }
                        },
                        {
                            has_child:
                            {
                                type: "user_item_relation",
                                query:
                                {
                                    bool:
                                    {
                                        must:
                                        [
                                            {
                                                term:
                                                {
                                                    user_id: pUserIdInput
                                                }
                                            }
                                            /*{
                                                term:
                                                {
                                                    favorite: 1
                                                }
                                            }*/

                                        ]
                                    }
                                },
                                inner_hits: {
                                   highlight: {
                                     order: 'score',
                                     fields: {
                                         title: { number_of_fragments: 0 },
                                         body: { number_of_fragments: 3 }
                                     }
                                  }
                                 }
                            }
                        }
                    ]
                }
            },
            highlight: {
                 order: 'score',
                 fields: {
                   title: { number_of_fragments: 0 }
                 },
                 highlight_query: {
                   bool: {
                   should: [
                     {
                         query_string:
                         {
                             fields: [ 'title', 'body'],
                             query: pUserInput,

                         }
                     }
                   ]
                 }
             }
        }
    }
```

What I'm doing wrong here? 
Thanks in advance.
</comment><comment author="martijnvg" created="2016-03-04T14:12:43Z" id="192297044">@borjakhet You still get the same error? This should work, I checked it locally. Maybe try to run with @clintongormley minimal reproduction via Sense / curl commands? So that it is easier to see what happens?
</comment><comment author="borjakhet" created="2016-03-04T14:20:44Z" id="192299040">It works, it was my API fault, I was reading `_source.title` instead of `highlight.title`
</comment><comment author="vicmosin" created="2016-07-25T09:56:01Z" id="234911202">Also have this issue with 2.1.0 and using `highlight_query` only solves it.. Otherwise error is thrown. Should we wait for a bugfix? Current solution looks more like a workaround, even [docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-highlighting.html#_highlight_query) explains the true meaning of using `highlight_query`... 
</comment><comment author="benbenwilde" created="2016-07-29T21:52:51Z" id="236303163">+1
This was quite confusing for me as I was highlighting on parent and child documents at the same time.

Essentially it seems it will work by merely including a highlight_query clause in every highlight clause, even if it's just a match all.

Also, I was using 2.3 and thought it would be fixed by now, seeing all the references to 2.1. Looks like it won't come till 2.4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pipeline config changes should be updated to all ingest nodes in a sync manner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14998</link><project id="" key="" /><description>Based on the review item: https://github.com/s1monw/elasticsearch/commit/042ef8c20476459f92ea1c2ea12bc1950211e693#diff-c69f941ab536509edea2ed0715c9a0adR169

Right now changes that are made to pipelines via the put pipeline api are not immediately picked up by all ingest nodes. Each ingest node periodically polls the .ingest index in the background and if there are changes it updates the in-memory representation of the pipelines.

If changes made to a pipeline via the put pipeline api would also trigger that all ingest nodes update their pipelines then things becomes much more predictable. This process isn't heavy, so I think we shouldn't be concerned about performance if we trigger a reload after each call the put pipeline api.

This should make using node ingest easier and also would make testing easier. (no need to wrap assertions inside assertBusy(...) blocks).

If we were to add an internal pipeline refresh api then the put and delete pipeline apis would you invoke that after each modification. The internal refresh api should trigger a reload of all ingest nodes and return when all the reloads have been completed.
</description><key id="118787923">14998</key><summary>Pipeline config changes should be updated to all ingest nodes in a sync manner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-25T08:51:17Z</created><updated>2016-01-11T17:10:15Z</updated><resolved>2016-01-11T17:10:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-26T15:21:59Z" id="159937035">There is a complication with only relying on the sync push mechanism. Changes can be missed, for example when ingest node is starting up and hasn't yet read all ingest documents from the .ingest index and another node is pushing the latest update of a pipeline, which the current node isn't ready to accept yet.

So instead we should keep the current async pipeline loading and as an addition to that we should add the sync refresh of pipelines on all other nodes if a node accepts a put or delete pipeline request. This way we're sure that we never ever miss pipelines on nodes (even during restarts) and because of the sync refresh all ingest nodes that are currently started are guaranteed to have the latest changes made to pipelines after a put or delete pipeline call. So this still makes testing easier and we don't have to rely on `assertBusy(...)` snippets in tests.

If we add the sync pipeline refresh and the put and delete pipeline apis run that as part of their operation then we can increase the interval to async load pipelines to a higher value. 
</comment><comment author="javanna" created="2016-01-11T17:10:15Z" id="170621287">This has been implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow extra config for integ test to be anything project.file() accepts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14997</link><project id="" key="" /><description>This change delays the lookup for whatever is passed to extra config as
the source file to happen at execution time. This allows using eg a task
which generates a file, but maintains the checks that the file is not a
dir and that it exists at runtime.
</description><key id="118786079">14997</key><summary>Allow extra config for integ test to be anything project.file() accepts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T08:37:59Z</created><updated>2015-11-25T17:00:55Z</updated><resolved>2015-11-25T17:00:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-25T15:25:29Z" id="159640269">LGTM, was this causing a problem somewhere?
</comment><comment author="rjernst" created="2015-11-25T17:00:49Z" id="159672328">&gt; was this causing a problem somewhere?

Not directly, but this is a precursor to future changes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add wrapper around gradle Exec task for easier logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14996</link><project id="" key="" /><description>The Exec task outputs stdout/stderr to the standard streams by default.
However, to keep output short, we currently capture this, and only
output if the task failed. This change makes a small wrapper around Exec
to facilitate this behavior anywhere we use Exec.
</description><key id="118784987">14996</key><summary>Add wrapper around gradle Exec task for easier logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T08:28:27Z</created><updated>2015-11-25T19:26:33Z</updated><resolved>2015-11-25T19:26:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-25T19:23:01Z" id="159706599">+1 we should always show the subprocess output when doing any exec-type tasks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>##&#22914;&#20309;&#23545;aggregations&#32858;&#21512;&#32467;&#26524;&#22312;&#36827;&#34892;&#26465;&#20214;&#26597;&#35810;&#65292;&#36807;&#28388;&#20986;&#31526;&#21512;&#26465;&#20214;&#30340;buck&#65292;&#38024;&#23545;es2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14995</link><project id="" key="" /><description>&#20363;&#22914;
**&#19968;&#12289;&#25968;&#25454;&#28304;**
curl -XPUT 'http://localhost:9200/order/static_type/1' -d '{  "orderid": "00001","time":1,"f1":"A" }'
curl -XPUT 'http://localhost:9200/order/static_type/2' -d '{  "orderid": "00001","time":2,"f2":"B1" }'
curl -XPUT 'http://localhost:9200/order/static_type/3' -d '{  "orderid": "00001","time":3 ,"f2":"B2","f3":"C1"}'
curl -XPUT 'http://localhost:9200/order/static_type/4' -d '{  "orderid": "00001","time":4 ,"f3":"C2"}'
curl -XPUT 'http://localhost:9200/order/static_type/5' -d '{  "orderid": "00001","time":5 ,"f4":"D"}' 
curl -XPUT 'http://localhost:9200/order/static_type/6' -d '{  "orderid": "00001","time":6 ,"f5":2}'
curl -XPUT 'http://localhost:9200/order/static_type/7' -d '{  "orderid": "00002","time":1 ,"f1":"E"}'
curl -XPUT 'http://localhost:9200/order/static_type/8' -d '{  "orderid": "00002","time":3 ,"f1":"A","f2":"B"}'
**&#20108;&#12289;&#25353;orderid&#20570;aggregations&#32858;&#21512;**
{
  "aggregations": {
    "orderid": {
      "terms": {
        "field": "orderid"
      },
      "aggregations": {
        "f2s": {
          "terms": {
            "field": "f2"
          }
        },
        "f1s": {
          "terms": {
            "field": "f1"
          }
        },
        "tmax": {
          "max": {
            "field": "time"
          }
        },
        "tmin": {
          "min": {
            "field": "time"
          }
        }
      }
    }
  }
}
**&#19977;&#12289;&#32467;&#26524;&#20004;&#26465;&#35760;&#24405;&#65306;**
![image](https://cloud.githubusercontent.com/assets/4476265/11390685/66fee60c-9387-11e5-9755-7f8f7cdfe162.png)

**&#22235;&#12289;&#24819;&#35201;&#30340;&#32467;&#26524;&#23601;&#26159;&#65292;&#23545;&#19977;&#20013;&#30340;&#20004;&#26465;&#32858;&#21512;&#32467;&#26524;&#35760;&#24405;&#38598;&#21512;&#33021;&#22815;&#26681;&#25454;orderid&#65292;f1s,f2s &#20316;&#20026;&#26597;&#35810;&#26465;&#20214;&#65292;&#22914;&#26524;&#19981;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#23545;&#24212;&#25972;&#26465;&#19981;&#35201;&#12290;**
&#30456;&#24403;&#20110;&#23558;&#32858;&#21512;&#32467;&#26524;&#20316;&#20026;&#31532;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#25454;&#28304;&#65292;&#20363;&#22914;&#65292;**&#31579;&#36873;f1s=e  AND f2s =b&#30340;&#32467;&#26524;&#65292;&#24819;&#24471;&#21040;&#30340;&#26159;&#19978;&#36848;&#19977;&#20013;00002 &#36825;&#19968;&#34892;&#25968;&#25454;**&#12290;
**&#35831;&#38382;&#38656;&#35201;&#24590;&#26679;&#22312;&#19978;&#36848;json&#19978;&#21152;&#19978;&#31579;&#36873;&#26465;&#20214;&#65292;&#35874;&#35874;&#65281;**
{
  "aggregations": {
    "orderid": {
      "terms": {
        "field": "orderid"
      },
      "aggregations": {
        "f2s": {
          "terms": {
            "field": "f2"
          }
        },
        "f1s": {
          "terms": {
            "field": "f1"
          }
        },
        "tmax": {
          "max": {
            "field": "time"
          }
        },
        "tmin": {
          "min": {
            "field": "time"
          }
        }
      }
    }
  }
}
</description><key id="118776471">14995</key><summary>##&#22914;&#20309;&#23545;aggregations&#32858;&#21512;&#32467;&#26524;&#22312;&#36827;&#34892;&#26465;&#20214;&#26597;&#35810;&#65292;&#36807;&#28388;&#20986;&#31526;&#21512;&#26465;&#20214;&#30340;buck&#65292;&#38024;&#23545;es2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zqr688</reporter><labels /><created>2015-11-25T07:06:26Z</created><updated>2015-11-25T07:45:47Z</updated><resolved>2015-11-25T07:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xzer" created="2015-11-25T07:33:58Z" id="159525248">&#36825;&#37324;&#19981;&#26159;&#23547;&#27714;&#24110;&#21161;&#30340;&#22320;&#26041;&#65292;&#36825;&#37324;&#26159;&#25253;&#21578;bug&#30340;&#22320;&#26041;&#65292;&#22914;&#26524;&#20320;&#38656;&#35201;&#24110;&#21161;&#65292;&#24212;&#35813;&#21435;&#35770;&#22363;&#32780;&#19981;&#26159;&#36825;&#37324;&#12290;&#32780;&#19988;&#65292;&#36825;&#37324;&#30340;&#20132;&#27969;&#35821;&#35328;&#26159;&#33521;&#35821;&#65292;&#32780;&#19981;&#26159;&#20013;&#25991;&#12290;

This place is for bug reporting rather than usage help, if you need help, you should go to the forum. And also the communication language of this place is English rather than Chinese.
</comment><comment author="dadoonet" created="2015-11-25T07:39:31Z" id="159525919">Yes. Please use https://discuss.elastic.co/c/in-your-native-tongue and choose your favorite language there :) 
</comment><comment author="zqr688" created="2015-11-25T07:45:47Z" id="159526666">OK&#65292;Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_ttl filed document a configure line is error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14994</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-ttl-field.html

default TTL this chapter
PUT my_index
{
  "mappings": {
    "my_type": {
      "_ttl": {
        "enabled": true,
        "defaut": "5m"
      }
    }
  }
}

default is writed "defaut"
please update it ,thank you
</description><key id="118754011">14994</key><summary>_ttl filed document a configure line is error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whybangbang</reporter><labels /><created>2015-11-25T03:42:30Z</created><updated>2015-11-25T04:01:40Z</updated><resolved>2015-11-25T03:57:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-25T04:01:40Z" id="159485579">Thanks for noticing and reporting this. This has been fixed in b6da075505e656e0944807b0025af68a32eb61ed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe nodes should apply cluster state updates in batches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14993</link><project id="" key="" /><description>This commit applies the general mechanism for applying cluster state updates in batches to tribe nodes.

Relates #14899, relates #14725
</description><key id="118741589">14993</key><summary>Tribe nodes should apply cluster state updates in batches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Tribe Node</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-25T01:43:56Z</created><updated>2015-12-16T17:31:35Z</updated><resolved>2015-12-16T17:10:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-25T01:45:04Z" id="159461026">@bleskes I'll rebase this pull request on master when #14899 is reintegrated there. The salient commit for this review is thus 476ab3c91b038f5a328bf9360c87e4ee792643d0 pending #14899 (all the changes for that commit are in [TribeService.java](https://github.com/elastic/elasticsearch/pull/14993/files#diff-3811b0f0f0cde049775cc366e74139a7L23)).
</comment><comment author="jasontedor" created="2015-11-26T14:28:13Z" id="159927446">@bleskes I've rebased this pull request on the latest changes from #14899.
</comment><comment author="jasontedor" created="2015-12-01T15:33:56Z" id="161003001">@bleskes This pull request has been rebased on master since #14899 has been integrated there.
</comment><comment author="bleskes" created="2015-12-03T15:15:42Z" id="161671433">I think we can go further with this one. Now it just saves on cluster state update task, but the work done is the same. I think each executor can pick the latest task (double check that batching maintains order, it should :)) and only apply that. 

We could in theory  also  have a global executor (i.e., for all clients) which takes the tasks, groups them by source cluster and applies them to a shared builder but then we run the risk of exceptions in one client blocking updates from all client. I don't think it's worth it.
</comment><comment author="jasontedor" created="2015-12-14T20:56:45Z" id="164556935">&gt; I think we can go further with this one.

@bleskes Addressed in 74adaca4cebd9c0fde32a5080d7cfbbf2dbe9746.
</comment><comment author="jasontedor" created="2015-12-15T15:08:47Z" id="164792166">@bleskes In addition to 74adaca4cebd9c0fde32a5080d7cfbbf2dbe9746, I also pushed 479e50adb26b2f5453841273c05cdb17c530065e to only return a new cluster state instance in the tribe service if there were changes to the cluster state.
</comment><comment author="bleskes" created="2015-12-16T14:52:12Z" id="165131841">LGTM. Double checking - do we have a test that makes sure tasks are passed in order?
</comment><comment author="jasontedor" created="2015-12-16T14:58:03Z" id="165133283">&gt; LGTM. Double checking - do we have a test that makes sure tasks are passed in order?

I don't think so; should we address that in a separate issue?
</comment><comment author="bleskes" created="2015-12-16T15:14:30Z" id="165137279">&gt; I don't think so; should we address that in a separate issue?

++ on another issue.. It's an important semantics. We should test it under heavy concurrency. 
</comment><comment author="jasontedor" created="2015-12-16T16:01:54Z" id="165155995">&gt; ++ on another issue.. It's an important semantics. We should test it under heavy concurrency.

I opened #15483 to track this.
</comment><comment author="jasontedor" created="2015-12-16T17:31:35Z" id="165185409">Integrated to master in 709740efd2dac87f58888c7ef00937a7b2e20776 and backported to 2.x in 8fa5c68d6a245bff864ad6edecfed6b1c3227a34.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to specify extra configuration files for integ test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14992</link><project id="" key="" /><description>This change allows copy extra files into the integ test cluster before
it runs. However, it explicitly forbids overwriting elasticsearch.yml,
since that is generated.
</description><key id="118724972">14992</key><summary>Add ability to specify extra configuration files for integ test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T23:21:20Z</created><updated>2015-11-24T23:23:42Z</updated><resolved>2015-11-24T23:23:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-24T23:23:20Z" id="159437439">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add meta processor that allows to modify the metadata attributes of document being processed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14991</link><project id="" key="" /><description>This also adds template support only for the new `meta` processor. I opened #14990 to discuss how template support should further be adopted in ingest.

PR for #14644
</description><key id="118722504">14991</key><summary>[Ingest] Add meta processor that allows to modify the metadata attributes of document being processed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-24T23:03:09Z</created><updated>2015-11-26T14:47:30Z</updated><resolved>2015-11-26T14:47:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-26T11:08:46Z" id="159884083">left a few comments, I like it!
</comment><comment author="javanna" created="2015-11-26T14:35:58Z" id="159928888">left two super minor comments, LGTM though, no need for another review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Pipeline template usage </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14990</link><project id="" key="" /><description>This issue's purpose is to collect ideas on mustache template usage in pipelines. The mustache initial template support will be added via the `meta` processor (#14991), but it leaves adoption of templates by other processors open. 

Open questions:
- What other processors and settings should be templatable? If some options are just strings and some are templates then how should ingest know this at pipeline parse time? Do we need a template json object that distinguish between a template or regular string? Or any string that contains `{{` and `}}` is considered a template. The latter makes things less verbose IMO.
- How should we refer to the original document in the pipeline? Right now in the meta processor the source of the document being processed is directly accessible in the moustache template, but maybe we want to make the the source available under the `_source` prefix? (e.g. `_source.field1` or `_source.field1.field2` etc.) This allows to for example put metadata attributes of document being processed under a different prefix. Also do we want like the update api use the `ctx` prefix in front of `_source`?
- It isn't possible the access array/list elements in the current mustache implementation. I think we want to add support for that?
</description><key id="118722481">14990</key><summary>[Ingest] Pipeline template usage </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2015-11-24T23:02:58Z</created><updated>2016-01-11T17:07:23Z</updated><resolved>2016-01-11T17:07:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-27T17:39:50Z" id="160180999">About how to refer to _source and metadata: There should be no need to add special variable prefixes in templates like: `ctx.*` or `ctx._source.*`. A regular field name in ES can't have the same name is metadata (like `_index`, `_id` or `_routing`)

So referring to routing can just be done like this: `some string {{_routing}}` and a normal field  is just `some string {{my_field}}`. We can just use the convention that already exists in ES' mapping.

For ingest specific pipeline variables we can add a special `_ingest` field to document while it is in the pipeline. So the ingest timestamp can just be accessed like this in templates: `some string {{_ingest.ingest_timestamp}}`. The `_ingest` field will only be available during the pipeline execution and will not end up in the actual index request that is being send to the ES. (only ES metadata will end up being added to the index request send to the index / bulk api)
</comment><comment author="martijnvg" created="2015-12-08T09:27:23Z" id="162826373">The plan forward with templating:
- Since many processors will allow templating for a subset of settings there should be template infrastructure for this. The idea is now that `IngestDocument` will accept a value type instead of a map of maps. This value type can be a simple string, map of maps, templated string or a map of maps with a bunch of templated values. A processor implementation is responsible for providing the right value type implementation. So if tempating is allowed for an specific setting and an templated string is provided then the value type impl makes sure that when the value type is going to be resolved into its native representation the templates string gets resolved too.
- Both template config keys and values can be temptable, but this is up to the processor. 
- The syntax in previous comment will be used. There is a conflict with the `_ingest` field that will hold pipeline specific variables, because there can also be a field in the _source named `_ingest`. In ES only meta data fields are disallowed to be used in the source of a document. In any case if `_ingest.*` is being used in a template then we will assume is referred to pipeline metadata. If there is also a field with the name `_ingest` it has to be prefixed with `_source`. In all cases the `_source` prefix can be prepended to refer to any field in the source. Since `_source` is also a meta field in ES this can never be used as a field in the source itself. So we can safely use it as prefix for referencing fields in the source of a document.
</comment><comment author="javanna" created="2016-01-11T17:07:23Z" id="170620391">This has been implemented
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After upgrade 2.0.0 -&gt; 2.1.0 NoSuchFileException while deleting tlog files on *every* shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14989</link><project id="" key="" /><description>After upgrading a single node elasticsearch installation with 3 indexes, on startup Elasticsearch produces a message in the log file like the following:

```
[2015-11-24 22:36:38,840][WARN ][index.translog           ] [PANGAEA Elasticsearch] [portals_v1][1] failed to delete temp file /pangaea/elasticsearch/data/pangaea/nodes/0/indices/portals_v1/1/translog/translog-1417903832395804337.tlog
java.nio.file.NoSuchFileException: /pangaea/elasticsearch/data/pangaea/nodes/0/indices/portals_v1/1/translog/translog-1417903832395804337.tlog
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

You get one message per shard. The file really does not exist! The index works fine afterwards.

If you restart elasticsearch again the same errors occur again, but with different filenames.

Operating system is Ubuntu 14.04; debian package installation:
root@pangaea-mw1:~# uname -a
Linux pangaea-mw1 3.13.0-66-generic #108-Ubuntu SMP Wed Oct 7 15:20:27 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

This is not a blocker for me because everything works and I don't care about tranlog, but users that rely on it working may have problems.

All indexes were recreated a week ago with 2.0.0.
</description><key id="118720264">14989</key><summary>After upgrade 2.0.0 -&gt; 2.1.0 NoSuchFileException while deleting tlog files on *every* shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">uschindler</reporter><labels /><created>2015-11-24T22:47:22Z</created><updated>2016-02-05T07:11:25Z</updated><resolved>2015-11-25T10:26:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-25T10:15:55Z" id="159562055">my bad I guess, I think this is a side-effect of https://github.com/elastic/elasticsearch/pull/14695 I will look. It's shouldn't be a problem but the message is annoying!  sorry for the noise
</comment><comment author="s1monw" created="2015-11-25T10:26:14Z" id="159564052">this has already been fixed here https://github.com/elastic/elasticsearch/pull/14872
</comment><comment author="uschindler" created="2015-11-25T11:04:26Z" id="159572827">OK thanks @s1monw,
I was not sure if this is a real "error" and may cause corrumption so I better opened an issue (@rmuir suggested this). The log level was warning anyway, but a stack trace always alerts you.
</comment><comment author="s1monw" created="2015-11-25T11:20:31Z" id="159576514">yeah no worries - thanks for opening the issue
</comment><comment author="kirrg001" created="2015-11-26T15:01:48Z" id="159933630">same here

[2015-11-26 15:01:25,607][WARN ][index.translog           ] [Marrow] [staging-stats-2015.09.20][4] failed to delete temp file /data/elasticsearch/elasticsearch/nodes/0/indices/test/4/translog/translog-4738663133382677499.tlo
</comment><comment author="damm" created="2015-11-28T08:52:31Z" id="160265096">I get this; however it never recovers? 
</comment><comment author="uschindler" created="2015-11-28T10:28:54Z" id="160271757">The messages are repeated on every server restart, so yes it does not recovers from that state. According to @s1monw and the [WARN] message this is just a warning and does not affect your index. So just wait for update #14872 to get released.
</comment><comment author="damm" created="2015-11-28T21:06:47Z" id="160335534">Ouch I hope 2.1.1 hits soon then
</comment><comment author="Namita26" created="2015-11-30T10:32:58Z" id="160590652">Get the similar error! Should I wait for next release ?

[2015-11-30 15:54:05,217][WARN ][index.translog           ] [Miguel O'Hara] [questions][2] failed to delete temp file /Users/kechit/softwares/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/questions/2/translog/translog-1286082166987208532.tlog
java.nio.file.NoSuchFileException: /Users/kechit/softwares/elasticsearch-2.1.0/data/elasticsearch/nodes/0/indices/questions/2/translog/translog-1286082166987208532.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1126)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
</comment><comment author="clintongormley" created="2015-11-30T11:12:34Z" id="160600703">@Namita26 this log message is harmless. just ignore it
</comment><comment author="damm" created="2015-11-30T19:41:02Z" id="160736698">That's the confusing part is I've actually had failures with that same message... it should be harmless but it doesn't seem to be? 
</comment><comment author="Namita26" created="2015-12-01T05:45:53Z" id="160859645">Cool. I was able to do everything what I wanted even if this warning was coming! So :) 
</comment><comment author="masifpak" created="2015-12-04T07:34:58Z" id="161898870">My log file dbrdsa.log.2015-12-03 filled upto 160GB. Can it be deleted automatically otherwise it will occupy my whole storage.
</comment><comment author="kgignatyev" created="2015-12-13T17:45:10Z" id="164280192">Noisy logs are not good, please consider making non-critical issues less visible, otherwise it is hard to distinguish between real issues and harmless notes
</comment><comment author="dnltsk" created="2016-02-04T22:06:03Z" id="180072570">I am running 2.1.0 on CentOS and have the same issue. All I do is using the _bulk API 10mb packages and querying via GET once a minute. But the reported error is real: the translog folders are growing because clean-up job is failing.
see "du -h" command

```
4,0K    ./_state
66M ./2/index
34G ./2/translog
4,0K    ./2/_state
34G ./2
133M    ./3/index
8,0K    ./3/translog
4,0K    ./3/_state
133M    ./3
132M    ./1/index
34G ./1/translog
4,0K    ./1/_state
34G ./1
53M ./0/index
34G ./0/translog
4,0K    ./0/_state
34G ./0
132M    ./4/index
8,0K    ./4/translog
4,0K    ./4/_state
132M    ./4
101G    .
```

the elasticsearch logfiles are full of messages with the following pattern:

```
[2016-02-04 22:00:21,828][WARN ][cluster.action.shard     ] [Ruby Thursday] [my_application][1] received shard failed for [my_application][1], node[H6mW8DMZTJ2wENc013Q42A], [P], v[119], s[INITIALIZING], a[id=0GK9JCovTresNYzyj0y7Cw], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-04T21:53:42.844Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-30.tlog]; nested: EOFException[read requested past EOF. pos [44120241] end: [44120241]]; ]], indexUUID [rofptGBoSLuIrNqkyBpvNA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-30.tlog]; nested: EOFException[read requested past EOF. pos [44120241] end: [44120241]]; ]
[my_application][[my_application][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-30.tlog]; nested: EOFException[read requested past EOF. pos [44120241] end: [44120241]];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [my_application][[my_application][1]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-30.tlog]; nested: EOFException[read requested past EOF. pos [44120241] end: [44120241]];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:178)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    ... 5 more
Caused by: [my_application][[my_application][1]] EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-30.tlog]; nested: EOFException[read requested past EOF. pos [44120241] end: [44120241]];
    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:175)
    ... 11 more
Caused by: ElasticsearchException[unexpected exception reading from translog snapshot of /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-30.tlog]; nested: EOFException[read requested past EOF. pos [44120241] end: [44120241]];
    at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:102)
    at org.elasticsearch.index.translog.TranslogReader.access$000(TranslogReader.java:46)
    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:297)
    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)
    at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
    ... 12 more
Caused by: java.io.EOFException: read requested past EOF. pos [44120241] end: [44120241]
    at org.elasticsearch.index.translog.ImmutableTranslogReader.readBytes(ImmutableTranslogReader.java:79)
    at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:91)
    ... 17 more
[2016-02-04 22:00:21,948][WARN ][index.translog           ] [Ruby Thursday] [my_application][1] failed to delete temp file /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-2956456055072727048.tlog
java.nio.file.NoSuchFileException: /var/lib/elasticsearch/my_elasticsearch/nodes/0/indices/my_application/1/translog/translog-2956456055072727048.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1126)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="dnltsk" created="2016-02-05T07:11:25Z" id="180231727">Aright. An upgrade from 2.1.0 to 2.1.1 fixed it! 
Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Copy rest specs for plugins that are added to integ test cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14988</link><project id="" key="" /><description>This change allows having rest tests using the api spec of plugins that
the rest test is testing against.
</description><key id="118718570">14988</key><summary>Copy rest specs for plugins that are added to integ test cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T22:35:53Z</created><updated>2015-11-24T22:38:43Z</updated><resolved>2015-11-24T22:38:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-24T22:38:19Z" id="159428528">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot API: Use local time zone in _snapshot output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14987</link><project id="" key="" /><description>In the Snapshot API, would it be possible to show local time instead of Zulu time?

```
GET _snapshot/woodfield-repo/_all

{
  "snapshots": [
    {
      "snapshot": "snapshot_1",
      "version_id": 1070399,
      "version": "1.7.3",
      "indices": [
        "logstash-2015.11.10"
      ],
      "state": "SUCCESS",
      "start_time": "2015-11-10T22:09:42.135Z",
      "start_time_in_millis": 1447193382135,
      "end_time": "2015-11-10T22:09:42.884Z",
      "end_time_in_millis": 1447193382884,
      "duration_in_millis": 749,
      "failures": [],
      "shards": {
        "total": 3,
        "failed": 0,
        "successful": 3
      }
    }
  ]
}
```
</description><key id="118704032">14987</key><summary>Snapshot API: Use local time zone in _snapshot output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">inqueue</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>enhancement</label></labels><created>2015-11-24T21:14:05Z</created><updated>2016-02-15T10:49:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="inqueue" created="2015-11-24T21:16:30Z" id="159407389">customer feature request
</comment><comment author="clintongormley" created="2016-02-14T17:44:32Z" id="183936885">Of course, a client in Europe requesting info from a server in the US may well want the client's local time zone, instead of the server's time zone.  Perhaps time zone could be passed in as a parameter instead.
</comment><comment author="bleskes" created="2016-02-15T10:49:55Z" id="184164391">If we do anything here we should be indeed be explicit and allow the user to specify the time zone we will to use to render date fields. That said IMO we should just stick to using UTC for everything and let any client layer/UI  do the translation, at least for the moment. This a rabbit hole as we all know. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify adding plugins that are another project in the build</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14986</link><project id="" key="" /><description>The current mechanism for adding plugins to the integTest cluster is to
have a FileCollection. This works well for the integTests for a single
plugin, which automatically adds itself to be installed. However, for qa
tests where many plugins may be installed, and from other projects, it
is cumbersome to add configurations, dependencies and dependsOn
statements over and over. This simplifies installing a plugin from
another project by moving this common setup into the cluster
configuration code.
</description><key id="118700655">14986</key><summary>Simplify adding plugins that are another project in the build</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T20:53:54Z</created><updated>2015-11-24T21:09:40Z</updated><resolved>2015-11-24T21:09:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-24T21:01:10Z" id="159403812">looks good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Does NodeJS client support(Node Client) like Java client ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14985</link><project id="" key="" /><description>Hi,
Does NodeJS client support automatically traffic routing to the node need to process the request like, Java client does (node client).
In Java client, application know which node need to process the request, so reduce network hop from two to only one. Does this feature exist in NodeJS client ?

Thank.
</description><key id="118699686">14985</key><summary>Does NodeJS client support(Node Client) like Java client ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sunchanras</reporter><labels /><created>2015-11-24T20:48:20Z</created><updated>2015-11-28T16:56:38Z</updated><resolved>2015-11-28T16:56:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T16:56:38Z" id="160318428">This is the right repo for the JS client: https://github.com/elastic/elasticsearch-js
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What is the best way to compress a snapshot data?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14984</link><project id="" key="" /><description>We have a lot of data in our daily indexes (like 200GB), so what is the best way to compress a daily index snapshot. As we need to store some of them up to 6 month due to legal issues.

May be it worth adding snapshot data compression along with metadata?
</description><key id="118695563">14984</key><summary>What is the best way to compress a snapshot data?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">notxcain</reporter><labels /><created>2015-11-24T20:24:12Z</created><updated>2015-12-01T07:19:42Z</updated><resolved>2015-11-28T16:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T16:55:46Z" id="160318359">Hi @notxcain 

The best place to ask questions like these is the forum: http://discuss.elastic.co/
</comment><comment author="notxcain" created="2015-12-01T07:19:42Z" id="160880568">Ok! Here it is https://discuss.elastic.co/t/what-is-the-best-way-to-compress-a-snapshot-data/35990
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set soft limit on the number of nested fields per index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14983</link><project id="" key="" /><description>Today dynamic templates can be used to eg. dynamically map objects to "nested" fields. This is quite dangerous as nested fields limit the scalability of indices. For instance an index that has 1M docs with 100 nested docs each requires as much resources as a 100M docs index. By forbidding dynamic mappings on object fields, we would at least ensure that all the nested mappings are explicit.
</description><key id="118692049">14983</key><summary>Set soft limit on the number of nested fields per index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label></labels><created>2015-11-24T20:06:09Z</created><updated>2016-04-06T07:41:53Z</updated><resolved>2016-01-19T09:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-27T10:23:32Z" id="160106656">Discussed in FixItFriday, decided:
- Need to add documentation explaining the costs incurred by nested fields
- Add a per-index soft limit (safeguard, see #11511) on the number of nested fields
</comment><comment author="ywelsch" created="2015-12-11T13:55:40Z" id="163941624">I discussed this with @jpountz, and the current implementation of mappings would need this check to be baked into the implementation of simulating the merging of mappings, which makes it unnecessarily complicated. At the moment, work is under way to make this easier (separate merge and update of mappings, see #15313), so we would only need to check the limit on the merged mapping before updating it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent result from _cat/aliases vs actual state of the indices.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14982</link><project id="" key="" /><description>This was briefly describe in this issue #14818 
When I call `GET _cat/aliases` or `GET _aliases` I got all the alias associated with an open index, if later I associated the same alias with an closed index, if I try to call the alias I get a `index_closed_exception`, even when is closed, I understand this, but, is that the desire behavior to the `GET _cat/aliases`  or `GET _aliases` endpoints (to just show the alias associated with an open index)?

P.S: I said is inconsistent because is not showing all the indices with an alias.
</description><key id="118689326">14982</key><summary>Inconsistent result from _cat/aliases vs actual state of the indices.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">abrahamduran</reporter><labels><label>:Aliases</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-11-24T19:50:19Z</created><updated>2016-01-14T13:24:40Z</updated><resolved>2016-01-14T13:24:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T16:55:03Z" id="160318294">This has been made consistent in 2.0.  Closing
</comment><comment author="abrahamduran" created="2015-11-28T17:21:06Z" id="160321887">You may be wrong, because I'm using ES 2.0 and I still see this issue, consider reopening or letting me know if this was solved on 2.1 exactly, thanks.
</comment><comment author="clintongormley" created="2015-11-28T17:54:12Z" id="160324356">@AIsaac08 I ran the reproduction shown in #14818 and as @ppf2 said, saw that it had been fixed in 2.0.  If you are seeing something different, please provide a recreation that I can run.
</comment><comment author="clintongormley" created="2015-12-19T12:32:53Z" id="165981661">I've just reread this issue and realised i misunderstood the initial description.  Today, the cat-aliases and GET alias/es requests only return open indices, but a search request against an alias will throw a closed-index exception.

I think it was a mistake to only show open indices in the aliases requests.  We should show all indices associated with an alias.

There has been a suggestion to remove any aliases automatically when closing an index, but I think this is not the correct behaviour.  If a user wants this, then they can simply `DELETE {index}/_alias/*` before closing the index.
</comment><comment author="abrahamduran" created="2015-12-21T15:34:31Z" id="166332156">I have the same thought, if a user wants to remove an alias, the user should explicitly remove the alias. I would prefer that the aliases requests should show all indices with an alias/es associated to them.
</comment><comment author="javanna" created="2016-01-13T10:41:48Z" id="171250976">Note that `GET alias/_alias` will return all indices, including the closed ones, while `GET _aliases` and `GET _cat/aliases` will only return the open ones. This behaviour doesn't seem to be configurable, I think we should make this consistent and return all indices in all cases.
</comment><comment author="clintongormley" created="2016-01-13T11:31:20Z" id="171260893">I agree with making `_cat/aliases` consistent. There is this issue (https://github.com/elastic/elasticsearch/issues/15817) for removing the `GET _aliases` API.
</comment><comment author="javanna" created="2016-01-13T11:37:46Z" id="171262512">I think my comment was a bit premature :) I did some more digging and it turns out that the `expand_wildcards` option can be used in all cases to control what gets returned. This is because this api allows to specify index names as well, if none are specified that is considered a wildcard expression.

I also stand corrected on what I said above, let me summarize the current behaviour:

`GET _alias/alias` returns open only indices
`GET _alias/alias?expand_wildcards?open,closed` returns all indices

Unfortunately the cat api doesn't support `expand_wildcards`, not sure whether we should expose it or change it to always return all indices.

Also we may want to change the default behaviour of `_alias` and `_aliases` endpoint.
</comment><comment author="clintongormley" created="2016-01-13T11:43:20Z" id="171263345">Hmm OK - thanks for the update @javanna.  I think that `GET _alias` and `_cat/aliases` should both return closed indices by default (ie `expand_wildcards=open,closed`).  Perhaps `_cat/aliases` could be expanded to include an indication of whether the index is open or closed, but it probably doesn't need to support the `expand_wildcards` option.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check that the declared versions match the backward indices.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14981</link><project id="" key="" /><description>This adds assertions to OldIndexBackwardsCompatibilityIT so that we get a failure if either the elasticsearch or the lucene version don't match what the cluster state/index declare.
</description><key id="118687716">14981</key><summary>Check that the declared versions match the backward indices.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>review</label><label>test</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T19:41:53Z</created><updated>2015-11-25T15:56:33Z</updated><resolved>2015-11-25T09:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-24T19:53:46Z" id="159387050">Ah good check! +1
</comment><comment author="nik9000" created="2015-11-24T19:59:40Z" id="159388465">LGTM
</comment><comment author="rmuir" created="2015-11-24T20:01:04Z" id="159388816">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better documentation for enabling slowlogs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14980</link><project id="" key="" /><description>Currently we mention that slowlogs can be enabled dynamically, and we have an example of how to enable them in the configuration. However it is much useful to have an example to how to enable it dynamically per index.

Looks like we have added more information that this is a **per index settings** which is great, but having examples for this will be better since for production environment we don't wan't to stop nodes to enable it.
</description><key id="118684981">14980</key><summary>Better documentation for enabling slowlogs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Logging</label><label>adoptme</label><label>docs</label></labels><created>2015-11-24T19:26:58Z</created><updated>2016-09-28T19:29:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mahdibh" created="2016-09-28T19:29:14Z" id="250273547">It would also be great to clarify what exactly is the latency that the slow log reports. Is that total time since the query was received (ie, including time sitting in the queue) or is that purely the time it took once a search thread picked up the request from the queue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unallocated shards because of "index.auto_expand_replicas":"0-all" and "cluster.routing.allocation.same_shard.host": true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14979</link><project id="" key="" /><description>Setting both options `"index.auto_expand_replicas":"0-all"` and `"cluster.routing.allocation.same_shard.host": true` causes unallocated shards in a cluster. 

E.g. in a cluster with 5 physical hosts and 10 Elasticsearch instances (2 instances/host) the settings cause 5 unallocated shards per an index.

Is this the correct behavior?
Thanks!

P.S. Tested on Elasticsearch-1.4.4
</description><key id="118682824">14979</key><summary>Unallocated shards because of "index.auto_expand_replicas":"0-all" and "cluster.routing.allocation.same_shard.host": true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">slovit</reporter><labels /><created>2015-11-24T19:15:31Z</created><updated>2015-11-28T11:29:44Z</updated><resolved>2015-11-27T15:52:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-27T15:52:12Z" id="160165090">This is the expected behavior. `auto_expand_replicas` expands the number of replicas based on the number of available **nodes** (which are Elasticsearch instances, not physical hosts). It does not take any other allocation constraints into account. Please use https://discuss.elastic.co for future questions of this kind.
</comment><comment author="clintongormley" created="2015-11-28T11:29:44Z" id="160279332">Duplicate of https://github.com/elastic/elasticsearch/issues/2869
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Crating index with setting and mapping  v2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14978</link><project id="" key="" /><description>Crating index with setting and mapping  in version 2.0 throws following error while in version 1.7 everything is fine. the same command

```
"error": {
    "root_cause": [
      {
        "type": "parse_exception",
        "reason": "malformed, expected settings to start with 'object', instead was [VALUE_STRING]"
      }
    ],
```
</description><key id="118681701">14978</key><summary>Crating index with setting and mapping  v2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">1amirjalai</reporter><labels><label>feedback_needed</label></labels><created>2015-11-24T19:09:16Z</created><updated>2016-02-14T17:42:36Z</updated><resolved>2016-02-14T17:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-24T19:11:21Z" id="159376156">Well you need at least to explain what you did.
</comment><comment author="1amirjalai" created="2015-11-24T19:14:18Z" id="159376925">I send a request like this for creating index

Url: http://xxxxxxx:9200/xxxxxx, 
    Request: {
  "settings": {
       "index": {
      "analysis": {
        "analyzer": {
          "autocomplete": {
            "tokenizer": "standard",
            "filter": [
              "lowercase",
              "autocompletefilter"
            ],
            "type": "custom"
          },
.
.
.
.
.

.
.

.
"mappings": {
    "ppppp": {
      "properties": {
        "productId": {
          "type": "integer"
        },

.
...
.
.
.
.
</comment><comment author="dadoonet" created="2015-11-24T19:25:26Z" id="159379819">It sounds good. But I can't really tell if I can not reproduce with a concrete example.
I totally understand that you don't want to send all details which is perfect but could create a reproduction script which would help to understand what is happening?

In case you missed it, also read https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking-changes-2.0.html. May you are doing something which is not allowed anymore.
</comment><comment author="clintongormley" created="2015-11-28T16:51:48Z" id="160318162">We are now stricter about not ignoring malformed JSON or incorrect parameters.  Without the actual request that you sent, this will be difficult to diagnose.
</comment><comment author="clintongormley" created="2016-02-14T17:42:36Z" id="183936696">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a test that upgrades succeed even if a mapping contains fields that come from a plugin.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14977</link><project id="" key="" /><description>Relates to #14828
</description><key id="118674293">14977</key><summary>Add a test that upgrades succeed even if a mapping contains fields that come from a plugin.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>review</label><label>test</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T18:31:00Z</created><updated>2015-11-25T16:01:03Z</updated><resolved>2015-11-25T09:49:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-24T19:48:52Z" id="159385885">LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is it possible to stream translog changes out of Elasticsearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14976</link><project id="" key="" /><description>I posted [this question](http://stackoverflow.com/questions/33860130/is-there-a-way-to-stream-translog-changes-out-of-elasticsearch) to Stack Overflow the other day and have gotten no responses:

&gt; In some systems, there are ways to hook in to write-ahead log changes as they happen. For example, in HBase one can write a [coprocessor](https://hbase.apache.org/testdevapidocs/org/apache/hadoop/hbase/coprocessor/SampleRegionWALObserver.html) to stream WAL edits out of the database as they happen. [A extensions for PostgreSQL](https://github.com/confluentinc/bottledwater-pg) exist that lets you do the same thing.
&gt; 
&gt; I've Googled and I've not found a way to harness the Elasticsearch translog. I've looked at the plugin APIs but not found anything that lets you get at the translog. Is there a way?
</description><key id="118669165">14976</key><summary>Is it possible to stream translog changes out of Elasticsearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dminkovsky</reporter><labels /><created>2015-11-24T18:00:42Z</created><updated>2015-11-24T20:25:07Z</updated><resolved>2015-11-24T20:22:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-24T18:57:48Z" id="159372486">@dminkovsky this is on our road map, tracked by this (old) issue: https://github.com/elastic/elasticsearch/issues/1242

It's blocking on the work done in #10708 

Closing as duplicate.
</comment><comment author="dminkovsky" created="2015-11-24T20:25:07Z" id="159395282">Thank you Boaz.

On Tuesday, November 24, 2015, Boaz Leskes notifications@github.com wrote:

&gt; Closed #14976 https://github.com/elastic/elasticsearch/issues/14976.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14976#event-473749783.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Missing Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14975</link><project id="" key="" /><description /><key id="118658680">14975</key><summary>Aggregations Refactor: Refactor Missing Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-24T17:09:54Z</created><updated>2015-11-25T09:12:53Z</updated><resolved>2015-11-25T09:06:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-24T19:44:05Z" id="159384528">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Filter Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14974</link><project id="" key="" /><description /><key id="118642952">14974</key><summary>Aggregations Refactor: Refactor Filter Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-24T16:09:18Z</created><updated>2015-11-24T18:21:14Z</updated><resolved>2015-11-24T18:21:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-24T16:10:24Z" id="159317206">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for headers in REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14973</link><project id="" key="" /><description>This adds support for arbitrary headers sent with each REST request, it
will allow us to test things like different xcontent-encoding (see
50_with_headers.yaml for what this looks like).

Headers are specified at the same level as `catch`, so a request would
look like:

``` yaml
- do:
    headers:
      Content-Type: application/yaml
    get:
      index: test_1
      type:  _all
      id:    1
```
</description><key id="118635914">14973</key><summary>Add support for headers in REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>review</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T15:37:12Z</created><updated>2015-11-29T11:40:36Z</updated><resolved>2015-11-24T18:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-24T17:54:03Z" id="159355296">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix gradle to populate plugin list for bats tests?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14972</link><project id="" key="" /><description>I always forget this one when adding/backporting a plugin: latest instance https://github.com/elastic/elasticsearch/pull/14970

One of the pain points with maven was adding plugins/jars, you had to modify the build in like 87 different places to update various lists. I think we should try to reduce this in general. I know we can improve it, since for example qa/smoke-test-plugins no longer has a hardcoded list and instead just "includes all plugins".

There are a few other places with this redundancy too: the help file for pluginmanager, also one of its tests has the same list of plugins and we should think about those too.

As far as the bats test goes, one trick i see is that it needs to behave differently if the plugin is just a site plugin.
</description><key id="118634230">14972</key><summary>fix gradle to populate plugin list for bats tests?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>build</label></labels><created>2015-11-24T15:29:10Z</created><updated>2015-11-24T19:34:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-24T19:34:17Z" id="159382018">We could certainly be more automatic here, yeah.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename Data leftovers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14971</link><project id="" key="" /><description>Rename TransportData to WriteableIngestDocument and corresponding tests plus variables and methods.
</description><key id="118627746">14971</key><summary>Rename Data leftovers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-24T15:04:09Z</created><updated>2015-11-24T15:22:23Z</updated><resolved>2015-11-24T15:22:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-24T15:05:58Z" id="159294890">Thanks @javanna LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] add mapper attachments plugin to vagrant tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14970</link><project id="" key="" /><description /><key id="118627639">14970</key><summary>[test] add mapper attachments plugin to vagrant tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.2.0</label></labels><created>2015-11-24T15:03:41Z</created><updated>2015-11-24T15:27:52Z</updated><resolved>2015-11-24T15:27:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-24T15:10:22Z" id="159296728">++ thanks @brwe !
LGTM
</comment><comment author="dadoonet" created="2015-11-24T15:12:32Z" id="159297902">Wondering if that is needed for master branch as well? Or already done?
</comment><comment author="rmuir" created="2015-11-24T15:14:31Z" id="159299164">I missed it on backport: master actually does have it in its vagrant tests.
</comment><comment author="rmuir" created="2015-11-24T15:17:50Z" id="159301072">These were added in https://github.com/elastic/elasticsearch/commit/9b0a47d8e315a7bcd41606977278ead9a485581d, thanks for fixing this in 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Remove internal `translated` flag from LineStringBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14969</link><project id="" key="" /><description>The `translated` flag makes LineStringBuilder stateful and gets set to true under certain conditions when building a Shape or Geometry from the ShapeBuilder. This makes building operations not be idempotent (calling build() multiple times on a ShapeBuilder changes the original internal state). This PR fixes this by passing along a local copy of the `translated` flag that is only updated during the building process and created again on any subsequent calls to build() or buildGeometry(). Also moving some static code blocks that are only used in PolygonBuilder from ShapeBuilder.
</description><key id="118623025">14969</key><summary>Geo: Remove internal `translated` flag from LineStringBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T14:40:31Z</created><updated>2015-12-04T12:02:02Z</updated><resolved>2015-12-04T12:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-24T14:43:15Z" id="159289164">@nknize could you take a look at this if this looks right to you? I think I need this for the refactoring in #14416.
</comment><comment author="nknize" created="2015-12-04T05:24:02Z" id="161880220">One minor variable name change and LGTM!
</comment><comment author="cbuescher" created="2015-12-04T12:02:02Z" id="161950940">@nknize thanks for the review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>run bwc test also as integ test and share methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14968</link><project id="" key="" /><description>We had no itegration test before with long terms and several shards only
a bwc test.

related to #14948
</description><key id="118620064">14968</key><summary>run bwc test also as integ test and share methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-11-24T14:28:07Z</created><updated>2015-11-26T09:05:09Z</updated><resolved>2015-11-26T09:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-11-24T14:28:31Z" id="159283019">@nik9000 would you like to take a look?
</comment><comment author="nik9000" created="2015-11-24T19:37:44Z" id="159383044">Looks great! Every time I do this I have to tweak the pom to make sure the shared test method are included. Do you have to do that too? It typically comes up when I do a clean compile.
</comment><comment author="brwe" created="2015-11-25T17:06:08Z" id="159674224">I placed them in org/elasticsearch/test which is packaged (thanks @s1monw for the tip). I can put it somewhere else though and tweak the pom instead. Would that be better?
</comment><comment author="nik9000" created="2015-11-25T17:07:14Z" id="159674492">&gt; I placed them in org/elasticsearch/test which is packaged

Ah. Whatever works is fine with me!
</comment><comment author="nik9000" created="2015-11-25T17:07:30Z" id="159674601">So LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IO problem while reading files with API signatures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14967</link><project id="" key="" /><description>While working on the ingest plugin, we run tests from the `plugins/ingest` subdirectory so we don't have to execute all tests all the time. Some times we get the below error, which seems to get solved most of the times by going back to the root folder and running `gradle clean`. Some times that is not enough though, and I am still trying to figure out what causes it and how to fix it. In the stacktrace, forbidden-api complains about a missing class (`PathUtils`) in the elasticsearch jar, but that class seems to be there. Might also be that this is a forbidden-api issue, I really don't know.

```
FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':plugins:ingest:forbiddenApisMain'.
&gt; IO problem while reading files with API signatures.

* Try:
Run with --info or --debug option to get more log output.

* Exception is:
org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':plugins:ingest:forbiddenApisMain'.
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46)
    at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35)
    at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64)
    at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)
    at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52)
    at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)
    at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53)
    at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
    at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203)
    at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185)
    at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:62)
    at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50)
    at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:25)
    at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:110)
    at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37)
    at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)
    at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23)
    at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43)
    at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32)
    at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)
    at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30)
    at org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:154)
    at org.gradle.internal.Factories$1.create(Factories.java:22)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:52)
    at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:151)
    at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:32)
    at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:99)
    at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:93)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:62)
    at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:93)
    at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:82)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:94)
    at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)
    at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:43)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28)
    at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77)
    at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47)
    at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:52)
    at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)
    at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)
    at org.gradle.util.Swapper.swap(Swapper.java:38)
    at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.health.DaemonHealthTracker.execute(DaemonHealthTracker.java:47)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:66)
    at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:71)
    at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.health.HintGCAfterBuild.execute(HintGCAfterBuild.java:41)
    at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)
    at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)
    at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:246)
    at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54)
    at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40)
Caused by: org.gradle.api.resources.ResourceException: IO problem while reading files with API signatures.
    at de.thetaphi.forbiddenapis.gradle.CheckForbiddenApis.checkForbidden(CheckForbiddenApis.java:531)
    at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:227)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:220)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:209)
    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:585)
    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:568)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61)
    ... 68 more
Caused by: java.io.FileNotFoundException: JAR entry org/elasticsearch/common/io/PathUtils.class not found in /Users/lucacavanna/dev/elasticsearch/core_ingest/core/build/libs/elasticsearch-3.0.0-SNAPSHOT.jar
    at de.thetaphi.forbiddenapis.Checker.getClassFromClassLoader(Checker.java:243)
    at de.thetaphi.forbiddenapis.Checker.addSignature(Checker.java:350)
    at de.thetaphi.forbiddenapis.Checker.parseSignaturesFile(Checker.java:460)
    at de.thetaphi.forbiddenapis.Checker.parseSignaturesFile(Checker.java:431)
    at de.thetaphi.forbiddenapis.Checker.parseSignaturesFile(Checker.java:411)
    at de.thetaphi.forbiddenapis.Checker.parseSignaturesFile(Checker.java:416)
    at de.thetaphi.forbiddenapis.gradle.CheckForbiddenApis.checkForbidden(CheckForbiddenApis.java:520)
    ... 76 more
```
</description><key id="118613804">14967</key><summary>IO problem while reading files with API signatures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>build</label></labels><created>2015-11-24T13:56:26Z</created><updated>2015-11-24T23:30:11Z</updated><resolved>2015-11-24T19:41:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-24T19:31:23Z" id="159381295">Are you using the gradle daemon? This is 99% of the issues like this that I have seen: the gradle daemon gets stuck in some messed up state. Running `gradle --stop` will kill all daemons.
</comment><comment author="javanna" created="2015-11-24T19:41:09Z" id="159383857">oh that might be it, thanks for the tip. Will kill the daemon thanks @rjernst .
</comment><comment author="javanna" created="2015-11-24T19:44:13Z" id="159384573">cc/ @martijnvg since I know you were having the same problem.
</comment><comment author="uschindler" created="2015-11-24T20:57:37Z" id="159402990">See: https://github.com/policeman-tools/forbidden-apis/issues/75 and https://github.com/policeman-tools/forbidden-apis/pull/76

This is actually a bug in Grade's spyware daemon and I refuse to fix this. The hack used in this PR is just horrible. Please don't use the daemon it breaks all the time and crushes whole JVM all the time.
</comment><comment author="javanna" created="2015-11-24T21:06:23Z" id="159405020">duly noted /me disables the daemon thanks @uschindler !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Scripted Metric Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14966</link><project id="" key="" /><description /><key id="118610052">14966</key><summary>Aggregations Refactor: Refactor Scripted Metric Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-24T13:36:50Z</created><updated>2015-11-24T16:44:09Z</updated><resolved>2015-11-24T16:43:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-24T16:13:58Z" id="159318178">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for number fields in more_like_this queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14965</link><project id="" key="" /><description>more_like_this only supports string field types.  Lets suppose that I have a field "creator", which is the numerical ID of whoever created some document.  I should be able to run a more_like_this on the creator field to find other records this person has created.
</description><key id="118601362">14965</key><summary>Support for number fields in more_like_this queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels><label>:More Like This</label><label>discuss</label></labels><created>2015-11-24T12:44:57Z</created><updated>2017-04-13T20:20:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="davidklebanoff" created="2017-04-13T20:20:17Z" id="294011146">I believe this may be possible if creator has a field type of "keyword" and you have a query like:

```json
{
    "query": {
        "more_like_this" : {
            "fields" : ["creator"],
            "like": [
              {
                  "_index": "foo",
                  "_type": "bar",
                  "_id": "123"
              }
          ],
            "min_term_freq" : 1,
            "max_doc_feq" : 1,
            "minimum_should_match" : 1
        }
    }
}
```</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Persist currently started allocation IDs to index metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14964</link><project id="" key="" /><description>These allocation IDs serve as candidates to decide future primary shards

Subtask of #14739
</description><key id="118600739">14964</key><summary>Persist currently started allocation IDs to index metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T12:40:59Z</created><updated>2015-12-04T10:33:50Z</updated><resolved>2015-12-04T10:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-24T13:42:30Z" id="159270486">left minor comments LGTM otherwise
</comment><comment author="bleskes" created="2015-11-24T21:50:16Z" id="159415635">This looks good. I left some minor suggestion. Note the comment about storing allocation ids per shard in the meta data. I know the allocation Ids are unique so in theory we can throw them in one big pile, but I think it will be clearer in terms of code and API later on to have them separated. 
</comment><comment author="ywelsch" created="2015-11-25T15:03:48Z" id="159634381">Pushed a larger set of changes, please have another look @bleskes and @s1monw. I store the allocation ids now on a per shard id basis and introduced a new class that helps with the serialization / cluster diffs.
</comment><comment author="bleskes" created="2015-11-26T16:19:28Z" id="159952267">thx @ywelsch . This looks great. Left some comments here and there. One thing I think is forgotten (because it's not evident at all) is adding the allocation ids to the toXContent serialization of the cluster state.
</comment><comment author="ywelsch" created="2015-11-27T14:47:30Z" id="160153411">Pushed another set of changes. In IndexMetaData, the activeAllocationIds are now stored as `ImmutableOpenIntMap&lt;Set&lt;String&gt;&gt;`. To enable serialization for maps that have non-diffable values, I heavily refactored DiffableUtils. I also introduced the possibility to specify how key values of map should be serialized. For activeAllocationIds (map from shard id to set of allocation id), we can use VInt-based serialization of keys (as we know that shard ids are small positive numbers).
</comment><comment author="bleskes" created="2015-11-30T10:19:44Z" id="160588261">Nice one @ywelsch . Left some minor comments.
</comment><comment author="ywelsch" created="2015-11-30T15:33:41Z" id="160662506">@bleskes Another set of changes:
- Reworked DiffableTests so that it now parameterizes over the following:
  - map implementation type (JDK Map, ImmutableOpenMap, ImmutableOpenIntMap)
  - map value type (Diffable or non-diffable)
  - specific values that are added / removed. We now compare randomly-generated sets.
- Moved value extraction from IndexMetaData constructor to its builder
- `supportsDiffableValues()` is now applied after equals check
- removed unnecessary builder methods
</comment><comment author="bleskes" created="2015-12-01T09:36:21Z" id="160909537">LGTM. Left some minor suggestions (first on the commit - sorry for the noise). I would love it if @s1monw or @imotov will also look at the changes to the diff infra as the were involved in it as well. 
</comment><comment author="ywelsch" created="2015-12-01T12:13:46Z" id="160950206">pushed minor changes to address @bleskes's suggestions
- check correctness of diffmap
- randomize serialization
</comment><comment author="s1monw" created="2015-12-02T10:55:06Z" id="161257563">I will look again this afternoon... sorry for the delay
</comment><comment author="s1monw" created="2015-12-02T20:13:37Z" id="161420244">i left some cosmetic comments LGTM in general
</comment><comment author="s1monw" created="2015-12-04T09:13:10Z" id="161915271">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0 refuse to start with custom path.conf on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14963</link><project id="" key="" /><description>In elasticsearch 2.0, I'm trying to set a custom config directory on windows, but get an error message saying "Unrecognized option".
Using the syntax `--path.conf=C:\windows\system32`
According to the list of changes in comand line flags, the double-dash arguments should come at the end.(1) 
It works for me if I edit the elasticsearch.bat file and move the `!newparams!` part to the end

Changing elasticsearch.bat line 46 from 

```
"%JAVA_HOME%\bin\java" %JAVA_OPTS% %ES_JAVA_OPTS% %ES_PARAMS% !newparams! -cp "%ES_CLASSPATH%" "org.elasticsearch.bootstrap.Elasticsearch" start
```

to

```
"%JAVA_HOME%\bin\java" %JAVA_OPTS% %ES_JAVA_OPTS% %ES_PARAMS% -cp "%ES_CLASSPATH%" "org.elasticsearch.bootstrap.Elasticsearch" start !newparams!
```

[1]. https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_setting_changes.html#_command_line_flags
</description><key id="118590548">14963</key><summary>2.0 refuse to start with custom path.conf on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">babadofar</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2015-11-24T11:47:35Z</created><updated>2016-02-05T16:25:44Z</updated><resolved>2016-02-05T16:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-24T14:17:18Z" id="159279787">This is indeed a bug (see also #13991). How about using `-Des.path.conf=...` in the meanwhile?
</comment><comment author="babadofar" created="2015-11-25T08:18:56Z" id="159531492">thanks, that actually works! I thought I tried it but probably screwed up something else in the meantime;) 
</comment><comment author="clintongormley" created="2016-02-05T16:25:44Z" id="180428538">Closed by #15320
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use ObjectParser to parse AllocationID</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14962</link><project id="" key="" /><description>Completes #14831
</description><key id="118586574">14962</key><summary>Use ObjectParser to parse AllocationID</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T11:24:25Z</created><updated>2015-11-24T11:46:43Z</updated><resolved>2015-11-24T11:46:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-24T11:26:16Z" id="159236306">LGTM. Thanks @ywelsch 
</comment><comment author="s1monw" created="2015-11-24T11:36:24Z" id="159238195">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Java Client API composable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14961</link><project id="" key="" /><description>### Background / High Level Goal

While working on #14620 I noticed that the current listener-based client API does not compose, i.e. I cannot build processing pipelines based on a response future.

For example, in the Java client implementation classes, I want to write code like:

```
// 'X' is some not yet specified interface potentially extending ActionFuture 
// for backwards compatibility
public X&lt;BulkRequest&gt; bulk(BulkRequest request) {
   return Retry.withBackoff(() -&gt; client.bulk(request))
     // not important here what #doCleanup does exactly
    .onCompletion((response) -&gt; doCleanup()) 
    .onException((ex) -&gt; doCleanup());
}
```

And I want to do all that without knowing what any potential caller of the Java client API is doing. The caller should just be able to add its operations on top, i.e. we want to build a pipeline of operations, e.g.:

```
client()
  .bulk(request)
  .onCompletion((response) -&gt; updateUI())
  .onException((ex) -&gt; showErrorMessage(ex));
```
### Current Status

Chaining of listeners is currently also doable but listeners need to delegate explicitly and the code gets very tightly coupled and ugly.
### Possible Solution Approach

Starting with Java 8, the JDK provides `CompletableFuture` which allows to create processing pipelines. However, the `CompletableFuture` API is very hard to understand and I doubt we need its whole complexity for our use case.

Nevertheless, it could be worthwhile to base the internal implementation on `CompletableFuture` and just expose parts the API.
### Necessary Steps
1. Start investigating whether `CompletableFuture` is a good fit for our requirements and flesh out an updated client API (ideally without breaking backwards compatibility, even on master)
2. Deprecate `ActionListener` and all methods in the Java client API that take one as parameter (to be removed with next major version)
3. Add new methods to `ActionFuture` to allow for chaining. This way, the current API methods that return an `ActionFuture` will stay but we can get rid of all `ActionListener` methods. Deprecated methods based on `ActionListener` can be simply reimplemented internally on top of the new API.
4. Revisit implementations of ActionFuture (whether we still need them at all) and provide a new default implementation that delegates to `CompletableFuture`
5. Update the client documentation with hints on how to migrate the API
6. (As separate PRs) Adapt internal implementation to take advantage of pipelining.
</description><key id="118567419">14961</key><summary>Make Java Client API composable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-11-24T09:39:06Z</created><updated>2015-11-28T11:04:29Z</updated><resolved>2015-11-27T10:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-24T10:05:13Z" id="159215053">I think we need to make up our minds on whether and when we are going to have a java REST client that will ideally replace the java api for users, so that the java api becomes mainly internal. And whether the REST client should implement the same `Client` interface or not. Also this proposal makes me think about #9201 which is about changing the java api too, but if we are going to have a REST client, seems like these changes are relevant only if we need things internally.
</comment><comment author="danielmitterdorfer" created="2015-11-24T10:22:47Z" id="159220879">@javanna Thanks for the pointer to the other issue, I wasn't aware of that. As the other one already touches this topic (and even in a broader context), I sense it would be best to keep the discussion in #9201 and close this one.
</comment><comment author="danielmitterdorfer" created="2015-11-27T10:01:29Z" id="160102830">I'm closing this now as Luca has already pointed to #9201 and to me this is just a duplicate of that ticket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE in TransportShardBulkAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14960</link><project id="" key="" /><description>```
[2015-11-23 14:19:16,486][WARN ][action.bulk              ] [Wong] unexpected error during the primary phase for action [indices:data/write/bulk[s]]
java.lang.NullPointerException
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:112)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:370)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:120)
        at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:78)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:216)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:213)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

This is with a 4-node cluster (1 client, 3 data+master) all running ES 2.0.0 and I've noticed this logline on 2 of the 4 nodes.
I'm afraid I don't have any other context aside from this log message, but I hope that the NPE by itself is enough.
</description><key id="118564873">14960</key><summary>NPE in TransportShardBulkAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thomasmarkus</reporter><labels><label>:Bulk</label><label>adoptme</label><label>bug</label></labels><created>2015-11-24T09:22:32Z</created><updated>2016-02-14T17:41:28Z</updated><resolved>2016-02-14T17:41:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="guysoft" created="2015-11-24T15:41:46Z" id="159308104">What version of Java do you have installed? 
We had that kind of error with java 7
</comment><comment author="thomasmarkus" created="2015-11-24T15:51:30Z" id="159310741">```
{
        "version" : "1.8.0_66-internal",
        "vm_name" : "OpenJDK 64-Bit Server VM",
        "vm_version" : "25.66-b01",
        "vm_vendor" : "Oracle Corporation",
        "count" : 3
}
```
</comment><comment author="clintongormley" created="2015-11-28T16:41:45Z" id="160317638">@jimferenczi could you try to hunt this down?
</comment><comment author="bleskes" created="2015-11-29T10:10:10Z" id="160396897">I think I've seen in this in the past where an index was deleted while a bulk request was in flight. @guysoft can that be the case?
</comment><comment author="guysoft" created="2015-11-29T15:21:21Z" id="160422143">@bleskes not sure anymore, gave up on that server, installed on a fresh VM and now works.
</comment><comment author="bleskes" created="2015-11-29T17:47:49Z" id="160437343">@guysoft thanks. Are you saying it wasn't a one off thing but rather a permanent issue with that VM?
</comment><comment author="guysoft" created="2015-11-30T10:48:11Z" id="160595531">In my case, yes. I think we had some configuration settings that we missed, or added a wrong one somewhere.
</comment><comment author="clintongormley" created="2016-02-14T17:41:28Z" id="183936344">No further info - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow customizing wait condition and cluster settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14959</link><project id="" key="" /><description>The current wait condition for an integ test cluster being up is a
simple http get on the root path for elasticsearch. However, it is
useful to allow having arbitrary wait conditions. This change reworks
the wait task to first check that each node process started successfully
and has a socket up, followed by an arbitrary wait condition which
defaults to the current http get.

Also, cluster settings are allowed to be added, and overriden. Finally,
custom setup commands are made relative to the elasticsearch home dir
for each node.
</description><key id="118560051">14959</key><summary>Allow customizing wait condition and cluster settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-24T08:51:28Z</created><updated>2015-11-24T16:24:02Z</updated><resolved>2015-11-24T16:24:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-24T16:21:07Z" id="159320205">looks good. i think it might be good to be cautious and keep it at 60 waits vs 30, i don't trust that a jenkins server will be fast, or not get temporarily overwhelmed/swapping something like that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Would you append "preference" that is like search request option on ForceMergeRequest?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14958</link><project id="" key="" /><description>I think, ForceMergeRequest can be performed into each shards.
If this idea is right, I hope that you implement this option on ForceMergeRequest.

This is just my opinion, you don't need to think seriously.

Thanks.
</description><key id="118552772">14958</key><summary>Would you append "preference" that is like search request option on ForceMergeRequest?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HowookJeong</reporter><labels><label>:Index APIs</label><label>feedback_needed</label></labels><created>2015-11-24T07:53:40Z</created><updated>2015-12-02T12:02:41Z</updated><resolved>2015-12-02T12:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T16:32:56Z" id="160316807">What is the use case?
</comment><comment author="HowookJeong" created="2015-12-02T07:11:22Z" id="161204097">I usually use a force merge or _optimize because I have many kind of heavy requests which are index, delete and update by using bulk.
These operation create many segment files and deleted docs.
It is inefficient to storage and getting slower.
Also, This operation use a lot of system resources.
If the operation can request to shard level, I could save system resources and our cluster will be stable.

I think, the idea is very useful.
So, I hope you'll consider try again.

Thanks.
</comment><comment author="clintongormley" created="2015-12-02T12:02:41Z" id="161272228">Force merge is a heavy operation any way you look at it.  You should not run it on an index that is still being modified. Instead, you should leave segment maintenance up to the background merge process, which (in recent versions) will throttle indexing if merges are not keeping up. 

Also, it should be run on your "cold storage" nodes, not your hot nodes, so that it doesn't interfere with other processes.

i don't think that preference will help your situation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-fields will create field names with dots '.'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14957</link><project id="" key="" /><description>One of the breaking changes of Elasticsearch 2.0 was that field names are no longer allowed to have dots: https://github.com/elastic/elasticsearch/pull/12068

However, if you use the new multi-field syntax, Elasticsearch will create field name with dots. The documentation supports the behavior. The aggregation in the example is on a field named "city.raw":

https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-fields.html

The unit tests in the code look for field names with dots: https://github.com/elastic/elasticsearch/blob/2.1/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java#L88

The tests pass because the check in ObjectMapper$TypeParser only checks the original name.

The field name is built in ContentPath:
https://github.com/elastic/elasticsearch/blob/2.1/core/src/main/java/org/elasticsearch/index/mapper/ContentPath.java#L50
https://github.com/elastic/elasticsearch/blob/2.1/core/src/main/java/org/elasticsearch/index/mapper/ContentPath.java#L80-L87

These field names will fail if the indices are run through the MetaDataIndexUpgradeService. Are dots allowed in this context?
</description><key id="118531086">14957</key><summary>Multi-fields will create field names with dots '.'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brusic</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label></labels><created>2015-11-24T04:50:44Z</created><updated>2015-12-02T19:45:43Z</updated><resolved>2015-12-02T19:45:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-24T12:14:31Z" id="159249975">well spotted, thanks!  Simple recreation, that runs on 2.1 (or on 1.7 and upgrades without warning to 2.1):

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "string",
          "fields": {
            "bar.baz": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": "test"
}

GET t/t/_search
{
  "query": {
    "match": {
      "foo.bar.baz": "test"
    }
  }
}
```
</comment><comment author="dadoonet" created="2015-11-24T16:50:19Z" id="159334571">I think it's the same root cause as described for the mapper attachment plugin. Linking to it here https://github.com/elastic/elasticsearch-mapper-attachments/issues/169
</comment><comment author="brwe" created="2015-11-30T15:45:47Z" id="160666619">I took a look and must admit I do not fully understand what the issue is.

@brusic What exactly is the error you see? I wrote a test for the example and all seems well (https://github.com/brwe/elasticsearch/commit/43e94268e4d59d85b46fb204275d3b24c288bad6#diff-523d9598b02bfef9341f7aa20ebfdf47R543). The ContentPath builds the field path not the name and if I understand correctly that contains as many dots as there are levels.
I think I am missing something?

@clintongormley We should not be able to create multi fields with dots in the name, that is a bug for sure. I'll try to fix
</comment><comment author="brusic" created="2015-11-30T20:35:44Z" id="160753913">It all depends on how strict should Elasticsearch should be regarding non-dots-in-field names. Better yet, what is the difference between a field name and a path?

At the Lucene level, multifield will create a field name with a dot in it. Inspected a one-shard index with Luke, and the fields created using the fields syntax will have a dot. Clinton's example does not fully show the impact since it uses a field name with a dot. Using 'bar' instead of 'bar.baz'.

```
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "string",
          "fields": {
            "bar": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": "test"
}

GET t/t/_search
{
  "query": {
    "match": {
      "foo.bar": "test"
    }
  }
}
```

A field name of foo.bar was created. Is this field name allowed? There was a discussion on the mailing list were these field names were getting flagged as incorrect when a user attempted to upgrade:

https://discuss.elastic.co/t/elasticsearch-2-mapping/34366/9

Either dots should never be allowed or the fields created with multifields should not be flagged as incorrect since they are paths and not field names.
</comment><comment author="rjernst" created="2015-11-30T20:47:42Z" id="160756952">&gt; A field name of foo.bar was created

This isn't true. We do not create a field with name `foo.bar`. We create the equivalent of the following mapping:

```
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "string",
          "copy_to": "foo.bar",
          "properties": {
            "bar": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}
```

The `name` of a field is _just_ the name. In this case, the `name` of the multi field is `bar`.  It's path is how you access it in a query, which uses dots to separate path elements. So `bar` is underneath a `foo` field, and so its path is `foo.bar`. This path is why dots are no longer allowed in field names: we cannot distinguish between `foo.bar` being a field on its own, or an object field `foo` with a subfield `bar`.
</comment><comment author="rjernst" created="2015-11-30T20:49:45Z" id="160757687">FYI, the mapping I showed there wont' actually work IIRC. It was just used to explain how multi fields work conceptually.  Only an `object` field can have `properties` (ie subfields) hence why the general purpose `fields` exists for concrete data types.
</comment><comment author="brwe" created="2015-12-01T13:51:49Z" id="160975724">I think the confusion stems from the ambiguous use of the words `field name`. We have 1) a _Lucene_ field name which can contain as many dots as needed (we create them for example [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java#L306) and this is also what we can see in Luke) and 2) a `field name` in _elasticsearch mapping_ which is just the innermost identifier for a value in a nested json structure and may not contain any dots in elasticsearch mapping.

For example:

```
{
  "a": {
     "b": {
        "c":  "All mappings and no play makes Britta a dull programmer"
     }
  }
}
```

1) will be `a.b.c`
2) will be `c` 

The path we were talking about before is `a.b` and is needed on Lucene level I think to distinguish `a.b.c` for example from another field that also has the name `c` but is nested in objects `d` and `e` (and therefore has path `d.e`). 
Let me know if this makes sense.

Also, @brusic just to be sure: did you actually encounter any error? We only check the elasticsearch field name for dots, the Lucene field names are untouched by that.
</comment><comment author="rjernst" created="2015-12-01T15:18:53Z" id="160998729">&gt;   The path we were talking about before is a.b

One minore correction: path would be `a.b.c` there. It is like path vs name for File in java. 
</comment><comment author="brusic" created="2015-12-02T19:45:43Z" id="161411744">I agree with Britta in that the confusion is related to the ambiguity of the words field name, which I acknowledged in my previous comment. I have not seen the error myself, but have seen it a couple of times on the forum. Since I was curious what was happening behind the scenes, I checked it out myself, only looking at the Lucene field names.

If someone else has a problem with upgrading, they can create a new issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot configure AsyncAppender in logging.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14956</link><project id="" key="" /><description>In our team, we tried to configure AsnycAppender in ES version 1.7.3 and 2.0.0, we tried bunch of stuffs but finally found out that it is not possible from the beginning.

In the below source codes, ES reads logging.\* file and make it as java properties, pass it to PropertyConfigurator of log4j 1.2.17

https://github.com/elastic/elasticsearch/blob/v1.7.3/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
https://github.com/elastic/elasticsearch/blob/v2.0.0/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java

but in the links below it says this.
https://github.com/apache/log4j/blob/v1_2_17/src/main/java/org/apache/log4j/PropertyConfigurator.java

```
   &lt;p&gt;The &lt;code&gt;PropertyConfigurator&lt;/code&gt; does not handle the
   advanced configuration features supported by the {@link
   org.apache.log4j.xml.DOMConfigurator DOMConfigurator} such as
   support custom {@link org.apache.log4j.spi.ErrorHandler ErrorHandlers},
   nested appenders such as the {@link org.apache.log4j.AsyncAppender
   AsyncAppender}, etc.
```

How about using DOMConfigurator when logging configuration is set in logging.xml?
</description><key id="118523124">14956</key><summary>Cannot configure AsyncAppender in logging.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-11-24T03:10:57Z</created><updated>2016-05-20T20:35:26Z</updated><resolved>2016-05-20T20:35:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-20T20:35:22Z" id="220712011">Closing in favor of #17697.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms aggregation by `_parent` with multi-level parent mapping returns bad buckets. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14955</link><project id="" key="" /><description>With a multi-level mapping of parent/child relationships on an index, searching with a `terms` aggregation by `_parent` on the "middle" child document type (mapped as a child of one type and a parent of another) yields "duplicate" buckets with bogus values for the `_parent` keys.

Here is an example, with document types `my_parent`, `my_child`, and `my_grandchild`.  The query attempts to find, for a given list of parent IDs, the counts of children of those parents, aggregated by `color`.  When `my_grandchild` is not part of the mapping, the correct expected result buckets are returned.  When `my_grandchild` _is_ included in the mapping, the "duplicate" buckets with bogus `_parent` keys are also included.  The bogus bucket `_parent` keys are the `id`s of the `my_child` documents, though there are no `my_grandchild` documents, and the `my_grandchild` document type was not targetted by the query, and would not have matched the `_parent` values given in the query.

To reproduce, run the following script using the `WITH_GRANDCHILD` environment variable to dictate whether the `my_grandchild` mapping should be included or not.  For example, run `env WITH_GRANDCHILD=true bash script.sh` to include the `my_grandchild` mapping:

``` bash
#!/usr/bin/env bash

ES_HOST="${ES_HOST:-http://localhost:9200}"
WITH_GRANDCHILD="${WITH_GRANDCHILD:-false}"

echo "env: ES_HOST=${ES_HOST}"
echo "     WITH_GRANDCHILD=${WITH_GRANDCHILD}"

function es() {
  echo
  echo "es ${1} ${2} ${3}"
  curl -X"${1}" "${ES_HOST}/${2}" -d "${3}"
  echo
}

es DELETE my_index

if [[ X"${WITH_GRANDCHILD}" == X"false" ]]; then
  es PUT my_index '{
    "mappings": {
      "my_parent":     {},
      "my_child":      { "_parent": { "type": "my_parent" } }
    }
  }'
else
  es PUT my_index '{
    "mappings": {
      "my_parent":     {},
      "my_child":      { "_parent": { "type": "my_parent" } },
      "my_grandchild": { "_parent": { "type": "my_child"  } }
    }
  }'
fi

echo
echo "---"

es PUT 'my_index/my_parent/P1' '{ "text": "first parent" }'
es PUT 'my_index/my_parent/P2' '{ "text": "second parent" }'

es PUT 'my_index/my_child/P1_C1?parent=P1' '{ "color": "red" }'
es PUT 'my_index/my_child/P1_C2?parent=P1' '{ "color": "yellow" }'
es PUT 'my_index/my_child/P1_C3?parent=P1' '{ "color": "yellow" }'
es PUT 'my_index/my_child/P2_C1?parent=P2' '{ "color": "green" }'
es PUT 'my_index/my_child/P2_C2?parent=P2' '{ "color": "green" }'
es PUT 'my_index/my_child/P3_C1?parent=P3' '{ "color": "blue" }'

es POST 'my_index/_refresh'

echo
echo "---"

es GET 'my_index/my_child/_search?search_type=count&amp;format=yaml' '{
  "query": {
    "terms": {
      "_parent": [ "P1", "P2" ]
    }
  },
  "aggs": {
    "parents": {
      "terms": { "field": "_parent" },
      "aggs": {
        "colors": {
          "terms": { "field": "color" }
        }
      }
    }
  }
}'
```

---

For completeness, here is the expected output of the final query (in YAML format):

``` yaml

---
took: 2
timed_out: false
_shards:
  total: 5
  successful: 5
  failed: 0
hits:
  total: 5
  max_score: 0.0
  hits: []
aggregations:
  parents:
    doc_count_error_upper_bound: 0
    sum_other_doc_count: 0
    buckets:
    - key: "P1"
      doc_count: 3
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "yellow"
          doc_count: 2
        - key: "red"
          doc_count: 1
    - key: "P2"
      doc_count: 2
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "green"
          doc_count: 2
```

---

And here is the bogus output of the final query (with `WITH_GRANDCHILD=true` set):

``` yaml

---
took: 2
timed_out: false
_shards:
  total: 5
  successful: 5
  failed: 0
hits:
  total: 5
  max_score: 0.0
  hits: []
aggregations:
  parents:
    doc_count_error_upper_bound: 0
    sum_other_doc_count: 0
    buckets:
    - key: "P1"
      doc_count: 3
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "yellow"
          doc_count: 2
        - key: "red"
          doc_count: 1
    - key: "P2"
      doc_count: 2
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "green"
          doc_count: 2
    - key: "P1_C1"
      doc_count: 1
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "red"
          doc_count: 1
    - key: "P1_C2"
      doc_count: 1
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "yellow"
          doc_count: 1
    - key: "P1_C3"
      doc_count: 1
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "yellow"
          doc_count: 1
    - key: "P2_C1"
      doc_count: 1
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "green"
          doc_count: 1
    - key: "P2_C2"
      doc_count: 1
      colors:
        doc_count_error_upper_bound: 0
        sum_other_doc_count: 0
        buckets:
        - key: "green"
          doc_count: 1
```
</description><key id="118509376">14955</key><summary>Terms aggregation by `_parent` with multi-level parent mapping returns bad buckets. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jemc</reporter><labels /><created>2015-11-24T01:04:26Z</created><updated>2015-11-28T16:31:26Z</updated><resolved>2015-11-28T16:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T16:31:26Z" id="160316729">Duplicate of https://github.com/elastic/elasticsearch/issues/8332
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Concurrent calls to scroll API scrambles results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14954</link><project id="" key="" /><description>I'm not sure whether this is by design or not, but it appears that calling the scroll API with multiple requests to the same scroll ID concurrently will return mixed up results, whereby the meta fields (`_index`, `_type`, `_id` etc) get put together with the wrong document `_source`.

I've written a JS [migration script](https://gist.github.com/TheDeveloper/e91f7b7044d4f3a33b2f) for node which uses redis to parallelise scroll requests across multiple workers / machines. The idea is to scale the number of fetches on the scan/scroll ID to speed up migration. If it changes, the new ID is picked up by the workers via Redis.

It works fine when running a single worker, which procedurally calls scroll, receives the documents then bulk inserts them to the new cluster before performing another scroll request. However, as soon as I start up a further one or more parallel workers, which will start issuing multiple requests to the scroll API at the same time, I start seeing weird results coming back, whereby the meta fields are being mixed up with `_source` bodies from other documents as mentioned above.

Is this expected behaviour when multiple scroll requests are reading from the same scroll ID at the same time?

The script uses the official elasticsearch-js client node module, v8.2.0. I don't think there should be anything in the client causing this given that all it does is streams the http response and JSON decodes it.

I'm seeing this on a 10-node cluster running ES 1.7.1. There are thousands of type mappings and hundreds of millions of documents in 2 indexes split into a dozen shards each.
</description><key id="118507606">14954</key><summary>Concurrent calls to scroll API scrambles results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheDeveloper</reporter><labels /><created>2015-11-24T00:49:04Z</created><updated>2015-11-24T13:11:04Z</updated><resolved>2015-11-24T12:41:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-24T12:41:24Z" id="159256441">The scroll API is indeed intended to be consumed by a single thread. You can open multiple scrolls concurrently. That's not a problem. 
</comment><comment author="TheDeveloper" created="2015-11-24T13:11:04Z" id="159262219">Ok, makes sense. Thanks for clarifying.

Would it be worth considering responding with an error to clients if they try to read from the scroll API from more than one request at the same time? Despite it being incorrect usage I am still able to do it and it results in broken behaviour. I'm thinking this'd be particularly helpful preventing async clients from accidentally overlapping their requests.

Also, perhaps a note could be added to the docs [https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-request-scroll.html](docs) about this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle missing logging.yml better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14953</link><project id="" key="" /><description>If you start Elasticsearch and use a custom configuration directory (eg. --path.conf) and do not copy over the logging.yml file, the node will start up but with no log files generated and the following message written to standard out:

```
log4j:WARN No appenders could be found for logger (bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```

The log4j link will take you to a generic faq page:

```
Why do I see a warning about "No appenders found for logger" and "Please configure log4j properly"?
This occurs when the default configuration files log4j.properties and log4j.xml can not be found and the application performs no explicit configuration. log4j uses Thread.getContextClassLoader().getResource() to locate the default configuration files and does not directly check the file system. Knowing the appropriate location to place log4j.properties or log4j.xml requires understanding the search strategy of the class loader in use. log4j does not provide a default configuration since output to the console or to the file system may be prohibited in some environments. Also see FAQ: Why can't log4j find my properties in a J2EE or WAR application?.
```

Not sure if there is a way for us to catch this and report a more Elasticsearch specific warning to the log indicating that the logging.yml file cannot be found at &lt;location&gt;, etc..
</description><key id="118496842">14953</key><summary>Handle missing logging.yml better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label></labels><created>2015-11-23T23:23:33Z</created><updated>2016-02-14T17:39:20Z</updated><resolved>2016-02-14T17:39:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-24T08:10:24Z" id="159190829">++ I think we should try to fallback to a default one available in the classloader or fail starting elasticsearch if this file is mandatory.

I'd prefer the former.
</comment><comment author="rjernst" created="2015-11-24T09:02:10Z" id="159200695">@dadoonet Don't we ship our distributions with a logging.yml? I don't think we should be lenient. Otherwise, the user may not realize that their configuration is not getting picked up, for example, with a misnamed file or the file in the wrong location.
</comment><comment author="kimchy" created="2015-11-24T09:06:34Z" id="159202063">note that this is not required when someone embeds us, and it is their responsibility to configure logging external to us.
</comment><comment author="aodj" created="2015-11-25T13:14:16Z" id="159603720">Whats the fix for this? I'm currently trying to upgrade from 1.7 to 2.0 and the (previously working) argument of `-Des.default.path.conf=...` just leaves me with no logs.
</comment><comment author="rmuir" created="2015-11-25T13:38:16Z" id="159610451">I agree the warning is not helpful, it is the worst to get with a java application. I understand people use this as a client library too, but it would be great, when running as a server to fail with a simple `NoSuchFileException("/wherever/conf/logging.yml")`. Then the user has a chance to correct it without reading a bunch of confusing logging FAQ etc.
</comment><comment author="aodj" created="2015-11-26T15:57:03Z" id="159947138">As a further point of discussion: this error presents even when the `logging.yml` file is present, but incorrectly configured. I was trying to debug an issue whilst trial running the upgrade between 1.7 and 2.0 and ran into this problem. As a result I stumbled upon  this Github issue and concluded that the problem lay with the `logging.yml` file not being found, only to later discover that this same error message is displayed if the logging system fails to initialise properly. 

You can read more [here](https://discuss.elastic.co/t/logging-not-initialised-properly-after-upgrade-to-2-0/35542/4?u=aodj) including [diffs](https://www.diffnow.com/?report=k2hax) between the shipped (and working) `logging.yml` and the one that I was using (which worked with 1.7)
</comment><comment author="clintongormley" created="2016-02-14T17:39:20Z" id="183935153">Closing in favour of #16585
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rework extra plugins support to be through sibling directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14952</link><project id="" key="" /><description>This change removes the subdirectory support for extra-plugins, and
replaces it with an iteration of directories under a sibling directory "x-plugins".
</description><key id="118486103">14952</key><summary>Rework extra plugins support to be through sibling directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T22:18:49Z</created><updated>2015-11-23T23:51:34Z</updated><resolved>2015-11-23T23:51:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-23T23:49:06Z" id="159106583">LGTM. This is working _wonderfully_ for me locally in IntelliJ.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Lucene to 5.4.0-snapshot-1715952</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14951</link><project id="" key="" /><description>Get the fix for https://issues.apache.org/jira/browse/LUCENE-6906
</description><key id="118485576">14951</key><summary>Upgrade Lucene to 5.4.0-snapshot-1715952</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T22:16:03Z</created><updated>2015-11-28T16:39:10Z</updated><resolved>2015-11-23T22:25:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-23T22:21:44Z" id="159085981">looks good
</comment><comment author="jpountz" created="2015-11-24T08:20:06Z" id="159192280">Thanks @mikemccand !
</comment><comment author="mikemccand" created="2015-11-24T14:36:40Z" id="159285996">YW @jpountz!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only trace log shard not available exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14950</link><project id="" key="" /><description>This commit changes the behavior of the logging in
TransportBroadcastByNodeAction#onShardOperation to only trace log
exceptions that are considered shard-not-available exceptions. This
makes the logging consistent with how these exceptions are handled in
the response.

Relates #14927
</description><key id="118460392">14950</key><summary>Only trace log shard not available exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T19:59:44Z</created><updated>2015-12-19T16:30:23Z</updated><resolved>2015-12-14T21:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-23T20:02:43Z" id="159047511">Thanks Jason. Correct me if I'm wrong, but isn't the problem coming from https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java#L387 ?

The method changed here is about total node failures...
</comment><comment author="jasontedor" created="2015-11-23T20:06:42Z" id="159048434">See ce3eb4bb5ec447d7384a4bea775413ad0583b6a8.
</comment><comment author="jasontedor" created="2015-12-14T18:18:42Z" id="164515764">@bleskes Can this be integrated into master?
</comment><comment author="bleskes" created="2015-12-14T19:12:56Z" id="164529813">left one comment.
</comment><comment author="bleskes" created="2015-12-14T21:19:24Z" id="164562903">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`function_score` should have a bulk scorer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14949</link><project id="" key="" /><description>Some queries return bulk scorers that can be significantly faster than iterating naively over the scorer. By giving `function_score` a BulkScorer that would delegate to the wrapped query, we could make it faster in some cases.
</description><key id="118436629">14949</key><summary>`function_score` should have a bulk scorer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2015-11-23T17:58:29Z</created><updated>2017-03-30T03:05:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2017-03-14T00:47:31Z" id="286288357">@Plokji I'm afraid this would not be a good issue to get started as it touches rather low-level functionality. I would recommend that you have a look at the issues labeled "low hanging fruit" if you're just getting started.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix significant terms reduce for long terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14948</link><project id="" key="" /><description>Significant terms were not reduced correctly if they were long terms.
Also, clean up the bwc test a little. Upgrades are not needed.

related to #13522
</description><key id="118435210">14948</key><summary>Fix significant terms reduce for long terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T17:50:33Z</created><updated>2016-01-22T18:39:17Z</updated><resolved>2015-11-24T09:53:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-23T17:53:46Z" id="159010911">LGTM. Is there any chance of making the BWC test share more code with the non-bwc tests? I find that keeps them passing with more regularity. That can wait for another Pr though.
</comment><comment author="brwe" created="2015-11-23T17:56:32Z" id="159011580">Yeah I can do that. I don't think we have an integ test for long terms with significant terms so it would make a lot of sense to add one. will make another pr for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should ELasticSearch work with Windows Server 2008 R2 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14947</link><project id="" key="" /><description>I recently upgraded to elastic search 2.0 on my Windows Server 2008 R2 machine, I am now getting a java error when trying to run it, is 2008 still supported?
</description><key id="118425137">14947</key><summary>Should ELasticSearch work with Windows Server 2008 R2 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acb122</reporter><labels /><created>2015-11-23T17:01:17Z</created><updated>2015-11-23T17:29:05Z</updated><resolved>2015-11-23T17:29:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-23T17:02:20Z" id="158996560">Yes. But may be you can describe which exact error you have.

A better place could be to ask that in discuss.elastic.co.
</comment><comment author="acb122" created="2015-11-23T17:13:21Z" id="158999347">it was this url https://www.elastic.co/support/matrix which seems to show it wasn't supported. But no issues online i could find.
</comment><comment author="acb122" created="2015-11-23T17:24:24Z" id="159002093">[2015-11-23 16:23:15,579][INFO ][rest.suppressed          ] /o/_mapping/field/*
Params: {ignore_unavailable=false, allow_no_indices=false, index=o, include_defa
ults=true, fields=*, _=1448295795436}
[o] IndexNotFoundException[no such index]
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$Wildca
rdExpressionResolver.resolve(IndexNameExpressionResolver.java:560)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concre
teIndices(IndexNameExpressionResolver.java:127)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concre
teIndices(IndexNameExpressionResolver.java:71)
        at org.elasticsearch.action.admin.indices.mapping.get.TransportGetFieldM
appingsAction.doExecute(TransportGetFieldMappingsAction.java:57)
        at org.elasticsearch.action.admin.indices.mapping.get.TransportGetFieldM
appingsAction.doExecute(TransportGetFieldMappingsAction.java:40)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAct
ion.java:70)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58
)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClien
t.java:347)
        at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)

```
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.do
```

Execute(BaseRestHandler.java:83)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClien
t.java:347)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(
AbstractClient.java:1177)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.getField
Mappings(AbstractClient.java:1377)
        at org.elasticsearch.rest.action.admin.indices.mapping.get.RestGetFieldM
appingAction.handleRequest(RestGetFieldMappingAction.java:66)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.
java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.j
ava:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.
java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.
java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServ
er.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest
(NettyHttpServerTransport.java:348)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpR
equestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.message
Received(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleCha
nnelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceive
d(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived
(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:29
6)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessage
Received(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(Repl
ayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived
(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(Ope
nChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:26
8)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:25
5)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(Abstract
NioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNi
oSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioW
orker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnabl
e.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProof
Worker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.
java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-11-23 16:23:16,421][INFO ][rest.suppressed          ] /op/_mapping/field/*
 Params: {ignore_unavailable=false, allow_no_indices=false, index=op, include_de
faults=true, fields=*, _=1448295796296}
[op] IndexNotFoundException[no such index]
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$Wildca
rdExpressionResolver.resolve(IndexNameExpressionResolver.java:560)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concre
teIndices(IndexNameExpressionResolver.java:127)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concre
teIndices(IndexNameExpressionResolver.java:71)
        at org.elasticsearch.action.admin.indices.mapping.get.TransportGetFieldM
appingsAction.doExecute(TransportGetFieldMappingsAction.java:57)
        at org.elasticsearch.action.admin.indices.mapping.get.TransportGetFieldM
appingsAction.doExecute(TransportGetFieldMappingsAction.java:40)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAct
ion.java:70)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58
)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClien
t.java:347)
        at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)

```
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.do
```

Execute(BaseRestHandler.java:83)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClien
t.java:347)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(
AbstractClient.java:1177)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.getField
Mappings(AbstractClient.java:1377)
        at org.elasticsearch.rest.action.admin.indices.mapping.get.RestGetFieldM
appingAction.handleRequest(RestGetFieldMappingAction.java:66)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.
java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.j
ava:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.
java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.
java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServ
er.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest
(NettyHttpServerTransport.java:348)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpR
equestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.message
Received(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleCha
nnelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceive
d(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived
(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:29
6)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessage
Received(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(Repl
ayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived
(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(S
impleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerC
ontext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(Ope
nChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultCh
annelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:26
8)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:25
5)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(Abstract
NioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNi
oSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioW
orker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnabl
e.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProof
Worker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.
java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:617)
        at java.lang.Thread.run(Thread.java:745)
</comment><comment author="dadoonet" created="2015-11-23T17:27:03Z" id="159003010">Ha. I misread the issue. I thought you were talking about 2012 not 2008.
</comment><comment author="dadoonet" created="2015-11-23T17:29:05Z" id="159003528">About the "issue", sounds like you PUT a mapping in a non existing index or so?

May be you could describe what you are doing on discuss.elastic.co?

Closing here as not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>copy_to of mapper attachments metadata field isn't working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14946</link><project id="" key="" /><description>_From @gpaul on November 17, 2015 10:43_

The following set of steps against a fresh elasticsearch 2.0.0 instance with v3.0.2 of this plugin installed shows that copy_to isn't working for the name field. I doubt it is working for the other metadata fields, either.

You can copy/paste this in your shell.

```
# create a mapping
curl -XPOST 'http://localhost:9200/test_copyto' -d '{
  "mappings": {
    "person": {
      "properties": {
        "copy_dst": { "type": "string" },
        "doc": {
      "type": "attachment",
      "fields": {
        "name": { "copy_to": "copy_dst" }
          }
    }
      }
    }
  }
}'
## =&gt; {"acknowledged":true}


# index a document, specifying a document name
curl -XPOST 'http://localhost:9200/test_copyto/person/1' -d '{
  "doc": {
    "_content": "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=",
    "_name": "my-attachment-name.doc"
  }
}'
## =&gt; {"_index":"test_copyto","_type":"person","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}


# search for the document by its contents
curl -XPOST 'http://localhost:9200/test_copyto/person/_search' -d '
{
  "query": {
    "query_string": {
      "default_field": "doc.content",
      "fields": ["copy_dst", "doc.content"],
      "query": "ipsum"
    }
  }
}
'
## =&gt; {"took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits": "total":1,"max_score":0.04119441,"hits":[{"_index":"test_copyto","_type":"person","_id":"1","_score":0.04119441,"_source":{
#  "doc": {
#    "_content": "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=",
#    "_name": "my-attachment-name.doc"
#  }
#}}]}}


# search for the document by its name
curl -XPOST 'http://localhost:9200/test_copyto/person/_search' -d '
{
  "query": {
    "query_string": {
      "default_field": "doc.name",
      "fields": ["doc.name"],
      "query": "my-test.doc"
    }
  }
}
'
## =&gt; {"took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.02250402,"hits":[{"_index":"test_copyto","_type":"person","_id":"1","_score":0.02250402,"_source":{
#  "doc": {
#    "_content": "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=",
#    "_name": "my-attachment-name.doc"
#  }
#}}]}}

# search for the document by the copy_dst field
curl -XPOST 'http://localhost:9200/test_copyto/person/_search' -d '
{
  "query": {
    "query_string": {
      "default_field": "copy_dst",
      "fields": ["copy_dst"],
      "query": "my-test.doc"
    }
  }
}
'
## =&gt; {"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

_Copied from original issue: elastic/elasticsearch-mapper-attachments#190_
</description><key id="118414556">14946</key><summary>copy_to of mapper attachments metadata field isn't working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Mapper Attachment</label><label>bug</label><label>regression</label></labels><created>2015-11-23T16:10:54Z</created><updated>2016-03-24T20:03:27Z</updated><resolved>2015-12-02T13:04:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-23T16:10:55Z" id="158982117">_From @gpaul on November 23, 2015 13:37_

Ping. Should I open this issue against the main elasticsearch repository now that this plugin is moving there?
</comment><comment author="dadoonet" created="2015-11-23T16:10:55Z" id="158982118">Did you try the same script with elasticsearch 1.7? I'd like to know if it's a regression or if it has always been there.

I know that copy_to feature is supposed to work for the extracted text but I don't think it worked for metadata.

If I'm right (so it's not an issue but more a feature request), then you can open it in elasticsearch repo.
If I'm wrong (so it's a regression), then keep it here.
</comment><comment author="dadoonet" created="2015-11-23T16:10:55Z" id="158982122">_From @gpaul on November 23, 2015 14:58_

It seems like a regression:
elasticsearch 1.7.0 with mapper-attachments 2.7.1
yields

```
curl -XPOST 'http://localhost:9200/test_copyto/person/_search' -d '
 {
   "query": {
     "query_string": {
       "default_field": "copy_dst",
       "fields": ["copy_dst"],
       "query": "my-test.doc"
     }
   }
 }
 '
#{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.02250402,"hits":[{"_index":"test_copyto","_type":"person","_id":"1","_score":0.02250402,"_source":{
#  "doc": {
#    "_content": "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=",
#    "_name": "my-test.doc"
#  }
#}}]}}
```
</comment><comment author="dadoonet" created="2015-11-23T16:10:56Z" id="158982123">Thank you @gpaul 
</comment><comment author="dadoonet" created="2015-11-23T16:10:56Z" id="158982127">_From @hhoechtl on November 23, 2015 15:2_

It's also not working with the .content field
</comment><comment author="dadoonet" created="2015-11-23T16:25:24Z" id="158986462">I created a test for elasticsearch 1.7 and it is working well in 1.x series:

``` java
@Test
public void testCopyToMetaData() throws Exception {
    String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/integration/simple/copy-to-metadata.json");
    byte[] txt = copyToBytesFromClasspath("/org/elasticsearch/index/mapper/attachment/test/sample-files/text-in-english.txt");

    client().admin().indices().putMapping(putMappingRequest("test").type("person").source(mapping)).actionGet();

    index("test", "person", jsonBuilder().startObject()
            .startObject("file")
            .field("_content", txt)
            .field("_name", "name")
            .endObject()
            .endObject());
    refresh();

    CountResponse countResponse = client().prepareCount("test").setQuery(queryStringQuery("name").defaultField("file.name")).execute().get();
    assertThatWithError(countResponse.getCount(), equalTo(1l));

    countResponse = client().prepareCount("test").setQuery(queryStringQuery("name").defaultField("copy")).execute().get();
    assertThatWithError(countResponse.getCount(), equalTo(1l));
}
```

I created a test for 2.\* branches which demonstrates the regression from 2.0.
It can be reused to fix this issue: https://github.com/elastic/elasticsearch-mapper-attachments/commit/30aeda668a9090f28929a74d59b9bf81e1738161:

``` yml
"Copy To Feature":

    - do:
        indices.create:
            index: test
            body:
              mappings:
                doc:
                  properties:
                    copy_dst:
                      type: string
                    doc:
                      type: attachment
                      fields:
                        name:
                          copy_to: copy_dst
    - do:
        cluster.health:
          wait_for_status: yellow

    - do:
        index:
            index: test
            type: doc
            id: 1
            body:
              doc:
                _content: "e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0="
                _name: "name"

    - do:
        indices.refresh: {}

    - do:
        search:
            index: test
            body:
                query:
                    match:
                        doc.content: "ipsum"

    - match: { hits.total: 1 }

    - do:
        search:
            index: test
            body:
                query:
                    match:
                        doc.name: "name"

    - match: { hits.total: 1 }

    - do:
        search:
            index: test
            body:
                query:
                    match:
                        copy_dst: "name"

    - match: { hits.total: 1 }
```

@rjernst Could you give a look please?
</comment><comment author="rjernst" created="2015-11-30T22:22:23Z" id="160780665">@dadoonet One odd thing I see is using a copy_to from within a multi field.  Seems like we should disallow that? We removed support here:
https://github.com/elastic/elasticsearch/pull/10802/files#diff-ab54166cc098ea6f03350e68a8689a5bL432

 The copy's are now handled outside of the mappers, while before there was a lot of spaghetti sharing between object mappers and field mappers that made document parsing complex.  If we want to add it back, we will probably need a good refactoring in the way multi fields and copy_tos are handled. The problem is `copy_to` inside a multi field is essentially a nested `copy_to` since multi fields are conceptually just a copy to (see my notes in #10802).
</comment><comment author="dadoonet" created="2015-12-01T08:05:34Z" id="160889068">@clintongormley WDYT? 

Let me sum up the discussion.

Before 2.0, we were able to support:

``` js
PUT /test/person/_mapping
{
  "person": {
    "properties": {
      "file": {
        "type": "attachment",
        "fields": {
          "content": {
            "type": "string",
            "copy_to": "copy"
          },
          "name": {
            "type": "string",
            "copy_to": "copy"
          },
          "author": {
            "type": "string",
            "copy_to": "copy"
          },
          "title": {
            "type": "string",
            "copy_to": "copy"
          }
        }
      },
      "copy": {
        "type": "string"
      }
    }
  }
}
```

It means that extracted `content`, `title`, `name` (for filename) and `author` can be indexed in another field `copy` where the user can run a "fulltext" search.

From 2.0, this is not supported anymore for the reasons @rjernst described.

Should we document that mapper attachments does not support anymore `copy_to` feature on extracted/provided nested fields?
Note that we don't support [a global `copy_to` on the `BASE64` content](https://github.com/elastic/elasticsearch-mapper-attachments/issues/100) itself.

Or should we try to implement such a thing _only_ for mapper attachments plugin. I can't see another use case today. May be some other community plugins would like to have it but really unsure here.

IMO, users can always run search on multiple fields at the same time so instead of searching in `copy` in the above example, they could search on `file.content`, `file.title`, `file.name` and `file.author`. So not supporting this feature anymore does not sound as a big deal to me.

Thoughts?
</comment><comment author="clintongormley" created="2015-12-01T11:03:18Z" id="160935938">My feeling is that, long term, we should remove the `attachment` field type.  Instead, we should move Tika to being a processor in the node ingest API which  will read the binary attachment and add the contents and any meta fields to the `_source` field itself.  The result is that attachments stop being magical and become just like any other field which can be configured in standard ways.

With this goal in mind, it doesn't make sense to add complicated (and likely buggy) hacks to fix this regression in 2.x.  But, we should let the user know that copy-to on multi-fields is not supported: we should throw an exception at mapping time instead of silently ignoring the problem.
</comment><comment author="brwe" created="2015-12-01T16:24:25Z" id="161020907">So...I took a look at the copy_to and multi_fields and tried to throw an exception when we encounter copy_to in multi_fields - we could do that I guess. But I have the suspicion that re-adding the copy_to to multi_fields is just a matter of shifting three lines. I made a pr here so you can see what I mean: #15152 Tests pass but I might be missing something.
</comment><comment author="dadoonet" created="2015-12-01T16:29:27Z" id="161022420">That would be an awesome news @brwe. Was not expecting that. Are the tests I wrote for mapper attachments plugin work as well? Is that what you mean by `Tests pass`?
</comment><comment author="brwe" created="2015-12-02T11:13:08Z" id="161263824">@dadoonet I mean the tests that were there already and the tests I added in the pr. I suspect that your test would pass as well but did not check.

However, we ( @rjernst @clintongormley and I ) had a chat yesterday about this in here is the outcome:
Doing this fix is not a good idea for several reasons:
1. it would introduce a dependency between DocumentParser and FieldMapper again which was removed with much effort in #10802
2. long term we want to replace the implementation of multi fields with `copy_to` mechanism anyway
3. chains of `copy_to` ala

```
"a" : {
"type": "string",
"copy_to": "b"
},
"b": {
"type": "string",
"copy_to": "a"
}
```

should not be possible because they add complexity in code and usage.

This reduces flexibility of mappings and how they can transform data. However, the consensus is that elasticsearch is not the right place to perform these kind of transformations anyway and these kind of operations should move to external tools such as the planned node ingest plugin https://github.com/elastic/elasticsearch/issues/14049

Applying the fix I proposed would just delay the removal of the feature and therefore we think we should not do it.
</comment><comment author="gpaul" created="2015-12-02T11:47:32Z" id="161269697">It seems like you have removed a feature without providing a better alternative. The use of copy_to for custom '_all' fields is well-documented and very useful.
</comment><comment author="gpaul" created="2015-12-02T11:59:25Z" id="161271726">@brwe My 2c on your  discussion 
1. it would introduce a dependency between DocumentParser and FieldMapper again which was removed with much effort in #10802
   -&gt; if such cleanup code broke something - then perhaps it was merged too soon. A temporary patch to fix a broken feature until such time as it can be properly reengineered is not unreasonable.
2. long term we want to replace the implementation of multi fields with copy_to mechanism anyway
   -&gt; perfect - the process I would like to see in that case is: a deprecation note in 2.1 followed by a better alternative and migration path users can follow in preparation for 2.2 - keeping in mind that mappings from older versions of elasticsearch need to be upgraded. There are people who used ES as their primary data store - I'm not one of those, but having to rebuilding indexes when new ES versions are released is unfortunate.  
3. chains of copy_to ... should not be possible because they add complexity in code and usage.
   -&gt; by all means prohibit them. I didn't know this was possible in earlier versions as it seems too easy to define cycles.
</comment><comment author="clintongormley" created="2015-12-02T13:04:42Z" id="161285213">@gpaul If our resources were unlimited, then I would agree with you. However, in an effort to clean up a massive code base and to remove complexity, we have to do it incrementally and sometimes we have to remove things that worked before.  The mapping cleanup was 5 long months of work, and there is still a good deal more to be done.  It brought some huge improvements (just see how many issues were linked to https://github.com/elastic/elasticsearch/issues/8870) but meant that we couldn't support everything that we supported before.

Every hack that we add into the code adds technical debt and increases the likelihood of introducing new bugs.  We'd much rather focus our limited resources on making the system clean, stable, reliable, and maintainable.

This is why I don't want to make this change.  The workaround for your case is to search across multiple fields.
</comment><comment author="gpaul" created="2015-12-02T15:02:50Z" id="161325276">[woops, I was logged in as a friend of mine when I posted this comment a minute ago. I've removed it and this is a repost as myself &gt;&lt;]

That's fair. Thanks for all the hard work.

As I'll have to redesign my mappings anyway, should I avoid copy_to in its entirety going forward or is it just the multi-field case that was causing pain? I'd like to avoid features that are on their way out.
</comment><comment author="rjernst" created="2015-12-02T15:05:47Z" id="161325986">@gpaul It is just copy_to in a multi field. 
</comment><comment author="gpaul" created="2015-12-02T15:11:26Z" id="161327666">Got it, thanks.
</comment><comment author="spiderpug" created="2016-03-24T19:19:03Z" id="200981361">Hi, I think I've this problem as well.

Following the documentation (?) here:  https://github.com/elastic/elasticsearch-mapper-attachments#copy-to-feature  the `copyTo` on the `content` should work, but I cannot manage to. Isn't that the correct documentation?

I want to make use of this feature to copy the extracted content into a custom `_all` field. Any hints how to solve this?

Is there a content extraction service/endpoint I could make use of to index prepared content, so that I don't have to rely on copyTo?

_Edit_: These docs mention the `copyTo` feature as well:  https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-attachments-copy-to.html

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require the exception cause to ESLogger.warn and .error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14945</link><project id="" key="" /><description>This change makes the `Throwable cause` a required argument to `ESlogger.warn` and `.error`, and fixes places where we were not passing the current exception to these methods.

When there really is no exception, we can pass null.

The goal is to try to prevent issues like #14867 in the future.

I also found a few places that had the wrong number of {} vs the number of parameters passed.

Closes #14905 
</description><key id="118414278">14945</key><summary>Require the exception cause to ESLogger.warn and .error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label></labels><created>2015-11-23T16:09:35Z</created><updated>2015-11-28T17:32:51Z</updated><resolved>2015-11-25T17:11:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-23T16:12:23Z" id="158982555">I'll review!
</comment><comment author="mikemccand" created="2015-11-23T23:13:43Z" id="159097529">Thanks @rmuir and @nik9000, I folded in your feedback.
</comment><comment author="mikemccand" created="2015-11-25T17:11:35Z" id="159676195">I talked to @rmuir about this change and decided it's too dangerous because on back-port of future changes to releases &lt; 2.2.0, the "null" in `logger.warn,error` calls will be incorrect and javac won't tell you ... sigh.

I think, instead, we should aim for better compile time type-safety of our logging APIs so that important changes like this are not blocked, and we should take it in baby steps.  The first step I'm going to try now is to change the logger APIs to take `String...` instead of `Object...`: this means the caller must convert not-strings into strings (calling `.toString()` or `Arrays.toString()` or something), and enabling us to later ban `Throwable.toString` to make sure exceptions are coming in correctly so they are fully logged.
</comment><comment author="rmuir" created="2015-11-25T17:17:58Z" id="159678407">+1, we need to be careful and make sure that we don't introduce bugs/mistakes in trying to fix other bugs / mistakes. 

Lack of type safety is a real problem: allowing `Object...` probably causes a lot of the mistakes you found where `Throwable` goes to the wrong parameter slot: it makes the whole method too lenient. Another issue is allowing `Object...` + `Throwable` all in one logger statement: this is ripe for bugs. I noticed the JDK logger apis don't even offer such a possibility: you are either logging with args, or logging with a Throwable, but not both. 

I see improving compile-time safety as the only option to improve things: for logger code, runtime checks are of little value, its too risky that there is a "bug" in a logger statement that only happens in very exceptional circumstances and at a crazy logging level, we need to detect problems at compile time: means `javac`, `forbidden-apis`, etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>more_like_this doesn't understand index alias in doc reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14944</link><project id="" key="" /><description>After upgrading from 1.7 to 2.0 I have got a problem with my more_like_this query, which stopped returning results. 

Playing around with the query I was surprised to know that if I set a "like" document "_index" to an alias, no results are returned. When set to the real index name matches are found.

```
GET /_cat/aliases?v&amp;h=alias,index

alias    index         
traits   traits_1123   
analysis analysis_1123 
catalog  catalog_1123  
```

```
POST /traits/library/_search
{
    "filter": {
        "query": {
            "more_like_this": {
                "min_doc_freq": 1, 
                "fields": ["data.packages"], 
                "min_term_freq": 1, 
                "docs": [
                    {
                        "_type": "packages", 
                        "_id": "1_401f94", 
                        "_index": "analysis"
                    }
                ]  
            }    
        }
    }
}

{
   "took": 12,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```

```
POST /traits_1123/library/_search
{
    "filter": {
        "query": {
            "more_like_this": {
                "min_doc_freq": 1, 
                "fields": ["data.packages"], 
                "min_term_freq": 1, 
                "docs": [
                    {
                        "_type": "packages", 
                        "_id": "1_401f94", 
                        "_index": "analysis_1123"
                    }
                ]  
            }    
        }
    }
}

{
   "took": 23,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 12,
      "max_score": 1,
      "hits": [
         {
            "_index": "traits_1123",
            "_type": "library",
...
```
</description><key id="118409841">14944</key><summary>more_like_this doesn't understand index alias in doc reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leonid-s-usov</reporter><labels><label>:More Like This</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-23T15:50:19Z</created><updated>2016-04-20T13:14:37Z</updated><resolved>2016-04-20T13:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="adnansel" created="2015-11-26T14:45:56Z" id="159930670">Same here with v2.1.0. The "more_like_this" doesn't return values if an alias is provided.
</comment><comment author="leonid-s-usov" created="2015-11-26T14:54:45Z" id="159932371">I even looked at the latest code, but found nothing suspicious.. I am far from knowing the code well, but it seems that the semantics of getting the termvectors from the document is the same as in the `_mtermvectors` action, which does work with the same format of document specification, both when using alias and concrete name. We definitely need some attention from the elasticsearch source guru
</comment><comment author="leonid-s-usov" created="2015-12-01T09:28:05Z" id="160906868">Rolled back to 1.7 eventually :/
Hopefully this will get fixed soon
</comment><comment author="mstockerl" created="2016-02-28T15:23:44Z" id="189891528">I might found the problem at hand, but I need some help to fix it. 

As @leonid-s-usov pointed out, the problem occurs after the termvectors were fetched. 
[The responses from the **MultiTermVectorsRequest** are filtered for the given indices of the client call.](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java#L931)
As the responses contain the actual index and _not_ the alias name, every response gets filtered out. 

I think this check should also consider the alias of the indices. 
Can someone give me a hint, how to resolve this issue? I'm not sure, where to best fetch the information about the alias -&gt; index mappings. 

Thanks in advance. 
</comment><comment author="brwe" created="2016-04-20T12:46:25Z" id="212409260">Had a brief discussion with @jpountz who agrees that the decrease in performance is probably negligible. Backported to 2.x 2.3 and 2.2 (e4c20ab, 884ad25 and 4dcdd95). Thanks again @mstockerl !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Could elasticsearch highlight wildcard query search term?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14943</link><project id="" key="" /><description>I haven't find the way to highlight my wildcard query term. as following steps
- Create index

```
curl -XPUT 'http://127.0.0.1:9200/foo' -d '{
    "settings" : {
        "index" : {
            "number_of_shards" : 1
        }
    }}'
```
- Create mapping 

```
curl -XPOST http://127.0.0.1:9200/foo/_mapping/test -d '
{
    "test" : {
        "properties" : {
            "content" : {"type" : "string", "store" : true, "index": "not_analyzed", "doc_values": true}
        }
    }
}
'
```
- Index Some documents

```
curl -XPUT http://127.0.0.1:9200/foo/test/1 -d '{"content": "this is a foobar documents"}'
```
- Search in wildcard query (with highlight)

```
{
  "query": {
    "bool": {
      "should": [
        {
          "wildcard": {
            "content": "*foo*"
          }
        }
      ]
    },
"highlight": {
    "fields": {
      "content": {}
    }
  }
  }
```

While I try another highlight options from the highlight referrences https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html
such as `{"term_vector" : "with_positions_offsets"}` ... those options don't make any sense. 

Those responses were like that

```
"highlight": {"content": "&lt;em&gt;this is a foobar documents&lt;/em&gt;"} 
```

While the final highlight result I expected was  

```
"highlight": {"content": "this is a &lt;em&gt;foo&lt;/em&gt;bar documents"} 
```

Just only highlight `foo` term

Could anyone help me this issue? 
Did I miss some options which could get the expected results or elasticsearch doesn't provided highlight wildcard query term?
</description><key id="118407673">14943</key><summary>Could elasticsearch highlight wildcard query search term?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jkryanchou</reporter><labels /><created>2015-11-23T15:40:34Z</created><updated>2017-03-24T18:24:22Z</updated><resolved>2015-11-23T15:50:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-23T15:50:55Z" id="158975106">&gt; Could anyone help me this issue? 

It doesn't support what you want sadly. Highlighting is on the terms and wildcard queries expand to a list of matching terms. As much as I'd love to support the thing you are asking for I don't see that happening any time soon.
</comment><comment author="jkryanchou" created="2015-11-23T16:24:58Z" id="158986319">@nik9000 Thanks for your &#729;help. Is there any measure I could take for getting my expected results?
Or would elasticsearch plan to provide highlight wildcard term option to support this? I thought it could be a normal requirement  and not to hard to implement : ) 

or I just only highlight it in the front-end.
</comment><comment author="nik9000" created="2015-11-23T16:49:15Z" id="158992960">&gt; I thought it could be a normal requirement and not to hard to implement : )

The trouble is that not much work is going on in highlighting _and_ wildcard queries are generally a bad way to do full text search anyway. They are orders of magnitude slower than getting the analysis chain right for your use case. _and_ that the way that it currently works does a better job of explaining what is happening during search. And that is one of the goal of highlighting.

So, yeah, the supported way to do all this is to make the analyses spit out the terms you want to find and do match queries. For that read more about analysis. And if you need to do some kind of analysis that isn't built in then file an issue around that.
</comment><comment author="jkryanchou" created="2015-11-24T06:56:46Z" id="159172560">OK, I was so appreciated for your help and suggestions : ). I have read the chapter about the analysis. and known the reason why it isn't better to highlight in wildcard query.Thanks so much for your answer. :+1: 
</comment><comment author="Durisvk" created="2017-03-24T18:24:22Z" id="289106384">I needed this feature.. now I have to go through the all hits and do it by myself....</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Aggregation Include/Exclude RegEx does not work in case multi-region matches, look ahead groups etc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14942</link><project id="" key="" /><description>RegEx like `(?=.*elastic)` will match nothing. I it is because java Matcher.matches() may be used in elastic to implement it and it would not work with multi region scenarios. I think Matcher.find() should be used instead. I did not look in Elastic code for this but simulated exact behavior with using matches() vs find()
</description><key id="118404275">14942</key><summary>Terms Aggregation Include/Exclude RegEx does not work in case multi-region matches, look ahead groups etc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2015-11-23T15:25:44Z</created><updated>2015-11-28T15:54:59Z</updated><resolved>2015-11-28T15:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T15:54:59Z" id="160314319">Correct - we use the more limited but safer and faster Lucene FST regexes, not Java regexes here.  We're not planning on changing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to start elasticsearch in IT for tar distribution on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14941</link><project id="" key="" /><description>This seems to happen on windows only:

http://build-us-00.elastic.co/job/es_core_master_window-2008/2574/console

```
:distribution:tar:integTest#wait FAILED
:distribution:tar:integTest#wait (Thread[main,5,main]) completed. Took 30.087 secs.

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':distribution:tar:integTest#wait'.
&gt; Failed to start elasticsearch
```
</description><key id="118398271">14941</key><summary>Failed to start elasticsearch in IT for tar distribution on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>test</label></labels><created>2015-11-23T14:55:49Z</created><updated>2015-11-30T12:40:49Z</updated><resolved>2015-11-30T12:40:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-30T11:24:48Z" id="160604254">This is happening fairly regularly on the windows builds. Today there are 3 failures like this in 2 hours this morning. Example of one of the failures: http://build-us-00.elastic.co/job/es_core_master_window-2008/2595
</comment><comment author="rmuir" created="2015-11-30T12:40:49Z" id="160618283">These are fixed by @rjernst commit https://github.com/elastic/elasticsearch/pull/15098
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve stability of UpdateIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14940</link><project id="" key="" /><description>With this commit, we reduce the amount of work
that UpdateIT does and add progress logging.

Closes #14877
</description><key id="118386663">14940</key><summary>Improve stability of UpdateIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>test</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T14:09:16Z</created><updated>2015-11-28T16:24:52Z</updated><resolved>2015-11-23T16:56:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-23T14:37:34Z" id="158953254">LGTM
</comment><comment author="nik9000" created="2015-11-23T14:39:13Z" id="158953605">Left a variable naming comment but otherwise looks good to me.
</comment><comment author="danielmitterdorfer" created="2015-11-23T16:52:45Z" id="158993943">@jpountz , @nik9000. Thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use a java agent to disable ObjectInputStream for the entire JVM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14939</link><project id="" key="" /><description>We've banned `ObjectInputStream` and friends but they might still be used by libraries. Maybe we can plug that hole with [NotSoSerial](https://github.com/kantega/notsoserial)?
</description><key id="118386119">14939</key><summary>Use a java agent to disable ObjectInputStream for the entire JVM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2015-11-23T14:06:20Z</created><updated>2015-11-23T14:30:02Z</updated><resolved>2015-11-23T14:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-23T14:24:51Z" id="158947793">I don't think we should do this, I think its by far too risky. Changing bootclasspath to modify the behavior of the jvm (either directly or with agent)? no thanks. Looking at their code, I don't think its gonna work under java 9/jigsaw, I think the licensing situation here is still ambiguous, etc.

I also think this agent is an incomplete, risky, knee-jerk reaction to a specific vulnerability. Instead, I think we should just continue to tighten things up in general. For the specific concerns of third party code, e.g. we can start auditing third party libraries that we bring in with forbidden apis (https://github.com/policeman-tools/forbidden-apis/issues/38) to treat them more like our own code. Yes, we can in fact "know what that code is doing" :)

Overall, we can continue to lock things down to bring more reliability and security: e.g. do some real code cleanups to address some of the TODOs and exceptions we have in the security policy. This helps both our own code and third party code.

As far as JDK code, yes its kind of a special case, but I think addressing it by replacing classes in the JDK is the wrong technique. If we distrust the JVM, then we should sandbox it better, and the right guy to do that is the operating system. Today this is minimal (only dropping ability to execute), because its a black and white thing. This is a tradeoff to keep in mind. So if we want to expand e.g. system call filtering to do more, or if we want OS packaging to bring in its own JDK that we apparmor or whatever, we have to think about the implications: we can't have plugins declaring exceptions and stuff like we can at the java level.
</comment><comment author="rmuir" created="2015-11-23T14:30:02Z" id="158950888">Also, forgot to mention, containing the JVM at the OS level doesn't have to be complex. Even stuff like tidying up filesystem permissions in our packaging can be a great improvement there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split mutate processor into one processor per function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14938</link><project id="" key="" /><description>Split mutate processor into one processor per function
</description><key id="118384115">14938</key><summary>Split mutate processor into one processor per function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-23T13:54:21Z</created><updated>2015-11-24T13:58:19Z</updated><resolved>2015-11-24T13:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-11-24T01:29:32Z" id="159123026">just left a few questions, tests look great!
</comment><comment author="martijnvg" created="2015-11-24T09:14:57Z" id="159203593">This is great! Left some minor comments. I do wonder if we group some processors in a single package? Right now there are a lot of packages with a single class, which kind of defeats the purpose of packages imo. Regardless LGTM.
</comment><comment author="javanna" created="2015-11-24T11:01:25Z" id="159230275">I rebased and addressed comments, also added missing javadocs to each of the new processor. I agree regarding packages, I followed the "one processor per package" convention that we seem to have adopted, but we do end up with lots of packages, I guess we can discuss this as a followup.
</comment><comment author="martijnvg" created="2015-11-24T11:36:56Z" id="159238434">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UNASSIGNED shards after restore snap of a different cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14937</link><project id="" key="" /><description>Hello Team,
I've placed a backup taken from another Elastic cluster (one node cluster &#172;&#172;). I've restored this snapshot in another cluster (two nodes cluster) and all shards are "UNASSIGNED". I've tried all I could but there is no way to solve this issue.

Config file for both nodes is the following:

cluster.name: EXAMPLE
node.name: ${HOSTNAME}
network.bind_host: ${HOSTNAME}
network.publish_host: ${HOSTNAME}
network.host: ${HOSTNAME}
bootstrap.mlockall: true
discovery.zen.ping.timeout: 5s
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["generic_name1","generic_name2"]

The only difference is one of the is marked as "node.data: false" to avoid disk writing.
I'm using 2.0.0 version

What could be wrong?

Thanks in advance
</description><key id="118378567">14937</key><summary>UNASSIGNED shards after restore snap of a different cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leunamnauj</reporter><labels /><created>2015-11-23T13:17:59Z</created><updated>2015-11-23T18:20:02Z</updated><resolved>2015-11-23T18:20:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>rename Data to IngestDocument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14936</link><project id="" key="" /><description>Also move metadata to a map instead of separate fields. This would make it easier to operate on metadata in general from processors.
</description><key id="118368690">14936</key><summary>rename Data to IngestDocument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-23T12:13:36Z</created><updated>2015-11-23T18:55:33Z</updated><resolved>2015-11-23T17:26:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-11-23T12:21:27Z" id="158918087">and if/when we add stats collectors to this construct we'll rename it `MetaDataAndSourceAndStatsCollectors`? Personally I prefer any of the following options:
- `ProcessedDocument`
- `ProcessedDoc`
- `IngestDoc`
- `IngestDocument`
- `IngestedDoc`
- `IngestedDocument`
</comment><comment author="martijnvg" created="2015-11-23T12:32:39Z" id="158921394">We really wanted to get rid of the name `Data`, but we we couldn't settle on a good name. So I picked `MetaDataAndSource` as a name that described what it holds. But looking at the code now I'm also okay with `IngestDocument`.

(note: the other change here is that metadata is added to a map instead of separate fields)
</comment><comment author="uboness" created="2015-11-23T12:36:41Z" id="158922819">I'm also good with `IngestDocument`, lets see how the others feel about it.

&gt; (note: the other change here is that metadata is added to a map instead of separate fields)

++
</comment><comment author="javanna" created="2015-11-23T12:58:21Z" id="158926753">IngestDocument++ sounds good to me
</comment><comment author="martijnvg" created="2015-11-23T13:08:50Z" id="158929323">@javanna @uboness I've updated the PR.
</comment><comment author="javanna" created="2015-11-23T15:45:08Z" id="158973517">left one minor comment, LGTM though.
</comment><comment author="talevy" created="2015-11-23T18:55:33Z" id="159027322">woo! no more `Data`!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build and attach Javadoc artifact on release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14935</link><project id="" key="" /><description>This PR creates and attaches the Javadoc artifact for Elasticsearch (core) during a release and will upload that artifact to the Maven repository, along with the original binary artifact and the source artifact.

This change would enable users and developers use a service like http://www.javadoc.io/ to read the Elasticsearch Javadoc instead of having to build it themselves or read the complete source code.

If this PR is being merged (or at least a candidate for merging), I would port it to the Gradle build in the `master` branch as well.

Refs #1203
</description><key id="118367262">14935</key><summary>Build and attach Javadoc artifact on release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joschi</reporter><labels><label>docs</label><label>v2.3.0</label></labels><created>2015-11-23T12:02:02Z</created><updated>2016-01-19T11:49:21Z</updated><resolved>2016-01-18T23:36:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-24T08:56:48Z" id="159199435">I left one comment. Also, this has already been added in master for gradle in #14766
</comment><comment author="joschi" created="2015-11-25T14:17:45Z" id="159619562">Rebased on current `2.x` branch.
</comment><comment author="joschi" created="2015-12-27T14:34:46Z" id="167418088">Rebased on current 2.x branch and pulled in Javadoc fixes from #13702 and #14766.
</comment><comment author="s1monw" created="2016-01-06T08:21:44Z" id="169264752">@joschi thanks a lot for the effort, would it be possible to only commit the pom.xml file change and make it a bit more lenient or at least also do these changes on master? @rjernst WDYT?
</comment><comment author="joschi" created="2016-01-06T12:04:39Z" id="169308840">@s1monw Thanks for coming back to this PR.

The changes of the POM on themselves aren't too useful because it'll make the build fail due to the Javadoc lint errors that Java 8 will emit (not a problem on Java 7, though), see the previous comments by @rjernst (https://github.com/elastic/elasticsearch/pull/14935#discussion_r45709095).

The second commit pulls in the Javadoc fixes from #13702 and #14766 so that the build won't fail (can be checked by simply running `mvn javadoc:javadoc`). Those have already been applied to `master` so that this PR is purely for the 2.x line of Elasticsearch.
</comment><comment author="s1monw" created="2016-01-07T16:29:33Z" id="169717524">@joschi I am really wondering if we should go through the trouble and fix it in 2.x. It's happening in master and maybe we should just go with what we have to reduce the noise?
</comment><comment author="joschi" created="2016-01-07T16:36:31Z" id="169720431">@s1monw In the end it's your call. In my opinion having publicly available Javadoc for Elasticsearch 2.x is worth the relatively minor changes (and it's only Javadoc&#8230;) in this PR.
</comment><comment author="s1monw" created="2016-01-07T16:39:45Z" id="169721334">fair enough @rjernst I am ok with it, WDYT?
</comment><comment author="rjernst" created="2016-01-18T22:23:54Z" id="172669930">@joschi I'm happy to merge this, but I see some problems in the diff. It looks like some changes that were meant for master only were inadvertently backported. For example, look at TransportShardMultiPercolateAction.java. Your patch has this:

```
-                if (in.getVersion().onOrAfter(Version.V_2_3_0)) {
-                    shardRequest.startTime(in.readLong());
-                }
```

Does running `mvn verify` work for you? The patch is so large I'm concerned other unintentional changes could be backported that could have similar bad results. @s1monw Because of this, I am still not sure if it is worth the risk to backport.
</comment><comment author="joschi" created="2016-01-18T23:09:27Z" id="172677664">@rjernst I can't find any change of that file (`core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java`) in my branch compared to the `2.x` branch:

```
$ git rev-parse --abbrev-ref HEAD
publish-javadoc-artifact
$ git diff 2.x..publish-javadoc-artifact ./core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java
$
```

Maybe you're using an outdated state of this branch?

And for what it's worth, `mvn verify` successfully runs on this branch:

```
INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building Elasticsearch: Core 2.3.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[...]
[INFO] --- maven-failsafe-plugin:2.18.1:verify (verify) @ elasticsearch ---
[INFO] Failsafe report directory: /Users/joschi/src/elasticsearch/core/target/failsafe-reports
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10:20 min
[INFO] Finished at: 2016-01-19T00:21:42+01:00
[INFO] Final Memory: 46M/1324M
[INFO] ------------------------------------------------------------------------
```
</comment><comment author="javanna" created="2016-01-18T23:22:09Z" id="172679437">I remember reviewing such a change a few days ago, I think it was #15938, so the change has not been introduced with this PR.
</comment><comment author="rjernst" created="2016-01-18T23:31:58Z" id="172681323">Doh! Sorry for the confusion! The script I use to checkout a PR messed this up. I've reviewed now and I will merge shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Geobounds Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14934</link><project id="" key="" /><description /><key id="118363683">14934</key><summary>Aggregations Refactor: Refactor Geobounds Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-23T11:36:29Z</created><updated>2015-11-24T09:33:04Z</updated><resolved>2015-11-24T09:33:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-23T14:28:26Z" id="158949880">LGTM, I just left minor comments
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make PointBuilder, CircleBuilder &amp; EnvelopeBuilder implement Writable </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14933</link><project id="" key="" /><description>After refactoring the queries to make them parsable on the coordinating note and adding serialization and equals/hashCode capability to them. So far ShapeBuilders nested inside queries were still transported as a byte array that needs to be parsed later on the shard receiving the query. To be able to also serialize geo shapes this way, we also need to make all the implementations of ShapeBuilder implement Writable. This PR adds this to PointBuilder, CircleBuilder and EnvelopeBuilder and also adds tests for serialization, equality and hashCode.

Relates to #14416
</description><key id="118363682">14933</key><summary>Make PointBuilder, CircleBuilder &amp; EnvelopeBuilder implement Writable </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T11:36:29Z</created><updated>2015-11-28T15:17:01Z</updated><resolved>2015-11-25T08:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-11-24T21:46:54Z" id="159414783">This looks good to me. In serialization logic I'd like to see each readFrom on their own line (just like writeTo). Maybe its just me but its easier to read/maintain when you can track one-to-one read/write.
</comment><comment author="cbuescher" created="2015-11-25T08:13:33Z" id="159530431">@nknize thanks for the review, adressed your comments and will merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Processing pending deletes can block shard initialisation for 30 minutes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14932</link><project id="" key="" /><description>Test failure: http://build-us-00.elastic.co/job/es_core_master_window-2008/2553/testReport/junit/org.elasticsearch.indices.state/RareClusterStateIT/testDeleteCreateInOneBulk/

The test fails due to a race in acquiring `ShardLock` locks. 

When an index is deleted, an asynchronous process is started to process pending deletes on shards of that index. This process first acquires all `ShardLock` locks for the given index in numeric shard order. Meanwhile, the new index can already have been created, and some shard locks can already be held due to shard creation in `IndicesClusterStateService.applyInitializingShard`. For example, shard 0 is locked by `processPendingDeletes` but shard 1 is locked by `applyInitializingShard`. In that case, `processPendingDeletes` cannot lock shard 1 and blocks (and will hold lock on shard 0 for 30 minutes). This means that shard 0 cannot be initialised for 30 minutes.

Interesting bits of stack trace:

```
"elasticsearch[node_t1][generic][T#2]" ID=602 TIMED_WAITING on java.util.concurrent.Semaphore$NonfairSync@2fc45c3b
    at sun.misc.Unsafe.park(Native Method)
    - timed waiting on java.util.concurrent.Semaphore$NonfairSync@2fc45c3b
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
    at java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:409)
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:555)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:485)
    at org.elasticsearch.env.NodeEnvironment.lockAllForIndex(NodeEnvironment.java:429)
    at org.elasticsearch.indices.IndicesService.processPendingDeletes(IndicesService.java:649)
    at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.lockIndexAndAck(NodeIndexDeletedAction.java:101)
    at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.access$300(NodeIndexDeletedAction.java:46)
    at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction$1.doRun(NodeIndexDeletedAction.java:90)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Locked synchronizers:
    - java.util.concurrent.ThreadPoolExecutor$Worker@b17810e


"elasticsearch[node_t1][clusterService#updateTask][T#1]" ID=591 TIMED_WAITING on java.util.concurrent.Semaphore$NonfairSync@7fdcd730
    at sun.misc.Unsafe.park(Native Method)
    - timed waiting on java.util.concurrent.Semaphore$NonfairSync@7fdcd730
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
    at java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:409)
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:555)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:485)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:234)
    - locked org.elasticsearch.index.IndexService@707e1798
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:628)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:528)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    - locked java.lang.Object@773b911a
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:517)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Locked synchronizers:
    - java.util.concurrent.ThreadPoolExecutor$Worker@26f887da
```
</description><key id="118358717">14932</key><summary>Processing pending deletes can block shard initialisation for 30 minutes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Store</label><label>bug</label><label>jenkins</label></labels><created>2015-11-23T11:01:11Z</created><updated>2016-03-15T03:30:44Z</updated><resolved>2016-03-15T03:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-23T11:03:38Z" id="158905362">really this is a bug in how elasticsearch works altogether. all these locks and all the algs we have work on the index name rather than on it's uuid which is a huge problem. That's the place to fix this rather than changing the way we process pending deletes.
</comment><comment author="s1monw" created="2015-11-23T13:28:35Z" id="158933365">we already have some issues related to this: https://github.com/elastic/elasticsearch/issues/13264 and https://github.com/elastic/elasticsearch/issues/13265 (this one is rather unrelated but goes into the same direction of safety)
</comment><comment author="ywelsch" created="2015-12-01T10:50:42Z" id="160932738">I disabled the test on branches master, 2.x, 2.1 and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve trace logging in TransportReplicationAction and error reporting at RecoveryWhileUnderLoadIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14931</link><project id="" key="" /><description>Things that helped me traced down an issue.
</description><key id="118356662">14931</key><summary>Improve trace logging in TransportReplicationAction and error reporting at RecoveryWhileUnderLoadIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Logging</label><label>non-issue</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T10:46:17Z</created><updated>2016-08-31T22:39:35Z</updated><resolved>2015-11-23T12:56:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-23T12:26:03Z" id="158918771">I think that all of the calls to `logger#trace` touched in this change set should be wrapped in `if`-blocks guarded by the condition `logger.isTraceEnabled()` lest we always be needlessly allocating object arrays (because of the varargs parameter) (except for two that are already guarded so). Otherwise, LGTM.
</comment><comment author="bleskes" created="2015-11-23T12:49:25Z" id="158925311">@jasontedor thanks. I pushed an update. I chose to only protect the trace logs which are done under "normal" operation. The exceptional cases are not worth the extra if imho.
</comment><comment author="jasontedor" created="2015-11-23T12:51:01Z" id="158925561">@bleskes Okay. LGTM.
</comment><comment author="imotov" created="2016-08-31T22:39:35Z" id="243924410">testRecoverWhileRelocating failed today:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=debian/112/console
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create multi nodes on single cluster into ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14930</link><project id="" key="" /><description>Hi, is possible to create a multi nodes on a single cluster into ES 2.0?
I've tried the common config and some console commands but both don't work.
I known that some console commands are disabled but i can't create a second node on my db.

Someone can help me???
</description><key id="118354409">14930</key><summary>Create multi nodes on single cluster into ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cirot87</reporter><labels /><created>2015-11-23T10:32:24Z</created><updated>2015-11-23T10:35:13Z</updated><resolved>2015-11-23T10:35:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-23T10:35:13Z" id="158900313">Please join us on discuss.elastic.co.

We can help you better there.

We use this only for bug reports.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14929</link><project id="" key="" /><description>I just started learning elasticSearch.

I don't know anything about previous versions of elasticSearch, so my first inputs has been with elastic 2.0.

I used the "official" documentation in order to understand how does it work and how to interact.
I must say that for me was quite strange, that the Search APIs was explained much before the Mapping.

While trying examples of the Search API on local, i was wondering why the endpoints where not working.

I hope this feedback can help improve the Doc's.

DAVID

P.S: not sure if this is the right place to put this feedback.
</description><key id="118353967">14929</key><summary>Mapping </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mabodo</reporter><labels /><created>2015-11-23T10:30:10Z</created><updated>2015-11-28T15:14:49Z</updated><resolved>2015-11-28T15:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T15:14:49Z" id="160309868">thanks for the feedback @mabodo - it's tricky to get this right.  For instance, very often you can rely entirely on dynamic mappings.  We try to introduce these subjects in a more useful order for the beginnger in the [Definitive Guide](https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort and truncate pipeline aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14928</link><project id="" key="" /><description>This would be a parent pipeline aggregation which would take the following option:
- Sort - a bucketsPath that defines the aggregation(s) in each bucket to use to sort the buckets in the parent aggregation. If this is not provided no sorting is done on the buckets
- Offset - all buckets before this index in the sorted buckets list will removed. Defaults to 0 (i.e. don't remove any buckets from the beginning of the list)
- Size - all buckets after `offset + size` index in the sorted buckets list will be removed. Defaults to `buckets.size()` (i.e. don't remove any buckets from the end of the list)
</description><key id="118341033">14928</key><summary>Sort and truncate pipeline aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>feature</label></labels><created>2015-11-23T09:02:47Z</created><updated>2017-07-24T09:12:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-02-25T15:43:16Z" id="188847122">+1
</comment><comment author="taowen" created="2016-02-29T16:06:15Z" id="190270404">+1
</comment><comment author="davyfeng" created="2016-03-21T14:59:59Z" id="199326955">+1
</comment><comment author="appasahebs" created="2016-04-19T10:58:50Z" id="211860772">+1
</comment><comment author="ghost" created="2016-06-03T08:00:16Z" id="223515655">+1
</comment><comment author="soulne4ny" created="2016-09-07T12:23:11Z" id="245263576">+1

It is quite surprising that `bucket_script` is so much different from other aggregations that ordering aggregation buckets by `bucket_script` is impossible. `order` clause just does not see a `bucket_script` name.
</comment><comment author="dizzzyroma" created="2016-12-31T14:13:37Z" id="269866720">+1</comment><comment author="waiter" created="2017-02-08T09:30:18Z" id="278276335">+1</comment><comment author="dizzzyroma" created="2017-02-17T18:10:12Z" id="280724356">Any ideas when this will be in production?</comment><comment author="gqy2468" created="2017-02-22T08:05:13Z" id="281597257">+1</comment><comment author="xavigonz" created="2017-03-22T16:21:30Z" id="288454546">+1</comment><comment author="SimonKlausLudwig" created="2017-03-22T17:30:24Z" id="288476895">+1
</comment><comment author="developerworks" created="2017-04-13T07:30:45Z" id="293814587">+1</comment><comment author="thao6626" created="2017-05-10T12:55:06Z" id="300473005">+1</comment><comment author="54ebb" created="2017-07-24T03:24:25Z" id="317310289">+1</comment><comment author="claudiuolteanu" created="2017-07-24T09:12:24Z" id="317364562">+1</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After upgrading from ES 1.7 to 2.0, getting lots of IllegalIndexShardStateException when bulk creating indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14927</link><project id="" key="" /><description>Hello,

We have a tool that creates a language-contenttype matrix of indices at first run (it checks what indices are missing from the matrix), which functioned well using ES 1.7 but now we're getting lots of  `IllegalIndexShardStateException` exceptions.

So we issue a `_cluster/health?wait_for_status=yellow` before each index creation HTTP request, still same issue.

We wait 1000ms before the cluster health request, and 1000ms after each index creation request, still same issue.

Could it be that the yellow status is reached too fast? We always run with an empty ES instance when testing (ie remove `data` and `logs` dir before starting). Sometimes even the first index to be created seems to cause the error?

Tips for further investigation is appreciated.

```
[2015-11-23 07:37:58,716][INFO ][cluster.metadata         ] [my-name] [content__1fzc2hfheucqdv2_q5nmpa__en] creating index, cause [api], templates [], shards [5]/[1], mappings [content]
[2015-11-23 07:38:00,983][INFO ][cluster.metadata         ] [my-name] [content__p9p2lcwchuwe4atddvvd2w__en] creating index, cause [api], templates [], shards [5]/[1], mappings [content]
[2015-11-23 07:38:03,281][INFO ][cluster.metadata         ] [my-name] [content__tuslngocl06jrzeywjpdzw__en] creating index, cause [api], templates [], shards [5]/[1], mappings [content]
[2015-11-23 07:38:05,576][INFO ][cluster.metadata         ] [my-name] [content__hkwufjcos0urbpkg6t7dnq__en] creating index, cause [api], templates [], shards [5]/[1], mappings [content]
[2015-11-23 07:38:07,968][INFO ][cluster.metadata         ] [my-name] [content__w6mzamir00cz2uk6ot7ijw__en] creating index, cause [api], templates [], shards [5]/[1], mappings [content]
[2015-11-23 07:38:08,299][DEBUG][action.admin.indices.stats] [my-name] [indices:monitor/stats] failed to execute operation for shard [[content__w6mzamir00cz2uk6ot7ijw__en][3], node[NSXOyiZxTT63LKBFnm3ESA], [P], v[1], s[INITIALIZING], a[id=OckfAhmIRiSxOjXG8J4Mcg], unassigned_info[[reason=INDEX_CREATED], at[2015-11-23T06:38:07.969Z]]]
[content__w6mzamir00cz2uk6ot7ijw__en][[content__w6mzamir00cz2uk6ot7ijw__en][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:399)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:376)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:365)
  at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: [content__w6mzamir00cz2uk6ot7ijw__en][[content__w6mzamir00cz2uk6ot7ijw__en][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
  at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:957)
  at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:791)
  at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:612)
  at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
  at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
  at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:395)
  ... 7 more
```
</description><key id="118339148">14927</key><summary>After upgrading from ES 1.7 to 2.0, getting lots of IllegalIndexShardStateException when bulk creating indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bjorn-ali-goransson</reporter><labels /><created>2015-11-23T08:47:12Z</created><updated>2015-11-23T21:08:09Z</updated><resolved>2015-11-23T18:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-23T08:51:39Z" id="158878807">@jasontedor do you mind taking a look at this?
</comment><comment author="jasontedor" created="2015-11-23T18:03:42Z" id="159013624">The `IllegalIndexShardStateException`s that you are seeing in the log messages here are not from the index creation, but from stats requests that are being issued. These stats requests appear to be issued while the physical shards are in the state of `IndexShardState.RECOVERING` which is a state that physical shards pass through, for instance, upon index creation.

I think that the reason that you're seeing this behavior on 2.0 but were not seeing it on 1.7 is because of a [change](https://github.com/elastic/elasticsearch/commit/5cb86130ec65c508b25b503d228f0acfb1797663#diff-be8942a0b62ccd463e4b261e67d110a0R65) to the shards that are considered during an indices stats request.

This shouldn't be negatively impacting your cluster other than spamming your logs. If you want to prevent it, you'll need to track down the source of those stats requests and, if needed, make them wait for the index health to be green before issuing a stats request against the index.
</comment><comment author="bleskes" created="2015-11-23T19:28:21Z" id="159036985">@jasontedor correct if I'm wrong, but I don't see the change to IndexShard.docStats that enforces readAllowed() . Also, I think it's wrong to block stats report if the engine is already open and we are recovering. I'm thinking, for example, of reporting the translog length, or the memory signature of lucene, number of segments etc. If the engine is not yet open, we can ignore the shard as it is not relevant for the stats. I feel a trace message is more appropriate here. 
</comment><comment author="jasontedor" created="2015-11-23T19:43:39Z" id="159041563">&gt; correct if I'm wrong, but I don't see the change to IndexShard.docStats that enforces readAllowed()

@bleskes I wasn't suggesting it was a change to `IndexShard#docStats`; the [change](https://github.com/elastic/elasticsearch/commit/5cb86130ec65c508b25b503d228f0acfb1797663#diff-be8942a0b62ccd463e4b261e67d110a0R65) is to `TransportIndicesStatsAction#shards`.

&gt; If the engine is not yet open, we can ignore the shard as it is not relevant for the stats.

The shard doesn't get counted as "failed" in the stats response.

&gt; I feel a trace message is more appropriate here.

Okay.
</comment><comment author="bleskes" created="2015-11-23T19:51:57Z" id="159044280">Oh , I see. OK :) before we didn't even try... 

As I said, I think we can be less strict then readAllowed() but that's not a regression then. Re the trace - note how we ignore this error completely in the response:

```
 if (!TransportActions.isShardNotAvailableException(throwable)) {
                        exceptions.add(new DefaultShardOperationFailedException(throwable.getIndex(), throwable.getShardId().getId(), throwable));
                    }
```
</comment><comment author="jasontedor" created="2015-11-23T20:00:13Z" id="159046927">Yes, see #14950.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove old files replaced by gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14926</link><project id="" key="" /><description>This change removes files that are no longer needed with the gradle
build. The license checker was already rewritten in groovy. The plugin
descriptor template exists in buildSrc resources. log4j properties was
moved to the test framework. site_en.xml seems to be a legacy file,
there are no references to it anywhere in the maven build that I could
find. The update lucene script was just a helper for running the license
check in update mode, but that can be done with gradle using the
updateShas command. Finally, there was a leftover build.gradle from when
I attempted to make dev-tools a project of its own.
</description><key id="118333599">14926</key><summary>Remove old files replaced by gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T07:46:05Z</created><updated>2015-11-24T08:54:25Z</updated><resolved>2015-11-24T08:54:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-23T15:22:53Z" id="158966864">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back leaveTemporary and ifNoTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14925</link><project id="" key="" /><description>This change adds back the last of the missing test options to the junit4
wrapper. leaveTemporary is important in that setting it to true (which
is how we had it set in maven) removes the warnings we currently get
about a leftover file that cannot be deleted (from jna).
</description><key id="118329477">14925</key><summary>Add back leaveTemporary and ifNoTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T07:04:49Z</created><updated>2015-11-24T08:53:27Z</updated><resolved>2015-11-24T08:53:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-23T15:23:47Z" id="158967079">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>documentation error - Offline update </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14924</link><project id="" key="" /><description>https://www.elastic.co/guide/en/marvel/current/installing-marvel.html#offline-installation

Says bin/plugin install file:///...
the right command and what plugin install -h says:  bin/plugin install file:/...
</description><key id="118328401">14924</key><summary>documentation error - Offline update </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gadin</reporter><labels><label>:Plugins</label><label>feedback_needed</label></labels><created>2015-11-23T06:52:30Z</created><updated>2016-02-14T17:34:26Z</updated><resolved>2016-02-14T17:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T15:05:13Z" id="160307909">@gadin the existing instructions work for me.  Why do you think it is incorrect?
</comment><comment author="clintongormley" created="2016-02-14T17:34:26Z" id="183933888">no further feedback. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce that gradle idea was run before importing in intellij</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14923</link><project id="" key="" /><description>Due to how intellij imports gradle projects, the tasks which setup
resources are not run. This means we must be sure to run gradle idea
from the command line before importing into elasticsearch. This change
adds a simple marker file to indicate we have run from the command line
before importing. It won't help for new projects that add plugin
metadata, but it will at least make sure the initial project is set up
correctly.
</description><key id="118300244">14923</key><summary>Enforce that gradle idea was run before importing in intellij</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-23T00:10:32Z</created><updated>2015-11-23T10:16:21Z</updated><resolved>2015-11-23T03:03:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-23T02:29:59Z" id="158833566">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.1.0 Release notes page links are missing a slash (/)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14922</link><project id="" key="" /><description>In the 2.1.0 release notes:

https://www.elastic.co/guide/en/elasticsearch/reference/2.1/release-notes-2.1.0.html

All the PR and issue links are missing a final slash before the PR/issue number. 

For example the link for the CRUD PR is:

https://github.com/elastic/elasticsearch/pull11306

Should be:

https://github.com/elastic/elasticsearch/pull/11306
</description><key id="118297183">14922</key><summary>2.1.0 Release notes page links are missing a slash (/)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>docs</label></labels><created>2015-11-22T23:09:44Z</created><updated>2015-11-24T11:37:13Z</updated><resolved>2015-11-24T11:37:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Get multi node smoke tests working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14921</link><project id="" key="" /><description>This change adds back the multi node smoke test, as well as making the
cluster formation for any test allow multiple nodes. The main changes in
cluster formation are abstracting out the node specific configuration to
a helper struct, as well as making a single wait task that waits for all
nodes after their start tasks have run. The output on failure was also
improved to log which node's info is being printed.
</description><key id="118296346">14921</key><summary>Get multi node smoke tests working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-22T22:51:02Z</created><updated>2015-11-23T07:05:31Z</updated><resolved>2015-11-22T23:27:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-22T23:25:36Z" id="158817987">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed minor typo in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14920</link><project id="" key="" /><description>Just a minor documentation issue I found while reading through the code. Not major.
</description><key id="118287501">14920</key><summary>Fixed minor typo in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NullSoldier</reporter><labels><label>docs</label></labels><created>2015-11-22T20:46:23Z</created><updated>2015-11-28T15:01:08Z</updated><resolved>2015-11-28T15:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T15:00:54Z" id="160307279">thanks @NullSoldier - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add circuit breaker and/or cap for fixedbitset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14919</link><project id="" key="" /><description>Some users in the field have nodes that are running out of memory due fixedbitset cache using the majority of the heap.  See #10224.  Some of them are already on 1.7.x which means that they have the fixes that will help reduce the amount of fixedbitset loaded (#8440 and #8454).  It will be helpful to add protection to the product to prevent OOM scenarios.  Users are currently working around this by disabling eager loading of fixedbitsets (#14903) - and hoping that it will not eventually lazy load enough fixedbitset to hit the OOM again.
</description><key id="118285834">14919</key><summary>Add circuit breaker and/or cap for fixedbitset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Cache</label><label>:Circuit Breakers</label><label>discuss</label><label>enhancement</label></labels><created>2015-11-22T20:21:43Z</created><updated>2015-11-27T23:01:36Z</updated><resolved>2015-11-27T10:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-27T10:46:20Z" id="160111164">Discussed in Fixit Friday: there is a broader issue that nested fields are often abused, and fixedbitset memory usage is only one of the symptoms. The soft limit for nested fields that is proposed on #14983 should help avoid this issue most of the time.

Separately, we should work on ways to not have to load things into memory at all for nested. One obvious option would be to use numeric doc values, but there might be even better options.
</comment><comment author="schlosna" created="2015-11-27T21:56:44Z" id="160217986">It is a bit concerning that any unbounded "cache" exists which can grow and take nodes down due to OutOfMemoryError. Is it not feasible to provide reasonable bounds and circuit breakers to prevent this from occurring? Should we not also be able to flush the fixed bitset cache independently from other caches?
</comment><comment author="jpountz" created="2015-11-27T22:18:22Z" id="160219233">I don't like the memory usage of these bitsets either, hence my comment to move to disk-based data-structures.

Currently it is implemented as a cache, but it should really be part of the index. Every query that needs to load one entry in this cache is doomed to run in linear time with the number of documents in the index (like a linear scan), which is something that we usually try to avoid by all means.

I suspect that if these bitsets are an issue for memory usage, most of the time it means that there are many nested mappings, in which case memory usage of these bitsets is only one of the problems. Nested fields increase the sparsity of the index, which makes things less efficient in terms of disk, cpu and memory usage.
</comment><comment author="rmuir" created="2015-11-27T23:01:36Z" id="160224497">what happened to the idea to disable the dynamic generation of these? I think thats the root cause.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set an newly created IndexShard's ShardRouting before exposing it to operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14918</link><project id="" key="" /><description>The work for #10708 requires tighter integration with the current shard routing of a shard. As such, we need to make sure it is set before the IndexService exposes the shard to external operations.
</description><key id="118284845">14918</key><summary>Set an newly created IndexShard's ShardRouting before exposing it to operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-22T20:00:15Z</created><updated>2015-11-28T15:16:26Z</updated><resolved>2015-11-23T10:24:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-23T10:02:49Z" id="158892554">LGTM
</comment><comment author="s1monw" created="2015-11-23T11:05:11Z" id="158905627">good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to run as daemon?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14917</link><project id="" key="" /><description>I am use bin/elasticsearch to start service.
But when I ctrl+C to exit shell , then service also stop.
How can I run it daemon?
like bin/elasticsearch -d ?
</description><key id="118254335">14917</key><summary>How to run as daemon?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">perzy</reporter><labels /><created>2015-11-22T10:03:31Z</created><updated>2015-11-22T10:54:52Z</updated><resolved>2015-11-22T10:54:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-22T10:54:52Z" id="158745386">Please ask questions like this on discuss.elastic.co.

We provide packages like deb, rpm and windows so you can use them to run elasticsearch as a service.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce similar tasks run for dependencies first</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14916</link><project id="" key="" /><description>Gradle ensures task dependencies are executed in the correct order.
However, project dependencies only build what is needed for the
dependency. This means the order of higher level tasks are not
guaranteed. This change adds task ordering between test and integTest
for a project and its dependencies.
</description><key id="118252539">14916</key><summary>Enforce similar tasks run for dependencies first</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-22T09:22:20Z</created><updated>2015-11-22T15:41:51Z</updated><resolved>2015-11-22T15:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-22T12:26:06Z" id="158754171">LGTM. Left a suggestion that you can ignore and merge anyway if you want.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused imports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14915</link><project id="" key="" /><description /><key id="118251570">14915</key><summary>Remove unused imports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gm06041</reporter><labels /><created>2015-11-22T08:50:23Z</created><updated>2015-11-22T13:17:19Z</updated><resolved>2015-11-22T13:15:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-22T12:30:44Z" id="158754407">Github's got this trying to revert lots of changes to master right now.

Also, could you sign the CLA?
</comment><comment author="jasontedor" created="2015-11-22T13:17:19Z" id="158759019">The change set in this pull request shows reverts to changes that are integrated into master. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ban write access to system properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14914</link><project id="" key="" /><description>System properties are jvm-wide and change global behavior: we can't have e.g. one plugin just willy-nilly changing default ssl configuration (https://github.com/aws/aws-sdk-java/issues/552) or xml parser configuration (https://github.com/aws/aws-sdk-java/blob/62358657d7dc658116705feb0d4de0cee0d56a26/aws-java-sdk-core/src/main/java/com/amazonaws/util/XpathUtils.java#L104-L113), it completely destroys all isolation: this stuff can sneakily cause problems in unrelated code (e.g. another plugin or even core ES code).

This PR forbids `System.setProperty` and friends in forbidden APIs, and bans property write access at runtime with SecurityManager. Plugins that need to modify system properties for some reason will need to request permission in their `plugin-security.policy`.

It does not yet make system properties completely immutable: there is a whitelist of cleanups that must be done first. Trying to accomplish all these cleanups at once would make this change large, we can just iterate and whittle down the list.
</description><key id="118236964">14914</key><summary>Ban write access to system properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Settings</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-22T03:44:31Z</created><updated>2015-11-28T14:50:32Z</updated><resolved>2015-11-22T16:28:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-22T06:16:13Z" id="158716107">This looks great. The list of properties to work through as follow ups is the way to make changes like this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add precommit checks to standalone tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14913</link><project id="" key="" /><description /><key id="118217511">14913</key><summary>Add precommit checks to standalone tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-21T19:51:51Z</created><updated>2015-11-21T20:29:03Z</updated><resolved>2015-11-21T20:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-21T20:16:18Z" id="158678546">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change project attachment into special extra-plugins dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14912</link><project id="" key="" /><description>Currently we use the "gradle project attachment plugin" to support
building elasticsearch as part of another project. However, this plugin
has a number of issues, a large part of which is requiring consistent
use of the projectsPrefix.

This change removes projectsPrefix, and adds support for a special
extra-plugins directory in the root of elasticsearch. Any projects
checked out within this directory will be automatically added to
elasticsearch.
</description><key id="118215452">14912</key><summary>Change project attachment into special extra-plugins dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-21T19:16:41Z</created><updated>2015-11-22T16:45:27Z</updated><resolved>2015-11-22T16:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-22T16:21:18Z" id="158775348">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rejected task exception with ES Java BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14911</link><project id="" key="" /><description>Hi,
I am using Elasticsearch 1.4.4.and processing several documents using a BlockingThreadPoolExecutor.
The last step in the process(on each thread) is to index the document into elasticsearch. For this I'm using the BulkProcessor API.
I instantiate a bulkprocessor in the main thread, and pass it into the thread pool, and it is used by all the threads. I did this because I noticed that it internally uses a 'synchronized' add method, and the class description mentions "thread safe".

My problem is that I'm seeing a RejectedTaskExecution exception. Sometimes it shows up after just a couple of hundred requests at other times after several thousand. Irrespective, it shows up each and every time. 

My blockingthreadpoolexecutor uses a semaphore internally to throttle requests. And it is backed by an ArrayBlockingQueue. 

To ensure that the issue was related  with the call to BulkProcessor, I replaced the call to BulkProcessor.add(....) with a simple Thread.sleep(10,TimeUnit.seconds). My threadpoolexecutor works as expected.

I am including the relevant code below.
I hope I'm not doing something wrong, but I'm new to ES on the java api, any help is appreciated.

```
BlockingThreadPoolExecutor.java

public class BlockingThreadPoolExecutor extends ThreadPoolExecutor {
private final Semaphore semaphore;

public BlockingThreadPoolExecutor(int corePoolSize, int maximumPoolSize,
        long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) {
    super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue);
    semaphore = new Semaphore(corePoolSize + 50);
}

@Override
protected void beforeExecute(Thread t, Runnable r) {
    super.beforeExecute(t, r);
}

@Override
public void execute(final Runnable task) {
    boolean acquired = false;

    do {
        try {
            semaphore.acquire();

            acquired = true;

        } catch (final InterruptedException e) {

        }
    } while (!acquired);
    try {

        super.execute(task);
    } catch (final RejectedExecutionException e) {
        semaphore.release();
        throw e;
    }

}

@Override
protected void afterExecute(Runnable r, Throwable t) {
    super.afterExecute(r, t);
    if (t != null) {
        t.printStackTrace();
    }
    semaphore.release();
}
}
```

This is the main class from where everything is executed.

```
Main.java

public class Main{
public static void main(String[] args){

          BulkProcessor processor = BulkProcessor
            .builder(ElasticClientHolder.getInstance(), new Listener() {

                @Override
                public void beforeBulk(long executionId, BulkRequest request) {
                    // TODO Auto-generated method stub

                }

                @Override
                public void afterBulk(long executionId,
                        BulkRequest request, Throwable failure) {
                    // TODO Auto-generated method stub

                }

                @Override
                public void afterBulk(long executionId,
                        BulkRequest request, BulkResponse response) {
                    // TODO Auto-generated method stub
                }
            }).setBulkActions(500).build();



         BlockingQueue&lt;Runnable&gt; blockingQueue = new ArrayBlockingQueue&lt;Runnable&gt;(
            50);
     BlockingThreadPoolExecutor executor = new BlockingThreadPoolExecutor(
            1, 2, 5000, TimeUnit.MILLISECONDS, blockingQueue);

        for(int i=0; i &lt; 10000; i++){

                  MyBulkRunnable runnable = new MyBulkRunnable(processor);
                 executor.execute(runnable);

        }

       executor.shutDown();
       while(!executor.isTerminated){
       }
       try{
       processor.awaitClose(30l,TimeUnit.Seconds);
       }
       catch(Exception e){
       }
}

public static class MyBulkRunnable implements Runnable {

    private final BulkProcessor processor;

    public MyBulkRunnable(BulkProcessor processor){
        this.processor = processor;
    }

    @Override
    public void run() {
        // TODO Auto-generated method stub

        this.processor.add(new IndexRequest("test_index",
                "test_type").source(new HashMap&lt;String, Object&gt;() {
            {
                put("test_thread", Thread.currentThread().getId());
            }
        }));

    }

}

}
```
</description><key id="118201788">14911</key><summary>rejected task exception with ES Java BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rrphotosoft</reporter><labels /><created>2015-11-21T15:02:20Z</created><updated>2015-11-22T11:22:08Z</updated><resolved>2015-11-22T10:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-21T21:02:38Z" id="158680520">Hi @rrphotosoft 

I see what you are trying to achieve. You want to issue 10.000 index requests via the bulk API. However, the problem is that you are doing something slightly different, namely issuing 10.000 index requests from 10.000 threads (if just the pool would allow this but I assume the maximum number of threads is much lower than that). :) That's also the reason why you are getting `RejectedExecutionException`: you are completely overwhelming the poor pool.

Fortunately, it should be much simpler to solve your problem. You can see an example in the [Client API documentation](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html):

```
//Assuming you want to wait until all requests have finished
int numRequests = 10000;
CountDownLatch latch = new CountDownLatch(numRequests);

BulkProcessor bulkProcessor = BulkProcessor.builder(
        client,  
        new BulkProcessor.Listener() {
            @Override
            public void beforeBulk(long executionId,
                                   BulkRequest request) {  
            } 

            @Override
            public void afterBulk(long executionId,
                                  BulkRequest request,
                                  BulkResponse response) {
              latch.countDown();  
            } 

            @Override
            public void afterBulk(long executionId,
                                  BulkRequest request,
                                  Throwable failure) {
              latch.countDown(); // also count down on failure  
            } 
        })
        .setBulkActions(500) 
        .setBulkSize(new ByteSizeValue(1, ByteSizeUnit.GB)) 
        .setFlushInterval(TimeValue.timeValueSeconds(5)) 
        .setConcurrentRequests(1) 
        .build();

for (int requestId = 0; requestId &lt; numRequests; requestId++) {
     bulkProcessor.add(new IndexRequest("test_index", "test_type").source(Collections.singletonMap("request", requestId)));
}

System.out.println("All indexing requests issued. Waiting for responses...");
latch.await();
System.out.println("Indexing finished");
```

That's just adapted from the docs. I hope this gets you started.

Btw, we're about to add automatic backoff in bulk processor (see #14620) so it's even less likely that you encounter `RejectedExecutionException`. However, this feature is likely to be included just in Elasticsearch 2.2.0+.

Does this solve your problem? If not, I suggest you create a new discussion on the [Elasticsearch user forum](https://discuss.elastic.co/c/elasticsearch) and we close this ticket as I don't think this is an issue with Elasticsearch.
</comment><comment author="rrphotosoft" created="2015-11-22T03:24:32Z" id="158704322">Hi,
Thanks for your reply and the code snippet. 
In my initial post, I've set the core threadpool size to 1 and the max to 2. 
I have also tested this code with the Runnable  doing a simple (Thread.sleep) or some other task (like couting down a million integers). In each of these cases I never see a task rejected exception.
By "overwhelming the pool" do you mean, I'm overwhelming the &lt;b&gt;ES threadpool&lt;/b&gt; ? 

I looked at your commit:&lt;a href="https://github.com/danielmitterdorfer/elasticsearch/commit/d2e61513b2dd9b8482062d3fbeb1a34d3f1bfbe8"&gt; Retry&lt;/a&gt; :) just four days ago!!. Should I try to use that as is?

Thanks.
RrPhotosoft.
</comment><comment author="danielmitterdorfer" created="2015-11-22T10:47:22Z" id="158745088">Hi @rrphotosoft,

I should have run your code earlier. My first assumption was not entirely correct. First things first, you don't overwhelm any thread pool in Elasticsearch but your own thread pool.

The problem is that you have a race condition in your code. Your queue can take 50 runnables but the semaphore, which you seem to use for flow control, allows 50 + corePoolSize method entries. Consider this situation: The queue is full (50 entries), the semaphore allow count is zero and the pool is full. A runnable has finished, you release the semaphore in `#afterExecute()` (which is invoked from the pool thread),  `semaphore.acquire()` (in the main thread) passes and passes another runnable to #execute. The thing is that the pool has still some work to do after `#afterExecute()` and your race condition hits exactly this small window of time.

To avoid the race, replace the line:

`semaphore = new Semaphore(corePoolSize + 50);`

with

`semaphore = new Semaphore(50);`

You won't hit `RejectedExecutionException`any more.

I also don't know if this is just a minimalistic example to reproduce the problem but IMHO it won't buy you much to add entries from multiple threads to the bulk processor. I'd just go with a single thread that adds all index requests. With the single threaded approach your code gets _much_ simpler and you also avoid running into lock contention issues within the bulk processor.

The `Retry` class you've mentioned is brand new and meant to be used internally in `BulkProcessor` and not exposed in the client API but as the PR is not closed yet it is not sure whether this implementation is really coming as I've implemented it.

I am closing this issue now as there is no problem in Elasticsearch to me. I hope I could help. :)
</comment><comment author="rrphotosoft" created="2015-11-22T10:50:53Z" id="158745249">Hi,
I came across the solution just as you posted the response. Thanks for your help. 
Rrphotosoft.
</comment><comment author="danielmitterdorfer" created="2015-11-22T11:22:08Z" id="158748170">Another race condition :) You're welcome. Glad I could help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return a better exception message when `regexp` query is used on a numeric field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14910</link><project id="" key="" /><description>Fixes #14782
</description><key id="118163148">14910</key><summary>Return a better exception message when `regexp` query is used on a numeric field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">camilojd</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-21T02:27:12Z</created><updated>2015-11-21T04:33:17Z</updated><resolved>2015-11-21T03:42:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-21T02:49:51Z" id="158578923">I think this way to do it seems pretty sane. I'm not sure what the **rightest** way to do it is but this will certainly get the job done.
</comment><comment author="nik9000" created="2015-11-21T03:09:04Z" id="158581033">Tip: github emails me whenever you comment on a pull request but not when you push a new commit. I'm not really sure why. Anyway, the upshot is that if you comment after you send the patch and mention that you did so rather than before it tends to summon the reviewer quickly.
</comment><comment author="nik9000" created="2015-11-21T03:12:51Z" id="158581231">Ok! Looks good. Would you mind squashing the commits and amending the subject to include words like "Return a better exception message when `regexp` query is used on a numeric field" just so the log is easier to read? That'd be great!

Not everyone in Elasticsearch squashes commits on the way in. I tend to but not everyone does and I wouldn't require it of anyone except that the commit message on the first commit isn't super useful when you are looking at `git log`.

Thanks for doing this!
</comment><comment author="camilojd" created="2015-11-21T03:29:25Z" id="158583397">@nik9000 will take the notification/squashing tips into account (I'm new to all of it). Seems like everything's ok now.

Thank you for your patience!! :+1: :smiley: 
</comment><comment author="nik9000" created="2015-11-21T04:33:17Z" id="158589091">Merged to master, cherry picked to 2.x as b852bd3c75cfa7c99d6c64151bfe0fba3b241458 and 2.2 as e42ed38aefac3f04357cd078cfbc4321eae5072a.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not release unacquired semaphore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14909</link><project id="" key="" /><description>This commit adds an acquired flag to BulkProcessor#execute that is set
only after successful acquisition of a permit on the semaphore
there. This flag is used to ensure that we do not release a permit on
the semaphore when we did not obtain a permit on the semaphore.

Closes #14908
</description><key id="118147176">14909</key><summary>Do not release unacquired semaphore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Bulk</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T23:14:36Z</created><updated>2015-11-20T23:20:23Z</updated><resolved>2015-11-20T23:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-20T23:16:28Z" id="158553947">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkProcessor can release too many permits on its semaphore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14908</link><project id="" key="" /><description>BulkProcessor uses a semaphore to control the level of concurrency on its underlying client. Prior to acquiring a permit on this semaphore, BulkProcessor#execute invokes BulkProcessor.Listener#beforeBulk. If this method throws, then BulkProcessor#execute releases a permit on its semaphore. This is a mistake, and can lead to more than the intended number of threads having a permit on the semaphore.
</description><key id="118146645">14908</key><summary>BulkProcessor can release too many permits on its semaphore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Bulk</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T23:10:08Z</created><updated>2015-11-20T23:20:22Z</updated><resolved>2015-11-20T23:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Retrieve parent id from the request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14907</link><project id="" key="" /><description>Support to retrieve parent id once set through any request, irrespective of routing is set or not.
According to current codebase you can only retrieve parent id if it's not overwritten by routing.
As child documents can have same or different routing and parent id, depending on whether routing is enabled or not.

For example: GetRequest API
Let me know if I can help.
</description><key id="118143695">14907</key><summary>Retrieve parent id from the request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ayushsangani</reporter><labels /><created>2015-11-20T22:49:41Z</created><updated>2015-11-28T14:29:47Z</updated><resolved>2015-11-28T14:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T14:29:47Z" id="160304616">This works today:

```
PUT t
{
  "mappings": {
    "grandparent": {},
    "parent": {
      "_parent": {
        "type": "grandparent"
      }
    },
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT t/grandparent/1
{}

PUT t/parent/2?parent=1
{}

PUT t/child/3?parent=2&amp;routing=1
{}

GET t/child/3?fields=_parent,_routing&amp;routing=1
```

The search request returns all stored meta fields by default

```
GET t/_search
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Survive missing directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14906</link><project id="" key="" /><description /><key id="118141786">14906</key><summary>Survive missing directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T22:39:50Z</created><updated>2015-11-20T22:51:42Z</updated><resolved>2015-11-20T22:48:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-20T22:40:00Z" id="158545861">@rjernst this should fix it!
</comment><comment author="nik9000" created="2015-11-20T22:48:03Z" id="158548085">I'm just going to merge this to get the builds fixed. If we don't like it we can change it. Little harm done.
</comment><comment author="rjernst" created="2015-11-20T22:51:41Z" id="158549260">Looks good, thanks Nik
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require root-cause exception to ESLogger methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14905</link><project id="" key="" /><description>This is a spinoff of #14867 where we failed to log the root cause exception in a catch clause that could have shed light on #14866.

I think we should require root cause exceptions to our logger methods to 1) fix the remaining places that swallow exceptions (I've hit maybe 2 dozen or so), and 2) reduce the chances of swallowing exceptions in new code going forward.

It means that those times when there is no exception, we'd have to pass null, but I think that's a fair tradeoff so we don't repeat #14866.

I had started to work on this for all except `trace`, but then came across at least one place that was trace-logging an exception (and I'm pretty sure we `debug` and `info` exceptions in quite a few places) so I decided to just make this a required argument for all `ESLogger` methods, but this is apparently controversial so let's discuss it here.

I could only do this for `error` and `warn` but I'm not sure how many places we are swallowing exceptions are using the other levels ...

Just to get a sense of the things I'm finding ... some are wrong-order-of-arguments, e.g.:

```
try {
  failReplicaIfNeeded(request.internalShardId.getIndex(), request.internalShardId.id(), t);
} catch (Throwable unexpected) {
  logger.error("{} unexpected error while failing replica", request.internalShardId.id(), unexpected);
} finally {
  responseWithFailure(t);
}
```

others do a catch but then do nothing with the exception:

```
} catch (Throwable t) {
  logger.warn("[{}] failed to refresh-mapping in cluster state, types [{}]", index, refreshTask.types);
}
```

Others only include `getMessage` from the exception but should (usually?) include the full exception ... the message by itself can be ... tantalizing:

```
if (cause instanceof ConnectTransportException) {
  logger.debug("delaying recovery of {} for [{}] due to networking error [{}]", recoveryStatus.shardId(), recoverySettings.retryDelayNetwork(), cause.getMessage());
```
</description><key id="118119003">14905</key><summary>Require root-cause exception to ESLogger methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Exceptions</label><label>discuss</label></labels><created>2015-11-20T20:21:27Z</created><updated>2016-09-27T16:11:12Z</updated><resolved>2016-09-27T16:11:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-22T21:09:25Z" id="158799960">I'm +1 on chasing those places that catch an exception and log something without it and/or suppress the stack trace. This is bad and we should fix it I'm also +1 on preventing the sad but common mistake of putting the exception in the wrong place in the parameter list.

To that fact I'm +1 on requiring a throwable parameter to the error and warn log methods. Those are typically used in an error context and I feel that's the right trade off there.

However, when it comes to info/debug/trace that are used in many places which has nothing to do with an exception/error it feels like a big overkill and an unnatural restriction. To me it goes a step too far.

&gt; Others only include getMessage from the exception but should (usually?) include the full exception ... the message by itself can be ... tantalizing

I think Robert wants to put `getMessage` on the forbidden list. This will prevent this as well, right?
</comment><comment author="mikemccand" created="2015-11-22T22:29:45Z" id="158810278">OK I'll explore only requiring an exception for error and warn levels.  But we do unfortunately handle exceptions with info/debug/trace in places (not sure howe often, hopefully it's rare-ish) which means we lose enforcing those cases...

&gt; I think Robert wants to put getMessage on the forbidden list. This will prevent this as well, right?

And also `.toString()` ... I'll try that too.
</comment><comment author="bleskes" created="2015-11-23T10:39:47Z" id="158901347">Thanks Mike.

&gt;  But we do unfortunately handle exceptions with info/debug/trace in places (not sure howe often, hopefully it's rare-ish) which means we lose enforcing those cases...

I'm happy to help going through the list and review/fix the current things you found... 
</comment><comment author="luizamboni" created="2015-11-26T23:30:54Z" id="160004518">i m using 1.7 version, i do the exactly ando not appears in my elasticsearch.log file
</comment><comment author="luizamboni" created="2015-11-26T23:31:39Z" id="160004562">import org.elasticsearch.common.logging.*;
import java.text.SimpleDateFormat;

``` groovy
sdf = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss");

product_prices_gt_than_min = _source.product_prices.findAll{    
    sdf.parse(it["created_at"]).time.toInteger() &gt;= min_date_in_millis
}

product_prices_gt_than_max_too = product_prices_gt_than_min.findAll{    
    sdf.parse(it["created_at"]).time.toInteger() &lt;= max_date_in_millis
}

ESLogger logger = ESLoggerFactory.getLogger('product_prices_count_aggs');
println "#############################################";
logger.debug("#############################################");
logger.debug(product_prices_gt_than_min.size);
logger.debug(product_prices_gt_than_max_too.size);


return product_prices_gt_than_max_too;
```
</comment><comment author="s1monw" created="2015-11-27T10:50:20Z" id="160111762">I think we should start simple and make sure (assert) that any of the var args is not an exception. This will not catch all the places (only the once we exercise in our tests and it will not blow up in production but it's a good start I think.
</comment><comment author="luizamboni" created="2015-11-27T17:33:55Z" id="160180463">its not throws any error, the script woks...but not white log in anywhere...what make me more blind about what is happening...whithout any message that i can understand
the script product_prices_gt_than_max_too; 
</comment><comment author="jasontedor" created="2015-11-27T17:40:22Z" id="160181037">@luizamboni The issue that you are working through is unrelated to the issue under discussion here. May I suggest reaching out on [#elasticsearch](https://webchat.freenode.net/#elasticsearch), or on the [forums](https://discuss.elastic.co)?
</comment><comment author="dakrone" created="2016-09-27T16:11:12Z" id="249914106">I think this might be something we are no longer able to do, since we moved to not having `ESLogger` and use Log4j2's methods now instead, so it can be closed. Feel free to re-open if I'm mistaken.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip package tests on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14904</link><project id="" key="" /><description>They aren't going to work because the packages don't include the windows
files.
</description><key id="118114542">14904</key><summary>Skip package tests on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T19:55:05Z</created><updated>2015-11-20T19:58:44Z</updated><resolved>2015-11-20T19:58:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-20T19:55:14Z" id="158508672">@rjernst works just like you said it would.
</comment><comment author="nik9000" created="2015-11-20T19:56:10Z" id="158508873">And it skips starting the cluster. Very nice.
</comment><comment author="rjernst" created="2015-11-20T19:58:10Z" id="158509297">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document index.load_fixed_bitset_filters_eagerly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14903</link><project id="" key="" /><description>FixedBitSetFilterCache is a data structure that is loaded eagerly in memory (by default) to support nested query/filter and nested aggregations. However, the problem is that it can cause it to use too much heap for it is loaded for all nested fields (regardless of whether these fields are being used). To prevent this from happening, a common configuration workaround is to set `index.load_fixed_bitset_filters_eagerly: false` in the yml of the nodes and restart them to prevent the nodes from running OOM when attempting to eagerly load the fixedbitsets. 

At this point, I have seen enough occurrences of this in the field which necessitates the setting of `index.load_fixed_bitset_filters_eagerly: false` to prevent the nodes from running out of memory.  It may make sense to start documenting this setting unless there is a better solution we are already planning :)
</description><key id="118112494">14903</key><summary>Document index.load_fixed_bitset_filters_eagerly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Nested Docs</label><label>adoptme</label><label>docs</label></labels><created>2015-11-20T19:44:20Z</created><updated>2016-02-03T21:49:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-11-20T19:47:36Z" id="158506852">Related to #10224
</comment><comment author="ppf2" created="2015-11-22T20:02:20Z" id="158793928">Maybe we can also look into having ES set index.load_fixed_bitset_filters_eagerly automatically on startup (eg. by collecting stats on the number of nested fields on startup, or other relevant metrics) and making the decision on whether it is appropriate to load them eagerly vs. lazily based on the amount of heap available, etc..  Currently, users tend to find out that they have to set this option after their nodes have run out of memory.
</comment><comment author="clintongormley" created="2015-11-27T10:47:35Z" id="160111357">I think we should document this setting, but not add heuristics.  Instead, we'll add a soft limit to the number of nested docs (https://github.com/elastic/elasticsearch/issues/14983) and look at ways of removing the need for the fixed bitsets (https://github.com/elastic/elasticsearch/issues/14919#issuecomment-160111164)
</comment><comment author="pickypg" created="2016-02-03T21:49:43Z" id="179487975">I don't think that this was intentionally closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport mapper-attachments plugin to 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14902</link><project id="" key="" /><description>We got this thing in pretty good shape in master, but a big part of making it better was cleaning up the dependencies management: we use a parser whitelist and that does not work well with maven.

The solution here based on the fact that in master, gradle translates our whitelist into 24KB of horrible XML automatically for us, we can just paste that directly into maven if we need to upgrade tika. So, the POM.xml for this plugin has this warning:

```
&lt;!-- WARNING: AUTOMATICALLY GENERATED --&gt;
&lt;!-- TO UPGRADE: run gradle install on master, add the generated dependencies from build/poms, remove all 'provided' dependencies --&gt;
  &lt;dependencies&gt;
  ...
```

I think its a good solution for 2.x
</description><key id="118112124">14902</key><summary>Backport mapper-attachments plugin to 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Mapper Attachment</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-11-20T19:41:57Z</created><updated>2015-11-28T14:31:10Z</updated><resolved>2015-11-20T23:49:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-20T20:42:36Z" id="158518758">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue with upsert by file ES v1.5.2 - COuld only be a documentation bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14901</link><project id="" key="" /><description>Trying to do an upsert with a script file (inline scripting kept off). Simple use case (simplified further):
== mappings ===
"mappings":{
"sometype":{
"_source":{"enabled": true},
"_ttl":{"enabled":false},
"properties":{
"somefield":{"type":"long"},
"counter":{"type":"long"} }
}
}

Upsert done via curl....
curl -XPOST 'eshost:9200/test_index/sometype/11223344/_update' -d '{
"script":{
"file":"sometype-add"
}, 
"upsert":{
"somefield":"11223344",
"counter":0
}
}'

Have put a sometype-add.groovy in the scripts folder under /etc .. which does this:
ctx._source.counter += 1
Nothing too demanding here. 
If the doc exists, the counter is updated. However... if the doc does not exist, we fail with:
{"error":"RemoteTransportException[[nodename][inet[/esserverip:9300]][indices:data/write/update]]; nested: DocumentMissingException[[index][0] [data][11223344]: document missing]; ","status":404}

I jumped through all teh relevant hoops for a few hoours. tried doc_as_upsert.. nothing. Then , out of frustration, I changed the call around to this:

curl -XPOST 'eshost:9200/test_index/sometype/11223344/_update' -d '{
"upsert":{
"somefield":"11223344",
"counter":0
},
"script":{
"file":"sometype-add"
}
}'
.. and presto! If the doc exists, it adds to the counter.. if the doc does not exist it creates it. All you have to do is put the upsert first, and the script second.... I think this is an internal bugette... but one that needs highlighting....
</description><key id="118088354">14901</key><summary>Issue with upsert by file ES v1.5.2 - COuld only be a documentation bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bitonp</reporter><labels /><created>2015-11-20T17:32:52Z</created><updated>2015-11-28T14:15:24Z</updated><resolved>2015-11-28T14:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-28T14:15:24Z" id="160302048">Yep, that's a parsing bug, but long fixed.  I'd recommend upgrading 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>link to es-restlog plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14900</link><project id="" key="" /><description /><key id="118086783">14900</key><summary>link to es-restlog plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shikhar</reporter><labels><label>docs</label></labels><created>2015-11-20T17:23:32Z</created><updated>2015-11-28T14:09:27Z</updated><resolved>2015-11-28T14:09:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Split cluster state update tasks into roles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14899</link><project id="" key="" /><description>This commit splits cluster state update tasks into roles. Those roles
are:
- task info
- task configuration
- task executor
- task listener

All tasks that have the same executor will be executed in batches. This
removes the need for local batching as was previously in
MetaDataMappingService.

Additionally, this commit reintroduces batching on mapping update calls.

Relates #13627 
</description><key id="118084780">14899</key><summary>Split cluster state update tasks into roles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T17:12:51Z</created><updated>2015-12-04T21:39:25Z</updated><resolved>2015-11-30T14:19:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-25T20:25:15Z" id="159723196">Oops, I got confused with the Github interface (commenting on commit instead of PR). I like the change of associating failures with their originating task using a map instead of position :+1:.
</comment><comment author="jasontedor" created="2015-11-25T22:40:25Z" id="159747536">@ywelsch I liked your suggestion a lot so I pushed 66af98ea5b0a925e9c3dc55465c8aceb91813b6f.
</comment><comment author="bleskes" created="2015-11-30T12:42:27Z" id="160618545">Good job. LGTM. Left some minor comments. no need for another review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shell scripts should source elasticsearch.in.sh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14898</link><project id="" key="" /><description>Right now `bin/elasticsearch` sources a version of `elasticsearch.in.sh` - the first version it finds in a list of directories. All of our scripts should do that - including `bin/plugin` and any scripts we package as into plugins. I don't know that we should go so far as to automatically add it as part of the build process, but we should really make an effort to source the file in all the scripts.

This would allow:
1. We could move all the `/etc/sysconfig` et al handling into it rather than the init scripts and `bin/plugin`.
2. External packages could have a single place to stick their version of the `/etc/sysconfig` handling. See #14870 for FreeBSD wanting to do exactly this.
</description><key id="118083672">14898</key><summary>Shell scripts should source elasticsearch.in.sh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>v5.4.4</label></labels><created>2015-11-20T17:06:37Z</created><updated>2017-06-27T10:28:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-20T17:30:51Z" id="158467230">Seems like we can do a nice refactoring here to ensure all of our scripts act in a consistent way. +1
</comment><comment author="tlrx" created="2015-11-23T10:25:13Z" id="158897888">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update modified flag when removing a property and corresponding tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14897</link><project id="" key="" /><description>Update modified flag when removing a property and corresponding tests
</description><key id="118083150">14897</key><summary>Update modified flag when removing a property and corresponding tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-20T17:03:28Z</created><updated>2015-11-20T17:10:22Z</updated><resolved>2015-11-20T17:10:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-20T17:05:11Z" id="158461237">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Register field mappers at the node level.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14896</link><project id="" key="" /><description>This moves the registration of field mappers from the index level to the node
level and also ensures that mappers coming from plugins are treated no
differently from core mappers.

Closes https://github.com/elastic/elasticsearch/issues/14828
</description><key id="118078484">14896</key><summary>Register field mappers at the node level.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T16:40:04Z</created><updated>2015-11-28T16:34:08Z</updated><resolved>2015-11-24T08:28:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-20T16:40:48Z" id="158454127">@rjernst I assigned this one to you as you initially suggested the idea.
</comment><comment author="rjernst" created="2015-11-20T17:57:25Z" id="158474579">This is _fantastic_! I was afraid this would be complex, but it looks surprisingly straightforward. Given the simplicity, do you want to try to see what backporting entails? I'm pretty sure there will be a couple conflicts due to guice refactorings in 2.x that are not in 2.1, but I think it is at least worthwhile to try given the current state is all custom mappers are broken on old indexes being upgraded.
</comment><comment author="nik9000" created="2015-11-20T18:57:35Z" id="158494571">I like how much this cleans it up!
</comment><comment author="jpountz" created="2015-11-23T09:40:41Z" id="158888574">I just pushed more commits to make the naming more consistent (we used to have both "metadata mappers" and "root mappers" referring to the same thing, now only "metadata mappers") and fix the bug that @nik9000 found that the _field_names mapper would not see fields that would come from plugins.
</comment><comment author="jpountz" created="2015-11-23T13:53:30Z" id="158937854">@s1monw made me notice that onIndexService is also used by some external plugins. So I added Plugin.onIndexService back and will work on a different PR to get rid of it.
</comment><comment author="s1monw" created="2015-11-23T13:54:02Z" id="158937959">thanks!
</comment><comment author="nik9000" created="2015-11-23T15:46:05Z" id="158973804">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shift significant terms score test back into core without Groovy dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14895</link><project id="" key="" /><description>This completely removes any testing of Groovy-scripted significance scoring so is a cleaner PR to review than https://github.com/elastic/elasticsearch/pull/14886 which kept some significance test code in lang-groovy. So this is "option 1" from [the comment here](https://github.com/elastic/elasticsearch/pull/14886#issuecomment-158424384) and would replace that PR
</description><key id="118075909">14895</key><summary>Shift significant terms score test back into core without Groovy dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>review</label><label>test</label></labels><created>2015-11-20T16:26:16Z</created><updated>2015-11-23T11:03:30Z</updated><resolved>2015-11-23T11:03:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-20T16:31:54Z" id="158451977">LGTM
</comment><comment author="jpountz" created="2015-11-20T16:45:11Z" id="158455539">I just realized the class needs to be renamed to SignificantTermsSignificanceScoreIT as well to match the convention for integration tests
</comment><comment author="markharwood" created="2015-11-20T17:16:07Z" id="158463638">Gradle test just caught that. Will do.
</comment><comment author="markharwood" created="2015-11-20T21:11:42Z" id="158524449">Pushed to master in https://github.com/elastic/elasticsearch/commit/5a5f05a0e9193607081474acccca31d1b0cabbf7

Still working on backport to 2.x due to setup issues with eclipse
</comment><comment author="markharwood" created="2015-11-23T11:03:30Z" id="158905323">Backported to 2.x with https://github.com/elastic/elasticsearch/commit/4d9a64bf68073f5b13f33d406190ee6532f5a8dc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shift SearchTimeoutTests into core tests minus the Groovy dependency.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14894</link><project id="" key="" /><description>Removed the Groovy dependency for testing searches that timeout are flagged. Can now move back into core away from lang-groovy plugin
</description><key id="118067148">14894</key><summary>Shift SearchTimeoutTests into core tests minus the Groovy dependency.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>review</label><label>test</label></labels><created>2015-11-20T15:45:34Z</created><updated>2015-11-23T13:44:35Z</updated><resolved>2015-11-23T13:44:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-20T16:44:20Z" id="158455157">LGTM, just a minor change to perform to the test name
</comment><comment author="markharwood" created="2015-11-20T21:10:10Z" id="158524169">Pushed to master in https://github.com/elastic/elasticsearch/commit/c3a50d7ca2c507327c670614e01b54add9231469

Still working on the backport to 2.x due to eclipse IDE setup issues
</comment><comment author="markharwood" created="2015-11-23T13:44:35Z" id="158936217">Backported to 2.x with https://github.com/elastic/elasticsearch/commit/d0f3b41b667140437a5d5fc96fb819b37e69c485
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Cardinality Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14893</link><project id="" key="" /><description /><key id="118066642">14893</key><summary>Aggregations Refactor: Refactor Cardinality Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-20T15:43:09Z</created><updated>2015-11-24T17:00:55Z</updated><resolved>2015-11-24T16:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-24T16:57:12Z" id="159338092">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor/cardinality agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14892</link><project id="" key="" /><description /><key id="118066251">14892</key><summary>Refactor/cardinality agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels /><created>2015-11-20T15:41:46Z</created><updated>2015-11-20T15:42:02Z</updated><resolved>2015-11-20T15:42:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Why data nodes disk usage goes up, even if no additional data is added</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14891</link><project id="" key="" /><description>I have a 4 node cluster.
PN1   - 59% disk usage    -- Master and data
SN1   - 61% disk usage    - data
DN1  -  95% disk usage    -- data
DN2  -  49%                      -- data

I noted one of the node - DN1 had 87% disk usage and hitting the low disk watermark  and I was not able recovere shard. I deleted some old indices and restarted the Elastic search.

Now disk usage is 90% and high watermark is 90%.

Why the disk usage goes up in Elastic search even if no extra data is added ?

Why disk usage are different on different nodes ?
</description><key id="118059624">14891</key><summary>Why data nodes disk usage goes up, even if no additional data is added</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">satapsa</reporter><labels /><created>2015-11-20T15:09:16Z</created><updated>2015-11-20T17:13:54Z</updated><resolved>2015-11-20T17:13:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-20T17:13:53Z" id="158463188">These are better questions for the forums because its not clear at all that you have a bug in a released version of elasticsearch. If I had to guess its because Elasticsearch is moving files around to balance the cluster. Depending on your version it could be doing some silly things like moving files to a machine that doesn't have enough disk space for them. You can check (cat recovery)[https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-recovery.html] to see what is moving around right now.

&gt; Why disk usage are different on different nodes ?

Elasticsearch balances the total number of shards and doesn't balance the total disk usage across nodes. It uses hard limits on disk usage to push shards off of nodes that go over the limits but that is all it does with shard size.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure the remaining delay of unassigned shard is updated with every reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14890</link><project id="" key="" /><description>#14808 changed the way we calculate the remaining delay of unassigned shards to make sure that all components use the same basic details for making decision and don't rely on System.currentTimeStamp. The calculation was made whenever the ReplicaShardAllocator couldn't assign a shard. However we did it too late so, for example, if some shard had some in flight store fetch the delay information wasn't updated causing some tests to fail and making reasoning about time left tricky (some shards were updated, some not), causing issues with our reporting. Instead we should update the delay indication with every iteration.

For example: if a node left the cluster and an async store fetch was triggered. In that time no shard is marked as delayed (and strictly speaking it's not yet delayed). This caused test for shard delays post node left to fail. see : http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/2074/testReport/

 To fix this, the delay update is now done by the Allocation Service, based of a fixed time stamp that is determined at the beginning of the reroute.

 Also, this commit fixes a bug where unassigned info instances were reused across shard routings, causing calculated delays to be leaked.
</description><key id="118058002">14890</key><summary>Make sure the remaining delay of unassigned shard is updated with every reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T14:59:57Z</created><updated>2015-11-27T16:33:51Z</updated><resolved>2015-11-23T12:16:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-20T15:00:21Z" id="158424302">@ywelsch all yours :)
</comment><comment author="ywelsch" created="2015-11-23T09:11:51Z" id="158881877">I don't like exposing the UnassignedInfo constructor with unassignedTimeNanos. As consequence, we spread System.nanoTime() all over the codebase. In 99% of the cases, we pass System.nanoTime() directly to the constructor. We are only interested in setting this time explicitly when reason == NODE_LEFT (Otherwise it is unused anyhow). How about focusing only on that special place in deassociateDeadNodes (and not build a general abstraction with currentNanoTime()/getCurrentNanoTime() that is only relevant for one single call site)?

I like that updateDelay is now called directly in reroute. This opens up new possibilities to mock time, as deassociateDeadNodes and updateDelay are now both called from the same method.
</comment><comment author="bleskes" created="2015-11-23T10:21:55Z" id="158897311">Thx @ywelsch . I pushed another commit.

&gt; I don't like exposing the UnassignedInfo constructor with unassignedTimeNanos.

I added back the constructor which default the timestamps to now. My original thinking was that you have to think about time when creating this class, but this isn't the major reason for this change. I'm fine with changing it.

&gt;  We are only interested in setting this time explicitly when reason == NODE_LEFT (Otherwise it is unused anyhow).

It will be a big shame to loose the infrastructure we have now to delay based on any reason - we may use it in the future.

&gt; I like that updateDelay is now called directly in reroute. This opens up new possibilities to mock time, as deassociateDeadNodes and updateDelay are now both called from the same method.

This one is my main goal in this PR. Happy you like it.
</comment><comment author="ywelsch" created="2015-11-23T11:35:24Z" id="158910758">Two minor comments, LGTM o.w. Thanks @bleskes!
</comment><comment author="bleskes" created="2015-11-23T12:17:23Z" id="158917504">merged into master. Like with #14808 , let's wait a few days before back porting.
</comment><comment author="ywelsch" created="2015-11-27T16:33:51Z" id="160171468">backported to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add query profiler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14889</link><project id="" key="" /><description>Followup to #12974 and #6699

This PR adds a query profiler to time the various components of a Search API Request.  It shows profiling information (query, rewrite, collectors) for each Lucene search on each shard in the request.
### Request:

``` json
GET /test/_search
{
  "profile": true,
  "query" : {
    "match" : { "message" : "search test" }
  }
}
```
### Response

``` js
{
   "took": 25,
   "timed_out": false,
   "_shards": {...},
   "hits": {...},
   "profile": {
     "shards": [
        {
           "id": "[htuC6YnSSSmKFq5UBt0YMA][test][0]",
           "searches": [
              {
                 "query": [
                    {
                       "query_type": "BooleanQuery",
                       "lucene": "message:search message:test",
                       "time": "15.52889800ms",
                       "breakdown": {
                          "score": 0,
                          "next_doc": 24495,
                          "match": 0,
                          "weight": 8488388,
                          "build_scorer": 7016015,
                          "advance": 0
                       },
                       "children": [
                          {
                             "query_type": "TermQuery",
                             "lucene": "message:search",
                             "time": "4.938855000ms",
                             "breakdown": {
                                "score": 0,
                                "next_doc": 18332,
                                "match": 0,
                                "weight": 2945570,
                                "build_scorer": 1974953,
                                "advance": 0
                             }
                          },
                          {
                             "query_type": "TermQuery",
                             "lucene": "message:test",
                             "time": "0.5016660000ms",
                             "breakdown": {
                                "score": 0,
                                "next_doc": 0,
                                "match": 0,
                                "weight": 170534,
                                "build_scorer": 331132,
                                "advance": 0
                             }
                          }
                       ]
                    }
                 ],
                 "rewrite_time": 185002,
                 "collector": [
                    {
                       "name": "SimpleTopScoreDocCollector",
                       "reason": "search_top_hits",
                       "time": "2.206529000ms"
                    }
                 ]
              }
           ]
        }
     ]
  }
}
```

/cc @jpountz 
</description><key id="118050274">14889</key><summary>Add query profiler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Search</label><label>feature</label><label>release highlight</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T14:18:16Z</created><updated>2016-01-22T18:36:06Z</updated><resolved>2015-12-17T20:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-20T14:19:49Z" id="158413402">w00t
</comment><comment author="jpountz" created="2015-11-24T15:41:41Z" id="159308075">Note to reviewers: at this point, the feedback we're most interested in is whether the intrusiveness of this feature into the rest of the code base is ok or not. We tried to make it as little intrusive as possible and to have as few code changes outside of the search.profile package as possible but obviously we had to add some entry points (eg. QueryPhase). If you think it's too intrusive then we'd need to check if we missed some ways that we could make it better, or to reduce the feature set.
</comment><comment author="s1monw" created="2015-11-25T10:01:52Z" id="159557237">I took a close look at this and I really like what I am seeing! I think this could go in as is but I left a bunch of comments that I think we should address to reduce the impact of this feature even further. I am pretty impressed how cleanly this integrates. Lucene has gone a long way to make this possible!
</comment><comment author="polyfractal" created="2015-11-30T20:22:23Z" id="160750164">Ok, most of the comments have been addressed...left a few notes on the outstanding issues that I'm not sure the best way to resolve.

In a few cases while converting `Streamable` to `Writeable`, it made sense to merge the public interface with the internal representation, since the public interface was just re-exporting the exact same methods (and @jpountz tells me the serialization stuff will be visible anyway after the query refactoring, so no need to hide them).
</comment><comment author="s1monw" created="2015-11-30T20:29:28Z" id="160752444">cleanups look great. Lets figure out the remaining issues on chat tomorrow?
</comment><comment author="polyfractal" created="2015-12-02T22:24:11Z" id="161453153">@jpountz @s1monw Ok, took a stab at re-arranging the collector wrapping stuff.  Thoughts?  It's less invasive now, and hopefully not too magical (just wraps + returns, doesn't reach "inside" the profiler like the last iteration did)
</comment><comment author="s1monw" created="2015-12-03T08:21:12Z" id="161548257">@polyfractal I left some comments
</comment><comment author="polyfractal" created="2015-12-08T14:27:41Z" id="162897079">Pushed another attempt at moving state into the collector builder.  

I'm not really sure I like this.  I had to drag the TopDocsCallable code inside the collector builder, since a few of them were closely intertwined with the collector being built.  Also had to do a few less-than-graceful things, like allowing the option to disable/remove collection (for MatchAllDocs query optimization, etc).

Also not a fan of having to specialize a method for each collector type, will this be a maintenance burden?

So...eh.  Curious to see what you think.  Side note: out of town for week, won't make much progress on this.
</comment><comment author="s1monw" created="2015-12-08T14:29:42Z" id="162897764">&gt; Pushed another attempt at moving state into the collector builder.

zach, I think this is trickier than we though.... Should we go back to the original version and just go with it for now? I really want to get this in?
</comment><comment author="polyfractal" created="2015-12-08T14:32:25Z" id="162898952">Yeah, the changes make me squeamish...I don't like touching this much code in the core :)

++ reverting back to the `if (doProfile)` approach for now.
</comment><comment author="s1monw" created="2015-12-08T14:33:10Z" id="162899149">&gt; ++ reverting back to the if (doProfile) approach for now.

++ lets fix adriens commetns and push it 
</comment><comment author="polyfractal" created="2015-12-14T15:30:02Z" id="164467497">Sorry for delay, I'm home now.  Changes reverted and docs fixed up.  Test failures should be fixed now, will keep an eye on CI today.
</comment><comment author="jpountz" created="2015-12-14T15:48:12Z" id="164472432">Thanks @polyfractal ! I quickly looked at the last commits and I don't think we need to profile the `TwoPhaseIterator.matchCost` method as this should be a simple getter. We can directly delegate to the wrapped instance.
</comment><comment author="polyfractal" created="2015-12-15T20:45:51Z" id="164891349">@s1monw Whatcha think?  Anything else to be done?
</comment><comment author="s1monw" created="2015-12-17T14:19:21Z" id="165463428">**LGTM** :dancers: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Ingest better bulk failure handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14888</link><project id="" key="" /><description>With this change if ingest pipeline fails only the index request in the bulk fails whereas right now the entire bulk request is failed.

If pipeline processing fails then the index request gets reported as a failure in the bulk response in its respective place. The index request will not be executed after pipeline processing has been completed.

If pipeline failures occur a new bulk request will be created based on the current bulk request. Only index requests that have successfully been processed by the pipeline will be included in this new bulk request. Then when sending back the bulk response the new bulk request will map its response items in such a way that the order of all items and errors is the same and matches with the original bulk request that was sent.
</description><key id="118047923">14888</key><summary>[Ingest] Ingest better bulk failure handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-20T14:02:58Z</created><updated>2015-11-24T11:42:22Z</updated><resolved>2015-11-24T11:42:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-20T18:15:39Z" id="158479712">change looks great @mvg thanks a lot. I left some comments around code style and design plus tests, let me know what you think.
</comment><comment author="martijnvg" created="2015-11-23T09:38:33Z" id="158888223">@javanna I'v updated the PR.
</comment><comment author="javanna" created="2015-11-23T14:48:12Z" id="158957482">I left a few more comments, looks good, just a few small things that we can improve left I think. let me know what you think.
</comment><comment author="martijnvg" created="2015-11-23T16:22:27Z" id="158985649">@javanna thanks, I updated the PR.
</comment><comment author="javanna" created="2015-11-23T16:32:12Z" id="158988352">much better thanks @martijnvg . I left a question around testing, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merging BaseLineString and BasePolygonBuilder with subclass</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14887</link><project id="" key="" /><description>After the removal of some internal shape builders in #14482 the BaseLineStringBuilder has only one implementation, the LineStringBuilder. Same for the BasePolygonBuilder. This PR removes the abstract classes and merges them with their concrete implementation to simplify the inheritance hierarchy.
</description><key id="118040484">14887</key><summary>Merging BaseLineString and BasePolygonBuilder with subclass</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T13:14:59Z</created><updated>2015-11-21T11:52:31Z</updated><resolved>2015-11-21T11:52:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-20T13:16:06Z" id="158400461">@nknize here is the other cleanup PR we briefly talked about. Nothing really deleted here, just moving code from the former abstract base classes to the concrete implementations and some renaming.
</comment><comment author="nknize" created="2015-11-20T22:42:43Z" id="158546366">Thanks @cbuescher! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor SignificantTerms scoring tests back into core minus Groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14886</link><project id="" key="" /><description>This change splits SignificantTermsSignificanceScoreTests in two. Only groovy-related tests (File/inline and indexed scripts) are retained in the lang-groovy plugin&#8217;s test class. All other non-groovy test methods relating to core significance scores and native plugins moved to a new class in core-tests.
</description><key id="118034442">14886</key><summary>Refactor SignificantTerms scoring tests back into core minus Groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels /><created>2015-11-20T12:44:12Z</created><updated>2015-11-20T16:34:59Z</updated><resolved>2015-11-20T16:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-11-20T14:44:29Z" id="158419486">@jpountz Any chance of a review?
</comment><comment author="markharwood" created="2015-11-20T15:00:42Z" id="158424384">I can see "messy" package name will need addressing.
Two options here:
1) Just remove the lang-groovy test completely (the assumption being if core's scripted significance scores work with native code they will also work with Groovy scripts) or
2) Keep the lang-groovy test class but move to better package name.
</comment><comment author="jpountz" created="2015-11-20T15:07:44Z" id="158425932">I'd be in favor of option 1.
</comment><comment author="jpountz" created="2015-11-20T15:09:10Z" id="158426274">It's hard to review but assuming that you just splitted the code into two then I'm +1. If there are no objections, I suggest that we proceed with the removal of tests that rely too much on groovy in this PR?
</comment><comment author="markharwood" created="2015-11-20T16:26:38Z" id="158450669">This is cleaner: https://github.com/elastic/elasticsearch/pull/14895
</comment><comment author="markharwood" created="2015-11-20T16:34:59Z" id="158452738">Aborted in favour of https://github.com/elastic/elasticsearch/pull/14895
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move ValueCountTests to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14885</link><project id="" key="" /><description>This script moves ValueCountTests from plugins back to core
by using a mock script engine instead of Groovy.
</description><key id="118027808">14885</key><summary>Move ValueCountTests to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T12:09:06Z</created><updated>2015-11-20T12:23:50Z</updated><resolved>2015-11-20T12:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-20T12:15:32Z" id="158380669">LGTM
</comment><comment author="danielmitterdorfer" created="2015-11-20T12:18:33Z" id="158381337">@jpountz Thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moves SumTests out of lang-groovy and back into core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14884</link><project id="" key="" /><description>Renames SumTests to SumIT and puts it in the same package as the other aggregation tests. Also updates the tests to not require Groovy
</description><key id="118026484">14884</key><summary>Moves SumTests out of lang-groovy and back into core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T12:01:41Z</created><updated>2015-11-20T13:35:31Z</updated><resolved>2015-11-20T12:42:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-20T12:18:12Z" id="158381251">Added a comment on a very minor issue. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: `exists` query does not need to be wrapped inside `constant_score`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14883</link><project id="" key="" /><description>In the example we show an `exists` query inside a constant score query. While this is possible, it can mislead users to think it is necessary so we should remove it.
</description><key id="118022289">14883</key><summary>Docs: `exists` query does not need to be wrapped inside `constant_score`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label><label>review</label></labels><created>2015-11-20T11:34:57Z</created><updated>2015-11-20T17:10:10Z</updated><resolved>2015-11-20T17:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-20T16:47:27Z" id="158456263">LGTM, just left a comment about indentation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MissingValues don't work inside sub aggregations ()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14882</link><project id="" key="" /><description>`missing` values are great to work with - but they do not work within more complex queries.

Here a simple example to reproduce the bug.
Let's say, we have some index, with optional fields:

```
PUT /missing-values/test/1_with_my_field
{
  "someField": "optionalField_is_present",
  "optionalField": "present"
}
PUT /missing-values/test/1_without_my_field
{
  "someField": "optionalField_is_missing"
}
```

Now I want to use the fields in some term aggregation. I want information on all items; even if the optional field is missing, so I can use this code:

```
GET /missing-values/test/_search
{
  "aggregations" : {
    "test": {
      "terms" : {
        "field" : "optionalField",
        "missing": "n/a"
      },
      "aggregations" : {
        "test": {
          "terms" : {
            "field" : "someField"
          }
        }
      }
    }
  }
}
```

That works fine as long as the `missing` stuff is applied on the outer level only.
If I change the order of the fields:

```
GET /missing-values/test/_search
{
  "aggregations" : {
    "test": {
      "terms" : {
        "field" : "someField"
      },
      "aggregations" : {
        "test": {
          "terms" : {
            "field" : "optionalField",
            "missing": "n/a"
          }
        }
      }
    }
  }
}
```

I receive an exception due to internally used ordinals:
`org.elasticsearch.search.aggregations.support.MissingValues$6 cannot be cast to org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals$FieldData`
</description><key id="118016820">14882</key><summary>MissingValues don't work inside sub aggregations ()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">DavidHauger</reporter><labels><label>:Aggregations</label><label>:Fielddata</label><label>bug</label></labels><created>2015-11-20T11:04:32Z</created><updated>2016-01-06T08:32:39Z</updated><resolved>2016-01-06T08:32:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dam0vm3nt" created="2015-12-30T01:52:41Z" id="167916184">I can confirm this bug in `2.1.1` version of `ES` using `java API`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move AvgTests back to core.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14881</link><project id="" key="" /><description>This makes AvgTests use a mock plugin engine. I also removed the
textScriptExplicit\* methods for the base class since they only make sense for
a groovy script, not a mock script.
</description><key id="118016677">14881</key><summary>Move AvgTests back to core.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>review</label><label>test</label></labels><created>2015-11-20T11:03:26Z</created><updated>2015-11-20T11:41:14Z</updated><resolved>2015-11-20T11:41:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-20T11:32:45Z" id="158368900">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SecurityManager policy for ES 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14880</link><project id="" key="" /><description>tl;dr Is there any way to provide a plugin security policy in 2.0.0?

Trying to update the [Kubernetes discovery plugin](https://github.com/fabric8io/elasticsearch-cloud-kubernetes) to ES 2.0.0 &amp; can only get it to run by disabling security manager.

The plugin needs to read certain files from the filesystem outside of the normal ES paths &amp; I've written a policy that works with a normal Java main running with a security manager. However I can see no way to apply this by packaging in the plugin, although I do see this is possible in master.
</description><key id="118008721">14880</key><summary>SecurityManager policy for ES 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmidyson</reporter><labels><label>docs</label></labels><created>2015-11-20T10:15:01Z</created><updated>2016-03-17T16:47:07Z</updated><resolved>2015-11-20T18:02:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-20T17:00:59Z" id="158460256">Maybe have a look at what lang-groovy does in 2.x:
https://github.com/elastic/elasticsearch/blob/2.x/plugins/lang-groovy/src/main/plugin-metadata/plugin-security.policy
</comment><comment author="jimmidyson" created="2015-11-20T17:04:59Z" id="158461192">That's what I thought I was doing... Does this work for 2.0.0 or only an unreleased 2.x version?
</comment><comment author="rmuir" created="2015-11-20T17:06:21Z" id="158461520">We already document how to this in the plugin author's guide for 2.2+ (https://github.com/elastic/elasticsearch/pull/14108)

https://www.elastic.co/guide/en/elasticsearch/plugins/2.x/plugin-authors.html#_java_security_permissions
</comment><comment author="jimmidyson" created="2015-11-20T17:07:02Z" id="158461660">Yeah I've seen that but is that supported in 2.0?
</comment><comment author="rmuir" created="2015-11-20T17:12:15Z" id="158462779">No, it is coming in 2.2. 
</comment><comment author="jimmidyson" created="2015-11-20T18:02:08Z" id="158476655">OK thanks for feedback.
</comment><comment author="pires" created="2016-03-17T16:29:37Z" id="197961300">This doesn't seem to be working. Here's a plug-in installation:

```
- Plugin information:
Name: cloud-kubernetes
Description: Elasticsearch Kubernetes cloud plugin
Site: false
Version: 2.2.1
JVM: true
 * Classname: io.fabric8.elasticsearch.plugin.discovery.kubernetes.KubernetesDiscoveryPlugin
 * Isolated: true
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.io.FilePermission &lt;&lt;ALL FILES&gt;&gt; read
* java.net.NetPermission getProxySelector
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.
Installed cloud-kubernetes into /elasticsearch/plugins/cloud-kubernetes
```

Here's one error when running it:

```
[2016-03-17 16:17:17,992][WARN ][io.fabric8.elasticsearch.discovery.kubernetes.KubernetesUnicastHostsProvider] [Chloe Tran] Exception caught during discovery: access denied ("java.net.NetPermission" "getProxySelector")
java.security.AccessControlException: access denied ("java.net.NetPermission" "getProxySelector")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.net.ProxySelector.getDefault(ProxySelector.java:94)
```

Any hints?
</comment><comment author="pires" created="2016-03-17T16:47:07Z" id="197969656">Nevermind, got it to work. Missing the change in the code to run a privileged action.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Odd Primary Shard Distribution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14879</link><project id="" key="" /><description>Hello,
Using version **2.0**, with **3 nodes, 9 shards, 2 replicas** with default settings. 
At the beginning all of the primary shards were distributed over all 3 nodes evenly. After restarting some of the nodes, all of the primary shards are located on the same node, and even restarting the node which full of primary shards, not changed this distribution.
Here some screenshots from kopf plugin:
**After Index Created:**
&lt;img width="412" alt="screen shot 2015-11-20 at 12 05 28" src="https://cloud.githubusercontent.com/assets/3204601/11297126/0695ac4e-8f7f-11e5-916f-5f123274fa5e.png"&gt;
**After node-3 and node-1 restarted: (one by one)**
&lt;img width="408" alt="screen shot 2015-11-20 at 11 42 45" src="https://cloud.githubusercontent.com/assets/3204601/11296630/ea62767c-8f7b-11e5-8fff-e90004c4c13a.png"&gt;
**After node-2 restarted:**
&lt;img width="417" alt="screen shot 2015-11-20 at 12 00 43" src="https://cloud.githubusercontent.com/assets/3204601/11297035/5d60c6ea-8f7e-11e5-872c-3ff7016a6a0e.png"&gt;
I tried to use "cluster.routing.allocation.balance.primary" config but it's deprecated and not working on version 2.0.
So my questions are there is any way to distribute primary shards over all of the nodes evenly or there is any known downside about this behaviour?
</description><key id="118007245">14879</key><summary>Odd Primary Shard Distribution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kaanbasal</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2015-11-20T10:06:24Z</created><updated>2015-12-14T21:16:56Z</updated><resolved>2015-12-14T21:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-20T15:24:00Z" id="158431109">What happens is that as you restart nodes other replicas are elected as primaries. Since Elasticsearch does sync replication, all shard copies do the same work except for extreme cases (like a lot of document rejections, or extremely heavy update scripts). As such the location of the primaries shouldn't matter much (and we are working on those extreme cases where it still does).

Do you see any odd behavior due to those primaries?  
</comment><comment author="kaanbasal" created="2015-11-24T11:35:04Z" id="159237844">I made some tests about the behaviour of primary shard distribution based on the other users thoughts mentioned. I find that there is no CPU or RAM overhead on the node full of primary shards when making query or inserting new documents, on the other side the only thing I found odd is when a new node added to the cluster, the only node sending the data to the newly created node is that node which is full of primary shards and this behaviour makes Network and CPU overhead only on that machine.
</comment><comment author="bleskes" created="2015-11-24T12:03:07Z" id="159247918">&gt; the only node sending the data to the newly created node is that node which is full of primary shards

That is correct for indexing. Searches how ever should round robin and hit the new node as well.

&gt; behaviour makes Network and CPU overhead only on that machine.

which of two machines do you mean? I presume the one with primary, in which case I can see the network argument (but not the cpu). Is network a bottle neck for you?
</comment><comment author="kaanbasal" created="2015-11-24T12:25:19Z" id="159252166">Your assumption is right about the machine, it's the one with primary. For now network is not a bottle neck for us but I just wanted you to know this situation. 
For the CPU argument, I made some queries/inserts while indexing the newly added node, and the node with primary had a lot of CPU usage than the other nodes.
</comment><comment author="bleskes" created="2015-11-24T12:35:22Z" id="159255124">ok. Are you using update scripts? as I mentioned, those are run on the primary only, but are typically light weight. If not, can you run [the _hot_threads API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html?q=hot%20thre#cluster-nodes-hot-threads) and report what the node with the primary says?
</comment><comment author="kaanbasal" created="2015-11-24T12:46:54Z" id="159257484">I don't know which info do you need from the hot threads api response. Do you want me to send all of the  response or any specific value?

Edit: I don't use update script
</comment><comment author="bleskes" created="2015-11-24T12:55:40Z" id="159259485">If you gist them it would be great (with an indication of which node is the one with the primaries. That will tell us what it does.

&gt; On 24 Nov 2015, at 13:47, Kaan Basal notifications@github.com wrote:
&gt; 
&gt; I don't know which info do you need from the hot threads api response. Do you want me to send all of the response or any specific value?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="kaanbasal" created="2015-11-24T13:09:28Z" id="159261958">Ok, you can check it here: [Hot Threads](https://gist.github.com/kaanbasal/ebaffd30567435c142b5)

[Hot_Thread_Primary_Only_Indexing](https://gist.github.com/kaanbasal/ebaffd30567435c142b5#file-hot_thread_primary_only_indexing) is only primary node, while indexing to newly added node. (before making query/insert)

The others are while indexing &amp; querying/inserting at the same time. 
</comment><comment author="bleskes" created="2015-12-14T21:16:56Z" id="164562282">@kaanbasal sorry for taking so long to respond. The hot thread seem to suggest that the replicas are still recovering and pull data from the node with the primary. Specifically the hot threads show compression code, compressing the data before it is being sent. It is correct that when the index is going from yellow to green the primary has more to do then replicas. However, this period is temporary. The tricky part is that moving the primaries around for this purpose will create more traffic than letting the replicas finish recovering. I'm going to close this for now as it feels like the right tradeoff. Feel free to reopen if it doesn't add up.

Note that in master we're going to reduce the CPU spent compressing lucene files during recovery. They are already compressed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.0 update caused problems with date properties by adding epoch_millis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14878</link><project id="" key="" /><description>ES 2.0 upgrade added to all date type property mappings format epoch_millis. Unfortunately this was added at the beginning of the format parameter so it is used for conversion to string.

**For example when I have had following index:**

``` javascript
PUT test
{
  "mappings": {
    "item":
    {
      "properties": {
        "Value":
        {
          "type": "date",
          "format": "dateOptionalTime"
        }
      }
    }
  }
}
```

and the aggregations returns

``` javascript
{
   "Value": {
    "value": 1447993360390,
    "value_as_string": "2015-11-20T04:22:40.390Z"
  }
}
```

**After upgrade I have following:**

``` javascript
PUT test
{
  "mappings": {
    "item":
    {
      "properties": {
        "Value":
        {
          "type": "date",
          "format": "epoch_millis||dateOptionalTime"
        }
      }
    }
  }
}
```

and the aggregations returns

``` javascript
{
   "Value": {
    "value": 1447993360390,
    "value_as_string": "1447993360390"
  }
}
```

**I think epoch_millis should be added (if it is necessary - why?) at the end of format value, not at the beginning or this info should be in breaking changes for ES 2.0 version !!!**
</description><key id="118007118">14878</key><summary>ES 2.0 update caused problems with date properties by adding epoch_millis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zdeseb</reporter><labels /><created>2015-11-20T10:05:35Z</created><updated>2015-11-24T14:08:17Z</updated><resolved>2015-11-24T14:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-24T14:08:17Z" id="159277483">Ah, this is an unfortunate side effect of making `epoch_millis` explicit.  For upgraded indices, it was added as the first format because that reflects how values were parsed in 1.x.  For new indices, it is added as the second format (and so will do what you expect).

If you reindex your data, things will work as before.  Alternatively, specify the `format` parameter in your aggregation as a workaround.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve stability of UpdateIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14877</link><project id="" key="" /><description>`UpdateIT#testConcurrentUpdateWithRetryOnConflict()` has caused some trouble recently, especially on Windows. The test often times out and the build guard infrastructure kills the test. The root cause of the problem is that the test is performing an excessive number of index updates (currently up to 10.000).

We want to improve two things in this ticket:
1. Reduce the number of updates to at most 500. This is still enough to spot problems.
2. Improve logging so we know that the test is still making progress
</description><key id="118002062">14877</key><summary>Improve stability of UpdateIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>test</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T09:38:57Z</created><updated>2015-11-28T16:25:01Z</updated><resolved>2015-11-23T16:56:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-23T16:40:56Z" id="158990711">Sounds like it failed today again on 2.1 branch: http://build-us-00.elastic.co/job/es_core_21_debian/342/consoleFull
</comment><comment author="danielmitterdorfer" created="2015-11-23T16:46:40Z" id="158992191">@dadoonet It's not yet merged. I'll merge it soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expired documents purge is not working after installing Shield Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14876</link><project id="" key="" /><description>I am trying out the ElasticSearch purge functionality for expired documents. It works fine when I do not have Shield plugin configured. But, as soon as I have Shield plugin configured, I get the following error

2015-11-20 09:40:52,052][DEBUG][shield.authz.accesscontrol] [localhost] [test][4] couldn't locate the current request, field level security will only allow meta fields
2015-11-20 09:40:52,053][DEBUG][indices.ttl              ] [localhost] [test][4] purging shard
2015-11-20 09:40:52,054][WARN ][indices.ttl              ] [localhost] failed to purge
java.lang.IllegalStateException: opting out of the query cache. current request can't be found
       at org.elasticsearch.shield.authz.accesscontrol.OptOutQueryCache.doCache(OptOutQueryCache.java:62)
       at org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:853)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:834)
       at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:485)
       at org.elasticsearch.indices.ttl.IndicesTTLService.purgeShards(IndicesTTLService.java:204)
       at org.elasticsearch.indices.ttl.IndicesTTLService.access$000(IndicesTTLService.java:67)
       at org.elasticsearch.indices.ttl.IndicesTTLService$PurgerThread.run(IndicesTTLService.java:140)

I have username and password authentication enabled in Shield. I have enabled anonymous access as well. But, the error is same in all cases.

I am using Java 7.0, ElasticSearch 2.0 and Shield 2.0.
</description><key id="117994817">14876</key><summary>Expired documents purge is not working after installing Shield Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vikasgithub</reporter><labels /><created>2015-11-20T08:49:59Z</created><updated>2015-11-20T09:09:42Z</updated><resolved>2015-11-20T09:09:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-20T09:09:41Z" id="158331856">I think you are hitting this known limitation: https://www.elastic.co/guide/en/shield/current/limitations.html#_document_expiration__ttl
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search using alias + type returns {"error": null}</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14875</link><project id="" key="" /><description>Very strange. Using ES 1.7.3 on CentOS 6.3 x64.
1. First, create an alias for an existing index.
   
   ```
   POST /_aliases
   {
      "actions" : [
          { "add" : { "index" : "v2-log-2015-11-19", "alias" : "log-2015-11-19" } }
      ]
   }
   ```
2. Then perform a search using the new alias:
   
   ```
   POST /log-2015-11-19/webapp/_search?routing=webapp
   {
     "query": {
       "filtered": {
         "filter": {
           "range": {
             "logTime": {
               "from": 1447910416536,
               "to": 1447996816535
             }
           }
         }
       }
     }
   }
   ```
   
   Returns:
   
   ```
   {
      "error": null
   }
   ```
   
   The ES logs say:
   
   ```
   [2015-11-20 14:36:38,813][DEBUG][org.eclipse.jetty.io.nio ] [bumbobox.dev] created SCEP@2a7c878a{l(/10.11.12.13:45811)&lt;-&gt;r(/10.11.12.15:9200),s=0,open=true,ishut=false,oshut=false,rb=false,wb=false,w=true,i=0}-{AsyncHttpConnection@55a68ad0,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=-14,l=0,c=0},r=0}
   [2015-11-20 14:36:38,813][DEBUG][org.eclipse.jetty.server.Server] [bumbobox.dev] REQUEST /log-2015-11-19/webapp/_search on AsyncHttpConnection@55a68ad0,g=HttpGenerator{s=0,h=-1,b=-1,c=-1},p=HttpParser{s=6,l=7,c=-2},r=1
   [2015-11-20 14:36:38,814][DEBUG][index.analysis.rest      ] [bumbobox.dev] failed to parse search request parameters
   java.lang.NullPointerException
           at org.elasticsearch.index.analysis.rest.TokenizerRestAction.handleRequest(TokenizerRestAction.java:93)
           at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:53)
           at org.elasticsearch.rest.RestController.executeHandler(RestController.java:225)
           at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:170)
           at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:121)
           at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:83)
           at com.sonian.elasticsearch.http.jetty.handler.JettyHttpServerTransportHandler.handle(JettyHttpServerTransportHandler.java:69)
           at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:522)
           at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
           at org.eclipse.jetty.server.Server.handle(Server.java:366)
           at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
           at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
           at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
           at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:957)
           at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
           at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
           at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
           at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
           at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
           at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
           at java.lang.Thread.run(Thread.java:745)
   [2015-11-20 14:36:38,815][DEBUG][org.eclipse.jetty.server.Server] [bumbobox.dev] RESPONSE /log-2015-11-19/webapp/_search  400 handled=false
   ```
3. If you search the original index directly, then it works properly:
   
   ```
   POST /v2-log-2015-11-19/webapp/_search?routing=webapp
   {
     "query": {
       "filtered": {
         "filter": {
           "range": {
             "logTime": {
               "from": 1447910416536,
               "to": 1447996816535
             }
           }
         }
       }
     }
   }
   ```
   
   Returns:
   
   ```
   {
      "took": 2,
      "timed_out": false,
      "_shards": {
         "total": 1,
         "successful": 1,
         "failed": 0
      },
      "hits": {
         "total": 3960,
         "max_score": 1,
         "hits": [
            {
                   ....
   ```
4. Alternatively, if you remove the type from the alias based search request, the response is correct.
   
   ```
   POST /log-2015-11-19/_search
   {
     "query": {
       "filtered": {
         "filter": {
           "range": {
             "logTime": {
               "from": 1447910416536,
               "to": 1447996816535
             }
           }
         }
       }
     }
   }
   ```
   
   Returns:
   
   ```
   {
      "took": 165,
      "timed_out": false,
      "_shards": {
         "total": 15,
         "successful": 15,
         "failed": 0
      },
      "hits": {
         "total": 68061695,
         "max_score": 1,
         "hits": [
            {
                   ....
   ```

All the above is same when using any different type.

Is there anything I am doing wrong?
</description><key id="117970494">14875</key><summary>Search using alias + type returns {"error": null}</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kadishmal</reporter><labels /><created>2015-11-20T05:52:59Z</created><updated>2015-11-20T06:43:34Z</updated><resolved>2015-11-20T06:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kadishmal" created="2015-11-20T06:43:34Z" id="158296271">This seems to be related to our internal highlight plugin that we use.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not be lenient when parsing CIDRs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14874</link><project id="" key="" /><description>This commit fixes some leniency in the parsing of CIDRs. The leniency
that existed before includes not validating the octet values nor
validating the network mask; in some cases issues with these values were
silently ignored. Parsing is now done according to the guidelines in [RFC
4632](https://tools.ietf.org/html/rfc4632), [page 6](https://tools.ietf.org/html/rfc4632#section-3.1).

Closes #14862
</description><key id="117954033">14874</key><summary>Do not be lenient when parsing CIDRs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Search</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-11-20T02:43:42Z</created><updated>2015-11-28T16:31:49Z</updated><resolved>2015-11-24T01:51:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-20T02:45:36Z" id="158263216">Additionally, the `cidrMaskToMinMax` method that existed before returned `null` in some (but not all) cases where there was an issue parsing. Instead, this method now throws `IllegalArgumentException` in cases where parsing runs into trouble. The caller can now catch these exceptions which can be used to carry more information back to the initial source what the underlying issue is.
</comment><comment author="rmuir" created="2015-11-20T12:48:49Z" id="158389324">thanks a lot for simplifying the tests.
</comment><comment author="jasontedor" created="2015-11-23T16:54:06Z" id="158994354">@jpountz @rmuir Are there any outstanding issues that you would like for me to address?
</comment><comment author="jpountz" created="2015-11-23T18:19:13Z" id="159017621">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify where index.similarity.default.type is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14873</link><project id="" key="" /><description>Updating documentation to include adding the setting to `elasticsearch.yml`

Fixes: #14849 
</description><key id="117941897">14873</key><summary>Clarify where index.similarity.default.type is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wmlwang</reporter><labels><label>docs</label></labels><created>2015-11-20T00:46:40Z</created><updated>2015-11-24T14:01:43Z</updated><resolved>2015-11-24T14:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-24T14:01:37Z" id="159275994">Thanks @williammwang - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't delete temp recovered checkpoint file if it was renamed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14872</link><project id="" key="" /><description>#14695 introduced more careful handling in recovering translog checkpoints. Part of it introduced a temp file which is used to write a new checkpoint if needed. That temp file is not always used and thus needs to be cleaned up. However, if it is used we currently log an ugly warn message about failing to delete it.

Here is an example:

```
  1&gt; [2015-11-19 22:56:08,049][WARN ][org.elasticsearch.index.translog] [node_t1] [test][0] failed to delete temp file /home/boaz/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.rec
  1&gt; java.nio.file.NoSuchFileException: /home/boaz/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.recovery.RelocationIT_720114FFC2D82BCD-002/tempDir-018/data/TEST-CHILD_VM=[0]-CLUS
  1&gt;    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
  1&gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
  1&gt;    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
  1&gt;    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
```

@jpountz can you take a look?
</description><key id="117920130">14872</key><summary>Don't delete temp recovered checkpoint file if it was renamed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>bug</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T22:11:58Z</created><updated>2016-01-10T19:40:20Z</updated><resolved>2015-11-20T09:45:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-19T22:31:42Z" id="158221352">LGTM
</comment><comment author="jpountz" created="2015-11-20T09:49:01Z" id="158340087">Thanks @bleskes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix doc of nested_path sort option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14871</link><project id="" key="" /><description>In 2eadc6d5958a88187692e3514428413d9ffef508 the option of using a default path when sorting by nested field was [removed](https://github.com/elastic/elasticsearch/commit/2eadc6d5958a88187692e3514428413d9ffef508#diff-514b67619414b8fc77f08ddcc27f5680L240). The documentation was partially updated to reflect this, but the paragraph on `nested_path` itself was skipped and still referenced the possibility of using a default. I updated it to reflect that change.

I also made some of the surrounding language a little clearer.
</description><key id="117911941">14871</key><summary>Fix doc of nested_path sort option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jamiemccarthy</reporter><labels><label>docs</label></labels><created>2015-11-19T21:29:50Z</created><updated>2015-11-28T16:46:47Z</updated><resolved>2015-11-28T16:46:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-24T13:37:39Z" id="159269520">Hi @jamiemccarthy 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jamiemccarthy" created="2015-11-24T16:01:25Z" id="159313869">Hi @clintongormley ! I did sign the Individual Contributor License Agreement Under CCLA, on Nov. 19. I'm not sure why the test didn't find it. Maybe I could email you the Adobe Document Cloud PDF I got, if that would help, or I could sign it again and see if it takes...?

![PDF screenshot](http://i.imgur.com/m2i7EU3.png)
</comment><comment author="clintongormley" created="2015-11-28T16:43:53Z" id="160317795">found it, thanks @jamiemccarthy 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for FreeBSD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14870</link><project id="" key="" /><description>Add support for reading configuration variables from FreeBSD's
/etc/rc.conf and related service specific configuration files.
</description><key id="117911233">14870</key><summary>Add support for FreeBSD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">amishHammer</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2015-11-19T21:25:50Z</created><updated>2016-06-21T16:45:05Z</updated><resolved>2016-06-21T16:45:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-20T16:15:38Z" id="158447363">@amishHammer is the maintainer of the freebsd port, of Elasticsearch ([here](https://www.freshports.org/textproc/elasticsearch) and [here](https://www.freshports.org/textproc/elasticsearch2)). Freebsd ports use the untar from upstream, patch, repackage mechansim. We talked at Elastic{on} Chicago and whittled down what he had to patch in to this adapter layer between rc.conf and Elasticsearch. This is how the Elasticsearch 2 port that he published last night works.
</comment><comment author="rmuir" created="2015-11-20T16:17:48Z" id="158447897">how will it work with plugins? AFAIK, unfortunately `bin/plugin` does not look at elasticsearch.in.sh
</comment><comment author="nik9000" created="2015-11-20T16:25:11Z" id="158450291">&gt; how will it work with plugins? AFAIK, unfortunately bin/plugin does not look at elasticsearch.in.sh

Yeah - I was just checking on that. I'm now kind of wondering why we don't do at the `/etc/sysconfig` sorcery in `elasticsearch.in.sh` and always source it. I suspect he's got something else for `bin/plugin`.

Otherwise, the nice thing about this change is that many of the scripts that we do ship already source `elasticsearch.in.sh` - the shield scripts do for example.
</comment><comment author="amishHammer" created="2015-11-20T16:26:02Z" id="158450508">Ok bin/plugin does seem to need a little work plugins seem to get some
files in the wrong place (config).  However plugins (well shield) scripts
that get installed in bin/shield look at elasticsearch.in.sh so get the
correct configuration.

On Fri, Nov 20, 2015 at 10:18 AM, Robert Muir notifications@github.com
wrote:

&gt; how will it work with plugins? AFAIK, unfortunately bin/plugin does not
&gt; look at elasticsearch.in.sh
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14870#issuecomment-158447897
&gt; .
</comment><comment author="rmuir" created="2015-11-20T16:57:49Z" id="158459342">OK, if most scripts are already sourcing this file, I think we should just make a separate issue to fix the rest of the scripts to do it too? It seems like it can only bring better consistency.
</comment><comment author="clintongormley" created="2016-03-10T12:50:22Z" id="194828963">Hi @amishHammer 

Sorry this appears to have fallen between the cracks.  We'll pick it up again soon.  In the meantime, could I ask you to sign the CLA? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dakrone" created="2016-04-06T20:45:06Z" id="206558205">Hi @amishHammer, is there still interest in supporting this PR?
</comment><comment author="amishHammer" created="2016-04-27T16:23:56Z" id="215137905">Yes sorry for the delay had to do some internal button clicking to register the contribution due to the CLA
</comment><comment author="jasontedor" created="2016-04-27T16:45:56Z" id="215144017">Since we do not officially support FreeBSD, I would prefer that we not integrate this PR. But I wonder if this can be solved by just using `ES_INCLUDE` to source an alternate file from `elasticsearch.in.sh` that includes these changes and just source `elasticsearch.in.sh` from that other file? Note that on master `elasticsearch.in.sh` [does not really do anything other than set the classpath](https://github.com/elastic/elasticsearch/blob/f600c4ab9c4a575a269287bb5aaa22d828c56496/distribution/src/main/resources/bin/elasticsearch.in.sh).
</comment><comment author="clintongormley" created="2016-06-21T16:44:59Z" id="227499699">&gt; Since we do not officially support FreeBSD, I would prefer that we not integrate this PR.

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve integ test startup behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14869</link><project id="" key="" /><description>As part of the refactoring to allow --debug-jvm with gradle run, the way
java options are passed for integ tests was changed. However, we need to
make sure the jvm argline passed goes to ES_GC_OPTS because this
allows overriding things like which garbage collector we run, which we
do for testing from jenkins. This change adds back ES_GC_OPTS.
</description><key id="117908397">14869</key><summary>Improve integ test startup behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T21:09:40Z</created><updated>2015-11-19T22:13:18Z</updated><resolved>2015-11-19T22:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-19T21:46:37Z" id="158207545">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field stats: Index constraints should remove indices in the response if the field to evaluate is empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14868</link><project id="" key="" /><description>Index constraints should remove indices in the response if the field to evaluate if empty. Index constraints can't work with that and it is the same as if the field doesn't match.
</description><key id="117900181">14868</key><summary>Field stats: Index constraints should remove indices in the response if the field to evaluate is empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>bug</label><label>review</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T20:24:46Z</created><updated>2015-11-24T14:16:39Z</updated><resolved>2015-11-23T07:36:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-19T21:54:28Z" id="158209754">Wondering - what does it mean that an index response is empty? that it doesn't have the requested fields?
</comment><comment author="martijnvg" created="2015-11-20T09:00:26Z" id="158329721">@bleskes The index constraint didn't match either because there was no data or it just didn't match. But returning an empty hash isn't useful, so that is why why the PR changes that.
</comment><comment author="bleskes" created="2015-11-20T15:20:09Z" id="158429766">LGTM (though the description needs to be adapted)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include root-cause exception when we fail to change shard's index buffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14867</link><project id="" key="" /><description /><key id="117893895">14867</key><summary>Include root-cause exception when we fail to change shard's index buffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T19:51:28Z</created><updated>2015-11-24T23:35:51Z</updated><resolved>2015-11-19T20:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-19T19:57:07Z" id="158177676">LGTM
</comment><comment author="rmuir" created="2015-11-19T19:58:49Z" id="158178639">Can we remove the variants of error, warn, hell maybe all logger methods that _dont_ take throwable? 

Means you gotta think about it, pass null in the cases you don't have one.
</comment><comment author="rmuir" created="2015-11-19T20:15:51Z" id="158184715">and +1 to this fix, lets just make another issue. i think we still want to ban Throwable.toString()/getMessage() there too and just clean house. no fancy IDE refactoring tools, just a lot of beer and old fashioned cleanup grunt work, that's the only to hunt down and fix all these.
</comment><comment author="mikemccand" created="2015-11-19T20:41:56Z" id="158191856">&gt; Can we remove the variants of error, warn, hell maybe all logger methods that dont take throwable?

+1
</comment><comment author="mikemccand" created="2015-11-19T21:14:59Z" id="158200208">&gt; Can we remove the variants of error, warn, hell maybe all logger methods that dont take throwable?

OK I'm gonna give this a shot ...
</comment><comment author="nik9000" created="2015-11-20T16:55:06Z" id="158458490">&gt; OK I'm gonna give this a shot ...

_All_ of them? I get removing error that doesn't take one - its pretty rare to have a genuine error without an exception and in those cases we can just call it with null but I figure its reasonably common to have warnings and most trace and info logs won't have an exception to log.
</comment><comment author="mikemccand" created="2015-11-20T18:49:16Z" id="158491036">&gt; All of them? 

Yeah, unfortunately: we use all log levels (even TRACE!) in ES when logging an exception, so I'm exploring making it a required argument to all of them now.

Having to think about it and pass `null` if you don't have an exception seems like the lesser evil than the 2 dozen or so places I've found so far (including this original one, from #14866) that were failing to include the caught exception in their logging ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk items fail due to TranslogException via ClosedChannelException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14866</link><project id="" key="" /><description>We recently made the jump to start testing ES 2.0 against our codebase, which was pretty straightforward with only a couple of changes on the Query DSL.

However, once we put this into our test environment, we see constant streams of this (please note I have removed sensitive information, such as source, index and type):

```
[2015-11-19 18:21:10,865][DEBUG][action.bulk              ] [La Nuit] [pem:2015:47][5] failed to execute bulk item (index) index {[my-index][my-type][3279b5669abaf1458583678b37bbf2a2], source[{"my-source":true}]}
[my-index][[my-index][5]] TranslogException[Failed to write operation [Index{id='3279b5669abaf1458583678b37bbf2a2', type='my-type'}]]; nested: ClosedChannelException;
    at org.elasticsearch.index.translog.Translog.add(Translog.java:497)
    at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:532)
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:447)
    at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:556)
    at org.elasticsearch.index.engine.Engine$Index.execute(Engine.java:815)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1073)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:338)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:131)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
    at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:110)
    at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:199)
    at org.elasticsearch.common.io.Channels.writeToChannel(Channels.java:206)
    at org.elasticsearch.index.translog.BufferingTranslogWriter.flush(BufferingTranslogWriter.java:75)
    at org.elasticsearch.index.translog.BufferingTranslogWriter.add(BufferingTranslogWriter.java:62)
    at org.elasticsearch.index.translog.Translog.add(Translog.java:489)
    ... 13 more
```

This is a pretty constant thing, and there seems to be nothing around about what might cause it. All of our ES settings are pretty much the defaults, and we haven't (deliberately) changed anything between versions. 

Is there some change in ES 2.0 which might explain the above, such as maybe tweaking default thread counts for bulk (etc)? If so, where would I start looking to resolve this exception?

If I had to guess at the rate of failure, I'd say that `&gt; 50%` of bulk items receive this error. 

I'm not sure if this is related, but this is also showing up inside the logs; perhaps something regarding indexing is badly configured?

```
[2015-11-19 18:21:48,456][WARN ][indices.memory           ] [La Nuit] failed to set shard [my-index][4] index buffer to [309.3mb]
```
</description><key id="117876902">14866</key><summary>Bulk items fail due to TranslogException via ClosedChannelException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitfin</reporter><labels><label>:Bulk</label><label>discuss</label></labels><created>2015-11-19T18:27:42Z</created><updated>2015-11-26T15:11:13Z</updated><resolved>2015-11-26T13:47:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-19T19:26:20Z" id="158165589">Hmm very odd.  Could you provide the contents of your config file, and any cluster or index-level settings that you have?
</comment><comment author="mikemccand" created="2015-11-19T19:42:48Z" id="158172739">How many nodes in the cluster?  Does this also happen with a single node?

Can you turn on TRACE logging and recreate the issue then post the resulting log?  That exception seems to indicate the translog file handle was already closed when the bulk indexing tried to append to it.

I'll fix the `indices.memory` warning to tell us the root cause exception (it silently swallows it now ... grr).
</comment><comment author="mikemccand" created="2015-11-19T19:45:05Z" id="158173298">Is it possible you are closing the index at the same time as indexing?
</comment><comment author="mikemccand" created="2015-11-19T19:52:06Z" id="158175061">I opened https://github.com/elastic/elasticsearch/pull/14867 for the missing root-cause exception in changing shard's indexing buffer.
</comment><comment author="whitfin" created="2015-11-19T21:15:08Z" id="158200242">@clintongormley sure, see below. At a glance, it appears we are simply using a vanilla install (there are no cluster level settings). This is the config file our install is pointing to. I'll also point out in advance that this is currently only being used locally (i.e. from the same box), it doesn't allow outside connection.

```
# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please see the documentation for further information on configuration options:
# &lt;http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html&gt;
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#

#cluster.name: elasticsearch

#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
# node.name: node-1
#
# Add custom attributes to the node:
#
# node.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#

path.data: /mnt/data/elasticsearch/

#
# Path to log files:
#
# path.logs: /path/to/logs
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#

bootstrap.mlockall: true

#
# Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory
# available on the system and that the owner of the process is allowed to use this limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind adress to a specific IP (IPv4 or IPv6):
#
# network.host: 192.168.0.1
#
# Set a custom port for HTTP:
#

http.port: 9200

#
# For more information, see the documentation at:
# &lt;http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html&gt;
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
# gateway.recover_after_nodes: 3
#
# For more information, see the documentation at:
# &lt;http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html&gt;
#
# --------------------------------- Discovery ----------------------------------
#
# Elasticsearch nodes will find each other via unicast, by default.
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
# discovery.zen.ping.unicast.hosts: ["host1", "host2"]
#
# Prevent the "split brain" by configuring the majority of nodes (total number of nodes / 2 + 1):
#
# discovery.zen.minimum_master_nodes: 3
#
# For more information, see the documentation at:
# &lt;http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html&gt;
#
# ---------------------------------- Various -----------------------------------
#
# Disable starting multiple nodes on a single system:
#
# node.max_local_storage_nodes: 1
#
# Require explicit names when deleting indices:
#
# action.destructive_requires_name: true
```

@mikemccand this is only a single node at the moment, because we're just running sanity-type stuff. Indexing rate is only a couple of events per second, grouped into bulk using the `BulkProcessor` and released in 5s batches. If we are closing the index, it's not _us_ actively doing it, we've never closed indexes ;) I'm not sure exactly what's relevant from TRACE, so I grabbed a bunch of output above the actual Exception in hopes of finding the correct thing (this is before all of the Exceptions, so I assume it's useful).

```
[2015-11-19 21:07:21,232][TRACE][index.warmer             ] [La Nuit] [my-index][4] warming took [1.9ms]
[2015-11-19 21:07:21,232][TRACE][indices                  ] [La Nuit] [my-index][4] top warming [WarmerContext: ElasticsearchDirectoryReader(FilterLeafReader(_7nu(5.2.1):C44162) FilterLeafReader(_7nv(5.2.1):c6) FilterLeafReader(_7nw(5.2.1):c8) FilterLeafReader(_7nx(5.2.1):c3))]
[2015-11-19 21:07:21,232][TRACE][index.warmer             ] [La Nuit] [my-index][4] top warming took [18.4micros]
[2015-11-19 21:07:21,258][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][scheduler][T#1] DW: anyChanges? numDocsInRam=3 deletes=false hasTickets:false pendingChangesInFullFlush: false
[2015-11-19 21:07:21,258][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][scheduler][T#1] IW: nrtIsCurrent: infoVersion matches: false; DW changes: true; BD changes: false
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DW: anyChanges? numDocsInRam=3 deletes=false hasTickets:false pendingChangesInFullFlush: false
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: nrtIsCurrent: infoVersion matches: false; DW changes: true; BD changes: false
[2015-11-19 21:07:21,259][TRACE][index.shard              ] [La Nuit] [my-index][1] refresh with source: schedule
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DW: anyChanges? numDocsInRam=3 deletes=false hasTickets:false pendingChangesInFullFlush: false
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: nrtIsCurrent: infoVersion matches: false; DW changes: true; BD changes: false
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: flush at getReader
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DW: startFullFlush
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DW: anyChanges? numDocsInRam=3 deletes=false hasTickets:false pendingChangesInFullFlush: false
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWFC: addFlushableState DocumentsWriterPerThread [pendingDeletes=gen=0, segment=_7pt, aborted=false, numDocsInRAM=3, deleteQueue=DWDQ: [ generation: 3859 ]]
[2015-11-19 21:07:21,259][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWPT: flush postings as segment _7pt numDocs=3
[2015-11-19 21:07:21,261][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWPT: new segment has 0 deleted docs
[2015-11-19 21:07:21,261][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWPT: new segment has no vectors; no norms; docValues; no prox; freqs
[2015-11-19 21:07:21,261][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWPT: flushedFiles=[_7pt_Lucene50_0.dvm, _7pt_Lucene50_0.tip, _7pt_Lucene50_0.doc, _7pt_Lucene50_0.tim, _7pt.fdx, _7pt.fdt, _7pt_Lucene50_0.dvd, _7pt.fnm]
[2015-11-19 21:07:21,261][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWPT: flushed codec=Lucene50
[2015-11-19 21:07:21,261][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DWPT: flushed: segment=_7pt ramUsed=0.589 MB newFlushedSize=0.006 MB docs/MB=533.084
[2015-11-19 21:07:21,261][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: create compound file
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DW: publishFlushedSegment seg-private updates=null
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: publishFlushedSegment
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: publish sets newSegment delGen=7943 seg=_7pt(5.2.1):c3
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: now checkpoint "_7ps(5.2.1):C43988 _7pt(5.2.1):c3" [2 segments ; isCommit = false]
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: 0 msec to checkpoint
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: apply all deletes during flush
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: now apply all deletes for all segments maxDoc=43991
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] BD: applyDeletes: open segment readers took 0 msec
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] BD: applyDeletes: no segments; skipping
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] BD: prune sis=segments_43: _7ps(5.2.1):C43988 _7pt(5.2.1):c3 minGen=6713 packetCount=0
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: return reader version=20315 reader=StandardDirectoryReader(segments_43:20315:nrt _7ps(5.2.1):C43988 _7pt(5.2.1):c3)
[2015-11-19 21:07:21,262][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] DW: elasticsearch[La Nuit][refresh][T#9] finishFullFlush success=true
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt_Lucene50_0.dvm"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt_Lucene50_0.dvm"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt_Lucene50_0.dvm
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt_Lucene50_0.tip"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt_Lucene50_0.tip"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt_Lucene50_0.tip
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt_Lucene50_0.doc"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt_Lucene50_0.doc"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt_Lucene50_0.doc
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt_Lucene50_0.tim"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt_Lucene50_0.tim"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt_Lucene50_0.tim
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt.fdx"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt.fdx"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt.fdx
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt.fdt"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt.fdt"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt.fdt
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt_Lucene50_0.dvd"
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt_Lucene50_0.dvd"
[2015-11-19 21:07:21,263][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt_Lucene50_0.dvd
[2015-11-19 21:07:21,263][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete new file "_7pt.fnm"
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw.ifd] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IFD: delete "_7pt.fnm"
[2015-11-19 21:07:21,264][TRACE][index.store.deletes      ] [La Nuit][my-index][1] StoreDirectory.deleteFile: delete file _7pt.fnm
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] TMP: findMerges: 2 segments
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] TMP:   seg=_7ps(5.2.1):C43988 size=7.704 MB
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] TMP:   seg=_7pt(5.2.1):c3 size=0.006 MB [floored]
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] TMP:   allowedSegmentCount=4 vs count=2 (eligible count=2) tooBigCount=0
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] MS: now merge
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] MS:   index: _7ps(5.2.1):C43988 _7pt(5.2.1):c3
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] MS:   no more merges pending; now return
[2015-11-19 21:07:21,264][TRACE][index.engine.lucene.iw   ] [La Nuit] [my-index][1] elasticsearch[La Nuit][refresh][T#9] IW: getReader took 5 msec
[2015-11-19 21:07:21,264][TRACE][indices                  ] [La Nuit] [my-index][1] warming [WarmerContext: MultiReader(FilterLeafReader(_7pt(5.2.1):c3))]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [38.3micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [36micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [72.6micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [38.3micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [39.6micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [26.8micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [29.5micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [26.1micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [38micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [24.5micros]
[2015-11-19 21:07:21,264][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [35.2micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [33.9micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [45.4micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [34.8micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [46.4micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [55.3micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [35.4micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [35.2micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [37.4micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [36micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [40.2micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [32.8micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [30.2micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [28.4micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [30.8micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [32.9micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [36.4micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [39.4micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [37.5micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [34.8micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [33.7micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [34.5micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [29.5micros]
[2015-11-19 21:07:21,265][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [39.5micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warmed bitset for [QueryWrapperFilter(+*:* -QueryWrapperFilter(_type:__*))], took [24.8micros]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] warming took [2.2ms]
[2015-11-19 21:07:21,266][TRACE][indices                  ] [La Nuit] [my-index][1] top warming [WarmerContext: ElasticsearchDirectoryReader(FilterLeafReader(_7ps(5.2.1):C43988) FilterLeafReader(_7pt(5.2.1):c3))]
[2015-11-19 21:07:21,266][TRACE][index.warmer             ] [La Nuit] [my-index][1] top warming took [18micros]
[2015-11-19 21:07:21,742][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [16440] bytes, now [16440]
[2015-11-19 21:07:21,742][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [-16440] bytes, now [0]
[2015-11-19 21:07:23,639][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [16440] bytes, now [16440]
[2015-11-19 21:07:23,639][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [-16440] bytes, now [0]
[2015-11-19 21:07:24,839][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [16440] bytes, now [16440]
[2015-11-19 21:07:24,839][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [-16440] bytes, now [0]
[2015-11-19 21:07:26,433][TRACE][marvel.agent             ] [La Nuit] collecting data - collectors [index-stats-collector,cluster-stats-collector,cluster-info-collector,cluster-state-collector,index-recovery-collector,indices-stats-collector,shards-collector,node-stats-collector]
[2015-11-19 21:07:26,433][TRACE][marvel.agent.collector.indices] [La Nuit] collector [index-stats-collector] - collecting data...
[2015-11-19 21:07:26,508][TRACE][transport.tracer         ] [La Nuit] [5024019][indices:monitor/stats[n]] received response from [{La Nuit}{RPKw6aGPR-ilRvsQtufC4g}{127.0.0.1}{127.0.0.1:9300}]
[2015-11-19 21:07:26,508][TRACE][marvel.agent             ] [La Nuit] bulk [all] - adding [35] collected docs from [index-stats-collector] collector
[2015-11-19 21:07:26,510][TRACE][marvel.agent.collector.cluster] [La Nuit] collector [cluster-stats-collector] - collecting data...
[2015-11-19 21:07:26,517][TRACE][transport.tracer         ] [La Nuit] [2227][indices:data/write/bulk] received request
[2015-11-19 21:07:26,518][TRACE][index.shard              ] [La Nuit] [my-index][3] index [my-type][e942d5a4692df8f10d6d657067eb6d30][org.elasticsearch.index.mapper.ParseContext$Document@56eef3e5]
[2015-11-19 21:07:26,518][TRACE][index.shard              ] [La Nuit] [my-index][4] index [my-type][b661569def0e70e29ce28ff8c9933e44][org.elasticsearch.index.mapper.ParseContext$Document@7f97cfc5]
[2015-11-19 21:07:26,518][TRACE][index.shard              ] [La Nuit] [my-index][6] index [my-type][d64ba78b781f63f1c5c6fe590131601d][org.elasticsearch.index.mapper.ParseContext$Document@5d6fdfda]
[2015-11-19 21:07:26,518][TRACE][index.shard              ] [La Nuit] [my-index][7] index [my-type][60c150a56289b0f815bbe840165fe6d0][org.elasticsearch.index.mapper.ParseContext$Document@75a1fcab, org.elasticsearch.index.mapper.ParseContext$Document@4bdda35b]
[2015-11-19 21:07:26,518][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [16440] bytes, now [16440]
[2015-11-19 21:07:26,518][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [16440] bytes, now [32880]
[2015-11-19 21:07:26,518][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [-16440] bytes, now [16440]
[2015-11-19 21:07:26,518][TRACE][indices.breaker          ] [La Nuit] [request] Adjusted breaker by [-16440] bytes, now [0]
[2015-11-19 21:07:26,518][DEBUG][action.bulk              ] [La Nuit] [my-index][3] failed to execute bulk item (index) index {[my-index][my-type][e942d5a4692df8f10d6d657067eb6d30], source[{"my-source":true}]}
[my-index][[my-index][3]] TranslogException[Failed to write operation [Index{id='e942d5a4692df8f10d6d657067eb6d30', type='my-type'}]]; nested: ClosedChannelException;
    at org.elasticsearch.index.translog.Translog.add(Translog.java:497)
    at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:532)
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:447)
    at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:556)
    at org.elasticsearch.index.engine.Engine$Index.execute(Engine.java:815)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1073)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:338)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:131)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
    at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:110)
    at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:199)
    at org.elasticsearch.common.io.Channels.writeToChannel(Channels.java:206)
    at org.elasticsearch.index.translog.BufferingTranslogWriter.flush(BufferingTranslogWriter.java:75)
    at org.elasticsearch.index.translog.BufferingTranslogWriter.add(BufferingTranslogWriter.java:62)
    at org.elasticsearch.index.translog.Translog.add(Translog.java:489)
    ... 13 more
```
</comment><comment author="mikemccand" created="2015-11-20T20:01:34Z" id="158510110">Thanks for the log @zackehh, but can you post more of the prior log entries, so we can see a successful bulk indexing request before the failed one?  This entry shows a refresh, warming (both look fine) and then the one failed bulk indexing request ...
</comment><comment author="whitfin" created="2015-11-21T22:49:28Z" id="158688630">@mikemccand how can I tell if there's a successful entry?
</comment><comment author="mikemccand" created="2015-11-22T22:39:43Z" id="158814813">@zackehh it's the lines that look like this:

```
[2015-11-19 21:07:26,518][TRACE][index.shard              ] [La Nuit] [my-index][3] index [my-type][e942d5a4692df8f10d6d657067eb6d30][org.elasticsearch.index.mapper.ParseContext$Document@56eef3e5]
[20
```

But in general the more logs you can share the more likely we'll see something unusual here.
</comment><comment author="whitfin" created="2015-11-23T18:27:52Z" id="159020404">@mikemccand I'll try and get this to you; we've had to roll back in the testing environment due to release schedules, but I'll try find some time to get another instance up and running and try repro.
</comment><comment author="mikemccand" created="2015-11-23T19:58:10Z" id="159046022">@zackehh OK, thanks.  Especially, if there were any other exceptions in the logs, even much before the exceptions you posted, those would be good to see.
</comment><comment author="mikemccand" created="2015-11-24T23:36:28Z" id="159439685">@zackehh Can you test with Elasticsearch 2.1.0 (just released today) to see if the issue is still happening?  It has the fix for #14867 so if you hit that same exception it should be logged this time, and please post new logs if so!
</comment><comment author="whitfin" created="2015-11-24T23:41:11Z" id="159440472">@mikemccand sure thing. I'm gonna get a setup sorted with 2.x sometime soon (pending Thanksgiving), so I should hopefully have an answer soon:)
</comment><comment author="s1monw" created="2015-11-25T12:39:06Z" id="159595759">I can reproduce this issue but only if I have a concurrent shard failure happening. Do you see anything in the logs that starts with `[WARN] failed engine`?
</comment><comment author="s1monw" created="2015-11-26T15:11:13Z" id="159935230">@zackehh it would still be very very valueable to get more infos about what is going on on your side. ie. do you run into OOM, shard failures, out of disk etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve documentation about query DSL range query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14865</link><project id="" key="" /><description>Feedback from user:
"...a lot of our product revolves around Elasticsearch. we find a lot of our customers have a really hard time understanding the Elasticsearch documentation about the query DSL, because the segments described don't provide context on where they seem to fit in. ie:
                      https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html It shows you a helpful segment, but doesn't provide an easy way to jump from there to "ok how do I use this in an actual query"
</description><key id="117876509">14865</key><summary>Improve documentation about query DSL range query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">dedemorton</reporter><labels><label>:Query DSL</label><label>docs</label><label>enhancement</label></labels><created>2015-11-19T18:25:19Z</created><updated>2016-01-15T12:40:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>GeoPointV2 update docs and query builders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14864</link><project id="" key="" /><description>This PR updates the documentation for GeoPointField by removing all references to the coerce and doc_values parameters. By default DocValues are required in Lucene's new GeoPointField for boundary filtering - so this field parameter is not configurable. The QueryBuilders are updated to automatically normalize points (ignoring the coerce parameter) for any index created onOrAfter version 2.2.
</description><key id="117863247">14864</key><summary>GeoPointV2 update docs and query builders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Query DSL</label><label>docs</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T17:23:59Z</created><updated>2016-03-09T22:11:02Z</updated><resolved>2016-03-09T22:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T13:23:25Z" id="161638303">Also remove the reference to geo points on this page: https://www.elastic.co/guide/en/elasticsearch/reference/current/coerce.html

Should probably add a note about coerce/doc-values to breaking changes: https://www.elastic.co/guide/en/elasticsearch/reference/2.x/breaking-changes-2.2.html
</comment><comment author="clintongormley" created="2016-03-08T18:54:35Z" id="193913954">One typo otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter cloud azure credentials</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14863</link><project id="" key="" /><description>Bug introduced in #13779: we don't filter anymore credentials because we were filtering `cloud.azure.storage.account` and `cloud.azure.storage.key` but now credentials are like `cloud.azure.storage.XXX.account` and `cloud.azure.storage.XXX.key` where `XXX` can be a storage setting id.

Closes #14843.
</description><key id="117827626">14863</key><summary>Filter cloud azure credentials</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>regression</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T14:41:56Z</created><updated>2015-12-30T17:01:44Z</updated><resolved>2015-11-19T16:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-19T14:42:26Z" id="158076188">@imotov Could you review it please?
</comment><comment author="imotov" created="2015-11-19T14:49:26Z" id="158078265">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Current CIDR parsing will silently parse invalid CIDRs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14862</link><project id="" key="" /><description>I noticed this while reviewing #14773. Namely, the code that exists today in IPv4RangeBuilder.java and modifications proposed in #14773 will silently parse invalid IPv4 address/mask combinations. This is due to a regex pattern

``` java
private static final Pattern MASK_PATTERN = Pattern.compile("(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,3})");
```

that will match strings that do not represent valid combinations. For example, it will match "256.512.256.512/33".

The code then proceeds to capture these potentially invalid octets/mask and parse them.

``` java
int addr = (( Integer.parseInt(matcher.group(1)) &lt;&lt; 24 ) &amp; 0xFF000000)
  | (( Integer.parseInt(matcher.group(2)) &lt;&lt; 16 ) &amp; 0xFF0000)
  | (( Integer.parseInt(matcher.group(3)) &lt;&lt; 8 ) &amp; 0xFF00)
  | ( Integer.parseInt(matcher.group(4)) &amp; 0xFF);

int mask = (-1) &lt;&lt; (32 - Integer.parseInt(matcher.group(5)));
```

While there are regex patterns to match IPv4 address/masks, I think that a simple state machine would be vastly simpler.
</description><key id="117824295">14862</key><summary>Current CIDR parsing will silently parse invalid CIDRs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T14:24:19Z</created><updated>2016-01-22T18:30:52Z</updated><resolved>2015-11-24T01:51:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-19T14:54:23Z" id="158080063">Thanks for opening this issue. Let's fix this leniency!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update surefire to 2.19, checkstyle to 2.17, clean to 3.0.0, shade to 2.4.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14861</link><project id="" key="" /><description># Release notes for maven-surefire-plugin 2.19:
- new parser of test patterns, let's call it Test Filter API, related to
  parameters: test, ex/includes, ex/includesFile;
- a feature to interrupting the test-set after exceedded certain number of
  errors/failures
- new Doxia Version
- anchoring test class names
- shutdown operations
- command based communication between in-plugin and forked process
- improvements in JUnit and TestNG runners
- etc.

See http://www.mail-archive.com/announce@maven.apache.org/msg00710.html for details.
# Release notes for maven-checkstyle-plugin 2.17:
## Bug
- [MCHECKSTYLE-302] - Using inline configuration does not work with Maven 2.2.1
- [MCHECKSTYLE-304] - Using inline configuration, checkstyle-checker.xml is generated using DTD v1.2
- [MCHECKSTYLE-310] - Parrallel build failing with various errors
- [MCHECKSTYLE-311] - "mvn clean site -Preporting" fails with Could not find resource 'config/maven_checks.xml'
## Improvement
- [MCHECKSTYLE-291] - Change format of violation message
- [MCHECKSTYLE-293] - Update to use non deprecated method Checker.setClassLoader()
## Task
- [MCHECKSTYLE-307] - Upgrade to Checkstyle 6.11
- [MCHECKSTYLE-313] - Upgrade to Checkstyle 6.11.2
# Release Notes - Apache Maven Clean Plugin  Version 3.0.0

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317224&amp;version=12330417
## Improvements:
- [MCLEAN-56] - Make Plugin only 3.X compatible - get rid of Maven 2.
- [MCLEAN-62] - Upgrade to maven-plugins parent version 27
- [MCLEAN-63] - Make naming of properties consistent
- [MCLEAN-65] - Bump version to 3.0.0
- [MCLEAN-66] - Upgrade maven-shared-utils to 0.9
- [MCLEAN-67] - Change package name to org.apache.maven.plugins
- [MCLEAN-69] - Upgrade maven-shared-utils to 3.0.0
# Release Notes - Apache Maven Shade Plugin  Version 2.4.2

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317921&amp;version=12333008
## Bugs:
- [MSHADE-172] - "java.lang.ArithmeticException: / by zero" in MinijarFilter
- [MSHADE-190] - Shade does not relocate the contents of META-INF/services files
- [MSHADE-209] - [REGRESSION] "java.lang.ArithmeticException: / by zero" in MinijarFilter (reporter Jon McLean).
## Improvements:
- [MSHADE-205] - Better use of ClazzpathUnit for improved jar minimization (contribution of Benoit Perrot).
- [MSHADE-207] - Replace wrong link to codehaus with correct location
- [MSHADE-210] - Upgrade maven-plugins parent to version 28.
- [MSHADE-211] - Keep Java 1.5

Same as #14193 but on 2.x branch
</description><key id="117814854">14861</key><summary>Update surefire to 2.19, checkstyle to 2.17, clean to 3.0.0, shade to 2.4.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.2.0</label></labels><created>2015-11-19T13:35:49Z</created><updated>2015-11-19T15:59:44Z</updated><resolved>2015-11-19T15:59:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-19T14:39:15Z" id="158075382">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop Gradle from destroying IntelliJ project settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14860</link><project id="" key="" /><description>This commit stops Gradle from destroying the IntelliJ project settings
when a Gradle refresh is executed in IntelliJ.

Closes #14809
</description><key id="117806274">14860</key><summary>Stop Gradle from destroying IntelliJ project settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T12:44:16Z</created><updated>2015-11-19T12:45:43Z</updated><resolved>2015-11-19T12:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-19T12:45:28Z" id="158046801">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch master head is not working with V1.7.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14859</link><project id="" key="" /><description>Hi 

I have upgraded my elasticsearch from v1.4 to v1.7.3. The index service is up but I am unable to view in elasticsearch master head. Is it a know issue ??
If resolved already can you provide the solution .

Thanks
</description><key id="117793324">14859</key><summary>elasticsearch master head is not working with V1.7.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ashitpupu</reporter><labels /><created>2015-11-19T11:23:51Z</created><updated>2015-11-19T11:55:57Z</updated><resolved>2015-11-19T11:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-19T11:55:57Z" id="158035187">Please ask your questions on discuss.elastic.co. We can help there.

You are asking here support about the head plugin which is not part of this repository but a community plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add jar hell check before tests run</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14858</link><project id="" key="" /><description>Because jar hell checks run during static initialization of tests, a
failure will result in all tests failing. However, only the first test
within each jvm shows the jarhell failure, and later tests get a class
not found exception for the class that failed to load when static init
failed.

This change adds a task to run as part of precommit, which checks the
test runtime classpath for jarhell.

closes #14721
</description><key id="117781859">14858</key><summary>Add jar hell check before tests run</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T10:12:10Z</created><updated>2015-11-20T19:35:51Z</updated><resolved>2015-11-20T19:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-19T10:20:46Z" id="158015240">LGTM.
</comment><comment author="rjernst" created="2015-11-19T18:22:49Z" id="158146038">@rmuir I pushed a new commit added a comment explaining how the marker file works, and fixing the issue with when the marker is written (it now works regardless of log level).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect license header</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14857</link><project id="" key="" /><description>While refactoring I found the following:

https://github.com/elastic/elasticsearch/blob/148265bd164cd5a614cd020fb480d5974f523d81/core/src/main/java/org/elasticsearch/common/Base64.java

states in the license header the file to be licensed to Elastic, given the class comment this seems bogus. Maybe there are other files with the same issue in our code base, need to check.
</description><key id="117779246">14857</key><summary>Incorrect license header</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>adoptme</label><label>build</label></labels><created>2015-11-19T09:57:19Z</created><updated>2016-05-03T16:26:48Z</updated><resolved>2016-05-03T16:26:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T13:50:20Z" id="216533193">@MaineC why do you think this license header is incorrect? It says:

&gt; Licensed to Elasticsearch under one or more contributor license agreements

that covers public domain surely?
</comment><comment author="jasontedor" created="2016-05-03T14:04:49Z" id="216537847">The author of the code in question has the following comment on the [SourceForge page](http://iharder.sourceforge.net/current/java/base64/):

&gt; I have released this software into the Public Domain. That means you can do whatever you want with it. Really. You don't have to match it up with any other open source license &amp;em; just use it. You can rename the files, move the Java packages, whatever you want. If your lawyers say you have to have a license, contact me, and I'll make a special release to you under whatever reasonable license you desire: MIT, BSD, GPL, whatever.
</comment><comment author="clintongormley" created="2016-05-03T16:26:43Z" id="216584947">Sounds like we're good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use a specific matcher implementation for REST test blacklists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14856</link><project id="" key="" /><description>With this commit we replace the previously used PathMatcher
from the JDK with a specific matcher that is implemented for
this purpose and supports only simple globbing patterns
(i.e. *).

Closes #11391
</description><key id="117769672">14856</key><summary>Use a specific matcher implementation for REST test blacklists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T09:02:46Z</created><updated>2015-11-19T14:55:32Z</updated><resolved>2015-11-19T14:30:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-19T09:03:56Z" id="157994594">I've implemented simple globbing for REST test blacklists. What's allowed now?
- The pattern "/foo/bar/*baz" will match e.g. "/some/suite/foo/bar/my_baz"
- The pattern "/foo\,bar/baz,/bar" will expand to the two blacklist patterns "/foo,bar/baz" and "/bar", so it's possible to define paths with commas in them if the comma is escaped with a backslash.

The Javadoc of `BlacklistedPathPatternMatcher` explains the supported patterns also in a bit more detail.

And obviously PathMatcher is gone. :)

Could you please check @jaymode, @javanna?
</comment><comment author="jaymode" created="2015-11-19T13:01:15Z" id="158051305">LGTM. would be good if @javanna or someone else can also take a look
</comment><comment author="danielmitterdorfer" created="2015-11-19T14:04:19Z" id="158065095">I've added a license header that was missing for the test case I've added and I use `Strings.EMPTY_ARRAY` now instead of `new String[0]`. Would be great if you could have a final look @javanna, then I can merge the PR.
</comment><comment author="javanna" created="2015-11-19T14:23:06Z" id="158071671">+1 go ahead @danielmitterdorfer thanks
</comment><comment author="danielmitterdorfer" created="2015-11-19T14:55:32Z" id="158080496">Pushed to master and backported to 2.x. Thanks @jaymode and @javanna for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] [WIP] Introduce ConditionalProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14855</link><project id="" key="" /><description>closes #14647
</description><key id="117769670">14855</key><summary>[Ingest] [WIP] Introduce ConditionalProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-11-19T09:02:45Z</created><updated>2016-01-18T20:37:21Z</updated><resolved>2016-01-18T20:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-27T11:59:24Z" id="160123839">@talevy This looks good. I left a couple of comments.
</comment><comment author="talevy" created="2015-12-01T01:35:43Z" id="160818956">We decided to postpone this PR until failure handling is figured out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix integTest output if the elasticsearch script fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14854</link><project id="" key="" /><description>If there is a failure in the elasticsearch start script, we currently
completely lose the failure. This is due to how spawning works with ant.
This change avoids the issue by introducing an intermediate script,
built dynamically before running ant exec, which runs elasticsearch and
redirects the output to a log file. This essentially causes us to run
elasticsearch in the foreground and capture the output, but at the same
time keep a running script which ant can pump streams from (which will
always be empty).
</description><key id="117757667">14854</key><summary>Fix integTest output if the elasticsearch script fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T07:27:09Z</created><updated>2015-11-19T20:03:04Z</updated><resolved>2015-11-19T20:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-19T20:02:34Z" id="158180091">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchHit should not contains _routing and _timestamp fields in its iterator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14853</link><project id="" key="" /><description>version: 2.0, Java client.

I am migrating my program to elasticsearch 2.0 now, and I got failed when I tried to retrieve fields data via SearchHit.

At first, I use hit.iterator() to retrieve an iterator to access the fields which I specified in the query, and I got error because there are two unexpected fields named "_routing" and "_timestamp" contained in the iterator.

Then I checked the returned map from hit.fields() or hit.getFields(), which shows that the map actually contains the extra two fields.

As mentioned in the elasticsearch's guide, the new 2.0 will return meta information by default, but I believe the _routing and _timestamp fields should be considered as meta rather than the "fields" of query at client query api side.

So please remove the two fields from the fields of SearchHit and make them accessible via other meta-like methods.
</description><key id="117747992">14853</key><summary>SearchHit should not contains _routing and _timestamp fields in its iterator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xzer</reporter><labels><label>:Java API</label><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2015-11-19T06:06:19Z</created><updated>2016-05-12T12:55:36Z</updated><resolved>2016-05-12T12:55:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-19T12:21:29Z" id="158042625">@jpountz what do you think?
</comment><comment author="jpountz" created="2015-11-19T15:11:54Z" id="158085002">+1 I think this is a good point
</comment><comment author="xzer" created="2015-12-07T05:54:54Z" id="162417972">@clintongormley  I noticed that you tagged this issue as enhancement, but I think it is actually a bug which cause different data structure between java api and the raw json response format.
</comment><comment author="fltiago" created="2015-12-08T01:38:07Z" id="162730415">@xzer so, in your opinion those fields `_routing` and `_timestamp` should be in a `metaFields` attribute?
</comment><comment author="xzer" created="2015-12-08T01:59:30Z" id="162734717">@fltiago, yes, I think the _routing and _timestamp should be as meta fields rather than common fields of the query result since the responsed raw json format also does not include them in result fields.

to clarify, assume I performed the following query,

``` javascript
{
  "from": 0,
  "size": 10,
  "query": {
    "bool": {
      "must": {
        "terms": {
          "someIds": [
            788,
            2510,
            2534,
            3158
          ]
        }
      }
    }
  },
  "fields": "oneField",
}
```

then I got the following result:

``` javascript
"hits": [

    {
        "_index": "some_index",
        "_type": "some_type",
        "_id": "4108260_1991",
        "_score": 16.746162,
        "_routing": "1991",
        "_timestamp": 1445431608007,
        "fields": {
            "oneField": [
                4108260
            ]
        }
    }
    ,
    {
        "_index": "some_index",
        "_type": "some_type",
        "_id": "421549_2002",
        "_score": 16.746162,
        "_routing": "2002",
        "_timestamp": 1445450850478,
        "fields": {
            "oneField": [
                421549
            ]
        }
    }
```

On the java side, when the developer tries to iterate all the queried fields from SearchHit, he should never expect that there are other unknown fields except those are specified  by "fields" in the query.

I am agree with that for most cases which does not cause problems even there are two extra fields, but for some cases, like mine, it broke my program.
</comment><comment author="clintongormley" created="2016-05-12T12:55:36Z" id="218748437">Closing in favour of #18280
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decouple routing and primary operation logic in TransportReplicationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14852</link><project id="" key="" /><description>Currently the logic to route an operation to the primary and the local primary operation is tightly coupled in `TransportReplicationAction#PrimaryPhase`. This change decouples the routing and retrying logic to `ReroutePhase` and the primary operation to `PrimaryPhase`, allowing to perform more informed shard level operations in the future.
</description><key id="117745952">14852</key><summary>Decouple routing and primary operation logic in TransportReplicationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Recovery</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T05:43:51Z</created><updated>2015-12-10T22:18:10Z</updated><resolved>2015-12-10T06:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-11-27T07:00:18Z" id="160055515">@bleskes I have updated the PR to:
- allow `TransportReplicationAction`subclasses to only provide `ShardID` instead of `ShardIterator`
- simplified `PrimaryPhase` to work with `ShardID`

Though this is still a WIP (have to cleanup the `ReplicationPhase` logic around replicating to only non-local copies), would appreciate a review.
</comment><comment author="bleskes" created="2015-11-30T13:04:30Z" id="160622254">@areek I think this is going in the right direction. left comments here and there on the commit.
</comment><comment author="areek" created="2015-11-30T18:03:05Z" id="160707707">Thanks @bleskes for the feedback. I updated the PR to address them. 
Now subclasses for `TransportReplicationAction` sets the target shardID in `resolveRequest` and the base class is responsible for routing to the primary copy and replicating to all (non-local w.r.t. primary copy) replicas according to the cluster's routing table instead of relying on the subclasses to provide shard routings for replica copies.
</comment><comment author="bleskes" created="2015-12-03T11:15:30Z" id="161601879">I did a full review again and I really like where this is going. I would love it if @jasontedor gives a look as well.
</comment><comment author="areek" created="2015-12-03T19:20:04Z" id="161752687">I pushed the current changes to a feature branch `feature/refactor_replication_action` for CI. I will update the branch with any new commits here
</comment><comment author="areek" created="2015-12-07T21:31:45Z" id="162670013">Thanks @bleskes for the feedback :) I updated the PR to address them all and added some unit tests for the reroute and shard operation phases in replication action. A review would be awesome.
</comment><comment author="bleskes" created="2015-12-08T10:13:18Z" id="162839577">LGTM except some minor requests. Thanks areek for all the hard work. Let's see what @jasontedor has to say :)
</comment><comment author="jasontedor" created="2015-12-10T03:06:51Z" id="163471532">This is great; I really like this. The one hangup I continue to have is mutating the requests, but some of the concern there has been addressed by bringing it into the open in `ReroutePhase#doRun`.

LGTM.
</comment><comment author="areek" created="2015-12-10T06:35:40Z" id="163515329">Thanks for the thorough review, @bleskes and @jasontedor. I squashed all the commits and merged this to master. Will back port this to 2.x after letting it bake for some time.
</comment><comment author="areek" created="2015-12-10T22:18:02Z" id="163765675">back ported to 2.x in https://github.com/elastic/elasticsearch/commit/f1622aa60af3440591e0afddd892be30c07d96df
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong Percentile Rank calculation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14851</link><project id="" key="" /><description>I'm getting unexpected results with percentile rank statistic:

Request:

```
PUT percenttest
{
  "settings": {
    "number_of_shards": 1, 
    "number_of_replicas": 0
  }
}
POST percenttest/doc/1
{
  "i": 2.7
}
POST percenttest/doc/2
{
  "i": 85.9
}
POST percenttest/doc/3
{
  "i": 11.9
}
POST percenttest/doc/4
{
  "i": 51.5
}
GET percenttest/_search?search_type=count
{
  "aggs": {
    "percent_rank": {
      "percentile_ranks": {
        "field": "i",
        "values": [
          1,2,3,4,11,12,50,52,70,85,86,87
        ]
      }
    }
  }
}
```

Response:

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 4,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "percent_rank": {
         "values": {
            "1.0": 7.8804347826086945,
            "2.0": 10.597826086956522,
            "3.0": 13.31521739130435,
            "4.0": 16.032608695652172,
            "11.0": 28.790983606557376,
            "12.0": 29.815573770491806,
            "50.0": 36.40988372093024,
            "52.0": 37.86337209302326,
            "70.0": 50.94476744186046,
            "85.0": 61.84593023255813,
            "86.0": 62.57267441860465,
            "87.0": 63.29941860465116
         }
      }
   }
}
```

But there is no element &lt; 2 and I'm getting 10.6% for that.
Also 100% of values are &lt; 87, but I'm getting 65.3%.

Using ES 1.6
</description><key id="117732675">14851</key><summary>Wrong Percentile Rank calculation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rcrezende</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-11-19T03:22:53Z</created><updated>2015-12-08T20:21:04Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-19T11:54:47Z" id="158034861">The [docs](https://www.elastic.co/guide/en/elasticsearch/guide/current/percentiles.html#_percentile_ranks) indicate that this should be accurate for small data sets. (Also, the `compression` parameter doesn't seem to be accepted by this agg).

@polyfractal any ideas what is going on here?
</comment><comment author="clintongormley" created="2015-11-19T12:12:35Z" id="158039499">Related to #10216
</comment><comment author="polyfractal" created="2015-11-19T14:31:54Z" id="158073729">Yes, this appears (in part) to be related to #10216.  ES v1.6 is still on T-Digest 3.0, which interpolates the last centroid in a non-ideal manner.  Effectively this means the upper end of the ranks can become heavily skewed.  See the linked bug-fix in #10216 for more details.

Regarding the lower end, I think this may just be the nature of the approximation.  My understanding is that T-Digest maintains a list of weighted centroids, and computes ranks by finding the two closest centroids for a given rank and performing a weighted interpolation between them.  

In the case of the lower bound, there is no centroid "on the left" of the lowest value.  So T-Digest approximates this interpolation by using the midpoint between it's nearest neighbor on the right, and then just applying that to the left as a best guess.  

In this example, you have a centroid at `2.7` and one at `11.9`, so the midpoint is `(11.9-2.7)/2 == 4.6`.  This value is then used on the "left" of the lowest centroid, meaning T-Digest will continue to interpolate as low as -1.9:

``` json
"percent_rank": {
  "values": {
    "-2.0": 0,
    "-1.9": 0,
    "-1.8": 0.27173913043478104,
    "-1.0": 2.445652173913042,
    "0.0": 5.163043478260868,
    "1.0": 7.8804347826086945,
    "2.0": 10.597826086956522
  }
}
```

Which is obviously not idea :(

@jpountz I wonder, if we (ES or T-Digest) could track the min/max of the entire range and simply cutoff interpolation on the extreme ends?  The CDF would no longer be smooth, but it would behave in a more logical manner.  Alternatively I suppose you could just linearly interpolate to the min/max instead of "guessing" with the nearest centroid's delta.

But it makes me wonder...if the lowest centroid has a weight of one (e.g. it represents the truly lowest value), why does the algorithm try to interpolate to the "left" anyway?  Shouldn't it start interpolation from the centroid itself and go up?

Finally, would it make sense to implement a "++" version of this, ala HyperLogLog++?  We could maintain a list of values under a certain threshold.  If there are fewer than `n` values, the list is sorted and used to calculate percentiles/ranks.  If it breaches `n`, the array is replayed into a T-Digest sketch and proceeds as normal.

/cc @colings86 since he's worked with all the percentile ranks stuff a bunch too
</comment><comment author="rcrezende" created="2015-11-21T01:56:29Z" id="158574540">when you say "ES v1.6 is still on T-Digest 3.0", does that imply there is an alternative implementation in 2.0 we might get better precision?
</comment><comment author="polyfractal" created="2015-11-21T14:11:24Z" id="158645330">Ah, sorry, that was poor phrasing.  ES 2.0 is also still on T-Digest 3.0 (as per https://github.com/elastic/elasticsearch/issues/10216).  I'm not sure the status, it looks like there are still test failures due to the bugfix upstream.
</comment><comment author="polyfractal" created="2015-11-21T14:22:34Z" id="158646553">Just as a note, in this case, the accuracy should improve if you put some more data into the percentiles.  It is providing a poor estimation because there are only a few centroids (due to only having a handful of docs).  Which is skewing the T-Digest interpolation on both the high and low end. 

E.g. just adding a few more docs improves the ranks significantly:

``` json
POST percenttest/doc/_bulk
{"index":{}}
{"i":2.7}
{"index":{}}
{"i":85.9}
{"index":{}}
{"i":11.9}
{"index":{}}
{"i":51.5}
{"index":{}}
{"i":3}
{"index":{}}
{"i":15}
{"index":{}}
{"i":66.8}
{"index":{}}
{"i":32.1}
{"index":{}}
{"i":54.2}
{"index":{}}
{"i":71.3}
{"index":{}}
{"i":8.8}
{"index":{}}
{"i":72.9}
```

``` js
{
   "took": 4,
   "timed_out": false,
   "_shards": {...},
   "hits": {...},
   "aggregations": {
      "percent_rank": {
         "values": {
            "1.0": 0,
            "2.0": 0,
            "3.0": 8.743169398907105,
            "4.0": 11.475409836065573,
            "11.0": 26.747311827956988,
            "12.0": 29.435483870967744,
            "50.0": 56.18401206636501,
            "52.0": 57.6923076923077,
            "70.0": 77.59562841530055,
            "85.0": 86.92307692307692,
            "86.0": 87.56410256410257,
            "87.0": 88.2051282051282
         }
      }
   }
}
```
</comment><comment author="rcrezende" created="2015-11-23T19:41:16Z" id="159040978">For the suggested accuracy as result of adding more data, if we consider that any aggregation can be a result of a filter, I'd prefer obtaining a confidence interval though.
</comment><comment author="rcrezende" created="2015-11-26T18:05:16Z" id="159970580">Another alternative is to use the same approach OpenJDK uses for sorting arrays with a meta-algorithm. Instead of always use the same DualPivot algorithm, it has thresholds on the Array size to decide when to use Insertion-sort, QuickSort, CountingSort. See here: http://www.docjar.com/html/api/java/util/DualPivotQuicksort.java.html

So maybe there is a T where length of sequence &lt;T, the approximation algorithm starts is not worth (precision is bad) nor efficient (time). 
</comment><comment author="rcrezende" created="2015-12-08T20:21:04Z" id="163005517">Here is another example with more data and still the bug happening.

Note that percentile_rank for 90 is greater than for 95 which is wrong.

```
PUT percenttest_2
{
    "settings":
    {
        "number_of_shards": 1,
        "number_of_replicas": 0
    }
}
POST percenttest_2/doc/0
{"i": 47.600000}
POST percenttest_2/doc/1
{"i": 29.600000}
POST percenttest_2/doc/2
{"i": 44.700000}
POST percenttest_2/doc/3
{"i": 41.200000}
POST percenttest_2/doc/4
{"i": 60.900000}
POST percenttest_2/doc/5
{"i": 45.200000}
POST percenttest_2/doc/6
{"i": 49.700000}
POST percenttest_2/doc/7
{"i": 49.800000}
POST percenttest_2/doc/8
{"i": 59.400000}
POST percenttest_2/doc/9
{"i": 78.200000}
POST percenttest_2/doc/10
{"i": 100.000000}
POST percenttest_2/doc/11
{"i": 58.300000}
POST percenttest_2/doc/12
{"i": 54.200000}
POST percenttest_2/doc/13
{"i": 56.600000}
POST percenttest_2/doc/14
{"i": 51.900000}
POST percenttest_2/doc/15
{"i": 72.000000}
POST percenttest_2/doc/16
{"i": 87.500000}
POST percenttest_2/doc/17
{"i": 48.500000}
POST percenttest_2/doc/18
{"i": 58.300000}
POST percenttest_2/doc/19
{"i": 60.000000}
POST percenttest_2/doc/20
{"i": 48.200000}
POST percenttest_2/doc/21
{"i": 40.400000}
POST percenttest_2/doc/22
{"i": 33.200000}
POST percenttest_2/doc/23
{"i": 49.000000}
POST percenttest_2/doc/24
{"i": 35.700000}
POST percenttest_2/doc/25
{"i": 39.600000}
POST percenttest_2/doc/26
{"i": 39.600000}
POST percenttest_2/doc/27
{"i": 44.900000}
POST percenttest_2/doc/28
{"i": 39.000000}
POST percenttest_2/doc/29
{"i": 54.200000}
POST percenttest_2/doc/30
{"i": 61.000000}
POST percenttest_2/doc/31
{"i": 60.200000}
POST percenttest_2/doc/32
{"i": 36.700000}
POST percenttest_2/doc/33
{"i": 40.200000}
POST percenttest_2/doc/34
{"i": 58.600000}
POST percenttest_2/doc/35
{"i": 41.200000}
POST percenttest_2/doc/36
{"i": 27.600000}
POST percenttest_2/doc/37
{"i": 35.900000}
POST percenttest_2/doc/38
{"i": 42.700000}
POST percenttest_2/doc/39
{"i": 72.700000}
POST percenttest_2/doc/40
{"i": 40.700000}
POST percenttest_2/doc/41
{"i": 50.500000}
POST percenttest_2/doc/42
{"i": 53.200000}
POST percenttest_2/doc/43
{"i": 100.000000}
POST percenttest_2/doc/44
{"i": 42.700000}
POST percenttest_2/doc/45
{"i": 33.100000}
POST percenttest_2/doc/46
{"i": 45.400000}

GET percenttest_2/_search?search_type=count
{
  "aggs": {
    "percent_rank": {
      "percentile_ranks": {
        "field": "i",
        "values": [
          90,95,100
        ]
      }
    }
  }
}

```

Returns

```
{
   "took": 47,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 47,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "percent_rank": {
         "values": {
            "90.0": 95.01268787819637,
            "95.0": 0,
            "100.0": 100
         }
      }
   }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve test output on errors and when debugging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14850</link><project id="" key="" /><description>There were a number of subtle issues with the existing logging that
wraps events from Junit4 and ant. This change:
- Tweaks at what level certain events are logged
- Fixes -Dtests.output=always to force everything to be logged
- Makes -Dtests.class imply -Dtests.output=always
- Captures ant logging from junit4, so that direct jvm output will be
  logged on failure when not using gradle info logging
</description><key id="117715689">14850</key><summary>Improve test output on errors and when debugging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-19T00:48:29Z</created><updated>2015-11-19T18:39:35Z</updated><resolved>2015-11-19T18:39:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-19T13:45:45Z" id="158060341">awesome, this is a huge improvement.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify where index.similarity.default.type is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14849</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html#default-base) we tell users how to change `index.similarity.default.type` but not _where_ to change it.

I didn't want to assume we do it in `elasticsearch.yml`, so it'd be good to document this explicitly.
</description><key id="117714399">14849</key><summary>Clarify where index.similarity.default.type is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Search</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-11-19T00:36:21Z</created><updated>2015-11-24T14:01:43Z</updated><resolved>2015-11-24T14:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Document firewall/network configuration requirements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14848</link><project id="" key="" /><description>This is commonly available in other enterprise software product documentation.  There is generally a network configuration page/guide that tells network administrators what ports and if it's 1 way or bi-directional (eg. 9300 for transport, 9200 http) that they will have to open up for inter-node and client-cluster communications.  It will be helpful to have this information documented in our guide (on the network settings page, or an administration guide, etc..).
</description><key id="117713490">14848</key><summary>Document firewall/network configuration requirements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Network</label><label>adoptme</label><label>docs</label></labels><created>2015-11-19T00:28:01Z</created><updated>2016-01-15T12:40:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Gradle hides some details of jvm crashes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14847</link><project id="" key="" /><description>If the jvm crashes during a test run, we get limited output like this:

```
[ant:junit4] ERROR: JVM J3 ended with an exception, command line: /usr/local/jenkins/tools/hudson.model.JDK/JDK8/bin/java -ea -Xmx1024m -Xms1024m -XX:MaxDirectMemorySize=512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/mnt/jenkins/workspace/es_core_master_strong/core/build/heapdump -server -XX:+UseG1GC -XX:-UseCompressedOops -XX:+AggressiveOpts -Dtests.prefix=tests -Dtests.seed=AECA7AEAD5CCBA53 -Dtests.security.manager=true -Djava.io.tmpdir=./temp -Dtests.maven=true -Dtests.artifact=core -Dtests.heap.size=1024m "-Dtests.jvm.argline=-server -XX:+UseG1GC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Djava.awt.headless=true -Dtests.ifNoTests=fail -Dtests.nightly=false -Des.logger.level=DEBUG -Dtests.task=:core:integTest -Dtests.network=false -Djunit4.childvm.cwd=/mnt/jenkins/workspace/es_core_master_strong/core/build/testrun/integTest/J3 -Djunit4.childvm.id=3 -Djunit4.childvm.count=4 -classpath /mnt/jenkins/workspace/es_core_master_strong/core/build/classes/test:/mnt/jenkins/workspace/es_core_master_strong/core/build/resources/test:/mnt/jenkins/workspace/es_core_master_strong/core/build/classes/main:/mnt/jenkins/workspace/es_core_master_strong/core/build/resources/main:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-core/5.4.0-snapshot-1714615/d8d3f55ea169ec7a483c076d8d786d2fa7ec0447/lucene-core-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-analyzers-common/5.4.0-snapshot-1714615/95e58f09a8878e74f2cceba8aca898d7464782a8/lucene-analyzers-common-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-backward-codecs/5.4.0-snapshot-1714615/bf7600f7556e4cbdfb7b93947e0ef2312d4bfa6f/lucene-backward-codecs-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-grouping/5.4.0-snapshot-1714615/a77cedd6e706317f2f3c1b693736ed191a30a488/lucene-grouping-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-highlighter/5.4.0-snapshot-1714615/84d9d5617fae821a7cf1f0f5ac961f4128a936e9/lucene-highlighter-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-join/5.4.0-snapshot-1714615/9e7646fb8cfbb72e061eba83467a05671cfa5ee1/lucene-join-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-memory/5.4.0-snapshot-1714615/68417b06ee7adf7f751bbeabdb2ed14e5ef3172/lucene-memory-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-misc/5.4.0-snapshot-1714615/f12de7f380e1f40b9432c90a2bb0b9ba67e94c41/lucene-misc-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queries/5.4.0-snapshot-1714615/8313aa54e3d13e40bf7bf32b6ee68c7c4b547d6a/lucene-queries-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-queryparser/5.4.0-snapshot-1714615/62696f81612a2594c5eac93c9db57c108d3e951a/lucene-queryparser-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-sandbox/5.4.0-snapshot-1714615/a506a09e97070b74c226627ab8adcde9f32576ef/lucene-sandbox-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial/5.4.0-snapshot-1714615/22924c220508d02a30e9425f6293fcff1621a23a/lucene-spatial-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-spatial3d/5.4.0-snapshot-1714615/af3be612c990303f6c575411b436c967f665876c/lucene-spatial3d-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-suggest/5.4.0-snapshot-1714615/c797cf414ac18868e804afe6be65c30046a5d2dc/lucene-suggest-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.elasticsearch/securesm/1.0/c0c6cf986ba0057390bfcc80c366a0e3157f944b/securesm-1.0.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.3.1/1303efbc4b181e5a58bf2e967dc156a3132b97c0/commons-cli-1.3.1.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.carrotsearch/hppc/0.7.1/8b5057f74ea378c0150a1860874a3ebdcb713767/hppc-0.7.1.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.8.2/d27c24204c5e507b16fec01006b3d0f1ec42aed4/joda-time-2.8.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.joda/joda-convert/1.2/35ec554f0cd00c956cc69051514d9488b1374dec/joda-convert-1.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.6.2/123f29333b2c6b3516b14252b6e93226bfcd6e37/jackson-core-2.6.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-smile/2.6.2/395d18c1a1dd730b8026ee59c4067e5d2b45ba6e/jackson-dataformat-smile-2.6.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.6.2/4ae23088dd3fae47c66843f2e4251d7255ee140e/jackson-dataformat-yaml-2.6.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-cbor/2.6.2/1e13c575f914c83761bb8e2aca7dfd9e4c647579/jackson-dataformat-cbor-2.6.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.15/3b132bea69e8ee099f416044970997bde80f4ea6/snakeyaml-1.15.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.10.5.Final/9ca7d55d246092bddd29b867706e2f6c7db701a0/netty-3.10.5.Final.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.ning/compress-lzf/1.0.2/62896e6fca184c79cc01a14d143f3ae2b4f4b4ae/compress-lzf-1.0.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.tdunning/t-digest/3.0/84ccf145ac2215e6bfa63baa3101c0af41017cfc/t-digest-3.0.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.1.6/7495feb7f71ee124bd2a7e7d83590e296d71d80e/HdrHistogram-2.1.6.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.spatial4j/spatial4j/0.5/6e16edaf6b1ba76db7f08c2f3723fce3b358ecc3/spatial4j-0.5.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.vividsolutions/jts/1.13/3ccfb9b60f04d71add996a666ceb8902904fd805/jts-1.13.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.github.spullara.mustache.java/compiler/0.9.1/14aec5344639782ee76441401b773946c65eb2b3/compiler-0.9.1.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/log4j/apache-log4j-extras/1.2.17/85863614d82185d7e51fe21c00aa9117a523a8b6/apache-log4j-extras-1.2.17.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.2/8619e95939167fb37245b5670135e4feb0ec7d50/slf4j-api-1.6.2.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/net.java.dev.jna/jna/4.1.0/1c12d070e602efd8021891cdd7fd18bc129372d4/jna-4.1.0.jar:/mnt/jenkins/workspace/es_core_master_strong/test-framework/build/libs/test-framework-3.0.0-SNAPSHOT.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/randomizedtesting-runner/2.2.0/60de504132241be049564a3a34fd7dcc296e2ef0/randomizedtesting-runner-2.2.0.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-test-framework/5.4.0-snapshot-1714615/1e4bf816dd65707f0b2d8041af63e91df71cb7a0/lucene-test-framework-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.lucene/lucene-codecs/5.4.0-snapshot-1714615/3e324496abc07c4d32f2492a7941dd01824af734/lucene-codecs-5.4.0-snapshot-1714615.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.3.6/4c47155e3e6c9a41a28db36680b828ced53b8af4/httpclient-4.3.6.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.3.3/f91b7a4aadc5cf486df6e4634748d7dd7a73f06d/httpcore-4.3.3.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.3/f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f/commons-logging-1.1.3.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.10/4b95f4897fa13f2cd904aee711aeafc0c5295cd8/commons-codec-1.10.jar:/var/lib/jenkins/.gradle/caches/modules-2/files-2.1/com.carrotsearch.randomizedtesting/junit4-ant/2.2.0/d401c9c729deccd5db8a5df3102eb18793c2224/junit4-ant-2.2.0.jar com.carrotsearch.ant.tasks.junit4.slave.SlaveMainSafe -eventsfile /mnt/jenkins/workspace/es_core_master_strong/core/build/testrun/integTest/temp/junit4-J3-20151118_090500_411.events @/mnt/jenkins/workspace/es_core_master_strong/core/build/testrun/integTest/temp/junit4-J3-20151118_090500_411.suites -stdin
[ant:junit4] ERROR: JVM J3 ended with an exception: Forked process returned with error code: 137.
    at com.carrotsearch.ant.tasks.junit4.JUnit4.executeSlave(JUnit4.java:1483)
    at com.carrotsearch.ant.tasks.junit4.JUnit4.access$000(JUnit4.java:130)
    at com.carrotsearch.ant.tasks.junit4.JUnit4$2.call(JUnit4.java:963)
    at com.carrotsearch.ant.tasks.junit4.JUnit4$2.call(JUnit4.java:960)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

But we dont see the stderr from the jvm that crashed. This is either something we are omitting in the gradle logger that connects to junit4's events, or we are misconfiguring junit4.
</description><key id="117685862">14847</key><summary>Gradle hides some details of jvm crashes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label></labels><created>2015-11-18T21:29:19Z</created><updated>2015-11-19T01:21:31Z</updated><resolved>2015-11-19T01:21:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-18T21:29:55Z" id="157870448">we are missing stderr in general. we should stop using gradle's logging
</comment><comment author="rjernst" created="2015-11-19T01:19:04Z" id="157915817">I've opened #14850 to improve various aspects of how output is handled for tests. I don't think it handles this issue though, as with --info logging (which we have in jenkins) we are already getting the INFO ant messages, which is what extraneous jvm output is logged at by junit4. I did improve the case when _not_ running with --info, so that you will still see this output.

I also investigated piping stderr and stdout directly. This doesn't work for extraneous jvm output, since we have no control over that (things the jvm outputs directly, disregarding when System.out/err is changed). This is what junit4 logs on its own, and I've opened an issue there to improve the situation: randomizedtesting/randomizedtesting#222

For regular test stderr/stdout, the problem with not using gradle's logging is the stderr and stdout is captured while ant is running. Stderr actually comes through (as ant log messages) but stdout does not, when we are running with gradle's normal logging level (both appear when run with --info). I did fix a number of things in the referenced PR, though, which I think will improve the situation greatly. But unless we can actually reproduce the crash (or even know why it crashed and thus what was expected to be output that would indicate the reason for the crash), I don't know what else we could do here.

For reference, this is the original jenkins job that failed:
http://build-us-00.elastic.co/job/es_core_master_strong/5550/

FWIW, I looked through maven jenkins jobs for crashes, and found this one, which has no more information than that which we saw with gradle:
http://build-us-00.elastic.co/job/es_core_2x_metal/454/console
</comment><comment author="rjernst" created="2015-11-19T01:21:18Z" id="157916149">Given that the maven job referenced above, I don't think there is anything to do here with regards to gradle, so I am closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test rpm and deb signing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14846</link><project id="" key="" /><description>Adds a dummy gpg key to distribution/src/test/resources and uses it to sign the deb
and rpm every build. Then it tests that they are properly signed in the
vagrant tests.

Also renames `gpg.keyname` to `gpg.key` everywhere. The deb and rpm packages
used `gpg.key` and some other modules used `gpg.keyname`. Now they all use
`gpg.key`.

Finally reset gpg.keypath to ~/.gnupg in the release script to keep it working
after this patch lands. We'll be defaulting gpg.keypath to
`${project.parent.basedir}/src/test/resources/dummyGpg` to test rpm and
deb signing with every package build rather than just on release. The
release should default the keypath back to `~/.gnupg`.
</description><key id="117663468">14846</key><summary>Test rpm and deb signing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>test</label><label>v2.3.0</label></labels><created>2015-11-18T19:35:28Z</created><updated>2016-02-03T16:18:06Z</updated><resolved>2016-02-03T16:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-18T19:35:54Z" id="157835344">@spinscale can you have another look at this? Its been too long.
</comment><comment author="nik9000" created="2015-11-18T22:14:42Z" id="157881863">I just reran the tests - it passes on all of the vagrant machines.
</comment><comment author="nik9000" created="2016-01-28T16:10:48Z" id="176253516">@spinscale any chance you can look at this again? It looks like it is still merge-able but its been so long that I think it needs more eyes on it.
</comment><comment author="spinscale" created="2016-01-29T13:13:03Z" id="176747229">ran this on linux and osx (no rpm though), checked the sigs afterwards. Was good.

However when ran with a custom GPG key, it still seemed to fallback to the dummy one (or I named the arguments wrong?)

```
mvn clean verify -Dskip.unit.tests=true -Dgpg.key=ID -Dgpg.passphrase=SECRET -Dgpg.keyring=~/.gnupg/secring.gpg
```

log

```
[INFO] spawn rpm --define _gpg_name B6C6BB45 --define _gpg_path /home/spinscale/x-pack/elasticsearch/distribution/src/test/resources/dummyGpg --addsign elasticsearch-2.2.0-SNAPSHOT20160129130403.noarch.rpm
[INFO] Enter pass phrase:
[INFO] gpg: WARNING: unsafe permissions on homedir `/home/spinscale/x-pack/elasticsearch/distribution/src/test/resources/dummyGpg'
[INFO] gpg: skipped "$KEYID": No secret key
[INFO] gpg: signing failed: No secret key
[INFO] Pass phrase check failed or gpg key expired
```

did I get a parameter wrong?
</comment><comment author="nik9000" created="2016-01-29T14:33:02Z" id="176783157">&gt; `gpg.keyring`

I believe its `gpg.keypath` with this.  The trouble was that we used both of those names for it in different places. I just picked a name.
</comment><comment author="nik9000" created="2016-01-29T14:36:00Z" id="176784840">Maybe I should just do this against master and not maven at all. Its likely that all the build stuff has changed anyway there so any disruption I cause is less likely to matter.
</comment><comment author="spinscale" created="2016-01-29T15:25:19Z" id="176810732">ok, passed on Linux with `gpg.keypath` configured appropriately. Also the custom key was used (important for the release, which uses `gpg.keypath`). IMO we should still add this to 2.x, so it is part of CI. I guess the release process will be different on master, because the release script needs to be changed a lot to do all the gradle calls to build and deploy.
</comment><comment author="nik9000" created="2016-02-03T14:47:44Z" id="179272937">OK! I'm going to give it one last kick on my side and then try and forward port this to master as well.
</comment><comment author="nik9000" created="2016-02-03T16:18:04Z" id="179319951">OK! Passed locally. Hurray! I'll merge now and have a look at forward porting to master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix SnapshotBackwardsCompatibilityIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14845</link><project id="" key="" /><description>SnapshotBackwardsCompatibilityIT had drifted from its twin methods in
SharedClusterSnapshotRestoreIT. This creates SnapshotSharedTestCases which
makes the similar methods in both test cases use the same code, modulo the
cluster upgrade.

Related to #13522
</description><key id="117656501">14845</key><summary>Fix SnapshotBackwardsCompatibilityIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label></labels><created>2015-11-18T19:01:53Z</created><updated>2017-01-20T14:04:38Z</updated><resolved>2015-11-25T14:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-20T19:12:33Z" id="158498100">Rebased to resolve merge issues and renamed the shared test cases.
</comment><comment author="danielmitterdorfer" created="2015-11-24T10:01:38Z" id="159214341">@nik9000: I left a few comments.
</comment><comment author="nik9000" created="2015-11-25T14:51:51Z" id="159630779">Thanks @danielmitterdorfer! I've addressed the comments, squashed, and rebased. Since you gave it a LGTM in the comments I'll merge now.

Thanks against!
</comment><comment author="nik9000" created="2015-11-25T14:52:00Z" id="159630811">s/against/again/!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split the `string` field type into `text` and `keyword`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14844</link><project id="" key="" /><description>This is the first phase for #12394 which splits the `string` field type into
`text` and `keyword`. In short `text` should be used whenever analyzed `string`s
were used in the past while `keyword` should be used for the `not_analyzed` case.

The `string` field mapper is gone, but still supported: for indices that were
created before 3.0, a parser is registered that forwards to the text parser
when `index=analyzed` and to the `keyword` parser otherwise. In 3.0 it is not
possible to create string mappings anymore.

String fields are now dynamically mapped as text, to be consistent with the
previous behaviour that would map string fields as analyzed string fields.

As of 3.0, parsing becomes a bit stricter:
- `index` only accepts `true` and `false` (as a boolean or string, doesn't matter)
- `keyword` fields don't accept term vectors options
- `keyword` fields reject `positions` and `offsets` as `index_options`
</description><key id="117651335">14844</key><summary>Split the `string` field type into `text` and `keyword`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-11-18T18:37:55Z</created><updated>2016-02-26T13:32:33Z</updated><resolved>2016-02-26T13:32:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-18T18:40:49Z" id="157815057">The diff is large, but it's mostly because tests define lots of string fields that had to be replaced with either `text` or `keyword`. If you do a review, the classes to focus on are StringFieldMapper.Parser, KeywordFieldMapper and TextFieldMapper.
</comment><comment author="jpountz" created="2015-11-19T10:56:11Z" id="158022261">After discussing the change with @rjernst I changed the approach to keep StringFieldMapper alive so that TextFieldMapper and KeywordFieldMapper don't have to handle old bw logic of StringFieldMapper. This meands fields that are mapped as strings will still be strings in 3.0 and we will only be able to get rid of strings in 4.0 when 2.x will not be supported anymore.
</comment><comment author="jpountz" created="2015-11-25T17:12:19Z" id="159676474">There were a few conflicts due to the move of field mapper registration to the node level so I rebased the PR and fixed conflicts.
</comment><comment author="rjernst" created="2015-12-05T02:52:59Z" id="162134243">@jpountz I certainly don't want to have to wait until 4.0 to remove string field. Perhaps we could upgrade the mapping? We could keep the old string mapper in 2.x, and in 3.0 migrate any user with the old type to the equivalent (or closest there is) with text/keyword? This way we can deprecate the string field type in 2.x, and remove in 3.0.

Regarding the PR, do you think we could break this up into smaller pieces to make it more reviewable? There are so many files changed the PR tool cannot even display them all.  I'm thinking something like the following:
1. Add text field type
2. Add keyword field type
3. Change tests using string -&gt; text
4. Change tests using string -&gt; keyword
5. Deprecate string in 2.x
6. Add migration code to text/keyword on upgrade, removing string in master
</comment><comment author="jpountz" created="2015-12-06T21:54:03Z" id="162351331">Thanks @rjernst. I will break it up as suggested.
</comment><comment author="jpountz" created="2016-02-26T13:32:33Z" id="189275058">Closing: this is being superceded by smaller pull requests like #16637 and #16589.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter cloud azure credentials</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14843</link><project id="" key="" /><description>Bug introduced in #13779: we don't filter anymore credentials because we were filtering `cloud.azure.storage.account` and `cloud.azure.storage.key` but now credentials are like `cloud.azure.storage.XXX.account` and `cloud.azure.storage.XXX.key` where `XXX` can be a storage setting id.

See https://github.com/elastic/elasticsearch/blob/master/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java#L35
</description><key id="117648420">14843</key><summary>Filter cloud azure credentials</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>bug</label><label>regression</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T18:21:20Z</created><updated>2015-11-19T16:04:26Z</updated><resolved>2015-11-19T16:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Throw a meaningful error when loading metadata and an alias and index have the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14842</link><project id="" key="" /><description>In 1.x it is possible via index templates to create an index with an alias with the same name as the index. The index index must match the index template and have an alias with the same name as the index being created.

Relates to #14706.
</description><key id="117647747">14842</key><summary>Throw a meaningful error when loading metadata and an alias and index have the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T18:18:44Z</created><updated>2015-12-02T11:19:00Z</updated><resolved>2015-12-02T11:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-18T18:21:42Z" id="157809660">If someone runs into this issue, wouldn't it be better to go back to the old version, remove the alias and then attempt to upgrade again?
</comment><comment author="martijnvg" created="2015-11-18T18:23:46Z" id="157810423">good question. It just the exception that is being thrown is not nice... and we should off course prevent this from happening 
</comment><comment author="jpountz" created="2015-11-18T18:32:20Z" id="157812567">I'm +1 on making the exception better, but more reserved on adding this leniency... Leniency tends to hurt a lot in the long term.
</comment><comment author="clintongormley" created="2015-11-18T18:45:52Z" id="157817349">+1 on throwing a better exception, but not allowing aliases that clash with index names to be imported.
</comment><comment author="martijnvg" created="2015-11-19T06:29:25Z" id="157964645">@jpountz @clintongormley I've changed to PR to thrown a meaningful error instead.
</comment><comment author="javanna" created="2015-12-01T19:10:50Z" id="161065516">LGTM. this situation cannot be recreated anymore with 2.0+ right? so the only thing we can do is fail and ask the user to go back and remove the alias?
</comment><comment author="clintongormley" created="2015-12-02T10:33:09Z" id="161251559">@javanna correct, although just a note: the only way to remove the alias is to delete the index.
</comment><comment author="martijnvg" created="2015-12-02T10:56:06Z" id="161257748">@clintongormley @javanna yes, an index can't be created if there is an index template with an alias that has the same name as the index being created. The create index request will fail now with a class cast exception and when this pr gets in it will fail with a more meaningful error.

So:

```
PUT /_template/test
{
  "template": "test",
  "aliases": {
    "test": {}
  }
}
```

```
PUT /test
```

The create index call fails on &gt;= 2.0. Personally I would prefer that the create index template call would fail too. Now the create index call fails and then in order to create the index the index template needs to be modified. If the create index template call fails basically notify the user earlier in the process.

I'll merge this PR, because we need this for upgrades and I will open a follow up PR the adds extra validation to the create index template api.
</comment><comment author="javanna" created="2015-12-02T11:03:32Z" id="161261404">++ thanks for the explanation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field stats: Fix NPE for index constraint on empty index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14841</link><project id="" key="" /><description>The field does exist used in the index constraint does exist in the mapping, but no document uses it or the index is empty and this causes an NPE:

```
java.lang.NullPointerException
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:122)
    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:54)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.finishHim(TransportBroadcastAction.java:229)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.onOperation(TransportBroadcastAction.java:194)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:174)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:161)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:785)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:769)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:759)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:268)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$ShardTransportHandler.messageReceived(TransportBroadcastAction.java:264)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="117636873">14841</key><summary>Field stats: Fix NPE for index constraint on empty index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>bug</label><label>review</label><label>v2.0.2</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T17:23:59Z</created><updated>2015-12-05T12:52:07Z</updated><resolved>2015-11-18T17:29:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-18T17:26:43Z" id="157786889">LGTM
</comment><comment author="nik9000" created="2015-11-18T17:27:25Z" id="157787209">LGTM too
</comment><comment author="martijnvg" created="2015-11-18T17:29:42Z" id="157788442">Pushed via: https://github.com/elastic/elasticsearch/commit/1661ce5b8c665b915e71c5488243f13176444d29
</comment><comment author="dakrone" created="2015-12-04T21:01:54Z" id="162080959">@martijnvg looking at the git history for the `2.0` branch, it looks like this was _after_ the `v2.0.1` tag, so it should be tagged v2.0.2?
</comment><comment author="clintongormley" created="2015-12-05T12:52:07Z" id="162180910">I've updated the release notes to remove this change from 2.0.1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add support for removeProperty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14840</link><project id="" key="" /><description>Added the api to remove a property, so it can be used in the mutate processor (delete) to support inner fields specified using the dot notation.
</description><key id="117632146">14840</key><summary>add support for removeProperty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-18T17:01:43Z</created><updated>2015-11-19T11:06:48Z</updated><resolved>2015-11-19T11:06:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-19T11:03:12Z" id="158023754">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Output more information when gradle can't start elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14839</link><project id="" key="" /><description>When the build tries to start an elasticsearch instance but the start fails
if it fails to find the log file then we log a line about how we can't find
the file.

Also log the command that we attempted to use.
</description><key id="117614390">14839</key><summary>Output more information when gradle can't start elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T15:44:33Z</created><updated>2015-11-18T18:55:55Z</updated><resolved>2015-11-18T18:55:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-18T16:12:08Z" id="157763561">LGTM, thanks!
</comment><comment author="nik9000" created="2015-11-18T16:18:32Z" id="157765414">@rjernst do you have an opinion?
</comment><comment author="rjernst" created="2015-11-18T16:28:07Z" id="157768249">Should we log the command line and environment at info level outside of error conditions? Exec tasks log at info level what they are running (sort of) but now that we are using ant exec here we get nothing. 
</comment><comment author="nik9000" created="2015-11-18T16:33:53Z" id="157769962">&gt; Should we log the command line and environment at info level outside of error conditions? Exec tasks log at info level what they are running (sort of) but now that we are using ant exec here we get nothing.

I like logging it at error level if it fails. I can log it at info level if it doesn't?
</comment><comment author="rjernst" created="2015-11-18T16:36:58Z" id="157770744">Well my point is we should log it before we try to run the command. 
</comment><comment author="nik9000" created="2015-11-18T16:48:53Z" id="157775031">&gt; Well my point is we should log it before we try to run the command.

Sure - I can do that too.
</comment><comment author="nik9000" created="2015-11-18T17:25:50Z" id="157786618">@rjernst how about now?
</comment><comment author="rjernst" created="2015-11-18T18:40:02Z" id="157814789">LGTM, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI Failure] GeoDistanceRangeQueryTests.testToQuery fails due to numeric precision problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14838</link><project id="" key="" /><description>Via: http://build-us-00.elastic.co/job/es_core_master_centos/8587/testReport/junit/org.elasticsearch.index.query/GeoDistanceRangeQueryTests/testToQuery/

```
FAILURE 0.21s | GeoDistanceRangeQueryTests.testToQuery &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: a numeric value within &lt;-0.33952242975718117&gt; of &lt;-0.3395224297571812&gt;
   &gt;      but: &lt;-0.3395224297571813&gt; differed by &lt;0.3395224297571812&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([5BDFC4A476C6B339:AC24C69A074576D3]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.index.query.GeoDistanceRangeQueryTests.assertLegacyQuery(GeoDistanceRangeQueryTests.java:165)
   &gt;    at org.elasticsearch.index.query.GeoDistanceRangeQueryTests.doAssertLuceneQuery(GeoDistanceRangeQueryTests.java:120)
   &gt;    at org.elasticsearch.index.query.GeoDistanceRangeQueryTests.doAssertLuceneQuery(GeoDistanceRangeQueryTests.java:41)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.assertLuceneQuery(AbstractQueryTestCase.java:555)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.testToQuery(AbstractQueryTestCase.java:487)
   &gt;    at org.elasticsearch.index.query.GeoDistanceRangeQueryTests.testToQuery(GeoDistanceRangeQueryTests.java:203)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Reproduces with:

```
gradle :core:test -Dtests.seed=5BDFC4A476C6B339 -Dtests.class=org.elasticsearch.index.query.GeoDistanceRangeQueryTests -Dtests.method="testToQuery" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:+UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=pt -Dtests.timezone=America/New_York
```
</description><key id="117611715">14838</key><summary>[CI Failure] GeoDistanceRangeQueryTests.testToQuery fails due to numeric precision problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Geo</label><label>jenkins</label><label>test</label></labels><created>2015-11-18T15:31:37Z</created><updated>2015-11-18T20:10:11Z</updated><resolved>2015-11-18T20:09:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-18T15:31:47Z" id="157750065">@nknize can you take a look at this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Template: "Field name [myChild.name] cannot contain '.'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14837</link><project id="" key="" /><description>Hey,

I have a query template that looks like:

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "myChild",
          "filter": {
            "bool": {
              "must": [
                {
                  "term": {
                    "myChild.name": "{{name}}"
                  }
                },
                .......... more
              ]
            }
          }
        }
      }
    }
  }
}
```

Saving it causes:

```
{"msg":"[mapper_parsing_exception] Field name [myChild.name] cannot contain '.'
```

But running the exact same query not as template works as expected,

What is wrong?

btw this happens with `Elastic Search v2` (works fine with 1.7)

Thanks,

Asaf.
</description><key id="117610589">14837</key><summary>Query Template: "Field name [myChild.name] cannot contain '.'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">asaf</reporter><labels><label>:Search Templates</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-18T15:26:39Z</created><updated>2016-05-07T15:37:21Z</updated><resolved>2016-05-07T15:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T18:38:59Z" id="157814432">See https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots
</comment><comment author="asaf" created="2015-11-18T18:43:07Z" id="157816034">@clintongormley 

This is not the case,
The doc contains no dots, it contains nested properties,

I am talking about query templates,

Elastic probably performs "." field validation on the query template too and it restricts the query to be persisted, but from my understanding, the "nested" filter acts like the nested matcher, which as stated requires "." notation,

```
A nested filter behaves much like a nested query, except that it doesn&#8217;t accept the score_mode 
parameter. It can be used only in filter context&#8212;such as inside a filtered query&#8212;and it behaves like any 
other filter: it includes or excludes, but it doesn&#8217;t score.
```

Please re-open :)

Thanks.
</comment><comment author="clintongormley" created="2015-11-18T18:58:44Z" id="157822464">Ah read way too fast, apologies!
</comment><comment author="clintongormley" created="2015-11-18T19:08:00Z" id="157824961">A simpler recreation:

```
PUT _search/template/foo
{
  "query": {
    "match": {
      "foo.bar": "TEXT"
    }
  }
}
```
</comment><comment author="asaf" created="2015-11-18T19:17:05Z" id="157827225">@clintongormley :+1: 
</comment><comment author="MaineC" created="2015-11-25T13:01:11Z" id="159600996">From a first look at the code the problem seems to be that on indexing the template query we try to parse it as a regular document which includes trying to create a mapping for it. This fails for the reason @clintongormley mentioned earlier (code that complains: https://github.com/elastic/elasticsearch/blob/debfb84d38e8d24755996032fa2bdd2b81f7068f/core/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java#L262 ).

Not yet sure how to fix this issue though.
</comment><comment author="rjernst" created="2015-11-25T19:20:13Z" id="159706024">Why do we index the query template? Is it just for storage (although then I'm confused why it isn't just part of the cluster state)? Can we store it as a blob in a single field instead of using the structure from the query as the document?
</comment><comment author="MaineC" created="2015-11-26T09:33:50Z" id="159858037">@rjernst AFAIK it is just for storage (did a bit of digging, the original issue #5921 also suggests that).

&gt; (although then I'm confused why it isn't just part of the cluster state)

No idea. Maybe @clintongormley or @s1monw can comment on that.

&gt; Can we store it as a blob in a single field instead of using the structure from the query as the 
&gt; document?

IMHO that would make more sense than the current implementation.
</comment><comment author="clintongormley" created="2015-11-28T10:17:14Z" id="160271348">OK the root cause of this appears to be because indexed search templates can be passed without the outer `template` element (which is undocumented).

If you use the documented syntax:

```
PUT _search/template/foo
{
  "template": {
    "query": {
      "match_all": {}
    }
  }
}
```

Then the `.scripts` index contains the following mapping:

```
{
  ".scripts": {
    "mappings": {
      "mustache": {
        "properties": {
          "script": {
            "type": "object",
            "enabled": false
          },
          "template": {
            "type": "object",
            "enabled": false
          }
        }
      }
    }
  }
}
```

The `query` field is correctly set to `enabled: false`, so nothing in the query is indexed or even parsed.

However, without the outer `template` property:

```
PUT _search/template/foo
{
  "query": {
    "match_all": {}
  }
}
```

Then every element in the query generates a field:

```
{
  ".scripts": {
    "mappings": {
      "mustache": {
        "properties": {
          "query": {
            "properties": {
              "match_all": {
                "type": "object"
              }
            }
          },
          "script": {
            "type": "object",
            "enabled": false
          },
          "template": {
            "type": "object",
            "enabled": false
          }
        }
      }
    }
  }
}
```

So the solution here is to only accept a `template` property for indexed templates, and a `script` property for indexed scripts.
</comment><comment author="clintongormley" created="2015-11-29T11:56:02Z" id="160410822">Related to https://github.com/elastic/elasticsearch/issues/13729
</comment><comment author="brwe" created="2015-12-09T17:31:19Z" id="163333267">There are two ways to index scripts:
1. `PUT _search/template/foo`
2. `PUT .scripts/mustache/foo`

For the first we could add extra parsing logic somewhere that ensures that we start with "template" only and nothing else is allowed. For the second we would need a dedicated rest endpoint because currently this is just the regular index API that performs no checks whatsoever except for adding the "_default_" mapping here: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java#L136.

The trouble is that people can still use 2. with the java API and I have no idea how to hack the index API without being yelled at on github when I make the pull request :-) 
</comment><comment author="brwe" created="2015-12-10T13:09:19Z" id="163609675">Discussed with @clintongormley and we decided to fix this only for the _search/template endpoint and open a separate issue for the index api and discuss there
</comment><comment author="clintongormley" created="2016-02-14T17:30:05Z" id="183933672">Related to #16651
</comment><comment author="avalonabecker" created="2016-04-12T14:10:43Z" id="208925011">I got the Field name [myChild.name] cannot contain '.' when I forgot to put the "_search" at the end of my URL, querying for a nested field using the dot syntax from the documentation [here](https://www.elastic.co/guide/en/elasticsearch/guide/current/nested-query.html)  - just posting so others might save time on a newbie mistake there.
</comment><comment author="clintongormley" created="2016-05-07T15:37:21Z" id="217645473">Closed by https://github.com/elastic/elasticsearch/issues/16651
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Percentiles and Percentile Ranks Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14836</link><project id="" key="" /><description /><key id="117608270">14836</key><summary>Aggregations Refactor: Refactor Percentiles and Percentile Ranks Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T15:15:37Z</created><updated>2016-03-10T18:40:26Z</updated><resolved>2015-11-20T14:41:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-20T08:11:56Z" id="158317609">LGTM in general, I just left one comment. Could you also add javadocs to the factories as they will be exposed to users?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize min/max aggs to use index stats when applicable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14835</link><project id="" key="" /><description>Today if you run a min or max aggregation, we will always run the query and collect matches. However in the particular case that the query is a `match_all` query and that there are no deletions (which is fairly common for time series), we could use index statistics and return directly.
</description><key id="117608154">14835</key><summary>Optimize min/max aggs to use index stats when applicable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2015-11-18T15:14:54Z</created><updated>2015-11-18T18:43:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-18T16:22:44Z" id="157766705">I bet there are use cases where it'd be ok to ignore that there are deletes and using index statistics anyway. It'd be nice to have an option for that too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for `daitch_mokotoff`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14834</link><project id="" key="" /><description>[Daitch Mokotoff](https://en.wikipedia.org/wiki/Daitch%E2%80%93Mokotoff_Soundex) support has been added in Lucene 5.
So we can now support it as well.
</description><key id="117601040">14834</key><summary>Add support for `daitch_mokotoff`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis Phonetic</label><label>feature</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T14:43:14Z</created><updated>2015-11-19T14:02:38Z</updated><resolved>2015-11-19T13:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-18T18:26:49Z" id="157811230">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Align handling of interrupts in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14833</link><project id="" key="" /><description>### Background

In #14799, I've spotted that `BulkProcessor` does not handle `InterruptedException` properly. `InterruptedException` is the standard Java facility to request cancellation of an ongoing operation in another thread.

After some discussion (see #14799), we've decided to move the change to the client API to a dedicated ticket for documentation purposes as this change also interferes with the retry feature in the `BulkProcessor` (see #14620).
### Current Situation

There are two code paths in `BulkProcessor`, one for handling bulk requests in a blocking fashion (`concurrentRequests == 0` when constructing `BulkProcessor`)  and one that allows for concurrent bulk requests from the same client. Both handle interruption differently, the latter does it also wrong (by calling `Thread.interrupted` instead of `Thread.currentThread().interrupt()`).
### Desired Situation

Both code paths respond to interruption in a well-defined, documented and ideally consistent way (i.e. both either throw `InterruptedException` or restore the interrupt flag).
</description><key id="117598816">14833</key><summary>Align handling of interrupts in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T14:32:16Z</created><updated>2016-02-19T04:52:49Z</updated><resolved>2015-12-18T18:35:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-18T14:33:22Z" id="157729518">This ticket is currently blocked by #14620.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can we replace the field stats API with search/aggs?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14832</link><project id="" key="" /><description>Most information that the field stats API returns can also be computed with our search API:
- min value (min aggregation)
- max value (max aggregation)
- density (total hits of an exists query divided by the total hits of a match_all)

So for instance, you could get the min/max values of a field called `@timestamp` on all indices with the following aggregation:

``` javascript
{
  "aggs": {
    "by_index": {
      "terms": {
        "field": "_index"
      },
      "aggs": {
        "min_ts": {
          "min": {
            "field": "@timestamp"
          }
        },
        "max_ts": {
          "min": {
            "field": "@timestamp"
          }
        }
      }
    }
  }
}
```

The field stats also has field constraints, which could be implemented on top of the previous aggregations by adding a `bucket_selector` pipeline on the `by_index` aggregation:

``` javascript
{
  "aggs": {
    "by_index": {
      "terms": {
        "field": "_index"
      },
      "aggs": {
        "min_ts": {
          "min": {
            "field": "@timestamp"
          }
        },
        "max_ts": {
          "min": {
            "field": "@timestamp"
          }
        },
        "range_filter": {
          "bucket_selector": {
            "buckets_path": {
              "min_ts": "min_ts",
              "max_ts": "max_ts"
            },
            "script": "max_ts &gt;= 1447718400000 &amp;&amp; min_ts &lt;= 1447804800000"
          }
        }
      }
    }
  }
}
```

As-is this would perform more slowly than the field stats API because it would collect all documents. But I think not having the support this new field stats API is compelling. Moreover in case there are no deletions, we could have similar optimizations as the one we have when couting matches on a match_all when we can just leverage index stats.
</description><key id="117598610">14832</key><summary>Can we replace the field stats API with search/aggs?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-11-18T14:31:11Z</created><updated>2017-04-11T07:05:54Z</updated><resolved>2017-04-11T07:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2015-11-18T14:53:32Z" id="157738549">The reason Kibana needs _field_stats is speed alone. Currently aggregations don't perform fast enough for Kibana to use them for pre-flight requests in determining which indices to search. On my very small data set I get 138ms for an aggregation, and 23ms for _field_stats.
</comment><comment author="jpountz" created="2015-11-18T15:15:19Z" id="157744744">Wouldn't the request cache already help from that perspective?

Separately I think we can work on making such aggs less naive and use index statistics when applicable. I opened #14835
</comment><comment author="rashidkpc" created="2015-11-18T15:22:42Z" id="157746645">The request would cache, but only if the data and time ranges that are not changing right? Most Kibana users have constantly updating data sets and are using `[now-1M TO now]` style time ranges. We need to re-lookup the index list whenever the time range or underlying data changes.

#14835 should work just fine though assuming it still works with the terms agg in the example above
</comment><comment author="jpountz" created="2015-11-18T16:35:21Z" id="157770338">Good point about caching, the script from the bucket selector will change all the time, which is a pity since shards (where the caching is done) don't care about pipeline aggs, which are executed on the coordinating node.
</comment><comment author="jpountz" created="2015-11-18T16:36:26Z" id="157770606">As a side note, the caching would work if the bucket filtering was performed on Kibana side.
</comment><comment author="clintongormley" created="2015-11-27T11:10:51Z" id="160116220">Discussed this in FixItFriday.  Aggregations could be a viable replacement for field stats if:
- The min/max aggs can be optimized to use the field stats to return values from the index stats instead of running the full aggregation (requires a match all query and no deleted docs)
- A pipeline agg is used to filter down the result set to the interesting indices (and can still be cached in the results cache even with the moving boundary)
- The new dimensional format for date fields coming in Lucene 6 can expose the min/max values.
</comment><comment author="jpountz" created="2016-08-05T09:38:32Z" id="237807528">This surfaced again in today's Fixit Friday when discussing whether to remove the experimental tag in the field stats API (#19798) as we would really like for the field stats API to not be necessary for Kibana cc @s1monw .
</comment><comment author="jpountz" created="2017-04-11T07:05:54Z" id="293170328">The field stats API is getting replaced by field caps, which do not expose stats. Closing.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Persist allocation ID with shard state metadata on nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14831</link><project id="" key="" /><description>Implements first step of #14739
</description><key id="117587084">14831</key><summary>Persist allocation ID with shard state metadata on nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T13:30:05Z</created><updated>2015-11-24T10:19:39Z</updated><resolved>2015-11-23T16:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-19T15:15:45Z" id="158086086">left some minor comments.
</comment><comment author="ywelsch" created="2015-11-23T14:25:54Z" id="158948542">pushed new changes.
</comment><comment author="bleskes" created="2015-11-23T15:42:10Z" id="158972717">LGTM. Left one minor comment.
</comment><comment author="s1monw" created="2015-11-24T10:05:02Z" id="159215013">IMO this  was merged too early. I left some more comments on the relevant places
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Query doesn't support minimum_should_match without info in breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14830</link><project id="" key="" /><description>**minimum_should_match** parametr is not supported in Terms Query in ES 2.0 version and **there is no such info in ES 2.0 Breaking Changes** document.

Following query works on ES 1.7.3 but doesn't work on ES 2.0 - [terms] query does not support [minimum_should_match]

``` javascript
GET _all/_search
{
  "query": {
    "terms": {
      "FIELD": [
        "VALUE1",
        "VALUE2"
      ],
      "minimum_should_match": 1
    }
  }
}
```
</description><key id="117585243">14830</key><summary>Terms Query doesn't support minimum_should_match without info in breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zdeseb</reporter><labels /><created>2015-11-18T13:20:21Z</created><updated>2016-02-14T17:14:25Z</updated><resolved>2016-02-14T17:14:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xzer" created="2015-11-19T07:35:04Z" id="157976694">use bool query's should clause instead.

https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html
</comment><comment author="zdeseb" created="2015-11-19T08:01:17Z" id="157983926">I think Issue is, that info about this change is missing in ES 2.0 Breaking Changes document
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkProcessor retries after request handling has been rejected due to a full thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14829</link><project id="" key="" /><description>With this commit we introduce limited retries with a backoff logic to BulkProcessor
when a bulk request has been rejeced with an EsRejectedExecutionException.

The handling is encapsulated entirely in BulkProcessor and currently no
configuration parameters are exposed to the client.

Fixes #14620.
</description><key id="117572813">14829</key><summary>BulkProcessor retries after request handling has been rejected due to a full thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T12:04:48Z</created><updated>2015-12-17T13:33:04Z</updated><resolved>2015-12-17T13:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-18T12:14:07Z" id="157692894">The implementation introduces a (package private) class `Retry` which encapsulates the actual retry and backoff logic. It is intended to be used for all sorts of retries within Elasticsearch based on an arbitrary exception but is currently tied to the client API. This is mainly due to the fact that the current listener-based client API does not compose well.

I have thought whether it makes sense to adapt the client API to be more in line with `CompletableFuture`from JDK 8 which would be easier to compose. However, this is an entirely different (and probably huge) topic.

Depending on the review comments it could make sense to turn `Retry` into a proper public class and move it probably to `org.elasticsearch.common.util.concurrent` which would mean that we have to introduce a thin adapter layer for the client API.

I've also extracted `BulkRequestHandler` from `BulkProcessor` as it was already quite complex to understand and stuffing even more logic into the same class felt wrong to me.
</comment><comment author="tlrx" created="2015-11-25T10:14:30Z" id="159561788">Not much to say, I like it. Do you think we can add a test like `RejectionActionIT`?
</comment><comment author="danielmitterdorfer" created="2015-11-25T12:24:11Z" id="159592380">@tlrx Thanks for your review and your feedback. I'll fix the typo and I already looked at `RejectionActionIT`. I think it is hard to test as the only side effect you would notice is that your requests may slow down due to backoff. As backoff is finite it is still possible we get `EsRejectedExecutionException`, it is just less likely but in a very slow environment the test could lead to false positives.

One thing I have pondered is whether it makes sense to expose retry parameters (number of retries and initial pause time) to users of `BulkProcessor` via its builder at the risk of misconfigurations. This would allow us to raise the number of retries for the test significantly. If we don't want to increase the API surface area, a less intrusive option would be to expose the setters for retry parameters just as package local for the test.

What do you think?
</comment><comment author="tlrx" created="2015-11-25T14:26:03Z" id="159621353">@danielmitterdorfer I like the current code but I find it difficult to test or to configure because the retry and backoff logics are obfuscated in the implementation. 

I think we could add a default retry logic that retries on `EsRejectedException` and that can be configured (number of retries / initial delay) via the BulkProcessor builder. In addition, it would be nice to be able to plug its own retry and/or backoff logic. This would also make the code more testable. 
</comment><comment author="danielmitterdorfer" created="2015-11-30T07:43:17Z" id="160545909">I have started to implement an integration test based on your suggestion but I noticed that we do not get `EsRejectedExecutionException` in the callback `onFailure(Throwable)` but in `onResponse(BulkResponse)` by iterating over the results and checking individual items.

Consequently, we cannot just retry the whole operation but must split the bulk request into multiple bulk requests internally.

Consider this scenario for example:
1. First bulk request for ids [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] (concrete operation is irrelevant for this example)
2. We get success for [1, 2, 3, 4, 5] but a failure due to `EsRejectedExecutionException` for [6, 7, 8, 9, 10].
3. We backoff a bit and construct a new bulk request for [6, 7, 8, 9, 10].
4. We get success for [6, 7, 8, 9] but a failure due to `EsRejectedExecutionException` for [10].
5. We backoff again and construct a new bulk request for [10].
6. We get success for [10].

At this point, we have to notify the `BulkProcessor.Listener` by invoking its callback methods. I can think of two possibilities:
1. Call the callback method every time with the subset of the responses we got back. This is quite straightforward but we leak retry semantics to the listener which might (quite reasonably) expect that the callback method is just invoked once per bulk request.
2. Collect all responses (i.e. [1, 2, 3, 4, 5], [6, 7, 8, 9] and [10]) and construct a new synthetic bulk response and invoke the listener only once.

Although option 2 is more complex to implement, we do not leak retry semantics to `BulkProcessor.Listener`, hence I tend to prefer this approach. A detail in this case: I'd implement `BulkResponse.tookInMillis` in wall clock time semantics, i.e. sum of all BulkResponse.tookInMillis + sum of all backoff times. To me this is the most sensible option from a user perspective.

One additional thing that bothers me that it is possible that we invoke the bulk API with as little as one element but I think working around this problem in the async case we would introduce much more complex code and in the sync case it is not possible at all.

wdyt @tlrx? /cc: @bleskes 
</comment><comment author="bleskes" created="2015-11-30T08:01:24Z" id="160550010">I think of the bulk processor as an abstraction around the bulk API, allowing the user to work with single item requests and have the bulk done automatically behind the scenes. As such I don't think we need to worry about maintaining internal bundling of requests. These are not controlled by the user explicitly anyway.

I would go with the simplest implementation and throw the reject items back into the pending request list, while blocking outgoing requests for a given period of time. They will be executed when the block is removed. (i.e., option #1)

Last - looking at the code I see that our listener is bulk based:

```
    void afterBulk(long executionId, BulkRequest request, BulkResponse response);
```

I think it would be good if the listener also have methods that work on the item level - so people really wouldn't have to work with the bulks at all. You put item request in and you get iterm requests out. Whether we want to do it as part of the listener API or as a basic abstract listener implementation is a good question.

Last I wonder - what is the role of the execution ID in that listener - can you come up with a use case for it?
</comment><comment author="tlrx" created="2015-11-30T08:24:55Z" id="160553084">I agree with @bleskes suggestions :)
</comment><comment author="danielmitterdorfer" created="2015-11-30T08:29:56Z" id="160553800">@bleskes, @tlrx: Thanks for your feedback.

Regarding not maintaining bundling: that's good. :)

Regarding the signature `void afterBulk(long executionId, BulkRequest request, BulkResponse response)`. I like your idea of working just on item level.

But the interface is already there, so we can do either:
- Break backwards compatibility and change it the way you suggested (I also think it is better to get rid of `BulkResponse`in the listener API). I think this is just an option if we don't backport to 2.x..
- Introducing an abstract base class in addition to the interface with the new methods and deprecating the interface
- Introduce an additional interface, deprecate the existing one on 2.x and remove it in master (my preferred option).

I have no clue what the execution id is for, except for some kind of request correlation between a `beforeBulk` operation and an `afterBulk` operation. Otherwise the client would not know that (although I am not sure whether knowing that is relevant...).

So here is what I would do based on your suggestions:
1. Reissue the request internally on any failed items but failing immediately if any of the failures is not of type `EsRejectedExecutionException`.
2. (Pending on your feedback) introduce three new operations based on individual items in a new interface `BulkProcessor.ItemListener`, removing `BulkProcessor.Listener` in master and deprecating it in 2.x. This way we can introduce it without needing clients to change in 2.x. The interface looks like:

```
void beforeExecute(IndexRequest request);
void afterExecute(IndexRequest request, ActionWriteResponse response);
void afterExecute(IndexRequest request, Throwable failure);
```
</comment><comment author="bleskes" created="2015-11-30T15:08:23Z" id="160655901">&gt; except for some kind of request correlation between a beforeBulk operation and an afterBulk operation. Otherwise the client would not know that (although I am not sure whether knowing that is relevant...).

That's actually a valid thing. Imho it shouldn't be part of the listener but rather protected methods. if people want to override and do something custom they can do it by sub classing the class.
</comment><comment author="tlrx" created="2015-12-01T08:47:26Z" id="160896338">&gt; Reissue the request internally on any failed items but failing immediately if any of the failures is not of type EsRejectedExecutionException.

Just to be sure that I correctly understand this point: only rejected item requests should be re-added to the bulk processor. Other failures should directly call `afterExecute(IndexRequest request, ...)`

&gt; Imho it shouldn't be part of the listener but rather protected methods. if people want to override and do something custom they can do it by sub classing the class.

+1
</comment><comment author="danielmitterdorfer" created="2015-12-01T09:24:04Z" id="160906135">&gt; Just to be sure that I correctly understand this point: only rejected item requests should be re-added to the bulk processor. Other failures should directly call `afterExecute(IndexRequest request, ...)`

The exact behavior is:
- If and only if the bulk response consists of only items with success (or we ran out of retry attempts), we call the listener's `onResponse` callback.
- If and only if the bulk response consists of either items with success or items with failure == EsRejectedExecutionException, we will reissue another bulk request for the failed items.
- Otherwise, we fail immediately and delegate either to the listener's `onResponse` callback or `onFailure` callback (depending on which callback method has been called for our internal listener).
</comment><comment author="bleskes" created="2015-12-01T10:55:11Z" id="160934084">If we move to item level based listener (+1), we can call the listener on every successful item. failed item will be checked for a rejection, if so it will be put in the queue. Otherwise call the on failure method of the listener. Makes sense?
</comment><comment author="danielmitterdorfer" created="2015-12-02T10:39:13Z" id="161254295">@bleskes, @tlrx: Thanks for the hints. Could one of you look at the new version?

The implementation was more straightforward and self-contained by keeping the current listener API. Hence, I suggest we keep the listener API for this PR and open a new one for just changing the listener API as this is a good self-contained change, I think.

A short summary of the changes:
- The backoff logic checks now `BulkItemResponse#isFailed()` and its related failure instead of catching an actual exception. Failed items are collected and a request is reissued according to the backoff policy.
- The default backoff policy progresses now exponentially instead of geometrically. Its default configuration leads to the progression: 50ms, 60ms, 80ms, 150ms, 280ms, 580ms, 1.2s, 2.7s.
- The backoff policy can be easily changed by users of `BulkProcessor` via its builder; it is also possible to provide custom policies. The default is to have no backoff (behavior as it is now). I'll update the reference documentation in a separate PR.
- I've added `BulkProcessorRetryIT` which contrasts the behavior of `BulkProcessor` with and without backoff. We check in the backoff case that we do not accidentally introduce duplicates by retrying. 

One thing that bothers me and that I am not really sure about, is that I had to add a mock client for a unit test as unfortunately, retrying requests requires having access to the client. I could have abstracted the client behind another interface but this would just add unnecessary complexity.

As we do not use a mocking framework in core, I have added an "empty" implementation of `Client` in test-framework (`MockClient`). I have mixed feelings about mocking frameworks and I rather use mocking sparingly, but I sense this would be one of the cases where it  makes sense (but the result of #7489 indicates that the problems outweigh the benefits so I am fine with that).
</comment><comment author="danielmitterdorfer" created="2015-12-09T15:53:40Z" id="163300362">@nik9000 Thanks for your review; that was very helpful. I have updated the code and asked a few follow up questions. Could you please answer them? Thanks!
</comment><comment author="nik9000" created="2015-12-09T20:21:01Z" id="163381090">&gt; Could you please answer them? Thanks!

Done!
</comment><comment author="danielmitterdorfer" created="2015-12-10T12:10:02Z" id="163592492">@nik9000: I've pushed another commit incorporating your comments.
</comment><comment author="danielmitterdorfer" created="2015-12-15T17:18:54Z" id="164832067">@nik9000 I've changed the Javadoc and throw now an `IllegalArgumentException` instead of silently capping the value. Can you have a (final?) look please?
</comment><comment author="nik9000" created="2015-12-15T18:36:22Z" id="164850612">I've left another batch of comments. There isn't anything left that should block merging. Address whichever comments you think are important and merge when you are ready. No need to wait for me to review again. If we were in the same time zone it'd be worth it but we're way too many hours off.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 2.0 handler type for murmur3 not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14828</link><project id="" key="" /><description>I have updated 1.4.2 to 2.0.0. 
I installed plugin 
   `sudo bin/plugin install mapper-murmur3`

I had some data inserted in 1.4.2. After I updated to 2.0.0 I am getting this error while starting elasticsearch.

```
Exception: java.lang.IllegalStateException: unable to upgrade the mappings for the index [abc], reason: [no handler for type [murmur3] declared on field [hash]]
Likely root cause: MapperParsingException[no handler for type [murmur3] declared on field [hash]]
    at org.elasticsearch.index.mapper.core.TypeParsers.parseMultiField(TypeParsers.java:341)
    at org.elasticsearch.index.mapper.core.StringFieldMapper$TypeParser.parse(StringFieldMapper.java:201)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:315)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:228)
    at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:137)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:211)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:192)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:368)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:242)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:226)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

My mapping was:

```
PUT /abc
{
  "mappings": {
    "productView": {
      "properties": {
        "itemId": {
           "index": "not_analyzed",
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"type": "string",
           "fields": {
            "hash": {
              "type": "murmur3"
            }
          }
        }
      }
    }
  }
}
```

But when I delete the old data and recreate the index it works without problem.
</description><key id="117568425">14828</key><summary>elasticsearch 2.0 handler type for murmur3 not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">maksutspahi</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.1.1</label><label>v2.2.0</label></labels><created>2015-11-18T11:35:39Z</created><updated>2015-12-22T07:07:52Z</updated><resolved>2015-11-25T15:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-19T10:26:19Z" id="158016366">The issue reproduces for me. It seems to be because we try to upgrade the mappings before plugins have a chance to register mappers. So any mappers that are registered through a plugin are affected by this problem.
</comment><comment author="jpountz" created="2015-11-19T10:41:35Z" id="158019173">@rjernst @imotov Any ideas if/how we can fix it?
</comment><comment author="rjernst" created="2015-11-19T10:47:26Z" id="158020306">I think we need to move registering mappers to the node level (which makes sense anyways).  Instead of getting the mapper service to register them, the mapper service should take in all the custom types as args. I'm not sure if it will directly fix the issue, but at least then it seems like it should be possible. 
</comment><comment author="dadoonet" created="2015-11-19T11:59:06Z" id="158035899">&gt; It seems to be because we try to upgrade the mappings before plugins have a chance to register mappers. So any mappers that are registered through a plugin are affected by this problem.

++ That explains as well issues we have seen for mapper attachments plugin. See https://github.com/elastic/elasticsearch-mapper-attachments/issues/180
</comment><comment author="jpountz" created="2015-11-20T17:04:27Z" id="158461064">&gt; I think we need to move registering mappers to the node level

I opened #14896 for this.
</comment><comment author="jpountz" created="2015-11-25T15:59:16Z" id="159653609">This has been fixed by #14896 and then I added tests in #14977. This will be fixed as of 2.1.1. I considered backporting to 2.0 as well but there were so many conflicts that I got a bit afraid of doing more harm than good by introducing new bugs.
</comment><comment author="jmreymond" created="2015-12-19T06:37:22Z" id="165955063">same issue in 2.1.1
root@dedi:/usr/share/elasticsearch# ./bin/plugin list
Installed plugins in /usr/share/elasticsearch/plugins:
    - marvel-agent
    - license
    - mapper-attachments

[2015-12-19 07:31:59,246][INFO ][node                     ] [Manta] version[2.1.1], pid[28723], build[40e2c53/2015-12-15T13:05:55Z]
[2015-12-19 07:31:59,247][INFO ][node                     ] [Manta] initializing ...
[2015-12-19 07:32:00,314][INFO ][plugins                  ] [Manta] loaded [license, marvel-agent, mapper-attachments], sites []
[2015-12-19 07:32:00,356][INFO ][env                      ] [Manta] using [1] data paths, mounts [[/home (/dev/sda4)]], net usable_space [641.6gb], net total_space [857.8gb], spins? [possibly], types [ext4]
[2015-12-19 07:32:04,427][ERROR][gateway                  ] [Manta] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [evol2], reason: [Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:339)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:368)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:319)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:265)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
    ... 48 more
[2015-12-19 07:32:04,659][ERROR][gateway                  ] [Manta] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [evol2], reason: [Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:339)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:368)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:319)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:265)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
    ... 39 more
[2015-12-19 07:32:04,808][ERROR][gateway                  ] [Manta] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [evol2], reason: [Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:339)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:368)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:319)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:265)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
    ... 30 more
[2015-12-19 07:32:04,824][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalStateException: unable to upgrade the mappings for the index [evol2], reason: [Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]]
Likely root cause: java.lang.IllegalArgumentException: Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:368)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:319)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:265)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
</comment><comment author="dadoonet" created="2015-12-19T07:34:53Z" id="165957678">@jmreymond could you open a new issue about this? 
</comment><comment author="jpountz" created="2015-12-21T09:31:15Z" id="166245909">@jmreymond @dadoonet This is a different issue actually:

```
unable to upgrade the mappings for the index [evol2], reason: [Mapper for [docsfile] conflicts with existing mapping in other types:
[mapper [docsfile.content] cannot be changed from type [string] to [attachment]]]
```

The `evol2` index has at least two types, one of them is declaring `docsfile.content` as a `string` field and another type is declaring it as an `attachment` field. This kind of configuration caused bad bugs in elasticsearch 1.x, which is why it is not supported  anymore as of version 2.0 and the only fix is to reindex your data in different indices.
</comment><comment author="maksutspahi" created="2015-12-21T12:32:23Z" id="166294728">Hello, I could not figure out how to make this work with java client api in 2.1.1 when using local node which I am using for integration tests. 
My settings:

```
Settings settings = Settings.put("plugin.types", MapperMurmur3Plugin.class.getName())
```

With the local node:

```
Node node = nodeBuilder()
                .settings(settings)
                .local(true).node();
```

While index creation it gives me :

```
Caused by: org.elasticsearch.index.mapper.MapperParsingException: no handler for type [murmur3] declared on field [hash]
    at org.elasticsearch.index.mapper.core.TypeParsers.parseMultiField(TypeParsers.java:362) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.core.StringFieldMapper$TypeParser.parse(StringFieldMapper.java:203) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:310) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:223) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:139) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:140) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:121) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:391) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:265) ~[elasticsearch-2.1.1.jar:2.1.1]
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:378) ~[elasticsearch-2.1.1.jar:2.1.1]
```

My mapping is same as I posted in the first comment.

This error does not happen in 2.0.0 client. Is it related with this issue or am I not registering the plugin in the correct way ?
</comment><comment author="dadoonet" created="2015-12-21T13:25:48Z" id="166305093">@maksutspahi You can not start a plugin like this from 2.1.
Read also this: https://discuss.elastic.co/t/es-plugin-add-to-node-server-programmatically/37616/5
</comment><comment author="maksutspahi" created="2015-12-22T07:07:52Z" id="166535987">Thank you that worked. Also this bug seems fixed in 2.1.1 thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds exception objects to log messages.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14827</link><project id="" key="" /><description>Also forwards exception objects through failure listeners eliminates another 17
calls to getMessage().

Relates to #10021
</description><key id="117564267">14827</key><summary>Adds exception objects to log messages.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T11:06:41Z</created><updated>2015-11-25T09:54:13Z</updated><resolved>2015-11-25T09:42:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-18T19:30:47Z" id="157834020">This is great. These errors should be a little easier to debug now! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0 does not handle -Des.config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14826</link><project id="" key="" /><description>We used to run mutiple instances of elasticsearch on the same machine. Our startup script would look like

```
-Des.config=/data/es_instance1/conf/elasticsearch.yml
```

This is the error message

```
[2015-11-18 12:00:14,913][INFO ][bootstrap ] es.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed. 
```

Is this a known problem? I do not want to install elasticsearch binaries mutiple times on a single machine...
</description><key id="117563986">14826</key><summary>2.0 does not handle -Des.config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">runningman84</reporter><labels /><created>2015-11-18T11:04:56Z</created><updated>2015-12-15T19:57:34Z</updated><resolved>2015-11-18T11:09:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-18T11:09:25Z" id="157680684">It's a known feature. Not a known problem ;)

You can define a `path.conf` which points to a dir containing an `elasticsearch.yml` file. 
</comment><comment author="electrical" created="2015-11-18T11:13:06Z" id="157681355">@runningman84 With the ES puppet module we handle this by creating a directory per instance.
via `-Des.path.conf` you can set the path to the config directory.
</comment><comment author="bstascavage" created="2015-12-11T18:01:02Z" id="164004563">What a horrible step backwards.  I don't understand why, in 2015, you'd go from being able to specify a config file to specifying a directory that HAS to have a config file named a certain way.  

This broke our deployment and is a ridiculous omission
</comment><comment author="Jestar342" created="2015-12-15T19:57:34Z" id="164878456">As above. This is a total regression.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Visualization - Mouse over details</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14825</link><project id="" key="" /><description>Hello all,

I've been wondering if it is possible to change the mouseover details on for example a vertical barchart?
At the moment during a mouseover on a bar, it shows "filter" and "@timestamp" which is not very useful.
A combination of a custom field "status" or combination of a "@timestamp" + "status" + "host" is the result i'm looking for.
</description><key id="117556753">14825</key><summary>Visualization - Mouse over details</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">holeunlee</reporter><labels /><created>2015-11-18T10:21:05Z</created><updated>2015-11-18T10:57:56Z</updated><resolved>2015-11-18T10:57:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-18T10:57:56Z" id="157678674">Please ask questions on discuss.elastic.co and if you need to ask for a feature request about Kibana, please open it in Kibana project.

This repository is about elasticsearch only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Potential issue when testing plugins using ElasticsearchIntegrationTest?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14824</link><project id="" key="" /><description>Hi,

I've come across a potential issue when writing an integration test for one of our plugins using the ElasticsearchIntegrationTest class. This is in version 1.7.3.

When a plugin tries to make a request to Elasticsearch, it will receive an error due to the assert in org.elasticsearch.transport.Transports which is checking if the current thread is a transport thread.

I have created a github repo which demonstrates the issue: https://github.com/jaranflaath/es-plugin-integrationtest

Running the PluginTest will reproduce the situation.

I am not certain if this is an issue in Elasticsearch, or an issue with the way we try to test our plugin, so any input is welcome.

Thanks!
</description><key id="117535131">14824</key><summary>Potential issue when testing plugins using ElasticsearchIntegrationTest?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaranflaath</reporter><labels><label>:Network</label></labels><created>2015-11-18T08:03:44Z</created><updated>2015-11-19T05:33:26Z</updated><resolved>2015-11-19T02:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaranflaath" created="2015-11-18T10:17:49Z" id="157669504">After some more digging and discussions on Freenode, I have now found a solution and provided an example in the "solution" branch in the example repository: https://github.com/jaranflaath/es-plugin-integrationtest/tree/solution

However, I would love some feedback on what is the proper way to do this. 
</comment><comment author="jasontedor" created="2015-11-19T02:08:33Z" id="157924599">&gt; it will receive an error due to the assert in org.elasticsearch.transport.Transports which is checking if the current thread is a transport thread

This is by design. In your [code](https://github.com/jaranflaath/es-plugin-integrationtest/commit/89b755fcff38b4756dfba0b4d439e9ab8d5bce3c#diff-f70cd741ee6d7246193213cc8db54748R31), you're sending a secondary request on the thread that is handling the initial request to the endpoint `/cluster_status`. The handling of that initial request happens on a network thread, and by sending the secondary request on that thread you'd be blocking a network thread until a response comes back. Blocking network threads is bad and we go to great lengths to prevent it.

Your solution "works" because you got the [secondary request off of the network thread](https://github.com/jaranflaath/es-plugin-integrationtest/blob/b85452b121b1e0086abcf26071e3c904a374e8b4/src/main/java/ske/skatt/elasticsearch/PluginRestEndpoint.java#L34), but it has the problem that it is surreptitiously blocking the network thread by [busy waiting for a response](https://github.com/jaranflaath/es-plugin-integrationtest/blob/b85452b121b1e0086abcf26071e3c904a374e8b4/src/main/java/ske/skatt/elasticsearch/PluginRestEndpoint.java#L46-L48).

&gt; However, I would love some feedback on what is the proper way to do this.

The preferred way to handle situations like this is to attach a listener to the secondary request. You'll want to do something like this:

``` java
@Override
public void handleRequest(final RestRequest request, final RestChannel channel, Client client) {
  client.admin().cluster().prepareHealth().execute(new RestBuilderListener&lt;ClusterHealthResponse&gt;(channel) {
    @Override
    public RestResponse buildResponse(ClusterHealthResponse clusterIndexHealths, XContentBuilder xContentBuilder) throws Exception {
      return new BytesRestResponse(OK, clusterIndexHealths.getStatus().name());
    }
  });
}
```

This gets your secondary request off of the network thread and does not block the network thread waiting for a response to that request.
</comment><comment author="jaranflaath" created="2015-11-19T05:33:26Z" id="157956897">Thanks a lot for your feedback and suggested solution. Will give it a try :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field stats: Added `format` option for index constraints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14823</link><project id="" key="" /><description>Added a `format` for index constraints on date fields to allow dates of a different format to be specified than is defined in the date field's format setting in the mapping.

PR for #14804
</description><key id="117528840">14823</key><summary>Field stats: Added `format` option for index constraints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T07:12:02Z</created><updated>2015-11-18T13:25:17Z</updated><resolved>2015-11-18T13:25:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-18T09:01:43Z" id="157649227">@rjernst @bleskes I've updated the PR and removed the hardcoded fallback logic with an optional `format` option that can be specified in the field stats api.
</comment><comment author="bleskes" created="2015-11-18T09:54:13Z" id="157661524">Thx @martijnvg . Left some minor comments
</comment><comment author="martijnvg" created="2015-11-18T11:24:32Z" id="157683239">@boaz I've updated the PR based on your comments.
</comment><comment author="bleskes" created="2015-11-18T12:24:49Z" id="157694503">LGTM. Left some minor comments. No need for another review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch java class path for 1.7.1 uses 0.90.5 jar path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14822</link><project id="" key="" /><description>this is incorrect

```
# ps -aef | grep -i elasticsearch | grep -i --color 0.90.5
.... /usr/java/jdk1.7.0_55/bin/java..... -Delasticsearch -Des.foreground=yes -Des.path.home=/opt/elasticsearch/1.7.1 -cp :/opt/elasticsearch/1.7.1/lib/elasticsearch-0.90.5.jar:/opt/elasticsearch/1.7.1/lib/*:/opt/elasticsearch/1.7.1/lib/sigar/* -Des.config=/srv/etc/elasticsearch/elasticsearch.yml org.elasticsearch.bootstrap.Elasticsearch
```

looks like 0.90.5.jar is hard coded?
</description><key id="117527948">14822</key><summary>elasticsearch java class path for 1.7.1 uses 0.90.5 jar path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mnikhil-git</reporter><labels /><created>2015-11-18T07:03:59Z</created><updated>2015-11-18T10:04:11Z</updated><resolved>2015-11-18T10:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-18T09:44:45Z" id="157658734">It sounds like your update is wrong and that you have old jars remaining in `/opt/elasticsearch/1.7.1/lib/` dir.

Check for it.
</comment><comment author="mnikhil-git" created="2015-11-18T10:04:11Z" id="157664090">got it, it was because of elasticsearch.in.sh was from old installation was stil lurking around.. I had to reinstall the package to get it removed and it is working fine. thanks for the attention. this can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>allow to define own JAVA_OPTS to use another garbage collector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14821</link><project id="" key="" /><description>I would like to use a different garbage collector other than CMS which is enforced in elasticsearch script. current script does not let me, please update the script so there is a flexibility at the user side to do that.

JAVA_OPTS should not be updated with in the script unless the user wants.

thanks.
</description><key id="117527698">14821</key><summary>allow to define own JAVA_OPTS to use another garbage collector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mnikhil-git</reporter><labels /><created>2015-11-18T07:01:20Z</created><updated>2015-11-18T18:16:50Z</updated><resolved>2015-11-18T18:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T18:16:50Z" id="157807378">See https://github.com/elastic/elasticsearch/issues/5823#issuecomment-59782959
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>script_fields with filtered query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14820</link><project id="" key="" /><description>I already try to put script_fields inside filtered, inside query inside filtered, nothing works an i m not found any quote of how use the 2 features together...

```{
  "fields": [
    "_source"
  ],
  "script_fields": [
    {
      "status": {
        "script": "return 1;"
      }
    }
  ],
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "must": [
            {
              "term": {
                "seller_id": 1
              }
            }
          ],
          "must_not": []
        }
      },
      "filter": {
        "script": {
          "script": "if(_source.concurrents.findAll{ it.new_price &lt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size == 0 &amp;&amp; _source.concurrents.findAll{ it.new_price &gt; _source.new_price  &amp;&amp; concurrent_ids.contains(it.seller_id) }.size &gt; 0 ){ return true; }",
          "params": {
            "concurrent_ids": [
              2,
              3
            ]
          }
        }
      }
    }
  },
  "size": 500,
  "from": 0
}

```

the error message is :
{
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[II2sctcxR2eIZdEBEoBj-g][4ninjas_products_test][0]: SearchParseException[[4ninjas_products_test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"fields":["_source"],"script_fields":[{"status":{"script":"return 1;"}}],"query":{"filtered":{"query":{"bool":{"must":[{"term":{"seller_id":1}}],"must_not":[]}},"filter":{"script":{"script":"if(_source.concurrents.findAll{ it.new_price &lt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size == 0 &amp;&amp; _source.concurrents.findAll{ it.new_price &gt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size &gt; 0 ){ return true; }","params":{"concurrent_ids":[2,3]}}}}},"size":500,"from":0}]]]; nested: NullPointerException; }{[II2sctcxR2eIZdEBEoBj-g][4ninjas_products_test][1]: SearchParseException[[4ninjas_products_test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"fields":["_source"],"script_fields":[{"status":{"script":"return 1;"}}],"query":{"filtered":{"query":{"bool":{"must":[{"term":{"seller_id":1}}],"must_not":[]}},"filter":{"script":{"script":"if(_source.concurrents.findAll{ it.new_price &lt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size == 0 &amp;&amp; _source.concurrents.findAll{ it.new_price &gt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size &gt; 0 ){ return true; }","params":{"concurrent_ids":[2,3]}}}}},"size":500,"from":0}]]]; nested: NullPointerException; }{[II2sctcxR2eIZdEBEoBj-g][4ninjas_products_test][2]: SearchParseException[[4ninjas_products_test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"fields":["_source"],"script_fields":[{"status":{"script":"return 1;"}}],"query":{"filtered":{"query":{"bool":{"must":[{"term":{"seller_id":1}}],"must_not":[]}},"filter":{"script":{"script":"if(_source.concurrents.findAll{ it.new_price &lt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size == 0 &amp;&amp; _source.concurrents.findAll{ it.new_price &gt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size &gt; 0 ){ return true; }","params":{"concurrent_ids":[2,3]}}}}},"size":500,"from":0}]]]; nested: NullPointerException; }{[II2sctcxR2eIZdEBEoBj-g][4ninjas_products_test][3]: SearchParseException[[4ninjas_products_test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"fields":["_source"],"script_fields":[{"status":{"script":"return 1;"}}],"query":{"filtered":{"query":{"bool":{"must":[{"term":{"seller_id":1}}],"must_not":[]}},"filter":{"script":{"script":"if(_source.concurrents.findAll{ it.new_price &lt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size == 0 &amp;&amp; _source.concurrents.findAll{ it.new_price &gt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size &gt; 0 ){ return true; }","params":{"concurrent_ids":[2,3]}}}}},"size":500,"from":0}]]]; nested: NullPointerException; }{[II2sctcxR2eIZdEBEoBj-g][4ninjas_products_test][4]: SearchParseException[[4ninjas_products_test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"fields":["_source"],"script_fields":[{"status":{"script":"return 1;"}}],"query":{"filtered":{"query":{"bool":{"must":[{"term":{"seller_id":1}}],"must_not":[]}},"filter":{"script":{"script":"if(_source.concurrents.findAll{ it.new_price &lt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size == 0 &amp;&amp; _source.concurrents.findAll{ it.new_price &gt; _source.new_price &amp;&amp; concurrent_ids.contains(it.seller_id) }.size &gt; 0 ){ return true; }","params":{"concurrent_ids":[2,3]}}}}},"size":500,"from":0}]]]; nested: NullPointerException; }]",
"status": 400
}
```
</description><key id="117517117">14820</key><summary>script_fields with filtered query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luizamboni</reporter><labels /><created>2015-11-18T05:41:08Z</created><updated>2015-11-18T18:10:12Z</updated><resolved>2015-11-18T18:10:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="luizamboni" created="2015-11-18T05:43:06Z" id="157610197">is more readable here
https://gist.github.com/luizamboni/aac0dda810eebeb44fd0
</comment><comment author="clintongormley" created="2015-11-18T18:10:11Z" id="157805676">Hi @luizamboni 

Please ask questions like these in the forums instead: http://discuss.elastic.co/

I'd also advise checking the documentation - script fields need to be a named hash, not an array
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>when upgrade es from v1.1 to v1.7 . The has_parent  include prefix  Query  slow  200ms&#12290;What is the cause of the problem&#65311;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14819</link><project id="" key="" /><description>shards: 10 \* 2 | docs: 71,991,873 | size: 54.95GB

query:

  "filter" : {
      "has_parent" : {
        "filter" : {
          "prefix" : {
            "treecode" : "1#55#57#58#682#jieying.cheng@"
          }
        },
        "parent_type" : "treecode"
      }
    }
</description><key id="117507550">14819</key><summary>when upgrade es from v1.1 to v1.7 . The has_parent  include prefix  Query  slow  200ms&#12290;What is the cause of the problem&#65311;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">z-star</reporter><labels /><created>2015-11-18T03:43:33Z</created><updated>2015-11-18T18:07:29Z</updated><resolved>2015-11-18T18:07:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T18:07:29Z" id="157804990">Hi @z-star 

Parent-child has been completely rewritten in 2.0, so I would suggest trying it out there instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent result from _cat/aliases vs. /_aliases for closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14818</link><project id="" key="" /><description>```
PUT /index1foralias
PUT /index2foralias
POST /_aliases
{
    "actions" : [
        { "add" : { "index" : "index1foralias", "alias" : "testalias" } },
        { "add" : { "index" : "index2foralias", "alias" : "testalias" } }
    ]
}
GET /_cat/aliases?v
GET /_aliases
POST /index1foralias/_close
GET /_cat/aliases?v
GET /_aliases
```

When using the  _cat/aliases API it does not show indices associated to an alias that are closed.  

```
alias     index          filter routing.index routing.search 
testalias index2foralias -      -             -              
```

When using the _aliases API, it shows all indices associated to an alias regardless of their open/closed status

```
   "index1foralias": {
      "aliases": {
         "testalias": {}
      }
   }
   "index2foralias": {
      "aliases": {
         "testalias": {}
      }
   }
```
</description><key id="117505677">14818</key><summary>Inconsistent result from _cat/aliases vs. /_aliases for closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:CAT API</label></labels><created>2015-11-18T03:26:11Z</created><updated>2015-11-18T03:33:03Z</updated><resolved>2015-11-18T03:33:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-11-18T03:33:03Z" id="157594356">Actually previous test was on ES 1.7.  Just tested ES 2.0, both _aliases and _cat/aliases API are now returning just the open indices (and are consistent).  Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix RecoveryBackwardsCompatibilityIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14817</link><project id="" key="" /><description>It had drifted from its twin integration test, RecoveryFromGatewayIT. This
makes them shard 90% of their code so they can't drift.

Related to #13522
</description><key id="117488938">14817</key><summary>Fix RecoveryBackwardsCompatibilityIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label></labels><created>2015-11-18T01:13:27Z</created><updated>2015-11-20T18:33:31Z</updated><resolved>2015-11-20T18:00:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-18T13:54:24Z" id="157719419">@rjernst, I've moved the shared logic into its own class.
</comment><comment author="rjernst" created="2015-11-19T18:42:35Z" id="158151427">Ok, LGTM. I would personally name the class something different because TestCase to me implies it is a subclass of ESTestCase, but I also don't know what moniker to use...
</comment><comment author="nik9000" created="2015-11-20T18:00:22Z" id="158475920">&gt; Ok, LGTM. I would personally name the class something different because TestCase to me implies it is a subclass of ESTestCase, but I also don't know what moniker to use...

Cool. I renamed it to `ReusePeerRecoverySharedTest`.
</comment><comment author="nik9000" created="2015-11-20T18:01:31Z" id="158476421">Merged to 2.x.
</comment><comment author="nik9000" created="2015-11-20T18:33:31Z" id="158484603">And cherry-picked to 2.2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtered query using elasticsearch API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14816</link><project id="" key="" /><description>How to write the following filtered query using elasticsearch java api?

{
  "query": { 
    "bool": { 
      "must": [
        { "match": { "title":   "Search"        }}, 
        { "match": { "content": "Elasticsearch" }}  
      ],
      "filter": [ 
        { "term":  { "status": "published" }}, 
        { "range": { "publish_date": { "gte": "2015-01-01" }}} 
      ]
    }
  }
}
</description><key id="117486656">14816</key><summary>Filtered query using elasticsearch API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malinir123</reporter><labels /><created>2015-11-18T00:53:22Z</created><updated>2015-11-18T01:16:41Z</updated><resolved>2015-11-18T01:16:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-18T01:16:41Z" id="157566134">Please ask questions like this on http://discuss.elastic.co. We use github for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add --debug-jvm option to run and integTest tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14815</link><project id="" key="" /><description>Sometimes when running elasticsearch, it is useful to attach a remote
debugger. This change adds a --debug-jvm option (the same name gradle
uses for its tests debug option), which adds java agent config for a
remote debugger. The configuration is set to hava java suspend until the
remove debugger is attached.

closes #14772
</description><key id="117482379">14815</key><summary>Add --debug-jvm option to run and integTest tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T00:23:10Z</created><updated>2015-11-18T01:19:27Z</updated><resolved>2015-11-18T01:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-18T01:12:05Z" id="157565473">Left a note about the message printed but otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added release notes topic.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14814</link><project id="" key="" /><description>Created a Release Notes topic in the Elasticsearch reference. Putting the release notes in the docs will make it easier for users to find them and make them easier to maintain.
</description><key id="117480683">14814</key><summary>Added release notes topic.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">debadair</reporter><labels><label>docs</label><label>review</label></labels><created>2015-11-18T00:08:29Z</created><updated>2015-11-21T13:04:23Z</updated><resolved>2015-11-21T13:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="debadair" created="2015-11-18T03:20:30Z" id="157591648">Hold off reviewing/commenting--adding the release notes for the pre-GA versions.
</comment><comment author="clintongormley" created="2015-11-21T13:04:23Z" id="158636650">Release notes added to 2.\* docs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix missed comma in bool query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14813</link><project id="" key="" /><description>Seems that this mistake appear in 2.0 reference and higher. https://www.elastic.co/guide/en/elasticsearch/reference/2.0/query-dsl-bool-query.html 
</description><key id="117480476">14813</key><summary>Fix missed comma in bool query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gkopylov</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-18T00:06:46Z</created><updated>2015-11-18T00:32:33Z</updated><resolved>2015-11-18T00:32:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gkopylov" created="2015-11-18T00:27:29Z" id="157554068">I have signed CLA.
</comment><comment author="jasontedor" created="2015-11-18T00:32:15Z" id="157554807">LGTM. Thank you for noticing and fixing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add unit test for LinkedHashMap serialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14812</link><project id="" key="" /><description>This commit adds a unit test for LinkedHashMap serialization that tests
that the method of serialization writes the entries in the LinkedHashMap
in iteration order and that the reconstructed LinkedHashMap preserves
that order. This test is randomized and tests iteration order is
preserved whether the LinkedHashMap is ordered by insertion order or
access order.

Closes #14743
</description><key id="117477300">14812</key><summary>Add unit test for LinkedHashMap serialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T23:42:05Z</created><updated>2015-11-18T01:24:50Z</updated><resolved>2015-11-18T01:24:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-18T00:56:02Z" id="157561897">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unknown search element [common]: Common Terms Query not working?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14811</link><project id="" key="" /><description>I find myself unable to run a common terms query in the terms explained [in the most recent documentation for v2.0](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-common-terms-query.html).

Every time I try to run something along the lines of `curl -XGET 'http://localhost:9201/INDEX_NAME/_search' -d '{"common": { "FIELD_NAME": {"query": "SEARCH_TERMS", "cutoff_frequency": 0.001 }}}'`, which pretty much replicates the examples in the docs, I encounter the following error:

``` json
{
  "error": {
    "root_cause": [ {
      "type": "search_parse_exception",
      "reason": "failed to parse search source. unknown search element [common]",
      "line": 1,
      "col": 2}],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [ {
      "shard": 0,
      "index":"INDEX_NAME",
      "node": "b76-kVgxTWqwJTlBBba6xA",
      "reason": {
        "type": "search_parse_exception",
        "reason": "failed to parse search source. unknown search element [common]",
        "line": 1,
        "col": 2
      }
    } ]
  },
  "status": 400
}%
```

Am I missing something or is ES saying that "common" searches are not recognized?
</description><key id="117451273">14811</key><summary>unknown search element [common]: Common Terms Query not working?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">catalandres</reporter><labels /><created>2015-11-17T21:10:29Z</created><updated>2017-06-01T10:34:18Z</updated><resolved>2015-11-17T21:46:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-17T21:46:08Z" id="157519105">Questions like this are better asked on http://discuss.elastic.co.

Your query is malformed. The query must be under a "query" element.

```
curl -XGET 'http://localhost:9201/INDEX_NAME/_search' -d '{"query": {"common": { "FIELD_NAME": {"query": "SEARCH_TERMS", "cutoff_frequency": 0.001 }}}}'
```
</comment><comment author="louzadod" created="2017-05-31T13:06:15Z" id="305180544">The bug still happens.
I'm using:
- Grafana 4.3.2
- ElasticSearch 2.3.3
- Linux 64 bits #</comment><comment author="clintongormley" created="2017-06-01T10:34:18Z" id="305455576">It's not a bug.  The search body is missing the `query` parameter</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add CONTAINS relation to geo_shape query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14810</link><project id="" key="" /><description>At the time of `geo_shape` query conception, `CONTAINS` was not yet a supported spatial operation in Lucene. Since it is now available this PR adds `ShapeRelation.CONTAINS` to `GeoShapeQuery`. Randomized testing is included and documentation is updated.

closes #14713
Backport closes #17866
</description><key id="117447133">14810</key><summary>Add CONTAINS relation to geo_shape query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>feature</label><label>v2.2.0</label><label>v2.3.3</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T20:50:55Z</created><updated>2016-05-18T07:37:52Z</updated><resolved>2015-11-18T20:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-18T19:10:31Z" id="157825629">LGTM, thanks for the move to single node test!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IntelliJ - Gradle Integration broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14809</link><project id="" key="" /><description>I'm using the [Gradle IntelliJ integration](https://www.jetbrains.com/idea/help/working-with-gradle-projects.html) for Elasticsearch. When I refresh the linked Gradle projects in the Gradle Tool window, the folder `.idea` gets deleted (with all my settings) and I end up in an unrecoverable state (need to completely reimport the whole project from scratch with all my settings gone).

I see that as part of commit f327beac49a7a5097dad64adcb3e43c03980f122 the following lines were introduced:

```
tasks.cleanIdea {
    delete '.idea'
}
```

Can we undo this?
</description><key id="117437319">14809</key><summary>IntelliJ - Gradle Integration broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>build</label></labels><created>2015-11-17T20:00:14Z</created><updated>2015-11-19T12:45:39Z</updated><resolved>2015-11-19T12:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-17T20:10:51Z" id="157491595">Ha thanks! It happened to me today as well. I thought I did something stupid. :)
</comment><comment author="javanna" created="2015-11-19T10:59:46Z" id="158023122">I have the same problem and end up needing to reimport the project from scratch every day...
</comment><comment author="rjernst" created="2015-11-19T11:03:31Z" id="158023808">Please remove the lines Yannick mentioned above (or I can do it in the morning). I didnt realize it woukd have this impact. I will find another way to address the issue I was having. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify delayed shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14808</link><project id="" key="" /><description>This PR simplifies delayed shard allocation by moving the calculation of the delay to a single place (ReplicaShardAllocator) and removing the bridge between RoutingService and GatewayAllocator. A consequence of the simplification is that the delay can be slightly less accurate.
</description><key id="117429027">14808</key><summary>Simplify delayed shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T19:22:04Z</created><updated>2015-11-27T16:34:22Z</updated><resolved>2015-11-19T08:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-17T21:37:04Z" id="157515883">I like the decoupling in this!

One thing that is concerning though, is the APIs in `UnassignedInfo`, there are two `getTimestamp` methods, I think we should try to be as explicit as possible by putting both unit and purpose in the name (ie, `getDelayedTimestampNanos`, `getUnassignedTimestampMillis`), what do you think @ywelsch?
</comment><comment author="ywelsch" created="2015-11-18T11:25:53Z" id="157683478">Updated PR based on review comments. Naming is hard ;-)
</comment><comment author="bleskes" created="2015-11-18T13:11:33Z" id="157707889">Thanks @ywelsch . left some more comments.
</comment><comment author="ywelsch" created="2015-11-18T14:58:08Z" id="157739871">Pushed another set of changes.
</comment><comment author="bleskes" created="2015-11-18T16:15:49Z" id="157764617">Left some minor comments. O.w. LGTM. @dakrone can you take a look as well?
</comment><comment author="dakrone" created="2015-11-18T16:37:31Z" id="157770887">Left a minor comment and echoed one of Boaz's minor comments, other than that, LGTM also. Thanks for changing the naming, it's easier to read now.
</comment><comment author="ywelsch" created="2015-11-18T17:41:31Z" id="157794469">thanks @bleskes, @dakrone and @jasontedor for the comments. I pushed once more and will merge this in tomorrow.
</comment><comment author="bleskes" created="2015-11-19T07:32:36Z" id="157976418">Thx @ywelsch . Can we give it a couple of days under CI and push to 2.2 as well?
</comment><comment author="clintongormley" created="2015-11-19T12:31:33Z" id="158044414">Removing the 2.2 label until this PR is merged into 2.x
</comment><comment author="ywelsch" created="2015-11-27T16:34:22Z" id="160171577">backported to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel not working after changing cluster.name to sth. else than elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14807</link><project id="" key="" /><description>Marvel worked fine when the cluster was name elasticsearch. No I changed it and things went wrong. I have uninstalled marvel and licence and reinstalled with no success. I see there is a marvel index created in the indices directory:

root@moose:/var/lib/elasticsearch/hirschewiewir/nodes/0/indices# du -hs .marvel-es-*
16K     .marvel-es-2015.11.17
16K     .marvel-es-data

but the shareds are unassigned:
.marvel-es-data       0 p UNASSIGNED  
.marvel-es-data       0 r UNASSIGNED  

when I start elasticsearch i see in the logfile:
[2015-11-17 19:31:57,216][INFO ][plugins                  ] [moose] loaded [license, marvel], sites []

when I try to access the marvel plugin within kibana i get the following error message in the elasticsearch logfile: 

[2015-11-17 19:37:21,955][DEBUG][action.search.type       ] [moose] All shards failed for phase: [query_fetch]                                                                                                              [59/87]
[.marvel-es-data][[.marvel-es-data][0]] NoShardAvailableActionException[null]
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:151)
        at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.doExecute(TransportSearchQueryAndFetchAction.java:58)
        at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction.doExecute(TransportSearchQueryAndFetchAction.java:47)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:103)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:44)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
        at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
        at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
        at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:570)
        at org.elasticsearch.rest.action.search.RestSearchAction.handleRequest(RestSearchAction.java:84)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:348)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</description><key id="117422032">14807</key><summary>Marvel not working after changing cluster.name to sth. else than elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdiehm</reporter><labels><label>feedback_needed</label></labels><created>2015-11-17T18:45:22Z</created><updated>2016-01-18T10:49:10Z</updated><resolved>2016-01-18T10:49:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johnarnold" created="2015-11-17T20:59:55Z" id="157505801">I have the same symptoms, without changing cluster name.  See:
https://github.com/elastic/elasticsearch/issues/14789
</comment><comment author="rabeetwaqar1" created="2015-11-18T09:41:00Z" id="157658005">I have the same issue, But i'm also facing this while setting the cluster name to : elasticsearch
</comment><comment author="clintongormley" created="2015-11-18T18:25:12Z" id="157810813">Could you try using the [cluster reroute API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html)  with the `explain` query string param, to assign the primary marvel shard. It will tell you why the shard can't be assigned.
</comment><comment author="johnarnold" created="2015-11-18T23:01:32Z" id="157892737">I tried it on my cluster, got an error like this:

{
  "error" : {
    "root_cause" : [ {
      "type" : "remote_transport_exception",
      "reason" : "[flowmaster3][10.10.10.6:9300][cluster:admin/reroute]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "[allocate] trying to allocate a primary shard [.marvel-es-2015.11.18][0], which is disabled"
  },
  "status" : 400
}
</comment><comment author="johnarnold" created="2015-11-18T23:14:36Z" id="157895255">I figured out that allocating the primary replica to a specific node works to assign it.

curl -XPOST 'localhost:9200/_cluster/reroute?pretty' -d '{
    "commands" : [ 
        {
          "allocate" : {
              "index" : ".marvel-es-data", "shard" : 0, "node" : "flowdata1", "allow_primary" : true
          }
        }
    ]
}'

I had to do this for each marvel shard in: curl -XGET http://localhost:9200/_cat/shards | grep marvel
</comment><comment author="clintongormley" created="2015-11-19T11:43:59Z" id="158032329">@johnarnold i specifically didn't want you to force the primary.  instead I wanted to find out why the primary shard wasn't being allocated.  Nothing in the older logs giving a reason?
</comment><comment author="johnarnold" created="2015-11-21T05:15:06Z" id="158591170">I managed to repro shard allocation problem.

[explain.txt](https://github.com/elastic/elasticsearch/files/40744/explain.txt)
</comment><comment author="clintongormley" created="2015-11-28T14:45:49Z" id="160306688">OK so this is the interesting bit:

```
    ".marvel-es-2015.11.15" : {
      "shards" : {
        "0" : [ {
          "state" : "UNASSIGNED",
          "primary" : true,
          "node" : null,
          "relocating_node" : null,
          "shard" : 0,
          "index" : ".marvel-es-2015.11.15",
          "version" : 2,
          "unassigned_info" : {
            "reason" : "NODE_LEFT",
            "at" : "2015-11-21T04:50:34.073Z",
            "details" : "node_left[qKKxT9bPQeiVyioqNuhWlA]"
          }
        }, {
          "state" : "UNASSIGNED",
          "primary" : false,
          "node" : null,
          "relocating_node" : null,
          "shard" : 0,
          "index" : ".marvel-es-2015.11.15",
          "version" : 2,
          "unassigned_info" : {
            "reason" : "INDEX_CREATED",
            "at" : "2015-11-21T04:31:31.548Z"
          }
        } ]
      }
    },
```

The primary was assigned to a node which has since left, and the replica was never assigned.  Are you using shard allocation settings?
</comment><comment author="clintongormley" created="2016-01-18T10:49:10Z" id="172495694">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Supplier for StreamInput#readOptionalStreamable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14806</link><project id="" key="" /><description>This commit changes the signature of StreamInput#readOptionalStreamable
to accept a Supplier to create new streamables rather than requiring
callers to construct new instances. This has the advantage of avoiding
an allocation in cases when the stream indicates the resulting
streamable is null
</description><key id="117410716">14806</key><summary>Use Supplier for StreamInput#readOptionalStreamable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T17:47:53Z</created><updated>2015-11-17T18:41:53Z</updated><resolved>2015-11-17T18:41:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-17T18:01:00Z" id="157454009">LGTM
</comment><comment author="jasontedor" created="2015-11-17T18:41:53Z" id="157465734">Thanks for reviewing @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES gets stuck throwing thousands of errors per second</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14805</link><project id="" key="" /><description>We're having an issue where a bad query is sending ES into an endless error loop of ~5,000 errors being logged per second. This is with ES 1.7.2 on a 3 node cluster.

The error that triggers it can be any legit error (this is from a popular web service and our user input can be all kinds of crap &#128521;), but once it gets one of these in a loop, it's the same error over and over and the only way to stop them is to restart each node. And just for clarity, when I say it logs over and over, these are no longer requests coming into ES. It's ES doing this all on its own internally.

Here's examples: [es_errors.txt](https://github.com/elastic/elasticsearch/files/36942/es_errors.txt)

![log_search](https://cloud.githubusercontent.com/assets/24766/11218447/4eb5d158-8d12-11e5-8b4a-6caafc9b5ce8.png)

Have you guys ever seen this before?
</description><key id="117405790">14805</key><summary>ES gets stuck throwing thousands of errors per second</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">travisbell</reporter><labels /><created>2015-11-17T17:25:15Z</created><updated>2016-02-14T17:07:35Z</updated><resolved>2016-02-14T17:07:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-17T18:09:26Z" id="157456143">&gt; It's ES doing this all on its own internally.

Sure! For lots of things you'll see one copy of the error per shard. I've also seen this when I had my retry logic on my appserver too permissive.

Are you _sure_ it Elasticsearch making echo errors? Can you try running the same query with the same mappings on a local index with curl?
</comment><comment author="travisbell" created="2015-11-17T18:29:13Z" id="157461034">Hey @nik9000!

&gt; Are you sure it Elasticsearch making echo errors?

I am pretty sure. There is no spike in app server requests, or in traffic between the app servers and ES nodes.

We have never been able to replicate this problem locally, only under a real production load. And even then, it will be fine for 12 hours (with some errors here and there no big deal) and then all of a sudden she fires up (like in the graph above) and we see CPU usage spike. Restart nodes, rinse wash and repeat.
</comment><comment author="clintongormley" created="2016-02-14T17:07:35Z" id="183928359">Hi @travisbell 

I see no further info here, but looking at the error logs, it appears they are not the same errors but lots of different malformed queries/requests.  It looks like some misbehaving bot is targeting your site.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_field_stats does not accept epoch_millis for date fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14804</link><project id="" key="" /><description>It looks like `/_field_stats` does not accept milliseconds since epoch for gte/lte unless it is explicitly mapped as such. This isn't the case with say, range filters, which accept `format: "epoch_millis"`. This is super important to Kibana as there's no direct mapping from JODA formats to javascript date formatting libraries, so we've always sent everything as epoch. By default date fields are mapped to accept epoch as a format, but if the user uses a custom format, this won't happen.

We didn't notice this bug until Marvel which sets an explicit mapping format.
</description><key id="117402692">14804</key><summary>/_field_stats does not accept epoch_millis for date fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rashidkpc</reporter><labels><label>:Dates</label><label>blocker</label><label>bug</label><label>v2.0.1</label></labels><created>2015-11-17T17:12:42Z</created><updated>2015-11-18T13:25:17Z</updated><resolved>2015-11-18T13:25:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Data class improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14803</link><project id="" key="" /><description>- Better error messages and more test coverage in DataTests
- Better test isolation (tests don't end up using getProperty all the time)
- Removed core dependency from getProperty, shared code with containsProperty instead
- Made getProperty type safe
</description><key id="117395279">14803</key><summary>Data class improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-17T16:42:09Z</created><updated>2015-11-18T09:46:35Z</updated><resolved>2015-11-18T09:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-17T16:43:35Z" id="157426703">@talevy @martijnvg can you have a look please?
</comment><comment author="martijnvg" created="2015-11-18T09:23:06Z" id="157653318">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_histogram interval uses xM as xm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14802</link><project id="" key="" /><description>The `date_histogram` interval is parsing `"3M"` as though it were minutes (e.g., `"3m"`). This leads to aggregations that should be invalid being accepted that create a massive number of buckets given the wrong data set.

An example aggregation that has a painfully large response (5 months created a ~700K lines of pretty printed JSON with only 4 documents in the index):

``` json
{
  "aggregations": {
    "byMonth": {
      "date_histogram": {
        "field": "begintime",
        "interval": "3M"
      },
      "aggregations": {
        "attrs": {
          "terms": {
            "field": "primaryattribution"
          },
          "aggregations": {
            "avg_by_attrs": {
              "avg": {
                "field": "pageviews"
              }
            }
          }
        }
      }
    }
  }
}
```

As an important note, `"1M"` appropriately gets interpreted as `"month"`.
</description><key id="117386535">14802</key><summary>date_histogram interval uses xM as xm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-17T16:00:51Z</created><updated>2016-08-03T07:51:55Z</updated><resolved>2016-08-03T07:51:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-11-23T05:38:38Z" id="158857175">We only support arbitrary values for time units up to weeks in the TimeValue class. `date_histogram` used it to parse the time interval. And in 2.0, when parsing timevalue, we made it case insensitive ( https://github.com/elastic/elasticsearch/pull/11437), so now it accepts `M` as minute.

We may keep it as case insensitive, change the time unit Moth from `M` to `mon` or something clearer to user. @clintongormley  WDYT 
</comment><comment author="colings86" created="2015-11-23T09:11:12Z" id="158881741">I think we need to keep month as `M` as this is what we expect in our date formats and data math so it is consistent. While I think most validation in the codebase should be case-insensitive I think in this case it should not be. All the other options (`y`,`q`,`w`,`h`,`s`) should be case in-sensitive too as in the date format syntax their uppercase and lowercase forms still relate to the same interval, but `M` and `m` have different meaning in the date format and the date math so we need to honour that here IMHO
</comment><comment author="jpountz" created="2016-08-03T06:45:53Z" id="237156271">@colings86 was it fixed by #19649?
</comment><comment author="colings86" created="2016-08-03T07:51:55Z" id="237168108">@jpountz yes, sorry, will close this now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update the resiliency page to 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14801</link><project id="" key="" /><description /><key id="117355246">14801</key><summary>Update the resiliency page to 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label></labels><created>2015-11-17T13:33:02Z</created><updated>2015-11-29T11:21:15Z</updated><resolved>2015-11-29T11:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-19T13:09:06Z" id="158052805">LGTM.
</comment><comment author="clintongormley" created="2015-11-19T14:23:01Z" id="158071653">A couple of minor comments. Thanks for doing this!
</comment><comment author="bleskes" created="2015-11-23T12:15:42Z" id="158917271">@clintongormley I pushed another update.
</comment><comment author="clintongormley" created="2015-11-28T15:18:18Z" id="160309996">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor typos in query dsl docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14800</link><project id="" key="" /><description>When passing the example json snippets through the query parser while working
on #14249 some of the examples could not be parsed due to minor issues. This PR fixes those
examples.

Relates to #14249

@clintongormley would be good if you could have a look, the one interesting change is for the geo bounding box examples where on one occasion latitudes and longitudes had been switched I think.
</description><key id="117350002">14800</key><summary>Fix minor typos in query dsl docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T13:00:00Z</created><updated>2016-03-10T12:45:13Z</updated><resolved>2016-03-10T12:45:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T19:36:28Z" id="163367439">LGTM
</comment><comment author="MaineC" created="2016-01-19T09:53:55Z" id="172796975">@clintongormley I'm confused now - should this tiny fix wait for @nknize to comment as you requested or go in as is like @dakrone suggested?
</comment><comment author="clintongormley" created="2016-01-20T12:29:45Z" id="173190524">I'd just go ahead and push
</comment><comment author="nknize" created="2016-01-20T15:34:35Z" id="173240493">LGTM. Sorry for the delay. I changed my gmail and github notifications and this one was swallowed in the process.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore thread interrupt flag after an InterruptedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14799</link><project id="" key="" /><description>This commit replaces all occurrences of Thread.interrupted() with
Thread.currentThread().interrupt(). While the former checks and clears the current
thread's interrupt flag the latter sets it, which is actually intended.

Closes #14798
</description><key id="117336076">14799</key><summary>Restore thread interrupt flag after an InterruptedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Core</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T11:33:57Z</created><updated>2015-11-19T12:14:47Z</updated><resolved>2015-11-18T16:13:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-17T14:14:54Z" id="157381185">I've looked in the JDK source of `AbstractExecutorService`, `FutureTask` and similar classes. The standard implementation `ThreadPoolExecutor` ensures in the method `runWorker()` that the interrupt flag is cleared unless the pool is being shut down in which case the thread is actively interrupted.

The best reference on how to implement interruption policies in Java can be found in Java Concurrency in Practice, which basically advises that you should restore the flag or rethrow `InterruptedException` whenever you do not know what code is higher up the call stack (i.e. every time you do not extend `java.lang.Thread`).

@bleskes I am going to comment on each usage where the flag is restored soon.
</comment><comment author="bleskes" created="2015-11-18T10:28:19Z" id="157672362">Thx @danielmitterdorfer for digging into this. Left comments.
</comment><comment author="danielmitterdorfer" created="2015-11-18T13:09:00Z" id="157707038">@bleskes I've pushed a revised version and have commented on the parts where we still not agree entirely. :)
</comment><comment author="danielmitterdorfer" created="2015-11-18T13:51:44Z" id="157718866">@bleskes I've reverted the changes in `BulkProcessor`, removed the superfluous catch block in `AbstractRunnable` and throw `InterruptedException` in `IndicesService`.

On an unrelated note, I've noticed that `NodeIndexDeletedAction#lockIndexAndAck()` uses a timeout of 30 minutes when calling `#processPendingDeletes()` but the log statement a few lines below says "30 seconds". So one of them is wrong. I assume a timeout of 30 minutes is far too long and it should rather be 30 seconds.
</comment><comment author="bleskes" created="2015-11-18T15:25:58Z" id="157747919">LGTM. Left one minor question.
</comment><comment author="danielmitterdorfer" created="2015-11-18T15:36:38Z" id="157751300">@bleskes Thanks for your review efforts! I answered your last comment and will merge the PR soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread interrupt flag is not properly restored after an InterruptedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14798</link><project id="" key="" /><description>I've spotted multiple instances across the code base where we catch `InterruptedException` and call `Thread.interrupted()` afterwards.  The problem is that `Thread.interrupted()` queries and _clears_ the interrupted status (see also [Javadoc](http://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#interrupted--)) which is contrary to what should happen, namely to _set_ the interrupt flag again so code higher up the call stack can check the flag and act accordingly. This is done by calling `Thread.currentThread().interrupt()` (see [Javadoc](http://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#interrupt--)).
</description><key id="117329974">14798</key><summary>Thread interrupt flag is not properly restored after an InterruptedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Core</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T10:54:47Z</created><updated>2016-06-17T13:26:12Z</updated><resolved>2015-11-18T16:13:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mtraynham" created="2016-06-17T01:46:21Z" id="226660777">It'd be nice to have [BulkProcessor#L348](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java#L348) fixed back in 1.7.  I just ran into this today, as I have multiple data source threads running (using ExecutorService), all which populate a single BulkProcessor.  We `cancel` the source thread in some cases, which triggers an interrupt and thus the BulkProcessor may sometimes swallow it (more often than not).

It does look like the Throwable is returned back to the BulkListener, so maybe I can check if it's an InterruptedException and re-interrupt?  Thoughts?
</comment><comment author="danielmitterdorfer" created="2016-06-17T05:42:15Z" id="226685161">&gt; It'd be nice to have BulkProcessor#L348 fixed back in 1.7.

We only fix critical bugs (e.g. ones that lead to data loss) in 1.7 and I'd not consider this one critical enough.

&gt; It does look like the Throwable is returned back to the BulkListener, so maybe I can check if it's an InterruptedException and re-interrupt? Thoughts?

Admittedly, this is just a workaround but this should work. Just don't forget to remove it once you upgrade. ;) It won't hurt calling `Thread.currentThread().interrupt()` again in your listener but it is not necessary on newer versions of Elasticsearch (starting with Elasticsearch 2.2.0)
</comment><comment author="mtraynham" created="2016-06-17T13:26:12Z" id="226768130">Seems to work ok.  Thanks for the feedback @danielmitterdorfer !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove TODO about caching, which is already addressed in master/2.x.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14797</link><project id="" key="" /><description>Close #14673
</description><key id="117311104">14797</key><summary>Remove TODO about caching, which is already addressed in master/2.x.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>review</label><label>v2.1.0</label></labels><created>2015-11-17T09:00:22Z</created><updated>2015-11-17T14:40:00Z</updated><resolved>2015-11-17T14:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-17T09:47:55Z" id="157322337">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to use dynamic mapping and patch_match with mapper-attachment plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14796</link><project id="" key="" /><description>here my mapping : 

```
PUT my_index
{
  "mappings": {
    "my_type": {
      "dynamic_templates": [
        {
          "attachment": {
            "path_match": "*.my_attachment",
            "match_mapping_type": "string",
            "mapping": {
              "type": "attachment",
              "fields": {
                "content_length": {
                  "type": "integer",
                  "store": "yes"
                },
                "content_type": {
                  "type": "string",
                  "store": "yes"
                },
                "date": {
                  "type": "date",
                  "store": "yes"
                },
                "name": {
                  "type": "string",
                  "store": "yes"
                },
                "author": {
                  "type": "string",
                  "store": "yes"
                },
                "title": {
                  "type": "string",
                  "store": "yes"
                },
                "keywords": {
                  "type": "string",
                  "store": "yes"
                },
                "language": {
                  "type": "string",
                  "store": "yes"
                },
                "content": {
                  "type": "string",
                  "term_vector": "with_positions_offsets",
                  "store": "yes"
                }
              }
            }
          }
        }
      ]
    }
  }
}
```

when i try to put one document

```
PUT /my_index/my_type/my_id
{
  "joe": {
    "my_attachment": "IkdvZCBTYXZlIHRoZSBRdWVlbiIgKGFsdGVybmF0aXZlbHkgIkdvZCBTYXZlIHRoZSBLaW5nIg=="
  }
}
```

i receive error : 

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "Field name [joe.my_attachment] cannot contain '.'"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "Field name [joe.my_attachment] cannot contain '.'"
   },
   "status": 400
}
```

The mapping works find if i don't use path_match see #14794
</description><key id="117300051">14796</key><summary>Unable to use dynamic mapping and patch_match with mapper-attachment plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bdbogjoe</reporter><labels><label>:Plugin Mapper Attachment</label><label>bug</label></labels><created>2015-11-17T07:28:07Z</created><updated>2015-12-19T14:31:48Z</updated><resolved>2015-12-19T14:18:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-17T07:30:15Z" id="157298882">Most likely caused by https://github.com/elastic/elasticsearch-mapper-attachments/issues/169

From elasticsearch 2.0, mapper attachments plugin does not work if the attachment field is not a top level field but an inner field.
</comment><comment author="bdbogjoe" created="2015-11-17T07:39:43Z" id="157300528">ah ok thanks for your quick response, seems i have to revert to ES 1.7 until it is fixed

Any target date for 3.0.3 ?
</comment><comment author="bdbogjoe" created="2015-12-19T14:18:44Z" id="165987655">works fine with ES 2.1.1 and mapper-attachment 3.1.1

thanks
</comment><comment author="dadoonet" created="2015-12-19T14:31:48Z" id="165989260">Thank you for confirming!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>exception has been thrown out instead of returning an empty result if searching on an empty index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14795</link><project id="" key="" /><description>I delete the type 'mytype' with uri `curl -XDELETE http://test-elasticsearch:9200/myindex/mytype`
and then I run `curl -XGET 'http://test-elasticsearch:9200/myindex/mytype/_search?q=id_str:549678149&amp;sort=created_time:desc'` to search and sort the result, what I was expecting was an empty result, but I got this exception

```
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[Fwu5RndFSr6X-tNXxBmOZQ][myindex][0]: RemoteTransportException[[MyApp_Master_Frontend][inet[/127.0.0.1:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[[myindex][0]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"id_str:549678149\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}},\"sort\":[{\"created_time\":{\"order\":\"desc\"}}]}]]]; nested: SearchParseException[[myindex][0]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [No mapping found for [created_time] in order to sort on]]; }{[hQggziUbTEeRmGf2mDKqpg][myindex][1]: SearchParseException[[myindex][1]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"id_str:549678149\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}},\"sort\":[{\"created_time\":{\"order\":\"desc\"}}]}]]]; nested: SearchParseException[[myindex][1]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [No mapping found for [created_time] in order to sort on]]; }{[Fwu5RndFSr6X-tNXxBmOZQ][myindex][2]: RemoteTransportException[[MyApp_Master_Frontend][inet[/127.0.0.1:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[[myindex][2]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"id_str:549678149\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}},\"sort\":[{\"created_time\":{\"order\":\"desc\"}}]}]]]; nested: SearchParseException[[myindex][2]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [No mapping found for [created_time] in order to sort on]]; }{[hQggziUbTEeRmGf2mDKqpg][myindex][3]: SearchParseException[[myindex][3]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"id_str:549678149\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}},\"sort\":[{\"created_time\":{\"order\":\"desc\"}}]}]]]; nested: SearchParseException[[myindex][3]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [No mapping found for [created_time] in order to sort on]]; }{[Fwu5RndFSr6X-tNXxBmOZQ][myindex][4]: RemoteTransportException[[MyApp_Master_Frontend][inet[/127.0.0.1:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[[myindex][4]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"id_str:549678149\",\"lowercase_expanded_terms\":true,\"analyze_wildcard\":false}},\"sort\":[{\"created_time\":{\"order\":\"desc\"}}]}]]]; nested: SearchParseException[[myindex][4]: query[id_str:549678149],from[-1],size[-1]: Parse Failure [No mapping found for [created_time] in order to sort on]]; }]","status":400}
```

is it supposed to be this way? or should return an empty result instead?
</description><key id="117291737">14795</key><summary>exception has been thrown out instead of returning an empty result if searching on an empty index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jerryladoo</reporter><labels /><created>2015-11-17T06:23:34Z</created><updated>2015-11-18T01:07:12Z</updated><resolved>2015-11-17T06:39:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-17T06:39:46Z" id="157287210">You need to have a mapping for mytype. Which is not the case here.

Note that in 2.0 the former request you sent (DELETE) won't be possible anymore.
</comment><comment author="jerryladoo" created="2015-11-17T22:43:29Z" id="157533827">@dadoonet, thanks for lightening response.

But it did return me an empty result when I ran the same api without the `sort`,

```
http://test-elasticsearch:9200/myindex/mytype/_search?q=created_time:[* TO 2015-11-18T09:15:46]
```

or

```
http://test-elasticsearch:9200/myindex/mytype/_search?q=id_str:549678149
```

That means that the response from same api is inconsistent.
- with `sort` attached, it responds with an exception inside,
- without `sort` attached, it responds an empty result

is this also not a case?
</comment><comment author="rjernst" created="2015-11-18T01:07:12Z" id="157564684">Questions like this are better asked on http://discuss.elastic.co

Sorting on a non-existent field is not supported. As David said, you must first have a mapping for that field. The response is not inconsistent, the default sort will sort on `_score` which is a built in field.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to use dynamic mapping with mapper-attachment plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14794</link><project id="" key="" /><description>here a my mapping : 

```
DELETE my_index

PUT my_index
{
  "mappings": {
    "my_type": {
      "dynamic_templates": [
        {
          "attachment": {
            "match": "my_attachment",
            "match_mapping_type": "string",
            "mapping": {
              "type": "attachment",
              "fields": {
                "content_length": {
                  "type": "integer",
                  "store": "yes"
                },
                "content_type": {
                  "type": "string",
                  "store": "yes"
                },
                "date": {
                  "type": "date",
                  "store": "yes"
                },
                "name": {
                  "type": "string",
                  "store": "yes"
                },
                "author": {
                  "type": "string",
                  "store": "yes"
                },
                "title": {
                  "type": "string",
                  "store": "yes"
                },
                "keywords": {
                  "type": "string",
                  "store": "yes"
                },
                "language": {
                  "type": "string",
                  "store": "yes"
                },
                "content": {
                  "type": "string",
                  "term_vector": "with_positions_offsets",
                  "store": "yes"
                }
              }
            }
          }
        }
      ]
    }
  }
}
```

If i put this document works fine

```
PUT /my_index/my_type/my_id
{
"my_attachment":"IkdvZCBTYXZlIHRoZSBRdWVlbiIgKGFsdGVybmF0aXZlbHkgIkdvZCBTYXZlIHRoZSBLaW5nIg=="
  }
```

but if i want to use _content field, the dynamic mapping is not applied

```
PUT /my_index/my_type/my_id
{
  "my_attachment":{
    "_content":"IkdvZCBTYXZlIHRoZSBRdWVlbiIgKGFsdGVybmF0aXZlbHkgIkdvZCBTYXZlIHRoZSBLaW5nIg=="
  }
}
```

@dadoonet Any idea ?

Eric (from ENIB as well :smile: )
</description><key id="117285775">14794</key><summary>Unable to use dynamic mapping with mapper-attachment plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bdbogjoe</reporter><labels /><created>2015-11-17T05:27:59Z</created><updated>2015-11-18T16:22:12Z</updated><resolved>2015-11-18T15:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bdbogjoe" created="2015-11-17T06:07:59Z" id="157280700">using elasticsearch 2.0.0 and mapper-attachment 3.0.2
</comment><comment author="dadoonet" created="2015-11-17T06:11:09Z" id="157281446">Hi Eric 

1st: the former PUT is fine. Why do you want to use the later which is supposed to do the same thing? Are you also providing other metadata?

2nd: how do you know that the later does not work? 
</comment><comment author="bdbogjoe" created="2015-11-17T07:29:45Z" id="157298819">Hi,

1) I want to put language as well
2) because when i check mapping i have this : 

```
{
  "my_index" : {
    "mappings" : {
      "my_type" : {
        "dynamic_templates" : [ {
          "attachment" : {
            "mapping" : {
              "type" : "attachment",
              "fields" : {
                "date" : {
                  "store" : "yes",
                  "type" : "date"
                },
                "content_type" : {
                  "store" : "yes",
                  "type" : "string"
                },
                "keywords" : {
                  "store" : "yes",
                  "type" : "string"
                },
                "author" : {
                  "store" : "yes",
                  "type" : "string"
                },
                "name" : {
                  "store" : "yes",
                  "type" : "string"
                },
                "language" : {
                  "store" : "yes",
                  "type" : "string"
                },
                "title" : {
                  "store" : "yes",
                  "type" : "string"
                },
                "content_length" : {
                  "store" : "yes",
                  "type" : "integer"
                },
                "content" : {
                  "term_vector" : "with_positions_offsets",
                  "store" : "yes",
                  "type" : "string"
                }
              }
            },
            "match" : "my_attachment",
            "match_mapping_type" : "string"
          }
        } ],
        "properties" : {
          "my_attachment" : {
            "properties" : {
              "_content" : {
                "type" : "string"
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="dadoonet" created="2015-11-17T07:50:55Z" id="157301959">I think that's another problem. I guess that your dynamic mapping is not applied correctly here.

Could you create a full script which helps to reproduce your issue?
Something like:

```
DELETE index
PUT index
PUT index/type/id
GET index/_mapping
```
</comment><comment author="bdbogjoe" created="2015-11-17T08:31:19Z" id="157308035">here the full script : 

```
DELETE my_index

PUT my_index
{
  "mappings": {
    "my_type": {
      "dynamic_templates": [
        {
          "attachment": {
            "match": "my_attachment",
            "match_mapping_type": "string",
            "mapping": {
              "type": "attachment",
              "fields": {
                "content_length": {
                  "type": "integer",
                  "store": "yes"
                },
                "content_type": {
                  "type": "string",
                  "store": "yes"
                },
                "date": {
                  "type": "date",
                  "store": "yes"
                },
                "name": {
                  "type": "string",
                  "store": "yes"
                },
                "author": {
                  "type": "string",
                  "store": "yes"
                },
                "title": {
                  "type": "string",
                  "store": "yes"
                },
                "keywords": {
                  "type": "string",
                  "store": "yes"
                },
                "language": {
                  "type": "string",
                  "store": "yes"
                },
                "content": {
                  "type": "string",
                  "term_vector": "with_positions_offsets",
                  "store": "yes"
                }
              }
            }
          }
        }
      ]
    }
  }
}


PUT /my_index/my_type/my_id
{
  "my_attachment":{
    "_language":"en",
    "_content":"IkdvZCBTYXZlIHRoZSBRdWVlbiIgKGFsdGVybmF0aXZlbHkgIkdvZCBTYXZlIHRoZSBLaW5nIg=="
  }
}


get /my_index/_mapping/
```
</comment><comment author="dadoonet" created="2015-11-17T10:04:47Z" id="157325663">Thanks @bdbogjoe. That's indeed a bug.
</comment><comment author="bdbogjoe" created="2015-11-17T10:24:05Z" id="157329613">you are welcome, your are building a very nice tool, i'm pleased to help you to improve it
</comment><comment author="dadoonet" created="2015-11-18T15:23:58Z" id="157746961">Here is my theory and I don't think we can really fix it but just document it:
1. create a dynamic template which adds `"match": "my_attachment"` for a String field which matches `"match_mapping_type": "string"` (a String).
2. Index a document with `"attachment":"BASE64 ENCODED"`
3. This is matching the `string` type so we apply the mapping

Do the same thing but index a document like:

``` json
{
  "attachment": {
    "_content": "BASE64"
  }
}
```

Here the attachment field is now detected as an `object` not as a `string` anymore. So the dynamic mapping you defined can not be applied.

I don't think we can fix that. May be we can document that if you are using a dynamic template with a attachment field, you won't be able to use `_content` and other metadata fields. In that case I would not use a dynamic template.

I mean that in your example you have only one field named `my_attachment`, right? Is your intention to have an infinite number of such fields? 

If not, may be using an index template with a mapping which declares explicitly `my_attachment` as an attachment field would be better.

Adding label `docs` to this issue.
</comment><comment author="bdbogjoe" created="2015-11-18T15:32:56Z" id="157750373">i my document, i can have many attachment fields, that's why is was nice to the dynamic template

like

```
"lang":{
     "en":{
           "my_attachment":{
                 "_language":"en",
                 "_content":"...." 
          }
     }
     "fr":{
           "my_attachment":{
                 "_language":"fr",
                 "_content":"...." 
          }
     }
}
```
</comment><comment author="dadoonet" created="2015-11-18T15:39:40Z" id="157752040">I did not test so I don't know if this could work but what would happen if you change `"match_mapping_type": "string"` to `"match_mapping_type": "object"`?
</comment><comment author="bdbogjoe" created="2015-11-18T15:53:20Z" id="157757103">great !!

works fine, thanks
</comment><comment author="dadoonet" created="2015-11-18T15:57:21Z" id="157758189">Closing then as not an issue! :D 
</comment><comment author="bdbogjoe" created="2015-11-18T16:22:12Z" id="157766500">just have to update doc :smile: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elastic.co search performance bad</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14793</link><project id="" key="" /><description>we know there are many difference in elasticsearch 2.0, but i think 1.x is the main version in produce,we can't find many function by the search,if we can search in difference version?
</description><key id="117268257">14793</key><summary>elastic.co search performance bad</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whybangbang</reporter><labels /><created>2015-11-17T02:29:41Z</created><updated>2016-02-14T17:04:14Z</updated><resolved>2016-02-14T17:04:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-11-17T03:26:16Z" id="157256820">Thanks for mentioning this. I have raised an internal issue with the website team to see if we can make the search version contextual.

That way when you pick (eg) 1.7 docs, any searches will only look there. However we have always made the `current` docs match the latest released version of our products.
</comment><comment author="whybangbang" created="2015-11-17T03:45:35Z" id="157259377">thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>most fields in geopoint is error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14792</link><project id="" key="" /><description>we use the most fields in many case,title,name and so on,and it run very well,there is a geopoint field in our mapping ,we find geohash cell have a good performance, so we update the mapping 
PUT
{
  "item": {
    "properties": {
      "location": {
        "type": "geo_point",
        "fields": {
          "location_cell": {
            "type": "geo_point",
            "geohash": true,
            "geohash_prefix": true,
            "geohash_precision": 5
          }
        }
      }
    }
  }
}

the same operation have be used in string type many times,but geo point most fields is error
ERROR: TransportError(400, u'MapperParsingException[failed to parse]; nested: ElasticsearchParseException[geo_point expected]; ')

es version 1.5.2 
</description><key id="117265506">14792</key><summary>most fields in geopoint is error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whybangbang</reporter><labels /><created>2015-11-17T02:17:11Z</created><updated>2015-11-18T14:48:31Z</updated><resolved>2015-11-18T14:48:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T14:48:31Z" id="157737004">Hi @whybangbang 

Your mapping works fine, and you haven't provided sufficient information to figure out what you are doing when you get that error.  Either way, this sounds like a question that would be better asked in our forums: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Make run command work within plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14791</link><project id="" key="" /><description>We recently got a run command with gradle, but it is sometimes useful to
run ES with a specific plugin. This is a start, by making each esplugin
have a run command which installs the plugin and runs elasticsearch in
the foreground.
</description><key id="117259909">14791</key><summary>Build: Make run command work within plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T01:30:42Z</created><updated>2015-11-17T17:09:29Z</updated><resolved>2015-11-17T17:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-17T13:32:48Z" id="157371510">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use a marker file to indicate forbidden patterns has run</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14790</link><project id="" key="" /><description>This makes forbidden patterns a little smarter, so it does not need to
run on every build. It works because the marker file timestamp will be
compared against the source files (which are the inputs to forbidden
patterns).

closes #14788
</description><key id="117254749">14790</key><summary>Use a marker file to indicate forbidden patterns has run</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-17T00:50:41Z</created><updated>2015-11-17T01:22:10Z</updated><resolved>2015-11-17T01:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-17T01:20:48Z" id="157229245">Thanks! LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel IndexNotFoundException on ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14789</link><project id="" key="" /><description>Version:
johnar@flowmaster3:/usr/share/elasticsearch$ bin/elasticsearch --version
Version: 2.0.0, Build: de54438/2015-10-22T08:09:48Z, JVM: 1.8.0_60

Plugins:
johnar@flowmaster3:/usr/share/elasticsearch$ bin/plugin list
Installed plugins in /usr/share/elasticsearch/plugins:
    - marvel-agent
    - license
    - kopf
    - head

Full error description:
When trying to use the marvel plugin in Kibana4, I get a "no data" URL like:

https://elasticsearch:444/app/marvel#/no-data?_g=(refreshInterval:(display:'10%20seconds',pause:!f,value:10000),time:(from:now-1h,mode:quick,to:now))

In the elasticsearch log, I get an error that looks like the index was not created:
http://pastebin.com/crQgxDXM

Further up the log I'm also getting errors like:
[2015-11-16 22:40:20,981][INFO ][rest.suppressed          ] /_status Params: {index=_status}
java.lang.IllegalArgumentException: No feature for name [_status]
        at org.elasticsearch.action.admin.indices.get.GetIndexRequest$Feature.fromName(GetIndexRequest.java:82)
        at org.elasticsearch.action.admin.indices.get.GetIndexRequest$Feature.convertToFeatures(GetIndexRequest.java:95)
        at org.elasticsearch.rest.action.admin.indices.get.RestGetIndicesAction.handleRequest(RestGetIndicesAction.java:77)

If I search for marvel related logs:
http://pastebin.com/PF5vce7x
</description><key id="117243093">14789</key><summary>Marvel IndexNotFoundException on ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">johnarnold</reporter><labels><label>feedback_needed</label></labels><created>2015-11-16T23:24:31Z</created><updated>2015-11-24T13:50:24Z</updated><resolved>2015-11-24T13:50:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-11-17T08:23:49Z" id="157306732">Thanks for reporting. The logs are truncated so I can't be sure of what is going on.

What is the layout of your cluster? One or more nodes? Is Marvel installed on all nodes? Does Marvel export data locally or via HTTP to another cluster? 

Can you please share a full log (from start) and your configuration please (take care to remove any sensitive data)?
</comment><comment author="johnarnold" created="2015-11-17T17:08:09Z" id="157436707">[flowstash.txt](https://github.com/elastic/elasticsearch/files/36943/flowstash.txt)

[elasticsearch.yml.txt](https://github.com/elastic/elasticsearch/files/36945/elasticsearch.yml.txt)
Sure thanks for taking a look.  The cluster has (3) master nodes and (24) data nodes.  License and Marvel-agent plugins are installed on all nodes.  The marvel agent is using default config, exporting locally.

Some more digging reveals that I may only be seeing these errors on the first master node.  I spot checked the other two masters and a random data node and they're clean.  This master node also fell out of the cluster overnight...  Possibly something else bad going on with this node.

It is also worth noting, I'm still getting "no data" from the marvel app in kibana4, and the other nodes are posting errors like:
2015-11-17 15:51:54,205][DEBUG][action.admin.cluster.node.info] [flowmaster2] failed to execute on node [MzfyRE6sTMeZ6d-hGm_xlA]
RemoteTransportException[[flowdata13][10.10.10.23:9300][cluster:monitor/nodes/info[n]]]; nested: NotSerializableExceptionWrapper;
Caused by: NotSerializableExceptionWrapper[null]
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)
        at java.util.ArrayList$Itr.next(ArrayList.java:851)
        at org.elasticsearch.action.admin.cluster.node.info.PluginsInfo.writeTo(PluginsInfo.java:86)
        at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:284)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:97)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:75)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

and 

[2015-11-17 17:07:20,708][DEBUG][action.admin.indices.stats] [flowdata1] [indices:monitor/stats] failed to execute operation for shard [[ipfix.2m-2015.09.25.05][9], node[0VqzKuFbR0iQ0ijw1C_GPA], [R], v[15], s[INITIALIZING], a[id=IldoY0ygQieYS4VpcDzb0w], unassigned_info[[reason=NODE_LEFT], at[2015-11-17T16:35:51.063Z], details[node_left[ojAMM5pCRMadJxtk2xFfHQ]]]]
[ipfix.2m-2015.09.25.05][[ipfix.2m-2015.09.25.05][9]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: ShardNotFoundException[no such shard];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:399)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:376)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:365)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [ipfix.2m-2015.09.25.05][[ipfix.2m-2015.09.25.05][9]] ShardNotFoundException[no such shard]
        at org.elasticsearch.index.IndexService.shardSafe(IndexService.java:198)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:98)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:395)
        ... 7 more
</comment><comment author="johnarnold" created="2015-11-17T17:10:04Z" id="157437776">[flowdata1.log.txt](https://github.com/elastic/elasticsearch/files/36949/flowdata1.log.txt)
[flowmaster2.log.txt](https://github.com/elastic/elasticsearch/files/36950/flowmaster2.log.txt)

Two additional logfiles, one from a data node and one from a "otherwise working" master node.
</comment><comment author="johnarnold" created="2015-11-17T17:31:34Z" id="157445531">Also noticed marvel shards are unassigned:

johnar@flowmaster1:/var/log/elasticsearch$ curl -XGET http://localhost:9200/_cat/shards | grep marvel
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  1 1192k    1 16299    0     0  16003      0  0:01:16  0:00:01  0:01:15 16010.marvel-es-2015.11.17  0  p INITIALIZING                  10.10.10.34 flowdata24
.marvel-es-2015.11.17  0  r UNASSIGNED
.marvel-es-2015.11.16  0  r INITIALIZING                  10.10.10.34 flowdata24
.marvel-es-2015.11.16  0  p STARTED      4013754  870.2mb 10.10.10.31 flowdata21
.marvel-es-2015.11.15  0  p UNASSIGNED
.marvel-es-2015.11.15  0  r UNASSIGNED
100 1192k  100 1192k    0     0  1166k      0  0:00:01  0:00:01 --:--:-- 1167k
.marvel-es-data        0  p INITIALIZING                  10.10.10.34 flowdata24
.marvel-es-data        0  r UNASSIGNED
</comment><comment author="johnarnold" created="2015-11-18T23:19:42Z" id="157896249">I used cluster reroute API to force the primary and replica shards onto different nodes and my cluster is now green:

{
  "cluster_name" : "flowstash",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 25,
  "number_of_data_nodes" : 22,
  "active_primary_shards" : 7446,
  "active_shards" : 14892,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}

Still no data in Marvel.
</comment><comment author="tlrx" created="2015-11-19T09:58:28Z" id="158009356">@johnarnold thanks for all these log files, there are very valuable things in (out of memory errors, concurrent modification while getting the nodes stats, unassigned shards etc).

 I'll dig into them and I'll inform you soon.
</comment><comment author="tlrx" created="2015-11-19T10:49:34Z" id="158020664">@johnarnold Looks like flowmaster1 had an OutOfMemory error around [2015-11-17 05:38:26,357]... can you please see if the JVM created an heapdump on disk? Thanks
</comment><comment author="clintongormley" created="2015-11-19T13:44:26Z" id="158060104">@tlrx @johnarnold ...in which case the shards probably weren't being assigned because of the bug that has been fixed here: https://github.com/elastic/elasticsearch/pull/14695
</comment><comment author="tlrx" created="2015-11-19T15:33:03Z" id="158091114">@clintongormley thanks for the pointer. 

I also found a bug in marvel that may cause OOM errors if the shards stay unassigned for a long time. It will be fixed in next releases.

Anyway it does not explain why the shards of the marvel indices were not assigned. @johnarnold I'm curious to know why you have so many shards (24 per index) in your cluster? 

`"active_shards" : 14892` is a lot of shards...
</comment><comment author="johnarnold" created="2015-11-19T21:36:43Z" id="158205200">24 data nodes, 1 primary shard per node.  This spreads the indexing work across all the nodes, keeps shard size to a reasonable size.

Where would the heapdump be on disk?
</comment><comment author="tlrx" created="2015-11-24T13:45:41Z" id="159271303">&gt; Where would the heapdump be on disk?

Thanks but I think it's useless now: we found and fixed the issue and it will be released soon.

&gt; 24 data nodes, 1 primary shard per node. This spreads the indexing work across all the nodes, keeps shard size to a reasonable size.

Out of curiosity, what is a reasonable shard size in your case? And what is your indexing load?
</comment><comment author="clintongormley" created="2015-11-24T13:50:24Z" id="159273128">thanks @tlrx. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>forbiddenPatterns needs an up to date check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14788</link><project id="" key="" /><description>It looks like right now forbiddenPatterns always runs - it should probably declare itself up to date if neither the files nor the patterns have changed since the last time it runs. It'll save a few seconds per build.
</description><key id="117236428">14788</key><summary>forbiddenPatterns needs an up to date check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T22:44:40Z</created><updated>2015-11-17T01:22:08Z</updated><resolved>2015-11-17T01:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Dynamic (inline) groovy scripts that parse JSON do not work with 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14787</link><project id="" key="" /><description>I'm trying to migrate from 1.7 to 2.0 and I can't get my existing dynamic groovy script to work with ES. 
First, I added this line to elasticsearch.yml:
_script.inline: on_

With just this line I'm getting an error when I try to do bulk update with the following script for each update action:
`final Script updateScript = new Script("event = new groovy.json.JsonSlurper().parseText(_event);ctx._source.events += event", 
                                           ScriptType.INLINE, "groovy", ImmutableMap.of("_event", eventJson));`

I'm getting the error when I execute the action:
_GroovyScriptExecutionException[failed to run inline script [event = new groovy.json.JsonSlurper().parseText(_event);ctx._source.events += event] using lang [groovy]]; nested: NotSerializableExceptionWrapper[Could not initialize class groovy.json.internal.JsonParserCharArray];_

When I add these lines to elasticsearch.yml:
_script.engine.groovy.inline.aggs: on
script.engine.groovy.inline.mapping: on
script.engine.groovy.inline.search: on
script.engine.groovy.inline.update: on
script.engine.groovy.inline.plugin: on_

and try to execute an update with the same script, I get this error:
_GroovyScriptExecutionException[failed to run inline script [event = new groovy.json.JsonSlurper().parseText(_event);ctx._source.events += event] using lang [groovy]]; nested: NotSerializableExceptionWrapper; nested: SecurityException[access denied ("java.util.PropertyPermission" "groovy.json.internKeys" "read")];_

I tried to add 
`permission java.util.PropertyPermission "groovy.json.internKeys", "read";`
to java.policy file but it did not help.

I see  `permission java.util.PropertyPermission "*", "read,write";`
in org/elasticsearch/bootstrap/security.policy, so it is not clear to me why I'm getting this permission error unless groovy runs with its own security manager that does not 

The same script with 1.7 worked.

I have few other scripts for score functions that do not use JSON, and these work fine.

Thanks,
Peter.
</description><key id="117227264">14787</key><summary>Dynamic (inline) groovy scripts that parse JSON do not work with 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">plebedev</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-11-16T21:56:39Z</created><updated>2016-04-01T19:44:04Z</updated><resolved>2016-04-01T18:53:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-16T22:40:13Z" id="157194221">Relates to #14488, @rmuir we added the policy to allow JSON encoding from Groovy, do you think it's safe to add the policy to allow JSON decoding as well?
</comment><comment author="rmuir" created="2015-11-16T23:44:49Z" id="157210952">If it is just a property, it does not hurt. Of course, i strongly feel scripts should not be doing any of this.
</comment><comment author="plebedev" created="2015-11-17T16:35:58Z" id="157424432">As a workaround, I found _groovy.json.JsonSlurperClassic_ class that can be used instead of _groovy.json.JsonSlurper_ without causing any of the errors. 
</comment><comment author="fs-chris" created="2016-02-18T08:33:37Z" id="185597491">I've the very same issue with Elastic version 2.2.0.
Yet the JsonSlurperClassic workaround no longer works because the class is no longer available.

My test script is as simple as that:

&lt;pre&gt;import groovy.json.JsonSlurper;
def jsonSlurper = new JsonSlurper(); 
def message = jsonSlurper.parseText(_json);&lt;/pre&gt;

When first trying to run the script, I get the _SecurityException[access denied ("java.util.PropertyPermission" "groovy.json.internKeys" "read")_ .
Every successive attempt to run the script then gives _NoClassDefFoundError[Could not initialize class groovy.json.internal.JsonParserCharArray];_

I've tried with the following policy but without success:

&lt;pre&gt;grant {
    permission java.util.PropertyPermission "groovy.json.internKeys", "read";
    permission org.elasticsearch.script.ClassPermission "java.lang.Class";
    permission org.elasticsearch.script.ClassPermission "groovy.json.JsonSlurper";
    permission org.elasticsearch.script.ClassPermission "groovy.json.internal.*";
    permission org.elasticsearch.script.ClassPermission "groovy.json.internal.JsonParserCharArray";
    permission org.elasticsearch.script.ClassPermission "org.elasticsearch.common.logging.*";
};&lt;/pre&gt;

Can anyone please provide an example how to use the JsonSlurper in a Groovy script with Elastic 2.2.0?
Thanks,
Chris
</comment><comment author="bleskes" created="2016-02-26T01:24:54Z" id="189070783">@plebedev correct me if I'm wrong, but it seems like you use a script to update a document and the parameter for the script is JSON encoded as string. If that's the case, you can use a map of maps (i.e. JSON object ) as the value of the parameter which means you don't need to encode the JSON or alternatively you can decode it on your client. Does that make sense?
</comment><comment author="fs-chris" created="2016-02-26T07:03:39Z" id="189140965">@bleskes thanks for your suggestion.
However, in my case this is not a feasible solution as it would require me to parse the JSON in the client, which is indeed not desirable for my application.
</comment><comment author="bleskes" created="2016-02-26T15:09:39Z" id="189317097">@fs-chris thanks

&gt; which is indeed not desirable for my application

Can you elaborate on this - most notably I'm interested in understanding how parsing in your application is different than parsing in the ES process.
</comment><comment author="fs-chris" created="2016-02-26T15:27:25Z" id="189322501">@bleskes thanks for your interest.

I'm using Groovy in an update script. 
The script decides whether to process the JSON based on the existing document being updated.
The JSON needs to be parsed only in a very small fraction of all such update requests. Most cases update the document with another argument passed to the script at the same time.

So, always parsing in the application would be unecessary overhead in almost all cases.
First fetching the existing document from the index and then making the decision in the client would be even more overhead. 
We need to process several thousands of such requests per second. So performance is essential.

The approach with doing it in the script worked well with previous Elastic releases.
</comment><comment author="bleskes" created="2016-02-29T11:18:46Z" id="190161545">@fs-chris thanks. I see the discussion on #16808 , most notably @clintongormley 's suggestion:

&gt; &gt; We need to process several thousands of such requests per second. So performance is essential.
&gt; 
&gt; ... which indicates that this should be implemented in Java as well.
</comment><comment author="plebedev" created="2016-04-01T18:53:55Z" id="204521596">@bleskes, We've migrated to 2.2.1 and we had to change the code to pass JSONObject, so it is not parsed in elastinsearch. Works so far :)

Thanks!
</comment><comment author="bleskes" created="2016-04-01T19:44:04Z" id="204539969">@plebedev cool. Thanks for circling back and letting us know.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_score is implicitly converted to integer with groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14786</link><project id="" key="" /><description>It looks like _score is implicitly converted to integer in this groovy script: `_score * 1`. I assume this is a bug since _score contains a floating point value.

To reproduce:

```
# Create empty index
curl -XPUT 'http://localhost:9200/twitter/'

# Feed two documents
for a in doc1 doc2; do curl -XPUT http://localhost:9200/twitter/tweet/$a -d '{"foo":"foo"}'; done

# Search with script_score
curl http://localhost:9200/twitter/_search -d '{
  "query": {
    "function_score": {
      "query": {
        "term": {
          "foo": "foo"
        }
      },
      "functions": [
        {
          "script_score": {
            "script": "_score * 1"
          }
        }
      ]
    }
  }
}'
```

Notice that hits have score 0.0. Replace `_score * 1` by `_score` to get the correct score.

I am running elasticsearch 1.7.3 and groovy 2.4.4
</description><key id="117219553">14786</key><summary>_score is implicitly converted to integer with groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">larschri</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-11-16T21:13:44Z</created><updated>2015-11-21T00:12:10Z</updated><resolved>2015-11-19T11:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T14:41:48Z" id="157733210">A few interesting things here:
-  `_score * 1` is resulting in an integer, which can result in a score of 0 if the score is less than 1
- `_score * 10000` STILL results in a score of 0
- `_score * 1.0` works
- In plain groovy, `_score = 0.94; _score * 1` returns a floating point

@dakrone any ideas what is happening here?
</comment><comment author="larschri" created="2015-11-18T15:18:01Z" id="157745401">`_score * 10000` - My theory is that _score is casted to integer (zero) before the multiplication takes place.
</comment><comment author="rjernst" created="2015-11-18T21:20:41Z" id="157866972">This is because `_score` is an implementation of `Number`, since it wraps around lucene's `Scorer`. Groovy assumes it is an int because it doesn't know otherwise (and so calls `intValue()`).
</comment><comment author="clintongormley" created="2015-11-19T11:41:15Z" id="158031774">ah, thanks for the explanation @rjernst.   I don't think this is anything we can fix? So the solution here is to use `1.0` instead.  
</comment><comment author="larschri" created="2015-11-21T00:12:10Z" id="158564244">&gt; ah, thanks for the explanation @rjernst. I don't think this is anything we can fix? So the solution here is to use 1.0 instead.

Using `1.0` is fine when you are aware of the bug, but the bug is still there. Users expect `_score` to behave like a float, so why not use `float` instead of `ScoreAccessor`? If that's not possible, maybe it works to inherit from `java.math.BigDecimal` instead of `Number`? Elasticsearch could even start every groovy script with `_score = (float) _score;` to fix this.

BTW. This looks like the same problem http://stackoverflow.com/questions/30048963/groovy-numbermath-defaults-to-integermath-for-unkown-number-types
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also clean buildSrc when cleaning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14785</link><project id="" key="" /><description>This forces buildSrc to be cleaned when running gradle clean. It's what
someone would expect, even though buildSrc is a pseudo-independent
project.
</description><key id="117213268">14785</key><summary>Also clean buildSrc when cleaning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T20:43:00Z</created><updated>2015-11-16T20:50:33Z</updated><resolved>2015-11-16T20:50:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-16T20:49:01Z" id="157164851">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Lucene to 5.4.0-snapshot-1714615</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14784</link><project id="" key="" /><description>Simple upgrade, except I had to add `matchCost` impl in `GeoDistanceRangeQuery.java`.
</description><key id="117201487">14784</key><summary>Upgrade Lucene to 5.4.0-snapshot-1714615</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T19:39:03Z</created><updated>2015-11-16T21:07:52Z</updated><resolved>2015-11-16T21:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T21:03:17Z" id="157169424">LGTM

This makes me realize we should not even expose a two-phase iterator with GeoDistance.ALWAYS_INSTANCE. I can try to fix it in another PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make _type use doc values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14783</link><project id="" key="" /><description>_type should have got doc values with the change to default doc values.
However, due to how metadata fields have separate builders and special
constructors, it was not picking it up. This change updates the field
type for _type to have doc values.

closes #14781
</description><key id="117200970">14783</key><summary>Make _type use doc values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T19:36:03Z</created><updated>2015-11-18T14:27:29Z</updated><resolved>2015-11-16T21:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T21:04:56Z" id="157169766">LGTM. This will also help index sorting on _type once implemented :)
</comment><comment author="rmuir" created="2015-11-16T23:58:06Z" id="157213020">What type of docvalues is it? I dont see it explicit. Can we ensure it is SORTED not SORTED_SET and not BINARY?
</comment><comment author="rjernst" created="2015-11-17T00:38:58Z" id="157221032">Looks like it is SORTED_SET at the moment (already was in 1.x when it was enabled). I will create a new PR to change it to SORTED.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Confusing exception when `regexp` query used on numeric field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14782</link><project id="" key="" /><description>```
PUT t/t/1
{
  "num": 34
}

GET /_search
{
  "query": {
    "regexp": {
      "num": {
        "value": "34"
      }
    }
  }
}
```

returns:

   "error": {
      "root_cause": [
         {
            "type": "illegal_argument_exception",
            "reason": "expected '\"' at position 11"
         }
      ],
</description><key id="117197545">14782</key><summary>Confusing exception when `regexp` query used on numeric field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-16T19:17:10Z</created><updated>2015-11-21T03:42:21Z</updated><resolved>2015-11-21T03:42:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="camilojd" created="2015-11-21T02:31:40Z" id="158578100">Hi, I created a fix for this in the referenced PR, that I believe does the right thing.

All tests pass.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_type not using doc_values in ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14781</link><project id="" key="" /><description>```
POST /_cache/clear
DELETE testfd
POST testfd/type/1
{
  "test":"test"
}
GET testfd/_search
{
  "size": 0, 
  "aggs": {
    "NAME": {
      "terms": {
        "field": "_type",
        "size": 10
      }
    }
  }
}

GET /_cat/fielddata?v
```

Testing on ES 1.7 and ES 2.0, the above loads the same amount of fielddata into the fd cache.

On ES 1.7, put mapping api's change of _type to use doc_values or disable fd options do not throw an error, but they are not honored.

On ES 2.0, we explicitly throw an error when someone tries to modify the _type.  However, it also appears that we do not use doc_values for _type by default in ES 2.0.

If this is the expected behavior, it will be nice to document accordingly.  We do have use cases in the field where users are aggregating against the _type field.  Workaround is to tag each document with a custom "type" field and educate users to aggregate against this field instead of _type, though it will be helpful to provide a way to disable fielddata loading for _type as well.
</description><key id="117191737">14781</key><summary>_type not using doc_values in ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label></labels><created>2015-11-16T18:45:06Z</created><updated>2015-11-16T21:09:49Z</updated><resolved>2015-11-16T21:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-11-16T18:46:39Z" id="157132573">A workaround to this problem is to _not_ use `_type` at all. Have a single `_type` and add another field that represents the "type" of a document, then work against it. This has the added benefit that it simplifies the mapping.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add variable-length long encoding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14780</link><project id="" key="" /><description>This commit adds a method of encoding longs using a variable-length
representation. This encoding is an implementation of the zig-zag
encoding from protocol buffers. Numbers that have a small absolute value
will use a small number of bytes. This is achieved by zig-zagging
through the space of longs in order of increasing absolute value `(0, -1,
1, -2, 2, &#8230;, Long.MAX_VALUE, Long.MIN_VALUE) -&gt; (0, 1, 2, 3, 4, &#8230;, -2,
-1)`. The resulting values are then encoded as if they represent unsigned
numbers.
</description><key id="117184788">14780</key><summary>Add variable-length long encoding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T18:08:27Z</created><updated>2015-11-17T15:04:14Z</updated><resolved>2015-11-17T15:04:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T18:35:04Z" id="157129802">Maybe we should try to have the same naming as Lucene: [writeZLong](http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-core/5.2.1/org/apache/lucene/store/DataOutput.java#DataOutput.writeZLong%28long%29) and [readZLong](http://grepcode.com/file/repo1.maven.org/maven2/org.apache.lucene/lucene-core/5.2.1/org/apache/lucene/store/DataInput.java#DataInput.readZLong%28%29)? We could also reuse the implementation for the zig-zag encoding/decoding which is public in oal.util.BitUtil.
</comment><comment author="jasontedor" created="2015-11-16T19:16:03Z" id="157140573">@jpountz Thanks. I pushed efd4b71965c0ff84097436081ae1f0be4aaf1bd2.
</comment><comment author="jpountz" created="2015-11-16T21:39:49Z" id="157178945">How would you feel about reverting the vlong rename as well? I like to have the same name as in Lucene as well so that it's clear that DataInput.readVLong and StreamInput.readVLong are the same thing.
</comment><comment author="jasontedor" created="2015-11-16T21:59:44Z" id="157183985">&gt; How would you feel about reverting the vlong rename as well?

@jpountz I don't like it. This came about because the old `writeVLong` doesn't accept negatives and I [mistakenly thought](https://github.com/elastic/elasticsearch/pull/14651/files#r44882241) solely based on its name that it did and that the corresponding `writeVInt` does.
</comment><comment author="jpountz" created="2015-11-16T22:34:01Z" id="157192867">Sorry but this rename does not feel necessary to me, as the vlong encoding already suggests it's designed for small positive values. Then I guess we have two options: either write unsigned longs, or reject negative values as they are likely to be a mistake. We are already doing the latter, which seems to be a sensible choice to me.
</comment><comment author="rjernst" created="2015-11-16T22:38:21Z" id="157193808">I agree the rename should not be necessary. Rejecting negative values seems sufficient.
</comment><comment author="jasontedor" created="2015-11-16T22:59:16Z" id="157201910">&gt; the vlong encoding already suggests it's designed for small positive values.

@jpountz Yeah, the disagreement stems from me thinking that "positive" should be struck from that sentence fragment, and that `writeVInt` does support negative values.

I'm okay changing the name back. It's never worth bickering endlessly over issues like this.

I've reverted the name change in efcfd516093687f73713dae8e4b75a3559c00f0b. When I squash I'll change the initial commit message to remove the discussion of the method name change.
</comment><comment author="rjernst" created="2015-11-16T23:36:32Z" id="157209596">LGTM
</comment><comment author="jpountz" created="2015-11-17T08:37:31Z" id="157308928">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use JDK at JAVA_HOME for compiling/testing, and improve build info output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14779</link><project id="" key="" /><description>We currently enforce JAVA_HOME is set, but use gradle's java version to
compile, and JAVA_HOME version to test. This change makes the compile
and test both use JAVA_HOME, and clarifies which java version is used by
gradle and that used for the compile/test.
</description><key id="117174202">14779</key><summary>Use JDK at JAVA_HOME for compiling/testing, and improve build info output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T17:18:29Z</created><updated>2015-11-20T20:12:51Z</updated><resolved>2015-11-20T20:12:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-19T18:35:22Z" id="158149408">@rmuir I pushed a new commit handling the tmp file deletion.
</comment><comment author="rjernst" created="2015-11-20T19:33:09Z" id="158503397">@nik9000 I removed the debugging output.
</comment><comment author="nik9000" created="2015-11-20T19:46:19Z" id="158506337">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update update.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14778</link><project id="" key="" /><description>Bug fix in documentation: Java API [2.0] &#187; Document APIs &#187; Update API &#187; Upsert.
</description><key id="117164624">14778</key><summary>Update update.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rothvaw</reporter><labels><label>docs</label></labels><created>2015-11-16T16:35:51Z</created><updated>2015-11-18T14:22:14Z</updated><resolved>2015-11-18T14:21:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rothvaw" created="2015-11-16T16:51:03Z" id="157094884">I signed the CLA.
</comment><comment author="clintongormley" created="2015-11-18T14:22:14Z" id="157726926">thanks @rothvaw - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>updated yum repo doku - load gpg via https</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14777</link><project id="" key="" /><description>Ensure the YUM GPG-Key is imported via **https** to preserve users having unprotected sex with the internet.
</description><key id="117161750">14777</key><summary>updated yum repo doku - load gpg via https</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lhampe</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-11-16T16:22:24Z</created><updated>2015-11-18T19:09:23Z</updated><resolved>2015-11-18T19:09:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lhampe" created="2015-11-17T21:57:01Z" id="157521911">same issue @ elastic/logstash#4197
</comment><comment author="nik9000" created="2015-11-18T17:09:47Z" id="157780942">Yeah, thats horrible. I've read the apt instructions dozens of times but never the rpm ones.... I can't merge this unless you sign the CLA.
</comment><comment author="lhampe" created="2015-11-18T18:50:49Z" id="157818851">@nik9000 @clintongormley as i wrote in the linked ticket ( elastic/logstash#4197 ) it's ridiculous to waste 30 minutes just for contributing this single character ^^
So please create an PR signed by yourself or address this painfull issue to anyone who can fix it.
</comment><comment author="clintongormley" created="2015-11-18T19:09:23Z" id="157825328">Sorry you feel that way.  Thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decide whether to back port #14678 to 1.7.4 (ignored unallocated shards &amp; making allocation decision)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14776</link><project id="" key="" /><description>#14678 ( Take ignored unallocated shards into account when making allocation decision ) fixes #14670 ( Shard relocation happens while cluster is yellow ) but is non-trivial to back port to 1.7.4  . It's possible but will take some careful work. Opening this issue to make sure we re-evaluate if it's needed.
</description><key id="117143444">14776</key><summary>Decide whether to back port #14678 to 1.7.4 (ignored unallocated shards &amp; making allocation decision)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>blocker</label><label>discuss</label></labels><created>2015-11-16T15:01:49Z</created><updated>2015-12-16T14:50:38Z</updated><resolved>2015-12-03T09:16:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-03T09:16:20Z" id="161559736">closed by #15195
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add unique id to query names to avoid naming conflicts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14775</link><project id="" key="" /><description>In AbstractQueryTestCase we randomly add the `_name` property to some of the queries. There are exceptional cases where we assign the same name to two queries in the setup which leads to test failures later. Adding a counter to the base tests that gets appended to all random query names will avoid this name clashes.

Closes #14746 
</description><key id="117135349">14775</key><summary>Add unique id to query names to avoid naming conflicts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T14:27:29Z</created><updated>2015-11-16T14:34:09Z</updated><resolved>2015-11-16T14:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-11-16T14:30:46Z" id="157048611">Looks good and easy enough to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Leak fewer backwards nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14774</link><project id="" key="" /><description>Right now elasticsearch will leak backwards nodes if you kill the maven process running them. With #14666 they'll at least leak them on a port range that won't bother anyone and be killed during the next build. We might be able to prevent some of these leaks if we leave the subprocess's stdin/stderr/stdout open and manually drain them while they run, similarly to how ant does it. I'm filing this ticket to investigate "the ant trick".

I'm assigning to myself because I suspect I'll work on it but I'm happy to help anyone that feels like taking it if you want it before I get around to it.
</description><key id="117129774">14774</key><summary>Leak fewer backwards nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>test</label></labels><created>2015-11-16T14:00:46Z</created><updated>2015-11-18T16:10:32Z</updated><resolved>2015-11-17T17:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-17T17:58:24Z" id="157453329">I had a look at this - so far as I can tell the situation is no different with backwards nodes than it is with the ant-run nodes: either way they die if you `kill` the process and they don't die if you `kill -9` the process. I'm going to close this. We can reopen it if/when we think there is more to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow CIDR notation in query string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14773</link><project id="" key="" /><description>Allow IP ranges to be specified with CIDR notation in a query string query.

I refactored the cidrMaskToMinMax method from IPv4RangeBuilder so it could be used for the IpFieldMapper as well.

Closes #7464
</description><key id="117117819">14773</key><summary>Allow CIDR notation in query string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">wbowling</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-16T12:56:28Z</created><updated>2015-11-20T15:51:46Z</updated><resolved>2015-11-20T10:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-17T21:49:11Z" id="157519903">@jpountz can you take a look at this? It seems good to me but it's not my area of expertise.
</comment><comment author="jpountz" created="2015-11-19T13:44:27Z" id="158060109">Left two minor comments but this looks good to me. I like how this changes the termQuery method so that all query parsers will benefit from this change (`query_string`, `simple_query_string` and `match` in particular).
</comment><comment author="rmuir" created="2015-11-19T13:49:40Z" id="158061317">&gt; I like how this changes the termQuery method so that all query parsers will benefit from this change (query_string, simple_query_string and match in particular).

Note that "/" is a reserved character in the lucene query language. In particular it looks for content between two unescaped "/"'s and treats that as a regular expression. 
</comment><comment author="wbowling" created="2015-11-20T00:10:20Z" id="158239900">Ok moved the code to IpFieldMapper so it's out of the Guava code, moved MAX_IP and changed to null instead of POSITIVE/NEGATIVE_INFINITY for the term query.

Some parts have been moved twice now so let me know if I should squash this all into one commit
</comment><comment author="jpountz" created="2015-11-20T10:20:17Z" id="158348979">Thanks  @wbowling. I merged it manually via 18c9eba40a044df0e4601e2438239f714a93a9e5
</comment><comment author="cbuescher" created="2015-11-20T15:31:01Z" id="158433105">@jpountz initial commit message mentions this closes #7464, apparently this didn't work automatically. Should I close there as well or am In missing something?
</comment><comment author="jpountz" created="2015-11-20T15:51:46Z" id="158439219">Indeed! I just closed the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What port used to remote debugging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14772</link><project id="" key="" /><description>From the office doc(https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc), 

&gt; Debugging from an IDE
&gt; 
&gt; If you want to run elasticsearch from your IDE, you should execute gradle run It opens a remote debugging port that you can connect with your IDE.

I found the only to debug ES is execute `gradle run`, and invoke remote debug in IDE. But, it didn't tell about what the port open, or how to set a port to debug.
</description><key id="117117488">14772</key><summary>What port used to remote debugging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feuyeux</reporter><labels /><created>2015-11-16T12:54:36Z</created><updated>2016-02-24T03:21:19Z</updated><resolved>2015-11-18T01:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-17T21:50:11Z" id="157520138">@rjernst does `gradle run` still open a debugging port the way that `./run.sh` did, or do we need to update the documentation? (or maybe it needs to print the port)
</comment><comment author="rjernst" created="2015-11-17T21:57:08Z" id="157521934">It does not at the moment. I think we should add a `--debug` option to it, and print the info to add to your debugger (as well as configure the server to suspend until the debugger is connected).
</comment><comment author="dakrone" created="2015-11-17T22:04:02Z" id="157523589">&gt; I think we should add a --debug option to it, and print the info to add to your debugger (as well as configure the server to suspend until the debugger is connected).

+1
</comment><comment author="whateverblake" created="2016-02-24T02:59:06Z" id="188033890">does es has options like hadoop has  that i can use eclipse debug it 
</comment><comment author="whateverblake" created="2016-02-24T03:21:19Z" id="188041081">i have figure it out . add parameter  -D "-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=port" when you start elastic
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Order not working inside a date_histogram aggs ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14771</link><project id="" key="" /><description>POST http://localhost:9200/searchaggtests/searchaggtest/_search?&amp;search_type=count HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 289
Expect: 100-continue

{"aggs":{"testHistogramBucketAggregation":{"date_histogram":{"field":"dateofdetails","interval":"1M","format":"yyyy-MM-dd","offset":"1d","time_zone":"+02:00","order":{"dateofdetails":"desc"},"min_doc_count":2,"extended_bounds":{"min":0,"max":1000000}},"aggs":{"tophits":{"top_hits":{}}}}}}
## Result:

HTTP/1.1 503 Service Unavailable
Content-Type: application/json; charset=UTF-8
Content-Length: 293

{"error":{"root_cause":[],"type":"reduce_search_phase_exception","reason":"[reduce] ","phase":"query","grouped":true,"failed_shards":[],"caused_by":{"type":"illegal_argument_exception","reason":"Invalid order path [dateofdetails]. Cannot find aggregation named [dateofdetails]"}},"status":503}

Greetings Damien
</description><key id="117015586">14771</key><summary>Order not working inside a date_histogram aggs ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbod</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-15T20:29:58Z</created><updated>2017-05-14T02:44:45Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T13:24:39Z" id="157712121">This is weird for a few reasons...  
- you can't sort on fields, only on the result of aggregations so the exception is correct
- it shouldn't be returning a 503 response though
- when I try it (full example below) on 2.0, I don't get this exception, which i should
  
  POST _bulk
  {"index":{"_index":"searchtests","_type":"searchtest","_id":1}}
  {"id":1,"name":"one","details":"This is the details of the document, very interesting","circletest":{"type":"circle","coordinates":[45,45],"radius":"100m"},"location":[45,45],"lift":2.9,"dateofdetails":"2015-10-26T19:19:38.5961045Z"}
  {"index":{"_index":"searchtests","_type":"searchtest","_id":2}}
  {"id":2,"name":"two","details":"Details of the document two, leave it alone","circletest":{"type":"circle","coordinates":[46,45],"radius":"50m"},"location":[46,45],"lift":2.5,"dateofdetails":"2015-04-20T19:19:38.5961045Z"}
  {"index":{"_index":"searchtests","_type":"searchtest","_id":3}}
  {"id":3,"name":"three","details":"This data is different","circletest":{"type":"circle","coordinates":[37,42],"radius":"80m"},"location":[37,42],"lift":2.1,"dateofdetails":"2015-10-12T19:19:38.5961045Z"}

Full example:

```
POST searchtests/_search?search_type=count
{
  "aggs": {
    "testHistogramBucketAggregation": {
      "date_histogram": {
        "field": "dateofdetails",
        "interval": "1M",
        "format": "yyyy-MM-dd",
        "offset": "1d",
        "time_zone": "+02:00",
        "order": {
          "dateofdetails": "desc"
        },
        "min_doc_count": 2,
        "extended_bounds": {
          "min": 0,
          "max": 1000000
        }
      },
      "aggs": {
        "tophits": {
          "top_hits": {}
        }
      }
    }
  }
}
```

@colings86 any ideas?
</comment><comment author="colings86" created="2015-11-18T14:15:21Z" id="157725324">I get the same as @clintongormley.

This is a bug because an exception should be thrown and the status code returned should be 400 (bad request).
</comment><comment author="qwerty4030" created="2017-05-14T02:43:55Z" id="301287230">As of #22343 `histogram` and `date_histogram` aggregation order will now be validated during the shard search phase. We now get the same validation errors as the `terms` agg when an invalid sub-aggregation is given in the order.
```
PUT i

POST i/_search
{
  "aggs": {
    "a": {
      "date_histogram": {
        "field": "f", 
        "interval": "day", 
        "order": {
          "wrong": "asc"
        }
      }
    }
  }
}

response:
{
  "error": {
    "root_cause": [
      {
        "type": "aggregation_execution_exception",
        "reason": "Invalid aggregator order path [wrong]. Unknown aggregation [wrong]"
      }
    ],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [
      {
        "shard": 0,
        "index": "i",
        "node": "jUa3xSiEQTayFeYCYHt2jg",
        "reason": {
          "type": "aggregation_execution_exception",
          "reason": "Invalid aggregator order path [wrong]. Unknown aggregation [wrong]"
        }
      }
    ]
  },
  "status": 500
}
```
The `order` code was refactored in #22343 so it will now be easier to move order validation for all aggregations to the query parsing phase as a follow up PR.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: move SearchQueryIT back to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14770</link><project id="" key="" /><description>There is no groovy dependency there (it's now in lang groovy)
</description><key id="117014538">14770</key><summary>Test: move SearchQueryIT back to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-15T20:09:43Z</created><updated>2015-11-16T14:59:02Z</updated><resolved>2015-11-16T13:48:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-15T21:39:10Z" id="156856647">LGTM. For the record, it looks like at the time the test was moved with the groovy refactoring, `testMinScore()` used a script. It has since been refactored to use a `field_value_factor` function.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Execution Mode in Terms filter ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14769</link><project id="" key="" /><description>What happened the execution mode in the terms filter in es 2.0? It runs still but does not effect the search result.
Greetings Damien
</description><key id="117012762">14769</key><summary>Execution Mode in Terms filter ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbod</reporter><labels /><created>2015-11-15T19:37:25Z</created><updated>2015-11-18T13:16:58Z</updated><resolved>2015-11-18T13:16:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T13:16:11Z" id="157709426">Essentially all filters are now really queries.  For bwc, we don't throw an exception on execution mode, but it is ignored as it is no longer supported.
</comment><comment author="clintongormley" created="2015-11-18T13:16:58Z" id="157709561">See https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_query_dsl_changes.html#_queries_and_filters_merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>max_determinized_states in regexp filter not working ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14768</link><project id="" key="" /><description>If I set the max_determinized_states in a regexp filter, the search result is incorrect. Without this set, the filter returns the correct amount of Hits:
## Test data

POST http://localhost:9200/_bulk HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 877
Expect: 100-continue

{"index":{"_index":"searchtests","_type":"searchtest","_id":1}}
{"id":1,"name":"one","details":"This is the details of the document, very interesting","circletest":{"type":"circle","coordinates":[45.0,45.0],"radius":"100m"},"location":[45.0,45.0],"lift":2.9,"dateofdetails":"2015-10-26T19:19:38.5961045Z"}
{"index":{"_index":"searchtests","_type":"searchtest","_id":2}}
{"id":2,"name":"two","details":"Details of the document two, leave it alone","circletest":{"type":"circle","coordinates":[46.0,45.0],"radius":"50m"},"location":[46.0,45.0],"lift":2.5,"dateofdetails":"2015-04-20T19:19:38.5961045Z"}
{"index":{"_index":"searchtests","_type":"searchtest","_id":3}}
{"id":3,"name":"three","details":"This data is different","circletest":{"type":"circle","coordinates":[37.0,42.0],"radius":"80m"},"location":[37.0,42.0],"lift":2.1,"dateofdetails":"2015-10-12T19:19:38.5961045Z"}
## Incorrect

POST http://localhost:9200/searchtests/searchtest/_search HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 78
Expect: 100-continue

{"filter":{"regexp":{"name":{"value":"o.*"},"max_determinized_states":20000}}}
## OK

POST http://localhost:9200/searchtests/searchtest/_search HTTP/1.1
Content-Type: application/json
Host: localhost:9200
Content-Length: 46
Expect: 100-continue

{"filter":{"regexp":{"name":{"value":"o.*"}}}}

---

Greetings Damien
</description><key id="117012341">14768</key><summary>max_determinized_states in regexp filter not working ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbod</reporter><labels /><created>2015-11-15T19:26:31Z</created><updated>2015-11-18T13:15:07Z</updated><resolved>2015-11-18T13:15:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T13:15:07Z" id="157709242">You're specifying `max_determinized_states` at the wrong level.  It should be at the same level as `value`.  In 3.0 the parsing has been improved to throw an exception instead of silently swallowing the error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.5 - relocate on local disks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14767</link><project id="" key="" /><description>Hi,

I have multiple ES (1.5) servers with multiple disks.
I have added an additional disk to all of the servers and i want ES to relocate the shards so that the new disk will take an equal % of current storage usage.

For example:
before:
disk1 - 75%
disk2 - 75%

after:
disk1 - 50%
disk2 - 50%
disk3 - 50%

Preferably by smartly moving shards (I.E not have multiple replicas on same disk, moving equal # of primaries to all disks).

Is that possible?
</description><key id="116985449">14767</key><summary>1.5 - relocate on local disks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaharmor</reporter><labels /><created>2015-11-15T10:26:48Z</created><updated>2015-11-15T11:06:57Z</updated><resolved>2015-11-15T11:06:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-15T11:06:57Z" id="156802428">Please keep your question on discuss only: https://discuss.elastic.co/t/relocated-shard-in-local-disks/34612

This space is for issues only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add javadocs jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14766</link><project id="" key="" /><description>This change adds javadoc jars to core, test-framework and plugins. There
were a couple issues which javadoc found, but doclint did not already
find.
</description><key id="116983543">14766</key><summary>Add javadocs jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-15T09:49:45Z</created><updated>2015-11-20T20:21:42Z</updated><resolved>2015-11-20T20:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-15T13:56:36Z" id="156813441">Lgtm. The command line changes you made just make it ignore the missing docs, right?  I also see quiet. If there are docs warnings are they just eaten?
</comment><comment author="rmuir" created="2015-11-15T17:28:09Z" id="156835361">Why is the javac compilation not failing right now? Something in the build is broken there.
</comment><comment author="rmuir" created="2015-11-15T17:28:40Z" id="156835394">I suspect its gradle's non-forking compiler. Can we just fork an actual call to 'javac'? Then we know it works.
</comment><comment author="rjernst" created="2015-11-16T17:20:03Z" id="157105502">I created another PR to use forking for javac: #14779. However, it does not cause failures from doclint. Since we have doclint enabled in maven, and maven did not catch the issues I found here, it must be issues with doclint itself?
</comment><comment author="rjernst" created="2015-11-19T19:16:12Z" id="158162352">@nik9000 I added a comment explaining the -quiet. It was tricky to find (through lots of web searching) and is a workaround for what seems like a bug in gradle (the addStringOption method with a single parameter doesn't seem to work).
</comment><comment author="nik9000" created="2015-11-20T16:49:44Z" id="158457197">Fine by me. I left a comment about maybe filing an upstream bug and adding it to the comments but LGTM either way.
</comment><comment author="rjernst" created="2015-11-20T20:21:37Z" id="158514524">I added a comment referencing a gradle discuss thread on the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix calculation of next delay for delayed shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14765</link><project id="" key="" /><description>Currently the next delay is calculated based on System.currentTimeMillis() but the actual shards to delay based on the last time the GatewayAllocator tried to assign/delay the shard.

This introduces an inconsistency for the case where shards should have been delay-allocated between the GatewayAllocator-based timestamp and System.currentTimeMillis().

Failing test highlighting the issue:

http://build-us-00.elastic.co/job/es_core_master_centos/8512/testReport/junit/org.elasticsearch.cluster.routing/RoutingServiceTests/testDelayedUnassignedScheduleRerouteAfterDelayedReroute/

Relevant lines of log output:

```
[2015-11-13 19:25:05,559][DEBUG][org.elasticsearch.cluster.routing.allocation] [short_delay][0] failed shard [short_delay][0], node[node3], [R], v[3], s[STARTED], a[id=FBy1b6neT6yNj4McTu6jlg] found in routingNodes, failing it ([reason=NODE_LEFT], at[2015-11-13T18:25:05.559Z], details[node_left[node3]])
[2015-11-13 19:25:05,559][DEBUG][org.elasticsearch.cluster.routing.allocation] [long_delay][0] failed shard [long_delay][0], node[node1], [R], v[3], s[STARTED], a[id=owLPLgX8RB-ssPs-xvQyQg] found in routingNodes, failing it ([reason=NODE_LEFT], at[2015-11-13T18:25:05.559Z], details[node_left[node1]])
[2015-11-13 19:25:05,877][INFO ][org.elasticsearch.cluster.routing] delaying allocation for [2] unassigned shards, next check in [9.6s]
```

Analysis:
Two shards fail, one with a delay configured to 100ms ([short_delay][0]), the other configured to 10s ([long_delay][0]). 
The last line of log shows that 2 shards have been correctly recognised as being delay-unassigned. But, as the last log line has been executed more than 100ms after the node failure, only long_delay is taken into account for calculating the next check. 
</description><key id="116980820">14765</key><summary>Fix calculation of next delay for delayed shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v1.7.4</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-15T08:39:46Z</created><updated>2015-11-16T14:09:18Z</updated><resolved>2015-11-16T13:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-16T12:58:53Z" id="157021679">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Override Build-Date in jar manifest to be ISO 8601</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14764</link><project id="" key="" /><description>The gradle info plugin outputs the build date in an arbitrary date format. This change overrides it to use ISO 8601.
</description><key id="116977199">14764</key><summary>Override Build-Date in jar manifest to be ISO 8601</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-15T06:40:27Z</created><updated>2015-11-16T16:39:41Z</updated><resolved>2015-11-16T16:39:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T07:31:18Z" id="156946228">LGTM
</comment><comment author="nik9000" created="2015-11-16T15:00:23Z" id="157059658">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0 won't start - es.default.config is no longer supported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14763</link><project id="" key="" /><description>OS: Ubuntu 15.04
ES Version:  Upgrade from 2.0 Beta2 to 2.0 GA  from repo.

Full log error:
[2015-11-14 16:28:46,587][INFO ][bootstrap                ] es.default.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.

I searched for previous issues and found that the CONF_FILE variable was eliminated in the latest release and only CONF_DIR is to be used.  

I verified that I'm using the latest /etc/init.d/elasticsearch startup script, with no CONF_FILE defined:

```
root@flowmaster1:/etc/elasticsearch# grep -i conf /etc/init.d/elasticsearch
# Elasticsearch configuration directory
CONF_DIR=/etc/$NAME
# CONF_FILE setting was removed
if [ ! -z "$CONF_FILE" ]; then
    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config     directory and cannot be renamed."
DAEMON_OPTS="-d -p $PID_FILE --default.path.home=$ES_HOME     --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf= CONF_DIR"
```

I verified my /etc/default/elasticsearch  variable file has CONF_DIR line commented out:

```
root@flowmaster1:/etc/elasticsearch# grep -i conf /etc/default/elasticsearch
# Elasticsearch configuration directory
#CONF_DIR=/etc/elasticsearch
# Elasticsearch configuration file
#CONF_FILE=$CONF_DIR/elasticsearch.yml
# Configure restart on package upgrade (true, every other setting will lead to not restarting)
# the configured user can read and write into the data, work, plugins and log directories.
# For systemd service, the user is usually configured in file 
/usr/lib/systemd/system/elasticsearch.service
#     property is set at boot time in /usr/lib/sysctl.d/elasticsearch.conf
```

I don't see any reference to CONF_FILE or es.default.config anywhere in /etc.  I'm at a loss as to why this is failing.   I tried setting the CONF_DIR in both the startup script and the default variable file and no joy.
</description><key id="116938305">14763</key><summary>2.0 won't start - es.default.config is no longer supported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnarnold</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-11-14T17:10:43Z</created><updated>2016-12-16T20:46:06Z</updated><resolved>2016-02-14T16:59:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johnarnold" created="2015-11-14T17:23:45Z" id="156721550">I found root cause -- the CONF_FILE references were in /usr/lib/systemd/system/elasticsearch.service  (and thus why they didn't show up when searching /etc)

When upgrading from beta2 to GA, the elasticsearch.service file did not get overwritten, instead a secondary file was created:   elasticsearch.service.dpkg-dist

copying this over elasticsearch.service file allows the service to start.

I used ansible apt module to install 2.0 package -- is there an apt option to force the files to the right places?
</comment><comment author="johnarnold" created="2015-11-14T17:41:43Z" id="156723834">Also, confirmed that removing the -old- /usr/lib/system/system/elasticsearch.service and leaving the new elasticsearch.service.dpkg-dist in place works too.  So probably the pre-inst script for the debian packaging should remove the old elasticsearch.service file if it exists.
</comment><comment author="johnarnold" created="2015-11-16T18:47:34Z" id="157132804">I also discovered that with "interactive" install, apt package manager prompts to overwrite the .service file -- it's just on non-interactive that it defaults to creating a new one which is not used.  E.g. via Ansible apt module.  I think defaulting to overwrite the .service file may be a better option for most users.
</comment><comment author="clintongormley" created="2016-02-14T16:59:05Z" id="183923858">I don't think we should overwrite a file that has been edited, but the real problem is that, today, we require you to edit the `elasticsearch.service` file to uncomment `LimitMEMLOCK`.  Instead we should change the docs to recommend using a custom config file as suggested here: https://github.com/elastic/elasticsearch/issues/9357#issuecomment-172880212

Closing in favour of #9357
</comment><comment author="PhaedrusTheGreek" created="2016-12-16T20:46:06Z" id="267691873">As redundant as it might be to say - It's worth noting that it's the ` --default.config` flag that causes this on the CLI.  e.g.:

```
$ bin/elasticsearch --default.config=/etc/elasticsearch.yml
[2016-12-16 15:42:37,143][INFO ][bootstrap                ] es.default.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
```</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update gradle version from 2.6 to 2.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14762</link><project id="" key="" /><description>Changed the gradle version from 2.6 to 2.8

Closes #14761
</description><key id="116934960">14762</key><summary>update gradle version from 2.6 to 2.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvalleru</reporter><labels><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-11-14T16:13:21Z</created><updated>2015-11-14T18:07:29Z</updated><resolved>2015-11-14T18:07:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-14T18:07:12Z" id="156729526">Good catch! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update docs README.textile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14761</link><project id="" key="" /><description>Need to update https://github.com/elastic/elasticsearch#building-from-source minimum gradle version to 2.8
</description><key id="116934616">14761</key><summary>Update docs README.textile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvalleru</reporter><labels /><created>2015-11-14T16:06:22Z</created><updated>2015-11-14T18:07:29Z</updated><resolved>2015-11-14T18:07:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Transport options should be immutable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14760</link><project id="" key="" /><description>This commit changes TransportRequestOptions and TransportResponseOptions
to be immutable. This is to address an issue where the empty options
were being mutated permanently altering their state. Making these
objects immutable is just good, clean coding.
</description><key id="116893642">14760</key><summary>Transport options should be immutable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-14T04:01:53Z</created><updated>2015-11-15T01:05:01Z</updated><resolved>2015-11-14T23:29:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-14T04:54:22Z" id="156628468">LGTM
</comment><comment author="bleskes" created="2015-11-14T09:53:10Z" id="156679682">+1 on the initiative. Left minor comments
</comment><comment author="jasontedor" created="2015-11-14T13:05:04Z" id="156694310">@bleskes We are mutating the empty options today. Starting at [`TransportRequestOptions#sendRequest(DiscoveryNode, String, TransportRequest, TransportResponseHandler)`](https://github.com/elastic/elasticsearch/blob/d082868436a59b3a9c39a923a17f296d5f866f7d/core/src/main/java/org/elasticsearch/transport/TransportService.java#L280-L281), this proceeds to invoke [`TransportRequestOptions#sendRequest(DiscoveryNode, String, TransportRequest, TransportRequestOptions, TransportResponseHandler)`](https://github.com/elastic/elasticsearch/blob/d082868436a59b3a9c39a923a17f296d5f866f7d/core/src/main/java/org/elasticsearch/transport/TransportService.java#L285-L286) [passing in `TransportRequestOptions.EMPTY`](https://github.com/elastic/elasticsearch/blob/d082868436a59b3a9c39a923a17f296d5f866f7d/core/src/main/java/org/elasticsearch/transport/TransportService.java#L282). This method then invokes [`Transport#sendRequest(DiscoveryNode, long, String, TransportRequest, TransportRequestOptions)`](https://github.com/elastic/elasticsearch/blob/d082868436a59b3a9c39a923a17f296d5f866f7d/core/src/main/java/org/elasticsearch/transport/TransportService.java#L312) on the underlying transport. If the underlying transport is the `NettyTransport` we end up at the [fatal lines](https://github.com/elastic/elasticsearch/blob/d082868436a59b3a9c39a923a17f296d5f866f7d/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java#L814-L816):

``` java
if (compress) {
    options.withCompress(true);
}
```

Thus, if `transport.tcp.compress` is true, the `EMPTY` options are mutated by any send request that starts at the above chain. One request that starts at the above chain is [join pings from ZenDiscovery](https://github.com/elastic/elasticsearch/blob/d082868436a59b3a9c39a923a17f296d5f866f7d/core/src/main/java/org/elasticsearch/discovery/zen/membership/MembershipAction.java#L96).
</comment><comment author="jasontedor" created="2015-11-15T01:04:30Z" id="156768497">Thank you for reviewing @dakrone and @bleskes. This is integrated into [master](https://github.com/elastic/elasticsearch/commit/c09103c35bfe512f8dd83752451dda2893ed7e8e) and the [2.x](https://github.com/elastic/elasticsearch/commit/7c06afe27d8bd934dbd652158b42ff5c4949537a) branch for inclusion in 2.2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adds tests and guards against null values in some mutate methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14759</link><project id="" key="" /><description /><key id="116872674">14759</key><summary>adds tests and guards against null values in some mutate methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-13T23:14:47Z</created><updated>2015-11-16T09:32:32Z</updated><resolved>2015-11-16T09:32:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-16T09:25:46Z" id="156966097">LGTM thanks @talevy 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.0 deleteByQuery Plugin ignore routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14758</link><project id="" key="" /><description>Have index on two shards with two documents

```
curl 172.17.0.2:9200/_cat/shards?v
index                shard prirep state   docs store ip         node     
delete_by_query_test 1     p      STARTED    2 6.7kb 172.17.0.2 Elastica 
delete_by_query_test 0     p      STARTED    0  127b 172.17.0.2 Elastica 
```

Try to deleteByQuery with routing=2

```
curl -XDELETE 'http://172.17.0.2:9200/delete_by_query_test/_query?q=test&amp;routing=2'
{"took":0,"timed_out":false,"_indices":{"_all":{"found":1,"deleted":1,"missing":0,"failed":0},"delete_by_query_test":{"found":1,"deleted":1,"missing":0,"failed":0}},"failures":[]}
```

Shards after delete:

```
index                shard prirep state   docs store ip         node     
delete_by_query_test 1     p      STARTED    1 3.4kb 172.17.0.2 Elastica 
delete_by_query_test 0     p      STARTED    0  127b 172.17.0.2 Elastica 
```

There is no shard 2, seems that routing was just ignored. Can anybody confirm that it is a bug? Or I have wrong expectation?

Gist with curl commands: https://gist.github.com/ewgRa/ed66d809e4fa2fd8573a

Info:

```
curl 172.17.0.2:9200/
{
  "name" : "Elastica",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.0.0",
    "build_hash" : "de54438d6af8f9340d50c5c786151783ce7d6be5",
    "build_timestamp" : "2015-10-22T08:09:48Z",
    "build_snapshot" : false,
    "lucene_version" : "5.2.1"
  },
  "tagline" : "You Know, for Search"
}
```

```
curl 172.17.0.2:9200/_cat/plugins?v
name     component                        version type url 
Elastica delete-by-query                  2.0.0   j        
Elastica elasticsearch-mapper-attachments 3.0.2   j        
```
</description><key id="116866786">14758</key><summary>ES 2.0 deleteByQuery Plugin ignore routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ewgRa</reporter><labels /><created>2015-11-13T22:32:35Z</created><updated>2015-11-16T13:03:30Z</updated><resolved>2015-11-16T13:03:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-11-16T13:03:30Z" id="157022694">&gt; There is no shard 2, seems that routing was just ignored. Can anybody confirm that it is a bug? Or I have wrong expectation?

I think you have wrong expectation: the `routing` parameter is not supposed to be used to indicate which shard id you want to target. Instead, it can be used as index time to pass an arbitrary value that will be used to determine in which shard the document will be indexed (see [Routing a document to a shard](https://www.elastic.co/guide/en/elasticsearch/guide/current/routing-value.html) in The Definitive Guide). 

Then, various API accept the `routing` parameter in order to limit the number of shards that will be queried to find the document.

In case I'm not clear, here a sample how to use routing with Delete-By-Query plugin:

```
# creating index with 2 shards and 0 replicas
curl -XPUT 'http://localhost:9200/books' -d '{
    "settings" : {
        "number_of_shards" : 2,
        "number_of_replicas" : 0
    }
}'

# Indexing a first document with routing "key1"
curl -XPOST 'http://localhost:9200/books/book/1?routing=key1' -d '{"author":"Hemingway"}'

# Indexing a second document with routing "key2"
curl -XPOST 'http://localhost:9200/books/book/2?routing=key2' -d '{"author":"Twain"}'

# Refresh
curl -XGET 'http://localhost:9200/books/_refresh'

# Deleting Hemingway with the routing value "key2"
curl -XDELETE 'http://localhost:9200/books/_query?routing=key2' -d '{"query":{"match":{"author":"Hemingway"}}}'

# No documents deleted
{"took":0,"timed_out":false,"_indices":{"_all":{"found":0,"deleted":0,"missing":0,"failed":0}},"failures":[]}

# Deleting Hemingway with the right routing value "key1"
curl -XDELETE 'http://localhost:9200/books/_query?routing=key1' -d '{"query":{"match":{"author":"Hemingway"}}}'

# 1 document deleted
{"took":0,"timed_out":false,"_indices":{"_all":{"found":1,"deleted":1,"missing":0,"failed":0},"books":{"found":1,"deleted":1,"missing":0,"failed":0}},"failures":[]}
```

Note that the [Search Shards API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-shards.html?q=search%20shard) is very useful to know the indices &amp; shards a search request would be executed against. 

I'm closing this for now since I don't think there's a real issue. Please feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log failures immediately</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14757</link><project id="" key="" /><description>With this change all failures are immediately logged in gory detail.
</description><key id="116864012">14757</key><summary>Log failures immediately</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>discuss</label><label>review</label></labels><created>2015-11-13T22:14:32Z</created><updated>2015-11-29T11:52:31Z</updated><resolved>2015-11-29T01:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-13T22:16:04Z" id="156574323">This patch is almost certainly wrong but without it its you have to wait for the whole build to finish if something fails. If you break the tests so all of them fail _slowly_ then you have to wait a really, really long time to see the first failure.
</comment><comment author="rjernst" created="2015-11-14T06:05:45Z" id="156642285">Im not sure about this as is, and it at least needs a tweek to the logic. When we are info logging, the logic is correct, because all the output for a suite should be together (this is the same we had in maven). But I do see the advantage to getting error output faster, so if we limit this by reverting but changing the first if condition to take info logging disabled into account, then I think we will be ok.
</comment><comment author="nik9000" created="2015-11-29T01:23:13Z" id="160353723">I think we're ok here now that we get the failure count increasing. We at least know is happening. Most suites are so large this is a problem. We can reopen if we need.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't leak clients from ExternalTestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14756</link><project id="" key="" /><description>When the ExternalTestCluster can't fully build it was leaking a client.
These clients created several threads each, causing "fun" thread starvation
issues.
</description><key id="116863640">14756</key><summary>Don't leak clients from ExternalTestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T22:12:11Z</created><updated>2015-11-16T13:54:06Z</updated><resolved>2015-11-16T13:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-14T05:58:45Z" id="156641328">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Effectively remove transitive deps from generated maven poms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14755</link><project id="" key="" /><description>With gradle, deploying to maven means first generating poms. These are
filled in based on dependencies of the project. Recently, we started
disallowing transitive dependencies. However, this configuration does
not translate to maven poms because maven has no concept of excluding
all transitive dependencies.

This change adds exclusions for each of the transitive deps of each
dependency being added to the maven pom. It does so by creating dummy
configurations for each direct dependency (which does not have
transitive deps excluded), so that we can iterate the transitive deps
when building the pom.

Note, this should be simpler (just modifying maven's pom model), but
gradle tries to hide that from their api, causing us to need to
manipulate the xml directly.
https://discuss.gradle.org/t/modifying-maven-pom-generation-to-add-excludes/12744
</description><key id="116863433">14755</key><summary>Effectively remove transitive deps from generated maven poms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T22:11:01Z</created><updated>2015-11-13T23:06:14Z</updated><resolved>2015-11-13T23:06:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-13T22:37:07Z" id="156579143">OK - I get it. Just add some more documentation so the next poor bastard who has to hack on this has some clue. Its... unpleasant but required. But, yeah, just a big block comment explaining the hack....
</comment><comment author="nik9000" created="2015-11-13T22:37:16Z" id="156579164">LGTM if you add the comment
</comment><comment author="rjernst" created="2015-11-13T23:01:12Z" id="156582946">@nik9000 Thanks for the review. If you woudln't mind, could you look over the explanation I added? I want to make sure it actually makes sense, since my head is still spinning from maven/gradle/groovy/xml madness.
</comment><comment author="nik9000" created="2015-11-13T23:04:23Z" id="156583482">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Build work without git</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14754</link><project id="" key="" /><description>If you build elasticsearch without a git repository it was creating a null
shortHash which was causing Elasticsearch not to be able to form transport
connections.

Closes #14748
</description><key id="116863225">14754</key><summary>Make Build work without git</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T22:09:20Z</created><updated>2015-11-17T18:30:23Z</updated><resolved>2015-11-17T18:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-13T22:09:30Z" id="156572783">@jasontedor, would you like to look at this one?
</comment><comment author="rjernst" created="2015-11-14T05:56:04Z" id="156641081">Why would you have elasticsearch outside of git? Is this really something we need to support? That just means more leniency, but what do we gain as a  tradeoff?
</comment><comment author="nik9000" created="2015-11-14T12:43:24Z" id="156693415">Strictness is only good if you fail quickly and informatively. Right now
what we have is brittle because the build doesnt fail until the rest tests
and the failure message doesn't mention git. Something has got to change.

I'm happy to add a test that will fail earlier. I should have do that with
the first version of the patch.

If we want to require the git directory we really ought to do that at build
time. I never thought about going that route because it just seemed
backwards. I'm uncomfortable but could live with requiring git if we do it
at build time.

I build elasticsearch without a .git directory on one machine. I checkout
and edit the code on a slowish laptop and I sync it to a fast desktop with
unison. I don't sync .git because its large and binary and slow to sync.
It's a much smoother, faster process than coordinating the two nodes with
git.
On Nov 14, 2015 12:56 AM, "Ryan Ernst" notifications@github.com wrote:

&gt; Why would you have elasticsearch outside of git? Is this really something
&gt; we need to support? That just means more leniency, but what do we gain as a
&gt; tradeoff?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14754#issuecomment-156641081
&gt; .
</comment><comment author="dakrone" created="2015-11-15T20:23:59Z" id="156850114">&gt; Why would you have elasticsearch outside of git? Is this really something we need to support?

I can absolutely see a need for this, anyone that downloads ES from the [releases](https://github.com/elastic/elasticsearch/releases) page on github won't have a .git directory, but should still be able to build the project.
</comment><comment author="rjernst" created="2015-11-15T22:13:23Z" id="156862809">@dakrone You are right, we should make that work. However, it should not show "Unknown" for the build? We can override this when building the jar so it still has a build hash (which can be some arbitrary text stating this was a source release build?). Build date should still be there though (as of now).
</comment><comment author="dakrone" created="2015-11-16T05:47:25Z" id="156922482">&gt; However, it should not show "Unknown" for the build?

Personally I like `_n/a_`, but that's just me.
</comment><comment author="rjernst" created="2015-11-16T05:49:54Z" id="156922698">I'm fine with n/a, but my point is we should do this in the build, not make the code reading it more lenient. 
</comment><comment author="dakrone" created="2015-11-16T05:52:14Z" id="156922887">Oh sure, I'm totally fine with that also, I misunderstood what you were saying. Regardless of where the build writes a file however (or whatever it will end up doing), ES should still be able to form transport connections.
</comment><comment author="nik9000" created="2015-11-16T13:55:21Z" id="157033907">&gt; I'm fine with n/a, but my point is we should do this in the build, not make the code reading it more lenient.

Ok. I'll do that then.
</comment><comment author="nik9000" created="2015-11-16T20:55:00Z" id="157167229">&gt; Ok. I'll do that then.

I had a look at the releases - they just look like a tarball of the source sans the git directory. It really isn't possible to get the commit hash here.
</comment><comment author="rjernst" created="2015-11-16T21:14:34Z" id="157172966">We don't need the git hash. We can just put "n/a" like Lee suggested.
</comment><comment author="nik9000" created="2015-11-16T21:19:28Z" id="157174142">&gt; We don't need the git hash. We can just put "n/a" like Lee suggested.

Right. I just wanted to see if it was possible. Its not. I'll update the PR with N/A in a minute.
</comment><comment author="rjernst" created="2015-11-16T22:35:49Z" id="157193222">@nik9000 I think you misunderstood my suggestion. We can fix this in the jar itself, then no changes are necessary in Build.java.

If you look at BuildPlugin.configureJarManifest in buildSrc, you can add something like the following:

```
if (jarTask.manifest.attributes.containsKey('Change') == false) {
    jarTask.manifest.attributes('Change': 'N/A')
}
```
</comment><comment author="nik9000" created="2015-11-16T22:41:04Z" id="157194490">Yeah - I didn't think about doing it that way. You'd still need to change
Build.java so it'd complain on startup if it can't find the `Change` but
it'd be smaller. I'm not what your proposing is better than what I did
though.

On Mon, Nov 16, 2015 at 5:35 PM, Ryan Ernst notifications@github.com
wrote:

&gt; @nik9000 https://github.com/nik9000 I think you misunderstood my
&gt; suggestion. We can fix this in the jar itself, then no changes are
&gt; necessary in Build.java.
&gt; 
&gt; If you look at BuildPlugin.configureJarManifest in buildSrc, you can add
&gt; something like the following:
&gt; 
&gt; if (jarTask.manifest.attributes.containsKey('Change') == false) {
&gt;     jarTask.manifest.attributes('Change': 'N/A')
&gt; }
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14754#issuecomment-157193222
&gt; .
</comment><comment author="rjernst" created="2015-11-16T22:44:37Z" id="157196722">I don't see why any changes would be necessary in Build.java. We should fail hard if the jar does not have these values, no need to be nice about it. My proposed change adds "Change" when it is not there, and "Build-Date" is already always added to the jar.  The only reason leniency exists at all in Build.java is for tests.
</comment><comment author="nik9000" created="2015-11-16T22:46:24Z" id="157197639">We need to change Build.java so that when elasticsearch fails without that
values it fails with a useful error message.

On Mon, Nov 16, 2015 at 5:44 PM, Ryan Ernst notifications@github.com
wrote:

&gt; I don't see why any changes would be necessary in Build.java. We should
&gt; fail hard if the jar does not have these values, no need to be nice about
&gt; it. My proposed change adds "Change" when it is not there, and "Build-Date"
&gt; is already always added to the jar. The only reason leniency exists at all
&gt; in Build.java is for tests.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14754#issuecomment-157196722
&gt; .
</comment><comment author="rjernst" created="2015-11-16T22:48:12Z" id="157198432">I don't see why we need a useful error message for something that is a build bug, but I don't care I guess. LGTM if the comment about tests are changed back.
</comment><comment author="rjernst" created="2015-11-16T22:52:55Z" id="157200445">Actually, thinking about it more, I think doing the N/A part in the jar is better. As it is now, if I were to just remove the git info plugin for gradle, the build would happily proceed, and we would get "N/A" in Build.java. With us keeping it always in the jar, we continue to be able to rely on it, and fail if it isn't there.
</comment><comment author="nik9000" created="2015-11-16T23:14:30Z" id="157204910">That makes sense. I'll move it to the build.
On Nov 16, 2015 5:53 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; Actually, thinking about it more, I think doing the N/A part in the jar is
&gt; better. As it is now, if I were to just remove the git info plugin for
&gt; gradle, the build would happily proceed, and we would get "N/A". With us
&gt; keeping it always in the jar, we continue to be able to rely on it, and
&gt; fail if it isn't there.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14754#issuecomment-157200445
&gt; .
</comment><comment author="nik9000" created="2015-11-17T15:17:30Z" id="157400388">I've moved this to the build but I think I can do better....
</comment><comment author="nik9000" created="2015-11-17T15:36:22Z" id="157406071">OK - I think this is better. @rjernst, ready for review.
</comment><comment author="rjernst" created="2015-11-17T16:56:00Z" id="157430158">@nik9000 Are you synced with master? If you sync, then the 2 lines I suggested should work.
</comment><comment author="nik9000" created="2015-11-17T18:25:57Z" id="157460221">&gt; @nik9000 Are you synced with master? If you sync, then the 2 lines I suggested should work.

@rjernst, rebased and reworked.
</comment><comment author="rjernst" created="2015-11-17T18:28:31Z" id="157460873">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update progress logger after each test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14753</link><project id="" key="" /><description>This makes the rest tests **tons** more responsive.
</description><key id="116862411">14753</key><summary>Update progress logger after each test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T22:03:38Z</created><updated>2015-11-17T18:16:56Z</updated><resolved>2015-11-17T18:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-13T22:03:50Z" id="156571828">@rjernst, want to review?
</comment><comment author="nik9000" created="2015-11-16T21:03:26Z" id="157169460">@rjernst updated with some of you comments. Its not entirely what we discussed this morning but its still good.
</comment><comment author="rjernst" created="2015-11-16T21:24:06Z" id="157175166">This looks just as complicated. Why cant we have a simple setting to cause us to add the test name to the progress? We can enable this for rest tests.
</comment><comment author="nik9000" created="2015-11-16T21:37:32Z" id="157178358">&gt; This looks just as complicated. Why cant we have a simple setting to cause us to add the test name to the progress? We can enable this for rest tests.

I can't think of a way that that is actually simpler than this. I can add a setting but what would that let me take out?
</comment><comment author="nik9000" created="2015-11-17T14:09:17Z" id="157379375">@rjernst - I think this is ready for another review. I tried to simplify it. It still doesn't use a setting for the method name. What its doing now seems simple enough. I kept skipping the JVM id if there is one jvm and the suite number if there is one suite - they both really save a substantial amount of screen realestate that is useful for logging the long method names in the rest tests.
</comment><comment author="rjernst" created="2015-11-17T17:08:41Z" id="157436976">Two questions:
1. It seems to me, without doing a setting, this `suiteFinished` flag will cause the first suite run (even in multi suite runs) to emit individual tests?
2. Why all the formatting? I understand omitting stuff, but I don't understand trying to get fixed widths.
</comment><comment author="nik9000" created="2015-11-17T17:38:10Z" id="157447703">&gt; 1. Why all the formatting? I understand omitting stuff, but I don't understand trying to get fixed widths.

Fixed widths keeps the output from jumping around. It helps a lot with scanning the command line - everything is always in the same place.

&gt; 1. It seems to me, without doing a setting, this suiteFinished flag will cause the first suite run (even in multi suite runs) to emit individual tests?

Yeah - the first suite run will emit individual tests but it doesn't make a big difference there. I can suppress the test name if there is more than one suite pretty easy but having the test name for the first few seconds of the multi-suite runs doesn't look bad to me.
</comment><comment author="rjernst" created="2015-11-17T17:42:55Z" id="157449009">Having the test names emitted only for the first suite is very confusing to me, but I will do the work to change this to a parameter as a followup. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing setting name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14752</link><project id="" key="" /><description>I could be wrong but I'm pretty sure this setting is typed out backwards :smile: 
</description><key id="116859909">14752</key><summary>Fixing setting name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels><label>docs</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-11-13T21:47:39Z</created><updated>2016-03-10T18:15:05Z</updated><resolved>2015-11-14T10:12:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-14T10:11:26Z" id="156680967">LGTM. Good catch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Format the build_date as ISO local date time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14751</link><project id="" key="" /><description>This commit formats the build_date as ISO local date time. Note that
this does not include the timezone information as that is not currently
available in the `Build-Date` field in the packaged jar manifest.

Closes #14534
</description><key id="116843738">14751</key><summary>Format the build_date as ISO local date time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>build</label><label>review</label></labels><created>2015-11-13T20:15:51Z</created><updated>2015-11-18T13:44:53Z</updated><resolved>2015-11-18T12:57:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-15T06:41:22Z" id="156786312">@jasontedor We can just override the build date added by the info plugin. I opened a PR: #14764
</comment><comment author="clintongormley" created="2015-11-18T12:57:58Z" id="157703915">Closed by https://github.com/elastic/elasticsearch/pull/14764
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix `mvn verify` on jigsaw with 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14750</link><project id="" key="" /><description>Let's not release stuff with known java incompatiblities, e.g. we know will break in java 9, and could technically even break going forwards in java 8, e.g. calling http://docs.oracle.com/javase/7/docs/api/java/lang/management/RuntimeMXBean.html#getBootClassPath%28%29 without handling a possible UnsupportedOperationException.

This is a backport of all bugfixes so that 2.1 won't have compatibility issues.
</description><key id="116828875">14750</key><summary>fix `mvn verify` on jigsaw with 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v2.1.0</label></labels><created>2015-11-13T18:53:15Z</created><updated>2015-11-16T14:12:29Z</updated><resolved>2015-11-15T21:55:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-15T07:09:57Z" id="156787905">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch  2.0 multifield path just_name demands full path now</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14749</link><project id="" key="" /><description>Maybe more clear to show error when indexing if path is specified on multivalue field. Even if i say just_name full parh must be specified in queries
</description><key id="116820847">14749</key><summary>Elasticsearch  2.0 multifield path just_name demands full path now</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierresetteskog</reporter><labels /><created>2015-11-13T18:16:47Z</created><updated>2015-11-18T12:53:04Z</updated><resolved>2015-11-18T12:53:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T12:53:04Z" id="157702668">@pierresetteskog the `path` parameter is no longer supported in 2.0, except for pre-existing indices for bwc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>distribution tests hanging when built outside of a git repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14748</link><project id="" key="" /><description>On master on linux when I run `gradle distribution:zip:check` they seem like they are getting stuck and when I do `jstack pid_of_SlaveMainSafe | grep transport_client_external | wc -l` I get `1245`.
</description><key id="116818556">14748</key><summary>distribution tests hanging when built outside of a git repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T18:04:31Z</created><updated>2015-11-17T18:30:23Z</updated><resolved>2015-11-17T18:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-13T21:34:59Z" id="156562320">This is caused by a fun confluence of things:
1. Gradle wasn't logging _anything_ on test failure. It wouldn't even write out a counter so I could see something would happen. (#14753, #14757)
2. When we can't form the `ExternalTestCluster` we can leave its client open. (#14756)
3. When you build elasticsearch outside of a git repository, which I do, `Transport` is broken because `Build` contains something `null`. (#14754) &lt;----- This is the root cause.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow continuing failed build, which skips already successful test tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14747</link><project id="" key="" /><description>When running `gradle check` from the root of elasticsearch, if a failure occurs say in a a plugin, fixing that then requires either knowing which plugins were left and running for those explicitly, or taking the hit of running an entire `gradle check` again.

Gradle tests normally are run only if "output changes". We currently explicitly disable this for all tests, since we have randomized tests. However, we could tweak this slightly to allow a flag which identifies the build run you were previously running. Then the logic of whether to run the test could be based on this (we can write a marker when each test task succeeds, with the name of the unique build id). We would still want to make sure the test runs if the inputs change (ie to fix your plugin, you had to change core code). But if nothing changed, then we could skip eg core tests and quickly get back to running the plugins that had not run yet.

I'm thinking something like:

```
$ gradle check
...
Failed `gradle check`. You can continue your build after fixing errors with:
gradle check -Dtests.buildid=abcdefg
```
</description><key id="116817637">14747</key><summary>Allow continuing failed build, which skips already successful test tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2015-11-13T17:58:07Z</created><updated>2016-02-14T16:21:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-13T18:01:31Z" id="156502457">I agree this would be nice, I'm on my fourth run of `mvn verify`, and each time restarting because of a straggler here or there in a plugin or QA test.

At the same time, we shouldn't do it unless it can be done safely.
</comment><comment author="rjernst" created="2015-11-13T18:05:58Z" id="156503362">&gt; At the same time, we shouldn't do it unless it can be done safely.

I completely agree. Even if a buildid is passed in, and eg the core tests passed for that build id, we should still run core tests if core code changed, otherwise we are potentially hiding issues, where you could pass tests and think everything was ok. This is super important, otherwise there could be huge confusion on "my tests passed" but the code is totally broken.
</comment><comment author="clintongormley" created="2016-02-14T16:21:56Z" id="183913045">@rjernst has this been implemented yet?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reproducible - null Query in BoostingQueryBuilderTests throws assertion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14746</link><project id="" key="" /><description>Ran into this error while regression testing master.

```
Suite: org.elasticsearch.index.query.BoostingQueryBuilderTests
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=1817BD8B51E70A2 -Dtests.class=org.elasticsearch.index.query.BoostingQueryBuilderTests -Dtests.method="testToQuery" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=es_EC -Dtests.timezone=America/Martinique
FAILURE 0.01s J1 | BoostingQueryBuilderTests.testToQuery &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: null
   &gt;      but: was &lt;mapped_boolean:F^0.1&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([1817BD8B51E70A2:F67A79E6C49DB548]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.assertLuceneQuery(AbstractQueryTestCase.java:528)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.testToQuery(AbstractQueryTestCase.java:476)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="116811878">14746</key><summary>Reproducible - null Query in BoostingQueryBuilderTests throws assertion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Query Refactoring</label><label>bug</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T17:23:58Z</created><updated>2015-11-16T14:34:09Z</updated><resolved>2015-11-16T14:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-16T14:16:14Z" id="157043151">Great catch, found the root cause for this. When we set up those query builder tests we randomly assign the `_name` field to a query. In rare cases we might accidentally pick the same name for subqueries and the main query under test, like in this case:

```
{
  "boosting" : {
    "positive" : { },
    "negative" : {
      "term" : {
        "mapped_boolean" : {
          "value" : false,
          "boost" : 0.1,
          "_name" : "q"
        }
      }
    },
    "negative_boost" : 0.6666667,
    "boost" : 1.0,
    "_name" : "q"
  }
}
```

When storing the queries by name in the shard context namedQueries map, this can lead to conflicts. Will change the test setup so this cannot happen any longer.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NPE in Derivative Pipeline when current bucket value is null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14745</link><project id="" key="" /><description>Sequence of events that lead to the NPE:
1. `avg` metric returns NaN for buckets
2. `movavg` skips null **or NaN** values, and simply re-uses the existing bucket (e.g. doesn't add
   a 'movavg' field to the bucket).  This would apply to any pipeline that opts to skip a bucket instead of returning NaN
3. Derivative references `movavg`, the bucket resolution returns null because movavg wasn't added
   to the bucket, NPE when trying to subtract null values

So the root problem is that `movavg` will skip NaN values in addition to null, which is why this particular NPE was only found when combining the deriv with a movavg.  I think we should add the validation check just to prevent potential NPEs, because `resolveBucketValue()` is allowed to return a null Double.

But I wonder if the movavg behavior is also broken though?  The problem is that a single NaN will "infect" the entire movavg from that point onwards, since values depend on the previous.  A single NaN will make all following calculations fail, so they are simply ignored right now.

But, technically, NaN is a value, so perhaps this is bad behavior?  In any case, that'd be a separate PR/issue.

 /cc @colings86 
</description><key id="116811244">14745</key><summary>Fix NPE in Derivative Pipeline when current bucket value is null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T17:20:04Z</created><updated>2016-03-10T20:34:31Z</updated><resolved>2016-03-10T20:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-16T11:07:45Z" id="156996186">LGTM
</comment><comment author="clintongormley" created="2016-03-10T12:44:42Z" id="194826481">@polyfractal want to get this in?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong netty version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14744</link><project id="" key="" /><description>Hi,

I get the following error when using the elasticsearch 2.0 java client libraries:

**java.lang.NoClassDefFoundError: org/jboss/netty/util/internal/StackTraceSimplifier**

Elasticsearch 2.0 ships with the "netty-3.10.5.Final.jar" that should contain that class but it doesn't. Only much older versions of the netty jar like the "netty-3.2.5.Final.jar" contain that class.

Is this a problem in the distribution? I didn't test this with earlier versions but I thinks they have a similar problem.

Below is the stacktrace: I'm running my persistence service (that connects to elasticsearch) as a linux service using JAJSW (yet another java service wrapper)

INFO|3861/0|15-11-13 12:55:55|java.lang.reflect.InvocationTargetException
INFO|3861/0|15-11-13 12:55:55|  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
INFO|3861/0|15-11-13 12:55:55|  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
INFO|3861/0|15-11-13 12:55:55|  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
INFO|3861/0|15-11-13 12:55:55|  at java.lang.reflect.Method.invoke(Method.java:497)
INFO|3861/0|15-11-13 12:55:55|  at org.rzo.yajsw.app.WrapperJVMMain.executeMain(WrapperJVMMain.java:53)
INFO|3861/0|15-11-13 12:55:55|  at org.rzo.yajsw.app.WrapperJVMMain.main(WrapperJVMMain.java:36)
**INFO|3861/0|15-11-13 12:55:55|Caused by: java.lang.NoClassDefFoundError: org/jboss/netty/util/internal/StackTraceSimplifier**
INFO|3861/0|15-11-13 12:55:55|  at org.jboss.netty.logging.InternalLoggerFactory.&lt;clinit&gt;(InternalLoggerFactory.java:49)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.netty.NettyUtils.&lt;clinit&gt;(NettyUtils.java:90)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.transport.netty.NettyTransport.&lt;clinit&gt;(NettyTransport.java:106)
INFO|3861/0|15-11-13 12:55:55|  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
INFO|3861/0|15-11-13 12:55:55|  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
INFO|3861/0|15-11-13 12:55:55|  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
INFO|3861/0|15-11-13 12:55:55|  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
INFO|3861/0|15-11-13 12:55:55|  at org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:147)
INFO|3861/0|15-11-13 12:55:55|  at com.bynubian.elasticsearch.base.ElasticSearchDB.connect(ElasticSearchDB.java:219)
INFO|3861/0|15-11-13 12:55:55|  at com.bynubian.elasticsearch.base.ElasticSearchDB.connect(ElasticSearchDB.java:147)
INFO|3861/0|15-11-13 12:55:55|  at com.bynubian.services.persistence.server.PersistenceServer.initialize(PersistenceServer.java:92)
INFO|3861/0|15-11-13 12:55:55|  at be.landc.framework.service.server.shared.Server.initialize(Server.java:98)
INFO|3861/0|15-11-13 12:55:55|  at be.landc.framework.service.ipl.Boot.initialize(Boot.java:211)
INFO|3861/0|15-11-13 12:55:55|  at be.landc.framework.service.ipl.Boot.&lt;init&gt;(Boot.java:82)
INFO|3861/0|15-11-13 12:55:55|  at be.landc.framework.service.ipl.Boot.main(Boot.java:320)
INFO|3861/0|15-11-13 12:55:55|  ... 6 more
INFO|3861/0|15-11-13 12:55:55|Caused by: java.lang.ClassNotFoundException: org.jboss.netty.util.internal.StackTraceSimplifier
INFO|3861/0|15-11-13 12:55:55|  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
INFO|3861/0|15-11-13 12:55:55|  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
INFO|3861/0|15-11-13 12:55:55|  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
INFO|3861/0|15-11-13 12:55:55|  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
INFO|3861/0|15-11-13 12:55:55|  ... 48 more
</description><key id="116802580">14744</key><summary>Wrong netty version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frank-montyne</reporter><labels /><created>2015-11-13T16:31:34Z</created><updated>2015-11-18T20:19:46Z</updated><resolved>2015-11-18T13:31:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-13T16:38:32Z" id="156482322">Possibly jar hell in your setup. If you are using elasticsearch 2.0 as a client, first try this:

```
import org.elasticsearch.bootstrap.JarHell;

...

    JarHell.checkJarHell();
```

It might shed some light on the situation..
</comment><comment author="frank-montyne" created="2015-11-13T16:40:27Z" id="156482768">Thanks I'll give that a try
</comment><comment author="frank-montyne" created="2015-11-13T17:43:07Z" id="156498194">Running JarHell doesn't indicate that something is wrong
</comment><comment author="clintongormley" created="2015-11-18T12:43:53Z" id="157701063">Isn't the problem in JBoss?

```
INFO|3861/0|15-11-13 12:55:55| at org.jboss.netty.logging.InternalLoggerFactory.(InternalLoggerFactory.java:49)
INFO|3861/0|15-11-13 12:55:55| at org.elasticsearch.common.netty.NettyUtils.(NettyUtils.java:90)
```
</comment><comment author="jasontedor" created="2015-11-18T13:30:32Z" id="157713365">&gt; Isn't the problem in JBoss?

Netty use to be developed under JBoss so the package names from the 3.x line reflect that (it is no the longer case).

I think that the user has something unusual going on with Netty and their classpath. In particular, the `StackTraceSimplifier` class that is referred to was removed between 3.9.0 and 3.10.0 of Netty. Looking at the source of `InternalLoggerFactory` from say 3.9.0.Final, `StackTraceSimplifier` is used in the class initializer for `InternalLoggerFactory` (and this is not the case in 3.10.5.Final). If that class is not available, the class initializer for `InternalLoggerFactory` fails and that leads to a [`NoClassDefFoundError`](http://docs.oracle.com/javase/specs/jls/se7/html/jls-12.html#jls-12.4.2). The version of Netty that Elasticsearch depends on (3.10.5.Final) should not be looking for `StackTraceSimplifier` because it has been completely removed from Netty in that version.

I took a cursory peek at YAJSW, and it appears to have a Netty dependency (maybe Netty 3.6.6.Final?). With this older version of Netty and this wrapper, I suspect that this is the underlying source of the issue.
</comment><comment author="frank-montyne" created="2015-11-18T20:19:46Z" id="157850987">Indeed, jajsw is the culprit.

Thanks,
Frank

&gt; On 18 Nov 2015, at 08:31, Jason Tedor notifications@github.com wrote:
&gt; 
&gt; Isn't the problem in JBoss?
&gt; 
&gt; Netty use to be developed under JBoss so the package names from the 3.x line reflect that (it is no the longer case).
&gt; 
&gt; I think that the user has something unusual going on with Netty and their classpath. In particular, the StackTraceSimplifier class that you refer to was removed between 3.9.0 and 3.10.0 of Netty. If you look at the source of InternalLoggerFactory from say 3.9.0.Final, you'll see that it is used in the class initializer for InternalLoggerFactory (and this is not the case in 3.10.5.Final). If that class is not available, that class initializer for InternalLoggerFactory fails and that leads to a NoClassDefFoundError. The version of Netty that Elasticsearch depends on (3.10.5.Final) should not be looking for StackTraceSimplifier because it has been completely removed from Netty in that version.
&gt; 
&gt; I took a cursory peek at YAJSW, and it appears to have a Netty dependency (maybe Netty 3.6.6.Final?). I suspect that this is the underlying source of the issue.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>LinkedHashMap order is not preserved by StreamOutput.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14743</link><project id="" key="" /><description>LinkedHashMap order is not preserved by org.elasticsearch.common.io.stream.StreamOutput.java.  This is because the LinkedHashMap is cast to a Map, and then it's entry set is iterated on, rather than iterating on the LinkedHashMap itself.  

The relevant code fragment from StreamOutput.java:379-92:
        } else if (value instanceof Map) {
            if (value instanceof LinkedHashMap) {
                writeByte((byte) 9);
            } else {
                writeByte((byte) 10);
            }
            @SuppressWarnings("unchecked")
            Map&lt;String, Object&gt; map = (Map&lt;String, Object&gt;) value;
            writeVInt(map.size());
            for (Map.Entry&lt;String, Object&gt; entry : map.entrySet()) {
                writeString(entry.getKey());
                writeGenericValue(entry.getValue());
            }
        }

I discovered this in the course of writing a plugin and trying to return a LinkedHashMap from the run() method of AbstractExecutableScript.
</description><key id="116800807">14743</key><summary>LinkedHashMap order is not preserved by StreamOutput.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels><label>:Network</label></labels><created>2015-11-13T16:21:41Z</created><updated>2015-11-18T08:37:16Z</updated><resolved>2015-11-18T01:24:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-17T23:43:20Z" id="157546693">&gt; This is because the LinkedHashMap is cast to a Map, and then it's entry set is iterated on, rather than iterating on the LinkedHashMap itself.

The `Map#entrySet` method is abstract and is [implemented on `LinkedHashMap`](https://docs.oracle.com/javase/8/docs/api/java/util/LinkedHashMap.html#entrySet--). Even though `LinkedHashMap` extends `HashMap`, because of virtual method dispatch it is the implementation of `entrySet` on `LinkedHashMap` that will be the one invoked at runtime. It is this method that gives the predictable iteration order, and that is the order that the entries will be written to the stream. I wrote a test in #14812 that shows that this order is preserved by the serialization, and it is preserved whether or not the iteration order is insertion order or access order.

Can you explain what led to you thinking that the iteration order is not preserved by serialization? Do you have a reproducible test case that you can share?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass extended bounds into HistogramAggregator when creating an unmapped aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14742</link><project id="" key="" /><description>This fixes an issue where if the field for the aggregation was unmapped the extended bounds would get dropped and the resulting buckets would not cover the extended bounds requested.

Closes #14735
</description><key id="116780706">14742</key><summary>Pass extended bounds into HistogramAggregator when creating an unmapped aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T14:43:22Z</created><updated>2015-11-13T15:16:15Z</updated><resolved>2015-11-13T15:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-11-13T14:54:49Z" id="156453128">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add system CPU percent to OS stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14741</link><project id="" key="" /><description>This commit adds the system CPU percent reflecting the recent CPU usage
for the whole system.
</description><key id="116776462">14741</key><summary>Add system CPU percent to OS stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T14:19:20Z</created><updated>2016-01-11T23:09:16Z</updated><resolved>2015-11-17T18:49:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-13T14:34:53Z" id="156448978">There are three breaking changes with this pull request.

The first is that the `load_average` metric is now under the `cpu` object in the response.

The second is that `cpu` shows up by default in the cat nodes response.

The third is that `OsStats#getLoadAverage` has been replaced by `OsStats.Cpu#getLoadAverage`.
</comment><comment author="jasontedor" created="2015-11-17T15:22:29Z" id="157401747">@tlrx Are there any other issues that you would like me to address in this pull request?
</comment><comment author="tlrx" created="2015-11-17T16:51:35Z" id="157428882">@jasontedor I left a comment concerning `readOptionalStreamable / writeOptionalStreamable`, OTT LGTM
</comment><comment author="jasontedor" created="2015-11-17T18:47:17Z" id="157467828">Added usage of `readOptionalStreamable` and `writeOptionalStreamable` in 5f4f5996275e47e8137983b90d43336b8c6b50ec but I will rebase this on elastic/elasticsearch@ 043319c48272d7125917810aa434c8561672c516 when integrating into master.
</comment><comment author="NickCraver" created="2016-01-11T19:14:42Z" id="170657958">Was this the move from `load_average` being an array to a single value? This creates a great deal of complexity in parsing responses in a static way because it's one of the _very_ few changes that's breaking. The others are moved, which isn't an issue with null checks. A change in the object structure creates a rather large mess with the parsing across major versions - you effectively have to maintain a great deal of code twice.
</comment><comment author="jasontedor" created="2016-01-11T20:06:57Z" id="170674026">@NickCraver Are you referring to a breaking change for this in the 2.x line (specifically 2.2.0) or in master?
</comment><comment author="NickCraver" created="2016-01-11T20:29:14Z" id="170680502">@jasontedor We're testing against 2.1.0 for this run, so it's pre-2.2.0. The `load_average` was an array of the 3 values in 1.x (like you'd expect) - it's now a single integer which creates deserialization problems in any static way. When we have monitoring systems that span multiple versions this creates a tremendous pain point - especially since it's the only _differing_ type. Moved pieces which are `X ?? Y` style are much easier to deal with.
</comment><comment author="jasontedor" created="2016-01-11T20:56:54Z" id="170687398">@NickCraver The change set in this pull request is not in 2.1.0, only master and 2.2.0.

I did some digging. The change that you're referring to is 19e348a82ca37a93c65cf911e648241d328f4ee3 which was integrated in #12049. The reason for the change is because Sigar was removed as a dependency and the JVM only exposes the 1-minute load average excluding the 5-minute and 15-minute load averages as were also available via Sigar. 
</comment><comment author="NickCraver" created="2016-01-11T20:59:22Z" id="170688041">@jasontedor doh, you're right - I'll leave comments there - thanks! The main issue is the type change, a single entry array actually makes more sense there in case the load averages can be added back later and break no one supporting both.
</comment><comment author="jasontedor" created="2016-01-11T21:06:39Z" id="170689854">&gt; The main issue is the type change, a single entry array actually makes more sense there in case the load averages can be added back later and break no one supporting both.

@NickCraver Yeah, I can see the troubles, sorry you're going through them! I'd say it's unlikely this will be changed back to be an array in the 2.x line, and I think the only catalyst for change in 3.x would be if the 10- and 15-minute load averages are actually reintroduced.

And sorry to potentially add to your troubles, but do note that in this change set `load_average` was moved to be a field on the `cpu` object instead of being a standalone field on `os`.
</comment><comment author="jasontedor" created="2016-01-11T23:05:58Z" id="170726686">@NickCraver You piqued my interest; I opened #15907 to reintroduce the five-minute and fifteen-minute load averages on Linux, and the `load_average` field is an array again no matter the OS. I will investigate backporting this to the 2.x line (maybe 2.3.0) but no promises.
</comment><comment author="NickCraver" created="2016-01-11T23:09:16Z" id="170727482">@jasontedor This is why I love open source. &lt;3

Please let me know how this progresses and I'll make sure Opserver supports it properly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation on scripts does not deal with `null` correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14740</link><project id="" key="" /><description>Using version 1.7.1

I have one record in my index, with two nested records:  One is to establish a schema, the other is empty.

```
{
    "_index":"testing_631cd59d7020151113_135108",
    "_type":"test_result",
    "_id":"9BDA12F08CF9D63CD48994EF1E11966C256203AE",
    "_version":1,
    "_score":1,
    "_source":{"a":{"_b":[{"a":0,"b":0},{}]}}
}
```

My query is aggregating over a script, which just happens to return `null`.

```
{
    "aggs":{"_nested":{
        "aggs":{"_missing":{
            "aggs":{"t":{"stats":{"script":"doc[\"a._b.a\"].value"}}},
            "filter":{"missing":{"field":"a._b.a"}}
        }},
        "nested":{"path":"a._b"}
    }},
    "size":0
}
```

The aggregate over `"script":"doc[\"a._b.a\"].value"` is not the same as the aggregate over `"field": "a._b.a"`

```
{
    "took":1,
    "timed_out":false,
    "_shards":{"total":3,"successful":3,"failed":0},
    "hits":{"total":1,"max_score":0,"hits":[]},
    "aggregations":{"_nested":{
        "doc_count":2,
        "_missing":{
            "doc_count":1,
            "t":{"count":1,"min":0,"max":0,"avg":0,"sum":0}
        }
    }}
}
```

I would expect a count of zero (with `null` stats) instead:

```
"t":{"count":0,"min":null,"max":null,"avg":null,"sum":null}
```

I highly suspect the code responsible for aggregating script results is not discounting `nulls`
</description><key id="116774608">14740</key><summary>Aggregation on scripts does not deal with `null` correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klahnakoski</reporter><labels /><created>2015-11-13T14:08:18Z</created><updated>2015-11-18T12:54:59Z</updated><resolved>2015-11-18T12:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="klahnakoski" created="2015-11-13T18:45:23Z" id="156517459">It would appear this problem can be mitigated with a more complicated script:

```
"script":"(doc[\"a._b.a\"].isEmpty() ? null : doc[\"a._b.a\"].value)"
```
</comment><comment author="clintongormley" created="2015-11-18T12:54:59Z" id="157703322">@klahnakoski yes, nulls need to be handled by your script as appropriate for your needs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allocate primary shard based on allocation IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14739</link><project id="" key="" /><description>Persist allocation IDs of active shards in cluster state and use them to recover correct shards upon cluster restart (i.e., we can recover with only one copy and we make sure we recover the right ones and not a stale copy left around).

Relates to #14671
## Steps
- [x] Persist allocation ID with shard state metadata on nodes (#14831).
- [x] Persist currently started allocation IDs to index metadata (#14964).
- [x] Add allocation IDs to TransportNodesListGatewayStartedShards action. Use the above to assign a primary shard on recovery. In particular, don't assign a stale copy (#15281).
- [x] Extend reroute with an option to force assign a stale copy. This is only relevant for primaries (#15708).
- [x] Prioritize old primaries when choosing out existing _and_ previously active allocationIds (#16096).
- [x] Persist shard state immediately and not only on shard activation. This will help solving a race condition on primary relocation where the master has activated the target shard (removing old copy from the active set list) but the cluster crashes before the target node persisted the shard state (#16625)
- [x] Remove version from ShardRouting (#16243).
</description><key id="116773580">14739</key><summary>Allocate primary shard based on allocation IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>Meta</label><label>release highlight</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T14:02:52Z</created><updated>2016-02-29T13:34:02Z</updated><resolved>2016-02-29T13:34:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update error in documentation for multi-fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14738</link><project id="" key="" /><description>The mapping example for defining multi-fields with multiple analyzers had an error which would prevent the mapping from being applied.
</description><key id="116769172">14738</key><summary>Update error in documentation for multi-fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">petmit</reporter><labels><label>docs</label></labels><created>2015-11-13T13:32:42Z</created><updated>2015-11-17T16:33:18Z</updated><resolved>2015-11-17T16:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T16:33:18Z" id="157423213">thanks @petmit - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Uniform exceptions for TransportMasterNodeAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14737</link><project id="" key="" /><description>Throw MasterNotDiscoveredException whenever retry logic of TransportMasterNodeAction times out. This makes it easier to classify failures on client side.
</description><key id="116765106">14737</key><summary>Uniform exceptions for TransportMasterNodeAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T13:05:08Z</created><updated>2015-11-17T16:30:41Z</updated><resolved>2015-11-13T13:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-13T13:08:55Z" id="156427373">LGTM. Can we point at a build failure with an example of how this could have failed?
</comment><comment author="ywelsch" created="2015-11-13T13:21:41Z" id="156430346">Build failure:

http://build-us-00.elastic.co/job/es_core_2x_debian/590/testReport/junit/org.elasticsearch.cluster/SpecificMasterNodesIT/simpleOnlyMasterNodeElection/

Interesting log line:

```
[2015-11-12 16:09:50,531][DEBUG][action.admin.cluster.state] [node_t0] timed out while retrying [cluster:monitor/state] after failure (timeout [100ms])
NodeNotConnectedException[[node_t1][127.0.0.1:9421] Node not connected]
```

This test (`SpecificMasterNodesIT.testSimpleOnlyMasterNodeElection`) was relying on `MasterNotDiscoveredException` to be thrown in case where a master cannot be found within a certain timeout. With the change in #14222, the exception to be thrown was changed to the one actually causing the issue (`NodeNotConnectedException`). With this PR, we change it back to `MasterNotDiscoveredException` but wrap the cause.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass extended bounds into HistogramAggregator when creating an unmapped aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14736</link><project id="" key="" /><description>This fixes an issue where if the field for the aggregation was unmapped the extended bounds would get dropped and the resulting buckets would not cover the extended bounds requested.

Closes #14735
</description><key id="116761117">14736</key><summary>Pass extended bounds into HistogramAggregator when creating an unmapped aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label></labels><created>2015-11-13T12:35:50Z</created><updated>2015-11-16T14:08:50Z</updated><resolved>2015-11-13T12:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-13T12:36:14Z" id="156421199">@jpountz could you review please?

@clintongormley @jpountz should this go into 2.1 and 2.0 branches?
</comment><comment author="jpountz" created="2015-11-13T12:39:21Z" id="156421949">LGTM. And yes to both 2.0 and 2.1: the fix is low-risk.
</comment><comment author="colings86" created="2015-11-13T13:30:09Z" id="156431725">This commit had to be reverted as it broke the MovAvgIT tests. Will link to the commit/PR that superceds this when it is sorted
</comment><comment author="colings86" created="2015-11-13T15:16:45Z" id="156459223">This fix was reverted and superseded by https://github.com/elastic/elasticsearch/pull/14742
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>extended_bounds does not play nicely with .kibana index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14735</link><project id="" key="" /><description>If you try to use extended_bounds a request which contains the .kibana index the extended bounds are ignored. See the following sense script to reproduce (must be run on a cluster with a .kibana index to reproduce the bug):

``` js
POST index-1/doc/1
{
  "@timestamp": "2015-01-01"
}
POST index-2/doc/1
{
  "@timestamp": "2013-01-01"
}
POST index-3/doc/1
{
  "@timestamp": "2011-01-01"
}
POST index-4/doc/1
{
  "@timestamp": "2009-01-01"
}

# Correctly returns buckets from 1970 to 2015
GET /index-*/_search
{
    "aggs": {
        "series": {
            "date_histogram": {
                "field": "@timestamp",
                "interval": "1y",
                "extended_bounds": {
                    "min": 0,
                    "max": 1447407264268
                },
                "min_doc_count": 0
            }
        }
    },
    "size": 0
}

# Only return buckets between 2009 and 2015 (extended_bounds is ignored)
GET /index-*,.kibana/_search
{
    "aggs": {
        "series": {
            "date_histogram": {
                "field": "@timestamp",
                "interval": "1y",
                "extended_bounds": {
                    "min": 0,
                    "max": 1447407264268
                },
                "min_doc_count": 0
            }
        }
    },
    "size": 0
}
```
</description><key id="116753706">14735</key><summary>extended_bounds does not play nicely with .kibana index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-11-13T11:48:28Z</created><updated>2015-11-13T12:42:56Z</updated><resolved>2015-11-13T12:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Missing compress-lzf dependency causes silent error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14734</link><project id="" key="" /><description>When migrating an old installation from 0.90 to 2.0 got stuck in a strange behaviour where the client could not obtain masters. It would connect to the cluster, show up in the cluster but would timeout on getting the masters.

Turns out that on a default installation if you don't include compress-lzf as a dependency the client node will fail silently. 

I would suggest it should throw an error or at least warn in the logs.
</description><key id="116751753">14734</key><summary>Missing compress-lzf dependency causes silent error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MagmaRules</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2015-11-13T11:35:42Z</created><updated>2015-11-17T15:02:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>link to scripts: java.security.AccessControlException: access denied ("java.io.FilePermission"...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14733</link><project id="" key="" /><description>In ES 1.x I was able to symbolically link from a subdirectory of the scripts directory to a directory containing my groovy scripts. Now, with ES 2.0, on load, I get the following error:

Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/usr/local/Cellar/elasticsearch/2.0.0_1/libexec/config/scripts/my_directory/my_subdirectory" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
    at java.nio.file.Files.exists(Files.java:2385)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:157)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.createChild(FileWatcher.java:173)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.listChildren(FileWatcher.java:188)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.onDirectoryCreated(FileWatcher.java:299)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:162)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.createChild(FileWatcher.java:173)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.listChildren(FileWatcher.java:188)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.onDirectoryCreated(FileWatcher.java:299)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:162)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.access$000(FileWatcher.java:75)
    at org.elasticsearch.watcher.FileWatcher.doInit(FileWatcher.java:65)
    at org.elasticsearch.watcher.AbstractResourceWatcher.init(AbstractResourceWatcher.java:36)
    at org.elasticsearch.watcher.ResourceWatcherService.add(ResourceWatcherService.java:133)
    at org.elasticsearch.watcher.ResourceWatcherService.add(ResourceWatcherService.java:126)
    at org.elasticsearch.script.ScriptService.&lt;init&gt;(ScriptService.java:192)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    &lt;&lt;&lt;truncated&gt;&gt;&gt;
</description><key id="116733254">14733</key><summary>link to scripts: java.security.AccessControlException: access denied ("java.io.FilePermission"...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">twigbranch</reporter><labels /><created>2015-11-13T09:35:08Z</created><updated>2016-04-28T23:25:12Z</updated><resolved>2015-11-13T14:12:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-13T09:41:31Z" id="156378685">Have a look at https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_plugin_and_packaging_changes.html#_symbolic_links_and_paths
</comment><comment author="dadoonet" created="2015-11-13T09:42:55Z" id="156378929">Sounds like we advertised we can follow symlinks for scripts?
</comment><comment author="rmuir" created="2015-11-13T13:29:08Z" id="156431557">symbolic links are never supported. if they happen to work, its random chance.
</comment><comment author="dadoonet" created="2015-11-13T13:49:36Z" id="156435151">@rmuir do I misunderstand the doc I linked?
</comment><comment author="rmuir" created="2015-11-13T13:57:40Z" id="156436600">they are neither tested nor supported. we can't, we can't prevent some code from traversing the link (in which case unix permissions checks are different too, same problem).
</comment><comment author="dadoonet" created="2015-11-13T14:03:33Z" id="156438971">I totally agree with you Robert. I'm just asking if we must fix the doc or if I simply misunderstood that part.
</comment><comment author="rmuir" created="2015-11-13T14:05:59Z" id="156440035">What the documentation says works, that is not what is happening here.
</comment><comment author="dadoonet" created="2015-11-13T14:12:14Z" id="156442478">So I definitely don't understand this:

&gt; but no symlinks under that path will be followed (with the exception of path.scripts, which does follow symlinks).

My bad English I guess. Closing the "issue" then.
</comment><comment author="twigbranch" created="2015-11-13T17:40:11Z" id="156497432">I saw that part of the docs before I posted the issue.  I, also, am confused by it.  With ES 1.5.2, what I was doing worked without issue on each installation I tried, both on Centos and OSX.  The doc quoted above says path.scripts "does follow symlinks" and symlinks under that path will be followed.  How is my symlink NOT under the path.scripts path?

path.scripts is not explicitly set in my elasticsearch.yml, so ES appears to be using the default.

This is my symlink:
/usr/local/Cellar/elasticsearch/2.0.0_1/libexec/config/scripts/my_directory/my_subdirectory -&gt; /Users/tbr/projects/www/my_site/es/scripts
</comment><comment author="nik9000" created="2015-11-13T17:43:30Z" id="156498278">I believe the document means that `config/scripts` itself can be a symlink but that symlinks within the directory aren't supported. I haven't tried it though.
</comment><comment author="twigbranch" created="2015-11-13T17:55:37Z" id="156501194">Then the doc should say a symlink from path.scripts, but not within, will be followed.

Or, even better ES would follow the symlinks within path.scripts.
</comment><comment author="rmuir" created="2015-11-13T18:02:36Z" id="156502648">No, we don't need to do a recursive traversal for symlinks for security purposes, when symlinks themselves are broken from that perspective. Just don't use symlinks.
</comment><comment author="dadoonet" created="2015-11-13T18:05:48Z" id="156503324">I prefer this wording. Symlinks are forbidden. Period. This is clear IMO.
</comment><comment author="twigbranch" created="2015-11-13T18:07:07Z" id="156503599">OK. Then the doc should be updated - the combination of 'under' with plural for symlinks is confusing and makes it sound like you can have multiple symlinks within path.scripts.
</comment><comment author="rmuir" created="2015-11-13T18:09:44Z" id="156504152">I agree, and the purpose of adding `path.scripts` in the first place was so that people could configure a shared location without resorting to stuff like symlinks.
</comment><comment author="frutik" created="2016-04-27T15:02:00Z" id="215112046">path.scripts: /data/www/sample_name/releases/current/app/search/scripts

current - symlink to the directory with the current checkout of the application: releases/current -&gt; releases/2016042701. 
Elastic can not load scripts. What's wrong with the such setup? New approach is just horrible
</comment><comment author="frutik" created="2016-04-28T08:56:30Z" id="215357304">I investigated more, and issue exists only after switching symlink. I am getting 

[2016-04-28 04:33:46,444][WARN ][threadpool               ] [master-0] failed to run org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@4b018985
java.security.AccessControlException: access denied ("java.io.FilePermission" "/data/www/sample_name/releases/current/app/search/scripts" "read")

After restart of elasticsearch, everything works as expected
</comment><comment author="rjernst" created="2016-04-28T23:25:12Z" id="215591489">@frutik Symlinks in java security manager require adding permission for both the link and what the link points to. The `path.scripts` setting is read and followed at startup, and those permissions are added to the policy then. Simply changing the link while elasticsearch is running will not work. This is why restarting solved your problem, it re-read the link and added the setup the new appropriate permissions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ci] testLocalNodeMasterListenerCallbacks failing on windows-2012-r2 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14732</link><project id="" key="" /><description>We start a second node and wait for it to join. But the operation timed out.

``` java
        // start another node and set min_master_node
        internalCluster().startNode(Settings.builder().put(settings));
        assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes("2").get().isTimedOut());
```

Stacktrace:

```
java.lang.AssertionError
    at org.junit.Assert.fail(Assert.java:86)
    at org.junit.Assert.assertTrue(Assert.java:41)
    at org.junit.Assert.assertFalse(Assert.java:64)
    at org.junit.Assert.assertFalse(Assert.java:74)
    at org.elasticsearch.cluster.ClusterServiceIT.testLocalNodeMasterListenerCallbacks(ClusterServiceIT.java:671)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```

CI links:
- http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/2045/
- http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/2045/testReport/junit/org.elasticsearch.cluster/ClusterServiceIT/testLocalNodeMasterListenerCallbacks/

Tried to reproduce with on my laptop (OSX) but passed:

```
gradle :core:integTest -Dtests.seed=6F12D46214CD0404 -Dtests.class=org.elasticsearch.cluster.ClusterServiceIT -Dtests.method="testLocalNodeMasterListenerCallbacks" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:-UseCompressedOops" -Dtests.locale=fr_CA -Dtests.timezone=Asia/Dacca
```
</description><key id="116729363">14732</key><summary>[ci] testLocalNodeMasterListenerCallbacks failing on windows-2012-r2 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>adoptme</label></labels><created>2015-11-13T09:13:43Z</created><updated>2016-02-14T16:19:51Z</updated><resolved>2016-02-14T16:19:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-02T10:08:50Z" id="161246355">The issue here is that, due to concurrently running tests, the nodes cannot find each other. A funny asymmetry in the joining process actually happens in the test.

Steps:
- Test starts one master node (node_t0). It is bound to publish port 9480.
- Test starts second node (node_t1). It is bound to port 9486 due to **other tests/processes blocking ports** in range 9481-9485. Port 9486 is out of range of the normal pinging process (UnicastZenPing.LIMIT_LOCAL_PORTS_COUNT, which is 5). However, node_t1 can find node_t0 (which is in range) and both build a two node cluster.
- The test kills node_t0, and node_t1 becomes master in single-node cluster.
- Test starts a third node (node_t2). It is bound to publish port 9480 (which has become free as node_t0 left). With its pinging process, it cannot find the current master node_t1, however. As consequence, node_t2 becomes master as well (split brain). Test fails.
</comment><comment author="ywelsch" created="2015-12-02T11:09:22Z" id="161263139">@nik9000 Could this be related to #14774? As this test failure was 19 days ago already, maybe it is fixed now as part of your changes?
</comment><comment author="clintongormley" created="2016-02-14T16:19:51Z" id="183911930">Old test failure. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend usage of IndexSetting class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14731</link><project id="" key="" /><description> #14251 introduce a wrapper class for the index setting to distinguish them from the cluster settings and to allow some utility methods for common settings. This commit extends it's usage within the code base.

I decided to leave external listeners (used by plugins) alone, for now.
</description><key id="116720223">14731</key><summary>Extend usage of IndexSetting class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T07:58:30Z</created><updated>2015-11-13T15:42:07Z</updated><resolved>2015-11-13T14:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-13T10:38:51Z" id="156393598">Minor comments, LGTM o.w.

On a side note, we should get star imports under control, changing them back and forth will introduce more conflicts when we cherry-pick changes to older branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StatsAggegator has a missing character.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14730</link><project id="" key="" /><description>This is a minor issue or is not issue.

is it right StatsAggegator? (https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java)
I think that StatsAggegator will be StatsAggregator.

Thanks.
</description><key id="116719572">14730</key><summary>StatsAggegator has a missing character.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HowookJeong</reporter><labels><label>:Aggregations</label><label>:Java API</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-13T07:51:10Z</created><updated>2015-12-09T13:39:27Z</updated><resolved>2015-12-09T13:39:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T14:50:35Z" id="157390955">@colings86 we should probably fix this :)
</comment><comment author="martinstuga" created="2015-12-06T03:32:20Z" id="162266866">Submited a PR to fix this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch failed to execute on node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14729</link><project id="" key="" /><description>Hi all, am facing the below issue while checking the log file of Elasticsearch. My environment is AWS EC2 instance, am installed ELK-stack on the instance, am unable to load the logs data from logstash to elasticsearch...then am cross checked the log file of elasticsearch i seen the below message in elasticsearch.log..  
Please help me from this issue... 

[2015-11-13 00:52:40,858][INFO ][cluster.service          ] [May Parker] added {[Clearcut][TBNd6XIrTHqje7vD9qk2tA][ip-10-129-52-27.ap-southeast-1.compute.internal][inet[/10.129.52.27:9301]],}, reason: zen-disco-receive(join from node[[Clearcut][TBNd6XIrTHqje7vD9qk2tA][ip-10-129-52-27.ap-southeast-1.compute.internal][inet[/10.129.52.27:9301]]])
[2015-11-13 00:59:12,840][DEBUG][action.admin.cluster.node.stats] [May Parker] failed to execute on node [TBNd6XIrTHqje7vD9qk2tA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Clearcut][inet[/10.129.52.27:9301]][cluster:monitor/nodes/stats[n]] request_id [1572] timed out after [15001ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-11-13 00:59:42,839][DEBUG][action.admin.cluster.node.stats] [May Parker] failed to execute on node [TBNd6XIrTHqje7vD9qk2tA]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Clearcut][inet[/10.129.52.27:9301]][cluster:monitor/nodes/stats[n]] request_id [1605] timed out after [15000ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-11-13 01:00:12,839][DEBUG][action.admin.cluster.node.stats] [May Parker] failed to execute on node [TBNd6XIrTHqje7vD9qk2tA]
"elasticsearch.log" 239L, 40979C

Thanks,
Shravan K.
</description><key id="116714018">14729</key><summary>elasticsearch failed to execute on node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shravankodipaka</reporter><labels /><created>2015-11-13T06:59:53Z</created><updated>2015-11-13T09:03:51Z</updated><resolved>2015-11-13T07:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-13T07:11:21Z" id="156350217">Does not look like an issue here. Could you please open a thread on discuss.elastic.co instead?
</comment><comment author="shravankodipaka" created="2015-11-13T09:03:51Z" id="156368230">thank u....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use ant exec for starting elasticsearch in integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14728</link><project id="" key="" /><description>Currently elasticsearch in integ tests is started using an ant task on
windows, or gradle exec on everything else. However, gradle exec has
some flaws, one being Ctrl-C does not run finalizedBy tasks, which means
interrupting integ tests will leak a jvm. This change makes all systems
use ant exec. One caveat is, if there is any output by the jvm, we lose
it in ant bit heaven. But this is no different than what we had with
gradle. In the future, we should look at using a separate thread to
pump streams from the elasticsearch process.

closes #14701
</description><key id="116711666">14728</key><summary>Use ant exec for starting elasticsearch in integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T06:43:24Z</created><updated>2015-11-13T06:48:17Z</updated><resolved>2015-11-13T06:48:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-13T06:46:46Z" id="156341711">I like this much better than 2 different methods too, +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>spatial_stats aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14727</link><project id="" key="" /><description>This feature intends to add a `spatial_stats` metric aggregator designed for computing the most basic descriptive statistics for Exploratory Spatial Data Analysis. Much like the `extended_stats` aggregator for basic numerical analysis on non-spatial data sets, the `spatial_stats` aggregator will be specially designed for spatial data sets - specifically operating on `geo_point` fields. The aggregator will provide the following:
- Morans I - A measure of global spatial auto-correlation
- Geary's C - A measure of localized spatial auto-correlation (inversely related to Morans I)
- Getis-Ord - Hot spot analysis
- LISA - Indicators of local spatial association
</description><key id="116705823">14727</key><summary>spatial_stats aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>discuss</label><label>feature</label><label>high hanging fruit</label></labels><created>2015-11-13T05:59:28Z</created><updated>2015-11-13T06:00:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>fix gradle check under jigsaw</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14726</link><project id="" key="" /><description>followup to #14723 

for integ tests the outputsniffing is bogus, we don't need this and its a false positive on java 9 (which warns about our garbage collection settings, and this is not the place to deal with any of that).

also includes tests fixes ported forward
</description><key id="116702078">14726</key><summary>fix gradle check under jigsaw</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-13T05:16:00Z</created><updated>2015-11-13T05:57:32Z</updated><resolved>2015-11-13T05:57:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-13T05:18:59Z" id="156325970">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>memory leak in tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14725</link><project id="" key="" /><description>This happens when incoming documents have inconsistent type that causes replica shard allocation failure. While the cluster keeps trying to allocate the failed shards, tribe node memory usage grows unbounded. Eventually GC can't reclaim any space and tribe node hangs.  

![image](https://cloud.githubusercontent.com/assets/1834562/11138364/f5202b74-8975-11e5-80e1-f610ee3a15cc.png)
</description><key id="116694994">14725</key><summary>memory leak in tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yanjunh</reporter><labels /><created>2015-11-13T03:47:19Z</created><updated>2015-11-15T21:58:15Z</updated><resolved>2015-11-14T09:46:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-13T08:51:14Z" id="156364664">Thank for reporting.

&gt;  causes replica shard allocation failure

Can you clarify some more about actually fails?

Any chance you can share a heap dump of the tribe node? you can reach me privately on first name at elastic.co 
</comment><comment author="yanjunh" created="2015-11-14T05:48:21Z" id="156638638">The failure is [cluster.action.shard     ]. Here is part of the log message 

at[2015-11-12T17:03:45.861Z], details[Failed to perform [indices:data/write/bulk[s]] on replica, message [RemoteTransportException[[elk][inet[/10.29.22.33:9300]][indices:data/write/bulk[s][r]]]; nested: IllegalArgumentException[cannot change DocValues type from SORTED_NUMERIC to SORTED_SET for field "msg_.log_events.sql_query_args"]; ]]]
The cluster keeps trying to assign the shard, fail and then try to assign it again.

I took a heap histogram. there are large number of org.elasticsearch.cluster.routing.MutableShardRouting and it's super class ImmutableShardRouting. I suspect that large number of MutableShardRouting objects were created due to shard allocation thrashing. After we fixed the shard allocation thrashing, the tribe node heap usage eventually dropped. Seems there is a cleanup thread somewhere to remove reference to these object so GC can reclaim the space eventually. But when the heap is completely full, the cleanup thread can't run and the process hangs. Interestingly, this only affects tribe nodes. This is just a guess, I havent got time to trace deep into the code.

## num     #instances         #bytes  class name

   1:        301316     2863097560  [B
   2:      58282801     2373602544  [Ljava.lang.Object;
   3:      30699477     2210362344  org.elasticsearch.cluster.routing.MutableShardRouting
   4:      84192914     2020629936  org.elasticsearch.common.collect.SingletonImmutableList
   5:      29040738     1847249696  [C
   6:      26343947     1686012608  org.elasticsearch.cluster.routing.ImmutableShardRouting
   7:      40972736     1311127552  org.elasticsearch.common.collect.RegularImmutableList
   8:      44864300     1076743200  org.elasticsearch.index.shard.ShardId
   9:      13255026      848321664  org.elasticsearch.cluster.routing.IndexShardRoutingTable
  10:      44864300      717828800  org.elasticsearch.index.Index
</comment><comment author="bleskes" created="2015-11-14T09:46:30Z" id="156678348">I see thanks. The shard issues caused many clusters state to be published (with each failure and allocation). The tribe node at the moment doesn't have batching support when processing the changes and it seems the cluster state just piled up waiting to be processed. We are working to add batch (see #13627). Normal nodes have this already. 

I'm going to close this as it seems to be already covered by the PR I referenced.
</comment><comment author="yanjunh" created="2015-11-15T21:58:15Z" id="156859225">Cool. Thanks! I will take a look at the diff, just for my education. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch init script not starting on ubuntu 11.04</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14724</link><project id="" key="" /><description>When i tried to start the elastic search using init script its not working. However it works fine when i manually run from bin/elasticsearch
ES Version 1.7
</description><key id="116694504">14724</key><summary>elasticsearch init script not starting on ubuntu 11.04</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">titotp</reporter><labels><label>feedback_needed</label></labels><created>2015-11-13T03:41:32Z</created><updated>2015-11-14T02:03:11Z</updated><resolved>2015-11-14T02:03:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-13T11:04:25Z" id="156398636">Would you be able to provide any additional details about the nature of how it's not working? Are there any error or log messages that you can share?
</comment><comment author="titotp" created="2015-11-13T13:56:04Z" id="156436307">unfortunately the logs are not creating. I followed the ansible playbook 
https://github.com/Traackr/ansible-elasticsearch

There is no error message when it starts. Its starts ok, But when i check the status it shows stopped

http://pastebin.com/uQh8cTTh
</comment><comment author="nik9000" created="2015-11-13T14:22:48Z" id="156445527">So 11.04 isn't an officially supported version - 12.04, 14.04 are. And we test on 15.04 too because why not. Most of that testing is in 2.0 onwards and not so much 1.x. That being said I imagine it ought to work.

The first thing I do when I get into a situation like this is to run `bin/elasticsearch` as the `elasticsearch` user just to see if Java starts. If you don't get any log at all this is typically caused by something super early.

Another thing you can do is temporarily hack the init script to not run elasticsearch daemonized. If you do that it won't close stdout and you can read the logs.

It'd be nice to do this all automatically but its just hard in init.d land. In systemd its simpler _but_ we haven't done it because not everyone uses systemd.
</comment><comment author="titotp" created="2015-11-13T14:38:40Z" id="156449785">yes i am using 14.04 and it works fine when i run manually
bin/elasticsearch
</comment><comment author="nik9000" created="2015-11-13T15:11:08Z" id="156457196">&gt; yes i am using 14.04 and it works fine when i run manually bin/elasticsearch

Do you mean that 14.04 works fine when you run `bin/elasticsearch` or that 11.04 works fine when you run `bin/elasticsearch`?
</comment><comment author="titotp" created="2015-11-13T15:13:55Z" id="156458055">14.04 is the version i am using and it works fine with bin/elasticsearch. However it is not working with init script. 
</comment><comment author="nik9000" created="2015-11-13T15:22:32Z" id="156461792">Oh! In that case I imagine its some kind of invalid configuration. Can you try (temporarily) removing `-d` from the `start-stop-daemon` line? That should foreground elasticsearch and it should spit out _something_ when you run it.
</comment><comment author="titotp" created="2015-11-13T15:26:14Z" id="156463209">Thanks Nick for helping out. I tried removing the -d from init script and its still same.

PID_FILE="$PID_DIR/$NAME.pid"
DAEMON=$ES_HOME/bin/elasticsearch
DAEMON_OPTS="-p $PID_FILE --default.config=$CONF_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.work=$WORK_DIR --default.path.conf=$CONF_DIR"
DAEMON_OPTS="$DAEMON_OPTS -Des.max-open-files=true"

root@ip-10-1-5-141:/var/run/elasticsearch# /etc/init.d/elasticsearch status
- elasticsearch is not running
  root@ip-10-1-5-141:/var/run/elasticsearch# /etc/init.d/elasticsearch stop
- Stopping Elasticsearch Server
  ...done.
  root@ip-10-1-5-141:/var/run/elasticsearch# /etc/init.d/elasticsearch start
- Starting Elasticsearch Server
  root@ip-10-1-5-141:/var/run/elasticsearch# 
</comment><comment author="titotp" created="2015-11-13T15:31:45Z" id="156464438">Init script - http://pastebin.com/nVHnSiAE
</comment><comment author="nik9000" created="2015-11-13T15:45:10Z" id="156468440">Can you try deleting this line from your init script:

```
DAEMON_OPTS="$DAEMON_OPTS -Des.max-open-files=true"
```

That is the only difference I can see between it and the one that comes with 1.7.3. The trouble with lines like the one above is that they break anything with a space in it.

Its very strange that you don't get output from elasticsearch without the `-d`. Its been a while since I tried it. Maybe `start-stop-daemon` is eating it.  Can you try echoing the command that Elasticsearch would run from `bin/elasticsearch` and running it yourself as the right user?

I hate these sorts of issues and I'm really sorry about them.
</comment><comment author="titotp" created="2015-11-13T16:34:20Z" id="156481306">start-stop-daemon --start -b --user elasticsearch -c elasticsearch --pidfile /var/run/elasticsearch.pid --exec /usr/share/elasticsearch/bin/elasticsearch -- -d -p /var/run/elasticsearch.pid -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch

This works fine when i run it from cli. Not from init script
</comment><comment author="titotp" created="2015-11-14T02:03:11Z" id="156608161">@nik9000 figured out the issue. It was some stupid mistake on my side. I have set the elasticsearch_heap_size: 2g on a t2.micro instance that has only 1GB. I reduced that to 1GB  and its working fine.
Thanks for all the help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix mvn verify under jigsaw</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14723</link><project id="" key="" /><description>This is against 2.x branch since builds are working there currently. I will forward port test fixes to master.

This deals with two things:
1. Exception serialization tests: our exception will drop the new `module` information in java 9 `StackTraceElement` so the tests based on strings, etc will compare. Preserving this when both ends are java 9 might be complex, but I fixed the asserts so at least serialization tests are tested correctly.
2. 2.0 backwards tests are skipped for java 9, as 2.0 will die an instant death under jigsaw:

```
Exception in thread "main" java.lang.ExceptionInInitializerError
Likely root cause: java.lang.UnsupportedOperationException: Boot class path mechanism is not supported
at sun.management.RuntimeImpl.getBootClassPath(java.management@9.0/RuntimeImpl.java:99)
at org.elasticsearch.monitor.jvm.JvmInfo.&lt;clinit&gt;(JvmInfo.java:76)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:256)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Closes #13774
</description><key id="116692857">14723</key><summary>fix mvn verify under jigsaw</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.2.0</label></labels><created>2015-11-13T03:22:28Z</created><updated>2015-11-13T04:37:30Z</updated><resolved>2015-11-13T04:37:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-13T03:48:59Z" id="156315326">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close/Open Index Issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14722</link><project id="" key="" /><description>I'm having an issue where closing and opening an index in an integration test appears to fail to properly recover the indexed documents:

https://gist.github.com/ebradshaw/a4fa97b7eac4257c4a2e

Is this an issue, or am I missing something?
</description><key id="116691723">14722</key><summary>Close/Open Index Issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ebradshaw</reporter><labels /><created>2015-11-13T03:08:53Z</created><updated>2015-11-18T19:58:30Z</updated><resolved>2015-11-13T05:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ebradshaw" created="2015-11-13T03:18:19Z" id="156311221">Also, this is appearing in both 1.4.2 and 1.7.3.
</comment><comment author="dadoonet" created="2015-11-13T05:55:11Z" id="156330169">I think you should wait for a yellow status before sending your search request.

BTW you did not tell which exception you are getting and Es version.

Does not look like an issue to me.
</comment><comment author="ebradshaw" created="2015-11-13T12:07:02Z" id="156413437">Thanks for the quick reply, David.

I've tried this with both ES version 1.4.2 and 1.7.3.  Regarding the status, as I understand it ensureSearchable(...) verifies that the index status is GREEN, and I call this before doing the final count.  

I've also added a _refresh call into the test case, as that was necessary for the first assertion to pass:

https://gist.github.com/ebradshaw/a4fa97b7eac4257c4a2e

Regardless, the last assertion fails with:

java.lang.AssertionError: expected:&lt;1&gt; but was:&lt;0&gt;
    at __randomizedtesting.SeedInfo.seed([F88CF07515CF7DEF:C81F9BF1E097BCED]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.junit.Assert.failNotEquals(Assert.java:834)
    at org.junit.Assert.assertEquals(Assert.java:645)
    at org.junit.Assert.assertEquals(Assert.java:631)
    at com.elasticearth.plugin.aggregation.CloseOpenIT.testCloseOpen(CloseOpenIT.java:39)
</comment><comment author="ebradshaw" created="2015-11-14T15:12:13Z" id="156710132">@dadoonet It looks like the old index is deleted and a new one is created when the index is reopened.  When I look at the actual files in the index folder for the shard, all of the data files disappear and the segments_2 file is reverted to segments_1.  Does this still seem normal?
</comment><comment author="dadoonet" created="2015-11-15T11:35:01Z" id="156805074">Wondering if you should not call [flush](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-flush.html) before closing the index? 
</comment><comment author="ebradshaw" created="2015-11-15T15:22:02Z" id="156817490">@dadoonet Tried calling flush and updated the test case.  It didn't seem to make a difference.  

I also ran a similar test using a transport client connected to node running in a separate JVM.  It passed!  Maybe the issue is with the embedded node?
</comment><comment author="ebradshaw" created="2015-11-17T15:39:52Z" id="157407233">@dadoonet Unless I'm missing something, do you think we could reopen this issue?  While I can manage a workaround with a separate JVM, this isn't exactly test friendly :)
</comment><comment author="rjernst" created="2015-11-17T16:10:39Z" id="157416258">@ebradshaw Have you tried looking at existing tests in ES? This test does what you want I believe:
https://github.com/elastic/elasticsearch/blob/6e81b0dd7025fe10d49ade35b5852697ef39355e/core/src/test/java/org/elasticsearch/indices/state/OpenCloseIndexIT.java#L329

My only thought is it looks like you are running on an old version based on the test constructs. What version would that be? If it is before 2.0 (as it appears it to be), you could be getting hit with the translog being fsynced asynchronously.
</comment><comment author="dadoonet" created="2015-11-17T16:11:39Z" id="157416532">I wrote this test and it works fine on 2.0:

``` java
        client.prepareIndex("test", "type").setSource("foo", "bar").get();
        client.admin().cluster().prepareHealth("test").setWaitForYellowStatus().get();
        client.admin().indices().prepareClose("test").get();
        client.admin().indices().prepareOpen("test").get();
        client.admin().cluster().prepareHealth("test").setWaitForYellowStatus().get();
        long count = client.prepareCount("test").get().getCount();
        assertThat(count, is(1L));
```
</comment><comment author="ebradshaw" created="2015-11-18T01:18:38Z" id="157566371">@dadoonet That works for me also.  Must be time to upgrade to 2.0!  

@rjernst I was wondering if it might be something with the translog.  Looks like that testOpenCloseWithDocs wasn't there prior to 2.0.

Thanks for the help here guys.  BTW, it took me a bit of trouble getting the test to work in 2.0.  The [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/using-elasticsearch-test-classes.html) call out two dependencies , but I also found that I had to include hamcrest-all-1.3 + junit-4.12 with hamcrest-core excluded.  If I didn't do this, I was dealing with either NoClassDefFoundErrors or Jar Hell errors.
</comment><comment author="rjernst" created="2015-11-18T19:58:29Z" id="157844449">I know the issues with dependencies are fixed in master. We no longer have a test-jar there, but a full test-framework module, with proper dependencies. There, simply depending on the test-framework is sufficient.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle should check for jarhell of test classpath before running tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14721</link><project id="" key="" /><description>When we have jarhell in tests, we get the jarhell exception for the first class that was run, and then often class not found for every other suite (because we did not load the given jar). We could make this a little nicer, and easier to notice, by having a gradle task call JarHell independently before running test (as a final dependsOn for the test task). It would have the exact classpath the test would use.
</description><key id="116679709">14721</key><summary>Gradle should check for jarhell of test classpath before running tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2015-11-13T01:23:22Z</created><updated>2015-11-20T19:35:49Z</updated><resolved>2015-11-20T19:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-13T01:27:53Z" id="156289131">+1, it has a main method for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unwanted resuls on nested terms aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14720</link><project id="" key="" /><description>Hey, I'm having a problem with unwanted results on aggregation. (Using elasticsearch 1.7)
I'm simplifying my index and my query so you can see the issue
this is how my documents looks like:
{"message":{"info":"a"},"myNum":1}
{"message":{"info":"b"},"myNum":2}
{"message":{"info":"c"},"myNum":3}
{"message":[{"info":"c"},{"info":"a"}],"myNum":4}

this is how my mapping looks like:

```
"nestedType": {
        "properties": {
          "message": {
            "type": "nested",
            "properties": {
              "info": {
                "type": "string",
                "index": "not_analyzed"
              }
            }
          },
          "myNum": {
            "type": "long"
          }
        }
      }
}
```

I want to query only infos that equal to "a" and get their buckets.
so this is my query:

```
{
  "size": 0,
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "message",
          "query": {
            "match": {
              "message.info": {
                "query": "a",
                "type": "phrase"
              }
            }
          }
        }
      }
    }
  },
  "aggregations": {
    "message.name": {
      "nested": {
        "path": "message"
      },
      "aggregations": {
        "infos": {
          "terms": {
            "field": "message.info",
            "size": 10
          }
        }
      }
    }
  }
}
```

but then i get the unwanted "c" on results:

```
 "aggregations": {
    "message.name": {
      "doc_count": 3,
      "infos": {
        "doc_count_error_upper_bound": 0,
        "sum_other_doc_count": 0,
        "buckets": [
          {
            "key": "a",
            "doc_count": 2
          },
          {
            "key": "c",
            "doc_count": 1
          }
        ]
      }
    }
```

how can i remove him on query?
Thanks,
Eliran 
</description><key id="116662445">14720</key><summary>Unwanted resuls on nested terms aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eliranmoyal</reporter><labels /><created>2015-11-12T23:08:04Z</created><updated>2015-11-13T07:17:43Z</updated><resolved>2015-11-13T07:17:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eliranmoyal" created="2015-11-13T07:06:57Z" id="156348178">managed to resolve using another filter

```
{
  "size": 0,
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "message",
          "query": {
            "match": {
              "message.info": {
                "query": "a",
                "type": "phrase"
              }
            }
          }
        }
      }
    }
  },
  "aggregations": {
    "message.name": {
      "nested": {
        "path": "message"
      },
      "aggregations": {
        "filtering": {
          "filter": {
            "term": {
              "info": "a"
            }
          },
          "aggregations": {
            "infos": {
              "terms": {
                "field": "message.info",
                "size": 10
              }
            }
          }
        }
      }
    }
  }
}

```
</comment><comment author="dadoonet" created="2015-11-13T07:17:43Z" id="156351161">Thank you. Closing.
BTW use discuss.elastic.co for questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>url params parsing should not be lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14719</link><project id="" key="" /><description>If you use eg the `_analyze` api, and mispell the `tokenizer`, it will happily go on using the standard _analyzer_ (if you have filters specified, it will ignore them as well).

We should remove all leniency from url param parsing, it does not help anyone, but only causes confusion. Rest actions should remove parameters as they are parsed, and fail when there are extras left (eg `Unknown parameter "tokenzier" for _analyze`)
</description><key id="116635056">14719</key><summary>url params parsing should not be lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:REST</label><label>enhancement</label></labels><created>2015-11-12T20:38:22Z</created><updated>2016-10-04T16:45:29Z</updated><resolved>2016-10-04T16:45:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-12T20:50:41Z" id="156230082">++
</comment><comment author="martinstuga" created="2015-11-28T04:18:45Z" id="160245383">I would be happy to implement this change.

May I go ahead?
</comment><comment author="rjernst" created="2015-11-28T04:25:29Z" id="160247334">@martinstuga Please do! Note that it is a large task. It means handling every rest handler (although doing in a series of smaller PRs would be just fine and easier to review).
</comment><comment author="martinstuga" created="2015-11-28T04:53:54Z" id="160248598">This validation must be implemented on every single RestHandler?

I think that we can do it on a generic and less error prone way by checking if all passed params are used in the handleRequest method. Sounds good?
</comment><comment author="rjernst" created="2015-11-28T04:56:20Z" id="160248658">Yes, what I mean is, right now I believe each reat handler just does a get on the params they use. They will need to do a remove, and then have a check at the end (in shared code) that there are no params left. 
</comment><comment author="martinstuga" created="2015-11-28T05:01:00Z" id="160248874">I think there is a way to do everything with shared code. I'll implement it and then you check if it's correct.
Ok?
</comment><comment author="rjernst" created="2015-11-28T05:22:13Z" id="160249845">Sure, please do. 
</comment><comment author="martinstuga" created="2015-11-30T11:43:58Z" id="160607697">I have a few questions, in the case there're wrong parameters, which behaviour should we implement?

Throw an Exception?
Send a warning message in the response?

In order to keep backward compatibility, don't you think we should add an parameter to enable/disable this feature? 
</comment><comment author="nik9000" created="2015-11-30T14:12:14Z" id="160641305">&gt; Throw an Exception?

Yes. Make sure the exception is translated into a 400 level error.

&gt; In order to keep backward compatibility, don't you think we should add an parameter to enable/disable this feature?

Do it in master and add it to the breaking changes list I think. I'm not personally a huge fan of silently ignoring parameters even in 2.x but I think it'd be quite uncontroversial to do it in 3.0.
</comment><comment author="martinstuga" created="2015-12-06T03:16:12Z" id="162266018">I've already created a PR just with the validation do GET rest requests.
Please, validate if everything is OK and let me know.

Thanks.
</comment><comment author="martinstuga" created="2015-12-10T21:40:36Z" id="163757411">Please, anybody can give me some feedback about the PR #15462?

I'm waiting for this feedback to go ahead with the other rest services (PUT, DELETE,...)

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get packaging working in gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14718</link><project id="" key="" /><description>We skipped getting the rpm and deb packages working gradle. This makes it work.
</description><key id="116623459">14718</key><summary>Get packaging working in gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T19:34:43Z</created><updated>2015-11-18T19:17:11Z</updated><resolved>2015-11-18T19:17:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-12T19:38:50Z" id="156211689">@rjernst `gradle checkDebian8` passes! With the deb file! Its not done at all but its happening!
</comment><comment author="nik9000" created="2015-11-12T23:54:26Z" id="156274386">```
$ gradle checkPackagesAllDistros
```

...
...
...
...
...

```
BUILD SUCCESSFUL

Total time: 1 hrs 2 mins 48.191 secs

This build could be faster, please consider using the Gradle Daemon: https://docs.gradle.org/2.8/userguide/gradle_daemon.html
```

Gotta love the precision on the time measurement.
</comment><comment author="rjernst" created="2015-11-15T04:46:12Z" id="156779916">This is great @nik9000! I left some small style comments, but this looks good overall. It sucks we have to add the empty dir task, but at least you got something working.
</comment><comment author="nik9000" created="2015-11-18T03:17:16Z" id="157590042">Rebased and addressed the comments.
</comment><comment author="rjernst" created="2015-11-18T19:09:26Z" id="157825345">Great, thank you @nik9000! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce JAVA_HOME is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14717</link><project id="" key="" /><description>If we use JAVA_HOME consistently for tests, we can run tests with a
different version of java than gradle runs with. For example, this
enables running tests with jigsaw, but building with java 8. The only
caveat is intellij does not set JAVA_HOME. This change enforces
JAVA_HOME is set, but ignores for intellij.
</description><key id="116609673">14717</key><summary>Enforce JAVA_HOME is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T18:23:47Z</created><updated>2015-11-12T18:45:35Z</updated><resolved>2015-11-12T18:45:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-12T18:26:48Z" id="156192255">+1
</comment><comment author="jasontedor" created="2015-11-12T18:27:55Z" id="156192806">Would requiring and using [`org.grade.java.home`](https://docs.gradle.org/current/userguide/build_environment.html) work so that it can easily be set from IntelliJ too?
</comment><comment author="rmuir" created="2015-11-12T18:32:18Z" id="156194171">The idea is to be able to run tests etc against jigsaw builds. So that property is not interesting, thats the property for gradle itself.
</comment><comment author="rjernst" created="2015-11-12T18:33:02Z" id="156194338">@jasontedor Intellij doesnt set _anything_ for the current jdk the project is configured with. I don't think we should require `org.gradle.java.home` as in the normal case, running gradle with the same java you are testing with is fine. I personally think intellij is wrong here, but after creating an issue for them to set java home, they promptly closed the issue...
https://youtrack.jetbrains.com/issue/IDEA-147331
</comment><comment author="rmuir" created="2015-11-12T18:33:59Z" id="156194658">Yeah, we can't "not test jigsaw" because intellij is broken. Java 9 is coming too soon and if we just push this off until the last minute because of a broken IDE, then it will be this mad rush to fix things. We need to avoid that.
</comment><comment author="rjernst" created="2015-11-12T18:34:14Z" id="156194733">By "anything" I'm being a little harsh, but my point is there is nothing we can use in the intellij case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient remains in a dirty state after a failed write to a node, after the node comes back up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14716</link><project id="" key="" /><description>A TransportClient holds a reference to a TransportClientNodesService and that service will disconnect from a node on a failed write.  If the TransportClientNodesService is in sniffing mode, it will re-establish its TransportClientNodesService#transportService field as being connected.  However, that state is not transferred to the TransportClient, so all future writes fail.

After a failed write, the node service throws this exception:

```
e   ConnectTransportException  (id=5122)    
'e' referenced from:    
action  null    
address InetSocketTransportAddress  (id=5121)   
cause   ConnectException  (id=5127) 
detailMessage   "[][inet[es.listener/192.168.56.101:9300]] connect_timeout[30s]" (id=5128)  
node    DiscoveryNode  (id=4676)    
stackTrace  StackTraceElement[0]  (id=5070) 
suppressedExceptions    Collections$UnmodifiableRandomAccessList&lt;E&gt;  (id=5071)  
```

After the node comes back up, the sniffer will re-attach:

```
    [[Carolyn Trainer][a_SJUenqR5ykMkJ0VtQJhA][rcoe-mintx64-vbox][inet[/192.168.56.101:9300]]]
```

However, that state is not transferred to the TransportClient.  Attempting to use a client for a node that comes back up throws:

```
e   ElasticsearchIllegalStateException  (id=5067)   
    'e' referenced from:    
    cause   ElasticsearchIllegalStateException  (id=5067)   
    detailMessage   "client is closed" (id=5069)    
    stackTrace  StackTraceElement[0]  (id=5070) 
    suppressedExceptions    Collections$UnmodifiableRandomAccessList&lt;E&gt;  (id=5071)  
```
</description><key id="116603501">14716</key><summary>TransportClient remains in a dirty state after a failed write to a node, after the node comes back up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2015-11-12T17:49:34Z</created><updated>2015-11-12T20:25:42Z</updated><resolved>2015-11-12T20:25:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2015-11-12T18:34:03Z" id="156194681">BTW, this is on the 1.7 branch.  The elasticsearch client is used by the log4j2-elasticsearch plugin and it currently builds to 1.7.  If this sounds like a bug that was fixed in 2.x, I can test against that version.
</comment><comment author="ghost" created="2015-11-12T20:25:42Z" id="156224230">This is not an Elasticsearch issue.  Looks like a state problem in the log4j2-elasticsearch plugin's use of the TransportClient, which is relying on separate state handling from the internally held TransportClient#TransportService.  The fix belongs to that project.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.0.0 does not accept timestamp path and mapping. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14715</link><project id="" key="" /><description>Hi,

The following template mapping 

&gt; curl -XPUT http://localhost:9200/_template/service_01 -d '{
&gt; "template" : "service*",
&gt; "order":    1,
&gt; "settings" : {
&gt; "index.cache.field.type" : "soft",
&gt; "index.refresh_interval" : "5s",
&gt; "index.store.compress.stored" : true,
&gt; "mapping.allow_type_wrapper": true
&gt; },
&gt; "mappings" : {
&gt; "_default_" : {
&gt; "_all" : {"enabled" : false},
&gt; "_timestamp" : {"enabled" : true,"path" : "0STARTTIMESTAMP","format" : "YYYY/MM/dd HH:mm:ss"},
&gt; "properties" : {
&gt; "0STARTTIMESTAMP": {
&gt; "type": "date",
&gt; "format": "YYYY/MM/dd HH:mm:ss"
&gt; },
&gt; "STOPTIMESTAMP": {
&gt; "type": "date",
&gt; "format": "YYYY/MM/dd HH:mm:ss"
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }'

Works great in ES 1.4.4. For the below data. 

&gt; {"index":{"_index":"service_sapstats","_type":"mytype"}}
&gt; {"0STARTTIMESTAMP":"2015/02/13 13:00:00","*STOPTIMESTAMP":"2015/02/13 13:15:00","data":"somedata"}

But in ES 2.0.0 it throws the below exceptions. 

&gt; {"index":{"_index":"service_sapstats","_type":"SAM","_id":null,"status":400,"error":{"type":"mapper_parsing_exception","reason":"mapping [_default_]","caused_by":{"type":"mapper_parsing_exception","reason":"Mapping definition for [_timestamp] has unsupported parameters:  [path : 0STARTTIMESTAMP]"}}

in ES 2.0.0
</description><key id="116578594">14715</key><summary>ES 2.0.0 does not accept timestamp path and mapping. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-11-12T15:55:50Z</created><updated>2016-02-16T16:11:39Z</updated><resolved>2015-11-13T00:35:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-13T00:35:59Z" id="156280976">These features were explicitly removed from 2.0. See #8143 and #6677.
</comment><comment author="rjernst" created="2015-11-13T00:37:05Z" id="156281130">@nithyanv You can create your own `date` type field (which already supports timestamps).
</comment><comment author="l15k4" created="2016-02-16T16:11:39Z" id="184749228">@rjernst hey, do you know whether I have to re-index everything because of that : https://discuss.elastic.co/t/upgrade-from-1-7-x-to-2-2-0--timestamp-mapping-issue/41894

The reindexing would took 47 hours, so I'm kinda screwed...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Before 2.0.0] QueryParsingException - not of nested type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14714</link><project id="" key="" /><description>The below steps cause a query to fail with a `QueryParsingException` that seems to be fixed by either restarting elasticsearch or reindexing the data.

I've tested on all versions from 1.4.2 to 2.0.0. Only on version 2.0.0 does step 4 not throw an exception.

**1. Create mappings (2 nested fields)**

```
curl -XPOST 'http://127.0.0.1:9200/product' -d '
{
  "mappings": {
    "release": {
      "properties": {
        "formats": {
          "type": "nested",
          "properties": {
            "format_id": {
              "type": "integer"
            }
          }
        },
        "prices": {
          "type": "nested",
          "properties": {
            "supplier_short_name": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }
}'
```

`{"acknowledged":true}`

**2. Insert document**

```
curl -XPUT 'http://127.0.0.1:9200/product/release/MUS-REL-0003017001' -d '
{
    "formats": [
        {
            "format_id": 17
        }
    ],
    "prices": [
        {
            "supplier_short_name": "digital"
        }
    ]
}'
```

`{"_index":"product","_type":"release","_id":"MUS-REL-0003017001","_version":1,"created":true}`

**3. Try to update nested `formats` field with a string**
Update will fail which is expected as we are trying to update a nested field with a string.

```
curl -XPOST 'http://127.0.0.1:9200/product/release/MUS-REL-0003017001/_update' -d '
{
  "doc": {
    "formats": "MP3 320",
    "prices": [
      {
        "supplier_short_name": "digital"
      }
    ]
  }
}'
```

`MapperParsingException[object mapping for [release] tried to parse as object, but got EOF, has a concrete value been provided to it?]`

**4. Run the query**
Step 3 causes this query to fail which is not expected. To get the query to run you can restart elasticsearch or reindex the data.

```
curl -XPOST 'http://127.0.0.1:9200/product/release/_search' -d '
{
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "nested": {
              "path": "prices",
              "filter": {
                "and": [
                  {
                    "exists": {
                      "field": "prices"
                    }
                  }
                ]
              }
            }
          }
        ]
      }
    }
  }
}'
```

`nested: QueryParsingException[[product] [nested] nested object under path [prices] is not of nested type]`
</description><key id="116578344">14714</key><summary>[Before 2.0.0] QueryParsingException - not of nested type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JustinHook</reporter><labels /><created>2015-11-12T15:54:38Z</created><updated>2015-11-17T14:19:22Z</updated><resolved>2015-11-17T14:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T14:19:22Z" id="157382669">Hi @JustinHook 

Yes, this is a bug in the mappings.  Unfortunately, it is a really hard one to fix in 1.x  A lot of the changes in 2.0 were about cleaning up this kind of stuff.

We wouldn't be able to fix this in 1.x without backporting half of 2.0, so I'm going to close this as won't fix.

thanks for the good recreation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_shape contains support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14713</link><project id="" key="" /><description>In elasticsearch the relation between a query shape and an indexed shape can be `within`, `intersect` or `disjoint`. The underlying lucene would also support `contains`:
https://lucene.apache.org/core/5_3_1/spatial3d/org/apache/lucene/geo3d/GeoArea.html

Are there some special reasons why `contains` is not supported yet or was it just omitted because it's not important for the most usecases? I would appreciate the possibility to use the `contains` relation.
</description><key id="116578084">14713</key><summary>geo_shape contains support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">bogensberger</reporter><labels><label>:Geo</label><label>feature</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T15:53:09Z</created><updated>2016-01-06T23:09:48Z</updated><resolved>2015-11-18T20:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-11-17T20:01:38Z" id="157489056">Hi @bogensberger,

Thanks for the question. A few points to help answer your question:
- ES doesn't currently use the spatial3d package as its performance and API is not yet ready for integration.
- Lucene spatial now supports `CONTAINS` and it is extremely low hanging fruit to add. So I'll actually open a PR shortly to add this functionality to 2.2+. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add sync_id to cat shards API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14712</link><project id="" key="" /><description>This commit adds the ability to get the sync_id from the cat shards API.

Closes #14705
</description><key id="116572109">14712</key><summary>Add sync_id to cat shards API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T15:27:41Z</created><updated>2015-11-13T14:10:04Z</updated><resolved>2015-11-13T10:13:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-12T15:52:50Z" id="156146199">LGTM. Left a minor suggestion. I think this can go into 2.2 as well, right?
</comment><comment author="jasontedor" created="2015-11-12T17:51:07Z" id="156182555">&gt; I think this can go into 2.2 as well, right?

Yes.

I pushed a REST test in 34c25dff424af0f0e4320103837881a1a55c3175. Can you take a look @bleskes?
</comment><comment author="bleskes" created="2015-11-12T17:56:22Z" id="156183816">Thx @jasontedor . Made minor comments
</comment><comment author="bleskes" created="2015-11-13T10:09:49Z" id="156384906">LGTM, again.
</comment><comment author="rtoma" created="2015-11-13T10:18:52Z" id="156388316">Once done, can you please backport to 1.7.x?
</comment><comment author="jasontedor" created="2015-11-13T13:29:25Z" id="156431594">Integrated into [master](https://github.com/elastic/elasticsearch/commit/0a5323f4f7e2a12a16f4dc95f5b85edd52299571) and [2.x](https://github.com/elastic/elasticsearch/commit/9f1f376d057056f74cff44c802a4352b5871cad2) for inclusion in 2.2.0.
</comment><comment author="jasontedor" created="2015-11-13T14:10:04Z" id="156442073">&gt; Once done, can you please backport to 1.7.x?

@rtoma At this time, we are only pushing bug fixes to the 1.x line.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No andFilter alternative in Elasticsearch 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14711</link><project id="" key="" /><description>The documentation states that filters from FilterBuilders are available in QueryBuilders.
However that is not the case for FilterBuilders.andFilter(). There is no QueryBuilders.andQuery().

What is the appropriate replacement?
</description><key id="116571144">14711</key><summary>No andFilter alternative in Elasticsearch 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">plebedev</reporter><labels /><created>2015-11-12T15:22:47Z</created><updated>2015-11-13T16:20:36Z</updated><resolved>2015-11-13T16:20:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="plebedev" created="2015-11-13T16:20:35Z" id="156477922">Looks like boolQuery has to be used instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle build fails on ibm jdk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14710</link><project id="" key="" /><description>```
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version : 2.8
  JDK Version    : pxa6480sr1-20150417_01 (SR1) (IBM Corporation)
  OS Info        : Linux 4.2.0-16-generic (amd64)

FAILURE: Build failed with an exception.

* Where:
Build file '/home/rmuir/workspace/elasticsearch/core/build.gradle' line: 24

* What went wrong:
A problem occurred evaluating project ':core'.
&gt; Failed to apply plugin [id 'elasticsearch.build']
   &gt; Cannot invoke method contains() on null object

* Try:
Run with --info or --debug option to get more log output.

* Exception is:
org.gradle.api.GradleScriptException: A problem occurred evaluating project ':core'.
    at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:93)
    at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$1.run(DefaultScriptPluginFactory.java:148)
    at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:72)
    at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:153)
    at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38)
    at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25)
    at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34)
    at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55)
    at org.gradle.api.internal.project.AbstractProject.evaluate(AbstractProject.java:499)
    at org.gradle.api.internal.project.AbstractProject.evaluate(AbstractProject.java:86)
    at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:47)
    at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:35)
    at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:125)
    at org.gradle.internal.Factories$1.create(Factories.java:22)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:52)
    at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:122)
    at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:32)
    at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:99)
    at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:93)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:62)
    at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:93)
    at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:82)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:94)
    at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)
    at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:43)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28)
    at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77)
    at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47)
    at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51)
    at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28)
    at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43)
    at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210)
    at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35)
    at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)
    at org.gradle.launcher.Main.doAction(Main.java:33)
    at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)
    at org.gradle.launcher.GradleMain.main(GradleMain.java:23)
Caused by: org.gradle.api.internal.plugins.PluginApplicationException: Failed to apply plugin [id 'elasticsearch.build']
    at org.gradle.api.internal.plugins.DefaultPluginManager.doApply(DefaultPluginManager.java:160)
    at org.gradle.api.internal.plugins.DefaultPluginManager.apply(DefaultPluginManager.java:112)
    at org.gradle.api.internal.plugins.DefaultObjectConfigurationAction.applyType(DefaultObjectConfigurationAction.java:112)
    at org.gradle.api.internal.plugins.DefaultObjectConfigurationAction.access$200(DefaultObjectConfigurationAction.java:35)
    at org.gradle.api.internal.plugins.DefaultObjectConfigurationAction$3.run(DefaultObjectConfigurationAction.java:79)
    at org.gradle.api.internal.plugins.DefaultObjectConfigurationAction.execute(DefaultObjectConfigurationAction.java:135)
    at org.gradle.api.internal.project.AbstractPluginAware.apply(AbstractPluginAware.java:46)
    at org.gradle.api.plugins.PluginAware$apply.call(Unknown Source)
    at org.gradle.api.internal.project.ProjectScript.apply(ProjectScript.groovy:34)
    at org.gradle.api.Script$apply.callCurrent(Unknown Source)
    at build_a1fw0t7njqucp7ny71abi508r.run(/home/rmuir/workspace/elasticsearch/core/build.gradle:24)
    at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:91)
    ... 47 more
Caused by: java.lang.NullPointerException: Cannot invoke method contains() on null object
    at org.elasticsearch.gradle.BuildPlugin.configureRepositories(BuildPlugin.groovy:109)
    at org.elasticsearch.gradle.BuildPlugin$configureRepositories$0.callStatic(Unknown Source)
    at org.elasticsearch.gradle.BuildPlugin.apply(BuildPlugin.groovy:41)
    at org.elasticsearch.gradle.BuildPlugin.apply(BuildPlugin.groovy)
    at org.gradle.api.internal.plugins.ImperativeOnlyPluginApplicator.applyImperative(ImperativeOnlyPluginApplicator.java:35)
    at org.gradle.api.internal.plugins.RuleBasedPluginApplicator.applyImperative(RuleBasedPluginApplicator.java:44)
    at org.gradle.api.internal.plugins.DefaultPluginManager.doApply(DefaultPluginManager.java:144)
    ... 58 more


BUILD FAILED

Total time: 4.553 secs
```
</description><key id="116556699">14710</key><summary>gradle build fails on ibm jdk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-12T14:17:54Z</created><updated>2016-11-16T14:58:53Z</updated><resolved>2016-11-16T14:58:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-13T00:30:31Z" id="156280075">The code is this:

```
project.configurations.compile.dependencies.all { dep -&gt;
    if (!(dep instanceof ProjectDependency) &amp;&amp; dep.getGroup() != 'org.elasticsearch') {
        dep.transitive = false
    }
}
```

This means `dep` is null, which should be impossible since are iterating the dependencies...
</comment><comment author="clintongormley" created="2016-02-14T16:17:55Z" id="183910986">@rjernst is this still an issue?
</comment><comment author="rjernst" created="2016-06-15T01:25:52Z" id="226063929">We still get failures. First, we can't even run a build normally because we check for `jps` existing, which does not come with the ibm jdk. If I disable that check and just run `gradle test` in core, I see tons of access control failures because mockito under ibm jdk appears to use some different internal classes for proxy construction which our securemock does not add permissions for.
https://gist.github.com/rjernst/c8af285eeb3336488338ed455ff65800

I don't plan to work on this, but thought I would at least update to confirm there are still problems, even if not the original problems this issue was created for.
</comment><comment author="rmuir" created="2016-06-15T01:38:06Z" id="226065531">I discussed with ryan, the security issue is because mockito et all are using internal apis to do evilish stuff. So of course its not guaranteed they use the same one on IBM :) We just have to give mockito jar another permission in the `test-framework.policy` and then it works.
</comment><comment author="rjernst" created="2016-06-15T01:48:16Z" id="226066759">I pushed fa77d4d with the test permission updates. The remaining test failures in core are for probes:

```
Suite: org.elasticsearch.monitor.process.ProcessProbeTests
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=5F652E0448DCAB4D -Dtests.class=org.elasticsearch
.monitor.process.ProcessProbeTests -Dtests.method="testProcessStats" -Dtests.security.manager=true -
Dtests.locale=en-IE -Dtests.timezone=Pacific/Wake
FAILURE 0.02s J3 | ProcessProbeTests.testProcessStats &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError:
   &gt; Expected: a value greater than &lt;0L&gt;
   &gt;      but: &lt;-1L&gt; was less than &lt;0L&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([5F652E0448DCAB4D:1433482120E002E]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.monitor.process.ProcessProbeTests.testProcessStats(ProcessProbeTests.ja
va:56)
   &gt;    at java.lang.Thread.run(Thread.java:785)
```

It would appear the jmx beans we use are not all available in ibm jdk.
</comment><comment author="shahidhs-ibm" created="2016-11-16T13:44:25Z" id="260949646">@rmuir I am curious to know which Elasticsearch version you were working on? I believe at the time when you created this post, Elasticsearch was using Maven and not Gradle.
@rjernst We are trying to build Elasticsearch with IBM Java and still getting same issue. Do you have any plans to provide IBM Java support?
</comment><comment author="nik9000" created="2016-11-16T14:58:52Z" id="260967185">&gt; We are trying to build Elasticsearch with IBM Java and still getting same issue. Do you have any plans to provide IBM Java support?

I don't believe we have plans to support the IBM JDK. Maybe one day, not now though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation issue in License management section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14709</link><project id="" key="" /><description>In the license management section, 
https://www.elastic.co/guide/en/shield/current/license-management.html

The curl request given as example is -
curl -XPUT -u admin 'http://[host]:[port]/_license?acknowledge=true' -d @license.json

The index name used is _license_ it should have been _licenses_ instead. It's a small typo but can be frustrating for someone starting new with ELK stack.
</description><key id="116542815">14709</key><summary>Documentation issue in License management section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tarunsapra</reporter><labels /><created>2015-11-12T12:47:10Z</created><updated>2015-11-17T13:53:51Z</updated><resolved>2015-11-17T13:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T13:53:51Z" id="157375472">Hi @tarunsapra 

In 1.x, it used to be `PUT /_licenses` but in 2.0 it is `PUT /_license`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>real numbers from aggregation ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14708</link><project id="" key="" /><description>Hello guys.
A. Is there any way to fetch real numbers from aggregations (terms, cardinality) ?
At moment we have have almost 3 000 000 flat records and aggregated values with relative error.

B. how can I apply LIMIT(from, to) to aggregation? It possible for plain query, But I was not able to do that with aggregations.

Thanks.
</description><key id="116542102">14708</key><summary>real numbers from aggregation ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sonenko</reporter><labels /><created>2015-11-12T12:41:45Z</created><updated>2015-11-17T13:49:23Z</updated><resolved>2015-11-17T13:49:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T13:49:23Z" id="157374596">Hi @sonenko 

Please use the forums to ask questions like these: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeout mechanism for sending shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14707</link><project id="" key="" /><description>This commit adds a timeout mechanism for sending shard failures. The
requesting thread can attach a listener to the timeout event so that
handling it is part of the event chain.

Relates #14252 
</description><key id="116537596">14707</key><summary>Add timeout mechanism for sending shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T12:11:08Z</created><updated>2015-11-17T21:42:54Z</updated><resolved>2015-11-17T21:42:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-13T16:00:54Z" id="156473042">Left some comments.
</comment><comment author="bleskes" created="2015-11-13T19:52:56Z" id="156540705">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.ClassCastException when upgrading to 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14706</link><project id="" key="" /><description>Trying to upgrade from 1.7.2 to 2.0.0, following the instructions provided on:

https://www.elastic.co/guide/en/elasticsearch/reference/2.0/restart-upgrade.html

Shut the whole cluster down, removed old plugins, installed the new cloud-aws plugin.

When starting the first master node with 2.0.0 installed, I'm getting:

```
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
```

Full log:

```
root@cubitsearch-master-1:~# sudo -u elasticsearch /usr/bin/java -Xms3g -Xmx3g -Xmn200m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.0.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch -Des.max-open-files=true
[2015-11-12 11:49:58,131][INFO ][bootstrap                ] max_open_files [4096]
[2015-11-12 11:49:59,130][INFO ][node                     ] [cubitsearch-master-1] version[2.0.0], pid[29560], build[de54438/2015-10-22T08:09:48Z]
[2015-11-12 11:49:59,130][INFO ][node                     ] [cubitsearch-master-1] initializing ...
[2015-11-12 11:50:00,367][INFO ][plugins                  ] [cubitsearch-master-1] loaded [cloud-aws], sites []
[2015-11-12 11:50:00,514][INFO ][env                      ] [cubitsearch-master-1] using [1] data paths, mounts [[/ (/dev/xvda1)]], net usable_space [5.8gb], net total_space [7.8gb], spins? [no], types [ext4]
[2015-11-12 11:50:10,624][ERROR][gateway                  ] [cubitsearch-master-1] failed to read local state, exiting...
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2015-11-12 11:50:12,770][ERROR][gateway                  ] [cubitsearch-master-1] failed to read local state, exiting...
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2015-11-12 11:50:14,300][ERROR][gateway                  ] [cubitsearch-master-1] failed to read local state, exiting...
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Exception in thread "main" java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</description><key id="116535443">14706</key><summary>java.lang.ClassCastException when upgrading to 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">krisb78</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2015-11-12T11:56:07Z</created><updated>2015-12-03T09:03:44Z</updated><resolved>2015-12-03T09:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T13:43:28Z" id="157373473">Hi @krisb78 

Could you try it without the cloud aws plugin and see if you have the same problem?  Also, could you upload the output of `GET /_cluster/state` (which you'll need to get from the 1.7.2 cluster)

thanks
</comment><comment author="krisb78" created="2015-11-17T14:19:19Z" id="157382634">Hi @clintongormley ,

Here's my cluster state, I'll try to start the node without cloud-aws and let you know shortly.

[cluster_state.txt](https://github.com/elastic/elasticsearch/files/36746/cluster_state.txt)
</comment><comment author="Bartinger" created="2015-11-17T14:23:06Z" id="157384259">I just ran into the same issue.
[cluster-state.json.txt](https://github.com/elastic/elasticsearch/files/36756/cluster-state.json.txt)
</comment><comment author="krisb78" created="2015-11-17T14:23:40Z" id="157384464">Tried without cloud-aws, but got the same error:

```
root@cubitsearch-master-1:/usr/share/elasticsearch# sudo -u elasticsearch /usr/bin/java -Xms3g -Xmx3g -Xmn200m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.0.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch -Des.max-open-files=true
[2015-11-17 14:22:36,284][INFO ][bootstrap                ] max_open_files [4096]
[2015-11-17 14:22:37,329][INFO ][node                     ] [cubitsearch-master-1] version[2.0.0], pid[7642], build[de54438/2015-10-22T08:09:48Z]
[2015-11-17 14:22:37,329][INFO ][node                     ] [cubitsearch-master-1] initializing ...
[2015-11-17 14:22:37,664][INFO ][plugins                  ] [cubitsearch-master-1] loaded [], sites []
[2015-11-17 14:22:38,048][INFO ][env                      ] [cubitsearch-master-1] using [1] data paths, mounts [[/ (/dev/xvda1)]], net usable_space [5.8gb], net total_space [7.8gb], spins? [no], types [ext4]
[2015-11-17 14:22:46,488][ERROR][gateway                  ] [cubitsearch-master-1] failed to read local state, exiting...
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2015-11-17 14:22:48,683][ERROR][gateway                  ] [cubitsearch-master-1] failed to read local state, exiting...
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2015-11-17 14:22:50,248][ERROR][gateway                  ] [cubitsearch-master-1] failed to read local state, exiting...
java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:47)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Exception in thread "main" java.lang.ClassCastException: org.elasticsearch.cluster.metadata.AliasOrIndex$Index cannot be cast to org.elasticsearch.cluster.metadata.AliasOrIndex$Alias
    at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1008)
    at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:104)
    at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:97)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:223)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</comment><comment author="martijnvg" created="2015-11-18T16:07:13Z" id="157761765">@krisb78 Looking at your cluster stats I found that there is an index named `hotcoffee_v1` and also an alias named `hotcoffee_v1`. Alias and index names need to be unique in a cluster, so this causes this  unexpected failure. I wonder how this could have happened... did you by any chance do a restore with a rename?
</comment><comment author="krisb78" created="2015-11-18T16:28:08Z" id="157768252">This indeed is the case - I first created a template with the pattern set to 'hotcoffee_v1*', then started indexing into the 'hotcoffee_v1' index - the cluster (1.7.2) didn't complain.

So I have:

```
[LIVE] krisba@cubitsearch-client-2:~$ curl -XGET localhost:9200/_template/hotcoffee_v1?pretty
{
  "hotcoffee_v1" : {
    "order" : 0,
    "template" : "hotcoffee_v1*",
    "settings" : {
      "index.merge.policy.merge_factor" : "30",
      "index.refresh_interval" : "-1"
    },
    "mappings" : {
      "hotcoffee" : {
        "dynamic_templates" : [ {
          "store_generic" : {
            "mapping" : {
              "index" : "not_analyzed",
              "store" : true,
              "doc_values" : true
            },
            "match" : "*"
          }
        } ],
        "_ttl" : {
          "enabled" : true,
          "default" : "90d"
        },
        "properties" : {
          "pf_timestamp" : {
            "store" : "true",
            "doc_values" : true,
            "type" : "date"
          },
          "pf_prev_timestamp" : {
            "store" : "true",
            "doc_values" : true,
            "type" : "date"
          }
        }
      }
    },
    "aliases" : {
      "hotcoffee_v1" : { }
    }
  }
}
```

and then

```
[LIVE] krisba@cubitsearch-client-2:~$ curl -XGET localhost:9200/hotcoffee_v1?pretty
{
  "hotcoffee_v1" : {
    "aliases" : {
      "hotcoffee_v1" : { }
    },
    "mappings" : {
      "hotcoffee" : {
        "dynamic_templates" : [ {
          "store_generic" : {
            "mapping" : {
              "index" : "not_analyzed",
              "store" : true,
              "doc_values" : true
            },
            "match" : "*"
          }
        } ],
        "_ttl" : {
          "enabled" : true,
          "default" : 7776000000
        },
        "properties" : {
          "ach_name" : {
            "type" : "string",
            "index" : "not_analyzed",
            "store" : true,
            "doc_values" : true
          },
          "app_name" : {
            "type" : "string",
            "index" : "not_analyzed",
            "store" : true,
            "doc_values" : true
          },
          "appid" : {
            "type" : "long",
            "store" : true,
            "doc_values" : true
          },
          "pf_network" : {
            "type" : "string",
            "index" : "not_analyzed",
            "store" : true,
            "doc_values" : true
          },
          "pf_playtime_delta" : {
            "type" : "long",
            "store" : true,
            "doc_values" : true
          },
          "pf_prev_timestamp" : {
            "type" : "date",
            "store" : true,
            "doc_values" : true,
            "format" : "dateOptionalTime"
          },
          "pf_steamid" : {
            "type" : "string",
            "index" : "not_analyzed",
            "store" : true,
            "doc_values" : true
          },
          "pf_timestamp" : {
            "type" : "date",
            "store" : true,
            "doc_values" : true,
            "format" : "dateOptionalTime"
          },
          "pf_type" : {
            "type" : "string",
            "index" : "not_analyzed",
            "store" : true,
            "doc_values" : true
          },
          "playtime_2weeks" : {
            "type" : "long",
            "store" : true,
            "doc_values" : true
          },
          "playtime_forever" : {
            "type" : "long",
            "store" : true,
            "doc_values" : true
          },
          "timestamp_delta_minutes" : {
            "type" : "long",
            "store" : true,
            "doc_values" : true
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "creation_date" : "1433238565453",
        "uuid" : "tE5bQrzkT5-Hei6TMp7KVQ",
        "number_of_replicas" : "1",
        "number_of_shards" : "5",
        "refresh_interval" : "-1",
        "version" : {
          "created" : "1050299"
        },
        "merge" : {
          "policy" : {
            "merge_factor" : "30"
          }
        }
      }
    },
    "warmers" : { }
  }
}
```

I think I have one more index/template set up like that, it never caused me a problem in 1.7.2...

Has it changed in 2.0.0 ?
</comment><comment author="martijnvg" created="2015-11-18T16:38:02Z" id="157771036">No (except than the error that you're seeing that expects index and alias names to be unique). So this error needs to be fixed and be able to handle alias and index with the same name.

I do think that adding an alias with the same name as the index via index templates should not be allowed.
</comment><comment author="krisb78" created="2015-11-18T16:53:10Z" id="157776609">Hm, I see your point...

I'll check if renaming these aliases fixes the issue.
</comment><comment author="clintongormley" created="2015-11-18T18:45:54Z" id="157817361">&gt; I do think that adding an alias with the same name as the index via index templates should not be allowed.

agreed!  We should 
</comment><comment author="clintongormley" created="2015-11-18T18:48:54Z" id="157818351">It looks like trying to create an index and alias with the same name in 2.0 throws the same exception, so this should become clearer with the better exception proposed in #14842
</comment><comment author="clintongormley" created="2015-11-18T18:51:35Z" id="157819172">It looks like it is impossible to remove an alias with the same name as an index in 1.x: 

```
InvalidAliasNameException[[foo] Invalid alias name [foo], an index exists with the same name as the alias]
```

So I think the only solution is to reindex into a new index, and to delete the old index
</comment><comment author="krisb78" created="2015-11-19T10:40:29Z" id="158018973">Ok, I think I will be able to get out of this by making sure I don't create new indices with the same names as the aliases defined in my templates.

Thanks a lot for looking into this.
</comment><comment author="martijnvg" created="2015-12-03T09:03:44Z" id="161557549">Fixed via #14842
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose sync id marker in _cat/shards API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14705</link><project id="" key="" /><description>Is it possible to expose the sync id in the _cat/shards API?
This would make is much easier for operators to check ids, than using the indices stats API.
</description><key id="116513043">14705</key><summary>Expose sync id marker in _cat/shards API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rtoma</reporter><labels><label>:CAT API</label><label>enhancement</label></labels><created>2015-11-12T09:40:47Z</created><updated>2015-11-13T10:13:41Z</updated><resolved>2015-11-13T10:13:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-12T09:47:15Z" id="156051569">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle system policy correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14704</link><project id="" key="" /><description>Currently we have these lines:

```
//// System code permissions:
//// These permissions apply to the JDK itself:

grant codeBase "file:${{java.ext.dirs}}/*" {
  permission java.security.AllPermission;
};
```

But this is not really offically correct, and a problem going forwards, see https://bugs.openjdk.java.net/browse/JDK-8040059 and related issues (https://bugs.openjdk.java.net/secure/IssueNavigator.jspa?reset=true&amp;jqlQuery=labels+%3D+deprivilege)

Current java 9 config is here: http://hg.openjdk.java.net/jdk9/jdk9/jdk/file/tip/src/java.base/share/conf/security/java.policy

So we must properly bring in the system policy, and just disable the bad defaults it has (with escape hatch in case there is some issue with that). This makes things better for users out of box.

It also makes us properly behaved, respecting user and system configuration. See https://docs.oracle.com/javase/8/docs/technotes/guides/security/PolicyFiles.html for more information.

Closes #14690
</description><key id="116497416">14704</key><summary>Handle system policy correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T07:45:36Z</created><updated>2015-11-16T14:18:12Z</updated><resolved>2015-11-12T23:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-12T08:12:02Z" id="156030284">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify where min java version is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14703</link><project id="" key="" /><description>This moves the min java version used by elasticsearch to one place, a
constant in BuildPlugin. For me on java 9, this fixed my jar to have the
correct target/source versions.

closes #14702
</description><key id="116493691">14703</key><summary>Simplify where min java version is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T07:14:15Z</created><updated>2015-11-12T07:48:06Z</updated><resolved>2015-11-12T07:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-12T07:37:23Z" id="156025681">I tested on osx and linux. I believe the reason it failed before was the target version we set was overriden because we set it first as an ext var in the root allprojects, and then each project would import the java plugin, and that would overwrite the property and initialize to the version of java running.
</comment><comment author="rmuir" created="2015-11-12T07:46:43Z" id="156027035">looks good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle puts wrong java version in manifest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14702</link><project id="" key="" /><description>if i build a zip with java 9, then switch to java 8 to run it:

```
rmuir@beast:~/workspace/elasticsearch/distribution/zip/build/distributions/elasticsearch-3.0.0-SNAPSHOT$ bin/elasticsearch
Exception in thread "main" java.lang.IllegalStateException: /home/rmuir/workspace/elasticsearch/distribution/zip/build/distributions/elasticsearch-3.0.0-SNAPSHOT/lib/elasticsearch-3.0.0-SNAPSHOT.jar requires Java 1.9:, your system: 1.8
    at org.elasticsearch.bootstrap.JarHell.checkJavaVersion(JarHell.java:249)
    at org.elasticsearch.bootstrap.JarHell.checkManifest(JarHell.java:217)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:177)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:87)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:164)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```
</description><key id="116490875">14702</key><summary>gradle puts wrong java version in manifest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-12T06:48:24Z</created><updated>2015-11-12T07:48:04Z</updated><resolved>2015-11-12T07:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-12T06:49:45Z" id="156017221">actually the class files must be correct, we aren't that good. its the manifest that is wrong.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>integration test fails and leaks a JVM on java 9EA</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14701</link><project id="" key="" /><description>This is separate from the commons lang issue affecting 'deb' (#14700).
This one impacts 'zip'

```
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version : 2.8
  JDK Version    : 1.9.0-ea-b91 (Oracle Corporation)
  OS Info        : Linux 4.2.0-16-generic (amd64)
:core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:core:processResources
:core:classes
:core:jar
:distribution:zip:buildZip
:rest-api-spec:compileJava UP-TO-DATE
:rest-api-spec:processResources
:rest-api-spec:classes
:rest-api-spec:jar
:distribution:zip:processTestResources UP-TO-DATE
:distribution:zip:copyRestSpec
:test-framework:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:test-framework:processResources
:test-framework:classes
:test-framework:jar
:distribution:zip:compileTestJava
:distribution:zip:testClasses
:distribution:zip:integTest#clean
:distribution:zip:integTest#checkPrevious SKIPPED
:distribution:zip:integTest#stopPrevious SKIPPED
:distribution:zip:integTest#extract
:distribution:zip:integTest#configure
:distribution:zip:integTest#start
Java HotSpot(TM) 64-Bit Server VM warning: Option UseParNewGC was deprecated in version 9.0 and will likely be removed in a future release.

[2015-11-12 01:25:37,075][INFO ][node                     ] [Living Tribunal] version[3.0.0-SNAPSHOT], pid[26943], build[c2aec53/2015-11-12_01:25:06]
[2015-11-12 01:25:37,075][INFO ][node                     ] [Living Tribunal] initializing ...
[2015-11-12 01:25:37,117][INFO ][plugins                  ] [Living Tribunal] loaded [], sites []
[2015-11-12 01:25:37,132][INFO ][env                      ] [Living Tribunal] using [1] data paths, mounts [[/ (/dev/sdb1)]], net usable_space [85.7gb], net total_space [229.1gb], spins? [no], types [ext4]
[2015-11-12 01:25:38,411][INFO ][node                     ] [Living Tribunal] initialized
[2015-11-12 01:25:38,411][INFO ][node                     ] [Living Tribunal] starting ...
[2015-11-12 01:25:38,518][WARN ][common.network           ] [Living Tribunal] publish host: [_local_] resolves to multiple addresses, auto-selecting {127.0.0.1} as single publish address
[2015-11-12 01:25:38,518][INFO ][transport                ] [Living Tribunal] publish_address {127.0.0.1:9500}, bound_addresses {[::1]:9500}, {127.0.0.1:9500}
[2015-11-12 01:25:38,526][INFO ][discovery                ] [Living Tribunal] distribution_zip_integTest/QGrcE43yRtiRtIpn7RpmXQ
[2015-11-12 01:25:41,547][INFO ][cluster.service          ] [Living Tribunal] new_master {Living Tribunal}{QGrcE43yRtiRtIpn7RpmXQ}{127.0.0.1}{127.0.0.1:9500}[testattr=&gt;test], reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-11-12 01:25:41,595][WARN ][common.network           ] [Living Tribunal] publish host: [_local_] resolves to multiple addresses, auto-selecting {127.0.0.1} as single publish address
[2015-11-12 01:25:41,595][INFO ][http                     ] [Living Tribunal] publish_address {127.0.0.1:9400}, bound_addresses {[::1]:9400}, {127.0.0.1:9400}
[2015-11-12 01:25:41,596][INFO ][node                     ] [Living Tribunal] started
:distribution:zip:integTest#start FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':distribution:zip:integTest#start'.
&gt; Failed to start elasticsearch

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 39.31 secs
rmuir@beast:~/workspace/elasticsearch/distribution/zip$ jps
27032 Jps
21882 org.eclipse.equinox.launcher_1.3.100.v20150511-1540.jar
26943 Elasticsearch

```

Elasticsearch in fact started up fine, and you can use it. So there are two bugs, probably the first one to fix is the JVM leak, since i've hit that a few times this week already.
</description><key id="116488527">14701</key><summary>integration test fails and leaks a JVM on java 9EA</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-12T06:27:54Z</created><updated>2015-11-13T06:48:11Z</updated><resolved>2015-11-13T06:48:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>gradle check croaks on java 9 EA (non-jigsaw)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14700</link><project id="" key="" /><description>Looks like some bug in a gradle plugin with versioning:

```
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version : 2.8
  JDK Version    : 1.9.0-ea-b91 (Oracle Corporation)
  OS Info        : Linux 4.2.0-16-generic (amd64)
:core:compileJava UP-TO-DATE
:core:processResources UP-TO-DATE
:core:classes UP-TO-DATE
:core:jar UP-TO-DATE
:distribution:deb:buildDeb FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':distribution:deb:buildDeb'.
&gt; java.lang.NullPointerException (no error message)

* Try:
Run with --info or --debug option to get more log output.

* Exception is:
org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':distribution:deb:buildDeb'.
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46)
    at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35)
    at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64)
    at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)
    at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52)
    at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)
    at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53)
    at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
    at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203)
    at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185)
    at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:62)
    at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50)
    at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:25)
    at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:110)
    at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37)
    at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)
    at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23)
    at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43)
    at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32)
    at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)
    at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30)
    at org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:154)
    at org.gradle.internal.Factories$1.create(Factories.java:22)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:52)
    at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:151)
    at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:32)
    at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:99)
    at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:93)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)
    at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:62)
    at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:93)
    at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:82)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:94)
    at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)
    at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:43)
    at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28)
    at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77)
    at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47)
    at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51)
    at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28)
    at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43)
    at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210)
    at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35)
    at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)
    at org.gradle.launcher.Main.doAction(Main.java:33)
    at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)
    at org.gradle.launcher.GradleMain.main(GradleMain.java:23)
Caused by: java.lang.NullPointerException
    at org.apache.commons.lang3.SystemUtils.isJavaVersionAtLeast(SystemUtils.java:1371)
    at org.apache.commons.lang3.SystemUtils$isJavaVersionAtLeast.call(Unknown Source)
    at com.netflix.gradle.plugins.deb.filevisitor.DebFileVisitorStrategyFactory.getStrategy(DebFileVisitorStrategyFactory.groovy:18)
    at com.netflix.gradle.plugins.deb.DebCopyAction.visitDir(DebCopyAction.groovy:162)
    at com.netflix.gradle.plugins.packaging.AbstractPackagingCopyAction.this$dist$invoke$1(AbstractPackagingCopyAction.groovy)
    at com.netflix.gradle.plugins.packaging.AbstractPackagingCopyAction$StreamAction.methodMissing(AbstractPackagingCopyAction.groovy)
    at com.netflix.gradle.plugins.packaging.AbstractPackagingCopyAction$StreamAction.processFile(AbstractPackagingCopyAction.groovy:61)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.maybeVisit(NormalizingCopyActionDecorator.java:97)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.maybeVisit(NormalizingCopyActionDecorator.java:87)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.maybeVisit(NormalizingCopyActionDecorator.java:87)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.maybeVisit(NormalizingCopyActionDecorator.java:87)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.access$000(NormalizingCopyActionDecorator.java:52)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1$1.processFile(NormalizingCopyActionDecorator.java:64)
    at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1$1.processFile(DuplicateHandlingCopyActionDecorator.java:60)
    at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.processFile(CopyFileVisitorImpl.java:60)
    at org.gradle.api.internal.file.copy.CopyFileVisitorImpl.visitFile(CopyFileVisitorImpl.java:44)
    at org.gradle.api.internal.file.AbstractFileTree$FilteredFileTree$1.visitFile(AbstractFileTree.java:143)
    at org.gradle.api.internal.file.collections.SingletonFileTree.visit(SingletonFileTree.java:44)
    at org.gradle.api.internal.file.collections.FileTreeAdapter.visit(FileTreeAdapter.java:109)
    at org.gradle.api.internal.file.AbstractFileTree$FilteredFileTree.visit(AbstractFileTree.java:134)
    at org.gradle.api.internal.file.CompositeFileTree.visit(CompositeFileTree.java:58)
    at org.gradle.api.internal.file.copy.CopySpecActionImpl.execute(CopySpecActionImpl.java:37)
    at org.gradle.api.internal.file.copy.CopySpecActionImpl.execute(CopySpecActionImpl.java:24)
    at org.gradle.api.internal.file.copy.DefaultCopySpec$DefaultCopySpecResolver.walk(DefaultCopySpec.java:498)
    at org.gradle.api.internal.file.copy.DefaultCopySpec$DefaultCopySpecResolver.walk(DefaultCopySpec.java:500)
    at org.gradle.api.internal.file.copy.DefaultCopySpec$DefaultCopySpecResolver.walk(DefaultCopySpec.java:500)
    at org.gradle.api.internal.file.copy.DefaultCopySpec.walk(DefaultCopySpec.java:322)
    at org.gradle.api.internal.file.copy.CopySpecBackedCopyActionProcessingStream.process(CopySpecBackedCopyActionProcessingStream.java:36)
    at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator$1.process(DuplicateHandlingCopyActionDecorator.java:44)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator$1.process(NormalizingCopyActionDecorator.java:56)
    at com.netflix.gradle.plugins.packaging.AbstractPackagingCopyAction.execute(AbstractPackagingCopyAction.groovy:44)
    at org.gradle.api.internal.file.copy.NormalizingCopyActionDecorator.execute(NormalizingCopyActionDecorator.java:52)
    at org.gradle.api.internal.file.copy.DuplicateHandlingCopyActionDecorator.execute(DuplicateHandlingCopyActionDecorator.java:42)
    at org.gradle.api.internal.file.copy.CopyActionExecuter.execute(CopyActionExecuter.java:38)
    at org.gradle.api.tasks.AbstractCopyTask.copy(AbstractCopyTask.java:83)
    at com.netflix.gradle.plugins.packaging.SystemPackagingTask.super$5$copy(SystemPackagingTask.groovy)
    at com.netflix.gradle.plugins.packaging.SystemPackagingTask$_copy_closure23.doCall(SystemPackagingTask.groovy:110)
    at com.netflix.gradle.plugins.packaging.SystemPackagingTask$_copy_closure23.doCall(SystemPackagingTask.groovy)
    at com.netflix.gradle.plugins.packaging.SystemPackagingTask.copy(SystemPackagingTask.groovy:109)
    at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:227)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:220)
    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:209)
    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:585)
    at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:568)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80)
    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61)
    ... 57 more


BUILD FAILED
```
</description><key id="116487351">14700</key><summary>gradle check croaks on java 9 EA (non-jigsaw)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-12T06:12:43Z</created><updated>2016-02-29T22:51:26Z</updated><resolved>2016-02-29T22:51:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T16:17:14Z" id="183910721">@rjernst is this still an issue?
</comment><comment author="rjernst" created="2016-02-29T22:51:26Z" id="190435453">We fixed this a while ago by allowing to set JAVA_HOME separate from the java which gradle uses.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add extra validation into `cluster/stats`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14699</link><project id="" key="" /><description>The cluster health and cluster stats disagree on the status.
Add a extra validation step in `cluster/stats`.

This PR should fix #7390
</description><key id="116485946">14699</key><summary>Add extra validation into `cluster/stats`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Stats</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-12T05:56:07Z</created><updated>2015-11-17T14:26:14Z</updated><resolved>2015-11-12T19:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-12T08:16:39Z" id="156030858">LGTM. 

@danielmitterdorfer FYI, you might want to replace this code with your new ClusterStateHealth class.
</comment><comment author="xuzha" created="2015-11-12T19:46:36Z" id="156214143">2.1 : https://github.com/elastic/elasticsearch/commit/06e85d99758f11b9477d2916247f332e7e3ecd78
2.x : https://github.com/elastic/elasticsearch/commit/c2d7b82659423df9e4d8873aa18350f4cd28e8f0
</comment><comment author="xuzha" created="2015-11-12T19:46:50Z" id="156214278">Thx @bleskes  for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Change document field of _simulate with verbose=true to diff view</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14698</link><project id="" key="" /><description>This is in regards to the `_simulate` endpoint.

It would be nice to have a view of what was modified in a document between processors in a pipeline instead of seeing a whole new copy of the full document.

Currently, the output for a pipeline looks like this

```
{
      "error": false,
      "processor_results": [
        {
          "processor_id": "processor[mutate]-0",
          "error": false,
          "modified": true,
          "doc": {
            "_index": "index",
            "_type": "type",
            "_source": {
              "bar": "hello",
              "foo": 5
            },
            "_id": "id2"
          }
        },
        {
          "processor_id": "processor[mutate]-1",
          "error": false,
          "modified": true,
          "doc": {
            "_index": "index",
            "_type": "type",
            "_source": {
              "bar": "HELLO",
              "foo": 5
            },
            "_id": "id2"
          }
        }
      ]
    }
```

From this, you can see that the `doc` field shows all of the document's fields. It would be great to only show what has been modified from after the previous processor's mutations.

Something like this would be nice:

```
{
      "error": false,
      "processor_results": [
        {
          "processor_id": "processor[mutate]-0",
          "error": false,
          "modified": true,
          "doc_diff": {
            "updated": {
              "_source.foo": 5
            },
            "added": {},
            "removed": {}
          }
        },
        ...
      ]
    }
```

depends on: https://github.com/elastic/elasticsearch/pull/14572
</description><key id="116485405">14698</key><summary>[Ingest] Change document field of _simulate with verbose=true to diff view</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-12T05:51:00Z</created><updated>2016-02-14T18:02:37Z</updated><resolved>2016-02-14T18:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-11-12T05:59:27Z" id="156011101">implementation reference from Guava: [source](https://github.com/google/guava/blob/8ff490732db7a71adcf561171b5444ddc61c8990/guava/src/com/google/common/collect/Maps.java#L428), [docs](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/MapDifference.html)
</comment><comment author="clintongormley" created="2016-02-14T16:16:12Z" id="183910554">@talevy is this something you're still planning on investigating?
</comment><comment author="talevy" created="2016-02-14T18:02:37Z" id="183939090">@clintongormley I will go ahead and close it. This responsibility/feature has been pushed down to clients to implement. For example, Kibana computes diffs using the current complete response documents.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master can get stuck AsyncFetching forever</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14697</link><project id="" key="" /><description>While trying to upgrade our clusters to 1.7.3 I came across a fun issue with the async shard fetch mechanism, namely that the [cache](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L64) can get poisoned by nodes that change IDs. The basic issue is that [`processAsyncFetch`](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L270) passes `response.getNodes()` instead of `nodesIds`. This means that if the master thinks it is talking to node 1234 and the response comes back with "I'm node 3456" we [continue](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L178) and poison the cache in the process as the master will never get a response from node 1234.

Nodes should never change IDs, but I think they actually can when they first start up because of something like [LocalDiscovery](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java#L110-L111) which sets the localNode to one name and then [InternalClusterService](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L162-L163) sets it to something different. More generally it seems like one node shouldn't trust that it will get a response back from a node with the same ID that it sent to (I can't actually come up with a real world situation where the node both is still in the cluster and the id changes ... but ...). My understanding of Zen is that this can't happen (and nodes really _shouldn't_ ever change IDs) but if they do the master gets a poisoned cache and it will be stuck forever in [fetchData](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L121).

I think that passing the `nodesIds` instead of `response.getNodes()` is not a particularly good solution because it defends against something that should never happen, but it does seem like a [NodeEntry](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L333) "fetching" state really should expire after some amount of time to be robust. If I were to submit a PR that expired the async fetch cache after 20s or added some equivalent bookeeping, would you be opposed to that?
</description><key id="116467894">14697</key><summary>Master can get stuck AsyncFetching forever</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jolynch</reporter><labels><label>feedback_needed</label></labels><created>2015-11-12T02:42:54Z</created><updated>2015-11-17T09:54:52Z</updated><resolved>2015-11-17T09:54:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-16T16:24:20Z" id="157087180">Answering in another order:

&gt; Nodes should never change IDs

Node ids actually change all the time. You can think of a node ID as an equivalent of a process id - every time the node is restarted and we have a new process, the id changes and the node joins the cluster again. This allows us to make sure that network connections are re-opened to the node etc.

&gt; I think that passing the nodesIds instead of response.getNodes() is not a particularly good solution because it defends against something that should never happen, 

We are making sure that we use the information based on where it come from, not where we thought we sent it to. I think this is safer.

&gt; and poison the cache in the process as the master will never get a response from node 1234.

It's true that we do not populate the cache with the response and we keep on waiting for an answer from 1234. However, the [node fault detection will discover that a new node was started on the same port](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java#L245) and will remove the node from the cluster. At the point the cache is [cleaned](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java#L234) and the master will nog longer wait for a response. 

&gt; While trying to upgrade our clusters to 1.7.3 I came across a fun issue with the async shard fetch 

Do you have any more information about what you saw? Logs or similar will be great.
</comment><comment author="jolynch" created="2015-11-16T20:41:09Z" id="157163031">&gt; Node ids actually change all the time. 

Sorry you are right and I should be precise, node ids should have a 1:1 mapping with process ids. The problem I was observing was that this was not the case when our discovery plugin was setting the local node id to one value (similar to the linked method in `LocalDiscovery`) but the `InternalClusterService` sets it to something else. So in the same PID the node would change ids while joining the cluster between construction and when our discovery plugin's `doStart` method was called. This is clearly a bug in our discovery service and I have since defaulted to just using whatever id is provided by the injected `ClusterService` which has eliminated this bug. We didn't have this problem pre-1.7 because shard fetches were sync and did not depend on the node id.

&gt; It's true that we do not populate the cache with the response and we keep on waiting for an answer from 1234. However, the node fault detection will discover that a new node was started on the same port and will remove the node from the cluster. At the point the cache is cleaned and the master will nog longer wait for a response.

This is what I mean by I can't come up with a real world situation in which the node is still in the cluster but the ID has changed along the way. The only way I can see is if there was a similar issue with a `Discovery` provider (namely that the discovery provider set the node ID instead of letting the `ClusterService` do it for them) or if the separate invalidation doesn't work for some reason. That being said I can conceive of fun network partitions that could maybe make it possible, and it's pretty simple to make the async shard fetch mechanism as robust to such issues as the sync method was before so why not.

&gt; Do you have any more information about what you saw? Logs or similar will be great.

I'm not sure how generally applicable this is, but an example of when we got stuck (note the "ignoring already fetching" lines):

```
[2015-11-10 14:41:59,988] [TRACE] [test_node_redacted] [test_index][11], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores
[2015-11-10 14:41:59,988] [TRACE] [test_node_redacted] Can not allocate [[test_index][2], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]] on node [i2w0iPhaSbe5Etw7Ean2ZA] due to [SameShardAllocationDecider]
[2015-11-10 14:41:59,988] [TRACE] [test_node_redacted] Can not allocate [[test_index][2], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]] on node [MA-IP3CLRYWP26Y6m3fl0w] due to [SameShardAllocationDecider]
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage without relocations: [b8jBlkgMTWqVYnYU446AQg][10-40-26-45-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [b8jBlkgMTWqVYnYU446AQg][10-40-26-45-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] Node [b8jBlkgMTWqVYnYU446AQg] has 0.010641291131321395% used disk
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] [test_index][2], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] Can not allocate [[test_index][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]] on node [i2w0iPhaSbe5Etw7Ean2ZA] due to [SameShardAllocationDecider]
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage without relocations: [MA-IP3CLRYWP26Y6m3fl0w][10-40-11-209-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,989] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [MA-IP3CLRYWP26Y6m3fl0w][10-40-11-209-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] Node [MA-IP3CLRYWP26Y6m3fl0w] has 0.010612524124269385% used disk
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] [test_index][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage without relocations: [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] Node [i2w0iPhaSbe5Etw7Ean2ZA] has 0.010672553476894109% used disk
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] [test_index][9], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-10T22:30:42.770Z], details[node_left[8QKATzCtRe-Ef4CzQ-UVDg]]]: ignoring allocation, still fetching shard stores
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage without relocations: [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-10 14:41:59,990] [TRACE] [test_node_redacted] usage with relocations: [0 bytes] [i2w0iPhaSbe5Etw7Ean2ZA][10-40-11-207-uswest1adevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
```

I'm totally happy with saying "this shouldn't happen and if it does there is a bug elsewhere", but I'm also happy to submit a PR to make the async shard fetch robust to such inconsistencies if you like.
</comment><comment author="bleskes" created="2015-11-17T08:26:27Z" id="157307250">&gt; That being said I can conceive of fun network partitions that could maybe make it possible, and it's pretty simple to make the async shard fetch mechanism as robust to such issues as the sync method was before so why not.

Looking forward to a PR :)

&gt;  (note the "ignoring already fetching" lines):

I think that this is just working as we discussed/designed? If so, can we close the ticket?
</comment><comment author="jolynch" created="2015-11-17T08:59:35Z" id="157312682">Sure thing, and I can work on a PR if you think it's worth having some kind of expiration mechanism. I'm also fine with saying "this shouldn't happen and if it does it's a bug elsewhere". Either way feel free to close this, thank you for the feedback!
</comment><comment author="bleskes" created="2015-11-17T09:54:52Z" id="157323742">yeah, I don't think an expiration mechanism is something to add here. We already have the node FD pinging from the same node. Closing for now then. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update java.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14696</link><project id="" key="" /><description>Add few more breaking changes. 
</description><key id="116437684">14696</key><summary>Update java.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">plebedev</reporter><labels><label>docs</label></labels><created>2015-11-11T22:45:43Z</created><updated>2015-11-17T11:28:47Z</updated><resolved>2015-11-17T11:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T11:28:30Z" id="157342853">thanks @plebedev - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog recovery can repeatedly fail if we run out of disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14695</link><project id="" key="" /><description>If we run out of disk while recoverying the transaction log
we repeatedly fail since we expect the latest tranlog to be uncommitted.
This change adds 2 safety levels:
- uncommitted checkpoints are first written to a temp file and then atomically
  renamed into a committed (recovered) checkpoint
- if the latest uncommitted checkpoints generation is already recovered it has to be
  identical, if not the recovery fails

This allows to fail in between recovering the latest uncommitted checkpoint and moving
the checkpoint generation to N+1 which can for instance happen in a situation where
we can run out of disk. If we run out of disk while recovering the uncommitted checkpoint
either the temp file writing or the atomic rename will fail such that we never have a
half written or corrupted recovered checkpoint.
</description><key id="116410779">14695</key><summary>Translog recovery can repeatedly fail if we run out of disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-11T20:03:04Z</created><updated>2016-08-20T13:12:30Z</updated><resolved>2015-11-16T18:19:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-11T20:03:20Z" id="155895487">this was reported here: https://discuss.elastic.co/t/no-space-disaster/34158
</comment><comment author="jpountz" created="2015-11-12T10:43:17Z" id="156067979">I left some comments but the change looks good to me!
</comment><comment author="jpountz" created="2015-11-16T18:19:58Z" id="157125805">Merged via 1bdf29e2634e9cc8d02a84953bad879b5864353a
</comment><comment author="asifalis" created="2015-12-03T12:32:08Z" id="161620906">Hi expert,

I need expert guide to resolve my issue, i am using following version. 
kibana 4.2.0
elasticsearch 2.1

elasticsearch log are.

[root@centos elasticsearch]# more elasticsearch.log 
[2015-12-03 11:21:45,534][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, C
ONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed
[2015-12-03 11:21:45,987][INFO ][node                     ] [Lin Sun] version[2.1.0], pid[3991], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-12-03 11:21:45,987][INFO ][node                     ] [Lin Sun] initializing ...
[2015-12-03 11:21:46,108][INFO ][plugins                  ] [Lin Sun] loaded [], sites []
[2015-12-03 11:21:46,149][INFO ][env                      ] [Lin Sun] using [1] data paths, mounts [[/ (/dev/mapper/vg_centos-lv_root)]], net usable_space [7
.7gb], net total_space [17.1gb], spins? [possibly], types [ext4]
[2015-12-03 11:21:48,854][INFO ][node                     ] [Lin Sun] initialized
[2015-12-03 11:21:48,855][INFO ][node                     ] [Lin Sun] starting ...
[2015-12-03 11:21:49,128][INFO ][transport                ] [Lin Sun] publish_address {192.168.48.63:9300}, bound_addresses {192.168.48.63:9300}
[2015-12-03 11:21:49,159][INFO ][discovery                ] [Lin Sun] elasticsearch/pNZpYIdPQq-X4_OFlqUOPg
[2015-12-03 11:21:52,234][INFO ][cluster.service          ] [Lin Sun] new_master {Lin Sun}{pNZpYIdPQq-X4_OFlqUOPg}{192.168.48.63}{192.168.48.63:9300}, reason
: zen-disco-join(elected_as_master, [0] joins received)
[2015-12-03 11:21:52,354][INFO ][http                     ] [Lin Sun] publish_address {192.168.48.63:9200}, bound_addresses {192.168.48.63:9200}
[2015-12-03 11:21:52,355][INFO ][node                     ] [Lin Sun] started
[2015-12-03 11:21:52,452][INFO ][gateway                  ] [Lin Sun] recovered [1] indices into cluster_state
[2015-12-03 11:21:53,276][WARN ][index.translog           ] [Lin Sun] [.kibana][0] failed to delete temp file /var/lib/elasticsearch/elasticsearch/nodes/0/in
dices/.kibana/0/translog/translog-8220187682635755947.tlog
java.nio.file.NoSuchFileException: /var/lib/elasticsearch/elasticsearch/nodes/0/indices/.kibana/0/translog/translog-8220187682635755947.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1126)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

---

I tried to use localhost instead of IP in configuration file but no any luck.

My kibana console shown
Installed Plugins
Name    Status
plugin:kibana   Ready

plugin:elasticsearch    Unable to connect to Elasticsearch at http://0.0.0.0:9200. Retrying in 2.5 seconds.

plugin:kbn_vislib_vis_types     Ready
plugin:markdown_vis     Ready
plugin:metric_vis   Ready
plugin:spyModes     Ready 

Regards,

Asif,
</comment><comment author="clintongormley" created="2015-12-03T17:53:33Z" id="161730688">@asifalis please ask questions like these in the forum http://discuss.elastic.co/
</comment><comment author="daimoniac" created="2016-05-12T09:33:13Z" id="218706975">Currently experiencing this or similar issue with Version: 2.2.1, Build: d045fc2/2016-03-09T09:38:54Z, JVM: 1.8.0_66.

Stacktrace:
[2016-05-12 11:20:38,861][WARN ][indices.cluster          ] [spx-elastic-FPA2-02] [[releases_2][1]] marking and sending shard failed due to [failed recovery]
[releases_2][[releases_2][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/opt/esdata/spxA/nodes/0/indices/releases_2/1/translog/translog-867.ckp];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [releases_2][[releases_2][1]] EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/opt/esdata/spxA/nodes/0/indices/releases_2/1/translog/translog-867.ckp];
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:155)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1510)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1494)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:969)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:941)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
        ... 5 more
Caused by: java.nio.file.NoSuchFileException: /opt/esdata/spxA/nodes/0/indices/releases_2/1/translog/translog-867.ckp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
        at java.nio.file.Files.newByteChannel(Files.java:361)
        at java.nio.file.Files.newByteChannel(Files.java:407)
        at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
        at java.nio.file.Files.newInputStream(Files.java:152)
        at org.elasticsearch.index.translog.Checkpoint.read(Checkpoint.java:82)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:330)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:179)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:208)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:151)
        ... 11 more

/opt/esdata/spxA/nodes/0/indices/releases_2/1/translog# ls -al
total 60
drwxr-xr-x 2 elasticsearch elasticsearch  4096 May 12 11:31 .
drwxr-xr-x 5 elasticsearch elasticsearch  4096 May 10 13:40 ..
-rw-r--r-- 1 elasticsearch elasticsearch    20 May 12 04:44 translog-865.ckp
-rw-r--r-- 1 elasticsearch elasticsearch    20 May 12 04:44 translog-866.ckp
-rw-r--r-- 1 elasticsearch elasticsearch    43 May 12 04:44 translog-867.tlog
-rw-r--r-- 1 elasticsearch elasticsearch 34570 May 12 04:44 translog-868.tlog
-rw-r--r-- 1 elasticsearch elasticsearch    20 May 12 04:44 translog.ckp
</comment><comment author="jinleileiking" created="2016-08-20T13:12:29Z" id="241199409">I encounter this. But I have disk space.
ENV:

```
JVM: 1.8.0_45 ES: 2.1.0
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not concurrently run the same request on the same shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14694</link><project id="" key="" /><description>Under worst case conditions, a request that is bad for the system, thereby taking a long time to process, often gets repeated as a user--or even an automated system--becomes impatient while they await their response. If an **exactly** duplicated request comes into the same node to run on the same shard (otherwise it's not an exact match of course), then it would be ideal in all but one scenario to simply dupe the eventual response from the original request.

Unfortunately, the exact match nature of this eliminates a slew of requests from benefiting from it (at least until 3.0 when we can start to recognize inner parts better to drop irrelevant filters), but it provides a lot of protection from a very common problem: a user hitting refresh to send what was a recoverable situation into a downward spiral leading to `OutOfMemoryError` if you are lucky or, worse, permanent GCs that never recover.

The scenario where this is less ideal is the case where a request expects the most up-to-date information (e.g., details from segments that may have been created since the original request started running). Since search is not real time, and the performance of the system is at stake from a presumably slow, repeated request, this seems like an edge case to me. Perhaps providing a flag to use the existing behavior (just create another shard level request) would be enough to allow those cases to "demand" this behavior.

It should be as simple as adding a new response listener to the original request, which entirely side steps adding another thread to do the exact same work, but "simple" ends there as I do not believe we provide any way to easily find/attach to other requests. As a result, this is related to task management in #6914.
</description><key id="116381914">14694</key><summary>Do not concurrently run the same request on the same shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-11-11T17:25:18Z</created><updated>2017-04-26T09:13:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2017-04-26T09:13:26Z" id="297310961">I think this is at least partly solved by the request cache now? if the query is cachable then subsequent requests for the same exact search will either wait on the single cached future for the request or just retrieve the cached result if the query has finished executing?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't cache top level field data for fields that don't exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14693</link><project id="" key="" /><description>Just return an empty field data instance, like we do currently the same for segment level field data.
</description><key id="116380981">14693</key><summary>Don't cache top level field data for fields that don't exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-11T17:20:15Z</created><updated>2015-11-16T12:50:52Z</updated><resolved>2015-11-16T05:31:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-12T10:47:06Z" id="156069554">LGTM
</comment><comment author="martijnvg" created="2015-11-16T05:31:42Z" id="156919635">Pushed via 491e30cfbf13440d68f324820edeeac9c8c3599a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Accessing Parent Document Values in Aggregations from Scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14692</link><project id="" key="" /><description>Provide the ability to access values from parent documents in child aggregations, specifically in aggregation scripts.
</description><key id="116369107">14692</key><summary>Support Accessing Parent Document Values in Aggregations from Scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels /><created>2015-11-11T16:20:57Z</created><updated>2015-11-17T13:39:24Z</updated><resolved>2015-11-17T13:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gingerwizard" created="2015-11-11T16:21:19Z" id="155833389">@colings86 used to track the work we discussed.
</comment><comment author="jpountz" created="2015-11-12T11:14:40Z" id="156077068">I'm having trouble understanding how such a feature could be implemented in a scalable way.
</comment><comment author="clintongormley" created="2015-11-17T13:39:24Z" id="157372689">Yeah, this just doesn't sound doable in any way that doesn't suck. 

Still difficult but at least within the realms of possibility would be a `parent` aggregation: https://github.com/elastic/elasticsearch/issues/14692
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percentiles aggregation: method parsing issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14691</link><project id="" key="" /><description>Currently, although tdigest is the default method, `compression` can only be set if it's wrapped in a `tdigest` object:

```
{
    "aggs" : {
        "load_time_outlier" : {
            "percentiles" : {
                "field" : "load_time",
                "tdigest": {
                    "compression": 200
                }
            }
        }
    }
}
```

Otherwise, the following exception is thrown: `Unexpected token VALUE_NUMBER in [load_time_outlier].`

The [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search-aggregations-metrics-percentile-aggregation.html#search-aggregations-metrics-percentile-aggregation-compression) however states that it can be set at the root agg level:

```
{
    "aggs" : {
        "load_time_outlier" : {
            "percentiles" : {
                "field" : "load_time",
                "compression" : 200 
            }
        }
    }
}
```

On a similar note, the hdr histogram can only be provided as an object as well:

```
{
    "aggs" : {
        "load_time_outlier" : {
            "percentiles" : {
                "field" : "load_time",
                "hdr": {
                    "number_of_significant_value_digits" : 3
                }
            }
        }
    }
}
```

but the documentation states that it can be specified as so:

```
{
    "aggs" : {
        "load_time_outlier" : {
            "percentiles" : {
                "field" : "load_time",
                "percents" : [95, 99, 99.9],
                "method" : "hdr", 
                "number_of_significant_value_digits" : 3 
            }
        }
    }
}
```

I think the objects make sense, since there are options that only pertain to certain methods, but I think `compression` should still be allowed at the root level if a method isn't specified, since tdigest is taken by default.
</description><key id="116361111">14691</key><summary>Percentiles aggregation: method parsing issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">gmarz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>docs</label></labels><created>2015-11-11T15:44:21Z</created><updated>2016-01-15T12:40:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T11:18:46Z" id="157340998">@colings86 could you take a look please?
</comment><comment author="colings86" created="2015-11-17T11:23:16Z" id="157341936">To me this is a documentation bug. It looks like the documentation didn't get updated form the previous incarnation of the settings (which were never released).

I think we should only allow the object definition and not allow `compression` on the root level. This is the way the parsing works in the code today. This makes the parsing easier in code and makes it easier to explain the syntax to users. If you are just using the defaults then you don't need to specify anything and you get tdigest with compression set to 100. If you want to change the settings then you must define the method as well as the settings so it is explicit. Otherwise we could end up with situations where a user specifies compression at the root level and hdr as the method and doesn't realise why this doesn't work.
</comment><comment author="gmarz" created="2015-11-18T14:10:15Z" id="157724097">@colings86 Agreed, that makes sense.

A bit of nit pick though, but probably worth nothing: the null checks [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java#L151) and [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java#L160) aren't necessary as these will get caught in the parsing loop above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Newrelic agent with 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14690</link><project id="" key="" /><description>Posting this in case its related to https://github.com/elastic/elasticsearch/issues/13785 

I'm trying to run ES 2.0.0 with a newrelic agent, but I'm getting

```
Exception in thread "main" java.lang.IllegalStateException: failed to load bundle [] due to jar hell
Likely root cause: java.security.AccessControlException: access denied ("java.io.FilePermission" "/usr/local/newrelic/newrelic.jar" "read")
```

See complete log below:

```
root@cubitsearch-client-1:~# sudo -u elasticsearch /usr/bin/java -Xms3g -Xmx3g -Xmn200m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Dnewrelic.config.file=/usr/local/newrelic/elasticsearch.yml -javaagent:/usr/local/newrelic/newrelic.jar -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.0.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch -Des.max-open-files=true
Nov 11, 2015 12:16:17 +0000 [1946 1] com.newrelic INFO: New Relic Agent: Loading configuration file "/usr/local/newrelic/elasticsearch.yml"
Nov 11, 2015 12:16:17 +0000 [1946 1] com.newrelic WARN: New Relic Agent: Unable to write log file: /var/log/newrelic/newrelic_agent.log. Please check permissions on the file and directory.
log4j:WARN No appenders could be found for logger (com.newrelic.agent.deps.org.apache.http.client.protocol.RequestAddCookies).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
[2015-11-11 12:16:54,791][INFO ][bootstrap                ] max_open_files [4096]
[2015-11-11 12:16:56,233][INFO ][node                     ] [cubitsearch-client-1] version[2.0.0], pid[1946], build[de54438/2015-10-22T08:09:48Z]
[2015-11-11 12:16:56,233][INFO ][node                     ] [cubitsearch-client-1] initializing ...
Exception in thread "main" java.lang.IllegalStateException: failed to load bundle [] due to jar hell
Likely root cause: java.security.AccessControlException: access denied ("java.io.FilePermission" "/usr/local/newrelic/newrelic.jar" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)
    at java.security.AccessController.checkPermission(AccessController.java:559)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:206)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:145)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:154)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:91)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:173)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:340)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:113)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:144)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

I tried setting a custom java policy for my `elasticsearch` user by creating `/usr/share/elasticsearch/.java.policy` with the following content:

```
grant codeBase "file:/usr/local/newrelic/-" {
       permission java.security.AllPermission;
       permission java.net.SocketPermission "*.newrelic.com", "connect,accept,resolve";
};
```

, but to no effect.

I also tried to add the above to the global java policy, but it didn't work either.

I managed to get the agent to work by disabling the security manager entirely (`-Dsecurity.manager.enabled=false`), but I don't really want to do this in production.
</description><key id="116356471">14690</key><summary>Newrelic agent with 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">krisb78</reporter><labels /><created>2015-11-11T15:22:43Z</created><updated>2015-11-12T23:09:12Z</updated><resolved>2015-11-12T23:09:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-11T17:36:02Z" id="155856529">We can fix it, here is the deal:
1. existing 1.x startup scripts had a bug that would add `CWD` to `CLASSPATH` (https://github.com/elastic/elasticsearch/issues/12000). Java classloaders are lenient, so this went undetected forever. In many cases such as running as a service, `CWD` would be `/`, which is terrible: it would mean total read access to all files on the system. So we initially (and still do) jumped through hoops to avoid granting access to what is in `CLASSPATH` etc, since it was too arbitrary and crazy.
2. as far as ubuntu's agent (#13785), that is unrelated to your situation, in that case its an unwanted agent and its being added in a sneaky way that the user is not aware of. We detect that and kick it out, if you want an agent, it should be that you asked for it explicit (like in your case, where you are asking to run newrelic because you pass `-javaagent:/usr/local/newrelic/newrelic.jar`)
3. `ES_CLASSPATH` was exposed directly to users, but this is problematic, because most likely it will just result in pain, its not a good or tested way to "plugin code", no isolation etc, we have the plugin mechanism for that.

we fixed and added safety around these situations too in https://github.com/elastic/elasticsearch/pull/13880.

Anyway, I think we can now safely add permissions to `CLASSPATH`, to ensure agents work, since now we defend against the silliness, and that will solve it for your agent or any other agent where the user has explicitly configured it.
</comment><comment author="rmuir" created="2015-11-11T17:40:15Z" id="155857574">By the way, this is the best way to workaround it in the meantime: https://docs.oracle.com/javase/tutorial/ext/basics/install.html
</comment><comment author="rmuir" created="2015-11-11T17:44:09Z" id="155858462">&gt; I tried setting a custom java policy for my elasticsearch user by creating /usr/share/elasticsearch/.java.policy with the following content:

Yeah, we don't read from any of the standard locations and add to our policy. Its worth looking at too, since what you did probably should have worked, but i want to investigate first, just concerned about what kind of stuff might be in these standard locations on various distros etc.
</comment><comment author="krisb78" created="2015-11-11T17:57:08Z" id="155861427">Thanks a lot, I'll try the workaround.
</comment><comment author="krisb78" created="2015-11-11T18:08:11Z" id="155863966">FTR, I also tried to pass in the policy explicitly by adding `-Djava.security.policy=/usr/share/elasticsearch/.java.policy` to `ES_JAVA_OPTS`, but it didn't work either.
</comment><comment author="rmuir" created="2015-11-11T18:26:41Z" id="155868249">yeah, I see this is the mechanism newrelic support recommends to their users: https://discuss.newrelic.com/t/flux-scheduler-not-able-to-load-newrelic-jar/2114

so its less about us changing our classpath logic I think, changing that would only mean a more confusing exception follows (like newrelic not having access to connect somewhere, write to its log file, something like that). 

its more about allowing you to do this in the system files like they recommend (without having to resort to installing it into an extension directory). and if you don't configure access for your agent then it predictably fails on that agent jar.
</comment><comment author="rmuir" created="2015-11-11T19:12:07Z" id="155881872">&gt; just concerned about what kind of stuff might be in these standard locations on various distros etc.

its kind of a downer, on java 7 &lt; update 51:

```
// default permissions granted to all domains

grant {
        // Allows any thread to stop itself using the java.lang.Thread.stop()
        // method that takes no argument.
        // Note that this permission is granted by default only to remain
        // backwards compatible.
        // It is strongly recommended that you either remove this permission
        // from this policy file or further restrict it to code sources
        // that you specify, because Thread.stop() is potentially unsafe.
        // See the API specification of java.lang.Thread.stop() for more
        // information.
        permission java.lang.RuntimePermission "stopThread";

        // allows anyone to listen on un-privileged ports
        permission java.net.SocketPermission "localhost:1024-", "listen";
```

After update 51 the SocketPermission improves into two (they broke back compat here and changed how listen ports are checked, yet kept stopThread for back compat?!?!?!):

```
        // allows anyone to listen on dynamic ports
        permission java.net.SocketPermission "localhost:0", "listen";

        // permission for standard RMI registry port
        permission java.net.SocketPermission "localhost:1099", "listen";
```

On java 8 and 9 the RMI is cleaned up and removed, but ephemeral port and stopThread remains. Looks like in 9 they work on cleaning up their own house with permissions to jdk internals though, which is a positive.

Anyway, sucking in the system/user configuration naively has some downsides and undoes some work, but its probably what we should do. Just makes life difficult as usual, I'll work up something.
</comment><comment author="krisb78" created="2015-11-12T10:19:08Z" id="156059190">Awesome, thanks @rmuir !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Value Count Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14689</link><project id="" key="" /><description /><key id="116351606">14689</key><summary>Aggregations Refactor: Refactor Value Count Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-11T15:02:06Z</created><updated>2015-11-16T13:09:35Z</updated><resolved>2015-11-16T13:09:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-11T21:20:00Z" id="155913628">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Stats and Extended_Stats Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14688</link><project id="" key="" /><description /><key id="116351437">14688</key><summary>Aggregations Refactor: Refactor Stats and Extended_Stats Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-11T15:01:24Z</created><updated>2015-11-16T13:08:25Z</updated><resolved>2015-11-16T13:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-12T10:50:14Z" id="156070766">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Max, Avg, and Sum Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14687</link><project id="" key="" /><description /><key id="116351279">14687</key><summary>Aggregations Refactor: Refactor Max, Avg, and Sum Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-11-11T15:00:45Z</created><updated>2015-11-16T13:06:12Z</updated><resolved>2015-11-16T13:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-12T10:51:54Z" id="156071298">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Watcher/License plugins killed Elasticsearch cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14686</link><project id="" key="" /><description>So we've been having issues with watcher, which we installed to test out alerting on our Staging cluster. And this week we've had our Staging cluster completely die, unable to connect to any master node for longer than 5 mins at a time. Looking through the logs we found:

```
[2015-11-10 16:48:28,531][INFO ][license.plugin.core      ] [CHIVLXSTG045] license for [watcher] - valid
[2015-11-10 16:48:28,536][ERROR][watcher.license          ] [CHIVLXSTG045]
#
# Watcher license will expire on [Wednesday, December 09, 2015]. All configured actions on
# all registered watches are throttled (not executed) on Watcher license expiration.
# Watches will continue be evaluated and watch history will continue being recorded.
# Have a new license? please update it. Otherwise, please reach out to your support contact.
#
```

But this shouldn't be an issue for a month, however uninstalling the watcher and license plugins allowed the cluster to get back to a healthy state. Seems like this is a pretty large bug.
</description><key id="116346333">14686</key><summary>Watcher/License plugins killed Elasticsearch cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rhealitycheck</reporter><labels><label>feedback_needed</label></labels><created>2015-11-11T14:36:06Z</created><updated>2016-02-14T16:05:11Z</updated><resolved>2016-02-14T16:05:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T11:17:31Z" id="157340791">Hi @rhealitycheck 

That message is normal when you have an autogenerated license, and shouldn't affect anything else until it actually expires.  It is just telling you that your trial license will expire on that date, and what will happen once it expires.

I think you have another issue going on.  Any other details?
</comment><comment author="rhealitycheck" created="2015-11-17T13:56:59Z" id="157376104">The only error messages we received was that a master node could not be found. There were no errors in the data nodes' logs. Our client nodes couldn't connect to the data nodes because a master could not be found. You could not query directly to the cluster because a master could not be found all the while in the logs you could see a master had been elected. The cluster returned to a normal state after uninstalling the license plug-in with no other changes made to the cluster. Also the date the "Watcher license" was set to expire in that error appears to be different in the several of the logs.
</comment><comment author="clintongormley" created="2015-11-18T15:25:44Z" id="157747802">Are you sure you had the license plugin installed on all nodes (including client nodes)?
</comment><comment author="rhealitycheck" created="2015-11-18T15:28:45Z" id="157749204">I'm pretty sure we had it on all nodes but I honestly don't remember right now if it was on the client nodes as well as the data nodes. I can confirm it was on all data nodes. The strangest part of this behavior was that we had the Watcher and License plugins installed for about a month and a half after which we saw this strange behavior.
</comment><comment author="clintongormley" created="2015-11-18T15:30:35Z" id="157749754">Either you should have seen it immediately, or after one month, no?  Something else must have changed.  I wonder if another node joined the cluster without the license plugin?
</comment><comment author="rhealitycheck" created="2015-11-18T15:35:59Z" id="157751126">So this would be expected if the license plugin was not installed on a node that joined the cluster but as long as all of them have the plugin it should be fine, correct? I can try and replicate and see if the issue is reproduced if we have it installed on all nodes.
</comment><comment author="clintongormley" created="2015-11-18T15:58:15Z" id="157758412">@rhealitycheck i think so.  See https://github.com/elastic/elasticsearch/issues/13445 for more
</comment><comment author="rhealitycheck" created="2015-11-18T16:02:45Z" id="157759720">Sure I'll try to reproduce. 

But just to be clear none of the data nodes had issues joining to the cluster unlike #13445, so I find it strange that even directly querying those nodes returned that no master was available. I can see how the client nodes wouldn't be able to talk to the data nodes if they didn't have it installed but for the entire cluster to be unresponsive seems extreme.
</comment><comment author="clintongormley" created="2015-11-18T18:41:10Z" id="157815178">@rhealitycheck sure, which is why i think that this is a red herring and there is something else at play.  perhaps another trawl through the logs, esp for stack traces?
</comment><comment author="clintongormley" created="2016-02-14T16:05:11Z" id="183908597">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switching from log4j to slf4j breaks ES 2.0 bin/plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14685</link><project id="" key="" /><description>When using slf4j instead of log4j, the ES 2.0 plugin tool breaks with

```
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/log4j/PropertyConfigurator
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:128)
    at org.elasticsearch.plugins.PluginManagerCliParser.main(PluginManagerCliParser.java:67)
Caused by: java.lang.ClassNotFoundException: org.apache.log4j.PropertyConfigurator
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 2 more
```

There is a strict dependency on log4j in the `PluginManagerCliParser` which should not be there. 

`EsLoggerFactory` handles support of log4j, slf4j, and jdk logging gracefully at https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/logging/ESLoggerFactory.java#L33-L48

Reproduction:
- remove log4j jar from lib
- add slf4j-api-1.7.13.jar to lib (and probably log4j2 or other slf4j impls)
- start of ES 2.0 works
- start of `bin/plugin` breaks

If you could upgrade from log4j to log4j2 by default, it would be much appreciated.
</description><key id="116345261">14685</key><summary>Switching from log4j to slf4j breaks ES 2.0 bin/plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>:Logging</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-11-11T14:30:21Z</created><updated>2016-03-16T19:38:20Z</updated><resolved>2016-02-14T16:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T16:04:52Z" id="183908470">Closing in favour of https://github.com/elastic/elasticsearch/issues/16585
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>transport client created by plugin cannot load plugin classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14684</link><project id="" key="" /><description>In 2.0, if a plugin has code that creates a transport client and specifies the plugin using the `plugin.types` setting, the transport client cannot be created due to a ClassNotFoundException.

``` java
TransportClient transportClient = TransportClient.builder()
                    .settings(Settings.builder()
                            .putArray("plugin.types", MyPlugin.class.getName()
                            ...)
                    .build();
```

results in:

```
ElasticsearchException[Failed to load plugin class [org.elasticsearch.MyPlugin]]; nested: ClassNotFoundException[org.elasticsearch.MyPlugin];
    at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:398)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:101)
    at org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:114)
```

This fails since the classloader used by the `PluginsService` does not know about the plugin jars and therefore plugin classes.

I've verified that this is not a problem in 2.1 since the plugin loading mechanism has been changed to pass in the actual plugin classes. Its worth discussing if/how this should be fixed in 2.0.x
</description><key id="116344635">14684</key><summary>transport client created by plugin cannot load plugin classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>bug</label></labels><created>2015-11-11T14:26:19Z</created><updated>2015-11-16T13:08:09Z</updated><resolved>2015-11-16T13:08:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-15T07:25:02Z" id="156788259">The fix that is in 2.1 was complex. I don't think we should make such a complicated change in a bugfix release.
</comment><comment author="bleskes" created="2015-11-15T20:39:30Z" id="156851554">@rjernst can you point at the commit that fixed this in 2.1? my search foo fails me.
</comment><comment author="rjernst" created="2015-11-15T20:51:25Z" id="156852291">It was the refactoring to remove plugin.types
</comment><comment author="rjernst" created="2015-11-15T20:59:21Z" id="156853084">@bleskes See #13055
</comment><comment author="jaymode" created="2015-11-16T13:08:06Z" id="157023515">Closing since this is fixed in 2.1 and the change is too big for a bugfix release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>lang-javascript plugin cannot find script files on Ubuntu</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14683</link><project id="" key="" /><description>I am trying to run a script from a file within a query and get the following error:
`ElasticsearchIllegalArgumentException[Unable to find on disk script my_script];`

I have the file my_script.js in the scripts directory. If I change the lang to groovy and create a file called my_script.groovy the query works as expected.

The same query works fine on a windows machine with lang='js'.

I tried changing the file extension to .javascript and the lang to javascript but get the same error.

```
              "estimated_cost": {
                "sum": {
                  "field": "estimated_cost",
                    "script_file": "my_script",
                    "lang": "js",
                    "params":{
                        "time_column":"calendar_time"
                    }
                }
```

I am using ElasticSearch 1.7.3 with the 2.7.0 plugin.

As a sidenote it would be great if the exception would actually write the full path to the actual file it tries to find.
</description><key id="116340685">14683</key><summary>lang-javascript plugin cannot find script files on Ubuntu</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BorisKozo</reporter><labels /><created>2015-11-11T14:03:22Z</created><updated>2015-11-18T18:39:36Z</updated><resolved>2015-11-17T11:10:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T11:10:39Z" id="157339671">It is likely that you have a compilation error, which you can see in your log file.
</comment><comment author="BorisKozo" created="2015-11-17T11:17:57Z" id="157340866">Well, clearly you ignored the statement that the same query (obviously with the same file!) works on my Windows machine.

But I worked around the problem by sending the script with a POST statement which allows me to use the script ID in the query. Needless to say I sent the content of the exact same file you claim to have a compilation error.
</comment><comment author="clintongormley" created="2015-11-17T11:21:46Z" id="157341612">@BorisKozo i did miss that.  But is there anything in the logs?  It should try to compile the script when you start ES, and will throw an exception if it was unable to do so.
</comment><comment author="BorisKozo" created="2015-11-17T11:24:36Z" id="157342187">The log only has the error I pasted in my original post.
</comment><comment author="clintongormley" created="2015-11-18T15:16:26Z" id="157744997">Something is weird then, because I tried it out on 1.7.3 and it worked just fine for me.  Any more info you can provide to debug this? Are you using a package instead of the zip? Any other settings?
</comment><comment author="BorisKozo" created="2015-11-18T15:33:11Z" id="157750439">I think you were right in the first place.
There was something wrong with the file. I deleted the entire content and replaced with just return 0; and it worked. 
I cannot figure out what was wrong with it, maybe some line ending issue moving it from Windows to Linux. 

In any case I am sticking with the script_id way for now.

Thanks for your help.
</comment><comment author="clintongormley" created="2015-11-18T18:39:36Z" id="157814627">thanks for letting us know
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include murmur3 client side reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14682</link><project id="" key="" /><description>Using the murmur3 mapper that is available on git. Have looked for it for a while, saw many people who where asking why it wasn't in the docs.
</description><key id="116339127">14682</key><summary>Include murmur3 client side reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RubieV</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-11-11T13:52:03Z</created><updated>2016-01-10T17:31:15Z</updated><resolved>2016-01-10T17:31:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T11:00:54Z" id="157337562">Hi @RubieV 

Thanks for the PR, but I don't understand why you're linking to the code for the murmur3 plugin?  The docs already link to to murmur3 plugin docs here https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-murmur3.html
</comment><comment author="RubieV" created="2015-11-17T11:07:23Z" id="157339122">Hi Clinton,

The guide mentions doing hashing client side, but don't give any reference
implementation on how to do that. The murmur3 docs are just documentation
on how to use ElasticSearch murmur3 plugin.

Therefore I linked that actual integration to the guide so it is clear how
to implement it without searching the whole Internet and finding many
others looking for reference implementation.

Maybe the link isn't the most fashionable way of linking those two together
but will certainly be useful.  What do you suggest?

Ruben
Op 17 nov. 2015 12:01 schreef "Clinton Gormley" notifications@github.com:

&gt; Hi @RubieV https://github.com/RubieV
&gt; 
&gt; Thanks for the PR, but I don't understand why you're linking to the code
&gt; for the murmur3 plugin? The docs already link to to murmur3 plugin docs
&gt; here
&gt; https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-murmur3.html
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14682#issuecomment-157337562
&gt; .
</comment><comment author="clintongormley" created="2015-11-18T15:14:20Z" id="157744485">Ah OK. Thanks for the explanation.  I'm leaning towards thinking that either you know how to hash something, or you'd just use the plugin.  Why would just copy/paste the code from the plugin?
</comment><comment author="RubieV" created="2016-01-04T14:08:03Z" id="168685849">@clintongormley Sorry for the late response! So, in our solution we calculate a hash the moment a nested document comes in. We have to calculate a hash for that document for the processes that are to be executed on that document, before it is indexed. 

However we want to be exactly consistent with ElasticSearch in creating a hash, therefore I want the exact same implementation. 

I think it might help to include the way ES is creating the hash in the documentation of ES, under the section client side hashing. After all, that is what we're doing, and I was expecting to find an example implementation there as well.
</comment><comment author="clintongormley" created="2016-01-10T17:31:15Z" id="170373090">Hi @RubieV 

That sounds pretty use case specific to me.  I'm loathe to add a link to this code because there is a good chance it'll 404 pretty soon, and we won't know about it.

Thanks, but I think I'm not going to merge this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add stale shard issue to Resiliency page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14681</link><project id="" key="" /><description>This commit adds a simple description of the stale shard issue to the
Resiliency page.

Relates #14671
</description><key id="116335994">14681</key><summary>Add stale shard issue to Resiliency page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-11-11T13:29:21Z</created><updated>2015-11-11T15:40:48Z</updated><resolved>2015-11-11T15:33:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-11T15:32:03Z" id="155817719">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ordering aggregation buckets by script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14680</link><project id="" key="" /><description>Hi!
It would be great if we can order aggregation buckets by some sort of scripting, because it seems, that metric aggregation is not enough. My case is - I need to group documents by buckets, then determine in each bucket maximum and minimum timestamp and sort buckets by this difference. I could manage to do it if substract min aggregation result from max_aggregation result or even using 0 and 100 percentile, but it seems that now it is impossible.
Thanks in advance!
</description><key id="116332884">14680</key><summary>Ordering aggregation buckets by script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">antmat</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-11-11T13:06:20Z</created><updated>2016-02-14T16:03:52Z</updated><resolved>2016-02-14T16:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AlexMiroshnikov" created="2015-11-11T13:14:34Z" id="155783166">+1, I find it useful too.
</comment><comment author="jpountz" created="2015-11-12T11:12:23Z" id="156076685">I'm a bit on the fence as such a feature would increase the accuracy issues of the terms aggregations. Maybe we should consider only keeping the ability to sort by term or doc count on the terms aggregation and then deal about all other sort options with pipeline aggregations. I think the would make the trade-off clearer? (ie. terms are selected first and only sorted afterwards)
</comment><comment author="thao6626" created="2016-01-13T08:11:05Z" id="171211622">+1, sometimes, we need the value of the bucket_script in pipeline aggregations, and also,we need to sort the value
</comment><comment author="clintongormley" created="2016-02-14T16:03:52Z" id="183908237">Closing in favour of #14928
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure: SearchQueryIT.testConstantScoreQuery - all shards failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14679</link><project id="" key="" /><description>Build URL: http://build-us-00.elastic.co/job/es_core_master_debian/7823/testReport/junit/org.elasticsearch.search.query/SearchQueryIT/testConstantScoreQuery/

Cannot reproduce locally

Reproduction command:

```
gradle :plugins:lang-groovy:integTest -Dtests.seed=ED48A577973A709C -Dtests.class=org.elasticsearch.search.query.SearchQueryIT -Dtests.method="testConstantScoreQuery" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=573m -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:+UseCompressedOops" -Dtests.locale=sr_BA -Dtests.timezone=America/Yellowknife
```

Stack trace:

```
Failed to execute phase [query], all shards failed
    at __randomizedtesting.SeedInfo.seed([ED48A577973A709C:6772A8B75A54A1D6]:0)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:230)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:810)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:788)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[Gazelle][127.0.0.1:9500][indices:data/read/search[phase/query]]]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
Caused by: [test_1][[test_1][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
    at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:907)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:725)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:550)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:527)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:278)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:346)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:343)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="116311306">14679</key><summary>Test Failure: SearchQueryIT.testConstantScoreQuery - all shards failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>jenkins</label></labels><created>2015-11-11T11:04:55Z</created><updated>2015-11-15T20:49:03Z</updated><resolved>2015-11-15T20:49:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-15T20:49:03Z" id="156852118">closed in https://github.com/elastic/elasticsearch/commit/18a75fb30c9d186dac0c232d9ca85445892ee5ca
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take ignored unallocated shards into account when making allocation decision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14678</link><project id="" key="" /><description>ClusterRebalanceAllocationDecider did not take unassigned shards into account
that are temporarily marked as ignored. This can cause unexpected behaviour
when gateway allocator is still fetching shards or has marked shards as ignored
since their quorum is not met yet.

Closes #14670
</description><key id="116310022">14678</key><summary>Take ignored unallocated shards into account when making allocation decision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v1.7.4</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-11T10:55:26Z</created><updated>2015-12-03T09:16:30Z</updated><resolved>2015-11-16T13:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-11T15:19:07Z" id="155812961">@s1monw I don't understand the removal of the transactionId stuff, is that not used anywhere?
</comment><comment author="bleskes" created="2015-11-11T19:20:30Z" id="155883915">left some minor comments. LGTM o.w.
</comment><comment author="s1monw" created="2015-11-11T19:46:28Z" id="155890679">&gt; @s1monw I don't understand the removal of the transactionId stuff, is that not used anywhere?

it's not used anymore
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Allows upgrade of indexes with only search_analyzer specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14677</link><project id="" key="" /><description>Previously if an index was created before 2.0 using the default index_analyzer but specifying an explicit search_analyzer the index could not be started in 2.0 because we require 'analyzer' (formerly index_analyzer) to be set if you define an explicit search_analyzer. This change allows indexes created before 2.0 to use the default analyzer and specify an explicit search_analyzer. Indexes created on or after 2.0 are still required to explicitly set the analyzer if they want to explicitly set the search_analyzer.

Closes #14383
</description><key id="116303976">14677</key><summary>Mapping: Allows upgrade of indexes with only search_analyzer specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-11-11T10:20:14Z</created><updated>2015-11-16T13:14:42Z</updated><resolved>2015-11-16T09:02:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-11T15:12:03Z" id="155809147">@rjernst would you be able to review this for me?
</comment><comment author="rjernst" created="2015-11-11T15:14:48Z" id="155810820">Can you add a test?
</comment><comment author="colings86" created="2015-11-12T08:33:44Z" id="156033437">@rjernst I've added to the unit tests for string field mapping. I also added a field mapping to the index bwc python script and generated a new bwc index for 1.7.3 which contains the new field for testing.
</comment><comment author="rjernst" created="2015-11-12T08:55:10Z" id="156036856">@colings86 Please regenerate all of the indexes. There is a helper script to do that.
</comment><comment author="colings86" created="2015-11-12T10:20:22Z" id="156059578">@rjernst I've added a new commit with all the bwc indices regenerated
</comment><comment author="jpountz" created="2015-11-12T10:54:07Z" id="156072053">LGTM (but please wait for @rjernst to have a look as well before merging)
</comment><comment author="rjernst" created="2015-11-12T17:03:54Z" id="156167262">A few small suggestions. LGTM.
</comment><comment author="rjernst" created="2015-11-14T04:27:06Z" id="156625399">LGTM
</comment><comment author="colings86" created="2015-11-16T09:09:54Z" id="156961266">Pushed to the 2.0, 2.1 and 2.x branches. This fix shouldn't need to go into master since we won't need to support this kind of migration in 3.0. @clintongormley could you confirm that?
</comment><comment author="clintongormley" created="2015-11-16T12:51:36Z" id="157018966">@colings86 correct, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES returned back items are not equal the  hits total number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14676</link><project id="" key="" /><description>query:
GET /floriation/provider_goods/_search
{"query": {"bool": {"minimum_should_match": 0, "should": [], "must": [{"match": {"provider_goods_category_id": "1"}}]}}, "size": "20", "from": 0, "fields": ["id"]}
results:
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 16,
      "max_score": 1,
      "hits": [
         {
            "_index": "floriation",
            "_type": "provider_goods",
            "_id": "66",
            "_score": 1,
            "fields": {
               "id": [
                  66
               ]
            }
         },
         {
            "_index": "floriation",
            "_type": "provider_goods",
            "_id": "80",
            "_score": 1,
            "fields": {
               "id": [
                  80
               ]
            }
         }
      ]
   }
}
why and how should i do?
</description><key id="116301607">14676</key><summary>ES returned back items are not equal the  hits total number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hzruandd</reporter><labels /><created>2015-11-11T10:06:43Z</created><updated>2015-11-16T20:46:09Z</updated><resolved>2015-11-16T20:46:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-16T20:46:08Z" id="157164127">I'm sorry but I have no idea what you mean.  This sounds like a question for the forums: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update the disco node id seed on node restart in test cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14675</link><project id="" key="" /><description>To make sure the new node's id is changed.
</description><key id="116297683">14675</key><summary>Update the disco node id seed on node restart in test cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-11-11T09:37:20Z</created><updated>2016-03-10T18:15:05Z</updated><resolved>2015-11-15T21:03:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-11T10:29:39Z" id="155729313">LGTM
</comment><comment author="ywelsch" created="2015-11-11T10:33:39Z" id="155729887">LGTM 2 :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change Field stats API response to include both number and string based min and max values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14674</link><project id="" key="" /><description>Added `min_value_as_string` and `max_value_as_string` response elements for all number based fields. The existing `min_value` and `max_value` will return the values as numbers instead.

PR for #14404
</description><key id="116278020">14674</key><summary>Change Field stats API response to include both number and string based min and max values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>breaking</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-11T06:58:22Z</created><updated>2016-02-01T11:30:03Z</updated><resolved>2015-11-23T08:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-19T09:27:58Z" id="158000606">Code change LGTM. I would love someone with more feeling for the Aggs API to sanity check the xContent output and make sure the two are now consistent. /cc @colings86 @jpountz 
</comment><comment author="colings86" created="2015-11-20T14:00:02Z" id="158409493">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove hacks in 2.1 branch fixed by lucene 5.3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14673</link><project id="" key="" /><description>Followup to https://github.com/elastic/elasticsearch/pull/14669

There is a TODO in IndexShard.java that should be dealt with (I think its a workaround for some bad behavior of caching around MatchAllDocsQuery that is fixed in this 5.3.1 upgrade)

**For the future, I would greatly prefer if we did not add version-based asserts to IndexShard.java like this** 

This class masks all exceptions completely, so nobody ever sees your assert. Instead all tests hang, and when they finally fail its just stuff like "expected: GREEN, got: YELLOW" which does not help the guy trying to do the upgrade.

See #13319 for more information about that.
</description><key id="116269138">14673</key><summary>remove hacks in 2.1 branch fixed by lucene 5.3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>blocker</label><label>v2.1.0</label></labels><created>2015-11-11T05:30:26Z</created><updated>2015-11-17T18:03:49Z</updated><resolved>2015-11-17T18:03:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-11T05:37:40Z" id="155672276">I marked this as a blocker, i think its a better solution than adding those asserts? This way, the cleanup still takes place, but its easier since you can iterate (e.g. upgrade lucene, without having to also make a bunch of other scary changes all at once, just to make tests pass)
</comment><comment author="jpountz" created="2015-11-17T18:03:49Z" id="157454719">Closed via #14797
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to aws 1.10.33</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14672</link><project id="" key="" /><description>Security issues have been fixed. This removes our hacks.
</description><key id="116259550">14672</key><summary>Upgrade to aws 1.10.33</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>upgrade</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-11T03:53:25Z</created><updated>2015-11-16T20:35:36Z</updated><resolved>2015-11-11T04:12:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-11T03:58:04Z" id="155654886">Nice! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not allow stale replicas to automatically be promoted to primary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14671</link><project id="" key="" /><description>Consider a primary shard `P` hosted on node `p` and its replica shard `Q` hosted on node `q`. If `p` is isolated from the cluster (e.g., through node failure, a flapping NIC, or an excessively long garbage collection pause), indexing operations can continue on `q` after `Q` is promoted to primary; these indexing operations will be acknowledged to the requesting clients. If `q` is subsequently isolated before `p` rejoins and before a new replica is assigned to another node in the cluster, the subsequent rejoining of `p` can currently lead to `P` being promoted to primary again. The indexing operations acknowledged by `q` will be lost.

A mechanism needs to be built to prevent the automatic promotion of a stale shard in such a scenario and instead only promote a non-stale shard to primary (if a non-stale shard is availabie). The only scenario in which a stale shard should be promoted to primary is through manual intervention by a system operator (e.g., in cases when `q` suffers a total hardware failure).

Relates #10933
</description><key id="116243337">14671</key><summary>Do not allow stale replicas to automatically be promoted to primary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>enhancement</label><label>resiliency</label></labels><created>2015-11-11T01:39:24Z</created><updated>2016-02-14T16:02:47Z</updated><resolved>2016-02-14T16:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-11T12:59:32Z" id="155778051">Thanks @jasontedor . can we also update the resiliency page?
</comment><comment author="jasontedor" created="2015-11-11T15:34:43Z" id="155819031">@bleskes Added to the Resiliency page in #14681.
</comment><comment author="bleskes" created="2015-11-11T18:20:50Z" id="155866809">Thanks Jason!

On 11 nov. 2015 4:35 PM +0100, Jason Tedornotifications@github.com, wrote:

&gt; @bleskes(https://github.com/bleskes)Added to the Resiliency page in#14681(https://github.com/elastic/elasticsearch/pull/14681).
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/14671#issuecomment-155819031).
</comment><comment author="clintongormley" created="2016-02-14T16:02:47Z" id="183908165">Closed by #15281
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard relocation happens while cluster is yellow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14670</link><project id="" key="" /><description>This is _cluster/health output after one of nodes restarted:

```
{
  "cluster_name" : "&lt;cluster_name&gt;",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 14,
  "number_of_data_nodes" : 10,
  "active_primary_shards" : 1022,
  "active_shards" : 1839,
  "relocating_shards" : 2,
  "initializing_shards" : 0,
  "unassigned_shards" : 205,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0
}
```

It says cluster is in `yellow` status but it is relocating shards.
This cluster doesn't have custom `cluster.routing.allocation.allow_rebalance` configuration. So, it should be using `indices_all_active`.

No relocation should happen in this case?
</description><key id="116230164">14670</key><summary>Shard relocation happens while cluster is yellow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">masaruh</reporter><labels><label>:Allocation</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T23:57:36Z</created><updated>2015-11-16T14:58:23Z</updated><resolved>2015-11-16T13:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-11T00:04:25Z" id="155608074">@masaruh all the shards have initialized, so there isn't a reason why relocation wouldn't be allowed. Also, just because the cluster is yellow doesn't mean that the particular index relocating isn't green?
</comment><comment author="masaruh" created="2015-11-11T01:58:22Z" id="155630724">All primaries have initialized but not all shards are, right?

It says:

```
        /**
         * Re-balancing is allowed only once all shards on all indices are active. 
         */
        INDICES_ALL_ACTIVE;
```

I expect relocation happens only when cluster state is green.
</comment><comment author="dakrone" created="2015-11-11T02:03:20Z" id="155631343">@masaruh ahh okay, I see what you are saying, my mistake for misinterpreting it :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Lucene to 5.3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14669</link><project id="" key="" /><description>This is a minor release that fixes a few bugs.

See the release notes for details:

https://lucene.apache.org/core/5_3_1/changes/Changes.html

Change-Id: Iabce1eac263ae8744b9a01e11d53461bc3b23924
</description><key id="116225747">14669</key><summary>Upgrade Lucene to 5.3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">dpursehouse</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.1.0</label></labels><created>2015-11-10T23:23:35Z</created><updated>2015-11-20T14:07:15Z</updated><resolved>2015-11-11T06:54:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-10T23:39:17Z" id="155603261">Can you regenerate the sha1's for the artifacts? `mvn verify` should fail unless this is done.

There is a script to do it, but its not in this branch. Its here: https://github.com/elastic/elasticsearch/blob/2.x/dev-tools/update_lucene.sh

I think it should be committed here too. It just needs one modification: drop the line for lang-expressions since its not factored out into a plugin here.

Thanks for doing this!
</comment><comment author="dpursehouse" created="2015-11-11T00:31:43Z" id="155612331">@rmuir thanks for the quick feedback.  I've done as suggested and pushed some more commits.

Note that I had to rewrite the branch because I accidentally did the first commit with the wrong author email address :(
</comment><comment author="rmuir" created="2015-11-11T02:27:01Z" id="155637520">this looks great! thanks again
</comment><comment author="rmuir" created="2015-11-11T06:03:01Z" id="155678031">I tried out your branch and ran tests, I think we missed a few things still. We need to bump the lucene version in `Version.java`, and also deal with the assert in `IndexShard.java`. 

I think we should defer the IndexShard assert, I want to make sure the changes in that workaround (e.g. optimization for empty disjunctions case) is really all covered by https://issues.apache.org/jira/browse/LUCENE-6748: https://github.com/elastic/elasticsearch/issues/14673

Something like this? If you don't mind to just push to your branch, then we can stay on this PR.

```
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -268,7 +268,7 @@ public class Version {
     public static final int V_2_0_1_ID = 2000199;
     public static final Version V_2_0_1 = new Version(V_2_0_1_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_1_0_ID = 2010099;
-    public static final Version V_2_1_0 = new Version(V_2_1_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_3_0);
+    public static final Version V_2_1_0 = new Version(V_2_1_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_3_1);

     public static final Version CURRENT = V_2_1_0;

index 351e20e..2563d99 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -263,7 +263,7 @@ public class IndexShard extends AbstractIndexShardComponent {
         if (indexSettings.getAsBoolean(IndexCacheModule.QUERY_CACHE_EVERYTHING, false)) {
             cachingPolicy = QueryCachingPolicy.ALWAYS_CACHE;
         } else {
-            assert Version.CURRENT.luceneVersion == org.apache.lucene.util.Version.LUCENE_5_3_0;
+            // https://github.com/elastic/elasticsearch/issues/14673
             // TODO: remove this hack in Lucene 5.4, use UsageTrackingQueryCachingPolicy directly
             // See https://issues.apache.org/jira/browse/LUCENE-6748
             // cachingPolicy = new UsageTrackingQueryCachingPolicy();
```
</comment><comment author="dpursehouse" created="2015-11-11T06:26:56Z" id="155680555">@rmuir I added a couple more commits.  Please try again.
</comment><comment author="dpursehouse" created="2015-11-11T06:33:25Z" id="155681248">`mvn test` is passing for me now.
</comment><comment author="rmuir" created="2015-11-11T06:38:33Z" id="155681836">Yeah its looking good. For these lucene upgrades, I usually spin up a `mvn verify` because these upgrades modify packaging, licensing, impact plugins etc, I have it already running now, will push when its done.
</comment><comment author="rmuir" created="2015-11-11T06:55:02Z" id="155685597">thanks @dpursehouse !
</comment><comment author="dpursehouse" created="2015-11-11T06:59:04Z" id="155686318">No problem, thanks @rmuir for the quick feedback and merge.

Can you point me in the direction of where I can find the release schedule for v2.1.0?
</comment><comment author="rmuir" created="2015-11-11T07:06:00Z" id="155687265">The most up to date view would be https://github.com/elastic/elasticsearch/issues?q=is%3Aopen+is%3Aissue+label%3Av2.1.0

A lot of those look like TODOs that got pushed out from 2.0.0 to me and just ended out with 2.1.0 slapped on them, so I don't think the situation is as intimidating as it looks :)
</comment><comment author="dpursehouse" created="2015-11-11T07:11:14Z" id="155689459">Thanks.  If there are any low-hanging-fruit type issues I would be happy to help as much as I can.  I'd like to get my hands on 2.1.0 as soon as possible :)

https://gerrit-review.googlesource.com/#/c/58081/
</comment><comment author="rmuir" created="2015-11-11T07:38:32Z" id="155693595">you can additionally add the `adoptme` label to find issues with no owner yet. 

https://github.com/elastic/elasticsearch/issues/14673 is one idea, as its a followup to this one. It just means being a little careful: looking at git history/blame and ensuring that no other optimizations were slipped into the custom cache code, maybe double-checking https://issues.apache.org/jira/browse/LUCENE-6748, especially the "All" tab on jira and looking at what exactly got backported to 5.3.1 (since the issue doesn't have that label, yet is in CHANGES.txt, we should be careful). Then if its all ok, we can remove it and run tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove transitive dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14668</link><project id="" key="" /><description>Transitive dependencies can be confusing and hard to deal with when
conflicts arise between them. This change removes transitive
dependencies from elasticsearch, and forces any dependency conflicts to
be resolved manually, instead of automatically by gradle.

closes #14627
</description><key id="116222347">14668</key><summary>Remove transitive dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T23:01:52Z</created><updated>2015-11-11T20:28:25Z</updated><resolved>2015-11-11T03:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-10T23:18:14Z" id="155599087">This looks great! 

I am glad test-framework is no longer dragging in stuff like `ant` into the classpath. it makes everything transparent, e.g. that test-framework still has problems, like bringing in commons libraries via httpclient. David has a PR to remove that httpclient dep, but i think it got hung up on some java 7 stuff, we should really visit that, since we require java 8. i dont like that it drags in two commons-libraries, thats worse than guava.

Transitive dependencies are evil, doing that treats third party dependencies like they can be willy-nilly, but for any serious project, you should do them explicit like this, so you know what you are dragging in. They should not be magically sucked in but handled like nuclear weapons.
</comment><comment author="s1monw" created="2015-11-11T20:28:25Z" id="155900995">good! being explicit is important I think! I didn't know that's an option.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable GeoPointV2 with backward compatibility testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14667</link><project id="" key="" /><description>This PR removes all noreleases and cuts over to Lucene 5.4 GeoPointField type. Included are randomized testing updates to unit and integration test suites for ensuring full backward compatability with existing geo_point indexes.

closes #9859
closes #10761
closes #11159
</description><key id="116221155">14667</key><summary>Enable GeoPointV2 with backward compatibility testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T22:54:24Z</created><updated>2015-11-17T14:40:49Z</updated><resolved>2015-11-13T05:03:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-11-11T16:17:46Z" id="155832267">@rjernst if you wouldn't mind giving this a once over. The majority of the change is removing the noreleases and cutting over to `.before(V_2_2_0)`. But I've also added random index version creation  to test backward compatibility.
</comment><comment author="rjernst" created="2015-11-12T18:05:05Z" id="156185958">The cutover looks fine, but I am concerned about the tests.
</comment><comment author="rjernst" created="2015-11-12T20:33:42Z" id="156225944">LGTM, thanks for the fixes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kill backwards nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14666</link><project id="" key="" /><description>[test] Lots of work around leaking backwards nodes

We have a problem where we can leak backwards nodes. Several problems, really.
This commit addresses most of them:
1. Windows will always leak backwards nodes. We address this by killing the
   node directly rather than killing the bat process that runs it.
2. If we leak any zombie backwards nodes we never kill them. We address this
   by attempting to any straggler backwards compatibility nodes before running
   any backwards compatibility tests. It can also be used to kill any straggler
   nodes with this rather obtuse maven invocation:

mvn -pl qa/backwards/shared/ -Pkill test

That'll just kill the nodes without doing anything else and should be useful
for continuous integration.
1. Backwards nodes can steal ports that regular test nodes want to use. This
   is especially nasty if they leak and poison subsequent test runs. We address
   this by moving their transport ports to 29200+.

We also refuse to start the backwards node if it doesn't have a port range
configured. This prevents accidentally running the external nodes in the
standard port range and _forces_ you to put them in 29200+.

Along for the ride is a cleanup to track the running backwards nodes by
transport address rather than http address. Both are useful but only transport
is useful in the context of the tests in which we run them.

Finally this removes the restriction that backwards compatibility tests run
in a single JVM. I added this restriction early on in the process of reviving
the backwards compatibility tests because I was worried about port conflicts.
Now that we're very very careful with ports this isn't require. Running on
multiple JVMs speeds up the process 70% for me. It might not do it for
everyone but 70% is still awesome.

Note: This has been edited to include the entire commit message.
</description><key id="116198574">14666</key><summary>Kill backwards nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-11-10T20:49:16Z</created><updated>2015-11-16T15:08:46Z</updated><resolved>2015-11-16T14:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-10T21:02:40Z" id="155564827">I'm not really comfortable with this patch but I don't know anything better right now. It offers some much needed defense against the zombies that can be created by the backwards compatibility tests.

For those who haven't been following too closely:

For about a week now 2.1 and 2.x branches have been running backwards compatibility tests - quite successfully in the case of linux and failing every time one windows. The root cause seems to be the way we fork out the backwards nodes for testing. There isn't a guarantee that the nodes are killed after the tests are finished. And the windows machines in CI are getting gummed up, filled with a dozen backwards compatibility elasticsearch processes that just won't die.

So I thought about a few solutions:
1. Kill all the backwards processes after every test.
2. Teach Elasticsearch to optionally poll a port and kill itself if the port isn't available. Hook this to the ExternalNodeService we use for backwards compatibility testing.
3. Launch yet another process that monitors the elasticsearch process and the backwards compatibility process and kills elasticsearch if the backwards compatibility process dies.

Option 3 isn't particularly good because its still prone to the same sorts of failure that we're prone to already - its just the monitor process that has to die for you to be broken.

Option 2 isn't particularly good because its polluting Elasticsearch with logic that is really only useful for testing. It'd guarantee no zombies, but it'd be impossible to apply to already released artifacts as I can't go back in time and commit that. I'm not sure how "I have knowledge from the future" would work as a code review justification. Also its not so clean.

Option 1 isn't great because its very OS dependent and it only works if you can hook it into CI - because you can't rely on the testing process to not die so that it can kill the forked backwards process.

Another other option would be to put the kill code in dev-tools and just run it before all tests. That option just feels icky but I can't think of how to explain it in words.

The only thing I'm really sure about is that this kills the processes on windows and linux.
</comment><comment author="nik9000" created="2015-11-10T21:50:19Z" id="155577714">OK! I think I've figured out the issue on windows. ExternalNodeService uses `Process.destroy` to kill elasticsearch and that works on linux and OSX because process.kill is actually killing the bash script that runs elasticsearch which in turn kills its child, the elasticsearch process. That doesn't work on windows. I don't know exactly why, it could be lots of things:
1. On windows `process.kill` is more like `kill -9` so maybe the bat script doesn't have a chance to kill its child before dying.
2. Maybe bat scripts just don't kill their children by default.

Either way I can work around this. Its ugly, but I can do it!
</comment><comment author="rmuir" created="2015-11-10T22:03:05Z" id="155582059">That sounds like it might be right. I don't know anything about windows .bat but maybe it has options for this? Would be nice if we could set things up right and remove the JNA native code we are using to handle killing the controlling terminal window too.
</comment><comment author="nik9000" created="2015-11-10T22:33:15Z" id="155589157">&gt; Would be nice if we could set things up right and remove the JNA native code we are using to handle killing the controlling terminal window too.

Woah. I had no idea.
</comment><comment author="nik9000" created="2015-11-10T22:33:55Z" id="155589318">BTW, as it stands now this patch is ugly but it actually passes on windows. Its horrible, but it "works" for some definition of works.
</comment><comment author="rmuir" created="2015-11-10T22:41:51Z" id="155591137">note that this grepping of the log (maybe it should be done with an actual socket connection?) will be a little fragile. i know at least 2.2 might print different things than 2.1 and so on due to network changes.
</comment><comment author="rmuir" created="2015-11-10T22:42:20Z" id="155591249">but yeah, i think its ok, just giving out the warning.
</comment><comment author="nik9000" created="2015-11-11T02:26:53Z" id="155637435">&gt; but yeah, i think its ok, just giving out the warning.

Fair enough. I think I might be able to get away with using the PID as the id instead of the port - and in that case I can force the pidfile through.

The part I'm really uncomfortable with is the `taskkill`, `ps aux`, business. Basically all the OS specific process management. I don't know of a way around its still funky.
</comment><comment author="nik9000" created="2015-11-11T18:14:17Z" id="155865354">@bleskes I think this resolves the rest of the things you were concerned about with these tests. And it gets them passing on windows! It needs reasonably hacky external process stuff, but it works.
</comment><comment author="nik9000" created="2015-11-11T19:35:22Z" id="155888018">@javanna you can now run the BWC tests in parallel again! Hurray!
</comment><comment author="nik9000" created="2015-11-12T15:57:01Z" id="156147310">@javanna asked me in a meeting how this code is different from the code that we use for external nodes in ant so I dug into it and its almost exactly the same. The only difference is that the ant code uses the pidfile and the ExternalNodeService uses the pid logged at startup. The only reason they are different there is that ExternalNodeService already is doing text analysis on the logs so its simpler to get the pid that way. Its more brittle and it should probably switch to the pidfile, but for now I don't think its worth it. But the actual mechanism of action - the actual `kill` code, is exactly the same. 
</comment><comment author="nik9000" created="2015-11-16T13:57:08Z" id="157034624">Ok - at this point lack of reviews has turned into lack of objections in my mind. I'm going to squash and rebase and merge this and see if I can get the windows builds working. @rmuir suggested trying not closing the streams to see if that helps prevent leaks. I'm going to file another issue and work on that soon.
</comment><comment author="nik9000" created="2015-11-16T15:08:46Z" id="157061710">Merged to 2.x and backported to 2.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get correct jps path on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14665</link><project id="" key="" /><description>This commit fixes an issue with the jps executable sometimes being
available under jps.exe but not jps.
</description><key id="116189214">14665</key><summary>Get correct jps path on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T19:57:13Z</created><updated>2015-11-10T20:05:29Z</updated><resolved>2015-11-10T20:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-10T20:02:43Z" id="155549843">This is to address builds that are failing on Windows machines because the existence check for the jps executable is failing; instead we should be checking for "jps.exe" on these boxes.
</comment><comment author="nik9000" created="2015-11-10T20:03:25Z" id="155550038">Its worth trying. I've just set up a windows box for testing other stuff and might be able to test this using it later if you'd like. It looks fine though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Category completion suggestion -- support "and" instead of "or"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14664</link><project id="" key="" /><description>Currently if you have a completion context mapping defined, you can search for mappings that fall into `category_a` OR `category_b`, eg. `red` or `blue`

```
....
    "context": {
                "color": ["red", "blue"]
            }
```

However, I want to be able to find keys that are both `red` _and_ `blue`. Is there something fundamentally different about this, or is it a relatively simple change?
</description><key id="116182254">14664</key><summary>Category completion suggestion -- support "and" instead of "or"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rcoh</reporter><labels /><created>2015-11-10T19:23:22Z</created><updated>2015-11-16T23:27:23Z</updated><resolved>2015-11-16T19:59:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-16T19:59:12Z" id="157152972">Hi @rcoh 

&gt; Is there something fundamentally different about this, or is it a relatively simple change?

There is something fundamentally different about this: a context is basically added as a prefix to all the suggestions in that context, so the only way to do "and" is to combine both tags into a single tag (and thus a single prefix), something you can do manually yourself: 

```
color: ["red", "blue", "blue_red"]
```
</comment><comment author="rcoh" created="2015-11-16T20:58:20Z" id="157168315">Interesting. Does that mean having a context with a high cardinality is potentially very inefficient?
</comment><comment author="rcoh" created="2015-11-16T22:48:21Z" id="157198459">Also, is the FST per shard or is it global for the index?
</comment><comment author="areek" created="2015-11-16T22:59:33Z" id="157201964">Hey @rcoh,

&gt; Interesting. Does that mean having a context with a high cardinality is potentially very inefficient?

Yes the higher the cardinality of (index-time) contexts the more inefficient the FST becomes in terms of memory. Using multiple contexts at query-time will not have a significant impact on query performance.

&gt; Also, is the FST per shard or is it global for the index?

FST is a shard-level data structure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade minimum gradle version to 2.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14663</link><project id="" key="" /><description>In gradle 2.7 (or groovy 2.3.10, not sure which), there appears to be a
bug on linux where using a fully qualified class (without an import
statement) does not work. This change forces gradle 2.8 or above. It
also moves the logic around a little for the version check so the build
info is printed before checks against that info.
</description><key id="116181716">14663</key><summary>Upgrade minimum gradle version to 2.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T19:21:01Z</created><updated>2015-11-10T19:23:00Z</updated><resolved>2015-11-10T19:22:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-10T19:21:29Z" id="155537522">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate elasticsearch-native-script-example to the main repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14662</link><project id="" key="" /><description>I am currently maintaining a separate project that demonstrates how to build different native scripts for [elasticsearch-native-script-example](https://github.com/imotov/elasticsearch-native-script-example). I think it would make sense to add this project to the main repo similarly to [jvm-example](https://github.com/elastic/elasticsearch/tree/master/plugins/jvm-example). We just need to makes sure that the added project (as well as jvm-example itself) can be built as a stand-alone project. 
</description><key id="116172652">14662</key><summary>Migrate elasticsearch-native-script-example to the main repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>build</label><label>won't fix</label></labels><created>2015-11-10T18:34:32Z</created><updated>2016-08-11T22:13:16Z</updated><resolved>2016-08-11T22:13:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-10T18:55:46Z" id="155530665">++ 
</comment><comment author="imotov" created="2016-08-11T22:13:16Z" id="239309598">See discussion in #19334. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add circuit breaker name to logging package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14661</link><project id="" key="" /><description>This allows different circuit breakers to have different logging levels.
It's useful when diagnosing problems (say for instance with the
fielddata breaker) and not seeing the enormous amount of logging from
the request breaker.

The log messages use the breaker name for logging, so example logging
will look like:

```
[2015-11-10 09:51:52,993][TRACE][indices.breaker.fielddata] [fielddata] Adding [27b][body] to used bytes [new used: [27b], limit: 623326003 [594.4mb], estimate: 27 [27b]]
[2015-11-10 09:51:53,000][TRACE][indices.breaker.fielddata] [fielddata] Adjusted breaker by [453] bytes, now [480]
[2015-11-10 09:51:53,016][TRACE][indices.breaker.request  ] [request] Adjusted breaker by [16440] bytes, now [16440]
[2015-11-10 09:51:53,018][TRACE][indices.breaker.request  ] [request] Adjusted breaker by [-16440] bytes, now [0]
```
</description><key id="116153022">14661</key><summary>Add circuit breaker name to logging package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T17:07:12Z</created><updated>2016-02-04T23:06:53Z</updated><resolved>2015-11-10T18:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-10T18:26:02Z" id="155522553">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mutate processor improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14660</link><project id="" key="" /><description>Remove code duplications from ConfigurationUtils
Make sure that the mutate processor doesn't use Tuple as that would require to depend on core.
Also make sure that the MutateProcessor tests don't end up testing the factory as well.
Make processor getters package private as they are only needed in tests.
Add new tests to MutateProcessorFactoryTests
</description><key id="116146775">14660</key><summary>Mutate processor improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-10T16:39:40Z</created><updated>2015-11-11T10:34:07Z</updated><resolved>2015-11-11T10:34:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-11-10T22:17:22Z" id="155585466">:+1: LGTM besides my one comment on implementing a special `equalsTo` for `GsubExpression`'s pattern equivalence
</comment><comment author="martijnvg" created="2015-11-11T03:07:05Z" id="155647514">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`copy_to` does not work for nested objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14659</link><project id="" key="" /><description>I am trying to copy a field from the parent object into a nested object in order to perform a filtered sort. This however seems to have no effect.

A complete description can be found here: http://stackoverflow.com/questions/33618467/how-to-add-properties-from-a-root-object-in-a-nested-object-for-sorting
</description><key id="116142282">14659</key><summary>`copy_to` does not work for nested objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EECOLOR</reporter><labels /><created>2015-11-10T16:23:56Z</created><updated>2015-11-16T20:51:48Z</updated><resolved>2015-11-16T19:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-16T19:50:20Z" id="157149684">Hiya

I understand your use case (took a while to figure out). I don't think we should add functionality to copy_to every nested doc, or allow access to the parent doc during sorting.  I think the right solution here is to just add the parent field to each nested document before indexing.  
</comment><comment author="EECOLOR" created="2015-11-16T20:51:48Z" id="157165647">Fair enough, thank you for looking into it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Delete-by-Query with Shield</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14658</link><project id="" key="" /><description>closes #14527
</description><key id="116141436">14658</key><summary>Fix Delete-by-Query with Shield</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugin Delete By Query</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T16:20:24Z</created><updated>2015-11-16T14:21:38Z</updated><resolved>2015-11-10T19:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-11-10T16:23:07Z" id="155471869">@javanna Can you have a look please? Thanks
</comment><comment author="javanna" created="2015-11-10T16:33:22Z" id="155478332">left two minor comments, LGTM otherwise, thanks @tlrx 
</comment><comment author="javanna" created="2015-11-10T17:01:29Z" id="155487283">LGTM
</comment><comment author="tlrx" created="2015-11-10T20:19:48Z" id="155553942">@javanna Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sum aggregation hangs on a random shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14657</link><project id="" key="" /><description>The aggregation query stucks on a random shard (different each time we restart the cluster). There is no errors in logs. It just doesn't return anything (hanging until you close connection).

BUT, if you replace `field: foo` by `script: doc['foo'].value` everything works fine.

Iterating over the shards (settings `preference=_shards:X`) show that the query hangs only on one particular shard.

ElasticSearch Version: 1.7.4
Number of documents: 11
Mapping: 

``` json
"foo": {
  "index": "not_analyzed",
  "norms": {
     "enabled": false
  },
  "doc_values": true,
  "type": "integer"
}
```

Query:

``` json
{
   "query": { // some query which returns ZERO documents },
   "aggs": {
      "broken_sum": { "sum": { "field": "foo" } }
   }
}
```
</description><key id="116134621">14657</key><summary>Sum aggregation hangs on a random shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SHSE</reporter><labels /><created>2015-11-10T15:51:48Z</created><updated>2015-11-17T11:31:59Z</updated><resolved>2015-11-17T11:19:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SHSE" created="2015-11-11T15:47:50Z" id="155822383">It looks like the reason was `mlockall = true` and two ElasticSearch instances per host. 

After setting `mlockall` to `false` everything works fine.
</comment><comment author="clintongormley" created="2015-11-17T11:19:44Z" id="157341170">That sounds like you are trying to assign too much heap to each Elasticsearch instance.  Disabling mlockall will cause severe problems with slow garbage collection later on.
</comment><comment author="SHSE" created="2015-11-17T11:31:30Z" id="157343355">@clintongormley the server has 256 GB RAM and the `ES_HEAP_SIZE` is set to 30GB. There are 2 instances of ElasticSearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>'bucket_script' not supported as a top-level aggregation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14656</link><project id="" key="" /><description>Hi,

Why is the 'bucket_script' not supported as a top-level aggregation?

I have to produce a top-level ratio of sums... The following code gives me an error:

```
GET /sales-2015.11.*/sale/_search
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "per_day": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "day"
      },
      "aggs": {
        "sum_of_sales": {
          "sum": {
            "field": "sales"
          }
        },
        "sum_of_vendors_count": {
          "sum": {
            "field": "vendors_count"
          }
        },
        "ratio_of_sales_per_vendors_count": {
          "bucket_script": {
            "buckets_path": {
              "sum_of_sales": "sum_of_sales",
              "sum_of_vendors_count": "sum_of_vendors_count"
            },
            "script": "sum_of_sales / sum_of_vendors_count"
          }
        }
      }
    },
    "sum_of_all_sales": {
      "sum": {
        "field": "sales"
      }
    },
    "sum_of_all_vendors_count": {
      "sum": {
        "field": "vendors_count"
      }
    },
    "ratio_of_all_sales_per_all_vendors_count": {
      "bucket_script": {
        "buckets_path": {
          "sum_of_all_sales": "sum_of_all_sales",
          "sum_of_all_vendors_count": "sum_of_all_vendors_count"
        },
        "script": "sum_of_all_sales / sum_of_all_vendors_count"
      }
    }
  },
  "size": 0
}
```

It gives me the following error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "aggregation_execution_exception",
            "reason": "Invalid pipeline aggregation named [ratio_of_all_sales_per_all_vendors_count] of type [bucket_script]. Only sibling pipeline aggregations are allowed at the top level"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "sales-2015.11.01",
            "node": "N3_ZEgs-RAGPOssZrCcL_w",
            "reason": {
               "type": "aggregation_execution_exception",
               "reason": "Invalid pipeline aggregation named [ratio_of_all_sales_per_all_vendors_count] of type [bucket_script]. Only sibling pipeline aggregations are allowed at the top level"
            }
         }
      ]
   },
   "status": 500
}
```

How can I get it working?

Thanks!
</description><key id="116122988">14656</key><summary>'bucket_script' not supported as a top-level aggregation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davmrtl</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-11-10T14:59:31Z</created><updated>2016-10-18T06:43:47Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-16T19:32:19Z" id="157145039">@colings86 Could you take a look at this please?
</comment><comment author="colings86" created="2015-11-17T08:29:49Z" id="157307807">@davmrtl This is by design. The bucket_script aggregation is a [Parent Pipeline Aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline.html#search-aggregations-pipeline) and therefore needs to be inside a multi-bucket aggregation to run since it expects to be given buckets from its parent aggregation and add its results to those buckets. For now, if you want to run arbitrary calculations using top-level aggregation results you will need to  do this in your client side application.
</comment><comment author="claudio-viola" created="2016-08-03T14:29:17Z" id="237253007">HI!
Is there any other way to implement this functionality not on the client_side?
Are we forced to use scripted metrics https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-scripted-metric-aggregation.html  which will basically perform the aggregation and do the ratio?

Will this ever be implemented?
Thank you!
</comment><comment author="ebuildy" created="2016-10-13T10:12:07Z" id="253472692">@claudio-viola did you try to use https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-bucket-global-aggregation.html? 
</comment><comment author="anhzhi" created="2016-10-18T06:41:43Z" id="254420443">i get a very ugly solution like:

``` json
 {
   "size": 0,
   "aggs": {
      "result": {
         "terms": {"script":"'a big trick'"},
         "aggs": {
            "Total&#35745;&#25968;": {
                "value_count": {
                    "field": "_index"
                }
            },
            "TCP": {
                "filter": {
                    "term": {
                        "_type": "mprobe_tcp"
                    }
                },
                "aggs": {
                    "&#35745;&#25968;2": {
                        "value_count": {
                            "field": "_index"
                        }
                    }
                }
            },
            "&#30334;&#20998;&#27604;": {
                "bucket_script": {
                    "buckets_path": {
                        "TCP&#35745;&#25968;2": "TCP&gt;&#35745;&#25968;2",
                        "Total&#35745;&#25968;": "Total&#35745;&#25968;"
                    },
                    "script": "TCP&#35745;&#25968;2/Total&#35745;&#25968;"
                }
            }
        }
      }
   }
}
```

i wish that can help 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[WIP] Add and use isEmpty() public method to the BlobPath.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14655</link><project id="" key="" /><description /><key id="116111793">14655</key><summary>[WIP] Add and use isEmpty() public method to the BlobPath.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexVengrovsk</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-11-10T14:00:49Z</created><updated>2015-11-30T14:12:53Z</updated><resolved>2015-11-30T14:12:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexVengrovsk" created="2015-11-10T14:11:13Z" id="155430996">I signed the CLA as Alex Vengrovskyi and I am a member of Elasticsearch 
</comment><comment author="alexVengrovsk" created="2015-11-10T14:48:21Z" id="155439444">@dadoonet I am sorry for such a trivial mistake. I fixed it. Thank you!
</comment><comment author="dadoonet" created="2015-11-10T14:55:33Z" id="155441399">@alexVengrovsk So you did not compile your code before sending the PR, right?
BTW I'm curious if you need this for a repository plugin or something else?

@imotov Wanna review this change?
</comment><comment author="alexVengrovsk" created="2015-11-10T15:03:31Z" id="155444056">Yes, I didn't. I made changes using github interfase. 
</comment><comment author="dadoonet" created="2015-11-10T15:13:37Z" id="155447471">@alexVengrovsk So can you please explain why you need this change?
And if you want it to be merged, could you please run `gradle test` so we are all sure you did not break anything.
</comment><comment author="alexVengrovsk" created="2015-11-10T15:18:13Z" id="155448815">I can do it later. I wanted to make an improvemeth to the code, to make a contribution to the project. 
</comment><comment author="dadoonet" created="2015-11-10T15:24:34Z" id="155450479">Thank you. We really appreciate contributions in code, tests, documentation, wherever!
</comment><comment author="clintongormley" created="2015-11-16T19:39:09Z" id="157146719">@alexVengrovsk I think David is asking why this code should be added? What's the use case? Also, any changes should be accompanied by tests and (if required), documentation.
</comment><comment author="dadoonet" created="2015-11-30T14:12:53Z" id="160641466">No news on that and does not seem really needed. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix IndexSearcherWrapper interface to not depend on the EngineConfig</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14654</link><project id="" key="" /><description>We only passed the engine config since we had no chance to get the cache
etc. from the IndexSearcher. Now that we have these getters we can just pass the
searcher instead.
</description><key id="116110850">14654</key><summary>Fix IndexSearcherWrapper interface to not depend on the EngineConfig</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T13:55:26Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-11-10T16:09:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-10T15:24:42Z" id="155450516">LGTM
</comment><comment author="martijnvg" created="2015-11-10T15:37:48Z" id="155456380">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disruption rules in MockTransportService should match all bound addresses of a node.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14653</link><project id="" key="" /><description>Currently, disruption rules apply only to the main bound address of a node. As consequence, integration tests that use Network Partitions must currently ensure that nodes are bound to a single local interface (e.g. 127.0.0.1 / IPv4) in order to correctly work. 

This PR changes the disruption rules to work on all transport addresses that are bound by a node.

Relates to #14625 
</description><key id="116107128">14653</key><summary>Disruption rules in MockTransportService should match all bound addresses of a node.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Network</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T13:35:53Z</created><updated>2015-11-12T16:22:15Z</updated><resolved>2015-11-12T16:22:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-10T14:00:59Z" id="155427327">I think this is great. Good not to have to specifiy a host. I left some comments. Mostly around documentation. Also, it would be great to adda  note here (and to the commit) as to why we doing it - i.e., that unicast ping doesn't use disco nodes but rather constructs wrappers around explicit addresses. 
</comment><comment author="ywelsch" created="2015-11-10T17:12:09Z" id="155491024">Pushed another set of changes.
- Removed TransportAddress from NetworkPartition.
- Added Javadoc to MockTransportService
- Added method `extractTransportAddresses(TransportService)` that combines publish and bound addresses.
</comment><comment author="bleskes" created="2015-11-11T12:52:52Z" id="155772368">LGTM. Great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only allow rebalance operations to run if all shard store data is available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14652</link><project id="" key="" /><description>This commit prevents running rebalance operations if the store allocator is
still fetching async shard / store data to prevent pre-mature rebalance decisions
which need to be reverted once shard store data is available. This is typically happening
on rolling restarts which can make those restarts extremely painful.

Closes #14387

this is the 1.7 backport of #14591 
</description><key id="116100867">14652</key><summary>Only allow rebalance operations to run if all shard store data is available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v1.7.4</label></labels><created>2015-11-10T12:52:07Z</created><updated>2015-11-17T10:55:09Z</updated><resolved>2015-11-11T13:04:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-10T12:52:29Z" id="155412402">@bleskes can you take a quick look - it's the 1.7 backport
</comment><comment author="bleskes" created="2015-11-10T12:59:24Z" id="155413492">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Sequence Numbers and enforce Primary Terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14651</link><project id="" key="" /><description>This PR (against the feature/seq_no branch) adds a counter to each write operation on a shard. This sequence numbers is indexed into lucene using doc values, for now (we will probably require indexing to support range searchers in the future).

On top of this, primary term semantics are enforced and shards will refuse write operation coming from an older primary.

Other notes:
- The add SequenceServiceNumber is just a skeleton and will be replaced with much heavier one, once we have all the building blocks (i.e., checkpoints).
- I completely ignored recovery - for this we will need checkpoints as well.
- A new based class is introduced for all single doc write operations. This is handy to unify common logic (like toXContent).
- For now, we don't use seq# as versioning. We could in the future.

Relates to #10708 
</description><key id="116098403">14651</key><summary>Add Sequence Numbers and enforce Primary Terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label><label>enhancement</label></labels><created>2015-11-10T12:35:06Z</created><updated>2015-11-19T14:36:17Z</updated><resolved>2015-11-19T14:36:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-10T13:09:44Z" id="155415288">looks great I left some comments
</comment><comment author="bleskes" created="2015-11-13T19:57:50Z" id="156541751">@jasontedor can you take another look?
</comment><comment author="jasontedor" created="2015-11-18T22:37:34Z" id="157887503">@bleskes I have a few nits, and some concerns about the `-1`s. It seems to be that there are at least two distinct uses of `-1` (uninitialized, and no-op) and I wonder if they should be separated and use some constant fields? Otherwise, LGTM.
</comment><comment author="bleskes" created="2015-11-19T13:14:20Z" id="158053741">@jasontedor  +1 to the -1 :) . pushed another round. Also rebased on latest master so we can sue the writeZLong thing.
</comment><comment author="jasontedor" created="2015-11-19T13:20:20Z" id="158054773">LGTM.
</comment><comment author="bleskes" created="2015-11-19T14:36:13Z" id="158074737">Merged into `feature/seq_no`. Thanks @jasontedor.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove AbstractLegacyBlobContainer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14650</link><project id="" key="" /><description>`AbstractLegacyBlobContainer` was kept for historical reasons (see #13434).
We can migrate Azure and S3 repositories to use the new methods added in #13434 so we can remove `AbstractLegacyBlobContainer` class.
</description><key id="116071579">14650</key><summary>Remove AbstractLegacyBlobContainer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>:Plugin Repository S3</label><label>:Snapshot/Restore</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T09:44:32Z</created><updated>2015-11-16T19:33:25Z</updated><resolved>2015-11-10T15:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-10T09:45:03Z" id="155372047">@imotov Wanna look? I come to this while looking at how to fix #14633.
</comment><comment author="imotov" created="2015-11-10T14:00:00Z" id="155427008">Nice change! I left a couple of minor comments. Otherwise, LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Output test failure log to a file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14649</link><project id="" key="" /><description>When running the test suite I sometimes put the output into a file, so I can grep / less it later. Here is what I tried to do with the new gradle build:

```
gradle test integTest &gt; ~/test.txt
```

Sadly, the test file only contains the fact that a test failed, but not the log output nor the reproduction line.

```
Tests with failures:
  - org.elasticsearch.recovery.RecoveryWhileUnderLoadIT.testRecoverWhileRelocating
```

Please educate me if I'm doing this wrong. O.w. it would be great if we can come up with a way to do this.
</description><key id="116067698">14649</key><summary>Output test failure log to a file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>build</label></labels><created>2015-11-10T09:21:28Z</created><updated>2015-11-19T09:03:58Z</updated><resolved>2015-11-19T09:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-10T16:00:47Z" id="155464579">@bleskes Have you tried redirecting stdout and stderr? Gradle outputs its stuff to stderr (this is why tasks can do `println` and the output is not caught). It should detect the output is not to a terminal and omit the progress line.
</comment><comment author="rmuir" created="2015-11-10T16:03:23Z" id="155465497">I think its confusing too though fwiw, i expect stdout to go to stdout. is it possible to not use gradle logging here?
</comment><comment author="rjernst" created="2015-11-10T16:05:14Z" id="155466025">AFAIK, gradle logging goes to stderr (or maybe it is error logs go to stderr and info logs go to stdout?). I'll do some more testing to see for sure..
</comment><comment author="rmuir" created="2015-11-10T16:33:21Z" id="155478323">but can we skip gradle logging all together and just send it directly to stdout?
</comment><comment author="bleskes" created="2015-11-10T16:37:25Z" id="155480110">the screen is long gone, but if memory serves, there was indeed some gradle output there but not the failed logs...
</comment><comment author="rjernst" created="2015-11-10T16:56:18Z" id="155485392">@bleskes Are you saying the test file only had that single summary statement of the test failures, and your screen had nothing else for tests at all?
</comment><comment author="bleskes" created="2015-11-10T16:59:25Z" id="155486302">@rjernst It had more things like the test summary (so many tests run etc.), but nothing about the failure (it had the stalled test messages)
</comment><comment author="rjernst" created="2015-11-19T01:04:48Z" id="157913802">@bleskes I ran the same command in core as you, with an fake error in that test. The only difference being I captured both stderr and stdout.
https://gist.github.com/rjernst/e412ec16001e6acf6141

When I run the same command as you, I see the actual error go to my console still (including the repro line). So, the answer to this issue is, you need to capture both stderr and stdout. Without the ability to reproduce what you saw, I don't know what else to do here.
</comment><comment author="bleskes" created="2015-11-19T09:03:58Z" id="157994597">Thanks @rjernst for taking a look. I'll try that and reopen the issue if something doesn't work. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problematic numbers using regex for numeric fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14648</link><project id="" key="" /><description>Using regex query for a numeric field (eg. long type) fails for certain numbers (eg. 34).

{"query":{"query_string":{"query":"some_numeric_field:/34/"}}}

Non regex query works fine.

{"query":{"query_string":{"query":"some_numeric_field:34"}}}

It might be connected to ASCII (34 is ").
</description><key id="116063353">14648</key><summary>Problematic numbers using regex for numeric fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Robitx</reporter><labels /><created>2015-11-10T08:50:10Z</created><updated>2015-11-16T19:18:57Z</updated><resolved>2015-11-16T19:18:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-16T19:18:57Z" id="157141368">Nice catch :)

You can't use the regexp query on numeric fields.  I've opened https://github.com/elastic/elasticsearch/issues/14782 to improve the exception
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add comparison processor for supporting conditionals in pipeline definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14647</link><project id="" key="" /><description>The Ingest Pipeline should support the ability for conditional branching of processor execution.

The proposal here is to define a `CompareProcessor` which will accept a list of conditions that must all be met for specific processors to be executed. This processor would have a `conditions` parameter that would accept various boolean operators, such as `gt`, `lt`, `gte`, `lte`, `eq`, `neq`. If all of the provided conditions evaluate to true, then a set of processors defined in a field potentially called `if_processors` would be executed. If any conditions in this set evaluate to false, then processors in the `else_processors` list option would be executed. This type of conditional can be though of as an "if/else" processor, where the `conditions` field is a compounded boolean expression joined with an `AND`. Continuing with this analogy -- the `if_processors` block would be the `if` block, and the `else_processors` would be the else block.

Whether both `if_processors` and `else_processors` configuration options would be necessary is still up in the air, at least one must be defined.

Since the `CompareProcessor` is just another processor, nested comparisons would be possible.

Here is a potential processor definition for the `CompareProcessor`

``` json
{
  "compare": {
    "conditions": [
      "&lt;field_name1&gt;" : { "&lt;OP1&gt;": x },
      "&lt;field_name2&gt;" : { "&lt;OP2&gt;": y },
      "&lt;field_name3&gt;" : { "&lt;OP3&gt;": z }
    ],
    "if_processors": [...],
    "else_processors": [...]
  }
}
```

Here is an example pipeline definition with a Compare Processor

``` json
{
  "description": "_description",
  "processors": [
    {
      "compare": {
        "conditions": [
          "ip_address": { "eq": "127.0.0.1" },
          "user_age": { "gt": 13 }
        ],
        "if_processors": [
          "compare" : {
            "conditions" : [
               "country_name" : { "neq": "USA" }
               "if_processors" : [
                 {
                   "geoip" : {
                     "source_field" : "ip"
                   }
                 }
               ]
            ]
          }
        ],
        "else_processors" : [
          {
            "mutate" : {
              "remove" : ["country_name"]
            }
          }
        ]
      },
      "mutate" : {
        "update" : {
          "field2" : "_value"
        }
      }
    }
  ]
}
```
</description><key id="116058349">14647</key><summary>[Ingest] Add comparison processor for supporting conditionals in pipeline definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-10T08:09:39Z</created><updated>2016-07-20T17:33:22Z</updated><resolved>2016-07-20T17:33:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-10T17:10:06Z" id="155489886">Should we try and expand a bit more on what can be a condition and what syntax should be supported there? For instance can there be field values on the right side or only constants?

I would probably change naming, if it's an if why call it compare? and I think true_processors and false_processors might get confusing, why not use simply if and else and maybe call the processor "condition" or something along those lines? I think we should have @clintongormley have a look at the api and validate it also :)
</comment><comment author="talevy" created="2015-11-10T21:24:15Z" id="155571593">&gt; Should we try and expand a bit more on what can be a condition and what syntax should be supported there? For instance can there be field values on the right side or only constants?

I was thinking of enabling the same mustache templating that is introduced [here](https://github.com/elastic/elasticsearch/issues/14644).

an example templated field value: `{{ctx._source.department}}`

This would enable fields to be compared to other fields.

```
"conditions" [
  "first_name": {"neq": "{{ctx._source.last_name}}"}
]
```

&gt; I would probably change naming, if it's an if why call it compare? and I think true_processors and false_processors might get confusing, why not use simply if and else and maybe call the processor "condition" or something along those lines?

do you propose something more similar to this syntax?

```
{
  "condition": {
    "conditions": [
      "field.hello" : { "neq":  "world" },
      "foo" : { "gt": 10 }
    ],
    "if": [...],
    "else": [...]
  }
}
```
</comment><comment author="talevy" created="2015-11-18T01:25:06Z" id="157567318">OK, @javanna @martijnvg, let me know what you think!

here is an in-progress branch of what I've come up with so far: [link](https://github.com/talevy/elasticsearch/tree/if_conditions_else_processor/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/conditional)

a bit about what has been done:

Conditionals would be described as a separate processor within the pipeline. Branches of execution would be delegated within the `ConditionalProcessor`.

here is an example ConditionalProcessor config definition:

```
{
  "field_conditions": {
    "message" : { "neq": "hello world" },
    "age": { "gte": 13, "lt": 18 }
  },
  "match": [
    ... processor definitions to be executed in order (if all field_conditions match) ...
  ],
  "otherwise": [
    ... processor definitions to be executed in order (if any field condition in field_conditions fails to match) ...
  ]
}
```

To not create a dependency on the metadata/mustache-templating work, this Processor will start off only supporting hardcoded values provided within the config.

One _major_ limitation of what has been done so far is that there is no "OR" logic anywhere... I will keep exploring to see how I could get that in. Also, all comparisons except for `neq` and `eq` require that the field values being compared are numbers.

*\* all naming is still up in the air, just trying out other names.
</comment><comment author="martijnvg" created="2015-11-18T20:12:29Z" id="157847950">&gt; To not create a dependency on the metadata/mustache-templating work, this Processor will start off only supporting hardcoded values provided within the config.

I think this is ok for the first version of the `condition` processor, At some point we also need to evaluate where else tempting is required and then we can get back to this.

&gt; One major limitation of what has been done so far is that there is no "OR" logic anywhere... 

Maybe we should then add an extra level to the `field_comparison`? Like this:

``` json
...
"field_conditions": {
   "and" : {
       "message" : { "neq": "hello world" },
       "age": { "gte": 13, "lt": 18 }   
    },
    "or" :  {
      ...
   }
  }
...
```

If any comparisons are defined in the `and` element, then all must match and if there are comparisons defined in the `or` element then at least one must match.
</comment><comment author="talevy" created="2015-11-19T00:16:18Z" id="157906267">I realize there is a huge limitation with this format...

a check to see if a field is either all lowercase or all uppercase for example...

if we place the boolean operator outside the field map, this would not be possible since the field name is a key.

``` json
"field_conditions": {
  "or": {
    "message" : { "eq": "hello world" },
    "message" : { "eq": "HELLO WORLD" }
  }
}
```

if we placed the boolean operator within each field, it would still not work since `eq` would appear as a key twice.

``` json
"field_conditions": {
  "message" : { "or" : { "eq": "hello world", "eq": "HELLO WORLD"} }
}
```

How about this?

``` json
"field_conditions": {
  ("and"|"or") : [
    { "message" : { "neq" : "hello world" } },
    { "message" : { "neq" : "HELLO WORLD" } },
    { "age" : { "gte" : 13 } },
    { "age" : { "lt" : 18 } }
  ]
}
```

the biggest beef I have with the suggestion above is that is can look rather 
verbose when lots of relational comparisons are done against the same field...
</comment><comment author="talevy" created="2015-11-19T00:33:54Z" id="157909135">since it is starting to grow in complexity, I thought it would be a good thought experiment to see what 
a version of conditionals with nesting would look like:

``` json
"field_conditions": {
  "or" : [
      {
        "and" : [
          { "teenager" : { "eq" : true } },
          { "age" : { "gte" : 13 } },
          { "age" : { "lte" : 19 } }
        ]
      },
      {
        "and" : [
          { "teenager" : { "eq" : false } },
          { "not" : { "and" : [
                          { "age" : {"gte" : 13} },
                          { "age" : {"lte" : 19} }
                    ] }
          }
        ]
      }
  ]  
}
```

hmmm... looks like too much for me.

what do you think?
</comment><comment author="martijnvg" created="2015-11-19T06:05:07Z" id="157960978">This looks complex :) My idea is that the conditions don't support nesting. (all conditions need to be on the same level, so there is a list of 'and' conditions and a list of 'or' conditions) Nesting could be achieved via adding another `condition` processor in the `match` or `otherwise` part of the condition.
</comment><comment author="talevy" created="2015-11-19T09:01:01Z" id="157994069">so, I think I have implemented most of what you meant. I'll go ahead and make a PR for it now so that it is easier to view the branch
</comment><comment author="clintongormley" created="2015-11-20T14:40:33Z" id="158418575">How about this:

```
{
  "if": {
    "all": [
      {"message" : { "==": "Hello"}},
      { "txt": { "in": ["one", "ONE"]}},
      { "date": { "lt": "2015", "gt": "2010"}},
      { "any": [
        {"status": { "==": "featured"}},
        { "adult": { "!=": false}}
      ]}
    ]
  },
  "then": [...],
  "else": [...]
}
```

So the `if` processor takes exactly one of `all`| `any`|`none`, plus `then` and optionally `else`
</comment><comment author="clintongormley" created="2015-11-20T14:44:15Z" id="158419426">oh i mixed `==` and `lt` etc...  could settle on symbol or word operators, don't mind
</comment><comment author="talevy" created="2015-11-20T15:24:06Z" id="158431132">thanks @clintongormley!

Offline, we were possibly discussing a simpler processor that would only operate on a single field.

here is what that looks like: https://github.com/talevy/elasticsearch/blob/if_conditions_else_processor/docs/plugins/ingest.asciidoc#field-conditional-processor

&gt; ...
&gt;       { "txt": { "in": ["one", "ONE"]}}
&gt; ...

The statement above demonstrates something that is similar, but still lacking in the way this processor 
handles operands. It is quite common for people do have the following conditionals in their 
logstash configurations:

```
if ("cdn_log" in [tags]) {
   ...
}
```

Since the subject here is not a field, and instead it is a simple string value, this relationship will probably need to be supported. Since we do not yet have string templates (mustache templates) for field referencing, I think only supporting field references instead of raw string values is ok for now.

does that make sense?
</comment><comment author="uboness" created="2015-11-23T14:20:58Z" id="158945026">why are we trying to write a scripting language in json? this all looks off to me. We should probably just use _real_ scripts for conditions... otherwise, there's no end to this.
</comment><comment author="uboness" created="2015-11-27T17:18:59Z" id="160178618">another option is to have the following structure:

``` js
{
  "condition" : {
    "if" : {
      // a conditional structure
    },
    "then" : [
      // a list of processor that will run if the condition is not met
    ],
    "else" : [
      // a list of processor that will run if the condition is not met
    ]
  }
}
```

where there can be different conditional structures:

``` js
{
  "if" : {
    "script" : "_doc.key == 2"
  }
}
```

or

``` js
{
  "if" : {
    "compare_fields" : {
      { "eq" : { "field" : "key", "value" : "2"} }
    }
  }
}
```
</comment><comment author="byronvoorbach" created="2016-05-31T12:33:44Z" id="222674819">From reading this issue, I understand that there are multiple ways of implementing such a feature. Are conditionals still on the roadmap, or have you dropped this for now?
</comment><comment author="talevy" created="2016-05-31T17:53:40Z" id="222767508">@byronvoorbach. Conditionals have been pushed off the roadmap for now. The plan is to assess its use-cases after the initial release of the Ingest API.

For now, any error-related branching can be achieved via the [on_failure](https://www.elastic.co/guide/en/elasticsearch/reference/master/handling-failure-in-pipelines.html) option
</comment><comment author="eskibars" created="2016-07-20T16:56:05Z" id="234011308">I still agree with @uboness on

&gt; why are we trying to write a scripting language in json? this all looks off to me. We should probably just use real scripts for conditions... otherwise, there's no end to this.

We have a real scripting language in Elasticsearch that we've now embedded in the Ingest Node processors.  If we do this, I think all the manual `gt`, `lt`, `gte`, etc operators would create more pain than just allowing a painless script that just returns true/false
</comment><comment author="talevy" created="2016-07-20T17:33:22Z" id="234022052">@eskibars indeed, now that we have the flexibility of a real language and the boolean operators that come with it... there is no need for this.

in the future, if someone wants to make a designated `bool` processor that executes another processor if the `bool_script` evaluates to true, then that is easily done with scripts.

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Compare processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14646</link><project id="" key="" /><description>The compare processor is able to execute specific chains of processors based on how a value condition within document being processed evaluates. 

This processor can be configured with a list of conditions. Each condition needs to be configured with a path to extract the value in the document being processed from, an operation (&lt;, &lt;=, ==, &gt;, &gt;=) and an expected value. The processor has two list of processors, one list will be executed with all the conditions evaluate to `true` and one list of processors that will be executed when at least one condition evaluates to `false`.

The structure of the processor would look something like this:

``` json
{
  "description": "_description",
  "processors": [
    {
      "compare": {
        "conditions": [
            "&lt;field1&gt;": { "&lt;op1&gt;": &lt;match_value&gt; }, //and
            "&lt;field2&gt;": { "&lt;op2&gt;": &lt;match_value&gt; },
            "&lt;field3&gt;": { "&lt;op3&gt;": &lt;match_value&gt; },
            ...
        ],
        "true_processors": [...],
        "false_processors": [...]
      }
    }
  ]
}
```

(NOTE: temporary names)

The idea behind the structure of the processor is that nested of compare conditions is supported.
</description><key id="116057818">14646</key><summary>[Ingest] Compare processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-10T08:05:45Z</created><updated>2015-11-10T08:15:01Z</updated><resolved>2015-11-10T08:15:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-10T08:15:01Z" id="155354553">Superseded by: #14647
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consolidate dependencies specified in multiple places</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14645</link><project id="" key="" /><description>Some dependencies must be specified in a couple places in the build.
e.g. randomized runner is specified both in buildSrc (for the gradle
wrapper plugin), as well as in the test-framework.

This change creates buildSrc/version.properties which acts similar to
the set of shared version properties we used to have in the maven parent
pom.
</description><key id="116055073">14645</key><summary>Consolidate dependencies specified in multiple places</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T07:44:50Z</created><updated>2015-11-10T15:49:46Z</updated><resolved>2015-11-10T15:49:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-10T08:57:54Z" id="155361698">Not a gradle expert but what I see makes sense to me
</comment><comment author="rjernst" created="2015-11-10T15:33:41Z" id="155454471">I pushed a new commit adding back the removed eclipse config.
</comment><comment author="rmuir" created="2015-11-10T15:47:04Z" id="155458977">+1, this is a nice cleanup
</comment><comment author="jpountz" created="2015-11-10T15:49:07Z" id="155459522">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Allow document metadata to be modified via the mutate processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14644</link><project id="" key="" /><description>The mutate processor should be able to modify the meta data (index, type, id etc.). This should be configured in the `meta` section of the `mutate` processor. The following meta data properties should be able to be modified in the `meta` section:  `_index`, `_type`, `_id`, `_routing`, `_parent`, `_timestamp` and `_ttl`.

Mustache templates should be used in order to refer to actual field values in the documents being processed.

For example:

``` json
{
   "meta" : {
      "_index" : "mycompany-{{department}}"
   }
}
```

The index being index to would depend on the `department` field in a document being processed.

Pipeline example:

``` json
{
   "description" : "...",
   "processors" : [
      {
         "grok" : {
            "field" : "message",
            "pattern": "%{IPORHOST:destination} %{DATESTAMP:timestamp} %{DATA:message}"
         }
      },
      {
         "mutate" : {
            "meta": {
               "_index": "logs-{{destination}}",
               "_timestamp": "{{timestamp}}"
            },
            ...
         }
      }
   ]
}
```

In the above example the destination and the timestamp properties extracted by the `grok` processor are used by the `mutate` processor to determine to what index to send the log to and to attach the timestamp to the `_timestamp` meta field.

For now the plan is to add template support only to the `meta` options of the `mutate` processor, but the idea is that later almost any processor and pipeline option should be temptable.

Open question remains if all processors should be able to modify the metadata of a document. 
</description><key id="116046244">14644</key><summary>[Ingest] Allow document metadata to be modified via the mutate processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-10T07:01:28Z</created><updated>2016-01-11T17:07:57Z</updated><resolved>2016-01-11T17:07:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-11T17:07:57Z" id="170620543">This has been implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove SimpleProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14643</link><project id="" key="" /><description>SimpleProcessor was introduced as a test Processor. It is not needed now that we have Mutate, Date, Geoip, and Grok Processors merged in.

migrated most uses of SimpleProcessor to MutateProcessor
</description><key id="116016705">14643</key><summary>remove SimpleProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-10T03:15:30Z</created><updated>2015-11-10T03:18:08Z</updated><resolved>2015-11-10T03:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-10T03:17:01Z" id="155271644">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update snowball document page.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14642</link><project id="" key="" /><description>snowball project page is moved to http://snowballstem.org
</description><key id="116015149">14642</key><summary>Update snowball document page.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yokotaso</reporter><labels /><created>2015-11-10T02:59:34Z</created><updated>2015-11-10T03:00:10Z</updated><resolved>2015-11-10T03:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Dates and epoch_millis as floats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14641</link><project id="" key="" /><description>Attempting to migrate to 2.x, I've encountered a strange problem with range filters not able to match any of the mappings I try holding epoch_millis. Maybe related to #10971

Given a document with a time_stamp field in the mapping:

```
          "time_stamp": {
            "type": "date",
            "format": "epoch_millis"
          }
```

With some data:

```
station: "ASN00033312",
precip_tenths_mm: 2,
time_stamp: 1390712400000,
location: {
lat: -23.2233,
lon: 150.605
}
```

I would expect the following to match:

```
"query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "time_stamp": {
             "gte": 1390712400000,
             "format" : "epoch_millis"
                }
            }
            }
          ]
        }
      }
    }
  }
```

The above returns no document, even though there are thousands that should respond. Removing the format at query time, doesn't help. Mapping the time_stamp field as format "basic_date", as I had in 1.x, doesn't seem to return anything either.

Bug? Pebcak?
</description><key id="116012796">14641</key><summary>Dates and epoch_millis as floats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">royrusso</reporter><labels><label>:Dates</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-11-10T02:35:19Z</created><updated>2017-01-30T11:08:50Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="royrusso" created="2015-11-10T03:29:21Z" id="155273673">I seem to have tracked it down... On indexing, my epoch timestamps are float format, ie. 1390798800000.0

ES is ignoring them, perhaps because of...

```
        "index.mapping.ignore_malformed" : true,
        "index.mapping.coerce" : true
```

I would've expected ES to "coerce" the float to an int and treat it as a epoch_millis, but instead it seems to be ignoring them altogether.

Not sure if this is the expected behaviour, so feel free to open/close the issue depending.
</comment><comment author="clintongormley" created="2015-11-17T13:23:02Z" id="157369314">Hi @royrusso 

Your diagnosis seems correct.  I agree that we could coerce floats to longs for epoch_millis.
</comment><comment author="tj" created="2016-08-22T16:37:31Z" id="241472155">&#128077; this is a pretty bad bug IMO, Go's json encoder may give you floats, and JSON numerical values by definition are floats, there's only the one type. Is there a workaround?
</comment><comment author="ritsu1228" created="2016-12-23T00:46:12Z" id="268922141">I got similar situation and ES rejects timestamp as floating point values returned by json parser. `coerce` does not seem to work with `date` type either. Is there any plan to either allow floating point as `date` or add `coerce` option to `date` type?</comment><comment author="janko-m" created="2017-01-30T11:08:25Z" id="276035624">Just wanted to mention that I'm also having problems with this when upgrading from Elasticsearch 1.x to 2.x. Elasticsearch 1.x accepted floats with the `date_time` format, but now 2.x doesn't do that anymore, and I tried specifying `date_time||epoch_seconds`, but it still doesn't parse floats (where the part after decimal point represents milliseconds). So it doesn't seem like there is a way to get the old behaviour.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Geoip improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14640</link><project id="" key="" /><description>- renamed `ip_field` option to `source_field`, because it can contain a ip or hostname.
- added a new `fields` option to control what fields are added by geoip processor
- instead of by default adding all fields, only `country_code`, `city_name`, `location`, `continent_name` and `region_name` fields are added by default.
</description><key id="116011034">14640</key><summary>[Ingest] Geoip improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-10T02:20:02Z</created><updated>2015-11-10T04:51:19Z</updated><resolved>2015-11-10T04:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-10T03:41:21Z" id="155274756">@talevy I've updated the PR based on your comments.
</comment><comment author="talevy" created="2015-11-10T04:40:27Z" id="155285599">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix automatic installation of plugin for integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14639</link><project id="" key="" /><description>The esplugin gradle plugin automatically adds the pluging being built to the integTest cluster. However, there were two issues with this. First was a bug in the name, which should have been the configured
esplugin.name instead of the project name. Second, the files configuration was overcomplicated (trying to use the groovy spreader operator after delaying calls to singleFile). Instead, we can just pass the file collections (which will just be a single file at execution time).
</description><key id="116005306">14639</key><summary>Fix automatic installation of plugin for integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-10T01:24:50Z</created><updated>2015-11-10T03:57:36Z</updated><resolved>2015-11-10T03:57:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-10T03:56:11Z" id="155276630">good, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/kibana plugin --install marvel --url [url] unable to validate cert path for local https</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14638</link><project id="" key="" /><description>Hi,

Our deployment automation has an internal repo for all elasticsearch related artifacts. I'm currently receiving...

Error: self signed certificate in certificate chain
Plugin installation was unsuccessful due to error "Not a valid url."

when accessing this internal repo over https. Is there any way to configure a trust store for kibana plugin downloads?

This is similar to https://github.com/elastic/elasticsearch/issues/14373 which has been resolved however I'm not sure how to get around this one due to kibana not being a java app.
</description><key id="116003864">14638</key><summary>bin/kibana plugin --install marvel --url [url] unable to validate cert path for local https</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels /><created>2015-11-10T01:11:10Z</created><updated>2015-11-10T05:51:05Z</updated><resolved>2015-11-10T05:51:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-10T05:51:05Z" id="155302970">Could you open this in kibana github repository ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Native Scripts not being detected properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14637</link><project id="" key="" /><description>So I previously had scripted_metric working with `native` lang scripts on 1.7.x, but after upgrading to 2.x, these scripts no longer work:

```
{
    "responses": [
        {
            "error": {
                "root_cause": [
                    {
                        "type": "script_exception",
                        "reason": "Failed to compile inline script [my_native_script] using lang [native]"
                    }
                ],
                "type": "search_phase_execution_exception",
                "reason": "all shards failed",
                "phase": "query",
                "grouped": true,
                "failed_shards": [
                    {
                        "shard": 0,
                        "index": "pem:2011:42",
                        "node": "0MnrWhfZR7-citBmgkHpHw",
                        "reason": {
                            "type": "script_exception",
                            "reason": "Failed to compile inline script [my_native_script] using lang [native]",
                            "caused_by": {
                                "type": "illegal_argument_exception",
                                "reason": "Native script [my_native_script] not found"
                            }
                        }
                    }
                ],
                "caused_by": {
                    "type": "script_exception",
                    "reason": "Failed to compile inline script [my_native_script] using lang [native]",
                    "caused_by": {
                        "type": "illegal_argument_exception",
                        "reason": "Native script [my_native_script] not found"
                    }
                }
            }
        }
    ]
}
```

I haven't changed the code for the plugin except extending `Plugin` over `AbstractPlugin`:

```
package com.example.plugin;

import com.example.plugin.registry.ScriptsRegistry;
import org.elasticsearch.plugins.Plugin;
import org.elasticsearch.script.ScriptModule;

public class NativeScriptsPlugin extends Plugin {

    @Override
    public String name() {
        return "native-scripts";
    }

    @Override
    public String description() {
        return "Module containing native scripts for Elasticsearch queries";
    }

    @SuppressWarnings("unused") //called by Elasticsearch
    public void onModule(ScriptModule module) {
        // this just adds each string -&gt; class pair
        ScriptsRegistry.getRegisteredScripts().forEach(module::registerScript);
    }

}
```

Might I be doing something incorrectly?
</description><key id="115984523">14637</key><summary>Native Scripts not being detected properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitfin</reporter><labels /><created>2015-11-09T22:43:44Z</created><updated>2015-12-18T07:34:26Z</updated><resolved>2015-11-10T00:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="whitfin" created="2015-11-10T00:18:51Z" id="155241250">@rjernst gah! I was previously defining it this way:

```
settings.put("script.native.my-native-script.type", "my-script");
```

Didn't realise this was no longer available. I refactored to using `plugin.types` and all is well now.
</comment><comment author="rjernst" created="2015-11-10T00:22:34Z" id="155241786">@zackehh `plugin.types` was always meant for tests, and is already gone for 2.1.

The correct way to add a native script is with the ScriptModule, like so:

```
module.registerScript("my_native_script", MyNativeScriptFactory.class);
```
</comment><comment author="whitfin" created="2015-11-10T00:41:07Z" id="155246441">@rjernst then I'm confused. The above is what I do in production, but my tests use the embedded Elasticsearch - how do I get that to read my plugin? The only thing to work so far for my tests (on 2.0) is `plugin.types`.

```
Settings.Builder settings =
                Settings.settingsBuilder();
settings.put("node.name", "embedded");
settings.put("cluster.name", "elasticsearch-test");
settings.put("path.home", tempPath.toString());
settings.put("http.enabled", true);
settings.put("node.master", true);
settings.put("http.port", 9299);
settings.put("transport.tcp.port", 9399);
settings.put("plugin.types", NativeScriptsPlugin.class.getName());
```
</comment><comment author="rjernst" created="2015-11-10T00:45:24Z" id="155247114">@zackehh `plugin.types` is still there in 2.0. For 2.1, there is a new `nodePlugins()` method which integ tests override to add plugins for tests.
</comment><comment author="whitfin" created="2015-11-10T00:49:20Z" id="155247739">@rjernst cool, thank you - do you happen to have a docs link for that?
</comment><comment author="rjernst" created="2015-11-10T00:53:00Z" id="155248287">https://github.com/elastic/elasticsearch/blob/2.1/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java#L1718
</comment><comment author="whitfin" created="2015-11-10T00:55:46Z" id="155248760">@rjernst awesome, appreciate it! :)
</comment><comment author="whitfin" created="2015-12-10T21:01:54Z" id="163748194">@rjernst is there a way to use plugins without having to migrate all our test code to use ESIntegTestCase? 
</comment><comment author="rjernst" created="2015-12-10T21:18:26Z" id="163751998">You could start a real elasticsearch server (ie bin/elasticsearch) after having installed the plugins.
</comment><comment author="whitfin" created="2015-12-10T21:24:47Z" id="163753810">@rjernst so no easy way to get to this point from:

```
NodeBuilder.nodeBuilder().settings(settings).data(true).local(false).node();
```

^ this is what we're doing in our local env.

```
settings.put("plugin.types", NativeScriptsPlugin.class.getName());
```

Is what we used in 2.0. Is there no straightforward migration path?
</comment><comment author="rjernst" created="2015-12-18T07:34:26Z" id="165703085">@zackehh The path is to use the test framework. We spend a lot of time making this useful as we develop our own plugins, so it is the right thing to use. But I also would not even use ESIntegTestCase. Instead, I would create a rest test and use the integration test setup that the 2.0 plugin parent pom provides. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More BWC hardening</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14636</link><project id="" key="" /><description>1. Nicer output format for when nodes fail to form a cluster or when TRACE
   logging its output.
2. Don't log reproduction options that are implied by the project.
3. ExternalNode should fail the test if it wasn't fed a port range. Because
   not having a port range is very likely to cause the tests to break each
   other.
</description><key id="115984087">14636</key><summary>More BWC hardening</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-11-09T22:40:46Z</created><updated>2015-11-10T13:50:23Z</updated><resolved>2015-11-10T13:49:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-09T22:40:57Z" id="155221828">@bleskes - we talked about these.
</comment><comment author="s1monw" created="2015-11-10T08:06:46Z" id="155352732">LGTM
</comment><comment author="bleskes" created="2015-11-10T09:36:48Z" id="155369644">LGTM2 . Thanks nik
</comment><comment author="nik9000" created="2015-11-10T13:50:23Z" id="155424672">Merged to 2.1 and cherry-picked to 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reword some documentation to make it more obvious that doc values are a columnar representation of the data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14635</link><project id="" key="" /><description>Some users may already be familiar with column stores, so saying more explicitly
that doc values are a columnar representation of the data may help them better
and/or more quickly understand what doc values are about.
</description><key id="115983023">14635</key><summary>Reword some documentation to make it more obvious that doc values are a columnar representation of the data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label></labels><created>2015-11-09T22:34:24Z</created><updated>2015-11-10T08:43:05Z</updated><resolved>2015-11-10T08:43:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-09T22:42:03Z" id="155222073">LGTM
</comment><comment author="martijnvg" created="2015-11-10T07:05:56Z" id="155333263">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated the correct doc for Kafka Consumer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14634</link><project id="" key="" /><description>Updated the correct doc for Kafka Consumer with link to latest repository.

Pls review and merge. Thank You.
</description><key id="115981171">14634</key><summary>Updated the correct doc for Kafka Consumer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reachkrishnaraj</reporter><labels><label>docs</label></labels><created>2015-11-09T22:23:32Z</created><updated>2015-11-17T13:12:16Z</updated><resolved>2015-11-17T13:12:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T13:12:03Z" id="157366948">thanks @reachkrishnaraj - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timeout waiting for connection from pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14633</link><project id="" key="" /><description>We have a few Found customers met this issue, and has the following error log: 

```
Caused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Timeout waiting for connection from pool
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:473)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:297)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3672)
    at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1079)
    at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1042)
```

I suspect this is because `repository-s3` plugin doesn't close s3 objects. According to the [AWS SDK document](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3.html#getObject%28com.amazonaws.services.s3.model.GetObjectRequest%29), we should execute `object.close()` in order to liberate that connection. `S3Object` opens a connection for each object. That are not liberated even if the object is garbage collected.
</description><key id="115978651">14633</key><summary>Timeout waiting for connection from pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/xuzha/following{/other_user}', u'events_url': u'https://api.github.com/users/xuzha/events{/privacy}', u'organizations_url': u'https://api.github.com/users/xuzha/orgs', u'url': u'https://api.github.com/users/xuzha', u'gists_url': u'https://api.github.com/users/xuzha/gists{/gist_id}', u'html_url': u'https://github.com/xuzha', u'subscriptions_url': u'https://api.github.com/users/xuzha/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1799964?v=4', u'repos_url': u'https://api.github.com/users/xuzha/repos', u'received_events_url': u'https://api.github.com/users/xuzha/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/xuzha/starred{/owner}{/repo}', u'site_admin': False, u'login': u'xuzha', u'type': u'User', u'id': 1799964, u'followers_url': u'https://api.github.com/users/xuzha/followers'}</assignee><reporter username="">xuzha</reporter><labels /><created>2015-11-09T22:08:30Z</created><updated>2015-11-10T21:36:17Z</updated><resolved>2015-11-10T21:36:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-10T09:14:22Z" id="155364693">@xuzha I totally agree with your findings.
</comment><comment author="dadoonet" created="2015-11-10T09:33:21Z" id="155368982">I looked at the code but apparently there is no real way today to close the `S3Object`. Actually `BlobContainer` interface needs an `InputStream` with:

``` java
    /**
     * Creates a new InputStream for the given blob name
     */
    InputStream readBlob(String blobName) throws IOException;
```

When we use it for example in BlobStoreRepository class we call:

``` java
    /**
     * Reads snapshot index file
     * &lt;p&gt;
     * This file can be used by read-only repositories that are unable to list files in the repository
     *
     * @return list of snapshots in the repository
     * @throws IOException I/O errors
     */
    protected List&lt;SnapshotId&gt; readSnapshotList() throws IOException {
        try (InputStream blob = snapshotsBlobContainer.readBlob(SNAPSHOTS_FILE)) {
            BytesStreamOutput out = new BytesStreamOutput();
            Streams.copy(blob, out);
            ArrayList&lt;SnapshotId&gt; snapshots = new ArrayList&lt;&gt;();
            try (XContentParser parser = XContentHelper.createParser(out.bytes())) {
                if (parser.nextToken() == XContentParser.Token.START_OBJECT) {
                    if (parser.nextToken() == XContentParser.Token.FIELD_NAME) {
                        String currentFieldName = parser.currentName();
                        if ("snapshots".equals(currentFieldName)) {
                            if (parser.nextToken() == XContentParser.Token.START_ARRAY) {
                                while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                                    snapshots.add(new SnapshotId(repositoryName, parser.text()));
                                }
                            }
                        }
                    }
                }
            }
            return Collections.unmodifiableList(snapshots);
        }
    }
```

There is no way here to call `S3Object#close`. I think we should may be add a `closeInputStream()` method or something like this in `BlobContainer` interface. @imotov WDYT?
</comment><comment author="xuzha" created="2015-11-10T18:38:18Z" id="155525522">@dadoonet I took another look this morning

I think this is fine in master, in `Streams.copy(blob, out)` we do close inputstream after use. `S3object.close()` is just close the `inputstream`, so I think this issue is fine in 3.0.

But in 2.x and 2.1 `google.common.io.ByteStreams` do not close the streams. That are not liberated after `readSnapshotList()`
</comment><comment author="dadoonet" created="2015-11-10T19:00:44Z" id="155531953">Interesting. Indeed we found with @imotov today when looking at the code that calling actually `S3object.close()` is useless and won't fix the issue but we did not look at 2.x branch I guess! Awesome. Do you think you'll have time for a PR or do you want me to look at it on thursday?
</comment><comment author="xuzha" created="2015-11-10T19:03:57Z" id="155532750">Thanks @dadoonet, I could make a PR for this later today.
</comment><comment author="xuzha" created="2015-11-10T21:36:10Z" id="155574405">@dadoonet, we do close the stream in 2.x,  within the `try {}` block. 

I double checked. This should be a false alarm. I'm sorry, closing the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add javadocs to IndexingStats.Stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14632</link><project id="" key="" /><description>This commit adds some basic javadocs to IndexingStats.Stats.
A user on the forum asked for what getIndexingCurrent is so I figured
i can just fix it in the code.
</description><key id="115975346">14632</key><summary>Add javadocs to IndexingStats.Stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T21:51:44Z</created><updated>2016-01-10T17:19:16Z</updated><resolved>2016-01-05T08:37:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-09T22:14:49Z" id="155215992">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.7.X Reroutes seem really fragile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14631</link><project id="" key="" /><description>I've been trying to get 1.7.3 into production, but I've been having difficulty getting reroutes to work quite right. Originally the issue was that our custom discovery plugin did not call reroute properly on node join and leave**, but even now it appears we run into tons of issues where nodes leave and their shards are never re-assigned until either another node joins or some other membership change that triggers a reroute.

I started diving into it and noticed that 9223cebf6d removed periodic reroutes, which to me seems pretty strange. My understanding of the current design in the 1.7 branch is that when events happen that imply a reroute, the various subsystems trigger reroutes in the `RoutingService`, where the reroutes themselves are managed by one shot timer (because of shard delay) which means that if for whatever reason when that timer fires (e.g. clock skew, system load, or just a bug like #14445) the routing subsystem decides _not_ to do anything, that routing update will be lost forever until the next update.

I am confused by this architecture choice. In my mind the correct implementation continues to reroute until the cluster state is out of yellow. From a correctness point of view it seems there really should be no one shot timers, it should just be a periodic "are there unassigned shards, oh yes there are we should try to fix that".

I guess my question in this issue is: why remove periodic reroutes which are somewhat vital to the correctness of the datastore? It seems like this choice just opens up the system to failure (e.g. #12566, #14445, #14010, etc ...), and the much safer choice is to have the master call reroute until the unassigned shards are healthily allocated whenever a state change occurs that requires a reroute. I understand the desire to not always be calling reroute, but at the very least the [`doRun`](https://github.com/elastic/elasticsearch/blob/05d4530971ef0ea46d0f4fa6ee64dbc8df659682/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java#L136) method in `clusterChanged` should check that the reroute caused the cluster to transition to green and if not schedule another reroute right?

*\* Why it is the responsibility of the discovery subsystem to inform the shard routing subsystem to reroute is confusing to me as it already publishes cluster state changes which is sufficient information; rerouting may be an optimization but why is it relied on for correctness ...?
</description><key id="115936358">14631</key><summary>1.7.X Reroutes seem really fragile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jolynch</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2015-11-09T18:29:05Z</created><updated>2016-02-14T15:59:17Z</updated><resolved>2016-02-14T15:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jolynch" created="2015-11-12T02:20:55Z" id="155976556">Even with reroutes on all node join, leaves, and master changes I still run into situations where shards hang out permanently unassigned. This usually happens when a node leaves and never comes back. I have confirmed that if I manually call reroute or schedule a task on the threadpool that just calls reroute whenever there are unassigned shards, my cluster is able to recover.

I'll keep working on trying to come up with a reproduction in a test, but before I spend too much time is there a strong opposition to periodically checking that all shards are assigned and rerouting if not? 
</comment><comment author="bleskes" created="2015-11-16T19:49:21Z" id="157149424">As you said, we tried the route of periodic reroutes. It was our experience that it made the code more complex and was hiding bugs in other places - but not completely fixing them. 

Note that we typically do the re-route inlined with another change in order to save on cluster state publishing and do adapt allocation immediately.

That said I agree that the current implementation is  tricky and have proven to have bugs. @ywelsch has started working what would hopefully be a simpler approach.

&gt; I'll keep working on trying to come up with a reproduction in a test, but before I spend too much time is there a strong opposition to periodically checking that all shards are assigned and rerouting if not?

I don't think periodic reroute is coming back. It would be great if you can reproduce or give some more info as to what went wrong (assuming it's not related to your custom discovery). I would love to know what the bug exactly was so we can make sure it's fixed.
</comment><comment author="jolynch" created="2015-11-16T21:03:11Z" id="157169395">&gt; It was our experience that it made the code more complex and was hiding bugs in other places - but not completely fixing them.

Fair enough, thank you for the context!

&gt; I don't think periodic reroute is coming back. It would be great if you can reproduce or give some more info as to what went wrong (assuming it's not related to your custom discovery). I would love to know what the bug exactly was so we can make sure it's fixed.

Bummer, ok well I'll do it from our discovery plugin as a way to get moving to 1.7.3 while I find a reproduction using Zen. When we were first having issues I tried switching to Zen for a bit and still observed the behaviour so I switched back to our disco plugin to continue debugging other issues. I remember being able to fairly consistently reproduce with Zen by doing the following:
1. Start a 5 node cluster
2. Create ~10 indices with ~5 shards a piece
3. Pick a node Y, stop it and wait 60s
4. Wait X seconds past 60s for the shards to be re-assigned.

Strangely reliably, the _first_ time I would follow this procedure X would be ~=0, but if I tried stopping the same node again I would often see the shards stay unassigned. It was a while since I ran the experiment but I think that the following was a snippet from the logs at that time, maybe the throttle is a clue (I'll work on getting a cleaner repro)

```
[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v1][2], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[STARTED]] to node [lwzhmdi-SAaI8ADYj2oJYg]
[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v12][12], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] to node [lwzhmdi-SAaI8ADYj2oJYg]
[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v12][2], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[STARTED]] to node [lwzhmdi-SAaI8ADYj2oJYg]
[2015-11-06 18:05:47,755] [TRACE] [host_redacted] Assigned shard [[test_index_v4][2], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[STARTED]] to node [lwzhmdi-SAaI8ADYj2oJYg]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Assigned shard [[test_index_v12][14], node[lwzhmdi-SAaI8ADYj2oJYg], [R], s[INITIALIZING], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] to node [lwzhmdi-SAaI8ADYj2oJYg]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Start allocating unassigned shards
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Can not allocate [[test_index_v12][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [hBAmZ5DmStmQXKl_9F-TEw] due to [SameShardAllocationDecider]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Can not allocate [[test_index_v12][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [pI7Ow3A8Sc6DdNC1VMFhOg] due to [SameShardAllocationDecider]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] usage without relocations: [lwzhmdi-SAaI8ADYj2oJYg][10-40-26-46-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] usage with relocations: [0 bytes] [lwzhmdi-SAaI8ADYj2oJYg][10-40-26-46-uswest1cdevc.dev.yelpcorp.com] free: 304.9gb[99.9%]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Node [lwzhmdi-SAaI8ADYj2oJYg] has 0.010636288173571984% used disk
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] Can not allocate on node [routingNode ([10-40-26-46-uswest1cdevc.dev.yelpcorp.com][lwzhmdi-SAaI8ADYj2oJYg][10-40-26-46-uswest1cdevc][10.40.26.46], [5 assigned shards])] remove from round decision [THROTTLE]
[2015-11-06 18:05:47,756] [TRACE] [host_redacted] No eligable node found to assign shard [[test_index_v12][0], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] decision [THROTTLE]
[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [hBAmZ5DmStmQXKl_9F-TEw] due to [SameShardAllocationDecider]
[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]] on node [pI7Ow3A8Sc6DdNC1VMFhOg] due to [SameShardAllocationDecider]
[2015-11-06 18:05:47,757] [TRACE] [host_redacted] No Node found to assign shard [[test_index_v12][1], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:56:58.873Z], details[node_left[lwzhmdi-SAaI8ADYj2oJYg]]]]
[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][3], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:55:34.871Z], details[node_left[It-kO-ryT8iELRml2uaCyw]]]] on node [hBAmZ5DmStmQXKl_9F-TEw] due to [SameShardAllocationDecider]
[2015-11-06 18:05:47,757] [TRACE] [host_redacted] Can not allocate [[test_index_v12][3], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:55:34.871Z], details[node_left[It-kO-ryT8iELRml2uaCyw]]]] on node [pI7Ow3A8Sc6DdNC1VMFhOg] due to [SameShardAllocationDecider]
[2015-11-06 18:05:47,757] [TRACE] [host_redacted] No Node found to assign shard [[test_index_v12][3], node[null], [R], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-11-07T01:55:34.871Z], details[node_left[It-kO-ryT8iELRml2uaCyw]]]]
```

Testing configuration:

```
action:
  auto_create_index: false
  disable_delete_all_indices: true
bootstrap:
  mlockall: true
cluster:
  name: &lt;cluster name&gt;
discovery.zen.fd.ping_timeout: 5s
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: [&lt;list of hosts&gt;]
gateway:
  expected_nodes: 1
  recover_after_nodes: 1
  recover_after_time: 1m
http:
  compression: true
  port: &lt;port&gt;
index:
  auto_expand_replicas: false
  number_of_replicas: 2
  number_of_shards: 5
node:
  data: true
  rack: &lt;rack name&gt;
  master: true
  name: &lt;hostname&gt;
```

Since periodic re-routes are out in core I'll add them in our discovery plugin as a temporary workaround and then dig into this and see if I can reproduce it asap.
</comment><comment author="bleskes" created="2015-11-17T08:30:04Z" id="157307856">&gt; maybe the throttle is a clue 

I think this might be indeed one of the issues we discovered with the current approach. Hopefully it will be solved with the new one.

&gt; (I'll work on getting a cleaner repro)

That would be great.
</comment><comment author="clintongormley" created="2016-02-14T15:59:17Z" id="183907796">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> elasticsearch/distribution/src/main/packaging/scripts/preinst is plain wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14630</link><project id="" key="" /><description>elasticsearch-2.0.0.deb

elasticsearch/distribution/src/main/packaging/scripts/preinst is plain wrong, because (at least on debian/ubuntu systems) the ES_ENV_FILE (/etc/default/elasticsearch) is not yet in place/configured and therefore this script now modifies the system in a manner, which might not be allowed per system/org policy!!!

If you wanna try, e.g. on ubuntu:

```
apt-get purge elasticsearch
dpkg --unpack elasticsearch-2.0.0.deb
sed -r -e '/^#?ES_(USER|GROUP)=/ { s,^#,, ; s,=.*,=esearch, }' -i /etc/default/elasticsearch.dpkg-new
sed -e '/rmdir/ s,$, || true,' -i /var/lib/dpkg/info/elasticsearch.postrm       # yepp, buggy
dpkg --configure elasticsearch
```
</description><key id="115935258">14630</key><summary> elasticsearch/distribution/src/main/packaging/scripts/preinst is plain wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jelmd</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2015-11-09T18:22:18Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T17:02:23Z" id="157433869">Hi @jelmd 

Sorry to be obtuse, but could you talk me through the problem here? I'm not following.
</comment><comment author="jelmd" created="2015-11-23T04:50:58Z" id="158853711">Hi,

Have a look at http://man7.org/linux/man-pages/man1/dpkg.1.html#ACTIONS , option -i : here you can see the single steps executed, when a package gets installed. Obviously the package's preinst script gets executed BEFORE the package puts /etc/default/elasticsearch into place (this happens in step 6) aka configuration). However, in line 19 of the preinst script you are trying to source in this file and thus you created a chicken and egg problem. 

Last but not least now it is not possible anymore, to do a `dpkg --unpack e*.deb`, fix the script//etc/default/elasticsearch before it gets installed, i.e. when one runs finally the `dpkg --configure e*` ...

The 1.x packages did it right, 2.0.0 packages are plain wrong.
</comment><comment author="jordansissel" created="2016-06-09T04:42:17Z" id="224798598">@jelmd I took a glance at the file you mentioned, and it seems ok to me.

The way I read it, the env file is only loaded if it exists. 

Is the problem that the package would create a user 'elasticsearch' because the value ES_USER is unlikely to be modified by the operator at the time of `preinst` ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get jps path relative to the JVM running Gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14629</link><project id="" key="" /><description>This commit addresses an issue in getting a path to the jps bin. The
solution is to get the path to the JDK relative to the JVM running
Grade.

Closes #14614
</description><key id="115928176">14629</key><summary>Get jps path relative to the JVM running Gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T17:41:54Z</created><updated>2015-11-09T18:25:36Z</updated><resolved>2015-11-09T18:25:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-09T17:53:25Z" id="155139021">Works on my Macbook.
</comment><comment author="polyfractal" created="2015-11-09T18:00:41Z" id="155141006">Also worked on my Macbook Air.
</comment><comment author="rmuir" created="2015-11-09T18:07:17Z" id="155142580">+1 it is more consistent with elasticsearch startup logic, and its important that at least these two tasks are the same java installation.
</comment><comment author="rjernst" created="2015-11-09T18:19:13Z" id="155145449">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy scripting: java.lang.ClassFormatError: Illegal class name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14628</link><project id="" key="" /><description>Hi there,

the Groovy scripting service generates a hex sha1 digest of the script content which is used as a class name during compilation.

In version 1.7 (and probably current as well), if the digest happens to start with a digit, then scripts fail to run, throwing a ClassFormatError. I've only observed this with script files (i.e. under config/scripts).

My current workaround involves adding arbitrary comments and white space to the script until its content hash starts with a word character. An easy fix would be simply to prepend a valid character (`_` or whatever) to the `fake` class name.

Cheers,
Richard
</description><key id="115924623">14628</key><summary>Groovy scripting: java.lang.ClassFormatError: Illegal class name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">richardhundt</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-11-09T17:23:45Z</created><updated>2015-11-20T09:31:51Z</updated><resolved>2015-11-10T12:21:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-09T21:31:04Z" id="155203222">oh boy, do you wanna open a PR to fix this? I also wonder if you have a reproduction for this issue so we can add a test?
</comment><comment author="richardhundt" created="2015-11-10T06:52:06Z" id="155327315">Sure, I'll open a PR and have a test case.
</comment><comment author="s1monw" created="2015-11-10T08:05:01Z" id="155352477">thank you!
</comment><comment author="richardhundt" created="2015-11-10T08:53:59Z" id="155361019">Umm... has groovy scripting been removed from 2.x? I can see it in master, but now I'm not sure whether it'll be around in future. Which branch d'you want me to work against?
</comment><comment author="richardhundt" created="2015-11-10T08:59:01Z" id="155361897">nvm, it seems to have moved to core
</comment><comment author="s1monw" created="2015-11-10T11:06:09Z" id="155390287">&gt; nvm, it seems to have moved to core

yeah it's in `plugins/lang-groovy` now. Please make the PR against master
</comment><comment author="richardhundt" created="2015-11-10T12:21:19Z" id="155406699">I've managed to create a failing test in 1.7, and reproduced in 2.0 but not in master.

Turns out master doesn't use `GroovyClassLoader.parseClass()` anymore. There's a comment [here](https://github.com/elastic/elasticsearch/blob/master/plugins/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java#L174).

So it looks like it's fixed.

For posterity, this only fails if the script defines a method, which is probably why it hasn't bitten many people before.

For example, the following groovy snippet:

```
def answer(a) { return a + 2 }; return answer(40)
```

Hashes to `6e41335324a948a5e324e03445ea9c9f9c19e303` and then I see:

```
..., "reason":"failed to run indexed script [test] using lang [groovy]","caused_by":{"type":"class_format_error","reason":"Illegal class name \"6e41335324a948a5e324e03445ea9c9f9c19e303$answer\" in class file 6e41335324a948a5e324e03445ea9c9f9c19e303$answer"}
```

I have a test in the `messy` namespace, which you might want for regression, but not sure if it's worth polluting your code with.
</comment><comment author="vananiev" created="2015-11-11T17:23:29Z" id="155852395">We had this problem on elasticsearch-1.7.3 too. Thanks for explanation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>evaluate gradle leniency of dependency versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14627</link><project id="" key="" /><description>I was surprised to hit problems with my IDE (see https://github.com/elastic/elasticsearch/pull/14626)

Basically, when running 'gradle test', gradle sees jar hell (test framework has commons-codec:1.6 and tika has commons-codec:1.9) and it silently "upgrades" test-framework to 1.9

Of course, for the IDE, where you have linked individual projects, this trick does not work for it, and thats why jar hell was angry. And of course, plugins have isolated classloaders so only true conflicts with elasticsearch core are really an issue in practice.

But still this has me concerned, that gradle is upgrading the versions of dependencies to match each other. I see it also with junit (it changes 4.10 -&gt; 4.11). You can see it to if you inspect 'gradle dependencies' of any module.

Maybe we should turn this off and simply be explicit?
</description><key id="115918439">14627</key><summary>evaluate gradle leniency of dependency versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>discuss</label></labels><created>2015-11-09T16:55:39Z</created><updated>2015-11-11T03:33:47Z</updated><resolved>2015-11-11T03:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-09T16:58:16Z" id="155122901">At the very least, this causes real harm in that it makes IDEs more difficult to support. If this leniency was not happening, then 'gradle test' would have failed, rather than doing the automagic version upgrade.
</comment><comment author="rmuir" created="2015-11-09T17:08:39Z" id="155127484">I pinged @rjernst and he told me he wanted to disable this, just didnt have time to go thru all the deps.

I'll take a stab.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minor attachments cleanups: IDE test support and EPUB format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14626</link><project id="" key="" /><description>The documentation says we support EPUB, but the parser is not enabled.
This parser does not require any external dependencies, so I think its ok?

Separately, test-framework drags in an ancient commons-codec (via httpclient), which gradle
"upgrades", but IDEs can't handle this case and just hit jar hell. So just wire that to 1.9,
this allows running tests in the IDE for this plugin.
</description><key id="115914002">14626</key><summary>minor attachments cleanups: IDE test support and EPUB format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Mapper Attachment</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T16:33:24Z</created><updated>2015-11-17T16:56:12Z</updated><resolved>2015-11-09T17:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-09T16:39:34Z" id="155117584">LGTM
</comment><comment author="jpountz" created="2015-11-09T16:41:51Z" id="155118206">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disruption rules in MockTransportService should match all bound addresses of a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14625</link><project id="" key="" /><description>Currently, disruption rules apply only to the main bound address of a node. As consequence, integration tests that use Network Partitions must currently ensure that nodes are bound to a single local interface (e.g. 127.0.0.1 / IPv4) in order to correctly work. This is ugly as it requires setting `network.host` explicitly to `127.0.0.1` in all such tests.

Implementation details:
UnicastZenPing creates fake DiscoveryNode for each local network interface. If multiple interfaces are active, it creates one for IPv4 and one for IPv6 for each node. To add disruptions, MockTransportService provides methods that take as parameter a node to disrupt, but this only adds disruptions rules that match on the main bound address `DiscoveryNode.address()` of the node. This means that no rules exist to intercept connections trough other bound addresses.

Solution:
Disruption rules in MockTransportService should match all bound addresses of a node.

As future enhancement, the interface of MockTransportService / NetworkPartition could also be expanded so that disruptions can be specified directly on a per TransportAddress level. This provides more fine-granular testing possibilities (where connection over one interface works but the other not).
</description><key id="115911755">14625</key><summary>Disruption rules in MockTransportService should match all bound addresses of a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Network</label><label>bug</label><label>test</label></labels><created>2015-11-09T16:22:37Z</created><updated>2015-11-12T16:22:32Z</updated><resolved>2015-11-12T16:22:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-09T17:12:12Z" id="155128350">Thanks for digging @ywelsch . Makes sense.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate mapper attachements plugin to asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14624</link><project id="" key="" /><description>Followup for #14605
</description><key id="115889265">14624</key><summary>Migrate mapper attachements plugin to asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Mapper Attachment</label><label>docs</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T14:37:27Z</created><updated>2015-11-23T11:18:43Z</updated><resolved>2015-11-23T11:15:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-09T14:38:10Z" id="155080705">@clintongormley Could you give a look please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use always set java.home</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14623</link><project id="" key="" /><description>`System.getenv('JAVA_HOME')` relies on JAVA_HOME being setup by the user.
`System.getProperty('java.home')` is set by Java all the time.

Closes #14614
</description><key id="115873842">14623</key><summary>Use always set java.home</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label></labels><created>2015-11-09T13:14:11Z</created><updated>2015-11-09T20:57:30Z</updated><resolved>2015-11-09T13:15:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-09T13:15:02Z" id="155058402">We use `System.getProperty('java.home')` in a few places because it seems like Java always sets this.
</comment><comment author="jpountz" created="2015-11-09T13:15:30Z" id="155058469">LGTM
</comment><comment author="jasontedor" created="2015-11-09T14:07:59Z" id="155071434">Usually using `java.home` instead of `JAVA_HOME` is okay. However, in this case, it's being used to get a handle to `jps` which comes with the JDK and not the JRE. But `java.home` always get you a path to the JRE, not the JDK. I think we need to do something a little bit different here.
</comment><comment author="nik9000" created="2015-11-09T14:10:15Z" id="155072733">++ This was wrong. It got us past the "can't even run tasks" step and I didn't test it carefully enough.
</comment><comment author="jasontedor" created="2015-11-09T19:41:28Z" id="155169081">Since this was reverted, I removed the version label so that it doesn't get pulled into the release scripts. See #14629 for a different solution.
</comment><comment author="nik9000" created="2015-11-09T19:53:23Z" id="155172121">++ thanks.
</comment><comment author="rmuir" created="2015-11-09T20:57:30Z" id="155190842">&gt; Usually using java.home instead of JAVA_HOME is okay. However, in this case, it's being used to get a handle to jps which comes with the JDK and not the JRE. But java.home always get you a path to the JRE, not the JDK. I think we need to do something a little bit different here.

I think we should require JAVA_HOME and add some hack for intellij based on a sysprop it sets.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch configuration(MAX_LOCKED_MEMORY) is not working properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14622</link><project id="" key="" /><description>## Problem

I have problem with `MAX_LOCKED_MEMORY` property. I set the `MAX_LOCKED_MEMORY` property unlimited in the ES configuration file where under the `/etc/init.d`. [ES document](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html) says so. Anyway, after that I execute the following command to see the values for ES user

``` bash
su elasticsearch --shell /bin/bash --command "ulimit -a"
```

then I got this result

``` bash
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 31829
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 65535
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 31829
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
```

as you can see, `max locked memory` is default.
## Approach

After that I added the following line into `/etc/security/limits.conf` 

``` bash
elasticsearch    soft    memlock        unlimited
elasticsearch    hard    memlock        unlimited
```

Then I got this result;

``` bash
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 31828
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) unlimited
open files                      (-n) 65535
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 31828
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
```

As you can see, `max locked memory` is now set the `unlimited`.
</description><key id="115867729">14622</key><summary>Elasticsearch configuration(MAX_LOCKED_MEMORY) is not working properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ziyasal</reporter><labels /><created>2015-11-09T12:28:56Z</created><updated>2015-11-09T13:42:54Z</updated><resolved>2015-11-09T13:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T13:42:54Z" id="155066122">@ziyasal the config file is only called for bin/plugin or when running the init scripts. It doesn't change the values for the default profile of the elasticsearch user.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Time zone issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14621</link><project id="" key="" /><description>Timezone shouldn't be part of date time or date for auto detection of date
</description><key id="115860510">14621</key><summary>Time zone issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">priyadarsh</reporter><labels /><created>2015-11-09T11:42:27Z</created><updated>2015-11-17T13:35:06Z</updated><resolved>2015-11-17T13:35:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="priyadarsh" created="2015-11-09T12:10:37Z" id="155047453">Signed the aggreement
</comment><comment author="jpountz" created="2015-11-09T12:23:32Z" id="155050188">Sorry but I'm not sure to understand the motivation of this change? The timezone part is indeed optional and if you don't provide it then elasticsearch will assume UTC. But if you do provide it then elasticsearch will still auto-detect dates and make use of the time-zone parameter to parse the date correctly.
</comment><comment author="priyadarsh" created="2015-11-12T05:11:45Z" id="156000683">Sorry for not being clear. Auto detection of date string was not working in 2.0 when timezone is passed. So I assumed that timezone is not required. I should have raised an issue. Please close this request if the timezone is optional and I will raise a separate issue.
</comment><comment author="jpountz" created="2015-11-12T10:56:00Z" id="156072367">&gt; Auto detection of date string was not working in 2.0 when timezone is passed.

Can you give an example of document that fails auto-detection of a date field so that we can look into the problem?
</comment><comment author="clintongormley" created="2015-11-17T13:35:06Z" id="157371919">@priyadarsh Auto-detection of dates with time zones works correctly:

```
DELETE test 

PUT test/test/1
{
  "date": "2010-01-01T12:00:00+02:00"
}

GET test/_mapping
```

return:

```
{
  "test": {
    "mappings": {
      "test": {
        "properties": {
          "date": {
            "type": "date",
            "format": "strict_date_optional_time||epoch_millis"
          }
        }
      }
    }
  }
}
```

Note: The time zone must use two digits for the hour, ie `+02:00`, but not `+2:00`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkProcessor should have OOB support for handling EsRejectedExecutionException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14620</link><project id="" key="" /><description>The [BulkProcessor](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java) is handy wrapper around the bulk API. It aggregates incoming requests and sends them in one bulk once enough requests have been accumulated, or some time have passed. At the moment it passes any failures to the user via a listener. 

It would be great if have a default listener implementation that deals with EsRejectedExecutionException correctly - I.e., backs off for a while and retries.
</description><key id="115852085">14620</key><summary>BulkProcessor should have OOB support for handling EsRejectedExecutionException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Bulk</label><label>:Java API</label><label>enhancement</label></labels><created>2015-11-09T10:53:10Z</created><updated>2016-02-19T04:53:22Z</updated><resolved>2015-12-17T13:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-09T10:54:13Z" id="155027641">+1
</comment><comment author="jpountz" created="2015-11-09T10:54:14Z" id="155027644">+1
</comment><comment author="ESamir" created="2015-11-09T10:54:33Z" id="155027696">+1
</comment><comment author="s1monw" created="2015-11-09T11:01:55Z" id="155029022">+1 
</comment><comment author="danielmitterdorfer" created="2015-11-09T13:12:25Z" id="155057937">I'd like to tackle this. However, I have a couple of questions:

I have not understood why a listener is needed. Would it also be ok if we made `BulkProcessor` itself capable of backing off and retrying? I think this could simplify usage.

Btw, I have also two questions about this part of `BulkProcessor#execute()`:

```
} catch (InterruptedException e) {
    Thread.interrupted();
    listener.afterBulk(executionId, bulkRequest, e);
}
```

I think instead of `Thread.interrupted()` it should be `Thread.currentThread().interrupt()`. Also does it make sense to notify the listener of interruption (might make sense but the listener needs to be aware of this)?
</comment><comment author="danielmitterdorfer" created="2015-11-18T12:17:24Z" id="157693398">Just as a quick update. I've extracted everything related to the handling of interrupts to a separate issue (see #14798).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.4.0-snapshot-1712973.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14619</link><project id="" key="" /><description /><key id="115848979">14619</key><summary>Upgrade to lucene-5.4.0-snapshot-1712973.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>review</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T10:32:03Z</created><updated>2015-11-09T21:38:45Z</updated><resolved>2015-11-09T21:38:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-09T14:25:24Z" id="155077789">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Many Unclear Warnings after upgrade to v1.7.3: "no index mapper found for field"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14618</link><project id="" key="" /><description>After upgrading to 1.7.3, I see many warning in the log file:

[2015-11-09 10:15:33,919][WARN ][index.codec              ] [data_xxx] [index-name] no index mapper found for field: [field-name] returning default doc values format

[2015-11-09 10:15:31,798][WARN ][index.codec              ] [data_xxx] [index-name] no index mapper found for field: [field-name] returning default postings format

I've checked the github, forums, ... and couldn't find the root cause and how can this be resolved.

Please advice, let me know if you like me to submit more info. Thanks
</description><key id="115848791">14618</key><summary>Many Unclear Warnings after upgrade to v1.7.3: "no index mapper found for field"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">segalziv</reporter><labels /><created>2015-11-09T10:30:50Z</created><updated>2015-11-09T15:18:00Z</updated><resolved>2015-11-09T13:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T13:17:32Z" id="155058788">Hi @segalziv 

It looks like you upgraded from an old version and are running into this change: https://github.com/elastic/elasticsearch/pull/7604

You will need to reindex into a new index without those custom mappings
</comment><comment author="segalziv" created="2015-11-09T15:18:00Z" id="155093054">@clintongormley Thanks for answering fast!

The index is a daily (logs) index, which was created in 1.7.3. to begin with. So I can't understand how reindexing will help.

I have to add that there were some mapping race condition warnings on this index, which feels related:

[2015-11-09 09:41:23,932][WARN ][action.bulk              ] [data_xxx] failed to perform indices:data/write/bulk[s] on remote replica [data_xxx][A_uuXlcwTnKmQs2VCJLuOQ][ip-0-0-0-0][inet[/0.0.0.0:9300]]{max_local_storage_nodes=1, zone=us-east-1a, master=false}[index-name][8]
org.elasticsearch.transport.RemoteTransportException: [data_xxx][inet[/0.0.0.0:9300]][indices:data/write/bulk[s][r]]
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse [field-name]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:411)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:487)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:466)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:566)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:250)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:229)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: unknown property [referrer]
    at org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateFieldForString(StringFieldMapper.java:331)
    at org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateField(StringFieldMapper.java:277)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:401)
    ... 15 more 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add search processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14617</link><project id="" key="" /><description>The search processor should be able to run any search request to the local or external cluster (using the new http client) and enrich document with the returned search response.
</description><key id="115845649">14617</key><summary>[Ingest] Add search processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-09T10:10:46Z</created><updated>2016-09-07T16:48:56Z</updated><resolved>2016-09-07T16:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-08-18T08:06:33Z" id="240652907">Changed the scope of this feature to run searches instead of just focussing on the percolator. (which is still possible now percolator is part of the search api)
</comment><comment author="dadoonet" created="2016-09-07T16:48:56Z" id="245344675">Closing. See discussion on #20340.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Dealing with ingest load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14616</link><project id="" key="" /><description>Pipelines will have different performance characteristic depending on the number of processors, what processors have been configured and the type of documents are being enriched (a log or a complex document with many fields).

A pipeline should be able to use a different thread pool than the default ingest thread pool. Also it should be possible to run pipelines on specific nodes (or _not_ on specific nodes) based on node metadata.
</description><key id="115844542">14616</key><summary>[Ingest] Dealing with ingest load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-09T10:06:20Z</created><updated>2016-02-14T15:57:28Z</updated><resolved>2016-02-14T15:57:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T15:57:28Z" id="183907678">Ingest processors now run on the index or bulk threads.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch index unix timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14615</link><project id="" key="" /><description>Hi team~
My data  containing a timestamp field whose value is an integer representing the number of seconds since epoch (php unix timestamp),like 1440494956
I've been reading ES docs and have found this:

http://www.elasticsearch.org/guide/reference/mapping/date-format.html

But it seems that if I want to submit unix timestamp and want them stored in a 'date' field (integer field is not useful for me) I have only two options:

Implement my own date format
Convert to a supported format at the sender

Is there any other option I missed?

Thanks!
</description><key id="115842231">14615</key><summary>ElasticSearch index unix timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RicherdLee</reporter><labels /><created>2015-11-09T09:52:53Z</created><updated>2015-11-10T01:54:46Z</updated><resolved>2015-11-09T10:24:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-09T10:24:19Z" id="155022027">Please post user questions on discuss.elastic.co. We can better help you there.
</comment><comment author="clintongormley" created="2015-11-09T13:12:47Z" id="155058024">In 2.0, you can use `epoch_second`.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_mapping_changes.html#_date_fields_and_unix_timestamps
</comment><comment author="RicherdLee" created="2015-11-10T01:54:26Z" id="155258066">@dadoonet ,thanks ,I will post my questions on discuss.elastic.co next time.
</comment><comment author="RicherdLee" created="2015-11-10T01:54:45Z" id="155258111">@clintongormley ,thanks for your help,it works for me
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle requires JAVA_HOME to be set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14614</link><project id="" key="" /><description>I get the following error when running the build:

```
* What went wrong:
A problem occurred configuring project ':distribution:tar'.
&gt; Ambiguous method overloading for method java.io.File#&lt;init&gt;.
  Cannot resolve which method to invoke for [null, class java.lang.String] due to overlapping prototypes between:
    [class java.lang.String, class java.lang.String]
    [class java.io.File, class java.lang.String]
```

which is due to the following problem:

```
Caused by: groovy.lang.GroovyRuntimeException: Ambiguous method overloading for method java.io.File#&lt;init&gt;.
Cannot resolve which method to invoke for [null, class java.lang.String] due to overlapping prototypes between:
    [class java.lang.String, class java.lang.String]
    [class java.io.File, class java.lang.String]
    at org.elasticsearch.gradle.test.ClusterFormationTasks$_configureCheckPreviousTask_closure16.doCall(ClusterFormationTasks.groovy:282)
    at org.gradle.api.internal.ClosureBackedAction.execute(ClosureBackedAction.java:67)
    at org.gradle.util.ConfigureUtil.configure(ConfigureUtil.java:130)
    at org.gradle.util.ConfigureUtil.configure(ConfigureUtil.java:110)
    at org.gradle.api.internal.AbstractTask.configure(AbstractTask.java:488)
    at org.gradle.api.internal.tasks.DefaultTaskContainer.create(DefaultTaskContainer.java:93)
    at org.gradle.api.tasks.TaskContainer$create$3.call(Unknown Source)
    at org.elasticsearch.gradle.test.ClusterFormationTasks.configureCheckPreviousTask(ClusterFormationTasks.groovy:278)
    at org.elasticsearch.gradle.test.ClusterFormationTasks.configureTasks(ClusterFormationTasks.groovy:94)
    at org.elasticsearch.gradle.test.ClusterFormationTasks$configureTasks$1.callStatic(Unknown Source)
    at org.elasticsearch.gradle.test.ClusterFormationTasks.setup(ClusterFormationTasks.groovy:49)
    at org.elasticsearch.gradle.test.ClusterFormationTasks$setup.call(Unknown Source)
    at org.elasticsearch.gradle.test.RestIntegTestTask$_closure1.doCall(RestIntegTestTask.groovy:74)
    at org.gradle.listener.ClosureBackedMethodInvocationDispatch.dispatch(ClosureBackedMethodInvocationDispatch.java:40)
    at org.gradle.listener.ClosureBackedMethodInvocationDispatch.dispatch(ClosureBackedMethodInvocationDispatch.java:25)
    at org.gradle.internal.event.AbstractBroadcastDispatch.dispatch(AbstractBroadcastDispatch.java:44)
    at org.gradle.internal.event.BroadcastDispatch.dispatch(BroadcastDispatch.java:79)
    at org.gradle.internal.event.BroadcastDispatch.dispatch(BroadcastDispatch.java:30)
    at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
    at com.sun.proxy.$Proxy12.afterEvaluate(Unknown Source)
    at org.gradle.configuration.project.LifecycleProjectEvaluator.notifyAfterEvaluate(LifecycleProjectEvaluator.java:67)
    ... 41 more
```

And indeed at ClusterFormationTasks.groovy:282 we have the following

``` groovy
            commandLine new File(System.getenv('JAVA_HOME'), 'bin/jps'), '-l'
```

So if JAVA_HOME is not set then Groovy won't know whether to use the File(File, String) or the File(String, String) constructor. Could we just expect jps to be on the PATH or at least detect JAVA_HOME automatically when it is not set?
</description><key id="115832392">14614</key><summary>Gradle requires JAVA_HOME to be set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>build</label></labels><created>2015-11-09T08:48:44Z</created><updated>2015-11-09T18:25:34Z</updated><resolved>2015-11-09T18:25:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-09T13:03:01Z" id="155056264">I fixed this once before. I can do this again!
</comment><comment author="jpountz" created="2015-11-09T13:07:27Z" id="155056974">That would be great. Thanks @nik9000 !
</comment><comment author="rjernst" created="2015-11-09T15:43:44Z" id="155100117">I have a fix for this in a branch, the problem is intellij. I had this check a long time ago, but removed because it caused problems for intellij users (but not for me because I have java home set for intellij). My current fix simply ignores the check when intellij runs gradle, but I'm apprehensive about that. Maybe others have an opinion?
</comment><comment author="nik9000" created="2015-11-09T15:48:46Z" id="155101464">&gt; Maybe others have an opinion?

Can you make IntelliJ blow up loudly and _tell_ the user how to set JAVA_HOME?
</comment><comment author="rjernst" created="2015-11-09T15:53:58Z" id="155102956">Yes and no. Certainly the error can be customized for intellij. But setting is complicated and I'm not even sure myself the exact steps to go through (I did it many months ago for another reason).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function score gauss min score </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14613</link><project id="" key="" /><description>we have use es in many case,and gauss function is our love,but gauss function don't have the min score,if the score_mode is multiply(default) ,one score is zero,and the doc's score will be zero
</description><key id="115828557">14613</key><summary>function score gauss min score </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whybangbang</reporter><labels><label>:Query DSL</label><label>feedback_needed</label></labels><created>2015-11-09T08:15:39Z</created><updated>2016-05-24T10:31:18Z</updated><resolved>2016-05-24T10:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T13:04:15Z" id="155056466">@whybangbang I'm afraid I don't understand what you mean.  Perhaps an example will help? `min_score` is not per function, it is per function-score query
</comment><comment author="whybangbang" created="2015-11-17T03:59:50Z" id="157261924">@clintongormley 
we don't know why time gauss function'score is 0 sometimes, 
for example(the example enlarge the problem )

```
{
  "gauss": {
    "updated_at": {
      "origin": 2440060781,
      "scale": "172800"
    }
  }
}
```

explain is 

```
{
  "value": 0,
  "description": "Function for field updated_at:",
  "details": [
    {
      "value": 0,
      "description": "exp(-0.5*pow(MIN[Math.max(Math.abs(1.44621567E9(=doc value) - 2.440060781E9(=origin))) - 0.0(=offset), 0)],2.0)/2.1539321544868954E10)"
    }
  ]
}
```

gauss'score == 0 and this document ' score = 0
</comment><comment author="shenzhun" created="2015-11-17T06:49:16Z" id="157288523">@clintongormley I guess @whybangbang means that does gauss function of ES have default value function, when the output score is 0, then default value could be used instead of 0.
</comment><comment author="clintongormley" created="2016-02-14T15:56:46Z" id="183907651">@brwe can you shed any light on this?
</comment><comment author="brwe" created="2016-02-14T17:31:29Z" id="183933749">The document is simply too far away from origin and hence the number is too small to fit into a float. That is why it is 0.
If I understand correctly you do not want the documents to be scored with 0 in that case? 
In that case you have to apply an additional filter to the function that only scores documents that are not too far away and another one that gives documents outside this range a "default" value. In you example this would mean something like:

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "filter": {
            "range": {
              "date": {
                "lt": 2440579181,
                "gt": 2439542381
              }
            }
          },
          "gauss": {
            "date": {
              "origin": 2440060781,
              "scale": 172800
            }
          }
        },
        {
          "filter": {
            "range": {
              "date": {
                "gte": 2440579181
              }
            }
          },
          "weight": 0.1
        },
        {
          "filter": {
            "range": {
              "date": {
                "lte": 2439542381
              }
            }
          },
          "weight": 0.1
        }
      ]
    }
  }
}
```

Let me know if that is what you meant.
</comment><comment author="clintongormley" created="2016-05-24T10:31:18Z" id="221229835">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Typo on the tribe node description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14612</link><project id="" key="" /><description /><key id="115801614">14612</key><summary>[DOCS] Typo on the tribe node description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arinto</reporter><labels><label>docs</label></labels><created>2015-11-09T05:56:52Z</created><updated>2015-11-09T13:01:09Z</updated><resolved>2015-11-09T13:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T13:01:09Z" id="155055990">thanks @arinto - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContentFactory.xContentType: allow for possible UTF-8 BOM for JSON XContentType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14611</link><project id="" key="" /><description>Fixes #14442.
</description><key id="115787612">14611</key><summary>XContentFactory.xContentType: allow for possible UTF-8 BOM for JSON XContentType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">camilojd</reporter><labels><label>:REST</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T04:06:50Z</created><updated>2017-01-20T14:04:49Z</updated><resolved>2015-11-09T11:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-09T10:41:16Z" id="155025313">This looks good to me. I will leave this PR open for some time to give a chance to other people to chime in, but if there are no objections I will merge this soon.
</comment><comment author="danielmitterdorfer" created="2015-11-09T11:08:16Z" id="155030126">Thanks @camilojd for the PR. It looks good to me too. I'll take care of merging and backporting.
</comment><comment author="danielmitterdorfer" created="2015-11-09T11:12:01Z" id="155030711">Merged to master, 2.x and 2.1
</comment><comment author="jpountz" created="2015-11-09T11:27:23Z" id="155033404">Should it go to 2.0.1 as well?
</comment><comment author="bleskes" created="2015-11-09T11:30:10Z" id="155034441">+1 on 2.0.1 . This can be very tricky to debug if you don't know what's going on (running hexdump is always an indication of fun).
</comment><comment author="danielmitterdorfer" created="2015-11-09T12:44:37Z" id="155053462">@jpountz, @bleskes: Sure, no objections. I was just under the impression that we backport by default only down to the 2.1 branch. I'll merge the change to 2.0 now.
</comment><comment author="jpountz" created="2015-11-09T12:50:45Z" id="155054454">@danielmitterdorfer This impression is right: we tend to not backport to bugfix branches unless:
- the bug can't be worked around easily
- the fix is low-risk

This change seemed eligible to me, hence my proposal, but in general it's good practice to be very careful about backporting changes to bugfix releases: these are releases that users should be able to run almost blindly, so we can't take any risks to introduce new bugs to these branches.
</comment><comment author="danielmitterdorfer" created="2015-11-09T12:51:35Z" id="155054563">@jpountz Thanks for the clarification. I'll keep that in mind.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update snowball document page.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14610</link><project id="" key="" /><description>snowball project page is moved to http://snowballstem.org/
</description><key id="115779652">14610</key><summary>Update snowball document page.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yokotaso</reporter><labels><label>docs</label></labels><created>2015-11-09T02:17:37Z</created><updated>2015-11-17T13:17:49Z</updated><resolved>2015-11-17T13:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T12:59:02Z" id="155055660">Hi @yokotaso 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="yokotaso" created="2015-11-10T03:01:47Z" id="155268911">@clintongormley 
Thank you for your information. And I'm sorry for your inconvenience.
I have signed CLA. Cloud you check #14610 ?
Thank you!
</comment><comment author="clintongormley" created="2015-11-17T13:17:49Z" id="157368445">thanks @yokotaso - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixup issues with 32-bit jvm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14609</link><project id="" key="" /><description>If you run tests under a 32-bit jvm, you will get a test failure in IndexStoreTests,
the logic there is wrong in the case of 32-bit (its NIOFSDirectory on linux).

Also if mlockall fails, you'll see huge bogus values (because of use of `long` instead of `NativeLong`)

finally add seccomp support for 32 bit too, and clean up all its `long` usage as well.
</description><key id="115779222">14609</key><summary>fixup issues with 32-bit jvm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-09T02:11:10Z</created><updated>2015-11-09T12:59:29Z</updated><resolved>2015-11-09T02:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-09T02:15:09Z" id="154903236">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update all-field.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14608</link><project id="" key="" /><description>Document the delimiter used for the _all field
</description><key id="115751424">14608</key><summary>Update all-field.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>docs</label></labels><created>2015-11-08T18:44:24Z</created><updated>2015-11-09T12:42:28Z</updated><resolved>2015-11-09T12:42:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Document wildcard for network interfaces in Network Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14607</link><project id="" key="" /><description>_From @inqueue on October 30, 2015 20:42_

Can we add documentation to [Network Settings](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html) that illustrates how to configure binding Elasticsearch to all network interfaces; i.e., using a wildcard?

elasticsearch.yml:

```
network.bind_host: 0
```

All interfaces are listening:

```
 $ netstat -p tcp -na | egrep '9200|9300'
tcp46      0      0  *.9200                 *.*                    LISTEN
tcp46      0      0  *.9300                 *.*                    LISTEN
```

We do not want to necessarily encourage this configuration, however, it would be very helpful to document how to do it.

As a side note, configuring `network.bind_host: 0.0.0.0` or `0.0.0` or `0.0` also works which is just unexpected. It seems just documenting a lone `0` value would suffice.

_Copied from original issue: elastic/docs#48_
</description><key id="115733477">14607</key><summary>Document wildcard for network interfaces in Network Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Network</label><label>discuss</label><label>docs</label></labels><created>2015-11-08T13:10:37Z</created><updated>2017-02-01T11:46:59Z</updated><resolved>2016-02-14T15:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T13:10:38Z" id="154825827">_From @inqueue on October 30, 2015 21:20_

I should note this configuration is not any different than versions prior to 2.x. The difference is Elasticsearch no longer binds to all interfaces by default which can be very unexpected if going to 2.0 from an upgrade for example. See [Breaking Changes in 2.0](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_network_changes.html) for more details on the change.
</comment><comment author="clintongormley" created="2015-11-08T13:10:38Z" id="154825828">_From @jprante on October 31, 2015 13:56_

A note regarding IPv6 address syntax would be helpful, too. 

To bind to all IPv6/IPv4 addresses, you can use

```
network.bind_host: "0"
network.bind_host: "::"
```

To bind to IPv4 loopback (localhost) only, you can use

```
network.bind_host: "0.0.0.0"
```

together with `JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses"`

To bind to IPv6 loopback (localhost) only, you can use

```
network.bind_host: "::1"
```
</comment><comment author="clintongormley" created="2015-11-08T13:10:39Z" id="154825829">_From @inqueue on October 31, 2015 18:46_

Thank you @jprante for your contributions. 

&gt; To bind to IPv4 loopback (localhost) only, you can use
&gt; 
&gt; `network.bind_host: "0.0.0.0"`

In my observations, this binds to all IPv6/IPv4 just like `network.bind_host: 0`:

```
bash-3.2$ uname -a
Darwin peanut.local 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64

bash-3.2$ grep bind_host elasticsearch.yml
network.bind_host: "0.0.0.0"

bash-3.2$ netstat -p tcp -tna | egrep "(9200|9300).*LISTEN"
tcp46      0      0  *.9200                 *.*                    LISTEN
tcp46      0      0  *.9300                 *.*                    LISTEN

bash-3.2$ curl -I http://192.168.5.120:9200
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

bash-3.2$ curl -I http://[fe80::a299:9bff:fe11:5f69%en0]:9200
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

bash-3.2$ curl -I http://127.0.0.1:9200
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

bash-3.2$ curl -I http://[::1]:9200
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```
</comment><comment author="clintongormley" created="2015-11-08T13:10:39Z" id="154825830">_From @jprante on October 31, 2015 19:25_

You must set `JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses"` to ES start script to disable IPv6, then `network.bind_host: "0.0.0.0"` is IPv4 only (and all IPv6 networking will give `java.net.SocketException: Protocol family unavailable`)
</comment><comment author="clintongormley" created="2015-11-08T13:10:39Z" id="154825831">_From @inqueue on October 31, 2015 19:29_

Ah, just as you mentioned in your previous comment @jprante. Thanks!
</comment><comment author="clintongormley" created="2015-11-08T13:10:40Z" id="154825832">_From @markwalkom on November 2, 2015 5:59_

+1 on this, be great to provide more clarification for users.
</comment><comment author="clintongormley" created="2015-11-08T13:10:40Z" id="154825833">Moving this issue to the elasticsearch repo
</comment><comment author="rmuir" created="2015-11-08T13:19:36Z" id="154826174">&gt; You must set JAVA_OPTS="$JAVA_OPTS -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses" to ES start script to disable IPv6, then network.bind_host: "0.0.0.0" is IPv4 only (and all IPv6 networking will give java.net.SocketException: Protocol family unavailable)

I don't think we should encourage this: then things like 'localhost' won't work as the user expects, and thats probably why they want to do this anyway.

This stuff has more to do with mapped addresses, and not those settings, there is only wildcard, and there is only disabling ipv6 completely, so i would let it be. The networking documentation is confusing enough already.
</comment><comment author="jprante" created="2015-11-08T23:07:17Z" id="154885301">@rmuir My concern is that users use wildcards and forget about IPv6. localhost works fine if there is a setting in `/etc/hosts/` like

```
127.0.0.1    localhost
::1          localhost
```

and most OS provide these defaults. A hint would be helpful so users can correctly restrict ES to bind to localhost, whether they want IPv4, IPv6, or both.
</comment><comment author="rmuir" created="2015-11-08T23:35:30Z" id="154886747">They can forget about ipv6 all they want: out of box, binding to 0.0.0.0, ::, or even ::FFFF:0.0.0.0 are all equivalent: they all behave the same and bind to "wildcard" (means both v4 and v6) without any confusing -D's.
</comment><comment author="wyardley" created="2015-11-12T23:32:50Z" id="156270707">Agree with this. Not only is '0' not documented at: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html
that page suggests:
"Currently an elasticsearch node may be bound to multiple addresses, but only publishes one."

However, if I try, e.g.,:

``` network.host:
  - "_local:ipv4_"
  - "_non_loopback:ipv4_"
```

(or similar for network.bind_host)
I still get:

`[2015-11-12 15:28:30,435][INFO ][org.elasticsearch.http.netty] [hostname] Bound http to address {127.0.0.1:9200}
[2015-11-12 15:28:30,436][INFO ][org.elasticsearch.http   ] [hostname] bound_address {127.0.0.1:9200}, publish_address {127.0.0.1:9200}`
</comment><comment author="rmuir" created="2015-11-13T00:02:31Z" id="156275653">I think the wording there in the docs is just bad. While internally elasticsearch works with multiple addresses in 2.0 (because a hostname can resolve to N, because interfaces can have N, to work well with dual stack environments, etc), you won't be able to pass an array or comma-separated list of addresses until 2.2 (https://github.com/elastic/elasticsearch/pull/13954).

But if you have a hostname in DNS or /etc/hosts and it resolves to multiple A/AAAA addresses, we will bind to all of them.
</comment><comment author="wyardley" created="2015-11-13T00:16:53Z" id="156277811">Yes, but isn't bad wording in the docs exactly what this ticket is about?

Given that the default behavior has changed in 2.0, though, the bad wording in the docs is significant, and I think it makes sense to explicitly document how to restore the expected behavior (and clarify that specifying multiple values, whether explicit addresses or "special" aliases, is not yet supported in 2.0).
</comment><comment author="rmuir" created="2015-11-13T00:28:04Z" id="156279555">I'm don't think its just bad wording in the docs. The configuration API itself is not good, stuff like `_non_loopback` is bad too (and that is removed in 3.0), because its totally arbitrary and depends on the order of interfaces.

Its also confusing that the documentation immediately jumps into expert stuff like `bind` vs `publish` which 99% of people should never care about unless they have a special proxy or static NAT configuration happening, that stuff just confuses everyone completely.

To me all that is important, is that the user makes an active decision to change `network.host` before exposing themselves to the world. I think the documentation should center around that, and all the other parameters and stuff should be expert, but yeah, it needs to be rewritten completely IMO.
</comment><comment author="shashidharrao" created="2015-11-29T19:59:44Z" id="160462235">I am still getting the error after changing the recommended settings. I am facing this issue Release 2.1.0 elastic search. Please help. 

[2015-11-29 11:55:55,866][WARN ][transport.netty          ] [Node1] exception caught on transport layer [[id: 0xfa429912]], closing connection
java.net.SocketException: Protocol family unavailable
        at sun.nio.ch.Net.connect0(Native Method)
        at sun.nio.ch.Net.connect(Net.java:435)
        at sun.nio.ch.Net.connect(Net.java:427)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:643)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
        at org.jboss.netty.channel.Channels.connect(Channels.java:634)
        at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:216)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:913)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:880)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:852)
        at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:250)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:395)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-11-29 11:55:57,361][WARN ][transport.netty          ] [Node1] exception caught on transport layer [[id: 0xe44a1084]], closing connection
java.net.SocketException: Protocol family unavailable
        at sun.nio.ch.Net.connect0(Native Method)
        at sun.nio.ch.Net.connect(Net.java:435)
        at sun.nio.ch.Net.connect(Net.java:427)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:643)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
        at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
        at org.jboss.netty.channel.Channels.connect(Channels.java:634)
        at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:216)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
        at org.jboss.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:913)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:880)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:852)
        at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:250)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:395)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</comment><comment author="markwalkom" created="2015-11-29T20:15:56Z" id="160463561">@shashidharrao please join us at https://discuss.elastic.co/ or at #elasticsearch on Freenode for troubleshooting help or general questions. 
We reserve Github for confirmed bugs and feature requests :)
</comment><comment author="schlitzered" created="2015-12-17T11:18:04Z" id="165426735">+1, also also like to know how to bind to multiple, or all addresses with elasticsearch 2.1

setting network.bind_host: "0.0.0.0" is not working,

even worse, it binds to ipv6.

tcp6       0      0 :::9320                 :::\*                    LISTEN      682/java  
tcp6       0      0 :::9200                 :::\*                    LISTEN      680/java  
tcp6       0      0 :::9300                 :::\*                    LISTEN      680/java  
tcp6       0      0 :::9210                 :::\*                    LISTEN      684/java  
tcp6       0      0 :::9310                 :::\*                    LISTEN      684/java  
tcp6       0      0 :::9220                 :::\*                    LISTEN      682/java

this is very unexpected :-/
</comment><comment author="clintongormley" created="2016-02-14T15:53:08Z" id="183907443">The network docs have been greatly improved in #15360
</comment><comment author="cyrilcyril70" created="2017-02-01T11:46:59Z" id="276638049">Worked on my AWS Linux Machine

cluster.name: myES_Cluster
node.name: ESNODE_CYR
node.master: true
node.data: true
transport.host: localhost
transport.tcp.port: 9300
http.port: 9200
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 2

I have tried with this on the elasticsearch.yml (key:value) and worked fine for me. But it takes 2 days to fix it :wink: :slight_smile: , going on with ES Doc is so tough.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Connection refused after upgrading to elasticsearch-2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14606</link><project id="" key="" /><description>Hi
After upgrading to elasticsearch-2.0 I'm failing to connect to the cluster.
When I do 

```
curl 'http://my-new-elasticsearch-dns:9200'
```

I get 

```
curl: (7) Failed to connect to my-new-elasticsearch-dns port 9200: Connection refused
```

If i do the same with some other elasticsearch cluster ver. 1.7.0 I get

```
{
  "status" : 200,
  "name" : "elasticsearch-node",
  "cluster_name" : "elasticsearch-cluster,
  "version" : {
    "number" : "1.7.0",
    "build_hash" : "929b9739cae115e73c346cb5f9a6f24ba735a743",
    "build_timestamp" : "2015-07-16T14:31:07Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

tried other actions such as `curl 'my-new-elasticsearch-dns:9200/_cat/indices/'`
and got same failure result.

Also the cluster (one single node) is up, but having some problems with the log:

```
log4j:WARN No appenders could be found for logger (bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```

don't know how to fix that.

any help?
thanks,
Noam
</description><key id="115723842">14606</key><summary>Connection refused after upgrading to elasticsearch-2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">noamgal</reporter><labels /><created>2015-11-08T10:48:50Z</created><updated>2015-11-09T09:19:44Z</updated><resolved>2015-11-08T11:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-08T11:29:40Z" id="154807984">Probably because you did not set network.host so it's bound to localhost only.
</comment><comment author="noamgal" created="2015-11-09T09:10:54Z" id="155002179">@dadoonet 
Thanks, working.
Afaik it wasn't mandatory on elasticsearch 1.7.
Are there any more changes required for ever. 2.0.
If yes - where can I find the list of all things? thanks
</comment><comment author="dadoonet" created="2015-11-09T09:19:44Z" id="155004424">In the breaking changes documentation I guess ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate mapper attachments plugin to main repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14605</link><project id="" key="" /><description>We got this plugin synced up to master, improved tests, fixed securitymanager issues, added over 200 test documents from tika test suite, trimmed the parsers to a more contained list (e.g. word, excel, openoffice, powerpoint, pdf, html, rtf, txt, etc), and added special security restrictions to try to better contain the parsers.

I think its in pretty good shape and we should merge it into the main repository. The fact is, people use it, and even if we don't like the idea of parsing things server side, its currently something functional. Otherwise we have to maintain it in a separate repository and that has proved difficult. If we have a better solution in the future for dealing with these documents, we can still probably reuse the testing and tika handling and so on.
</description><key id="115714619">14605</key><summary>Migrate mapper attachments plugin to main repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Mapper Attachment</label><label>feature</label><label>v5.0.0-alpha1</label></labels><created>2015-11-08T07:24:35Z</created><updated>2015-11-09T13:55:29Z</updated><resolved>2015-11-09T13:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-08T08:11:35Z" id="154788501">Thank you Robert for doing this. It will help a lot for maintain this plugin in a good shape until we propose something else.
I'm sure that found team will appreciate it as well! :)

I'll take care of the documentation rewrite to asciidoc after this will be merged.

Do you think we could also merge it in 2.1?
</comment><comment author="rmuir" created="2015-11-08T08:13:40Z" id="154788553">I set 2.2 because the issues i found were dealt with in ways that only 2.2+ can support.
</comment><comment author="dadoonet" created="2015-11-08T11:59:06Z" id="154813860">Can we backport it to 2.1 without the fixes you wrote? I mean that the external repo won't have those fixes so we will have the same situation but at least the plugin will have the exact same lifecycle as elasticsearch itself?
</comment><comment author="rmuir" created="2015-11-08T13:40:49Z" id="154827021">I'm not backporting bugs. This is a 3.0-only change now.
</comment><comment author="rjernst" created="2015-11-08T15:58:21Z" id="154835091">LGTM
</comment><comment author="rmuir" created="2015-11-09T13:54:07Z" id="155068285">Just to re-explain my reasoning for going to master only here. This PR is a three-pronged approach to fix security and insanity concerns:
1. whitelist parsers (requires master)
   This just simply cannot be done in maven, since it has no `transitive=false`. In gradle its easy:
   https://github.com/rmuir/elasticsearch/blob/migrate-mapper-attachments/plugins/mapper-attachments/build.gradle#L28-L51. I know what the maven equivalent would be: unmaintainable.
2. sandbox parsers (requires 2.2+)
   We need security improvements in 2.2+ to further restrict the parsers so we have less concerns about bugs in them (especially stupid stuff like xml flaws). 
3. improve tests

So if gradle goes back to 2.2, then its safe to backport this there too. But without these 3 things its an incomplete solution.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce name is set for all plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14604</link><project id="" key="" /><description>The plugin name currently defaults to the gradle project name. But the
gradle project name for standalone repo (like an external plugin would
be) defaults to the directory name of the repo. This is trappy, since it
depends on how the repo was checked out.

This change enforces the plugin name is always set.

closes #14603
</description><key id="115702906">14604</key><summary>Enforce name is set for all plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-08T01:00:48Z</created><updated>2015-11-08T01:19:22Z</updated><resolved>2015-11-08T01:19:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-08T01:09:56Z" id="154768694">looks good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin should fail if it does not set 'name'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14603</link><project id="" key="" /><description>Gradle is currently defaulting this based on the directory name, that is mega-trappy.

this is not one of those cases to save "a line of build" instructions. This should be explicit.
</description><key id="115698398">14603</key><summary>Plugin should fail if it does not set 'name'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-07T23:11:31Z</created><updated>2015-11-08T01:19:20Z</updated><resolved>2015-11-08T01:19:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add groovy build code (buildSrc/) in 'gradle eclipse'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14602</link><project id="" key="" /><description>This makes it a groovy project that works in eclipse.
You will have to install a plugin for groovy language support
(I used a snapshot build from https://github.com/groovy/groovy-eclipse/wiki)
</description><key id="115689359">14602</key><summary>add groovy build code (buildSrc/) in 'gradle eclipse'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-07T20:16:10Z</created><updated>2015-11-07T20:41:05Z</updated><resolved>2015-11-07T20:41:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-07T20:19:11Z" id="154744594">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow BucketScript to operate as a sibling pipeline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14601</link><project id="" key="" /><description>I've run across a number of scenarios where it would be useful to execute a `bucket_script` as a sibling, instead of a parent pipeline.  For example:

```
{
   "size": 0,
   "query": { ... },
   "aggs": {
      "widget_a_max": {
         "max": {
            "field": "widget_a"
         }
      },
      "widget_b_max": {
         "max": {
            "field": "widget_b"
         }
      },
      "widget_ratio": {
         "bucket_script": {
            "buckets_path": {
               "a": "widget_a_max",
               "b": "widget_b_max"
            },
            "script": "a / b"
         }
      }
   }
}
```

I keep running into this when trying to calculate ratios: you generate a number of metrics across potentially several aggs, then you just want to calculate a ratio of those metrics.  Often you end up in a situation where there is no bucket that holds all the data, and the only viable place to do the calculate is in a sibling.

/cc @colings86 
</description><key id="115674685">14601</key><summary>Allow BucketScript to operate as a sibling pipeline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-11-07T16:08:31Z</created><updated>2015-11-07T16:08:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow bucket_script to operate on SingleBucketAggregation objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14600</link><project id="" key="" /><description>Currently, the `bucket_script` pipeline is only capable of working with MultiBucketAggregations such as `histo` or `range`.  But there are a variety of use-cases where it would be nice to use it with a SingleBucket, like a filter.  It fails because the BucketScriptAgg attempts to cast the InternalFilter into a MultiBucketAgg and fails.

``` json
{
   "size":0,
   "aggs":{
      "connections":{
         "filter":{ ... },
         "aggs":{
            "users":{
               "cardinality":{
                  "field":"user_id"
               }
            },
            "average_distinct":{
               "bucket_script":{
                  "buckets_path":{
                     "distinct":"users",
                     "total":"_count"
                  },
                  "script":"total / distinct"
               }
            }
         }
      }
   }
}
```

```
org.elasticsearch.search.aggregations.bucket.filter.InternalFilter cannot be cast to org.elasticsearch.search.aggregations.InternalMultiBucketAggregation
```

You can skirt the issue by using a `filters` bucket with a single filter, but this is definitely a hack and rather gross:

```
{
   "size":0,
   "aggs":{
      "connections":{
         "filters":{
            "filters":[
               { ... }
            ]
         },
         "aggs":{
            "users":{
               "cardinality":{
                  "field":"user_id"
               }
            },
            "average_distinct":{
               "bucket_script":{
                  "buckets_path":{
                     "distinct":"users",
                     "total":"_count"
                  },
                  "script":"total / distinct"
               }
            }
         }
      }
   }
}
```

Other than complicating the code a bit more (needs to handle both Multi and Single), I don't think there is a technical reason why we couldn't support this?  I think we'd just need to implement an internal doReduce() that is overloaded for Multi / Single, and handles them as needed?

/cc @colings86 
</description><key id="115674302">14600</key><summary>Allow bucket_script to operate on SingleBucketAggregation objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aggregations</label><label>bug</label><label>discuss</label></labels><created>2015-11-07T15:59:16Z</created><updated>2015-11-24T13:15:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zcox" created="2015-11-23T21:31:30Z" id="159072008">:+1: on support for this.

It would also be great if `bucketScript` could work across sub-aggregations that are single buckets (e.g. filter). For example, if each document is a pageview event I'd like to calculate unique, new &amp; returning visitor counts over a time range:

```
{
  "size": 0,
  "aggs": {
    "visits": {
      "filters": {
        "filters": [
          {
            "bool": {
              "must": [
                {
                  "range": {
                    "time": {
                      "gte": "2015-10-25",
                      "lt": "2015-11-01",
                      "time_zone": "UTC"
                    }
                  }
                },
                {
                  "term": {
                    "eventType": "ProfileVisit"
                  }
                }
              ]
            }
          }
        ]
      },
      "aggs": {
        "unique-visitors-total": {
          "cardinality": {
            "field": "userId"
          }
        },
        "first-time-visits": {
          "filters": {
            "filters": [
              {
                "term": {
                  "firstTimeForVisit": true
                }
              }
            ]
          },
          "aggs": {
            "first-time-visitors-total": {
              "cardinality": {
                "field": "userId"
              }
            }
          }
        },
        "returning-visitors-total": {
          "bucket_script": {
            "buckets_path": {
              "uniqueVisitorsTotal": "unique-visitors-total",
              "firstTimeVisitorsTotal": "first-time-visits&gt;first-time-visitors-total"
            },
            "script": "uniqueVisitorsTotal - firstTimeVisitorsTotal"
          }
        }
      }
    }
  }
}
```

I can't even use the gross `filters` hack for this:

```
buckets_path must reference either a number value or a single value numeric metric aggregation, got: java.lang.Object[]
```

I think this is because `first-time-visitors-total` is inside a `filters` aggregation, so Elasticsearch thinks it has multiple bucket values, even though there's only 1 filter.

Maybe there's some way to re-organize this query to work around current limitations, but I can't find it and am pretty stuck. :cry: 

It seems like this should work though if the above query just used `filter` aggregations, since there would just be a single value for both `unique-visitors-total` and `first-time-visits&gt;first-time-visitors-total`.
</comment><comment author="zcox" created="2015-11-24T13:15:03Z" id="159262950">Another example use case is clickthrough rate:

```
        "impressions": {
          "filter": {
            "term": {
              "eventType": "Impression"
            }
          }
        },
        "clickthroughs": {
          "filter": {
            "term": {
              "eventType": "Clickthrough"
            }
          }
        },
        "clickthrough-rate": {
          "bucket_script": {
            "buckets_path": {
              "clickthroughCount": "clickthroughs._count",
              "impressionCount": "impressions._count"
            },
            "script": "clickthroughCount / impressionCount"
          }
        }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tribe</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14599</link><project id="" key="" /><description /><key id="115667664">14599</key><summary>tribe</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erwan-koffi</reporter><labels /><created>2015-11-07T13:43:23Z</created><updated>2015-11-07T14:03:09Z</updated><resolved>2015-11-07T13:47:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve integ test to match ant behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14598</link><project id="" key="" /><description>Many improvements to integ test behavior:
- Use spaces in ES path
- Use space in path for plugin file installation
- Use a different cwd than ES home
- Use jps to ensure process being stopped is actually elasticsearch
- Stop ES if pid file already exists
- Delete pid file when successfully killed

Also, refactored the cluster formation code to be a little more organized.

closes #14464
</description><key id="115648010">14598</key><summary>Improve integ test to match ant behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-07T08:45:15Z</created><updated>2015-11-07T21:09:08Z</updated><resolved>2015-11-07T21:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-07T21:06:51Z" id="154752039">@rmuir I pushed some comments and fixes.
</comment><comment author="rmuir" created="2015-11-07T21:07:42Z" id="154752091">OK, thanks very much for the second commit: +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put method addField on TopHitsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14597</link><project id="" key="" /><description>Exposes the addField method in SearchSourceBuilder to TopHitsBuilder

Closes #12962
</description><key id="115639504">14597</key><summary>Put method addField on TopHitsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">wbowling</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-07T06:21:54Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2015-11-09T10:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T17:20:24Z" id="154842267">@martijnvg could you take a look please?
</comment><comment author="martijnvg" created="2015-11-09T07:31:13Z" id="154982431">@wbowling Thanks! This PR looks good. I'll merge it in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm uses non-portable `--system` flag to `useradd`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14596</link><project id="" key="" /><description>This PR provides a fix for https://github.com/elastic/elasticsearch/issues/14211
</description><key id="115638875">14596</key><summary>rpm uses non-portable `--system` flag to `useradd`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">girirajsharma</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2015-11-07T06:06:47Z</created><updated>2016-06-29T22:39:27Z</updated><resolved>2016-04-06T20:46:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-18T17:51:54Z" id="165853040">@nik9000 I remember we spoke about the `-r` versus `--system` for older debian systems, do you remember what we said?
</comment><comment author="nik9000" created="2016-03-10T12:45:06Z" id="194826717">@dakrone I don't remember. Sorry!
</comment><comment author="dakrone" created="2016-04-06T20:46:06Z" id="206558811">As far as I could tell this is legitimate with older versions. We do have vagrant tests so I am going to merge this for the 5.0 release.
</comment><comment author="Roguebantha" created="2016-06-29T21:37:29Z" id="229496815">Glad to see this is getting fixed. It's causing RPM installation warnings/errors on RHEL5, however "-r" works perfectly fine. Is it possible that we could fix this for 2.3.4 as well?
</comment><comment author="ahelten" created="2016-06-29T22:39:27Z" id="229510622">Yes, please get this in 2.3.4 as well! Thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Build to correctly treat URLs and to not leak a file handle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14595</link><project id="" key="" /><description>The recent logic in Build leaks a file handle, and it incorrectly handles URLs. This means if the directory containing elasticsearch has a space, it won't work at all.

The only reasons test pass is because the gradle build is lenient about this at the moment.
</description><key id="115628307">14595</key><summary>Fix Build to correctly treat URLs and to not leak a file handle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-07T03:00:05Z</created><updated>2015-11-08T17:18:16Z</updated><resolved>2015-11-07T03:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-07T03:02:42Z" id="154607805">Part of the problem causing this, is the leniency of SuppressForbidden applied to the entire class. Let me fix that.
</comment><comment author="rmuir" created="2015-11-07T03:16:51Z" id="154609098">OK, this code doesn't suppress any IOExceptions anymore.
</comment><comment author="rjernst" created="2015-11-07T03:19:16Z" id="154609317">LGTM. I tested on a gradle branch that now uses spaces and it passes.
</comment><comment author="rmuir" created="2015-11-07T03:38:23Z" id="154613370">One last change: we can use JarInputStream here, since we are just reading the manifest. There will be no difference in speed, but it means MockFileSystems would have found the file handle leak in tests.

I also added a test that checks the path handling is correct (and it works even if codebase is not a jar).
</comment><comment author="rjernst" created="2015-11-07T03:40:27Z" id="154613588">+1, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field names cannot contain the . character in Elasticsearch 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14594</link><project id="" key="" /><description>Starting with Elasticsearch 2.0 field names can not contain the `.` character anymore.

I like to use `.` as separator in field names while using `_` instead of camel casing - i.e. `docker.label.some_stuff` instead of `docker_label_some_stuff` since it makes the parsing much easier.

Any chance field names with a `.` will be allowed in Elasticsearch 2.x?

See also https://discuss.elastic.co/t/field-name-cannot-contain/33251
</description><key id="115613739">14594</key><summary>Field names cannot contain the . character in Elasticsearch 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">digital-wonderland</reporter><labels /><created>2015-11-06T23:47:47Z</created><updated>2016-10-27T17:58:38Z</updated><resolved>2015-11-08T17:17:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T17:17:27Z" id="154842138">No.  Dots are used in paths all over the place, to represent keys in an object.  The fewer decision trees there are in the code, the less likely we're going to forget to implement something in a new area of code.

You could use dashes or slashes or a number of other characters instead.
</comment><comment author="Yasaswani" created="2016-06-16T05:48:51Z" id="226393873">We have an existing index which is created using Elasticsearch 1.0.2. We are planning to upgrade to 2.3.3. Also we have some fields which have dots in the filed names. Can we use de-dot filter to convert the dots to another character in the existing index? ie.. we don't want to delete the index and create new index with the updated mappings. or does de-dot filter work only for the new fields added?
</comment><comment author="clintongormley" created="2016-06-16T09:06:24Z" id="226429581">@Yasaswani you can't unfortunately.  reindexing is the only way forward
</comment><comment author="nickwilmes" created="2016-10-27T17:43:14Z" id="256717874">I just ran into this issue when upgrading to 2.3.  While it wouldn't be too hard to switch the dots to dashes or slashes, it goes against a lot of standards already in place.  For example, one of my plugins is collecting data from docker, who's standard is dots as separators.  ECS actually takes this one step further and enforces that standard.  The are also many others that use the same standard (DNS and Java, just to name a few).

By not allowing dots, I am now going to have to keep converting back and forth which is not only annoying but also a source of confusion and possible bugs in the future.
</comment><comment author="jhmartin" created="2016-10-27T17:47:43Z" id="256719034">+1 for consistency with java and Docker Metadata; https://docs.docker.com/engine/userguide/labels-custom-metadata/ describes how metadata should be namespaced similiar to Java using periods.  This is prime data to be loaded into ElasticSearch via LogStash, and prohibiting periods adds needless conversions to the input data (and breaks the upgrade path).
</comment><comment author="dakrone" created="2016-10-27T17:58:38Z" id="256721788">Dots in field names are allowed in ES 2.4 and 5.0, this is an older issues. See: https://github.com/elastic/elasticsearch/pull/19937
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add shard allocation explain API to explain why shards are (or aren't) UNASSIGNED</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14593</link><project id="" key="" /><description># Idea&lt;a id="sec-1" name="sec-1"&gt;&lt;/a&gt;

Relates to a comment on [#8606](https://github.com/elastic/elasticsearch/issues/8686) and supersedes [#14405](https://github.com/elastic/elasticsearch/issues/14405).

We currently have the `/_cluster/reroute` API, that, with the `explain` and
`dry_run` parameters allow a user to manually specify an allocation command and
get back an explanation for why that shard can or can not undergo the requested
allocation. This is useful, however, it requires a user to know which node a
shard should be on, and to construct an allocation command for the shard.

I would like to build an API to answer one of the most often asked questions:
"_Why is my shard UNASSIGNED?_"

Instead of it being shard and node specific, I envision an API that looks like:

```
GET /_cluster/allocation/explain
{
  "index": "myindex"
  "shard": 0,
  "primary": false
}
```

Which is basically asking "explain the allocation for a replica of shard 0 for
the 'myindex' index".

Here's an idea of how I'd like the response to look:

```
{
    "shard": {
        "index": "myindex"
        "shard": 0,
        "primary": false
    },
    "cluster_info": {
        "nodes": {
            "nodeuuid1": {
                "lowest_usage": {
                    "path": "/var/data1",
                    "free_bytes": 1000,
                    "used_bytes": 400,
                    "total_bytes": 1400,
                    "free_disk_percentage": "71.3%"
                    "used_disk_percentage": "28.6%"
                },
                "highest_usage": {
                    "path": "/var/data2",
                    "free_bytes": 1200,
                    "used_bytes": 600,
                    "total_bytes": 1800,
                    "free_disk_percentage": "66.6%"
                    "used_disk_percentage": "33.3%"
                }
            },
            "nodeuuid2": {
                "lowest_usage": {
                    "path": "/var/data1",
                    "free_bytes": 1000,
                    "used_bytes": 400,
                    "total_bytes": 1400,
                    "free_disk_percentage": "71.3%"
                    "used_disk_percentage": "28.6%"
                },
                "highest_usage": {
                    "path": "/var/data2",
                    "free_bytes": 1200,
                    "used_bytes": 600,
                    "total_bytes": 1800,
                    "free_disk_percentage": "66.6%"
                    "used_disk_percentage": "33.3%"
                }
            },
            "nodeuuid3": {
                "lowest_usage": {
                    "path": "/var/data1",
                    "free_bytes": 1000,
                    "used_bytes": 400,
                    "total_bytes": 1400,
                    "free_disk_percentage": "71.3%"
                    "used_disk_percentage": "28.6%"
                },
                "highest_usage": {
                    "path": "/var/data2",
                    "free_bytes": 1200,
                    "used_bytes": 600,
                    "total_bytes": 1800,
                    "free_disk_percentage": "66.6%"
                    "used_disk_percentage": "33.3%"
                }
            }
        },
        "shard_sizes": {
            "[myindex][0][P]": 1228718,
            "[myindex][0][R]": 1231289,
            "[myindex][1][P]": 1248718,
            "[myindex][1][R]": 1298718,
        }
    },
    "nodes": {
        "nodeuuid1": {
            "final_decision": "NO"
            "decisions": [
                {
                    "decider" : "same_shard",
                    "decision" : "NO",
                    "explanation" : "shard cannot be allocated on same node [JZU4UIPFQtWn34FyAH6VoQ] it already exists on"
                },
                {
                    "decider" : "snapshot_in_progress",
                    "decision" : "NO",
                    "explanation" : "a snapshot is in progress"
                }
            ],
            "weight": 1.9
        }
        "nodeuuid2": {
            "final_decision": "NO"
            "decisions": [
                {
                    "decider" : "node_version",
                    "decision" : "NO",
                    "explanation" : "target node version [1.4.0] is older than source node version [1.7.3]"
                }
            ],
            "weight": 1.3
        }
        "nodeuuid3": {
            "final_decision": "YES"
            "decisions": [],
            "weight": 0.9
        }
    }
}
```

Breaking down the parts:
## `shard`&lt;a id="sec-1-1" name="sec-1-1"&gt;&lt;/a&gt;

The same information passed into the request, so it is contained in the request
itself as well.
## `cluster_info`&lt;a id="sec-1-2" name="sec-1-2"&gt;&lt;/a&gt;

This roughly relates to https://github.com/elastic/elasticsearch/issues/14405,
however, I realized this API is not node-specific, so putting it in nodes' stats
API doesn't make sense. Instead, it's master-only information used for
allocation. Since this is gathered and used for allocation, it makes sense to
expose it here since it influences the final decision.
## `nodes`&lt;a id="sec-1-3" name="sec-1-3"&gt;&lt;/a&gt;

This is a map where each key is the node uuid (should probably include the node
name as well to be helpful). It has sub keys:
### `final_decision`&lt;a id="sec-1-3-1" name="sec-1-3-1"&gt;&lt;/a&gt;

A simple "YES" or "NO" for whether the shard can currently be allocated on this
node.
### `decisions`&lt;a id="sec-1-3-2" name="sec-1-3-2"&gt;&lt;/a&gt;

A list of all the "NO" decisions preventing allocation on this node. I could see
a flag being added to include all the "YES" decisions, but I think it should
default to showing "NO" only to prevent it being too verbose.
### `weight`&lt;a id="sec-1-3-3" name="sec-1-3-3"&gt;&lt;/a&gt;

I'd like to change the `ShardAllocator` interface to add the ability to "weigh"
a shard for a node, with whatever criteria it usually balances with. For
example, with the `BalancedShardsAllocator`, the weight would be the calculation
based on the number of shards for the index as well as the shard count.

I could see this being useful for answering the question "Okay, if all the
decisions where 'YES', where would this shard likely end up?".

It might be trickier to implement, but it could be added on at a later time.
# Conclusion&lt;a id="sec-2" name="sec-2"&gt;&lt;/a&gt;

I think this would go a long way towards helping users understand from a
cluster-level (instead of a single node-level) perspective why their shard can
or cannot be allocated.

Thoughts and feedback welcome! This is potentially a non-trivial amount of work
so I wanted to see what people thought before spending time implementing it.
</description><key id="115593549">14593</key><summary>Add shard allocation explain API to explain why shards are (or aren't) UNASSIGNED</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>:Cluster</label><label>discuss</label><label>feature</label><label>high hanging fruit</label></labels><created>2015-11-06T21:26:11Z</created><updated>2016-03-28T22:07:10Z</updated><resolved>2016-03-28T22:07:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seang-es" created="2015-11-06T21:29:36Z" id="154547310">This would be an extremely helpful feature for us.  We get tons of cases where the customer has unassigned shards and does not know why, and until we get logs from them, we have no answer.  A quick answer would help the user self-diagnose.  If it could be linked with _cat/shards to give us a single coherent display showing what's unassigned and why, it would be incredible.
</comment><comment author="ppf2" created="2015-11-06T21:32:54Z" id="154548093">Finally, great to see this API being discussed and planned!  Linking https://github.com/elastic/elasticsearch/issues/9471 as well.

+1 on a single API to get all the information/causes on why a shard cannot be unassigned.   It doesn't look like this one will cover the corruption scenario and the user will have to use the other API (https://github.com/elastic/elasticsearch/pull/11545)?  Having a single API will not only be helpful for admins but also facilitate future Marvel's reporting of unassigned shards and their reasons.
</comment><comment author="GlenRSmith" created="2015-11-06T21:34:25Z" id="154548432">:+1: How about the request body have properties that allow interrogation based on shard states?
</comment><comment author="dakrone" created="2015-11-06T21:37:30Z" id="154549246">&gt; How about the request body have properties that allow interrogation based on shard states?

I'm not sure what you mean by this, can you elaborate?
</comment><comment author="GlenRSmith" created="2015-11-06T21:41:47Z" id="154550198">&gt; I'm not sure what you mean by this, can you elaborate?

Skip the step of finding the unallocated shards and asking about each of them individually, and just

```
GET /_cluster/allocation/explain
{
  "index": "myindex"
  "shard": {
    "state": "unallocated"
  }
}
```

Not a literal proposal of form, but gets my intent across, I think.
</comment><comment author="clintongormley" created="2015-11-08T17:14:05Z" id="154841980">I like the idea a lot, but I'd reorganise things a bit, especially so it works better in the 500 node case.  I think the disk usage stats should also be available in the nodes stats API. In this API, I'd perhaps include the disk (relevant?) usage stats in the disk usage decider, rather than in a separate section (ie put it where it is relevant).

Perhaps default to a non-verbose output which doesn't list every YES decision, but only the first NO decision for each node.

Btw, what is this supposed to represent?

```
    "shard_sizes": {
        "[myindex][0][P]": 1228718,
        "[myindex][0][R]": 1231289,
        "[myindex][1][P]": 1248718,
        "[myindex][1][R]": 1298718,
    }
```
</comment><comment author="dakrone" created="2015-11-09T19:22:37Z" id="155164304">&gt; I think the disk usage stats should also be available in the nodes stats API.

I think this is doable, but is more complex because the stats are only available to the master node, so they're much harder to retrieve.

&gt; Perhaps default to a non-verbose output which doesn't list every YES decision, but only the first NO decision for each node.

This is similar to what I proposed, show all "NO" decisions (showing only the first one doesn't help since it could be a red herring for people to fix) and eliding all the "YES" decisions by default.

&gt; Btw, what is this supposed to represent? `... shard_sizes ...`

The ClusterInfoService gathers shard sizes every 30 seconds which are used for the disk decider, they aren't node-specific so they don't really belong in the nodes stats, so I figured they might be useful here.
</comment><comment author="dakrone" created="2015-11-09T21:03:08Z" id="155192294">&gt; I'd perhaps include the disk (relevant?) usage stats in the disk usage decider

Hmm... we could add a `public ToXContent supplementaryInfo()` method to the `AllocationDecider` method that would allow certain deciders to return any additional information. Interesting idea...
</comment><comment author="jordansissel" created="2016-01-22T22:23:20Z" id="174071704">I would also love this feature. +1 - Here's my story:

One scenario that is hard to debug (I ran into today) is if I configure ES cluster.routing.allocation.awareness.attributes to a nonsense attribute name (meaning, one that doesn't exist and is not set on any nodes), I will end up with new indices that are not able to allocate any shards. The explanation if I use `_reroute?dry_run&amp;explain` today is _slightly_ confusing, and as I understand it, the reroute API is a very advanced feature, so exposing newbies to it would be not so nice.

Having a friendlier (less advanced, whatever) way of asking why a shard is not assigned/allocated would be ever so lovely. &lt;3 &lt;3
</comment><comment author="pickypg" created="2016-02-12T18:59:44Z" id="183450962">Came across this while searching for this kind of feature. Definitely want! +1
</comment><comment author="bleskes" created="2016-03-01T14:13:03Z" id="190736670">I looked at this again and I like it.  I agree that it runs the danger of being too verbose. Here are some minor suggestions:

1) If we can't put the disk free stats on the node stats, I would output it last. Same goes for shard sizes. This assumes that a NO decision will tell the values it based things on (which might make these sections completely irrelevant)
2) Put the "yes" node first. We should be able to see in a quick glance if a shard can be assigned.
3) Try to rename nodes - I find it confusing. My only alternative is "possible_allocations" - though that's longish.

Last - why do we need to specify a primary flag in the request?
</comment><comment author="dakrone" created="2016-03-01T15:25:32Z" id="190769877">&gt; Last - why do we need to specify a primary flag in the request?

Allocation Deciders can have different rules for primary versus replica, additionally, when retrieving the shard from the cluster state, I plan to use the usassigned_info in the response as well, so we need to differentiate between the two.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Fix eclipse generation to add a core-tests projects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14592</link><project id="" key="" /><description>Eclipse does not have the ability to differentiate test dependencies
from main dependencies. This causes what looks like a circular
dependency through test-framework. This change sets up an additional
core-tests project for eclipse only, which removes this problem.
</description><key id="115573031">14592</key><summary>Build: Fix eclipse generation to add a core-tests projects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T19:39:40Z</created><updated>2015-11-06T19:55:53Z</updated><resolved>2015-11-06T19:55:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-06T19:52:07Z" id="154514945">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only allow rebalance operations to run if all shard store data is available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14591</link><project id="" key="" /><description>This commit prevents running rebalance operations if the store allocator is
still fetching async shard / store data to prevent pre-mature rebalance decisions
which need to be reverted once shard store data is available. This is typically happening
on rolling restarts which can make those restarts extremely painful.

Closes #14387
</description><key id="115570808">14591</key><summary>Only allow rebalance operations to run if all shard store data is available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T19:26:04Z</created><updated>2015-11-17T10:55:15Z</updated><resolved>2015-11-10T11:06:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-06T19:26:34Z" id="154508981">I tried to implement this with the least impact possible code wise to safely backport to 1.7
</comment><comment author="s1monw" created="2015-11-08T11:43:47Z" id="154809617">@bleskes I pushed a new commit
</comment><comment author="bleskes" created="2015-11-09T11:28:27Z" id="155033822">looks great. Left one minor comment in ReplicaShardAllocator
</comment><comment author="s1monw" created="2015-11-09T12:02:00Z" id="155046048">@bleskes pushed a new commit
</comment><comment author="bleskes" created="2015-11-09T12:02:47Z" id="155046164">LGTM. Thanks @s1monw 
</comment><comment author="s1monw" created="2015-11-09T21:29:00Z" id="155202692">@ywelsch you mentioned an integration test for this you added, is the test much different to my last commit (https://github.com/s1monw/elasticsearch/commit/7b5e323ec0581ce53d407e1de6f84709776ebfe0) and if so do you wanna add it to this PR?
</comment><comment author="ywelsch" created="2015-11-10T09:05:22Z" id="155362989">My test looked similar (no need to add more of the same). I did not rely on delayed shard allocation though but explicitly disabled allocation for the restart. Maybe the test can be randomised to pick one of these approaches and also chose a random number of shards?
</comment><comment author="s1monw" created="2015-11-10T10:54:06Z" id="155387966">&gt; Maybe the test can be randomised to pick one of these approaches and also chose a random number of shards?

I don't like the randominzation aspect here. I think the test should be really stable to just test what it is supposed to test. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use projectsPrefix in project references for vagrant tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14590</link><project id="" key="" /><description>It is important we use projectsPrefix whenever addinga direct project
dependency, so that attachment still works.
</description><key id="115565272">14590</key><summary>Use projectsPrefix in project references for vagrant tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T18:54:02Z</created><updated>2015-11-06T19:08:31Z</updated><resolved>2015-11-06T19:08:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-06T19:05:48Z" id="154502915">Lgtm
On Nov 6, 2015 1:54 PM, "Ryan Ernst" notifications@github.com wrote:

&gt; It is important we use projectsPrefix whenever addinga direct project
&gt; 
&gt; ## dependency, so that attachment still works.
&gt; 
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;   https://github.com/elastic/elasticsearch/pull/14590
&gt; Commit Summary
&gt; - Build: Use projectsPrefix in project references for vagrant tests
&gt; 
&gt; File Changes
&gt; - _M_ qa/vagrant/build.gradle
&gt;   https://github.com/elastic/elasticsearch/pull/14590/files#diff-0 (4)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elastic/elasticsearch/pull/14590.patch
&gt; - https://github.com/elastic/elasticsearch/pull/14590.diff
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14590.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not able to start elasticsearch service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14589</link><project id="" key="" /><description>I migrated from v1.7.1 to 2.0.0. Had some issues with this version. When I migrated back to previous version I got this error.
###### 

[2015-11-06 23:02:47,214][ERROR][bootstrap                ] Exception
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, org.elasticsearch.ElasticsearchException
  at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState
    for parameter 4 at org.elasticsearch.gateway.local.LocalGateway.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.LocalGateway
  while locating org.elasticsearch.gateway.Gateway
    for parameter 1 at org.elasticsearch.gateway.GatewayService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.GatewayService
Caused by: org.elasticsearch.ElasticsearchException
        at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:153)
        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:297)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadIndexState(LocalGatewayMetaState.java:400)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.loadState(LocalGatewayMetaState.java:388)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.&lt;init&gt;(LocalGatewayMetaState.java:164)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:210)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:77)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:245)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:137)
        at org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:73)
        at org.elasticsearch.cluster.metadata.MappingMetaData.&lt;init&gt;(MappingMetaData.java:298)
        at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:670)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState$3.fromXContent(LocalGatewayMetaState.java:351)
        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState$3.fromXContent(LocalGatewayMetaState.java:343)
        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.read(MetaDataStateFormat.java:176)
        at org.elasticse
</description><key id="115552837">14589</key><summary>Not able to start elasticsearch service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">new-player</reporter><labels /><created>2015-11-06T17:40:07Z</created><updated>2016-01-21T00:18:23Z</updated><resolved>2015-11-09T12:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T12:39:39Z" id="155052801">Once you have upgraded to a new version, you can't go back.
</comment><comment author="new-player" created="2015-11-09T12:41:40Z" id="155053048">Then how can I completely remove it.
</comment><comment author="skylarmb" created="2015-12-17T22:45:05Z" id="165605896">+1 Same issue. Initially installed 2.1, found out our application requires 1.7.3, so I uninstalled 2.1 and installed 1.7.3 and I get this same error. 

EDIT: I ended up resolving this issue here: https://github.com/elastic/elasticsearch/issues/10565#issuecomment-165616334
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce Java 8 in Gradle builds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14588</link><project id="" key="" /><description>This commit forces builds with Gradle to require at least Java 8.

Closes #14499
</description><key id="115541761">14588</key><summary>Enforce Java 8 in Gradle builds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T16:39:42Z</created><updated>2015-11-06T17:33:29Z</updated><resolved>2015-11-06T17:33:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-06T16:40:43Z" id="154464872">This gives:

```
11:39:21 [jason:~/src/elastic/elasticsearch] enforce-java-8 &#177; JAVA_HOME=`/usr/libexec/java_home -v 1.7` gradle assemble

FAILURE: Build failed with an exception.

* Where:
Build file '/Users/jason/src/elastic/elasticsearch/core/build.gradle' line: 24

* What went wrong:
A problem occurred evaluating project ':core'.
&gt; Failed to apply plugin [id 'elasticsearch.build']
   &gt; Java 8 or above is required to build Elasticsearch

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.
```
</comment><comment author="rjernst" created="2015-11-06T17:14:50Z" id="154476127">LGTM, one tiny nit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move DateHistogramTests back to core module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14587</link><project id="" key="" /><description>DateHistogramTests was using groovy scripts and for this reason was moved to the lang-groovy module. This moves it back and replaces use of groovy scripts by a mock script engine.
Removing three test cases that were testing doing some date manipulation using script, since these are more groovy script tests than testing the DateHistogram aggregation.
</description><key id="115536657">14587</key><summary>Move DateHistogramTests back to core module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T16:16:38Z</created><updated>2015-11-20T15:15:04Z</updated><resolved>2015-11-18T15:35:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-18T12:29:25Z" id="157695425">@jpountz I added the tests for value scripts, mind to take another look?
</comment><comment author="jpountz" created="2015-11-18T14:39:38Z" id="157732297">LGTM
</comment><comment author="cbuescher" created="2015-11-20T15:13:51Z" id="158427292">Also backported to 2.x with 2c2425d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle shards assigned to nodes that are not in the cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14586</link><project id="" key="" /><description>This commit addresses an issue in TransportBroadcastByNodeAction.
Namely, if in between a master leaving the cluster and a new one being
assigned, a request that relies on TransportBroadcastByNodeAction
(e.g., an indices stats request) is issued,
TransportBroadcastByNodeAction might attempt to send a request to a
node that is no longer in the local node&#8217;s cluster state.

The exact circumstances that lead to this are as follows. When the
master leaves the cluster and another node&#8217;s master fault detection
detects this, the local node will update its local cluster state to no
longer include the master node. However, the routing table will not be
updated. This means that in the preparation for sending the requests in
TransportBroadcastByNodeAction, we need to check that not only is a
shard assigned, but also that it is assigned to a node that is still in
the local node&#8217;s cluster state.

This commit adds such a check to the constructor of
TransportBroadcastByNodeAction. A new unit test is added that checks
that no request is sent to the master node in such a situation; this
test fails with a NullPointerException without the fix. Additionally,
the unit test TransportBroadcastByNodeActionTests#testResultAggregation
is updated to also simulate a master failure. This updated test also
fails prior to the fix.

Closes #14584
</description><key id="115526773">14586</key><summary>Handle shards assigned to nodes that are not in the cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T15:32:52Z</created><updated>2015-11-16T14:10:05Z</updated><resolved>2015-11-06T17:10:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-06T17:08:04Z" id="154474612">LGTM
</comment><comment author="jasontedor" created="2015-11-06T17:20:22Z" id="154477397">Integrated into [master](https://github.com/elastic/elasticsearch/commit/7d7926e10b28eb42096895819acfbb7c1e32a5f8), [2.0](https://github.com/elastic/elasticsearch/commit/aced785decd3199a69af219d221c39940eb34eb1), [2.1](https://github.com/elastic/elasticsearch/commit/eb530b42e573f393be54642414a5dce31c050c64), and [2.x](https://github.com/elastic/elasticsearch/commit/54aa013250b872fe5cafdc0c1351cb98ca9c3be7)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>delete-by-query plugin usage inside a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14585</link><project id="" key="" /><description>Hey,
I'm having trouble upgrading my plugin to support elasticsearch 2.0.0
The plugin used deleteByQuery so i referenced the delete-by-query from maven and everything compiled well. even worked on tests.

But, after installing the plugin , calling a function that used deletebyQuery caused an exception:
no_class_def_found_error
org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder

so I used maven-dependency-plugin with the copy goal and added the delete-by-query jar to my zip .
            `&lt;artifactItem&gt;
                                    &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
                                    &lt;artifactId&gt;delete-by-query&lt;/artifactId&gt;
                                    &lt;overWrite&gt;true&lt;/overWrite&gt;
                                    &lt;outputDirectory&gt;${project.build.directory}&lt;/outputDirectory&gt;
                                    &lt;destFileName&gt;deleteByQuery.jar&lt;/destFileName&gt;
                                &lt;/artifactItem&gt;`

But now I'm getting the weirdest error:
org.elasticsearch.action.deletebyquery.DeleteByQueryRequest cannot be cast to org.elasticsearch.action.deletebyquery.DeleteByQueryRequest

when  calling .get() on DeleteByQueryRequestBuilder

Help please!
</description><key id="115514920">14585</key><summary>delete-by-query plugin usage inside a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eliranmoyal</reporter><labels><label>:Plugins</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-11-06T14:30:37Z</created><updated>2016-12-10T15:30:56Z</updated><resolved>2016-09-27T16:05:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-06T14:42:49Z" id="154426846">this is not supported. Plugins can't depend on other plugins. the delete by query functionality is only available via the rest layer at this point. I will need to discuss this with others how we can make stuff like this work but at this point it's not supported
</comment><comment author="eliranmoyal" created="2016-12-10T15:27:33Z" id="266215917">hey , I thought that on elastisearch5.0 this will work , but I'm still getting this error.
there is still no way for another plugin to use delete-by-query java api?</comment><comment author="nik9000" created="2016-12-10T15:30:56Z" id="266216117">&gt; there is still no way for another plugin to use delete-by-query java api?

Right. We've not worked on this.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE onNodeFailure </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14584</link><project id="" key="" /><description>Hi,

we may have tried something stupid here:

```
$ curl 192.168.8.180:9200/_cat/nodes?h=name,version,jdk,ip,master,node.role
robocop      2.0.0 1.7.0_79 192.168.8.237 - d
Det R&#248;de Lyn 2.0.0 1.8.0_65 192.168.8.180 * -
stuart       2.0.0 1.8.0_65 192.168.8.13  - d
Gulle        2.0.0 1.7.0_85 192.168.8.21  m d 
```

"Det R&#248;de Lyn" is a windows machine.

all nodes have "Gulle" configured as unicast host

```
discovery.zen.ping.unicast.hosts: ["192.168.8.21"]
```

when Gulle shutdown the cluster broke down and the following shows up in the log of the stuart node. The logfile of the other nodes in the cluster dont show a similar NPE.

We have been unable to reproduce this, but thought you might be interested in this anyway.

```
[2015-11-06 13:28:07,710][INFO ][cluster.routing.allocation.decider] [stuart] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
[2015-11-06 13:28:47,859][INFO ][discovery.zen            ] [stuart] master_left [{Gulle}{F4pDMpoOSG6U0fBuxUzFWg}{192.168.8.21}{192.168.8.21:9300}], reason [transport disconnected]
[2015-11-06 13:28:47,859][WARN ][discovery.zen            ] [stuart] master left (reason = transport disconnected), current nodes: {{robocop}{LHRwN1SYTMC05oPnrnlhVg}{192.168.8.237}{192.168.8.237:9300}{rack=robo-desk, max_local_storage_nodes=1, master=false},{Det R&#248;de Lyn}{DmoJNBpOR8m1WKBPQojwag}{192.168.8.180}{192.168.8.180:9300}{data=false},{stuart}{TYWtWR00Q6Kv8pG6Fi09Yw}{192.168.8.13}{192.168.8.13:9300}{master=false},}
[2015-11-06 13:28:47,859][INFO ][cluster.service          ] [stuart] removed {{Gulle}{F4pDMpoOSG6U0fBuxUzFWg}{192.168.8.21}{192.168.8.21:9300},}, reason: zen-disco-master_failed ({Gulle}{F4pDMpoOSG6U0fBuxUzFWg}{192.168.8.21}{192.168.8.21:9300})
[2015-11-06 13:28:50,397][DEBUG][action.admin.cluster.state] [stuart] no known master node, scheduling a retry
[2015-11-06 13:28:50,398][DEBUG][action.admin.cluster.state] [stuart] no known master node, scheduling a retry
[2015-11-06 13:28:50,398][INFO ][rest.suppressed          ] /_stats/docs,store Params: {metric=docs,store}
java.lang.NullPointerException
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.onNodeFailure(TransportBroadcastByNodeAction.java:332)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.sendNodeRequest(TransportBroadcastByNodeAction.java:314)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.start(TransportBroadcastByNodeAction.java:284)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:217)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:77)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
        at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
        at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1177)
        at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.stats(AbstractClient.java:1482)
        at org.elasticsearch.rest.action.admin.indices.stats.RestIndicesStatsAction.handleRequest(RestIndicesStatsAction.java:102)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:348)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-11-06 13:28:50,398][DEBUG][action.admin.indices.get ] [stuart] no known master node, scheduling a retry
[2015-11-06 13:28:50,399][DEBUG][action.admin.cluster.health] [stuart] no known master node, scheduling a retry
[2015-11-06 13:28:51,291][INFO ][cluster.service          ] [stuart] detected_master {Det R&#248;de Lyn}{DmoJNBpOR8m1WKBPQojwag}{192.168.8.180}{192.168.8.180:9300}{data=false}, reason: zen-disco-receive(from master [{Det R&#248;de Lyn}{DmoJNBpOR8m1WKBPQojwag}{192.168.8.180}{192.168.8.180:9300}{data=false}])
[2015-11-06 13:29:13,992][INFO ][cluster.service          ] [stuart] added {{Gulle}{vDbnkAfsT96NnX_Q3cN1oA}{192.168.8.21}{192.168.8.21:9300},}, reason: zen-disco-receive(from master [{Det R&#248;de Lyn}{DmoJNBpOR8m1WKBPQojwag}{192.168.8.180}{192.168.8.180:9300}{data=false}])
[2015-11-06 13:29:55,312][INFO ][cluster.routing.allocation.decider] [stuart] updating [cluster.routing.allocation.enable] from [NONE] to [ALL]
```
</description><key id="115502689">14584</key><summary>NPE onNodeFailure </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bangert</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2015-11-06T13:15:38Z</created><updated>2015-11-09T13:05:00Z</updated><resolved>2015-11-06T17:10:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bangert" created="2015-11-06T13:33:30Z" id="154410663">hm, we were actually able to reproduce this.
This time, all remaining nodes showed the same NPE, while the new master node (Det R&#248;de Lyn) actually showed it twice (approx. 10 seconds between the two)
</comment><comment author="jasontedor" created="2015-11-06T13:46:28Z" id="154413748">This is happening because in between the master leaving the cluster and a new one getting assigned, the routing table won't get updated. As such, when the master leaves the cluster, the shards assigned to that node will not be reassigned. However, for any node that has detected the master as failed, it will update its local cluster state to no longer include the master in the list of nodes in the cluster. As such, the shards that were on the master will be in the cluster state but not associated with a node that is in the cluster state. When `sendNodeRequest` tries to look up the node to send the request to for such a shard there ends up being no such node. This leads to one NPE, which gets caught and cascades into a second NPE in the `onNodeFailure` handler. The correct course of action is to treat such shards as if it they are not available so that we never even attempt to send requests for them.

Thank you for the bug report. I'll be opening a pull request later to address this issue.
</comment><comment author="bangert" created="2015-11-09T08:31:00Z" id="154996222">Awesome work! Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add generic type to Processor.Factory and rename Geo*Tests to GeoIp*Tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14583</link><project id="" key="" /><description>Add generic type to `Processor.Factory` to avoid casting of processors after creation and rename `Geo*Tests` to `GeoIp*Tests`
</description><key id="115492223">14583</key><summary>Add generic type to Processor.Factory and rename Geo*Tests to GeoIp*Tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-11-06T11:56:48Z</created><updated>2015-11-06T12:04:28Z</updated><resolved>2015-11-06T12:04:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-06T12:00:44Z" id="154393338">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ingest] The fields geoip adds should be configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14582</link><project id="" key="" /><description>Also the geoip processor adds the ipfield, which is redundant, because it is the same as from the configured `ip_field`.
</description><key id="115491195">14582</key><summary>[ingest] The fields geoip adds should be configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-06T11:48:45Z</created><updated>2015-11-10T04:51:57Z</updated><resolved>2015-11-10T04:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-10T04:51:57Z" id="155288001">Pushed via #14640
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On Backup and Restore can I filter terms on Index?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14581</link><project id="" key="" /><description>Something like:

curl -XPUT 'http://xxxxxx:9200/_snapshot/20151106/snapshot_3' -d '{
  "indices": "metrics-*",
  "query": "env=pro",
  "ignore_unavailable": "true",
  "include_global_state": false
}'
</description><key id="115489369">14581</key><summary>On Backup and Restore can I filter terms on Index?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tearsakura</reporter><labels /><created>2015-11-06T11:34:28Z</created><updated>2015-11-06T11:38:19Z</updated><resolved>2015-11-06T11:38:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-06T11:38:19Z" id="154390147">please use our discuss list for questions like this https://discuss.elastic.co/ this is only for feature requests and bugs. 

the answer to your question is no you can't
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] geoip processor should be able to handle unknown ip address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14580</link><project id="" key="" /><description /><key id="115485339">14580</key><summary>[Ingest] geoip processor should be able to handle unknown ip address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-06T11:11:14Z</created><updated>2015-11-09T11:35:54Z</updated><resolved>2015-11-09T11:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-09T11:10:35Z" id="155030503">left a tiny comment, LGTM otherwise. I don't know what the right choice is between tagging or not tagging the document, but we can always go back and change this if we change our minds.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IT for plugins should use plugin name and not artifactId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14579</link><project id="" key="" /><description>In mapper attachments plugin, we needed to change the default value for `elasticsearch.plugin.name`. It was by default `artifactId` (elasticsearch-mapper-attachments) but it should have been `mapper-attachments`.

So we changed in `pom.xml`

```
&lt;elasticsearch.plugin.name&gt;mapper-attachments&lt;/elasticsearch.plugin.name&gt;
```

The problem is that integration tests expect to install a plugin which name is the `project.artifactId` and not the `elasticsearch.plugin.name`.

This commit fixes that.

Note that I have no idea on how to fix that in master branch. So I propose for now to push this in 2.x, 2.1 and 2.0 branches.
</description><key id="115481827">14579</key><summary>IT for plugins should use plugin name and not artifactId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v2.1.1</label><label>v2.2.0</label></labels><created>2015-11-06T10:51:01Z</created><updated>2015-11-23T10:59:57Z</updated><resolved>2015-11-23T10:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-09T07:42:03Z" id="154986157">@rmuir @rjernst Could you review it please?
Probably we don't need anymore a fix in master branch because of the recent PR you submitted in mapper-attachments project.
</comment><comment author="dadoonet" created="2015-11-10T07:13:16Z" id="155336011">Sounds like it's fixed in master with this: https://github.com/elastic/elasticsearch/pull/14639
</comment><comment author="dadoonet" created="2015-11-20T08:55:17Z" id="158328692">@bleskes Can you help on this one?
</comment><comment author="rjernst" created="2015-11-20T15:40:03Z" id="158435666">LGTM
</comment><comment author="dadoonet" created="2015-11-23T10:59:28Z" id="158904643">Closed with 95b0ad6 in 2.x and 2098c8f in 2.1 branches
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use fresh index settings instead of relying on @IndexSettings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14578</link><project id="" key="" /><description>With this commit fresh index settings when creating index related
components. Previously a Guice provider has been used together with a
custom @IndexSettings annotation. This turned out to be problematic
as Guice caches this value too aggressively.

Fixes #14319
</description><key id="115468358">14578</key><summary>Use fresh index settings instead of relying on @IndexSettings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Core</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-11-06T09:31:20Z</created><updated>2015-11-06T15:23:16Z</updated><resolved>2015-11-06T11:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-06T10:59:17Z" id="154380912">LGTM, thank you @danielmitterdorfer!
</comment><comment author="danielmitterdorfer" created="2015-11-06T11:06:21Z" id="154382328">Thanks @mikemccand for reviewing. @s1monw: Most of the changes are pretty straightforward but I think a few places are of interest, e.g. `IndexShardTests` and `IndexService`.
</comment><comment author="s1monw" created="2015-11-06T11:22:31Z" id="154384818">with this in place can we remove @IndexSettigns completely? Also maybe the provider in IndexSettingsModule
</comment><comment author="s1monw" created="2015-11-06T11:22:45Z" id="154384866">I left a question other than that LGTM too
</comment><comment author="danielmitterdorfer" created="2015-11-06T11:46:15Z" id="154391286">`@IndexSettings` and the corresponding provider should have gone with this commit already. Did I miss a spot?
</comment><comment author="s1monw" created="2015-11-06T11:47:37Z" id="154391478">I missed it - my bad LGTM
</comment><comment author="danielmitterdorfer" created="2015-11-06T11:49:32Z" id="154391747">Thanks for the review. I'll merge the fix and backport it also to 2.1 now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>subclass of   ActionFilter.Simple  was invoked twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14577</link><project id="" key="" /><description>hi, guys,   i wrote a Filter whice extends ActionFilter.Simple ,  and override the method : apply(String action, ActionResponse response, ActionListener listener).  when i invoke method: client.prepareSearch,  the apply method  was invoked twice whice is not i wanted, once is enough. is there sth i missed ?  here is my code :

public static class LoggingFilter extends ActionFilter.Simple {

```
    @Inject
    public LoggingFilter(Settings settings) {
        super(settings);
    }

    @Override
    public int order() {
        return 999;
    }

    @Override
    protected boolean apply(String action, ActionRequest request, ActionListener listener) {
        return true;
    }

    @Override
    protected boolean apply(String action, ActionResponse response, ActionListener listener) {
        if ("indices:data/read/search".equals(action)) {
            System.out.println("=========" + action + "======" + listener);
            System.out.println("=========" + response.hashCode() + "======" );
        }

        return true;
    }
}
```

public class ActionInvokeModule extends AbstractModule implements PreProcessModule {

```
    @Override
    protected void configure() {
        bind(LoggingFilter.class).asEagerSingleton();
    }

    @Override
    public void processModule(Module module) {
        if (module instanceof ActionModule) {
            ((ActionModule)module).registerFilter(LoggingFilter.class);
        }
    }
}
```

public class ActionInvokePlugin extends AbstractPlugin {

```
@Override
public Collection&lt;Class&lt;? extends Module&gt;&gt; modules() {
    Collection&lt;Class&lt;? extends Module&gt;&gt; modules = new ArrayList&lt;Class&lt;? extends Module&gt;&gt;();
    modules.add(ActionLoggingModule.class);
    System.out.println("==========loaded log plugin");
    return modules;
}
```

}

this is my client :
@Test
    public void testSimpleSearch() {
        SearchResponse response = client.prepareSearch("twitter2")
                .setTypes("tweet2")
                .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                .setQuery(QueryBuilders.termQuery("user", "wangwu"))             // Query
                .setFrom(0).setSize(60).setExplain(true)
                .execute()
                .actionGet();
        for (SearchHit hit : response.getHits().getHits()) {
            System.out.println("======" + hit.getSource());
        }

```
}
```
</description><key id="115463575">14577</key><summary>subclass of   ActionFilter.Simple  was invoked twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">whdwsl</reporter><labels /><created>2015-11-06T09:02:33Z</created><updated>2015-11-24T19:07:05Z</updated><resolved>2015-11-24T19:07:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-23T16:41:51Z" id="158990947">I think this is the same as #11710. If it happens only with search, it's because TransportSearchAction delegates to a subaction (depending on the search type) which causes the duplication you are seeing.
</comment><comment author="javanna" created="2015-11-24T19:07:05Z" id="159374961">Closing as duplicate of #11710 as it is caused by the same problem. Feel free to reopen if that's not the case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation for output of stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14576</link><project id="" key="" /><description>There is a lot of information in the stats API, but little of it is documented, and confusion arises...

e.g. 
_all.primaries.docs.count shows all the documents lucene sees, which may be different to the documents elasticsearch sees with a simple search if there are nested documents being used. (I think this statement is true... but is it?)
</description><key id="115462055">14576</key><summary>Documentation for output of stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">robin13</reporter><labels><label>:Stats</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-11-06T08:51:29Z</created><updated>2016-01-15T12:40:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="masaruh" created="2015-11-27T02:01:54Z" id="160019701">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>same field names may raise confliction with different type and analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14575</link><project id="" key="" /><description># Here is the statement about how this happens, see as follows:

curl -XPUT 'localhost:9200/company?pretty' -d '
{
  "mappings": {
    "basic": {
      "properties": {
        "id" : {"type" : "string", "index" : "not_analyzed"},
        "title" : {
            "type" : "string",
            "fields": {
                "raw": { 
                  "type":  "string",
                  "index": "not_analyzed"
                }
            }
        },
        "djjg" : {"type" : "string", "index" : "not_analyzed"}
      }
    },
    "investor": {
      "_parent": {
        "type": "basic" 
      },
       "properties": {
          "title" : {"type" : "string", "index" : "not_analyzed"},
          "type" : {"type" : "string", "index" : "not_analyzed"}
       }
    }
  }
# }'
# The following error messages occurs:

{
  "error" : {
    "root_cause" : [ {
      "type" : "mapper_parsing_exception",
      "reason" : "mapping [investor]"
    } ],
    "type" : "mapper_parsing_exception",
    "reason" : "mapping [investor]",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "Mapper for [title] conflicts with existing mapping in other types:\n[mapper [title] has different [index] values, mapper [title] has different [doc_values] values, cannot change from disabled to enabled, mapper [title] has different [analyzer], mapper [title] is used by multiple types. Set update_all_types to true to update [omit_norms] across all types., mapper [title] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [title] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]"
    }
  },
  "status" : 400
# }

I guess this error is raised due to same "title" fields used in parent and child mode :-)
Of cause, I can change the second one to "name", and that is fine.
But I think it should be not a big problem.
Any hint???

Regards,
Hawk
</description><key id="115458444">14575</key><summary>same field names may raise confliction with different type and analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hawkwang</reporter><labels /><created>2015-11-06T08:19:14Z</created><updated>2017-05-10T19:29:27Z</updated><resolved>2015-11-06T08:32:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-06T08:32:37Z" id="154344072">&gt; I guess this error is raised due to same "title" fields used in parent and child mode :-)
&gt; Of cause, I can change the second one to "name", and that is fine.

yes that's the reason, even though they seem to be isolated they are not. You have to either specify the same analyzer and type or use a different name.
</comment><comment author="isapir" created="2015-11-10T15:12:06Z" id="155447104">Is there a blog post or some article explaining this?  I'm trying to upgrade from ES1.7.1 to ES2.0.0 and I'm getting this error, and I don't understand what `Set update_all_types to true` means.  Where do I set `update_all_types`?

Would removing and reindexing all of the data fix this for me?  Or is the problem with the mappings?

```
2015-11-08 19:22:34 Commons Daemon procrun stderr initialized
Exception in thread "main" tion: unable to upgrade the mappings for the index [test1], reason: [Mapper for [_all] conflicts with existing mapping in other types:
[mapper [_all] has different [analyzer], mapper [_all] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [_all] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]]
Likely root cause: java.lang.IllegalArgumentException: Mapper for [_all] conflicts with existing mapping in other types:
[mapper [_all] has different [analyzer], mapper [_all] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [_all] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]
    at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:345)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:296)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:242)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:226)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</comment><comment author="clintongormley" created="2015-11-16T19:35:54Z" id="157145879">Yeah during upgrade you can't specify that option (see https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html#merging-conflicts for where you can use it).  

Essentially you need to fix or delete this index before upgrading.  The [migration plugin](https://github.com/elastic/elasticsearch-migration/) will help you to spot problematic indices before upgrading.
</comment><comment author="biancazzurri" created="2017-05-10T19:26:07Z" id="300587451">Please somebody explain why is this an expected behavior? To me it seems that its a non correct one. To my understanding there should be no connection between fields in different type</comment><comment author="dadoonet" created="2017-05-10T19:29:27Z" id="300588259">Please ask your question on discuss.elastic.co where we can certainly help.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong matched query in inner hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14574</link><project id="" key="" /><description>hi, all

I found the matched query in inner hits is not correct in some cases.

Suppose we have two documents: P for parent document, C for child document. A query looks like:

```
{
    "or": [    
   {
        "has_child": {
            "type": "C",
           "query": {
               A query named q1
           },
           "name": "xxx"
       },
       {
           "has_child": {
               "type": "C",
               "query": {
                  A query named q2
               },
              "name":  "yyy"
           }
       }
}
```

Searching for doc_type P.

**It's possible for a single document C that satisfies both q1 and q2.**

So, I wanna parse the es result to figure out which query is hitted by the matched query, what am I doing is:
- Find the inner object by xxx or yyy
- Find the matched query in the inner hits

What I expected is finding q1 in xxx or q2 in yyy, but actually I found both q1 and q2 in xxx (Actually q1 and q2 may be the same query except the name), but the named query q2 isn't defined in has_child query xxx.

==== More ====

The version of elasticsearch is 2.0.0 Release.
</description><key id="115432364">14574</key><summary>Wrong matched query in inner hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lipixun</reporter><labels /><created>2015-11-06T04:37:21Z</created><updated>2015-11-17T13:30:46Z</updated><resolved>2015-11-09T12:04:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T12:04:10Z" id="155046353">It'd be much easier to answer your question if you provided a complete example demonstrating exactly what you're doing.

I've come up with my own example which demonstrates that it works correctly:

```
PUT test 
{
  "mappings": {
    "parent": {

    },
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT test/parent/1
{}

PUT test/child/2?parent=1
{
  "foo": "bar"
}

PUT test/child/3?parent=1
{
  "foo": "baz"
}

PUT test/child/4?parent=1
{
  "foo": "bar baz"
}

GET test/parent/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "has_child": {
            "type": "child",
            "inner_hits": {
              "name": "bar"
            },
            "query": {
              "match": {
                "foo": "bar"
              }
            }
          }
        },
        {
          "has_child": {
            "type": "child",
            "inner_hits": {
              "name": "baz"
            },
            "query": {
              "match": {
                "foo": "baz"
              }
            }
          }
        }
      ]
    }
  }
}
```
</comment><comment author="lipixun" created="2015-11-10T03:56:20Z" id="155276637">All right, here is a complete query which I sent to es:

```
{
    "query": {
        "bool": {
            "must": [
                {
                    "has_child": {
                        "query": {
                            "bool": {
                                "must": [
                                    {
                                        "match": {
                                            "text": {
                                                "operator": "and",
                                                "query": "avalue",
                                                "_name": "q1"
                                            }
                                        }
                                    },
                                    {
                                        "term": {
                                            "doc_type": {
                                                "value": "XXX"
                                            }
                                        }
                                    }
                                ]
                            }
                        },
                        "inner_hits": {
                            "highlight": {
                                "fields": {
                                    "text": {}
                                }
                            },
                            "name": "child1",
                            "size": 1
                        },
                        "type": "es_emr",
                        "_name": "child1"
                    }
                },
                {
                    "has_child": {
                        "query": {
                            "bool": {
                                "must": [
                                    {
                                        "match": {
                                            "text": {
                                                "operator": "and",
                                                "query": "avalue",
                                                "_name": "q2"
                                            }
                                        }
                                    },
                                    {
                                        "term": {
                                            "doc_type": {
                                                "value": "YYY"
                                            }
                                        }
                                    }
                                ]
                            }
                        },
                        "inner_hits": {
                            "highlight": {
                                "fields": {
                                    "text": {}
                                }
                            },
                            "name": "child2",
                            "size": 1
                        },
                        "type": "es_emr",
                        "_name": "child2"
                    }
                }
            ]
        }
    }
}
```

We have a document type es_emr which is the child document of es_visit and we are searching the es_visit by the above query.

It's obviously that the q1 and q2 are exactly the same query in this case, but they're searched with a different term query in each has_child query.

q1 is searched with doc_type == 'XXX' and q2 is searched with doc_type == 'YYY' and there's no way for the field doc_type to match both XXX and YYY in one document.

The strange thing I found is that I got matched query [ "q1", "q2" ] in child1, but q2 is not defined in child1.
</comment><comment author="lipixun" created="2015-11-10T04:04:15Z" id="155278026">The returned es raw result (Only the inner_hits part of a hitted item):

```
"child1": {
    "hits": {
        "hits": [
        {
            "_type": "es_emr",
            "_routing": "someid",
            "_index": "indexname",
            "_score": 4.782811,
            "_source": { ... },
            "_parent": "The parent id",
            "highlight": {
            "text": [
                ....
            ]
            },
            "_id": "The document id",
            "matched_queries": [
            "q1",
            "q2"
            ]
        }
        ],
        "total": 1,
        "max_score": 4.782811
    }
}
```

@clintongormley 
</comment><comment author="clintongormley" created="2015-11-17T13:30:46Z" id="157371150">@lipixun you're really going to need to provide a full recreation so that I don't have to guess about anything and can just run it.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch fails to start tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14573</link><project id="" key="" /><description>Hi folks,

I'm trying to start tribe node using following config:

&lt;pre&gt;
transport.tcp.port: 9301
http.port: 9201
network.host: 0.0.0.0
path.data: /var/lib/elasticsearch/
path.logs: /var/log/elasticsearch/

tribe:
    kibana:
        cluster.name: logstash-kibana
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["127.0.0.1"]
    els:
        cluster.name: logstash-data
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.128.69.48", "10.128.75.237"]
&lt;/pre&gt;

This config resides in the file /etc/tribe-elasticseach/elasticsearch.yml. I'm starting it using following command:

&lt;pre&gt;
sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -Ddefault.path.conf=/etc/tribe-elasticsearch/
&lt;/pre&gt;

Elasticsearch fails with following output:

&lt;pre&gt;
[2015-11-05 17:07:42,433][INFO ][node                     ] [Bucky] version[2.0.0], pid[25943], build[de54438/2015-10-22T08:09:48Z]
[2015-11-05 17:07:42,434][INFO ][node                     ] [Bucky] initializing ...
[2015-11-05 17:07:42,596][INFO ][plugins                  ] [Bucky] loaded [], sites []
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/usr/share/elasticsearch/config/elasticsearch.yml" "read")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
        at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
        at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
        at java.nio.file.Files.exists(Files.java:2385)
        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:87)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.tribe.TribeService.&lt;init&gt;(TribeService.java:136)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
&lt;/pre&gt;

I'm not sure why it tries to access /usr/share/elasticsearch/config/elasticsearch.yml. There is no such file in the elasticsearch deb package. I created this file, but command above still fails with same output. Please advise how this can be resolved.

I'm running elasticsearch 2.0.0 installed from the debian package downloaded from the official site. I'm using ubuntu 14.

Thanks,
Kirill.
</description><key id="115413338">14573</key><summary>elasticsearch fails to start tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">kt97679</reporter><labels><label>:Packaging</label><label>:Tribe Node</label><label>bug</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-06T01:17:02Z</created><updated>2016-03-02T07:28:24Z</updated><resolved>2015-12-08T16:07:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T11:36:07Z" id="155037224">You're specifying the custom config file location incorrectly.  

See https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_setting_changes.html#_custom_config_file
</comment><comment author="kt97679" created="2015-11-09T19:05:07Z" id="155157461">Hi @clintongormley , thanks for quick response. As you can see from the description I've provided I was using option -Ddefault.path.conf. I tried again same command with option --path.conf. There was no exception because of config access issue, but I had to specify also --path.data and --path.logs because for some reason those settings were ignored in the config I've provided. In my config I also specify nonstandard ports to use and those settings are also not used. Any advise what can be wrong?

Thanks,
Kirill.
</comment><comment author="kt97679" created="2015-11-09T19:23:19Z" id="155164496">Looks like config is ignored completely. If I specify all options via command line I still get exception like above:

&lt;pre&gt;
# sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch --path.conf=/etc/tribe-elasticsearch/ --path.logs=/var/log/elasticsearch --path.data=/var/lib/elasticsearch/ --transport.tcp.port=9301 --http.port=9201 --network.host=0.0.0.0 --tribels.cluster.name=logstash-data --tribe.els.discovery.zen.ping.multicast.enabled=false --tribe.els.discovery.zen.ping.unicast.hosts=["10.128.69.48","10.128.75.237"]                                                                                                              
log4j:WARN No appenders could be found for logger (bootstrap).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/usr/share/elasticsearch/config/elasticsearch.yml" "read")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
        at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
        at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
        at java.nio.file.Files.exists(Files.java:2385)
        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:87)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.tribe.TribeService.&lt;init&gt;(TribeService.java:136)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
&lt;/pre&gt;
</comment><comment author="clintongormley" created="2015-11-17T12:55:24Z" id="157363763">Thanks for persisting.  I've managed to replicate this and it is indeed a bug.

When the tribe node attempts to instantiate a node for the tribe service, it checks for access to the config directory, but that setting is no longer available to it and so it defaults to checking for path.home.

This can be replicated with a simple config file, saved as `foo/elasticsearch.yml`:

```
node.name: foo

tribe:
    foo:
        cluster.name: bar
```

Start elasticsearch as:

```
./elasticsearch-2.0.0/bin/elasticsearch --path.conf foo/
```

And it fails with:

```
[2015-11-17 13:54:47,763][INFO ][node                     ] [foo] version[2.0.0], pid[5940], build[de54438/2015-10-22T08:09:48Z]
[2015-11-17 13:54:47,763][INFO ][node                     ] [foo] initializing ...
[2015-11-17 13:54:47,836][INFO ][plugins                  ] [foo] loaded [], sites []
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/Users/clinton/workspace/servers/elasticsearch-2.0.0/config/elasticsearch.yml" "read")
  at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
  at java.security.AccessController.checkPermission(AccessController.java:884)
  at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
  at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
  at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
  at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
  at java.nio.file.Files.exists(Files.java:2385)
  at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:87)
  at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
  at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
  at org.elasticsearch.tribe.TribeService.&lt;init&gt;(TribeService.java:136)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
  at &lt;&lt;&lt;guice&gt;&gt;&gt;
  at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
  at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
  at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
  at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
  at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</comment><comment author="clintongormley" created="2015-11-17T13:02:57Z" id="157365004">@javanna could you take a look at this please?
</comment><comment author="javanna" created="2015-11-17T18:36:50Z" id="157463650">I had a look at this. Only selected settings are forwarded to the inner tribe clients from the tribe node. `path.home` is one of them but `path.conf` is not. That said, if I remember correctly the tribe clients shouldn't read from configuration file (and sysprops) but only inherit a few settings from the parent node (like it happens in `TribeService`), something that we had enforced with #9721. I think something got lost with #13383 where `loadConfigSettings` was removed, which was our way to prevent loading anything from the config file. With that set to `false` I believe we wouldn't even check for the existence of the file, thus we wouldn't need any permission for that. At this point it seems to me that we would have to forward `path.conf` to the tribe clients just because we are going to check for its existence at some point although we have nothing to load from it (otherwise we check for path.home that we have no permissions for)? I think I'd need @rjernst to verify if what I explained makes any sense, it might be that I overlooked something.
</comment><comment author="rjernst" created="2015-11-17T21:21:02Z" id="157510683">If I understand the tribe node correctly, it is no different than any other client node (well, creating multiple client nodes internally). So to me, it should be passing along any settings it needs to configure the node (including `path.conf`).  However, I'm not sure what this has to do with the transport client? The transport client by definition now does not use the config file settings (and the stack trace shown above indicates the exception was from building a node, not a transport client).
</comment><comment author="javanna" created="2015-11-18T17:50:54Z" id="157798793">&gt; However, I'm not sure what this has to do with the transport client?

@rjernst it doesn't have to do directly with the transport client, but the inner tribe nodes have a similar requirement when it comes to loading from config file. They should not be reading out of the config file but only inherit some selected settings from their "parent" node (the actual tribe node), and that is why we were previously setting `loadConfigSettings` to `false`, which is now removed though. If my analysis is correct security manager barfs because we check if the config file exists while creating inner client nodes as part of `TribeService`, but we shouldn't need to read from that file at that point anyway. I could forward  the `path.conf` setting to the client nodes too, but I feel it is not the right fix given that we should not be reading from that file nor check if it exists. Not sure what the right fix is though.
</comment><comment author="javanna" created="2015-11-19T14:28:21Z" id="158072847">I looked deeper, I can confirm this is not just a problem around passing in the right `path.conf` to the inner nodes. The inner client nodes must not read from the main configuration file, something that was fixed in #9721. The option to not load from config settings for a node was though removed with #13383. I had expected `TribeUnitTests` to fail after that change but it doesn't unfortunately. If you try setting for instance `transport.tcp.port` in the configuration file, the tribe node will get that port, but the inner nodes will try to get that one too and will fail. The inner nodes should only get some selected settings from their parent node but never read from config file or system properties.
</comment><comment author="ESamir" created="2015-11-19T15:21:35Z" id="158087609">+1
Removing the path.conf did not resolve the issue 

the config used 

```
bootstrap:
  mlockall: true
cluster:
  name: tribe.elk.h2.com
discovery:
  zen:
    minimum_master_nodes: 2
    ping:
      unicast:
        hosts:
             - h2-clt01
             - h2-clt02
             - h2-clt03
network:
  host: h2-clt01
node:
  data: false
  master: true
  name: h2-ct01-h2-ct01
path:
  data: /data/h2-ct01
tribe:
  h2:
    cluster:
      name: elk.h2.com
    discovery:
      zen:
        ping:
          unicast:
            hosts:
                 - h2-cm01
                 - h2-cm02
                 - h2-cm03
  h3:
    cluster:
      name: elk.h3.com
    discovery:
      zen:
        ping:
          unicast:
            hosts:
                 - h3-cm01
                 - h3-cm02
                 - h3-cm03
```
</comment><comment author="clintongormley" created="2015-11-19T15:42:04Z" id="158094212">There is a workaround for this bug.  Assuming your tribe config directory is `/etc/tribe/`:

```
cd /etc
cp -a /etc/tribe /etc/tribe-client
echo "" &gt; /etc/tribe-client/elasticsearch.yml
chown -R elasticsearch /etc/tribe-client
```

Then edit  `/etc/tribe/elasticsearch.yml` and specify a `path.conf` for each tribe cluster, eg:

```
# arbitrary config
transport.tcp.port: 9301
http.port: 9201
network.host: 0.0.0.0
path.data: /var/lib/elasticsearch/
path.logs: /var/log/elasticsearch/

tribe:
    kibana:
        path.conf: /etc/tribe-client  ### ADD THIS LINE
        cluster.name: logstash-kibana
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["127.0.0.1"]
    els:
        path.conf: /etc/tribe-client  ### ADD THIS LINE
        cluster.name: logstash-data
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.128.69.48", "10.128.75.237"]
```

Then start elasticsearch as:

```
./bin/elasticsearch --path.conf /etc/tribe
```

The tribe node will use `/etc/tribe/` as its config directory.  Then the tribe node starts a node client for each cluster, and will use `/etc/tribe-client` as its config directory, but `/etc/tribe-client/elasticsearch.yml` is empty, so no settings will be loaded.
</comment><comment author="javanna" created="2015-11-19T16:24:43Z" id="158107977">Workaround above works, the only caveat is that depending on where the additional empty configuration file is located, we might not have the permissions to read from it. I think it should work if we simply add an empty configuration file under the tribe node config and point right to it, not only specifying its parent directory but the complete path that includes the filename:

```
tribe.t1.path.conf: /path/to/config/tribe.yml
tribe.t2.path.conf: /path/to/config/tribe.yml
```
</comment><comment author="ppf2" created="2015-11-19T17:48:42Z" id="158134081">Ran into this last night when attempting to set up a tribe node on 2.0.  This will also affect users who attempt to set a custom transport.tcp.port for the tribe node.  In this case, setting a custom transport.tcp.port for the tribe node causes a misleading `BindException[Address already in use];` exception when the port specified is not actually already in use.

```
cluster.name: elasticsearch_2_0_0_tribe_cluster
network.host: 127.0.0.1
transport.tcp.port: 11111
node.name: tribe_cluster_node1
tribe:
  t1:
    cluster.name: elasticsearch_2_0_0_cluster1
  t2:
    cluster.name: elasticsearch_2_0_0_cluster2
```

Settings for the 2 clusters:

```
cluster.name: elasticsearch_2_0_0_cluster2
network.host: 127.0.0.1
transport.tcp.port: 9301
http.port: 9201
node.name: cluster2_node1
```

and 

```
cluster.name: elasticsearch_2_0_0_cluster1
network.host: 127.0.0.1
transport.tcp.port: 9300
http.port: 9200
node.name: cluster1_node1
```

The problem is that the tribe node will not start up as long as I have the transport.tcp.port: 11111 in place.  If I don't set a custom transport port for the tribe node, it starts up fine and can connect with the 2 clusters.

The following is the error that shows up when I attempt to set transport.tcp.port for the tribe node.  Note that prior to starting the tribe node, I used lsof to confirm that there's no process on the machine using port 11111 (and it doesn't matter what port I set it to, as long as transport.tcp.port is set for the tribe node, it will throw the same bind exception).

```
[2015-11-19 01:09:12,816][DEBUG][discovery.zen.elect      ] [tribe_cluster_node1/t1] using minimum_master_nodes [-1]

[2015-11-19 01:09:12,816][DEBUG][discovery.zen.ping.unicast] [tribe_cluster_node1/t1] using initial hosts [127.0.0.1, [::1]], with concurrent_connects [10]

[2015-11-19 01:09:12,817][DEBUG][discovery.zen            ] [tribe_cluster_node1/t1] using ping.timeout [3s], join.timeout [1m], master_election.filter_client [true], master_election.filter_data [false]

[2015-11-19 01:09:12,817][DEBUG][discovery.zen.fd         ] [tribe_cluster_node1/t1] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]

[2015-11-19 01:09:12,817][DEBUG][discovery.zen.fd         ] [tribe_cluster_node1/t1] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]

[2015-11-19 01:09:12,820][DEBUG][script                   ] [tribe_cluster_node1/t1] using script cache with max_size [100], expire [null]

[2015-11-19 01:09:12,853][DEBUG][cluster.routing.allocation.decider] [tribe_cluster_node1/t1] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]

[2015-11-19 01:09:12,853][DEBUG][cluster.routing.allocation.decider] [tribe_cluster_node1/t1] using [cluster_concurrent_rebalance] with [2]

[2015-11-19 01:09:12,854][DEBUG][cluster.routing.allocation.decider] [tribe_cluster_node1/t1] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]

[2015-11-19 01:09:12,855][DEBUG][gateway                  ] [tribe_cluster_node1/t1] using initial_shards [quorum]

[2015-11-19 01:09:12,885][DEBUG][indices.recovery         ] [tribe_cluster_node1/t1] using max_bytes_per_sec[40mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]

[2015-11-19 01:09:12,886][DEBUG][indices.store            ] [tribe_cluster_node1/t1] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [10gb]

[2015-11-19 01:09:12,886][DEBUG][indices.memory           ] [tribe_cluster_node1/t1] using indexing buffer size [99mb], with indices.memory.min_shard_index_buffer_size [4mb], indices.memory.max_shard_index_buffer_size [512mb], indices.memory.shard_inactive_time [5m], indices.memory.interval [30s]

[2015-11-19 01:09:12,887][DEBUG][indices.cache.query      ] [tribe_cluster_node1/t1] using [node] query cache with size [10%], actual_size [99mb], max filter count [1000]

[2015-11-19 01:09:12,887][DEBUG][indices.fielddata.cache  ] [tribe_cluster_node1/t1] using size [-1] [-1b], expire [null]

[2015-11-19 01:09:12,897][INFO ][node                     ] [tribe_cluster_node1/t1] initialized

[2015-11-19 01:09:12,906][INFO ][node                     ] [tribe_cluster_node1] initialized

[2015-11-19 01:09:12,907][INFO ][node                     ] [tribe_cluster_node1] starting ...

[2015-11-19 01:09:12,924][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500

[2015-11-19 01:09:12,924][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false

[2015-11-19 01:09:12,947][DEBUG][transport.netty          ] [tribe_cluster_node1] using profile[default], worker_count[8], port[11111], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/3/6/1/1], receive_predictor[512kb-&gt;512kb]

[2015-11-19 01:09:12,957][DEBUG][transport.netty          ] [tribe_cluster_node1] binding server bootstrap to: 127.0.0.1

[2015-11-19 01:09:12,985][DEBUG][transport.netty          ] [tribe_cluster_node1] Bound profile [default] to address {127.0.0.1:11111}

[2015-11-19 01:09:12,986][INFO ][transport                ] [tribe_cluster_node1] publish_address {127.0.0.1:11111}, bound_addresses {127.0.0.1:11111}

[2015-11-19 01:09:12,993][DEBUG][discovery.local          ] [tribe_cluster_node1] Connected to cluster [Cluster [elasticsearch_2_0_0_tribe_cluster]]

[2015-11-19 01:09:12,996][INFO ][discovery                ] [tribe_cluster_node1] elasticsearch_2_0_0_tribe_cluster/baK4hDMwRiaKGS5D8ivYng

[2015-11-19 01:09:12,996][WARN ][discovery                ] [tribe_cluster_node1] waited for 0s and no initial state was set by the discovery

[2015-11-19 01:09:12,996][DEBUG][gateway                  ] [tribe_cluster_node1] can't wait on start for (possibly) reading state from gateway, will do it asynchronously

[2015-11-19 01:09:13,010][DEBUG][http.netty               ] [tribe_cluster_node1] Bound http to address {127.0.0.1:22222}

[2015-11-19 01:09:13,011][INFO ][http                     ] [tribe_cluster_node1] publish_address {127.0.0.1:22222}, bound_addresses {127.0.0.1:22222}

[2015-11-19 01:09:13,011][INFO ][node                     ] [tribe_cluster_node1/t2] starting ...

[2015-11-19 01:09:13,016][DEBUG][transport.netty          ] [tribe_cluster_node1/t2] using profile[default], worker_count[8], port[11111], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/3/6/1/1], receive_predictor[512kb-&gt;512kb]

[2015-11-19 01:09:13,022][DEBUG][transport.netty          ] [tribe_cluster_node1/t2] binding server bootstrap to: 127.0.0.1

[2015-11-19 01:09:13,039][INFO ][node                     ] [tribe_cluster_node1/t2] stopping ...

[2015-11-19 01:09:13,041][INFO ][node                     ] [tribe_cluster_node1/t2] stopped

[2015-11-19 01:09:13,042][INFO ][node                     ] [tribe_cluster_node1/t2] closing ...

[2015-11-19 01:09:13,048][INFO ][node                     ] [tribe_cluster_node1/t2] closed

[2015-11-19 01:09:13,048][INFO ][node                     ] [tribe_cluster_node1/t1] closing ...

[2015-11-19 01:09:13,052][INFO ][node                     ] [tribe_cluster_node1/t1] closed

Exception in thread "main" BindTransportException[Failed to bind to [11111]]; nested: ChannelException[Failed to bind to: /127.0.0.1:11111]; nested: BindException[Address already in use];

Likely root cause: java.net.BindException: Address already in use

at sun.nio.ch.Net.bind0(Native Method)

at sun.nio.ch.Net.bind(Net.java:444)

at sun.nio.ch.Net.bind(Net.java:436)

at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)

at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)

at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)

at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)

at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)

at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)

at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)

at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)

at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)

Refer to the log for complete error details.

[2015-11-19 01:09:13,058][INFO ][node                     ] [tribe_cluster_node1] stopping ...

[2015-11-19 01:09:13,064][INFO ][node                     ] [tribe_cluster_node1] stopped

[2015-11-19 01:09:13,064][INFO ][node                     ] [tribe_cluster_node1] closing ...

[2015-11-19 01:09:13,066][INFO ][node                     ] [tribe_cluster_node1] closed
```

Note that I cannot reproduce this on 1.7.2.  On 1.7.2, I can set up a custom transport.tcp.port for the tribe node and it will start up fine.  
</comment><comment author="javanna" created="2015-11-19T23:21:47Z" id="158231737">@ppf2 this happens because the tribe node process will start three nodes, the first one will get the configured port, and the second will try to get the same one as it reads from the same configuration file. The workaround provided by Clint above should work till we fix this properly.
</comment><comment author="rjernst" created="2015-11-19T23:44:40Z" id="158235829">@javanna I am going to explore having the tribe node have its own subclass of Node which can customize this single behavior (how to get the node's settings). I don't think we should add back this general purpose flag as we need to keep the tons of ways Nodes can be configured to a minimum.
</comment><comment author="javanna" created="2015-11-20T00:03:33Z" id="158238947">@rjernst thanks that sounds good to me. 
</comment><comment author="ppf2" created="2015-11-20T00:03:35Z" id="158238954">Confirmed that the workaround works to prevent the BindTransportException error, thx!
</comment><comment author="ppf2" created="2015-11-20T00:27:10Z" id="158242498">@rjernst Do we have a sense of whether the fix will make it to the upcoming 2.1 release? Or will it likely be after 2.1 (i.e. use the workaround until a later 2.x release)?
</comment><comment author="rjernst" created="2015-11-20T01:10:11Z" id="158248522">@ppf2 Definitely after 2.1. I would not want to destabilize 2.1 with a refactoring like this. 
</comment><comment author="ppf2" created="2015-11-20T01:12:15Z" id="158248822">@rjernst sounds good, thx!
</comment><comment author="clintongormley" created="2015-12-02T09:12:57Z" id="161229225">This requires some fairly extensive changes, so we will target this for 2.2.  In the meantime, we should document the workaround in the 2.1 docs.
</comment><comment author="rjernst" created="2015-12-08T04:22:34Z" id="162757384">I opened a PR to fix this here: #15300.

Note that I was able to do the fix simply enough that I think it will be ok to backport to 2.1.x
</comment><comment author="clintongormley" created="2015-12-09T12:15:45Z" id="163207391">thanks @rjernst 
</comment><comment author="ppf2" created="2015-12-10T02:26:47Z" id="163465900">Thanks @rjernst !
</comment><comment author="lb425" created="2015-12-12T23:02:06Z" id="164200558">I'm late to the party but thought this might be useful for anyone coming across this. I found that the dummy config file isn't needed to work around the issue. Instead for creating a new directory (/etc/tribe-client in the example) path.conf can reference the current configuration directory.

Using the above example where the config directory was /etc/tribe

# arbitrary config

transport.tcp.port: 9301
http.port: 9201
network.host: 0.0.0.0
path.data: /var/lib/elasticsearch/
path.logs: /var/log/elasticsearch/

tribe:
    kibana:
        path.conf: /etc/tribe  #
        cluster.name: logstash-kibana
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["127.0.0.1"]
    els:
        path.conf: /etc/tribe  #
        cluster.name: logstash-data
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.128.69.48", "10.128.75.237"]
</comment><comment author="kt97679" created="2015-12-22T00:25:55Z" id="166465839">Is this fixed in 2.1.1?
</comment><comment author="thn-dev" created="2015-12-30T10:50:28Z" id="167976115">With v2.1.1, I still have to specify path.conf and I used the valid path as mentioned above by lb425. In my case, I also had to specify path.plugins for similar reason. Otherwise, I kept getting AccessControlException error.

I did not have to specify both path.conf and path.plugins when I was using v1.7.3
</comment><comment author="thn-dev" created="2015-12-31T14:20:17Z" id="168202787">WRT ES v2.1.1, I have to do the following to get the tribe node talking to two different clusters: cluster A and cluster B

**# tribe node's configuration (elasticsearch.yml)**
**network.host:** 0.0.0.0
**transport.tcp.port:** 9300
**http.port:** 9200
**http.enabled:** true

tribe.**t1**.cluster.name: **&lt;cluster A&gt;**
tribe.**t1**.discovery.zen.ping.unicast.hosts: **&lt;cluster A's master node&gt;**
tribe.**t1**.discovery.zen.ping.multicast.enabled: false
tribe.**t1**.path.conf: **&lt;valid path/to/conf&gt;**
tribe.**t1**.path.plugins: **&lt;valid path/to/plugin&gt;**
tribe.**t1**.network.bind_host: **0.0.0.0**
tribe.**t1**.network.publish_host: **&lt;tribe node's IP&gt;**
tribe.**t1**.transport.tcp.port: **&lt;optional but different from tribe node port above&gt;**

_repeat the same block but replace "t1" to "t2" for cluster B and fill in proper info related to cluster B but keep the tribe.t2.network.\* the same with different tribe.t2.transport.tcp.port value from t1 if specified_
</comment><comment author="rjernst" created="2015-12-31T19:12:18Z" id="168236136">@thn-dev Setting network and path settings for tribe nodes (the t1, t2 here) should not be necessary. Can you share your full elasticsearch.yml for both the tribe node, as well as cluster A and cluster B?
</comment><comment author="thn-dev" created="2016-01-01T00:56:30Z" id="168270016">@rjernst I did not have to do network and path settings when I was using v1.7.3. It was a surprise to me when v2.1.1 kept giving me AccessControlException error message. Initially, it pointed to the "plugins" location, after I set it, it complained about the "config" location. If I did not do the network settings for t1 and t2, it was not able to connect to cluster A and/or B. This part is weird too. Again, I did not have to do this in v1.7.3.

All ES instances are installed using .rpm file, not .zip file.

My settings for tribe node is above with additional parameters
- cluster.name
- discovery.zen.ping.multicast.enabled: false

Cluster A and B, each has 1 master node, 3 data nodes with the following parameters' settings (I don't have all information with me at the moment)
- cluster.name: &lt;cluster A or B&gt;
- network.host: 0.0.0.0
- transport.tcp.port: 9300
- http.port: 9200 (master)
- http.enabled: true (master)
- discovery.zen.ping.multicast.enabled: false
- discovery.zen.ping.unicast.hosts: &lt;master node's IP&gt;
- path.conf: /data/es/config
- path.plugins: /data/es/plugins
- path.data: /data/es
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Simulate Endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14572</link><project id="" key="" /><description>I think it is ready.

Adds `_simulate` endpoint to enable the processing of provided documents without indexing
</description><key id="115407043">14572</key><summary>[Ingest] Simulate Endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-06T00:25:38Z</created><updated>2015-11-13T19:22:34Z</updated><resolved>2015-11-13T19:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-06T18:15:41Z" id="154490243">I did a round of review and left some comments and some questions, but it looks very good already!
</comment><comment author="talevy" created="2015-11-12T05:52:38Z" id="156009347">@martijnvg @javanna I have updated to reflect comments.

a possible followup issue to clean up the `doc` field when `verbose=true` : https://github.com/elastic/elasticsearch/issues/14698
</comment><comment author="martijnvg" created="2015-11-12T06:22:10Z" id="156013179">left a couple as small comments, but this PR looks good.
</comment><comment author="javanna" created="2015-11-12T14:56:41Z" id="156128093">hey @talevy I did an in depth review and left some comments. looks good though! let me know if you have any question.
</comment><comment author="talevy" created="2015-11-13T01:52:22Z" id="156292612">@javanna I don't know what I've done! :smile: 

In trying to clean things up and making them more straightforward to test... I introduced a few things that are not ideal. I left them as inline comments. let me know what you think! I do not think they are that big of a deal, but they added a lot of code.
</comment><comment author="javanna" created="2015-11-13T15:30:09Z" id="156464070">@talevy what you did looks good. I did simplify a few things around serialization in my last commit. I also moved some of the classes to implement Writeable rather than Streamable so we can have final fields and we can drop the default constructors that were needed for serialization only. I removed equals/hashcode when they required exceptions comparisons as that wasn't ideal and used only for testing, which can be simplified. Can you please have a look and tell me what you think?
</comment><comment author="talevy" created="2015-11-13T15:32:40Z" id="156464665">++ LGTM

that commit really addressed what I was trying to clean up
</comment><comment author="javanna" created="2015-11-13T19:07:42Z" id="156528049">I pushed a few more more cleanup commits and left a comment inline on error messages. One more thing is about the fact that we continue processing in case of failure. I understand why but it seems inconsistent given that we do the opposite in reality (when not simulating) till we will add support for continue_on_failure. Seems like this api should behave the same as the real one? What do you think?
</comment><comment author="talevy" created="2015-11-13T19:13:41Z" id="156530829">continuing through the processors is only done during a Verbose execution. It is still the case that the pipeline halts during a regular non-verbose _simulate run
</comment><comment author="javanna" created="2015-11-13T19:18:16Z" id="156532559">&gt; continuing through the processors is only done during a Verbose execution. It is still the case that the pipeline halts during a regular non-verbose _simulate run

ok I guess it's fine for now, we have to remember to adapt it to continue on failure once we add the flag I think
</comment><comment author="talevy" created="2015-11-13T19:19:59Z" id="156532931">&gt; ok I guess it's fine for now, we have to remember to adapt it to continue on failure once we add the flag I think

I agree. I added a note to the `continue_on_failure` ticket: https://github.com/elastic/elasticsearch/issues/14548
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fork Lucene PatternTokenizer to apply LUCENE-6814</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14571</link><project id="" key="" /><description>While LUCENE-6814 fixes #13721, it will only make it into Lucene 5.4. This will make it only available to elasticsearch 2.0.

There appears to be a very low chance of LUCENE-6814 getting fixed in 4.10, which elasticsearch 1.x uses. I've made a copy of Lucene 4.10.4 PatternTokenizer with the fix so that 1.x versions will also be able to benefit.
</description><key id="115403722">14571</key><summary>Fork Lucene PatternTokenizer to apply LUCENE-6814</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">achow</reporter><labels><label>:Core</label><label>bug</label><label>v1.7.4</label><label>v2.0.1</label><label>v2.1.0</label></labels><created>2015-11-05T23:58:28Z</created><updated>2015-12-16T14:53:48Z</updated><resolved>2015-11-06T19:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-06T08:39:27Z" id="154344961">&gt; While LUCENE-6814 fixes #13721, it will only make it into Lucene 5.4. This will make it only available to elasticsearch 2.0.

2.0 is based off Lucene 5.2.1 and 2.1 will be on 5.3 so this will only make it into 2.2 I think we also need to fork this into 2.1 and 2.0? @mikemccand WDYT
</comment><comment author="mikemccand" created="2015-11-06T10:34:00Z" id="154374802">&gt; I think we also need to fork this into 2.1 and 2.0? @mikemccand WDYT

+1

I'll push this to 1.7, 2.0 and 2.1.
</comment><comment author="mikemccand" created="2015-11-06T10:48:08Z" id="154377226">@achow Sorry, one more thing: can you restore the original (Apache) license headers on the forked sources?  Thank you.
</comment><comment author="achow" created="2015-11-06T19:21:20Z" id="154507482">@mikemccand updated.
</comment><comment author="mikemccand" created="2015-11-06T19:35:38Z" id="154511174">Thanks @achow, this looks great, I'll merge shortly!
</comment><comment author="mikemccand" created="2015-11-06T22:17:55Z" id="154559406">Still working to forward port this, struggling with the license checker which is angry that these sources have Apache copyright headers ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi_explain</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14570</link><project id="" key="" /><description>Would it be possible to have an API to request an explain response for multiple document ids at once? There is a bulk API, a multi_get API, and a multi_search API already. From what I can tell, the only options I have right now are

a) Issue multiple explain requests at once
b) Perform a search with explain=true and an additional ID filter

Neither of these are ideal.

Thanks!
</description><key id="115396536">14570</key><summary>multi_explain</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericjohnston1989</reporter><labels /><created>2015-11-05T23:11:17Z</created><updated>2015-11-09T11:24:24Z</updated><resolved>2015-11-09T11:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T11:24:24Z" id="155032569">Hi @ericjohnston1989 

The options you list are correct.  We won't add a bulk explain API as it signals the wrong intent.  Explain is a heavy operation and is intended for debugging only, not for use in production at scale.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include plugin descriptor requirement in breaking changes documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14569</link><project id="" key="" /><description>ES node no longer starts up when there is a plugin that does not have the plugin-descriptor.properties set.  While this requirement is documented [here](https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugin-authors.html#_plugin_descriptor_file) and the behavior of the node not starting up at all is intended, it will be helpful to include it also as a breaking change (eg. here: 
https://www.elastic.co/guide/en/elasticsearch/reference/current/_plugin_and_packaging_changes.html#_plugin_and_packaging_changes)

```
Exception in thread "main" java.lang.IllegalStateException: Unable to initialize plugins
Likely root cause: java.nio.file.NoSuchFileException: /path/to/plugins/test/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:315)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:380)
    at java.nio.file.Files.newInputStream(Files.java:106)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:301)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:107)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:268)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```
</description><key id="115387034">14569</key><summary>Include plugin descriptor requirement in breaking changes documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-11-05T22:11:43Z</created><updated>2015-11-09T11:18:09Z</updated><resolved>2015-11-09T11:18:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cross-index bool filter with has_child doesn't work properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14568</link><project id="" key="" /><description>Here is the test case setup: http://paste2.org/wMdx7nv5 
There is a contact -&gt; event parent-child association.
test_event_1 index has the following data: 

``` json
{"index": {"_type": "contact", "_id": "1" }}
{ "contact_id" : 1 }
{"index": {"_type": "event", "_id": "1", "parent": "1" }}
{ "flag" : 1 }
{"index": {"_type": "contact", "_id": "2" }}
{ "contact_id" : 2 }
{"index": {"_type": "event", "_id": "2", "parent": "2" }}
{ "flag" : 1 }
{"index": {"_type": "contact", "_id": "3" }}
{ "contact_id" : 3 }
{"index": {"_type": "event", "_id": "3", "parent": "3" }}
{ "flag" : 1 }
```

test_event_2 has the following data:

``` json
{"index": {"_type": "contact", "_id": "1" }}
{ "contact_id" : 1 }
{"index": {"_type": "event", "_id": "1", "parent": "1" }}
{ "flag" : 0 }
{"index": {"_type": "contact", "_id": "2" }}
{ "contact_id" : 2 }
{"index": {"_type": "event", "_id": "2", "parent": "2" }}
{ "flag" : 0 }
{"index": {"_type": "contact", "_id": "3" }}
{ "contact_id" : 3 }
{"index": {"_type": "event", "_id": "3", "parent": "3" }}
{ "flag" : 1 }
```

The following query should only return `contact_id: 3`, but instead returns `contact_id: 1, 2, 3`

The query: 

``` json
curl -XGET 'localhost:9200/test_events_alias/contact/_search
{
    "query": {
        "filtered": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "has_child": {
                                "type": "event",
                                "filter": {
                                    "bool": {
                                        "must": [
                                            {
                                                "term": {
                                                    "flag": 1
                                                }
                                            },
                                        ]
                                    }
                                }
                            }
                        }
                    ],
                    "must_not": [
                       {
                           "has_child": {
                                "type": "event",
                                "filter": {
                                    "bool": {
                                        "must": [
                                            {
                                                "term": {
                                                    "flag": 0
                                                }
                                            }

                                        ]
                                    }
                                }
                            }
                       }
                    ]
                }
            }
        }
    },
    "aggs": {
        "contact_ids": {
            "terms": {
                "field": "contact_id",
                "size": 1000
            }
        }
    }
}
```

Here is the query result: http://paste2.org/n5vwfc5x
Tested on ES 1.4 and 1.7
</description><key id="115379011">14568</key><summary>Cross-index bool filter with has_child doesn't work properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bogdanovich</reporter><labels /><created>2015-11-05T21:31:00Z</created><updated>2015-11-09T11:11:34Z</updated><resolved>2015-11-09T11:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T11:11:34Z" id="155030650">The parent-child relationship only works in a single index.  The child docs need to be in the same shard as the parent doc.  The contact doc with ID 1 in the test_event_1 index is completely independent of the contact doc with ID 1 in the test_event_2 index.

This is working exactly as expected.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster discovery does not work if plugins are not identical</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14567</link><project id="" key="" /><description>In this scenario:

A cluster with plugins installed (in this case license and marvel), appears to be working correctly. When a node without these plugins trys to connect to the cluster, the connection does not work correctly. Here is the logs:

http://pastie.org/private/bokrdcgsjncicplwzm0rg

Here is a trace log:

https://gist.github.com/benjfield/87f19d8c6c5afec124cc

From the nodes with the plugins it appears to be connected:

https://gist.github.com/benjfield/3706b893f42d8701f698

However from the broken node:

{"error":{"root_cause":[{"type":"master_not_discovered_exception","reason":"waited for [30s]"}],"type":"master_not_discovered_exception","reason":"waited for [30s]"},"status":503}
</description><key id="115362938">14567</key><summary>Cluster discovery does not work if plugins are not identical</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benjfield</reporter><labels /><created>2015-11-05T20:04:50Z</created><updated>2016-06-09T12:46:02Z</updated><resolved>2015-11-09T10:49:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T10:49:22Z" id="155026686">Yes, you need to install the licensing plugin on all nodes.  The transport client doesn't require licensing though.
</comment><comment author="benjfield" created="2015-11-09T11:23:37Z" id="155032441">Can this error message be improved to tell you this? Me and a guy on IRC spent the best part of the day working through potential issues on this before we discovered this problem. I'm also not sure this is documented on the website.
</comment><comment author="clintongormley" created="2015-11-09T13:36:31Z" id="155064599">Yes, there's already an issue open for that: https://github.com/elastic/elasticsearch/issues/13445

sorry, should have linked to it
</comment><comment author="crickes" created="2015-12-05T18:43:03Z" id="162234062">This one just caught me out too.
</comment><comment author="filipegmiranda" created="2016-06-09T12:46:02Z" id="224883673">Does this happen to all plugins we install in Elasticsearch?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.IllegalStateException: Type names equal for class LongFieldType and TTLFieldType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14566</link><project id="" key="" /><description>In class MappedFieldType method checkTypeName throws the above exception when the other class is a child of the parent class. I am specifically seeing this issue in the case of the TTLFieldType which is a child LongFieldType.

This check...

 } else if (getClass() != other.getClass()) {

...should maybe be changed to...
 } else if (!(getClass() instanceof other.getClass())) {

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
</description><key id="115362338">14566</key><summary>java.lang.IllegalStateException: Type names equal for class LongFieldType and TTLFieldType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rileytj</reporter><labels /><created>2015-11-05T20:01:19Z</created><updated>2016-07-20T11:58:55Z</updated><resolved>2015-11-05T20:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-05T20:47:29Z" id="154185578">This needs to stay an exact equality check. ttl and long are not exactly the same. They could have different underlying field type settings available. The data types of fields must be exactly identical across document types.
</comment><comment author="rileytj" created="2015-11-05T22:15:05Z" id="154212232">I understand what you are implying but...

If this method stays exactly as it's written, one could never submit a TTL value in the form {"_ttl": 1000}. In this example the value is a type long but the field class does not align.

http://stackoverflow.com/questions/4989818/instanceof-vs-getclass
</comment><comment author="ronlepper" created="2015-11-05T22:28:45Z" id="154215208">I'm having a similar problem after upgrading my test suite to use the elasticsearch 2.0 jars.  So how would you necessarily set TTLFieldType so that is not a long field type?  Would it need to be in timestamp form?  
</comment><comment author="rjernst" created="2015-11-05T22:45:34Z" id="154219853">@rileytj That should not be the case, `_ttl` is a builtin field. The mapping is not dynamically added, so whether the value is passed as a string or a number in json, it should coerce to a number.

@ronlepper Overriding the builtin `_ttl` field is not supported right now. If you need custom ttl behavior, you will need to implement this yourself. But TTL is generally a bad practice: use time-based indices instead.
</comment><comment author="rileytj" created="2015-11-05T23:54:58Z" id="154236666">See https://github.com/elastic/elasticsearch/issues/11882
</comment><comment author="Rajind" created="2016-07-20T11:58:55Z" id="233929011">I'm getting this exception on field ttl. how can i fix this ? I'm on 2.3

```
MapperParsingException[failed to parse]; nested: IllegalStateException[Mixing up field types: class org.elasticsearch.index.mapper.core.LongFieldMapper$LongFieldType != class org.elasticsearch.index.mapper.internal.TTLFieldMapper$TTLFieldType on field _ttl];
at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:154)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:529)
at org.elasticsearch.index.shard.IndexShard.prepareCreateOnPrimary(IndexShard.java:506)
at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:215)
at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224)
at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326)
at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:119)
at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: Mixing up field types: class org.elasticsearch.index.mapper.core.LongFieldMapper$LongFieldType != class org.elasticsearch.index.mapper.internal.TTLFieldMapper$TTLFieldType on field _ttl
    at org.elasticsearch.index.mapper.FieldMapper.updateFieldType(FieldMapper.java:397)
    at org.elasticsearch.index.mapper.FieldMapper.updateFieldType(FieldMapper.java:53)
    at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:622)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:442)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:262)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
    ... 18 more
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date parse error on range query in es 2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14565</link><project id="" key="" /><description>I have a weird problem where Java 8 adds a plus sign to iso dates if the year is more than 4 characters

```
LocalDateTime.of(10000, 01, 01, 0, 0).toString()
produces "+10000-01-01T00:00:00.000Z"
LocalDateTime.of(9999, 01, 01, 0, 0).toString()
produces "9999-01-01T00:00:00.000Z"
```

Before Elasticsearch 2.0 passing this into a range query on a date field was no problem either way but this started breaking during the migration to es 2:

This filter:

```
{
    "range":{
        "event_at":{
            "from":"0000-01-01T00:00:00.000Z",
            "to":"+10000-01-01T00:00:00.000Z"
        }
    }
}
```

Now generates the following error:

```
{
    "shard":0,
    "index":"inbot_activities_v29",
    "node":"4pFF0w30RO6ahMk9OEvqWQ",
    "reason":{
        "type":"parse_exception",
        "reason":"failed to parse date field [+10000-01-01T00:00:00.000Z] with format [strict_date_optional_time||epoch_millis]",
        "caused_by":{
            "type":"illegal_argument_exception",
            "reason":"Invalid format: \"+10000-01-01T00:00:00.000Z\" is malformed at \"-01-01T00:00:00.000Z\""
        }
    }
}
```

The workaround is obvious and already implemented on my side (-1 to 9999).

But this error was sufficiently weird that I wanted to report it. Maybe it can be fixed by moving from joda to java 8's new date API as I think has been debated already. Alternatively the date pattern used by default could be adapted to allow the + sign.
</description><key id="115360354">14565</key><summary>date parse error on range query in es 2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels><label>:Dates</label><label>discuss</label></labels><created>2015-11-05T19:50:55Z</created><updated>2015-11-09T12:28:44Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lynneker7" created="2015-11-05T20:16:39Z" id="154178282">I have similar problem but a little different. In ElasticSearch 2.0 latest. 

Document:

&lt;pre&gt;
&lt;code&gt;
{
    "date": "2015-11-05T05:33:54.431Z",
    "product_id": "27d898e783a16492cafc25a0f9a415d67970b811"
 }
&lt;/code&gt;
&lt;/pre&gt;

Query

&lt;pre&gt;
&lt;code&gt;
{
  "query": {
    "bool": {
      "must": [
        {
          "match_all": {}
        }
      ],
      "filter": [
        {
          "terms": {
            "product_id": [
              "27d898e783a16492cafc25a0f9a415d67970b811"
            ]
          }
        },
        {
          "range": {
            "date": {
              "lte": "2015-11-05"
            }
          }
        }
      ]
    }
  }
}
&lt;/code&gt;
&lt;/pre&gt;

Result:

&lt;pre&gt;
&lt;code&gt;
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": null,
    "hits": []
  }
}
&lt;/code&gt;
&lt;/pre&gt;
</comment><comment author="jillesvangurp" created="2015-11-05T21:58:10Z" id="154208192">@lynneker7 that looks like the problem is with your query. Change lte to gte and you should see the document since the timestamp is after the queried timestamp.
</comment><comment author="lynneker7" created="2015-11-06T12:19:44Z" id="154396098">@jillesvangurp Sorry you're absolutely right. Thanks 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore build properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14564</link><project id="" key="" /><description>This commit restores the build properties provided in
org.elasticsearch.Build. This class previously obtained the build hash
and timestamp from a resource es-build.properties that was included in
the jar and produced by the Maven resources plugin. After the switch to
Gradle, the production of this file was lost and these build properties
defaulted to &#8220;NA&#8221; in all instances.

The most important place that the build hash is used is in the plugin
manager to determine the URL of staging artifacts for plugins.

The build hash is also used in several responses including the /_nodes
response and the response to HTTP GET requests on the root path.

These properties can now be obtained from the jar manifest as they are
currently placed there by the gradle-info plugin. However, only the
short hash is provided. We now read the manifest for these properties
and no longer provide the full hash in responses to HTTP GET requests
on the root path.
</description><key id="115338753">14564</key><summary>Restore build properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-05T17:54:57Z</created><updated>2015-11-06T02:34:09Z</updated><resolved>2015-11-06T02:34:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-05T17:58:31Z" id="154138291">Fine by me.
</comment><comment author="rmuir" created="2015-11-05T18:01:24Z" id="154139009">do we know what uses these? we also have this information (and more) in the jar metadata.
</comment><comment author="jasontedor" created="2015-11-05T18:03:41Z" id="154139548">@rmuir It's used when you hit the root path against an Elasticsearch node. So today if you hit the root path against an Elasticsearch node built from master you will see

```
    "build_hash" : "${buildNumber}",
    "build_timestamp" : "NA",
```

in the response. This comes from the included resource `es-build.properties` having contents

```
version=${project.version}
hash=${buildNumber}
timestamp=${timestamp}
```

 With this pull request you will see

```
    "build_hash" : "b328e26160b47abe904dfd08e920d31f668b7e57",
    "build_timestamp" : "2015-11-05T17:45:12Z",
```

and `es-build.properties` now looks like

```
#Thu Nov 05 12:45:19 EST 2015
version=3.0.0-SNAPSHOT
timestamp=1446745512970
hash=b328e26160b47abe904dfd08e920d31f668b7e57
```

The relevant code is in [`Build`](https://github.com/elastic/elasticsearch/blob/b328e26160b47abe904dfd08e920d31f668b7e57/core/src/main/java/org/elasticsearch/Build.java#L42) and [`RestMainAction`](https://github.com/elastic/elasticsearch/blob/b328e26160b47abe904dfd08e920d31f668b7e57/core/src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java#L80-L81).
</comment><comment author="rmuir" created="2015-11-05T18:20:40Z" id="154143712">Right, but it could alternatively get it from jar metadata? and then we don't need this file?
</comment><comment author="rjernst" created="2015-11-05T19:06:22Z" id="154155126">I agree with @rmuir, we should use the jar manifest if possible.
</comment><comment author="jasontedor" created="2015-11-05T19:13:53Z" id="154158190">@rmuir @rjernst I've pushed commit 45475fa1cdfc48b84542377edb81cc88b4674e53 reverting the previous changes to the build and added e5b0d38e4ed617d7f7e39ba9e5682c660cfab625 to obtain these properties from the manifest.
</comment><comment author="rjernst" created="2015-11-05T19:22:59Z" id="154161914">LGTM
</comment><comment author="jasontedor" created="2015-11-06T02:18:27Z" id="154263746">The only place that the full hash was used is in the response to HTTP GET requests on the root path. All other places used the short hash. This means that we can safely use the gradle-info plugin (which provides the short hash as the `Change` attribute in the jar manifest) and just break the fact that the response to HTTP GET against the root path included the full hash.
</comment><comment author="rjernst" created="2015-11-06T02:22:51Z" id="154264252">Lgtm, thanks Jason
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add cluster-wide setting for total shard limit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14563</link><project id="" key="" /><description>This adds the `cluster.routing.allocation.total_shards_per_node`
setting, which limits the total number of shards across all indices on
each node. It defaults to -1 and can be dynamically configured.

Resolves #14456
</description><key id="115333003">14563</key><summary>Add cluster-wide setting for total shard limit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-05T17:25:16Z</created><updated>2015-11-09T18:23:13Z</updated><resolved>2015-11-09T18:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-06T12:04:17Z" id="154393819">I left some comments but this part of the code is out of my comfort zone so it would be nice that someone else look at it. Instead of having -1 as a default, I'm wondering if we should have a crazy high default value instead (say 10000?) that might already protect users, for instance if they are not aware of the cost of shards yet or if a buggy script starts creating indices in a loop because of the auto-create index feature? (I don't have a strong opinion here, just raising the idea for discussion)
</comment><comment author="ywelsch" created="2015-11-06T15:10:24Z" id="154433297">Added a few comments. I would not to inflate the indexShardLimit number in `canRemain`, but add a boolean parameter to `checkLimits`. This keeps decision messages sane but needs a distinction on the last if block.
</comment><comment author="dakrone" created="2015-11-06T16:52:35Z" id="154469600">I pushed a change addressing the feedback. Additionally, I changed the single method out into three distinct methods because, as @ywelsch mentioned, having a single one could lead to confusing messages as far as numbers are concerned.

Thanks for feedback @jpountz and @ywelsch 
</comment><comment author="jpountz" created="2015-11-09T10:52:30Z" id="155027340">LGTM from my end
</comment><comment author="ywelsch" created="2015-11-09T13:43:05Z" id="155066154">LGTM 2. One last nitpick (but feel free to push): I was confused a bit by the variable names `nodeCount` and `totalCount`, maybe they could be renamed to sth like `indexShardCount` and to `nodeShardCount` ?
</comment><comment author="jpountz" created="2015-11-09T13:44:26Z" id="155066431">+1 on the variable name change
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Dedicated Backward Compatibility Testing to GeoPointV2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14562</link><project id="" key="" /><description>This issue is to add testing which ensures that geopoint doc values work correctly under the following scenarios:
- pre-2.2 index with only pre 2.2 segments
- pre 2.2 index with a mix of pre and post 2.2. segments
- post 2.2 index
</description><key id="115331676">14562</key><summary>Add Dedicated Backward Compatibility Testing to GeoPointV2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Fielddata</label><label>:Geo</label><label>test</label></labels><created>2015-11-05T17:18:34Z</created><updated>2016-02-21T18:13:41Z</updated><resolved>2016-02-21T18:13:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T15:42:48Z" id="183906638">@nknize can this be closed now?
</comment><comment author="nknize" created="2016-02-21T18:13:41Z" id="186876043">Completed in #14667 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>publish_address should default to bound_address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14561</link><project id="" key="" /><description>When `network.publish_host` is not specified in a configuration, ES should try to reuse the value set for `network.bind_host`.

The current behaviour is that for following configration

```
....
network.bind_host: 10.88.12.218
# network.publish_host: NOT SET
```

The ES node will bind to correct network interface, but it will publish a different one (not sure how it's currently determined - ?first non-loopback inet? ). Which doesn't seem to make much sense.

Relevant log line:

```
[2015-11-05 14:47:52,744][INFO ][transport                ] [...] bound_address {inet[/10.88.12.218:9300]}, publish_address {inet[/10.88.64.171:9300]}
```

Workround is easy. Just set both `bind_host` and `publish_host` to a same value.
</description><key id="115295585">14561</key><summary>publish_address should default to bound_address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">aleszoulek</reporter><labels><label>:Network</label><label>docs</label></labels><created>2015-11-05T14:31:31Z</created><updated>2015-12-18T09:16:54Z</updated><resolved>2015-12-18T09:16:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-05T14:35:13Z" id="154077579">This stuff is fixed in 2.0. basically the code in 1.x is geared around the default of binding to `0.0.0.0`, so that logic there for publish address "works in most cases".

In 2.0, if you don't set publish address, we look at the actual addresses bound, sort them by ipv4/ipv6 stack preference, then by reachability scope, and select the first one.
</comment><comment author="nik9000" created="2015-11-05T14:35:44Z" id="154077752">This is indeed confusing. You are supposed to set `network.host` 99% of the time and ignore the other two. But the docs aren't great there.
</comment><comment author="rmuir" created="2015-11-05T14:39:25Z" id="154078660">&gt; This is indeed confusing. You are supposed to set network.host 99% of the time and ignore the other two. But the docs aren't great there.

Agreed, besides the problems in 1.x, I think its good for publish host selection to check bind_host before host in case the user did just this. There is no need to default publishing to host directly, to something we potentially didnt bind to, unless someone explicitly set publish host.
</comment><comment author="aleszoulek" created="2015-11-05T15:13:18Z" id="154088452">That's my fault indeed. I should have be more careful when reading an example configuration. I've seen the "Network And HTTP" section and the first option that's there is `#network.bind_host:` and this is what I've set. If I'd read the rest of the config file, there's just few lines bellow:

```
# Set both 'bind_host' and 'publish_host':
# 
# network.host: 192.168.0.1
```

Although it would be nice if it did what @rmuir described (check bind_host in publish host selection).
</comment><comment author="rmuir" created="2015-11-05T15:18:56Z" id="154089916">Yeah, if its easy to do that check, we should do it. anything we can do to make it less confusing. 

The documentation really needs improvement here (we've seen other problems). Basically network.host should be the recommended way, and the bind/publish stuff should be buried to the bottom of the page in some kind of advanced options section: you shouldn't mess with that unless you have a more complex configuration (like proxies, static NAT, something like that)
</comment><comment author="clintongormley" created="2015-12-18T09:16:54Z" id="165720892">Closed by #15360
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can't install sqlite plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14560</link><project id="" key="" /><description>Hi all, I'm trying to install the sqlite plugin (using Elasticsearch 2.0.0 on Ubuntu 14.04).
I followed the [docs](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-sqlite.html#plugins-inputs-sqlite) but I get the error shown below.

```
-&gt; Installing logstash-input-sqlite... 
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/logstash-input-sqlite/2.0.0/logstash-input-sqlite-2.0.0.zip ... 
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/logstash-input-sqlite/2.0.0/logstash-input-sqlite-2.0.0.zip]; 
nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/logstash-input-sqlite/2.0.0/logstash-input-sqlite-2.0.0.zip]; 
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```

Any advice on how to install the plugin would be very appreciated.
Thanks
</description><key id="115291950">14560</key><summary>can't install sqlite plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreufirefly</reporter><labels /><created>2015-11-05T14:15:52Z</created><updated>2015-11-05T16:20:29Z</updated><resolved>2015-11-05T16:20:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-05T16:20:29Z" id="154110170">It's a logstash plugin. Not elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarification about precedence of settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14559</link><project id="" key="" /><description>Resulting from discussion with @boaz
</description><key id="115285469">14559</key><summary>Clarification about precedence of settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>docs</label><label>review</label></labels><created>2015-11-05T13:40:15Z</created><updated>2016-03-10T13:30:14Z</updated><resolved>2016-03-10T13:30:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T09:32:25Z" id="155008760">What about transient cluster settings? I think we should explain how they interact with persistent and config settings (and I'm not entirely sure how that logic works).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase suggester collate doesn&#8217;t work in search template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14558</link><project id="" key="" /><description>First of all, I'm using Elasticsearch 1.7.3

I'm facing issues with `collate` in Phrase Suggester.

Please see this reproduction:

```
PUT /test/test/1
{
  "CompanyName": "coca-cola"
}

POST /test/_search
{
  "size": 0,
  "suggest": {
    "DidYouMean": {
      "text": "coce",
      "phrase": {
        "field": "_all",
        "size": 50,
        "direct_generator": [
          {
            "field": "_all",
            "suggest_mode": "popular",
            "min_word_length": 3
          }
        ],
        "collate": {
          "query": {
            "match": {
              "CompanyName": {
                "query": "{{suggestion}}",
                "operator": "and"
              }
            }
          },
          "prune": true
        }
      }
    }
  }
}
```

It brings me this result with `"collate_match": true`:

```
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "suggest": {
      "DidYouMean": [
         {
            "text": "coce",
            "offset": 0,
            "length": 4,
            "options": [
               {
                  "text": "coca",
                  "score": 0.6531368,
                  "collate_match": true
               },
               {
                  "text": "cola",
                  "score": 0.5476822,
                  "collate_match": true
               }
            ]
         }
      ]
   }
} 
```

But when I create a template using this query:

```
PUT /_search/template/suggest
{
  "template": {
    "size": 0,
    "suggest": {
      "DidYouMean": {
        "text": "{{SearchPhrase}}",
        "phrase": {
          "field": "_all",
          "size": 50,
          "direct_generator": [
            {
              "field": "_all",
              "suggest_mode": "popular",
              "min_word_length": 3
            }
          ],
          "collate": {
            "query": {
              "match": {
                "CompanyName": {
                  "query": "{{suggestion}}",
                  "operator": "and"
                }
              }
            },
            "prune": true
          }
        }
      }
    }
  }
}
```

And call it using this:

```
POST /test/_search/template
{
  "template": {
    "id": "suggest"
  },
  "params": {
    "SearchPhrase": "coca cole"
  }
}
```

I'm getting false results:

```
{
   "took": 6,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "suggest": {
      "DidYouMean": [
         {
            "text": "coca cole",
            "offset": 0,
            "length": 9,
            "options": [
               {
                  "text": "coca cola",
                  "score": 0.4727091,
                  "collate_match": false
               },
               {
                  "text": "coca coca",
                  "score": 0.39638618,
                  "collate_match": false
               }
            ]
         }
      ]
   }
}
```

Issue seems to be {{suggestion}} itself. I'm suspecting that when I call a template, it expects `suggestion` parameter from my template query, so it can pass a value. However - it's empty.

As a workaround, I modified template like this:

```
{
  "collate" : {
    "prune" : true,
    "query" : {
      "match" : {
        "_all" : {
          "query" : "{{=&lt;% %&gt;=}}{{suggestion}}&lt;%={{ }}=%&gt;"
        }
      }
    }
  }
}
```

Query was expecting a parameter, which was not supplied. Special characters (braces) had to be escaped, now it seems to perform right.

Can anyone confirm it's correct approach?
</description><key id="115283543">14558</key><summary>Phrase suggester collate doesn&#8217;t work in search template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">buinauskasevaldas</reporter><labels /><created>2015-11-05T13:28:04Z</created><updated>2015-11-09T09:22:00Z</updated><resolved>2015-11-09T09:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-09T09:22:00Z" id="155004918">That's one approach.  Another is simply to escape the `{}` as:

```
"query": "\\{\\{suggestion\\}\\}",
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log cluster health status changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14557</link><project id="" key="" /><description>With this commit, cluster health status changes are logged on INFO
level to spot changes easier for diagnostics.

Closes #11657
</description><key id="115274252">14557</key><summary>Log cluster health status changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Logging</label><label>breaking</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-05T12:33:41Z</created><updated>2016-03-11T08:58:42Z</updated><resolved>2015-11-16T14:16:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-05T12:33:57Z" id="154046348">I've extracted `ClusterHealth` from `ClusterHealthResponse` which contained all relevant stuff, so even though a lot of classes are touched, this is mainly due to the move. I've also implemented the straightforward approach that @nik9000 has sketched. I think it's much more robust in terms of possible events that can cause cluster health state changes (i.e. if there is a new condition on which the cluster health could change, this will just work without any changes).

Can somebody please review, maybe @nik9000?
</comment><comment author="bleskes" created="2015-11-05T13:19:01Z" id="154057302">I had a look and my concern about this approach is that it goes through all the cluster state for every change - may it be a setting change, a relocation on so forth. It also logs things on all the nodes - not sure if that was the intention. Folding this in the allocation service will allows us to also log why it became red/green which I think will make the feature more powerful. 
</comment><comment author="danielmitterdorfer" created="2015-11-06T07:37:09Z" id="154335638">Thanks for your feedback. I am about to finish another issue soon and will be looking into this one next.
</comment><comment author="danielmitterdorfer" created="2015-11-09T08:38:59Z" id="154997375">Hi @bleskes! I reimplemented the changes now in AllocationService. Only relevant changes are logged now. This means that (by intention) we also only log when the routing table has changed. Can you have a second look please?
</comment><comment author="bleskes" created="2015-11-09T16:12:46Z" id="155109950">looks great. Left some minor comments here and there..
</comment><comment author="danielmitterdorfer" created="2015-11-12T20:08:45Z" id="156220270">@bleskes Your review comments should now be reflected in the code. Can you have another look please? I have already adapted the changes made in #14699.
</comment><comment author="bleskes" created="2015-11-16T10:56:55Z" id="156991893">LGTM. Left minor comments. No need for another review after they are fixed.
</comment><comment author="dadoonet" created="2016-03-09T19:14:08Z" id="194459341">@danielmitterdorfer This change is a breaking change in the Java API. See discussion happening here: https://discuss.elastic.co/t/java-client-2-1-1-2-2-0-unexpected-compilation-errors-not-backward-compatible/43782

It means that people can't upgrade the cluster without upgrading the client as well.

We should either document it or fix it IMO.

@clintongormley thoughts?
</comment><comment author="joelstewart" created="2016-03-09T20:40:54Z" id="194498002">AFAIK, upgrading the server separately is not actually a problem.  I can run the 2.1.1 client builds against 2.2.0 server and get expected results.  The rolling release I was referring to had to do with development of internal products, I had no intention to cause undue alarm...

However, there is a problem:

1) Having built client libraries to 2.1.1, there is a compile problem when changing dependency to 2.2.0.  This was unexpected in a minor release.
2) When assembling a client library built to 2.1.1, into an app using 2.2.0 (runtime uses 2.2.0 due to maven dependency mediation) there are runtime problems.

See @dadoonet 's link for example code.
</comment><comment author="jprante" created="2016-03-09T21:10:57Z" id="194508275">@dadoonet I do not think there is a real fix for having moved a class around, it would be another surprise the class was moved back.

I expect Java API breakages between any release IMHO, and always upgrade clients &amp; servers to the exact same minor version. Documentation is always good. Maybe java developers should be warned in a special Java API breaking change notes each time for a release, but a general advisory what release numbers really mean would also be helpful.
</comment><comment author="dadoonet" created="2016-03-09T21:11:28Z" id="194508408">Indeed. False alarm. The serialization between the client and the nodes are still compatibles.

So it's only a concern on the client side which is easy to fix.

You have here to fix the client library and update it to 2.2 I guess.
</comment><comment author="clintongormley" created="2016-03-10T09:45:47Z" id="194758924">@danielmitterdorfer i've marked this as breaking.  Could you add a line to the breaking changes docs for 2.2.0 please?
</comment><comment author="danielmitterdorfer" created="2016-03-11T07:48:55Z" id="195238753">@clintongormley I've created PR #17066. Can you have a look please?
</comment><comment author="danielmitterdorfer" created="2016-03-11T08:58:42Z" id="195264741">PR is now closed and merged to master, 2.x and 2.2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mGET API doesn't have an option to not return metadata fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14556</link><project id="" key="" /><description>In the GET API you can specify the URL as `/{index}/{type}/{id}/_source` and it will return only the _source in the response (i.e. will not return the metadata fields such as _id and _type). This is done by the RestGetSourceAction and is only available in the REST API (AFAIK).

This option does not seem to be available on the mGET API. You cannot set /_source on the request to exclude the metadata fields from the responses. You also cannot specify this on each get individually.

I wonder if we should add this? I can't think of a concrete use case for this but it is an inconsistency between the GET and mGET APIs. Marking this issue as 'Discuss' to see if there is any usecases which this would be useful
</description><key id="115268426">14556</key><summary>mGET API doesn't have an option to not return metadata fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:CRUD</label><label>adoptme</label><label>enhancement</label></labels><created>2015-11-05T11:52:17Z</created><updated>2016-02-14T15:40:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-06T10:24:55Z" id="154372838">We discussed this today and we all agreed that we don't like the inconsistency. On the one hand this is a nice usecase for response filtering (https://github.com/elastic/elasticsearch/pull/14390) but it would still have `_source : {}` in the response. On the other hand having dedicated REST handlers is not nice either so I wonder if we can implement this as a general feature on GET and push the real rendering in to `GetResponse#toXContent()` and pass a boolean via the `Params` if we only want the source or skip metadata? We can then also deprecate the `/_source` Url and just use a param instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch lat/lon to geohash conversion issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14555</link><project id="" key="" /><description>ES:

``` json
"version" : {
    "number" : "1.5.2",
    "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    "build_timestamp" : "2015-04-27T09:21:06Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  }
```

Logstash:1.4.2

We do receive location as a string in our logs (JSON formatted) as `"appLocation":"51.506411,-0.08811"`
- Logstash script :

```
ruby {
      code =&gt; "event['geoip.location'] = event['appLocation'].split(',').map(&amp;:to_f)"
    }
```
- Kibana display data as well formatted geo_point :
  Index settings tab :
  `geoip.location     geo_point`
  Discover tab :
  `geoip.location = 51.506411, -0.08811`
- Elasticsearch mapping :

``` json
"geoip"  : {
           "type" : "object",
             "dynamic": true,
             "properties" : {
               "location" : { "type" : "geo_point" }
             }
         }
```
- Elasticsearch document query returns :
  `"geoip.location":[51.506411,-0.08811]`
- On Kibana, visualize tile map :
  Result when mouseover point : `{"lat": -0.08789, "lon": -0.17}`
  Same result in all precisions
  Point on the exact center of the map (?)

`GET/`

``` json
{
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "*"
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "gte": 1446572700000,
                  "lte": 1446745560000
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "aggs": {
    "2": {
      "geohash_grid": {
        "field": "geoip.location",
        "precision": 4
      }
    }
  }
}
```

Returns :

``` json
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "2": {
      "buckets": [
        {
          "key": "7zzz",
          "doc_count": 1
        }
      ]
    }
  }
}
```
- Revert 7zzz =&gt; [-0.09,-0.18]
</description><key id="115264379">14555</key><summary>Elasticsearch lat/lon to geohash conversion issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Shamanoid</reporter><labels /><created>2015-11-05T11:24:35Z</created><updated>2015-11-05T16:47:46Z</updated><resolved>2015-11-05T16:47:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Shamanoid" created="2015-11-05T16:47:37Z" id="154118380">Solved :

```
ruby {
       code =&gt; "event['geoip[location[lat]]']= event['appLocation'].split(',').map(&amp;:to_f)[0]"
     }
ruby {
       code =&gt; "event['geoip[location[lon]]']= event['appLocation'].split(',').map(&amp;:to_f)[1]"
     }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce strict pipeline configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14554</link><project id="" key="" /><description>PR for #14552
</description><key id="115242842">14554</key><summary>Enforce strict pipeline configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-11-05T09:31:41Z</created><updated>2015-11-06T04:23:23Z</updated><resolved>2015-11-05T16:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-05T12:55:18Z" id="154050933">left a couple of questions, looks good though, thanks @martijnvg !
</comment><comment author="martijnvg" created="2015-11-05T16:14:41Z" id="154108276">Thanks @javanna, I updated the PR.
</comment><comment author="javanna" created="2015-11-05T16:17:44Z" id="154109459">LGTM thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove transport profiles and the 87 ways to configure network settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14553</link><project id="" key="" /><description>Transport profiles:

These are really complicated, its hard for anyone to digest NettyTransport, and they lead to bugs like https://github.com/elastic/elasticsearch/pull/14535 (i think that is damage from the last time they were touched).

Overengineered and unnecessary, in a place where simplicity is key: no brainer.

Other network settings:

Every other setting looks like this:
        this.maxChunkSize = settings.getAsBytesSize("http.netty.max_chunk_size", settings.getAsBytesSize("http.max_chunk_size", new ByteSizeValue(8, ByteSizeUnit.KB)));

why do we need 2 ways to do it? It just makes things more complicated and there are far too many settings even if there is only one way for each.
</description><key id="115228839">14553</key><summary>remove transport profiles and the 87 ways to configure network settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>:Settings</label><label>v6.0.0</label></labels><created>2015-11-05T07:58:29Z</created><updated>2017-05-03T06:55:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-05T12:22:10Z" id="154044255">I recommend just having this as a breaking change for 3.0: I will work on removing this stuff. Its a new major release and thats why we have the ability to introduce breaking changes.

This stuff is really bad and causing a lot of problems.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strict pipeline / processor configuration validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14552</link><project id="" key="" /><description>Processor configuration validation should be strict:
- Improper use of options should result in failure.
- Use of non existing options should result in failure too.

Also the configuration validation should happen within the put pipeline api. Right now no validation happens, and the nodes running this plugin will fail silently in the background (pipeline doesn't get added and an error is printed in the log files)
</description><key id="115228349">14552</key><summary>Strict pipeline / processor configuration validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-05T07:55:43Z</created><updated>2015-11-05T16:40:18Z</updated><resolved>2015-11-05T16:40:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-05T16:40:18Z" id="154116277">added to feature branch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rename the `ingest` parameter to `pipeline_id` parameter </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14551</link><project id="" key="" /><description>This is a more descriptive name.
</description><key id="115225382">14551</key><summary>rename the `ingest` parameter to `pipeline_id` parameter </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-05T07:37:20Z</created><updated>2015-11-05T16:15:49Z</updated><resolved>2015-11-05T16:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-05T07:39:31Z" id="153978359">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cant not max id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14550</link><project id="" key="" /><description>use this DSL to request , result is  OK  &#12290; IN elasticsearch 1.7

```
{
    "size": 0,
    "query": {
        "max_id": {
            "max": {
                "field": "_id"
            }
        }
    }
}
```

And , Result in elasticsearch 1.5  is  error info .   I make sure id in es is Long type.

```
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[ZuqtC5UWSM-eUcBYweP6lA][listening_v2_201511][0]: RemoteTransportException[[es15][inet[/xxxx:9300]][indices:data/read/search[phase/query]]]; nested: SearchParseException[[listening_v2_201511][0]: from[-1],size[0]: Parse Failure [Failed to parse source [{\n    \"size\": 0,\n    \"query\": {\n        \"max_id\": {\n            \"max\": {\n                \"field\": \"_id\"\n            }\n        }\n    }\n}]]]; nested: QueryParsingException[[listening_v2_201511] No query registered for [max_id]]; }{[bgtZjiyeT9inmfWPWUWUSQ][listening_v2_201511][1]: RemoteTransportException[[es18][inet[/xxxxxx:9301]
```
</description><key id="115224623">14550</key><summary>cant not max id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rfyiamcool</reporter><labels /><created>2015-11-05T07:32:08Z</created><updated>2015-11-09T09:39:01Z</updated><resolved>2015-11-09T09:39:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-11-05T15:33:17Z" id="154094921">Are you trying to execute an aggregation?  That syntax looks like an aggregation, but you've placed it under the `query` parameter.  Try:

``` json
{
    "size": 0,
    "aggs": {
        "max_id": {
            "max": {
                "field": "_id"
            }
        }
    }
}
```
</comment><comment author="clintongormley" created="2015-11-09T09:39:00Z" id="155010061">Yep, looks like bad syntax.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>only allow code to bind to the user's configured port numbers/ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14549</link><project id="" key="" /><description>Random code shouldn't be listening on sockets elsewhere.

Today its the wild west, but we only need to grant access to what the user configured.

This means e.g. multicast plugin has to declare its intentions in its security.policy
</description><key id="115223317">14549</key><summary>only allow code to bind to the user's configured port numbers/ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-05T07:20:57Z</created><updated>2015-11-08T17:26:24Z</updated><resolved>2015-11-07T17:23:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-05T07:33:31Z" id="153977554">I think this is great (though I'm not an expert on the security config code). Left one comment ...
</comment><comment author="rmuir" created="2015-11-05T07:36:55Z" id="153977950">I added a comment. just to be clear here, when binding, code just calls http://docs.oracle.com/javase/8/docs/api/java/lang/SecurityManager.html#checkListen-int- passing the port number. so the "localhost" you see in the permission is not really relevant (the actual address being bound to is not checked by java security, only the port number).
</comment><comment author="s1monw" created="2015-11-06T20:37:48Z" id="154525349">LGTM please push as is
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest Plugin] continue_on_failure parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14548</link><project id="" key="" /><description>It would be nice to have a flag for ingest pipelines that run during index/bulk requests to allow for some configuration of how errors are handled during ingestion.

Currently, any exception that is caused during an ingest pipeline, the pipeline halts and the document being ingested is not indexed. Some environments may prefer to not block on such failures and continue to index the failed document anyways. For these scenarios it would be nice to have an `continue_on_failure` flag turned on to instruct Elasticsearch to index the document as it was before it was processed by the ingest pipeline. Such documents can be tagged so that they can be found in the index for re-processing.

`_simulate` should also support this flag to help with recreating pipeline behavior

depends on https://github.com/elastic/elasticsearch/pull/14888
</description><key id="115219548">14548</key><summary>[Ingest Plugin] continue_on_failure parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-11-05T06:43:32Z</created><updated>2016-01-11T17:09:21Z</updated><resolved>2016-01-11T17:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-05T07:10:58Z" id="153974910">@talevy maybe we should name the param `continue_on_failure`?
</comment><comment author="talevy" created="2015-11-05T07:14:25Z" id="153975325">deal! I'll update
</comment><comment author="javanna" created="2015-11-06T16:55:52Z" id="154471283">I think this flag is needed to decide whether to go on with following processors in the pipeline and still index the current document. It nicely applies to the index api, and the bulk api in the context of each single document.

What currently happens when something fails while indexing using bulk though, is that we fail the whole request, which doesn't seem convenient. I think ideally we should be able to go ahead with the other documents and add the failure to the response for that specific document even when `continue_on_failure` is set to `false`. This might be a high hanging fruit though and might require a separate issue/discussion, as `continue_on_failure` requires some more thinking on how to apply it to the bulk api.
</comment><comment author="martijnvg" created="2015-11-09T06:49:27Z" id="154966619">I think there is no doubt about whether we need this option.

I agree that in the bulk api providing item level granularity control for this flag is trickier, because we we can't set the failure response item in the bulk response. We can only decide whether to delegate to the bulk api. To achieve this I think we also need to have logic in `IngestActionFilter#apply(String, ActionResponse, ActionListener, ActionFilterChain)` (which currently just delegates).
</comment><comment author="javanna" created="2015-11-24T21:00:59Z" id="159403769">We sorted out the bulk api aspect with #14888, meaning that whenever a processor fails for a document it won't affect the other documents within the same bulk request.

In this issue we want focus on the behaviour for each single document. Let's say one processor fails as part of the pipeline execution, we want to be able to control what happens with that single document. The main questions are: should we go ahead with the following processors in the pipeline and index the document? If so how do we keep track of the fact that something failed?

**halt on failure**: if we don't continue, we return an error in the response (either the index api fails or that item will be marked as failed as part of the bulk response) and the client knows what went wrong. The document doesn't get indexed. I can't think of anything more that could be done in this case. This is how things currently work already.

**continue on failure**: if we continue, the client will not be notified of the error, hence we have to at least tag the document so that we keep track of what went wrong. We may also want to index the document somewhere else (different index). This requires additional configuration (which index and which field keeps track of what failed) hence we need to decide how to make it configurable.

**the error**:  easier things first, it seems clear that we want to be able to add a structured error to the document in a certain field (potentially configurable?), actually an array of errors given that more processors can fail at the same time if we continue on failure. Each item would be an object containing the pipeline id, the processor id and structured information about the error, similar to the structured errors that elasticsearch now returns from the rest layer.

Should the possible actions be limited to the ones described (pick the index, eventually pick the error field), or should we allow to configure a specific pipeline to be executed on failure so that whatever can be done through processors can be done on failure as well? The latter seems very flexible, maybe too flexible? Also potentially dangerous in case that pipeline ends up failing (on failure). The former is simpler and might be limiting but I think it would be good to try and come up with more options that we would need before going for the whole pipeline.

Another question is whether this additional on failure configuration (either a pipeline or specific options) should be at the processor level (each processor could take different actions) or unified for a pipeline. I tend to think that the latter is simpler to configure and covers most of the cases, with the additional benefit that it unifies what happens on failure for every processor in the pipeline. Having potentially a different behaviour for each processor might get confusing. Like the previous point, we should come up with good reasons why each processor should have a different behaviour, otherwise keep this at the pipeline level.

**on success**: in case everything works properly, do we need additional configuration for actions to be taken in case of success too? I don't think we do because in case of success the next processor in the chain will be executed, thus that next processor can be seen as the natural "on_success" processor.

I am marking this for discussion, it would be great to gather other thoughts on it.
</comment><comment author="uboness" created="2015-11-25T10:19:02Z" id="159562646">wondering if it'd be sufficient to have the following logic:
- by default, a failure in a processor fails the document processing in the pipeline.
- it should be possible to set the following on every processor:
  
  ``` json
  "on_error" : {
    "skip" : true | false,
    "mark" : true | false,
    "redirect" : &lt;another pipeline name&gt;
  }
  ```
- `skip` means the document continues to the next processor in the pipeline
- `mark` means the document is marked as failed (in a special meta data field)
- `redirect` will indicate that the document should be redirected to another pipeline. This will allow to have a "failure" pipeline that can potentially change the index name to a "failures" index and serve as a "catch all failures" strategy for multiple pipelines.
- `skip` and `redirect` are mutually exclusive.

also, perhaps there should be an option to set the `redirect_on_error` at the pipeline level?
</comment><comment author="javanna" created="2015-11-25T10:36:30Z" id="159566368">Could we unify skip and mark, meaning that whenever we continue, we always have to mark otherwise the failure is lost? Would it make sense to continue without marking?

My other concern is still whether a redirect pipeline is needed or we can trim down what can be done to a few specific options instead. What you propose should be enough, not sure if too much? What are the usecases for a pipeline? Do you have examples in mind?
</comment><comment author="uboness" created="2015-11-25T10:57:39Z" id="159571481">&gt; Could we unify skip and mark, meaning that whenever we continue, we always have to mark otherwise the failure is lost? Would it make sense to continue without marking?

we could have `mark` set to `true` by default... I don't think it hurts (don't know all use cases... but what if you have a processor that is "a nice to have enrichment" and don't care much for errors there.

&gt; My other concern is still whether a redirect pipeline is needed or we can trim down what can be done to a few specific options instead. What you propose should be enough, not sure if too much? What are the usecases for a pipeline? Do you have examples in mind?

tbh, I don't know what's enough. @talevy  can probably provide more input on that from the field. The rationale with a redirect pipeline is that it's as flexible as you want it and utilizes the same infra that we already have (pipelines).
</comment><comment author="talevy" created="2015-12-01T02:20:05Z" id="160826868">After some discussion, a sort of consensus around an `on_failure` block for all processors to have was decided on, instead of a `continue_on_failure` flag described by this issue.

An example of this is as follows:

```
{
  description: "my pipeline with on_failure",
  processors: [
    {
      "grok" : {
          ...
          on_failure : [
            { "set" : { "grok.failed" : true } },
            { "meta": { "_index" : "failed-index" } }
          ]
      }
    },
    {
       "date" : {
         ...
       }
    }
  ]
}
```

an `on_failure` will also be added to a pipeline as a whole:

```
{
   "description" : "...",
   "processors" : [ ... ],
   "on_failure" : [ ... ]
}
```

processors without an `on_failure` parameter defined will throw an exception and exit the pipeline immediately. processors with `on_failure` defined will catch the exception and allow for further processors to run. Exceptions within the `on_failure` block will be treated the same as the top-level.

If a user wishes to handle a failure, and still exit the pipeline immediately, a `fail` processor will be introduced. This processor will do nothing but throw an exception so that the pipeline exits.
</comment><comment author="talevy" created="2015-12-03T22:03:29Z" id="161799133">Since processors could exit with an exception at any point of their run time. The state of the IngestDocument can be incomplete. Since all existing processors are being moved to operate on one field at a time, this is not too much of an issue, but that is not a required rule. For example, a processor may be interested in added two fields, but fail while attempting to add the second field. This leaves the IngestDocument in a partially updated state before it was to be passed along to the `on_failure` block of processors. Is this a concern? If so, I am not sure what the best way of handling this would be. I believe it would require some "shadow" change-list that gets bundled up into a single transaction upon successful execution of a processor, otherwise there would not be a simple way to revert partial changes within a processor.

##### UPDATE: ignoring for now. partial updates will be allowed, initially.
</comment><comment author="javanna" created="2016-01-11T17:09:21Z" id="170620979">This has been implemented, every processor can be associated with a list of processors to be executed on failure. Same for a pipeline. The processor one has the precedence.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add back support for tests.jvm.argline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14547</link><project id="" key="" /><description /><key id="115218113">14547</key><summary>Build: Add back support for tests.jvm.argline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-05T06:32:20Z</created><updated>2015-11-05T07:59:46Z</updated><resolved>2015-11-05T07:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-05T06:42:17Z" id="153970696">+1

we can close https://github.com/elastic/elasticsearch/issues/14541
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Max doc frequency percentage in More Like This query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14546</link><project id="" key="" /><description>Closes #14114
</description><key id="115215603">14546</key><summary>Max doc frequency percentage in More Like This query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Michael-karn-ivanov</reporter><labels><label>:More Like This</label><label>enhancement</label><label>feedback_needed</label><label>review</label></labels><created>2015-11-05T06:08:24Z</created><updated>2016-09-12T21:21:05Z</updated><resolved>2016-09-12T21:21:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Michael-karn-ivanov" created="2015-11-05T06:12:24Z" id="153966401">Just signed CLA ..
</comment><comment author="danielmitterdorfer" created="2016-02-26T15:32:15Z" id="189323907">@Michael-karn-ivanov: Sorry, that we did not come back to you before. I have checked what's wrong with your CLA. You have signed the CLA with your gmail address but have committed the change with your work address (Microsoft domain). Can you please check whether your work email appears in the [email settings of your Github profile](https://github.com/settings/emails)? If not, can you please add it as secondary email (which is hidden by Github)? We will then rerun the CLA check again and it should then work.
</comment><comment author="clintongormley" created="2016-03-10T12:39:08Z" id="194822328">@danielmitterdorfer the CLA is now signed.  Could you take a look at this again?
</comment><comment author="danielmitterdorfer" created="2016-03-10T12:51:52Z" id="194829456">@Michael-karn-ivanov: I had a quick look at it. Can you please rebase on current master and also add documentation for the new parameter in `docs/reference/query-dsl/mlt-query.asciidoc`? After the rebase I'll have a closer look.
</comment><comment author="dakrone" created="2016-09-12T21:21:05Z" id="246497818">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bad docs for installing marvel with offline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14545</link><project id="" key="" /><description>The following steps:

```
bin/plugin install license file:///path/to/file/license-2.0.0.zip
bin/plugin install marvel-agent file:///path/to/file/marvel-agent-2.0.0.zip
bin/kibana plugin --install marvel --url file:///path/to/file/marvel-2.0.0.tar.gz
```

don't work on elasticsearch:2.0.0 docker image
source: https://www.elastic.co/guide/en/marvel/current/installing-marvel.html#offline-installation

Output:

```
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.0.0/license-2.0.0.zip ...
Downloading .......DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.0.0/license-2.0.0.zip checksums if available ...
Downloading .DONE
Installed license into /usr/share/elasticsearch/plugins/license
-&gt; Installing marvel-agent...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.0.0/marvel-agent-2.0.0.zip ...
Downloading .........DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.0.0/marvel-agent-2.0.0.zip checksums if available ...
Downloading .DONE
Installed marvel-agent into /usr/share/elasticsearch/plugins/marvel-agent
exec: "/usr/share/elasticsearch/bin/kibana": stat /usr/share/elasticsearch/bin/kibana: no such file or directory
```

Expecting:
- license and marvel-agent to NOT be downloaded
- bin/kibana to be present
</description><key id="115214659">14545</key><summary>Bad docs for installing marvel with offline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aanm</reporter><labels /><created>2015-11-05T05:56:47Z</created><updated>2016-07-04T10:35:32Z</updated><resolved>2015-12-04T08:36:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-05T05:59:29Z" id="153964978">To run marvel Kibana application you need to have Kibana! :)

May be we should add a line telling that to users? 
</comment><comment author="aanm" created="2015-11-05T06:00:11Z" id="153965047">@dadoonet So this doesn't work with docker where you have a container for kibana and a container for elasticsearch?
</comment><comment author="aanm" created="2015-11-05T06:02:50Z" id="153965345">@dadoonet ah, it means for kibana I need to install marvel on its container. But still the `bin/plugin install license file:///path/to/file/license-2.0.0.zip` should be `bin/plugin install file:///path/to/file/license-2.0.0.zip`.
Thanks
</comment><comment author="dadoonet" created="2015-11-05T06:17:08Z" id="153966893">Agreed. Same for marvel-agent BTW
</comment><comment author="tlrx" created="2015-12-04T08:36:55Z" id="161908676">Documentation has been fixed, thanks
</comment><comment author="jalpeshshelar" created="2016-07-04T10:35:32Z" id="230260331">How to install Marvel on Kibana in windows ?

bin/kibana plugin --install marvel --url file:///path/to/file/marvel-2.0.0.tar.gz
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryWhileUnderLoadIT#recoverWhileRelocating test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14544</link><project id="" key="" /><description>These fails sometime on CI on 2.x and 2.0 branches only:
- http://build-us-00.elastic.co/job/es_core_2x_metal/428/ (most recent)
- http://build-us-00.elastic.co/job/elasticsearch-20-oracle-jdk6/1477/
- http://build-us-00.elastic.co/job/elasticsearch-20-centos/397/

In the last part of the test, after bumping up the number of replicas the the expected number of documents seem not to be the same on all shards.
</description><key id="115213315">14544</key><summary>RecoveryWhileUnderLoadIT#recoverWhileRelocating test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label></labels><created>2015-11-05T05:42:12Z</created><updated>2016-03-22T18:13:07Z</updated><resolved>2016-02-14T15:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T15:36:33Z" id="183906298">Old test report. Closing
</comment><comment author="imotov" created="2016-03-22T16:42:46Z" id="199897059">Failed again on Mar 21 and again today. See https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-os-compatibility/os=centos/58/console and https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.2+multijob-os-compatibility/os=ubuntu/29/console
</comment><comment author="bleskes" created="2016-03-22T18:13:07Z" id="199946715">I pushed some [changes](https://github.com/elastic/elasticsearch/commit/ba4b321af33e73d6f933a7bde9a5f3b1c8d29a7b) to hopefully help debug when it happens again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Server Crash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14543</link><project id="" key="" /><description>I have three elastic servers all masters having total 32 GB ram each,16 GB allocated to elastic servers.
From last two days all elastic servers crashes 

Am using virtual environment with network file system to store data and logs
java version :8.60
Elastic search version is 1.7.0
Indices count:around 300
Total documents in whole elastic search : around 60 million

Below is my config

cluster.name: Cluster1

cluster.routing.allocation.disk.threshold_enabled: false
script.disable_dynamic: false
node.name: "Master1"

node.master: true
node.data: true

index.query.bool.max_clause_count: 50100
indices.fielddata.cache.size: 25%
indices.fielddata.cache.expire: 5m
action.disable_delete_all_indices: true
indices.cluster.send_refresh_mapping: false
index.cache.field.type: soft

path.data: \nas5\Elasticsearch\Data

path.logs: \nas5\Elasticsearch\Logs\Master1

bootstrap.mlockall: true

http.max_content_length: 999mb

indices.recovery.max_bytes_per_sec: 100mb

indices.recovery.concurrent_streams: 5

above config goes same for all three servers Master1,Master2,Master3.

here is my error log

```
 #
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_IN_PAGE_ERROR (0xc0000006) at pc=0x000000000404153b, pid=1820, tid=4464
#
# JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode windows-amd64 compressed oops)
# Problematic frame:
# J 10733 C2 org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Lorg/apache/lucene/store/Directory;Ljava/lang/String;Lorg/elasticsearch/common/collect/ImmutableMap$Builder;Lorg/elasticsearch/common/logging/ESLogger;Lorg/apache/lucene/util/Version;Z)V (290 bytes) @ 0x000000000404153b [0x00000000040405a0+0xf9b]
#
# Failed to write core dump. Call to MiniDumpWriteDump() failed (Error 0x8007001e: The system cannot read from the specified device.

)
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread (0x000000002e6d0000):  JavaThread "elasticsearch[Master101][fetch_shard_store][T#42]" daemon [_thread_in_Java, id=4464, stack(0x00000008fa2c0000,0x00000008fa3c0000)]

siginfo: ExceptionCode=0xc0000006, ExceptionInformation=0x0000000000000000 0x0000000031a08000 0x00000000c000020c 

Registers:
RAX=0x0000000000000000, RBX=0x0000000000007ffe, RCX=0x0000000000007ffe, RDX=0x00000003cfb6dc98
RSP=0x00000008fa3be9a0, RBP=0x00000007c045ee70, RSI=0x00000003cfb6dc10, RDI=0x000000000000002e
R8 =0x00000003cfb6dba8, R9 =0x0000000031a00000, R10=0x00000003cfb6dc98, R11=0x0000000000000000
R12=0x0000000000000000, R13=0x00000003cfb6d918, R14=0x00000008fa3be800, R15=0x000000002e6d0000
RIP=0x000000000404153b, EFLAGS=0x0000000000010246

Top of Stack: (sp=0x00000008fa3be9a0)
0x00000008fa3be9a0:   ffffffffffffffff 00000003cfb69b30
0x00000008fa3be9b0:   0000000000000010 000000000273ca1c
0x00000008fa3be9c0:   79f6b17bf808bdce 00000003cfb4acd0
0x00000008fa3be9d0:   0000000454c19650 00000003cfb57c50
0x00000008fa3be9e0:   0000000300000000 00000003cfb58bd8
0x00000008fa3be9f0:   00000003cfb6dc98 0000000000008006
0x00000008fa3bea00:   00000007c00016d0 00000003cfb6dc98
0x00000008fa3bea10:   00000003cfb6d180 0000000002847c08
0x00000008fa3bea20:   00000007c000da48 0000000000000000
0x00000008fa3bea30:   00000003cfb594d8 0000000002ff9138
0x00000008fa3bea40:   00000003cfb59478 00000003cfb6a2b8
0x00000008fa3bea50:   00000003cfb6a588 0000000300000001
0x00000008fa3bea60:   00000003cfb6a308 00000000028c2170
0x00000008fa3bea70:   00000003cfb4fb40 0000000379f68d28
0x00000008fa3bea80:   0000000000000023 000000220211a5c0
0x00000008fa3bea90:   0000000079f6da2d 000000000470b460 

Instructions: (pc=0x000000000404153b)
0x000000000404151b:   8b 58 1c 41 8b 48 18 44 2b d9 41 83 fb 08 0f 8c
0x000000000404152b:   7e 17 00 00 4d 8b 48 10 45 0f b6 58 2a 48 63 d9
0x000000000404153b:   4d 8b 0c 19 83 c1 08 41 89 48 18 45 85 db 0f 85
0x000000000404154b:   b6 17 00 00 49 0f c9 4d 8b 5f 60 4d 8b d3 49 81 


Register to memory mapping:

RAX=0x0000000000000000 is an unknown value
RBX=0x0000000000007ffe is an unknown value
RCX=0x0000000000007ffe is an unknown value
RDX=
[error occurred during error reporting (printing register info), id 0xc0000005]

Stack: [0x00000008fa2c0000,0x00000008fa3c0000],  sp=0x00000008fa3be9a0,  free space=1018k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  0x000000000404153b


---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x000000002ebc2000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#53]" daemon [_thread_blocked, id=4700, stack(0x00000009c5da0000,0x00000009c5ea0000)]
  0x000000002ebbe000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#52]" daemon [_thread_blocked, id=4176, stack(0x00000009042d0000,0x00000009043d0000)]
  0x000000002ebc2800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#51]" daemon [_thread_blocked, id=2692, stack(0x0000000039830000,0x0000000039930000)]
  0x000000002ebbd800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#50]" daemon [_thread_blocked, id=2180, stack(0x00000008f9560000,0x00000008f9660000)]
  0x000000002c881800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#49]" daemon [_thread_blocked, id=1308, stack(0x00000009c5a20000,0x00000009c5b20000)]
  0x000000002c881000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#48]" daemon [_thread_blocked, id=2796, stack(0x00000008fb750000,0x00000008fb850000)]
  0x000000002c882800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#47]" daemon [_thread_blocked, id=2004, stack(0x00000008fb880000,0x00000008fb980000)]
  0x000000002c880000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#46]" daemon [_thread_blocked, id=3176, stack(0x0000000041540000,0x0000000041640000)]
  0x000000002e6d3800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#45]" daemon [_thread_blocked, id=3236, stack(0x00000008fa3c0000,0x00000008fa4c0000)]
  0x000000002e6d1800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#44]" daemon [_thread_blocked, id=1068, stack(0x0000000040960000,0x0000000040a60000)]
  0x000000002e6d3000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#43]" daemon [_thread_blocked, id=4156, stack(0x00000008f7db0000,0x00000008f7eb0000)]
=&gt;0x000000002e6d0000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#42]" daemon [_thread_in_Java, id=4464, stack(0x00000008fa2c0000,0x00000008fa3c0000)]
  0x000000002e324800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#41]" daemon [_thread_blocked, id=4904, stack(0x000000003e6d0000,0x000000003e7d0000)]
  0x000000002e328000 JavaThread "elasticsearch[Master101][fetch_shard_store][T#40]" daemon [_thread_blocked, id=4092, stack(0x00000000403e0000,0x00000000404e0000)]
  0x000000002e326800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#39]" daemon [_thread_blocked, id=4244, stack(0x00000000410d0000,0x00000000411d0000)]
  0x000000002e323000 JavaThread "elasticsearch[Master101][generic][T#133]" daemon [_thread_blocked, id=588, stack(0x0000000040810000,0x0000000040910000)]
  0x000000002c763800 JavaThread "elasticsearch[Master101][generic][T#130]" daemon [_thread_blocked, id=3480, stack(0x000000003e7d0000,0x000000003e8d0000)]
  0x000000002c764800 JavaThread "elasticsearch[Master101][generic][T#134]" daemon [_thread_blocked, id=4208, stack(0x000000003e5a0000,0x000000003e6a0000)]
  0x000000002c75e800 JavaThread "elasticsearch[Master101][generic][T#135]" daemon [_thread_blocked, id=4696, stack(0x000000003d520000,0x000000003d620000)]
  0x000000002c75d800 JavaThread "elasticsearch[Master101][generic][T#136]" daemon [_thread_blocked, id=4348, stack(0x0000000036cb0000,0x0000000036db0000)]
  0x000000002e4ff000 JavaThread "elasticsearch[Master101][generic][T#137]" daemon [_thread_blocked, id=3708, stack(0x000000003e3c0000,0x000000003e4c0000)]
  0x000000002e4fd000 JavaThread "elasticsearch[Master101][generic][T#129]" daemon [_thread_blocked, id=3216, stack(0x000000003e2c0000,0x000000003e3c0000)]
  0x000000002e4fd800 JavaThread "elasticsearch[Master101][generic][T#131]" daemon [_thread_blocked, id=3132, stack(0x000000003e1c0000,0x000000003e2c0000)]
  0x000000002e500000 JavaThread "elasticsearch[Master101][generic][T#132]" daemon [_thread_blocked, id=3292, stack(0x000000003ce70000,0x000000003cf70000)]
  0x000000002e506000 JavaThread "elasticsearch[Master101][generic][T#128]" daemon [_thread_blocked, id=3588, stack(0x000000003a8b0000,0x000000003a9b0000)]
  0x000000002e502000 JavaThread "elasticsearch[Master101][generic][T#127]" daemon [_thread_blocked, id=2160, stack(0x0000000039df0000,0x0000000039ef0000)]
  0x000000002e504800 JavaThread "elasticsearch[Master101][flush][T#8]" daemon [_thread_blocked, id=1484, stack(0x000000003a240000,0x000000003a340000)]
  0x000000002e6d2000 JavaThread "elasticsearch[Master101][warmer][T#3]" daemon [_thread_blocked, id=4280, stack(0x000000003dbd0000,0x000000003dcd0000)]
  0x000000002e32a000 JavaThread "elasticsearch[Master101][merge][T#1]" daemon [_thread_blocked, id=4320, stack(0x000000003cc60000,0x000000003cd60000)]
  0x000000002e325800 JavaThread "elasticsearch[Master101][index][T#8]" daemon [_thread_blocked, id=4396, stack(0x000000003d300000,0x000000003d400000)]
  0x000000002e4f8800 JavaThread "elasticsearch[Master101][index][T#7]" daemon [_thread_blocked, id=3592, stack(0x000000003d120000,0x000000003d220000)]
  0x000000002e506800 JavaThread "elasticsearch[Master101][index][T#6]" daemon [_thread_blocked, id=2244, stack(0x000000003cd70000,0x000000003ce70000)]
  0x000000002e4fb800 JavaThread "elasticsearch[Master101][index][T#5]" daemon [_thread_blocked, id=4384, stack(0x000000003ca10000,0x000000003cb10000)]
  0x000000002e501800 JavaThread "elasticsearch[Master101][index][T#4]" daemon [_thread_blocked, id=2368, stack(0x000000003cb30000,0x000000003cc30000)]
  0x000000002e503800 JavaThread "elasticsearch[Master101][refresh][T#2]" daemon [_thread_blocked, id=4248, stack(0x000000003a7b0000,0x000000003a8b0000)]
  0x000000002e4fe800 JavaThread "elasticsearch[Master101][index][T#3]" daemon [_thread_blocked, id=4804, stack(0x0000000039f50000,0x000000003a050000)]
  0x000000002e505000 JavaThread "elasticsearch[Master101][index][T#2]" daemon [_thread_blocked, id=2272, stack(0x0000000039cc0000,0x0000000039dc0000)]
  0x000000002e4f9000 JavaThread "elasticsearch[Master101][index][T#1]" daemon [_thread_blocked, id=2144, stack(0x0000000039990000,0x0000000039a90000)]
  0x000000002e328800 JavaThread "elasticsearch[Master101][fetch_shard_store][T#34]" daemon [_thread_blocked, id=4796, stack(0x0000007273b60000,0x0000007273c60000)]
  0x000000002c87f800 JavaThread "elasticsearch[Master101][search][T#13]" daemon [_thread_blocked, id=2224, stack(0x0000005723570000,0x0000005723670000)]
  0x000000002c87e800 JavaThread "elasticsearch[Master101][search][T#12]" daemon [_thread_blocked, id=3460, stack(0x00000056bb740000,0x00000056bb840000)]
  0x000000002c87e000 JavaThread "elasticsearch[Master101][search][T#11]" daemon [_thread_blocked, id=3524, stack(0x00000056bb2a0000,0x00000056bb3a0000)]
  0x000000002c87d000 JavaThread "elasticsearch[Master101][search][T#10]" daemon [_thread_blocked, id=3436, stack(0x00000056bb5f0000,0x00000056bb6f0000)]
  0x000000002c87c800 JavaThread "elasticsearch[Master101][search][T#9]" daemon [_thread_blocked, id=4108, stack(0x00000056bb4b0000,0x00000056bb5b0000)]
  0x000000002c763000 JavaThread "elasticsearch[Master101][search][T#8]" daemon [_thread_blocked, id=4552, stack(0x0000005616480000,0x0000005616580000)]
  0x000000002c762000 JavaThread "elasticsearch[Master101][search][T#7]" daemon [_thread_blocked, id=168, stack(0x0000005615ef0000,0x0000005615ff0000)]
  0x000000002c761800 JavaThread "elasticsearch[Master101][search][T#6]" daemon [_thread_blocked, id=2356, stack(0x0000005616050000,0x0000005616150000)]
  0x000000002c760800 JavaThread "elasticsearch[Master101][search][T#5]" daemon [_thread_blocked, id=3168, stack(0x0000005615de0000,0x0000005615ee0000)]
  0x000000002c760000 JavaThread "elasticsearch[Master101][search][T#4]" daemon [_thread_blocked, id=2088, stack(0x0000005615a20000,0x0000005615b20000)]
  0x000000002c75f000 JavaThread "elasticsearch[Master101][search][T#3]" daemon [_thread_blocked, id=4352, stack(0x00000056157f0000,0x00000056158f0000)]
  0x000000002e4f7800 JavaThread "elasticsearch[Master101][search][T#1]" daemon [_thread_blocked, id=2216, stack(0x000000560ce10000,0x000000560cf10000)]
  0x000000002e4fc000 JavaThread "elasticsearch[Master101][search][T#2]" daemon [_thread_blocked, id=3456, stack(0x000000560cc10000,0x000000560cd10000)]
  0x000000002e324000 JavaThread "elasticsearch[Master101][listener][T#4]" daemon [_thread_blocked, id=4220, stack(0x0000000c5de00000,0x0000000c5df00000)]
  0x000000002ebbf800 JavaThread "elasticsearch[Master101][listener][T#3]" daemon [_thread_blocked, id=4532, stack(0x0000000c43d70000,0x0000000c43e70000)]
  0x000000002ebbf000 JavaThread "elasticsearch[Master101][listener][T#2]" daemon [_thread_blocked, id=2320, stack(0x0000000c3ceb0000,0x0000000c3cfb0000)]
  0x000000002ebc0800 JavaThread "elasticsearch[Master101][listener][T#1]" daemon [_thread_blocked, id=4876, stack(0x0000000c14ed0000,0x0000000c14fd0000)]
  0x000000002e500800 JavaThread "elasticsearch[Master101][management][T#2]" daemon [_thread_blocked, id=2972, stack(0x0000000060250000,0x0000000060350000)]
  0x000000002e4fa000 JavaThread "elasticsearch[Master101][fetch_shard_started][T#10]" daemon [_thread_blocked, id=2292, stack(0x0000000038b60000,0x0000000038c60000)]
  0x000000002e6cf000 JavaThread "DestroyJavaVM" [_thread_blocked, id=1188, stack(0x0000000002300000,0x0000000002400000)]
  0x000000002e6ce800 JavaThread "elasticsearch[keepAlive/1.7.0]" [_thread_blocked, id=2136, stack(0x00000000356a0000,0x00000000357a0000)]
  0x000000002e6cd800 JavaThread "elasticsearch[Master101][http_server_boss][T#1]{New I/O server boss #51}" daemon [_thread_in_native, id=4556, stack(0x0000000037a50000,0x0000000037b50000)]
  0x000000002e6cd000 JavaThread "elasticsearch[Master101][http_server_worker][T#16]{New I/O worker #50}" daemon [_thread_blocked, id=3264, stack(0x0000000037920000,0x0000000037a20000)]
  0x000000002e6cc000 JavaThread "elasticsearch[Master101][http_server_worker][T#15]{New I/O worker #49}" daemon [_thread_blocked, id=4520, stack(0x00000000376c0000,0x00000000377c0000)]
  0x000000002e6cb800 JavaThread "elasticsearch[Master101][http_server_worker][T#14]{New I/O worker #48}" daemon [_thread_blocked, id=4012, stack(0x0000000037150000,0x0000000037250000)]
  0x000000002e6ca800 JavaThread "elasticsearch[Master101][http_server_worker][T#13]{New I/O worker #47}" daemon [_thread_blocked, id=3772, stack(0x00000000374f0000,0x00000000375f0000)]
  0x000000002e6ca000 JavaThread "elasticsearch[Master101][http_server_worker][T#12]{New I/O worker #46}" daemon [_thread_blocked, id=2316, stack(0x0000000037300000,0x0000000037400000)]
  0x000000002e6c9000 JavaThread "elasticsearch[Master101][http_server_worker][T#11]{New I/O worker #45}" daemon [_thread_blocked, id=944, stack(0x0000000037040000,0x0000000037140000)]
  0x000000002e6c8800 JavaThread "elasticsearch[Master101][http_server_worker][T#10]{New I/O worker #44}" daemon [_thread_blocked, id=4828, stack(0x0000000034a50000,0x0000000034b50000)]
  0x000000002e6c7800 JavaThread "elasticsearch[Master101][http_server_worker][T#9]{New I/O worker #43}" daemon [_thread_blocked, id=4684, stack(0x0000000036f20000,0x0000000037020000)]
  0x000000002e6c7000 JavaThread "elasticsearch[Master101][http_server_worker][T#8]{New I/O worker #42}" daemon [_thread_blocked, id=4812, stack(0x0000000036db0000,0x0000000036eb0000)]
  0x000000002e6c6000 JavaThread "elasticsearch[Master101][http_server_worker][T#7]{New I/O worker #41}" daemon [_thread_blocked, id=3256, stack(0x00000000320b0000,0x00000000321b0000)]
  0x000000002e6c5800 JavaThread "elasticsearch[Master101][http_server_worker][T#6]{New I/O worker #40}" daemon [_thread_blocked, id=2516, stack(0x0000000035af0000,0x0000000035bf0000)]
  0x000000002e6c4800 JavaThread "elasticsearch[Master101][http_server_worker][T#5]{New I/O worker #39}" daemon [_thread_blocked, id=1252, stack(0x00000000359d0000,0x0000000035ad0000)]
  0x000000002ebc7000 JavaThread "elasticsearch[Master101][http_server_worker][T#4]{New I/O worker #38}" daemon [_thread_blocked, id=4252, stack(0x00000000358d0000,0x00000000359d0000)]
  0x000000002ebc6800 JavaThread "elasticsearch[Master101][http_server_worker][T#3]{New I/O worker #37}" daemon [_thread_blocked, id=4264, stack(0x0000000034230000,0x0000000034330000)]
  0x000000002ebc5800 JavaThread "elasticsearch[Master101][http_server_worker][T#2]{New I/O worker #36}" daemon [_thread_blocked, id=2664, stack(0x00000000357a0000,0x00000000358a0000)]
  0x000000002ebc5000 JavaThread "elasticsearch[Master101][http_server_worker][T#1]{New I/O worker #35}" daemon [_thread_blocked, id=3184, stack(0x00000000355a0000,0x00000000356a0000)]
  0x000000002ebc4000 JavaThread "elasticsearch[Master101][management][T#1]" daemon [_thread_blocked, id=4272, stack(0x00000000353c0000,0x00000000354c0000)]
  0x000000002ebc3800 JavaThread "elasticsearch[Master101][riverClusterService#updateTask][T#1]" daemon [_thread_blocked, id=4132, stack(0x0000000035250000,0x0000000035350000)]
  0x000000002ebc1000 JavaThread "elasticsearch[Master101][transport_client_timer][T#1]{Hashed wheel timer #1}" daemon [_thread_blocked, id=4916, stack(0x00000000350e0000,0x00000000351e0000)]
  0x000000002ebbc800 JavaThread "elasticsearch[Master101][clusterService#updateTask][T#1]" daemon [_thread_blocked, id=4344, stack(0x0000000034020000,0x0000000034120000)]
  0x000000002ebbc000 JavaThread "elasticsearch[Master101][[http_server_boss.default]][T#1]{New I/O server boss #34}" daemon [_thread_in_native, id=5116, stack(0x00000000346d0000,0x00000000347d0000)]
  0x000000002ebbb000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#16]{New I/O worker #33}" daemon [_thread_blocked, id=4844, stack(0x0000000034550000,0x0000000034650000)]
  0x000000002ebba800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#15]{New I/O worker #32}" daemon [_thread_blocked, id=1040, stack(0x0000000034380000,0x0000000034480000)]
  0x000000002ebb9800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#14]{New I/O worker #31}" daemon [_thread_blocked, id=4608, stack(0x0000000034120000,0x0000000034220000)]
  0x000000002ebb9000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#13]{New I/O worker #30}" daemon [_thread_blocked, id=4768, stack(0x0000000033da0000,0x0000000033ea0000)]
  0x000000002ebb8000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#12]{New I/O worker #29}" daemon [_thread_blocked, id=5044, stack(0x0000000033ef0000,0x0000000033ff0000)]
  0x000000002cc82000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#11]{New I/O worker #28}" daemon [_thread_blocked, id=1696, stack(0x00000000329f0000,0x0000000032af0000)]
  0x000000002cc81000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#10]{New I/O worker #27}" daemon [_thread_blocked, id=3936, stack(0x0000000033c90000,0x0000000033d90000)]
  0x000000002cc80800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#9]{New I/O worker #26}" daemon [_thread_blocked, id=1000, stack(0x0000000033430000,0x0000000033530000)]
  0x000000002cc7f800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#8]{New I/O worker #25}" daemon [_thread_blocked, id=4920, stack(0x0000000033b90000,0x0000000033c90000)]
  0x000000002cc7f000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#7]{New I/O worker #24}" daemon [_thread_blocked, id=2620, stack(0x0000000033930000,0x0000000033a30000)]
  0x000000002cc7e000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#6]{New I/O worker #23}" daemon [_thread_blocked, id=4164, stack(0x0000000033a40000,0x0000000033b40000)]
  0x000000002cc7d800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#5]{New I/O worker #22}" daemon [_thread_blocked, id=1028, stack(0x0000000033830000,0x0000000033930000)]
  0x000000002cc7c800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#4]{New I/O worker #21}" daemon [_thread_blocked, id=3032, stack(0x0000000032df0000,0x0000000032ef0000)]
  0x000000002cc7c000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#3]{New I/O worker #20}" daemon [_thread_blocked, id=972, stack(0x00000000336b0000,0x00000000337b0000)]
  0x000000002cc7b000 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#2]{New I/O worker #19}" daemon [_thread_blocked, id=3484, stack(0x0000000033540000,0x0000000033640000)]
  0x000000002cc7a800 JavaThread "elasticsearch[Master101][[http_server_worker.default]][T#1]{New I/O worker #18}" daemon [_thread_blocked, id=4892, stack(0x00000000331e0000,0x00000000332e0000)]
  0x000000002cc79800 JavaThread "elasticsearch[Master101][transport_client_boss][T#1]{New I/O boss #17}" daemon [_thread_blocked, id=4172, stack(0x0000000033320000,0x0000000033420000)]
  0x000000002cc79000 JavaThread "elasticsearch[Master101][transport_client_worker][T#16]{New I/O worker #16}" daemon [_thread_blocked, id=4004, stack(0x0000000032680000,0x0000000032780000)]
  0x000000002cc78000 JavaThread "elasticsearch[Master101][transport_client_worker][T#15]{New I/O worker #15}" daemon [_thread_blocked, id=3052, stack(0x00000000330d0000,0x00000000331d0000)]
  0x000000002cc77800 JavaThread "elasticsearch[Master101][transport_client_worker][T#14]{New I/O worker #14}" daemon [_thread_blocked, id=3912, stack(0x0000000032f00000,0x0000000033000000)]
  0x000000002cc76800 JavaThread "elasticsearch[Master101][transport_client_worker][T#13]{New I/O worker #13}" daemon [_thread_blocked, id=3024, stack(0x0000000032cf0000,0x0000000032df0000)]
  0x000000002cc75800 JavaThread "elasticsearch[Master101][transport_client_worker][T#12]{New I/O worker #12}" daemon [_thread_blocked, id=2644, stack(0x0000000032b00000,0x0000000032c00000)]
  0x000000002cc75000 JavaThread "elasticsearch[Master101][transport_client_worker][T#11]{New I/O worker #11}" daemon [_thread_in_vm, id=2416, stack(0x00000000328e0000,0x00000000329e0000)]
  0x000000002cc74000 JavaThread "elasticsearch[Master101][transport_client_worker][T#10]{New I/O worker #10}" daemon [_thread_in_vm, id=4816, stack(0x00000000327a0000,0x00000000328a0000)]
  0x000000002cc73800 JavaThread "elasticsearch[Master101][transport_client_worker][T#9]{New I/O worker #9}" daemon [_thread_in_vm, id=4800, stack(0x0000000032580000,0x0000000032680000)]
  0x000000002cc72800 JavaThread "elasticsearch[Master101][transport_client_worker][T#8]{New I/O worker #8}" daemon [_thread_in_vm, id=184, stack(0x0000000032450000,0x0000000032550000)]
  0x000000002cc6a000 JavaThread "elasticsearch[Master101][transport_client_worker][T#7]{New I/O worker #7}" daemon [_thread_blocked, id=2124, stack(0x00000000322e0000,0x00000000323e0000)]
  0x000000002cc66000 JavaThread "elasticsearch[Master101][transport_client_worker][T#6]{New I/O worker #6}" daemon [_thread_in_vm, id=2612, stack(0x00000000321b0000,0x00000000322b0000)]
  0x000000002cfe8000 JavaThread "elasticsearch[Master101][transport_client_worker][T#5]{New I/O worker #5}" daemon [_thread_blocked, id=2880, stack(0x0000000031730000,0x0000000031830000)]
  0x000000002cfe6000 JavaThread "elasticsearch[Master101][transport_client_worker][T#4]{New I/O worker #4}" daemon [_thread_blocked, id=2788, stack(0x0000000031fb0000,0x00000000320b0000)]
  0x000000002cfe4800 JavaThread "elasticsearch[Master101][transport_client_worker][T#3]{New I/O worker #3}" daemon [_thread_blocked, id=2172, stack(0x0000000031db0000,0x0000000031eb0000)]
  0x000000002b791000 JavaThread "elasticsearch[Master101][transport_client_worker][T#2]{New I/O worker #2}" daemon [_thread_blocked, id=2112, stack(0x0000000031bb0000,0x0000000031cb0000)]
  0x000000002b790000 JavaThread "elasticsearch[Master101][transport_client_worker][T#1]{New I/O worker #1}" daemon [_thread_blocked, id=3376, stack(0x0000000031a20000,0x0000000031b20000)]
  0x000000002b3d6000 JavaThread "elasticsearch[Master101][[ttl_expire]]" daemon [_thread_blocked, id=4304, stack(0x0000000031400000,0x0000000031500000)]
  0x000000002b406000 JavaThread "elasticsearch[Master101][master_mapping_updater]" [_thread_blocked, id=5076, stack(0x000000002ecd0000,0x000000002edd0000)]
  0x000000002feeb800 JavaThread "elasticsearch[Master101][scheduler][T#1]" daemon [_thread_in_vm, id=4224, stack(0x0000000031840000,0x0000000031940000)]
  0x000000002b6ce800 JavaThread "elasticsearch[Master101][[timer]]" daemon [_thread_blocked, id=3696, stack(0x000000002c100000,0x000000002c200000)]
  0x00000000298e4000 JavaThread "Service Thread" daemon [_thread_blocked, id=4508, stack(0x000000002b290000,0x000000002b390000)]
  0x000000002988a000 JavaThread "C1 CompilerThread3" daemon [_thread_blocked, id=4620, stack(0x000000002b080000,0x000000002b180000)]
  0x0000000029861800 JavaThread "C2 CompilerThread2" daemon [_thread_blocked, id=2260, stack(0x000000002af20000,0x000000002b020000)]
  0x0000000029860800 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=4444, stack(0x000000002a920000,0x000000002aa20000)]
  0x0000000029855000 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=2776, stack(0x000000002acc0000,0x000000002adc0000)]
  0x0000000029852000 JavaThread "Attach Listener" daemon [_thread_blocked, id=4736, stack(0x000000002ab90000,0x000000002ac90000)]
  0x000000002984d800 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=4412, stack(0x000000002aa20000,0x000000002ab20000)]
  0x000000002985a000 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=4716, stack(0x000000002a740000,0x000000002a840000)]
  0x00000000297ef000 JavaThread "Finalizer" daemon [_thread_blocked, id=3924, stack(0x000000002a560000,0x000000002a660000)]
  0x00000000297e6000 JavaThread "Reference Handler" daemon [_thread_blocked, id=4868, stack(0x000000002a380000,0x000000002a480000)]

Other Threads:
  0x00000000297e3000 VMThread [stack: 0x000000002a260000,0x000000002a360000] [id=4944]
  0x00000000298ea800 WatcherThread [stack: 0x000000002adc0000,0x000000002aec0000] [id=4636]

VM state:synchronizing (normal execution)

VM Mutex/Monitor currently owned by a thread:  ([mutex/lock_event])
[0x00000000021fd160] Safepoint_lock - owner thread: 0x00000000297e3000
[0x00000000021fd1e0] Threads_lock - owner thread: 0x00000000297e3000

Heap:
 par new generation   total 613440K, used 295871K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K,  48% used [0x00000003c0000000, 0x00000003d0473258, 0x00000003e1490000)
  from space 68096K,  42% used [0x00000003e5710000, 0x00000003e738cd30, 0x00000003e9990000)
  to   space 68096K,   0% used [0x00000003e1490000, 0x00000003e1490000, 0x00000003e5710000)
 concurrent mark-sweep generation total 16095680K, used 6672644K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43851K, capacity 44256K, committed 44452K, reserved 1089536K
  class space    used 5122K, capacity 5230K, committed 5344K, reserved 1048576K

Card table byte_map: [0x00000000126b0000,0x00000000146c0000] byte_map_base: 0x00000000108b0000

Marking Bits: (CMSBitMap*) 0x00000000021ea468
 Bits: [0x0000000016cf0000, 0x0000000026289c00)

Mod Union Table: (CMSBitMap*) 0x00000000021ea528
 Bits: [0x0000000026290000, 0x0000000026666670)

Polling page: 0x0000000000150000

CodeCache: size=245760Kb used=43602Kb max_used=43938Kb free=202157Kb
 bounds [0x00000000026d0000, 0x0000000005270000, 0x00000000116d0000]
 total_blobs=11005 nmethods=10423 adapters=493
 compilation: enabled

Compilation events (10 events):
Event: 11343.637 Thread 0x000000002988a000 15521       3       java.util.TreeMap$EntryIterator::next (5 bytes)
Event: 11343.638 Thread 0x000000002988a000 nmethod 15521 0x0000000003f76490 code [0x0000000003f76600, 0x0000000003f768e8]
Event: 11343.638 Thread 0x000000002988a000 15522       3       java.util.TreeMap$EntryIterator::next (5 bytes)
Event: 11343.638 Thread 0x000000002988a000 nmethod 15522 0x0000000004cf4690 code [0x0000000004cf4800, 0x0000000004cf4a08]
Event: 11343.639 Thread 0x000000002988a000 15524   !   3       org.elasticsearch.common.netty.util.internal.ConversionUtil::toBoolean (104 bytes)
Event: 11343.639 Thread 0x000000002988a000 nmethod 15524 0x0000000004352ad0 code [0x0000000004352d20, 0x0000000004353c18]
Event: 11343.639 Thread 0x000000002988a000 15525       3       sun.nio.ch.Net::setSocketOption (352 bytes)
Event: 11343.642 Thread 0x000000002988a000 nmethod 15525 0x0000000004d58bd0 code [0x0000000004d58f80, 0x0000000004d5b338]
Event: 11343.642 Thread 0x000000002988a000 15526       3       sun.nio.ch.SocketOptionRegistry::findOption (23 bytes)
Event: 11343.642 Thread 0x000000002988a000 nmethod 15526 0x0000000003d87d90 code [0x0000000003d87f60, 0x0000000003d88748]

GC Heap History (10 events):
Event: 6730.117 GC heap before
{Heap before GC invocations=476 (full 1):
 par new generation   total 613440K, used 559963K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K, 100% used [0x00000003c0000000, 0x00000003e1490000, 0x00000003e1490000)
  from space 68096K,  21% used [0x00000003e1490000, 0x00000003e22d6ce8, 0x00000003e5710000)
  to   space 68096K,   0% used [0x00000003e5710000, 0x00000003e5710000, 0x00000003e9990000)
 concurrent mark-sweep generation total 16095680K, used 6670433K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43772K, capacity 44128K, committed 44452K, reserved 1089536K
  class space    used 5111K, capacity 5230K, committed 5344K, reserved 1048576K
Event: 6730.189 GC heap after
Heap after GC invocations=477 (full 1):
 par new generation   total 613440K, used 36305K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K,   0% used [0x00000003c0000000, 0x00000003c0000000, 0x00000003e1490000)
  from space 68096K,  53% used [0x00000003e5710000, 0x00000003e7a846a0, 0x00000003e9990000)
  to   space 68096K,   0% used [0x00000003e1490000, 0x00000003e1490000, 0x00000003e5710000)
 concurrent mark-sweep generation total 16095680K, used 6670534K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43772K, capacity 44128K, committed 44452K, reserved 1089536K
  class space    used 5111K, capacity 5230K, committed 5344K, reserved 1048576K
}
Event: 7866.592 GC heap before
{Heap before GC invocations=477 (full 1):
 par new generation   total 613440K, used 581649K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K, 100% used [0x00000003c0000000, 0x00000003e1490000, 0x00000003e1490000)
  from space 68096K,  53% used [0x00000003e5710000, 0x00000003e7a846a0, 0x00000003e9990000)
  to   space 68096K,   0% used [0x00000003e1490000, 0x00000003e1490000, 0x00000003e5710000)
 concurrent mark-sweep generation total 16095680K, used 6670534K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43780K, capacity 44128K, committed 44452K, reserved 1089536K
  class space    used 5111K, capacity 5230K, committed 5344K, reserved 1048576K
Event: 7866.666 GC heap after
Heap after GC invocations=478 (full 1):
 par new generation   total 613440K, used 45069K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K,   0% used [0x00000003c0000000, 0x00000003c0000000, 0x00000003e1490000)
  from space 68096K,  66% used [0x00000003e1490000, 0x00000003e40935b0, 0x00000003e5710000)
  to   space 68096K,   0% used [0x00000003e5710000, 0x00000003e5710000, 0x00000003e9990000)
 concurrent mark-sweep generation total 16095680K, used 6671112K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43780K, capacity 44128K, committed 44452K, reserved 1089536K
  class space    used 5111K, capacity 5230K, committed 5344K, reserved 1048576K
}
Event: 8966.310 GC heap before
{Heap before GC invocations=478 (full 1):
 par new generation   total 613440K, used 590413K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K, 100% used [0x00000003c0000000, 0x00000003e1490000, 0x00000003e1490000)
  from space 68096K,  66% used [0x00000003e1490000, 0x00000003e40935b0, 0x00000003e5710000)
  to   space 68096K,   0% used [0x00000003e5710000, 0x00000003e5710000, 0x00000003e9990000)
 concurrent mark-sweep generation total 16095680K, used 6671112K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43804K, capacity 44192K, committed 44452K, reserved 1089536K
  class space    used 5115K, capacity 5230K, committed 5344K, reserved 1048576K
Event: 8966.353 GC heap after
Heap after GC invocations=479 (full 1):
 par new generation   total 613440K, used 39423K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K,   0% used [0x00000003c0000000, 0x00000003c0000000, 0x00000003e1490000)
  from space 68096K,  57% used [0x00000003e5710000, 0x00000003e7d8ff00, 0x00000003e9990000)
  to   space 68096K,   0% used [0x00000003e1490000, 0x00000003e1490000, 0x00000003e5710000)
 concurrent mark-sweep generation total 16095680K, used 6671222K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43804K, capacity 44192K, committed 44452K, reserved 1089536K
  class space    used 5115K, capacity 5230K, committed 5344K, reserved 1048576K
}
Event: 9983.773 GC heap before
{Heap before GC invocations=479 (full 1):
 par new generation   total 613440K, used 584767K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K, 100% used [0x00000003c0000000, 0x00000003e1490000, 0x00000003e1490000)
  from space 68096K,  57% used [0x00000003e5710000, 0x00000003e7d8ff00, 0x00000003e9990000)
  to   space 68096K,   0% used [0x00000003e1490000, 0x00000003e1490000, 0x00000003e5710000)
 concurrent mark-sweep generation total 16095680K, used 6671222K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43804K, capacity 44192K, committed 44452K, reserved 1089536K
  class space    used 5115K, capacity 5230K, committed 5344K, reserved 1048576K
Event: 9983.842 GC heap after
Heap after GC invocations=480 (full 1):
 par new generation   total 613440K, used 25941K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K,   0% used [0x00000003c0000000, 0x00000003c0000000, 0x00000003e1490000)
  from space 68096K,  38% used [0x00000003e1490000, 0x00000003e2de5640, 0x00000003e5710000)
  to   space 68096K,   0% used [0x00000003e5710000, 0x00000003e5710000, 0x00000003e9990000)
 concurrent mark-sweep generation total 16095680K, used 6671399K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43804K, capacity 44192K, committed 44452K, reserved 1089536K
  class space    used 5115K, capacity 5230K, committed 5344K, reserved 1048576K
}
Event: 11007.433 GC heap before
{Heap before GC invocations=480 (full 1):
 par new generation   total 613440K, used 571285K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K, 100% used [0x00000003c0000000, 0x00000003e1490000, 0x00000003e1490000)
  from space 68096K,  38% used [0x00000003e1490000, 0x00000003e2de5640, 0x00000003e5710000)
  to   space 68096K,   0% used [0x00000003e5710000, 0x00000003e5710000, 0x00000003e9990000)
 concurrent mark-sweep generation total 16095680K, used 6671399K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43804K, capacity 44192K, committed 44452K, reserved 1089536K
  class space    used 5115K, capacity 5230K, committed 5344K, reserved 1048576K
Event: 11007.495 GC heap after
Heap after GC invocations=481 (full 1):
 par new generation   total 613440K, used 29171K [0x00000003c0000000, 0x00000003e9990000, 0x00000003e9990000)
  eden space 545344K,   0% used [0x00000003c0000000, 0x00000003c0000000, 0x00000003e1490000)
  from space 68096K,  42% used [0x00000003e5710000, 0x00000003e738cd30, 0x00000003e9990000)
  to   space 68096K,   0% used [0x00000003e1490000, 0x00000003e1490000, 0x00000003e5710000)
 concurrent mark-sweep generation total 16095680K, used 6672644K [0x00000003e9990000, 0x00000007c0000000, 0x00000007c0000000)
 Metaspace       used 43804K, capacity 44192K, committed 44452K, reserved 1089536K
  class space    used 5115K, capacity 5230K, committed 5344K, reserved 1048576K
}

Deoptimization events (10 events):
Event: 11340.806 Thread 0x000000002ebbc800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x000000000309f80c method=org.elasticsearch.cluster.routing.RoutingNodes.assignedShards(Lorg/elasticsearch/index/shard/ShardId;)Ljava/util/List; @ 15
Event: 11340.806 Thread 0x000000002ebbc800 Uncommon trap: reason=class_check action=maybe_recompile pc=0x0000000003604bb4 method=org.elasticsearch.cluster.routing.allocation.AllocationService.electPrimariesAndUnassignedDanglingReplicas(Lorg/elasticsearch/cluster/routing/allocation/RoutingAllocation&#8221;&#192;&#230;Lg&amp;&#198;@
Event: 11340.807 Thread 0x000000002ebbc800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x000000000309e890 method=org.elasticsearch.cluster.routing.RoutingNodes.assignedShards(Lorg/elasticsearch/index/shard/ShardId;)Ljava/util/List; @ 15
Event: 11340.808 Thread 0x000000002ebbc800 Uncommon trap: reason=class_check action=maybe_recompile pc=0x0000000003039680 method=org.elasticsearch.cluster.routing.RoutingNodes.activeReplica(Lorg/elasticsearch/cluster/routing/ShardRouting;)Lorg/elasticsearch/cluster/routing/MutableShardRouting; @ 10M&#174;&#8216;g&amp;&#198;@
Event: 11340.809 Thread 0x000000002ebbc800 Uncommon trap: reason=class_check action=maybe_recompile pc=0x0000000003039680 method=org.elasticsearch.cluster.routing.RoutingNodes.activeReplica(Lorg/elasticsearch/cluster/routing/ShardRouting;)Lorg/elasticsearch/cluster/routing/MutableShardRouting; @ 10&#214;H&#239;&#173;&#141;&amp;&#198;@
Event: 11341.107 Thread 0x000000002ebbc800 Uncommon trap: reason=bimorphic action=maybe_recompile pc=0x0000000005064854 method=org.apache.lucene.util.IntroSorter.quicksort(III)V @ 156
Event: 11341.111 Thread 0x000000002ebbc800 Uncommon trap: reason=bimorphic action=maybe_recompile pc=0x0000000005064854 method=org.apache.lucene.util.IntroSorter.quicksort(III)V @ 156
Event: 11341.120 Thread 0x000000002ebbc800 Uncommon trap: reason=class_check action=maybe_recompile pc=0x00000000042731c0 method=java.io.BufferedOutputStream.write([BII)V @ 20
Event: 11343.637 Thread 0x000000002cc79800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x0000000004383e9c method=org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(Ljava/util/Set;J)V @ 13
Event: 11343.637 Thread 0x000000002cc79800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x000000000305c814 method=sun.nio.ch.SocketChannelImpl.isConnected()Z @ 12

Internal exceptions (10 events):
Event: 11343.856 Thread 0x000000002ebbe000 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cf42ff88) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11343.860 Thread 0x000000002c880000 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003ceda9ad0) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11343.877 Thread 0x000000002c882800 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cf4c7188) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11343.884 Thread 0x000000002e6d1800 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cf660fe0) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11344.188 Thread 0x000000002e6d3800 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cf604f50) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11344.241 Thread 0x000000002ebc2000 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cf187008) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11344.325 Thread 0x000000002c881000 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cff0df50) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11344.325 Thread 0x000000002e6d0000 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cfb55030) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11344.328 Thread 0x000000002c880000 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cf978fd0) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]
Event: 11344.376 Thread 0x000000002e324800 Exception &lt;a 'sun/nio/fs/WindowsException'&gt; (0x00000003cfdb2178) thrown at [C:\re\workspace\8-2-build-windows-amd64-cygwin\jdk8u60\4407\hotspot\src\share\vm\prims\jni.cpp, line 709]

Events (10 events):
Event: 11341.111 Thread 0x000000002ebbc800 DEOPT UNPACKING pc=0x000000000271582a sp=0x000000003411ec20 mode 2
Event: 11341.120 Thread 0x000000002ebbc800 Uncommon trap: trap_request=0xffffffde fr.pc=0x00000000042731c0
Event: 11341.120 Thread 0x000000002ebbc800 DEOPT PACKING pc=0x00000000042731c0 sp=0x000000003411e810
Event: 11341.120 Thread 0x000000002ebbc800 DEOPT UNPACKING pc=0x000000000271582a sp=0x000000003411e850 mode 2
Event: 11343.637 Thread 0x000000002cc79800 Uncommon trap: trap_request=0xffffff65 fr.pc=0x0000000004383e9c
Event: 11343.637 Thread 0x000000002cc79800 DEOPT PACKING pc=0x0000000004383e9c sp=0x000000003341f140
Event: 11343.637 Thread 0x000000002cc79800 DEOPT UNPACKING pc=0x000000000271582a sp=0x000000003341efc8 mode 2
Event: 11343.637 Thread 0x000000002cc79800 Uncommon trap: trap_request=0xffffff65 fr.pc=0x000000000305c814
Event: 11343.637 Thread 0x000000002cc79800 DEOPT PACKING pc=0x000000000305c814 sp=0x000000003341f080
Event: 11343.637 Thread 0x000000002cc79800 DEOPT UNPACKING pc=0x000000000271582a sp=0x000000003341f030 mode 2


Dynamic libraries:
0x000000013f0d0000 - 0x000000013f107000     C:\Program Files\Java\jre1.8.0_60\bin\java.exe
0x0000000077790000 - 0x0000000077939000     C:\Windows\SYSTEM32\ntdll.dll
0x0000000077570000 - 0x000000007768f000     C:\Windows\system32\kernel32.dll
0x000007fefd5a0000 - 0x000007fefd60c000     C:\Windows\system32\KERNELBASE.dll
0x000007fefdbc0000 - 0x000007fefdc9b000     C:\Windows\system32\ADVAPI32.dll
0x000007feff480000 - 0x000007feff51f000     C:\Windows\system32\msvcrt.dll
0x000007feff370000 - 0x000007feff38f000     C:\Windows\SYSTEM32\sechost.dll
0x000007fefda90000 - 0x000007fefdbbd000     C:\Windows\system32\RPCRT4.dll
0x0000000077690000 - 0x000000007778a000     C:\Windows\system32\USER32.dll
0x000007fefe4d0000 - 0x000007fefe537000     C:\Windows\system32\GDI32.dll
0x000007feff470000 - 0x000007feff47e000     C:\Windows\system32\LPK.dll
0x000007fefdca0000 - 0x000007fefdd69000     C:\Windows\system32\USP10.dll
0x000007fefbe40000 - 0x000007fefc034000     C:\Windows\WinSxS\amd64_microsoft.windows.common-controls_6595b64144ccf1df_6.0.7601.18837_none_fa3b1e3d17594757\COMCTL32.dll
0x000007feff5b0000 - 0x000007feff621000     C:\Windows\system32\SHLWAPI.dll
0x000007feff580000 - 0x000007feff5ae000     C:\Windows\system32\IMM32.DLL
0x000007feff710000 - 0x000007feff819000     C:\Windows\system32\MSCTF.dll
0x0000000074410000 - 0x00000000744e2000     C:\Program Files\Java\jre1.8.0_60\bin\msvcr100.dll
0x0000000066f30000 - 0x00000000677b9000     C:\Program Files\Java\jre1.8.0_60\bin\server\jvm.dll
0x000007fef8d20000 - 0x000007fef8d29000     C:\Windows\system32\WSOCK32.dll
0x000007feff410000 - 0x000007feff45d000     C:\Windows\system32\WS2_32.dll
0x000007feff460000 - 0x000007feff468000     C:\Windows\system32\NSI.dll
0x000007fef8a40000 - 0x000007fef8a7b000     C:\Windows\system32\WINMM.dll
0x000007fefc610000 - 0x000007fefc61c000     C:\Windows\system32\VERSION.dll
0x0000000077950000 - 0x0000000077957000     C:\Windows\system32\PSAPI.DLL
0x0000000074780000 - 0x000000007478f000     C:\Program Files\Java\jre1.8.0_60\bin\verify.dll
0x0000000074520000 - 0x0000000074549000     C:\Program Files\Java\jre1.8.0_60\bin\java.dll
0x00000000743f0000 - 0x0000000074406000     C:\Program Files\Java\jre1.8.0_60\bin\zip.dll
0x000007fefe540000 - 0x000007feff2c9000     C:\Windows\system32\SHELL32.dll
0x000007fefe060000 - 0x000007fefe263000     C:\Windows\system32\ole32.dll
0x000007fefd540000 - 0x000007fefd54f000     C:\Windows\system32\profapi.dll
0x00000000743d0000 - 0x00000000743ea000     C:\Program Files\Java\jre1.8.0_60\bin\net.dll
0x000007fefcbf0000 - 0x000007fefcc45000     C:\Windows\system32\mswsock.dll
0x000007fefd1f0000 - 0x000007fefd1f7000     C:\Windows\System32\wship6.dll
0x000007fef9ec0000 - 0x000007fef9ee7000     C:\Windows\system32\IPHLPAPI.DLL
0x000007fef9eb0000 - 0x000007fef9ebb000     C:\Windows\system32\WINNSI.DLL
0x000007fef9c30000 - 0x000007fef9c41000     C:\Windows\system32\dhcpcsvc6.DLL
0x000007fef9bd0000 - 0x000007fef9be8000     C:\Windows\system32\dhcpcsvc.DLL
0x000007fefd040000 - 0x000007fefd058000     C:\Windows\system32\CRYPTSP.dll
0x000007fefca20000 - 0x000007fefca67000     C:\Windows\system32\rsaenh.dll
0x000007fefd890000 - 0x000007fefd8ae000     C:\Windows\system32\USERENV.dll
0x000007fefd3a0000 - 0x000007fefd3af000     C:\Windows\system32\CRYPTBASE.dll
0x00000000743b0000 - 0x00000000743c1000     C:\Program Files\Java\jre1.8.0_60\bin\nio.dll
0x0000000074770000 - 0x000000007477d000     C:\Program Files\Java\jre1.8.0_60\bin\management.dll
0x000007fefb930000 - 0x000007fefb945000     C:\Windows\system32\NLAapi.dll
0x000007fef9820000 - 0x000007fef9835000     C:\Windows\system32\napinsp.dll
0x000007fefcc70000 - 0x000007fefcccb000     C:\Windows\system32\DNSAPI.dll
0x000007fef9850000 - 0x000007fef985b000     C:\Windows\System32\winrnr.dll
0x000007fefc6e0000 - 0x000007fefc6e7000     C:\Windows\System32\wshtcpip.dll
0x000007fef9860000 - 0x000007fef9868000     C:\Windows\system32\rasadhlp.dll
0x000007fef83b0000 - 0x000007fef8403000     C:\Windows\System32\fwpuclnt.dll
0x0000000180000000 - 0x0000000180038000     C:\Users\ldprsvc\AppData\Local\Temp\2\jna-36009638\jna3897599377805093171.dll
0x000007feec720000 - 0x000007feec845000     C:\Windows\system32\DBGHELP.DLL

VM Arguments:
jvm_args: -Xms16g -Xmx16g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des-foreground=yes -Des.path.home=E:\Share\elasticsearch-1.7.0 
java_command: org.elasticsearch.bootstrap.Elasticsearch
java_class_path (initial): ;E:\Share\elasticsearch-1.7.0/lib/elasticsearch-1.7.0.jar;E:\Share\elasticsearch-1.7.0/lib/antlr-runtime-3.5.jar;E:\Share\elasticsearch-1.7.0/lib/apache-log4j-extras-1.2.17.jar;E:\Share\elasticsearch-1.7.0/lib/asm-4.1.jar;E:\Share\elasticsearch-1.7.0/lib/asm-commons-4.1.jar;E:\Share\elasticsearch-1.7.0/lib/elasticsearch-1.7.0.jar;E:\Share\elasticsearch-1.7.0/lib/groovy-all-2.4.4.jar;E:\Share\elasticsearch-1.7.0/lib/jna-4.1.0.jar;E:\Share\elasticsearch-1.7.0/lib/jts-1.13.jar;E:\Share\elasticsearch-1.7.0/lib/log4j-1.2.17.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-analyzers-common-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-core-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-expressions-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-grouping-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-highlighter-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-join-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-memory-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-misc-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-queries-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-queryparser-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-sandbox-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-spatial-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/lucene-suggest-4.10.4.jar;E:\Share\elasticsearch-1.7.0/lib/spatial4j-0.4.1.jar;E:\Share\elasticsearch-1.7.0/lib/sigar/*
Launcher Type: SUN_STANDARD

Environment Variables:
JAVA_HOME=C:\Program Files\Java\jre1.8.0_60
PATH=C:\ProgramData\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;E:\Program Files (x86)\Microsoft SQL Server\100\Tools\Binn\;E:\Program Files\Microsoft SQL Server\100\Tools\Binn\;E:\Program Files\Microsoft SQL Server\100\DTS\Binn\;E:\Program Files (x86)\Microsoft SQL Server\100\Tools\Binn\VSShell\Common7\IDE\;E:\Program Files (x86)\Microsoft SQL Server\100\DTS\Binn\;E:\Program Files\Microsoft SQL Server\90\DTS\Binn\;E:\Program Files (x86)\Microsoft SQL Server\90\DTS\Binn\
USERNAME=ldprsvc
OS=Windows_NT
PROCESSOR_IDENTIFIER=Intel64 Family 6 Model 45 Stepping 7, GenuineIntel



---------------  S Y S T E M  ---------------

OS: Windows Server 2008 R2 , 64 bit Build 7601 (6.1.7601.18939)

CPU:total 8 (4 cores per cpu, 1 threads per core) family 6 model 45 stepping 7, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, aes, clmul, tsc, tscinvbit, tscinv

Memory: 4k page, physical 33553912k(10603016k free), swap 36172740k(12197716k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (25.60-b23) for windows-amd64 JRE (1.8.0_60-b27), built on Aug  4 2015 11:06:27 by "java_re" with MS VC++ 10.0 (VS2010)

time: Wed Nov 04 13:30:18 2015
elapsed time: 11365 seconds (0d 3h 9m 25s)


```
</description><key id="115206475">14543</key><summary>Elastic Server Crash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels /><created>2015-11-05T04:20:10Z</created><updated>2015-11-05T04:29:00Z</updated><resolved>2015-11-05T04:29:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-05T04:28:59Z" id="153953192">You are using ES on windows with a network filesystem. I don't recommend this: or you will run into this bug: https://bugs.openjdk.java.net/browse/JDK-6415680

Nothing we can do on our side here, it needs to be fixed in the JDK. Feel free to use the bug information to report it there (they will likely just close it as dup):

```
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>request dsl fields problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14542</link><project id="" key="" /><description>request body ,  keyword  = ['benz','join','','']

```
{
    "fields": "keyword",
     "size":10
}
```

res

```
{
    "took": 7,
    "timed_out": false,
    "_shards": {
        "total": 30,
        "successful": 29,
        "failed": 1,
        "failures": [
            {
                "index": "xxxx",
                "shard": 0,
                "status": 400,
                "reason": "RemoteTransportException[[Gazelle][inet[/xxxxxxx:9300]][indices:data/read/search[phase/fetch/id]]]; nested: ElasticsearchIllegalArgumentException[field [keyword] isn't a leaf field]; "
            }
        ]
    },
    "hits": {
        "total": 26528,
        "max_score": 1,
        "hits": []
    }
}
```
</description><key id="115201534">14542</key><summary>request dsl fields problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rfyiamcool</reporter><labels /><created>2015-11-05T03:26:47Z</created><updated>2015-11-05T03:30:27Z</updated><resolved>2015-11-05T03:30:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rfyiamcool" created="2015-11-05T03:30:27Z" id="153944074">Thanks U

```
{
  "_source": [
    "keyword"
  ]
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle build should pass tests.jvm.argline -&gt; junit4 argline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14541</link><project id="" key="" /><description>This is only being passed to integration tests today, but not unit tests.

Its used by jenkins to pass randomization parameters, and useful when developing to pass args to the jvm (e.g. enable additional debugging)
</description><key id="115199179">14541</key><summary>gradle build should pass tests.jvm.argline -&gt; junit4 argline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-05T03:00:19Z</created><updated>2015-11-05T07:59:58Z</updated><resolved>2015-11-05T07:59:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>colon is added</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14540</link><project id="" key="" /><description>"predict"  10 =&gt; "predict" : 10
</description><key id="115195856">14540</key><summary>colon is added</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KangYongKyun</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-11-05T02:32:34Z</created><updated>2015-11-17T14:55:33Z</updated><resolved>2015-11-17T14:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-05T17:26:53Z" id="154130190">@KangYongKyun thanks for this! Can you sign the [CLA](https://www.elastic.co/contributor-agreement/) and I will merge this in?
</comment><comment author="KangYongKyun" created="2015-11-13T08:53:57Z" id="156365080">I had signed the CLA
</comment><comment author="clintongormley" created="2015-11-17T14:55:33Z" id="157392575">thanks @KangYongKyun - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify recovery vs. relocation in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14539</link><project id="" key="" /><description>Somewhere in our documentation, it will be nice to clarify that relocation is a form/type of recovery, so that the [recovery settings](https://www.elastic.co/guide/en/elasticsearch/reference/master/recovery.html#recovery) actually apply to both shard recoveries and relocations (even though the parameter name of these settings refers to "recovery" only).
</description><key id="115181390">14539</key><summary>Clarify recovery vs. relocation in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2015-11-05T00:23:24Z</created><updated>2015-11-30T19:14:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Highlighter causing overflow of boolean max clause count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14538</link><project id="" key="" /><description>The following appears to be the case where the wildcard query (in this case, entered by a Kibana user) caused enough Lucene rewrites so that it overflows the boolean max clause count of 1024 by default.  While we could potentially workaround this by increasing the max clause count in Lucene/ES, it is not the ideal workaround given the memory implications.    

This appears to be a highlighter issue based on the stack.  Not sure if the highlighter honors these rewrite settings (https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-term-rewrite.html) ... Is there any safeguard that can be configured to prevent the highlighter from causing the great number of bool queries that is being generated?

```
[indices:data/read/search[phase/fetch/id]] 
Caused by: org.elasticsearch.search.fetch.FetchPhaseExecutionException: [logstash-2015.11.03][9]: query[filtered(+host:* +(message:20151103) +(message:main) +(message:dialer))-&gt;BooleanFilter(+cache(@timestamp:[1446555600000 TO 1446574259999]))],from[0],size[1000],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@69646af6&gt;!]: Fetch Failed [Failed to highlight field [message]] 
at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:134) 
at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:133) 
at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:194) 
at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:516) 
at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:868) 
at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:862) 
at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279) 
at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745) 
Caused by: org.apache.lucene.search.BooleanQuery$TooManyClauses: maxClauseCount is set to 1024
at org.apache.lucene.search.ScoringRewrite$1.checkMaxClauseCount(ScoringRewrite.java:72) 
at org.apache.lucene.search.ScoringRewrite$ParallelArraysTermCollector.collect(ScoringRewrite.java:149)
at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:79) 
at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:105) 
at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288) 
at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:217) 
at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:99) 
at org.elasticsearch.search.highlight.CustomQueryScorer$CustomWeightedSpanTermExtractor.extractUnknownQuery(CustomQueryScorer.java:89) 
at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:224) 
at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:474) 
at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:217) 
at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186) 
at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:197) 
at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:120) 
... 10 more
```
</description><key id="115171560">14538</key><summary>Highlighter causing overflow of boolean max clause count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Highlighting</label><label>enhancement</label></labels><created>2015-11-04T23:15:04Z</created><updated>2016-11-25T15:09:09Z</updated><resolved>2016-11-25T15:09:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-05T17:17:22Z" id="154126896">&gt; Is there any safeguard that can be configured to prevent the highlighter from causing the great number of bool queries that is being generated?

I honestly don't know if the highlighter respects the rewrite configuration - without trying it I'm 60% sure it does but that isn't saying much. For what its worth the experimental highlighter doesn't have to rewrite most wildcard queries so it won't hit this particular problem.
</comment><comment author="ppf2" created="2015-11-05T17:45:41Z" id="154134991">The experimental highlighter (https://github.com/elastic/elasticsearch/pull/5767) was added as a doc change.  As Nik noted above, this alternate highlighter does not rewrite wildcard queries so it will not run into this issue.  This is a request to enhance the out of the box highlighter for it to also handle wildcard queries in a more efficient way like the experimental highlighter.
</comment><comment author="clintongormley" created="2016-11-25T15:09:09Z" id="262975465">closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for Lucene 5.4 GeoPoint queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14537</link><project id="" key="" /><description>This PR adds query support for Lucene 5.4 GeoPointField type along with backward compatibility for "legacy" geo_point indexes.
</description><key id="115167963">14537</key><summary>Add support for Lucene 5.4 GeoPoint queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>feature</label><label>release highlight</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T22:52:53Z</created><updated>2016-01-22T18:32:34Z</updated><resolved>2015-11-05T18:22:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-05T14:30:01Z" id="154075808">awesome LGTM I just left some nitpicks :) feel free to merge without another review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add GeoPointV2 Field Mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14536</link><project id="" key="" /><description>This PR adds the abstraction layer to GeoPointFieldMapper needed to cut over to Lucene 5.4's new GeoPointField type while maintaining backward compatibility with 'legacy' geo_point indexes.
</description><key id="115167518">14536</key><summary>Add GeoPointV2 Field Mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>feature</label><label>release highlight</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T22:50:26Z</created><updated>2016-01-22T18:32:45Z</updated><resolved>2015-11-06T19:15:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-05T00:27:50Z" id="153914549">This looks pretty good. Most of the implementation of the mapper moving to a base class made it hard to review, but at a glance, it looks the same as it was. I am curious about a couple things:
1. What about tests? Seems like we should have some new tests? With the current code, it looks like tests of the new mapper arent even possible, because of the logic diverting to the old implementation for all index created versions?
2. What is the intended behavior when merging back to 2.x? Based on the norelease comment, it seems you intend to have the field type cutover to the new behavior in 2.2? This seems scary and dangerous to do in a minor release? It means, for example, if someone has an index template with a geo_point field, and they upgrade from 2.1 to 2.2, they suddenly have different behavior on the newer indexes that they create, without knowing it? It is unclear to me from the changes what behavior or settings differences exist, though, so I may be overcautious here.
</comment><comment author="nknize" created="2015-11-05T02:52:02Z" id="153938869">1. Because the feature branch is a 2K line merge I've split this into a 4 part PR - hence the // norelease. This way I can get plenty of eyes on without having one overwhelming PR. The final PR will cut over to this new field type by changing Version.onOrBefore to Version.before and it will update all of the remaining tests to exercise the new field (along with backcompat).
2. Yes, for 2.2 merge the intent is to cut over new indexes to this new field from 2.2+ and maintain backcompat with all old indexes. It was originally intended for the 2.0 release but it hadn't baked long enough in time for the "no new feature deadline". We could add another field parameter called `version` and give users the option to remain on the legacy type until 3.0. I'm really not a fan of this idea but it does give an "off ramp". I think 3 months of continuous baking in a feature branch is plenty of QA for this to finally merge?
</comment><comment author="nknize" created="2015-11-05T05:24:15Z" id="153960732">@rjernst I went ahead and removed the duped code and added the field mapper tests since it makes most sense to be a part of this PR.  Once I remove all // norelease the tests will exercise both the new field and backcompat legacy field.
</comment><comment author="rjernst" created="2015-11-06T17:26:21Z" id="154478690">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Port of publishAddress should match port of corresponding boundAddress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14535</link><project id="" key="" /><description>Relates to #14503, #14514, #14513.

I refactored the code a bit to make it easier to understand.
</description><key id="115166622">14535</key><summary>Port of publishAddress should match port of corresponding boundAddress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T22:45:09Z</created><updated>2015-11-08T23:15:00Z</updated><resolved>2015-11-06T09:17:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-04T22:54:39Z" id="153895708">@rmuir knows this code better than I and should have a look as well.

Note that this isn't a big refactoring of publish_address, just an attempt to pick a sane one when we bind to different ports of different interfaces.
</comment><comment author="ywelsch" created="2015-11-05T10:52:32Z" id="154029162">@rmuir Can you help reviewing this?
</comment><comment author="rmuir" created="2015-11-05T11:46:50Z" id="154038960">Yeah, give me a little time. I need time as I have to re-learn all the horrible transport profiles code: which is at the root cause of all this.
</comment><comment author="rmuir" created="2015-11-05T13:07:27Z" id="154055238">The http case must be also handled, but it should be significantly simpler. when looking at the list of bound addresses, find the first one where `address.equals(publishAddress) || address.isAnyLocalAddress()` and return that port. fail if we don't find it.

you can see this easily by doing e.g. `nc -l 127.0.0.1 9200` and running `/bin/elasticsearch` on a dual stack system.
</comment><comment author="rmuir" created="2015-11-05T13:20:53Z" id="154057667">The only other idea i had to make this better, would be to remove NetworkService.resolvePublishHostAddresses() completely: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/network/NetworkService.java#L149-L150

The idea being, the caller must deal with this themselves. We could still have the "sorting logic" as a shared method somewhere so that both http and transport can use it, but then it would not be `NetworkUtils.sortAddresses(List&lt;InetAddress&gt; list)` but instead something more useful/less trappy like `NetworkUtils.sortAddresses(List&lt;InetSocketAddress&gt; list)`. Beware: other code may call this stuff today, or the sorting logic directly (multicast plugin), so I think its better as a followup issue after fixing the bug.
</comment><comment author="ywelsch" created="2015-11-05T18:38:02Z" id="154147848">Pushed a set of changes to address the comments.

@rmuir I like the idea of inlining and adapting `resolvePublishHostAddresses()` at call sites. At the same time, settings should be cleaned up (at the moment it's so difficult to see where each of the different settings come into play...)
</comment><comment author="rmuir" created="2015-11-05T18:52:04Z" id="154151233">looks good, thanks! And I agree with your opinion about cleaning up settings first.
</comment><comment author="ywelsch" created="2015-11-06T09:43:01Z" id="154363828">Back porting this to 2.1 is a bit tricky as resolvePublishHostAddresses only takes a single host as parameter. I would prefer to keep this change to 2.2.0 and 3.0.0.
@nik9000 I removed the 2.1 label on #14503. If you don't agree, we can discuss it again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Builds don't include the build_hash or build_timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14534</link><project id="" key="" /><description>Builds of master don't include the build_hash or build_timestamp that usually are available. Is it possible to get this info back?

Output of `http://localhost:9200/`
![image](https://cloud.githubusercontent.com/assets/1329312/10951721/a439a70c-8304-11e5-8111-af5dddbc57a2.png)
</description><key id="115147186">14534</key><summary>Builds don't include the build_hash or build_timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">spalger</reporter><labels><label>build</label></labels><created>2015-11-04T21:02:54Z</created><updated>2015-11-18T15:46:13Z</updated><resolved>2015-11-18T12:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-12T20:04:13Z" id="156219102">Didn't see this before but I saw similar behavior and opened and integrated #14564. Note that we are only outputting the short hash now, and the timestamp format is slightly different than before. Does that work for you @spalger?
</comment><comment author="spalger" created="2015-11-13T16:23:36Z" id="156478636">@jasontedor is it possible to use a standard date format for the build_date? What was wrong with ISO 8601?
</comment><comment author="jasontedor" created="2015-11-13T16:26:14Z" id="156479364">&gt; is it possible to use a standard date format for the build_date?

@spalger It's possible. That build timestamp is put into the jar manifest by a Gradle plugin. So to keep it simple, we just grab the timestamp directly out of the manifest (as a string). If you have a need for ISO 8601, we can certainly do that.

&gt; What was wrong with ISO 8601?

Nothing. Frankly, I was surprised that the Gradle plugin that we are using wasn't putting it into that format.
</comment><comment author="spalger" created="2015-11-13T16:28:06Z" id="156479828">Cool, it would be much appreciated :)
</comment><comment author="jasontedor" created="2015-11-13T21:29:40Z" id="156561274">&gt; Cool, it would be much appreciated :)

@spalger I opened #14751 on our side to change the formatting. Note that the plugin we are using to get this information into the manifest in the first place does not include timezone information so without some extra complexity the best that we have right now is a local date time. I also opened nebula-plugins/gradle-info-plugin#22 to see if we can get that included in the basic info.
</comment><comment author="spalger" created="2015-11-13T21:32:32Z" id="156561843">Awesome, thanks @jasontedor 
</comment><comment author="clintongormley" created="2015-11-18T12:58:28Z" id="157704114">Closed by https://github.com/elastic/elasticsearch/pull/14764 and https://github.com/elastic/elasticsearch/pull/14564
</comment><comment author="spalger" created="2015-11-18T15:46:13Z" id="157753913">:dancer: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to Created Node Environment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14533</link><project id="" key="" /><description>When trying to run Elasticsearch for the first time I am receiving the following error...

"Exception in thread "main" java.lang.IllegalStateException: Failed to created node environment
Likely root cause: jave.nio.file.AccessDeniedException: /etc/elasticsearch-2.0.0/data/elasticsearch/nodes"

In looking into the Elasticsearch folders, "nodes" doesn't even exist. Did this somehow unpack incorrectly?
</description><key id="115114647">14533</key><summary>Failed to Created Node Environment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dgriff20</reporter><labels /><created>2015-11-04T18:15:18Z</created><updated>2016-10-24T03:51:01Z</updated><resolved>2015-11-08T16:46:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-04T19:48:01Z" id="153843472">unix file permissions are incorrect in this case.
</comment><comment author="dgriff20" created="2015-11-04T20:15:35Z" id="153850086">rmuir - Thank you for your response. I'm a bit confused on how that could happen. I downloaded everything under my profile and believe I am the owner. At any rate, what would be a good way to fix it?
</comment><comment author="dgriff20" created="2015-11-06T20:30:29Z" id="154523744">Additional errors in here read...

log4j:WARN No appenders could be found for logger (bootstrap)
log4j:WARN Please initialize the log4j system properly.

Then it goes on to be unable to access a folder that does not exist yet.

Unable to access 'path.logs' (usr/share/elasticsearch.logs)

Interestingly, if I make that directory, it moves on and stops at the NEXT directory it can't make/find.
</comment><comment author="clintongormley" created="2015-11-08T16:46:17Z" id="154840413">@dgriff20 it sounds like you're using a package to install Elasticsearch, but then starting it with bin/elasticsearch directly, instead of using the system start/stop scripts.
</comment><comment author="peasead" created="2016-10-24T03:50:39Z" id="255642602">@dgriff20 This doesn't appear to have been resolved with an update from you, so in the event that anyone else is having this issue:

I ran across this while I was learning about ELK and doing a few different variations of the installation. When I ran `yum remove elasticsearch` it left a lot of config files and directories around. 

To fix this issue, before starting for the first time (after a `yum remove elasticsearch`), I manually removed `/var/lib/elasticsearch`, `/etc/elasticsearch`, and `/usr/share/elasticsearch`. After that I proceeded with my fresh installation.

I **completely believe** that there are better ways to do all of this; but I was just learning and when removing and reinstalling ran into this problem...above is what fixed it for me. 

Hope that helps.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The package is of bad quality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14532</link><project id="" key="" /><description>I'm getting the message "The package is of bad quality" after trying to install elasticsearch in a 64-bit Elementary OS 0.3.1 Freya distribution with the latest elasticsearch deb package. The error has the following details:

Lintian check results for /home/user/Downloads/elasticsearch-2.0.0.deb:
E: elasticsearch: dir-or-file-in-var-run var/run/elasticsearch/
</description><key id="115112764">14532</key><summary>The package is of bad quality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JulioV</reporter><labels /><created>2015-11-04T18:05:51Z</created><updated>2015-11-06T10:27:57Z</updated><resolved>2015-11-06T10:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-04T18:14:01Z" id="153814535">I looked into this: https://lintian.debian.org/tags/dir-or-file-in-var-run.html

"/var/run may be a temporary filesystem, so any directories or files needed there must be created dynamically at boot time."

@electrical thoughts?
</comment><comment author="nik9000" created="2015-11-04T18:16:06Z" id="153815622">I believe we create the directories we need in `/var/run` on startup. So its not telling us anything we didn't already know, at least.
</comment><comment author="electrical" created="2015-11-05T10:13:30Z" id="154016374">@JulioV Lintian it self follows a set of very arbitrary rules that are set by Debian that have no impact it self on the package.

An other discussion was a while ago about the Logstash packages at https://github.com/elastic/logstash/issues/2011

If there are real operational issues with the packages we can look at those and fix them of course.
</comment><comment author="JulioV" created="2015-11-05T15:32:31Z" id="154094720">There are some problems. Apparently, the deb package create several directories:
- /etc/default/elasticsearch
- /etc/init.d/elasticsearch
- /etc/elasticsearch
- /run/elasticsearch
- /usr/share/doc/elasticsearch
- /usr/share/lintian/overrides/elasticsearch
- /usr/share/elasticsearch
- /usr/share/elasticsearch/bin/elasticsearch
- /var/lib/elasticsearch
- /var/log/elasticsearch

When I try to run  "/usr/share/elasticsearch/bin/elasticsearch" there are two exceptions: 

Failed to configure logging...
ElasticsearchException[Failed to load logging configuration]; nested: NoSuchFileException[/usr/share/elasticsearch/config];

And..

Exception in thread "main" java.lang.IllegalStateException: Unable to access 'path.conf' (/usr/share/elasticsearch/config)
Likely root cause: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/config

"/usr/share/elasticsearch/config" is empty.
</comment><comment author="electrical" created="2015-11-05T15:38:58Z" id="154096418">Hi,

Those errors are indeed expected since the init script + defaults file set different values with the bin/elasticsearch script to give the right paths to the config file at /etc/elasticsearch
See https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L101-L102 for those details.

If none of those options are given then it uses indeed the config directory you got the error for.
</comment><comment author="JulioV" created="2015-11-05T15:53:31Z" id="154100646">Should I look to "/etc/init.d/elasticsearch"? Because is empty as well.
</comment><comment author="electrical" created="2015-11-05T16:07:33Z" id="154104423">Not sure if "Elementary OS" does anything funky. I just did a test install on Ubuntu 14.04 and works fine including init script.
</comment><comment author="JulioV" created="2015-11-05T16:55:12Z" id="154121032">Maybe, it is supposed to be based on Ubuntu but it might be an internal issue. Either way, I just downloaded the ES zip file and ran it without problems.
</comment><comment author="s1monw" created="2015-11-06T10:27:46Z" id="154373362">thanks for reporting this issue. We don't officially support this OS but if you feel like submitting a PR for this please go ahead. I will close this issue for now since we don't have the resources to work on it any time soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle-ify vagrant tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14531</link><project id="" key="" /><description>This gets the tar and tar_plugins tests working in gradle. It does so by
adding a subproject, qa/vagrant, which adds the following tasks:
## Verification

checkPackages - Check the packages against a representative sample of the
                linux distributions we have in our Vagrantfile
checkPackagesAllDistros - Check the packages against all the linux
                          distributions we have in our Vagrantfile
## Package Verification

checkCentos6 - Run packaging tests against centos-6
checkCentos7 - Run packaging tests against centos-7
checkFedora22 - Run packaging tests against fedora-22
checkJessie - Run packaging tests against jessie
checkOel7 - Run packaging tests against oel-7
checkOpensuse13 - Run packaging tests against opensuse-13
checkPrecise - Run packaging tests against precise
checkSles12 - Run packaging tests against sles-12
checkTrusty - Run packaging tests against trusty
checkVivid - Run packaging tests against vivid
## Vagrant

vagrantHaltCentos6 - Shutdown the vagrant VM running centos-6
vagrantHaltCentos7 - Shutdown the vagrant VM running centos-7
vagrantHaltFedora22 - Shutdown the vagrant VM running fedora-22
vagrantHaltJessie - Shutdown the vagrant VM running jessie
vagrantHaltOel7 - Shutdown the vagrant VM running oel-7
vagrantHaltOpensuse13 - Shutdown the vagrant VM running opensuse-13
vagrantHaltPrecise - Shutdown the vagrant VM running precise
vagrantHaltSles12 - Shutdown the vagrant VM running sles-12
vagrantHaltTrusty - Shutdown the vagrant VM running trusty
vagrantHaltVivid - Shutdown the vagrant VM running vivid
vagrantUpCentos6 - Startup a vagrant VM running centos-6
vagrantUpCentos7 - Startup a vagrant VM running centos-7
vagrantUpFedora22 - Startup a vagrant VM running fedora-22
vagrantUpJessie - Startup a vagrant VM running jessie
vagrantUpOel7 - Startup a vagrant VM running oel-7
vagrantUpOpensuse13 - Startup a vagrant VM running opensuse-13
vagrantUpPrecise - Startup a vagrant VM running precise
vagrantUpSles12 - Startup a vagrant VM running sles-12
vagrantUpTrusty - Startup a vagrant VM running trusty
vagrantUpVivid - Startup a vagrant VM running vivid

It does not make the "check" task depend on "checkPackages" so running the
vagrant tests is still optional. They are slow and depend on vagrant and
virtualbox.

The Package Verification tasks are useful for testing individual distros.

The Vagrant tasks are listed in `gradle tasks` primarily for discoverability.
</description><key id="115103996">14531</key><summary>Gradle-ify vagrant tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T17:25:24Z</created><updated>2015-11-05T19:29:06Z</updated><resolved>2015-11-05T19:29:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-04T17:27:41Z" id="153800309">This is very WIP. It'll need rebasing and lots more work before its truly ready.

I've got to say:
1. Gradle starts very very slowly. 10 seconds on my laptop.
2. Gradle does a phenomenal job of building only the stuff that I need.
</comment><comment author="nik9000" created="2015-11-04T17:28:48Z" id="153800636">There is a vagrant gradle plugin but it is based on an old version of gradle and just running the command line version does a half decent job. The big thing missing is squashing the build output properly.
</comment><comment author="nik9000" created="2015-11-04T17:29:30Z" id="153800810">Oh! And the tests support output in [TAP](http://testanything.org/) which I bet we could use.
</comment><comment author="nik9000" created="2015-11-05T15:55:37Z" id="154101201">&gt; Oh! And the tests support output in TAP which I bet we could use.

I ended up not being able to use tap4j because it wants to parse the whole TAP stream at once. The parser doesn't have any publicly available way to interpret a single line. So regex! Its not too complex:

```
/(?&lt;status&gt;ok|not ok) \d+(?&lt;skip&gt; # skip (?&lt;skipReason&gt;\(.+\))?)? \[(?&lt;suite&gt;.+)\] (?&lt;test&gt;.+)/
```

I think in total the parsing bit was like 10 lines of groovy. Getting it strapped into the ProgressLogger and Logger infrastructure was 10x that.
</comment><comment author="nik9000" created="2015-11-05T16:59:09Z" id="154122201">The vagrant up process gets piped to a progress logger and looks like:

```
&gt; Building 98% &gt; :qa:vagrant:vagrantUpPrecise &gt; Installing openjdk-8-jdk &gt; Get:109 http://archive.ubuntu.com/ubuntu/ precise-updates/main libxcb1-dev amd64 1.8.1-1ubuntu0.2 [82.5 kB]
```

The testing looks like:

```
&gt; Building 83% &gt; :qa:vagrant:checkCentos6 &gt; Tests [11|00|00/63],      OK [TAR PLUGINS] install jvm-example plugin from a directory with a space
```
</comment><comment author="electrical" created="2015-11-05T17:00:46Z" id="154122607">Perhaps its just me but i never can remember the names like 'jessie' and which version of debian or ubuntu it is.
Perhaps useful to just use version numbers like all the rest of them?
</comment><comment author="rjernst" created="2015-11-05T17:02:49Z" id="154123156">+1 to using version numbers if we can (I am also confused by this, always).

I think this looks good and we should get it in. There are some things we can do to make it a bit cleaner (for example, I think we should have it run with check, but skip if vagrant is not available), but those improvements can happen with followups.
</comment><comment author="nik9000" created="2015-11-05T17:10:33Z" id="154125229">&gt; +1 to using version numbers if we can (I am also confused by this, always).

I'm happy to make the version number change.
</comment><comment author="nik9000" created="2015-11-05T19:24:43Z" id="154162451">&gt; I'm happy to make the version number change.

I've done that and pushed it. I'm running one last smoke test then I'm going to squash this and merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update health.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14530</link><project id="" key="" /><description>Changing network address of curl commands to "localhost" instead of 192.x.x.x
</description><key id="115098889">14530</key><summary>Update health.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JGrubb</reporter><labels><label>docs</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T17:01:03Z</created><updated>2015-11-17T21:47:51Z</updated><resolved>2015-11-17T21:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-04T17:07:40Z" id="153794120">@JGrubb thanks for this! Can you sign the [CLA](https://www.elastic.co/contributor-agreement?q=contributor) and I will merge this in?
</comment><comment author="JGrubb" created="2015-11-04T19:43:50Z" id="153842451">Done.  That CLA is a teensy bit much (imho) to just contribute to the documentation, but it's signed up now.
</comment><comment author="dakrone" created="2015-11-17T21:47:51Z" id="157519531">Cherry picked this to master (c491dcc), 2.x (120de90), and 2.1 (ff08403). Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moving static factory methods to ShapeBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14529</link><project id="" key="" /><description>Currently the abstract ShapeBuilder class serves too many different purposes, making it hard to refactor and maintain the code. In order to reduce the size and responsibilities, this PR takes a first step by 
moving the static factory methods used to create new shape builders out to a new `ShapeBuilders` class, similar to how we already do in `QueryBuilders`.
</description><key id="115098782">14529</key><summary>Moving static factory methods to ShapeBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T17:00:41Z</created><updated>2015-11-23T14:44:46Z</updated><resolved>2015-11-17T10:08:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-11-17T03:31:44Z" id="157257488">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exceptions in general</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14528</link><project id="" key="" /><description>Hi guys,

I'm using the elasticsearch-php library to build applications with, and there is one thing that really bugs me; Exceptions.

Now @polyfractal and the other contributors have done an exceptional job there, but they can't be blamed for poor exception output. What I mean is that the main exception I get is a `BadRequest400Exception`, which is pretty generic, and has to be because the value returned from elasticsearch itself is as the following:

``` json
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[5gPjjrRuSxiYzNBtadLr3Q][live][0]: SearchParseException[[live][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":[]},\"filter\":{\"bool\":{\"must\":[{\"nested\":{\"path\":\"hierarchy\",\"filter\":{\"bool\":{\"must\":[{\"term\":{\"hierarchy.level\":1}},{\"term\":{\"hierarchy.path\":\"\\/tips\"}}]}}}},{\"nested\":{\"path\":\"schedule\",\"filter\":{\"bool\":{\"must\":[],\"must_not\":{\"terms\":{\"schedule.type\":[\"withheld\"]}}}}}}]}}}},\"sort\":{\"schedule.start\":{\"order\":\"desc\",\"nested_filter\":{\"term\":{\"schedule.type\":\"published\"}}}},\"from\":0,\"size\":12}]]]; nested: QueryParsingException[[live] [nested] failed to find nested object under path [hierarchy]]; }{[5gPjjrRuSxiYzNBtadLr3Q][live][1]: SearchParseException[[live][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":[]},\"filter\":{\"bool\":{\"must\":[{\"nested\":{\"path\":\"hierarchy\",\"filter\":{\"bool\":{\"must\":[{\"term\":{\"hierarchy.level\":1}},{\"term\":{\"hierarchy.path\":\"\\/tips\"}}]}}}},{\"nested\":{\"path\":\"schedule\",\"filter\":{\"bool\":{\"must\":[],\"must_not\":{\"terms\":{\"schedule.type\":[\"withheld\"]}}}}}}]}}}},\"sort\":{\"schedule.start\":{\"order\":\"desc\",\"nested_filter\":{\"term\":{\"schedule.type\":\"published\"}}}},\"from\":0,\"size\":12}]]]; nested: QueryParsingException[[live] [nested] failed to find nested object under path [hierarchy]]; }{[5gPjjrRuSxiYzNBtadLr3Q][live][2]: SearchParseException[[live][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":[]},\"filter\":{\"bool\":{\"must\":[{\"nested\":{\"path\":\"hierarchy\",\"filter\":{\"bool\":{\"must\":[{\"term\":{\"hierarchy.level\":1}},{\"term\":{\"hierarchy.path\":\"\\/tips\"}}]}}}},{\"nested\":{\"path\":\"schedule\",\"filter\":{\"bool\":{\"must\":[],\"must_not\":{\"terms\":{\"schedule.type\":[\"withheld\"]}}}}}}]}}}},\"sort\":{\"schedule.start\":{\"order\":\"desc\",\"nested_filter\":{\"term\":{\"schedule.type\":\"published\"}}}},\"from\":0,\"size\":12}]]]; nested: QueryParsingException[[live] [nested] failed to find nested object under path [hierarchy]]; }{[5gPjjrRuSxiYzNBtadLr3Q][live][3]: SearchParseException[[live][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":[]},\"filter\":{\"bool\":{\"must\":[{\"nested\":{\"path\":\"hierarchy\",\"filter\":{\"bool\":{\"must\":[{\"term\":{\"hierarchy.level\":1}},{\"term\":{\"hierarchy.path\":\"\\/tips\"}}]}}}},{\"nested\":{\"path\":\"schedule\",\"filter\":{\"bool\":{\"must\":[],\"must_not\":{\"terms\":{\"schedule.type\":[\"withheld\"]}}}}}}]}}}},\"sort\":{\"schedule.start\":{\"order\":\"desc\",\"nested_filter\":{\"term\":{\"schedule.type\":\"published\"}}}},\"from\":0,\"size\":12}]]]; nested: QueryParsingException[[live] [nested] failed to find nested object under path [hierarchy]]; }{[5gPjjrRuSxiYzNBtadLr3Q][live][4]: SearchParseException[[live][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":[]},\"filter\":{\"bool\":{\"must\":[{\"nested\":{\"path\":\"hierarchy\",\"filter\":{\"bool\":{\"must\":[{\"term\":{\"hierarchy.level\":1}},{\"term\":{\"hierarchy.path\":\"\\/tips\"}}]}}}},{\"nested\":{\"path\":\"schedule\",\"filter\":{\"bool\":{\"must\":[],\"must_not\":{\"terms\":{\"schedule.type\":[\"withheld\"]}}}}}}]}}}},\"sort\":{\"schedule.start\":{\"order\":\"desc\",\"nested_filter\":{\"term\":{\"schedule.type\":\"published\"}}}},\"from\":0,\"size\":12}]]]; nested: QueryParsingException[[live] [nested] failed to find nested object under path [hierarchy]]; }]",
    "status": 400
}
```

This is a very small Json object with a message string that is a mix of strings and Json objects. How is this ever supposed to be parsed into useful information for an API to use to create more specific exceptions?

As elasticsearch is a Json outputting application, then surely some better responses can come from it's exceptions? I'd like to be able to better ascertain what went wrong in my applications, without having to do some expensive string matching to figure this out.

I'm not trying to just moan, believe me, as 90% of the time this stuff works a charm, but when it doesn't (even if it's my stupid fault) I'd like some better information.
</description><key id="115094842">14528</key><summary>Exceptions in general</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">designermonkey</reporter><labels /><created>2015-11-04T16:42:11Z</created><updated>2015-11-05T15:39:09Z</updated><resolved>2015-11-05T15:39:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="designermonkey" created="2015-11-04T16:43:36Z" id="153787582">And on a slight tangent, the exception above is because of a missing mapping, yet elasticsearch never even tells me that, it balks on the fact it can't use a nested path. Surely, the missing mapping is the issue, not the path?
</comment><comment author="dakrone" created="2015-11-04T16:56:49Z" id="153791148">@designermonkey what version of ES are you using? With 2.0 REST exceptions are now rendered in a structured way, to make it easier to extract useful information out of them. Additionally, we are working on parsing queries on the coordinating node, which will reduce the amount of shard failures (only a single exception instead of an exception for each shard)
</comment><comment author="designermonkey" created="2015-11-04T20:08:57Z" id="153848611">I'm on 1.7 and not sure about upgrading yet. Any breaking changes to know about? Can an existing index be upgraded?
</comment><comment author="polyfractal" created="2015-11-04T20:44:26Z" id="153857374">BTW, this is what the exception would look like under 2.0:

``` json
{
   "error": {
      "root_cause": [
         {
            "type": "query_parsing_exception",
            "reason": "[nested] failed to find nested object under path [hierarchy]",
            "index": "test",
            "line": 14,
            "col": 25
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test",
            "node": "R33m3wHNSbGW8mkHcJwlpQ",
            "reason": {
               "type": "query_parsing_exception",
               "reason": "[nested] failed to find nested object under path [hierarchy]",
               "index": "test",
               "line": 14,
               "col": 25
            }
         }
      ]
   },
   "status": 400
}
```

There are indeed breaking changes, specifically around mappings which might cause some trouble.  You can read about them here:  https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html

The new [Migration Plugin](https://github.com/elastic/elasticsearch-migration) should help identify potential issues before upgrading, at least on the index structure / mapping / analysis side of things.  Won't be able to help with changes to query or response syntax.
</comment><comment author="designermonkey" created="2015-11-05T09:49:21Z" id="154009345">That is so much better :) Thanks for the info guys.

@polyfractal would this require an upgrade of the php lib to v2 as well?
</comment><comment author="polyfractal" created="2015-11-05T15:18:47Z" id="154089876">Nope, no upgrade required.  The ES-PHP 1.x branch never tried to deserialize the error response, since sometimes ES would send back non-JSON exceptions (mostly with older versions of ES iirc).  So you'll just get back a blob of (structured) JSON using the ES-PHP 1.x branch. 

If you know you're always querying a 2.x branch of Elasticsearch, you should be able to safely `json_decode()` the exception message all the time.

So with ES-PHP 1.x you'll get something like this:

``` php
print_r($e-&gt;getMessage())
  {"error":{"root_cause":[{"type":"query_parsing_exception","reason":"[nested] failed to find nested object under path [hierarchy]","index":"test","line":14,"col":35}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"test","node":"PHHrUvTRQbuiTaAtGsESvA","reason":{"type":"query_parsing_exception","reason":"[nested] failed to find nested object under path [hierarchy]","index":"test","line":14,"col":35}}]},"status":400}

print_r($e-&gt;getCode())
  400

print_r($e-&gt;getPrevious()-&gt;getMessage())
  Client error response
  [status code] 400
  [reason phrase] Bad Request
  [url] http://localhost:9200/_search
```

Whereas with the ES-PHP 2.x branch you'd see:

``` php
print_r($e-&gt;getMessage())
  query_parsing_exception: [nested] failed to find nested object under path [hierarchy]

print_r($e-&gt;getCode())
  400

print_r($e-&gt;getPrevious()-&gt;getMessage())
  {"error":{"root_cause":[{"type":"query_parsing_exception","reason":"[nested] failed to find nested object under path [hierarchy]","index":"test","line":14,"col":35}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"test","node":"PHHrUvTRQbuiTaAtGsESvA","reason":{"type":"query_parsing_exception","reason":"[nested] failed to find nested object under path [hierarchy]","index":"test","line":14,"col":35}}]},"status":400}
```

I'm actually working on the exception handling right now (in part spurred by your exception ticket, in part by working ES 2.0 itself).  PHP exceptions can only accept a string as their error message, which means we decode the exception into a nice PHP array to build the "english reason"...then throw it away.  Which is not terribly friendly.

So I'm working right now to add a `getStructured()` method to the exceptions so you can get the full ES structured exception in array form. 
</comment><comment author="designermonkey" created="2015-11-05T15:39:09Z" id="154096465">Thanks for the feedback. I really do wish I had time to help you out there, family and heavy work commitments leave me with no time :(
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>delete-by-query failing with Shield enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14527</link><project id="" key="" /><description>On Found we are testing the delete-by-query plugin in 2.0.0. I have enabled the plugin and when I issue a delete via _query, I get the following response.

```
{"error":{"root_cause":[{"type":"security_exception","reason":"action [indices:data/read/search] is unauthorized for user []"}],"type":"security_exception","reason":"action [indices:data/read/search] is unauthorized for user []"},"status":403}
```
</description><key id="115083685">14527</key><summary>delete-by-query failing with Shield enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">hub-cap</reporter><labels><label>:Plugin Delete By Query</label><label>bug</label><label>v2.1.0</label></labels><created>2015-11-04T15:52:03Z</created><updated>2015-11-10T19:47:42Z</updated><resolved>2015-11-10T19:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add qa/evil-tests </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14526</link><project id="" key="" /><description>There are a couple concrete issues this PR tries to address:
- removes guava as a test dependency. A ton of work happened here, but I think users will be happier when its no longer a test dependency, either.
- ensure some "evil tests" like SeccompTests are running in jenkins. Today they are not running in jenkins, and this will fix that.
- remove bogus permissions like "accessUserInformation" that are only needed... to test pluginmanager, which itself does not run under securitymanager anyway (so the whole thing is currently silly).

In general, functionality should be tested in a realistic way. But sometimes we need to break the rules, this gives us a place to do it. Going forwards I would like to explore giving each test its own clean JVM here too (lets just make it a real sandbox for doing evil tests).

Maybe we are getting closer to the point where pluginmanager is really ready to be factored out of core and then those tests can be moved out of here, but for now I think its a better place, since we need to do evil stuff like `chown`/`chmod` but at the same time, we want to know if e.g. third party code is doing that.
</description><key id="115083463">14526</key><summary>Add qa/evil-tests </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-04T15:51:14Z</created><updated>2015-11-04T18:49:55Z</updated><resolved>2015-11-04T18:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-04T15:54:57Z" id="153772036">+1
</comment><comment author="jpountz" created="2015-11-04T16:00:20Z" id="153773657">LGTM!
</comment><comment author="rmuir" created="2015-11-04T16:15:44Z" id="153779594">This PR just waits on a tweak by @rjernst so that `test` for this module is called automatically by `check`, for some reason that currently isn't happening, not sure why.
</comment><comment author="rjernst" created="2015-11-04T18:38:12Z" id="153823514">@rmuir I pushed a fix for check depending on test. I also commented out maven local, and added a comment explaining when to uncomment. The other changes LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to store / update urls without \ (backslashes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14525</link><project id="" key="" /><description>We are using Elasticsearch with Laravel 4.2 and we need to maintain a database containing documents with urls as one of the fields. Everything works fine as far as the storing of the values for other fields are concerned.The problem starts when we begin to index or store urls in these documents.

The / (forward slash) in the url is escaped (accompanied by a backslash) which is causing a lot of problem. They get stored as follows:

"url":"http:\/\/www.xyz.in\/brands\/qwerty\/xyz"

Is there any work around to get rid of these backslashes while storing or indexing data.
</description><key id="115067007">14525</key><summary>Unable to store / update urls without \ (backslashes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adarshchacko</reporter><labels /><created>2015-11-04T14:41:47Z</created><updated>2015-11-04T15:10:32Z</updated><resolved>2015-11-04T15:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="adarshchacko" created="2015-11-04T15:10:32Z" id="153756748">We were using curl in php to call elasticsearch which was the root cause of the problem.

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI failure] DateHistogramTests, "The datetime zone id 'CET' is not recognised"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14524</link><project id="" key="" /><description>Cannot reproduce this so far, but there are test failures in
- org.elasticsearch.messy.tests.DateHistogramTests.testDSTBoundaryIssue9491
- org.elasticsearch.messy.tests.DateHistogramTests.testIssue8209

http://build-us-00.elastic.co/job/es_core_2x_small/1726

```
org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.raiseEarlyFailure(TransportSearchTypeAction.java:316)
    ... 10 more
Caused by: java.lang.IllegalArgumentException: The datetime zone id 'Asia/Jerusalem' is not recognised
    at org.joda.time.DateTimeZone.forID(DateTimeZone.java:229)
    at org.elasticsearch.search.aggregations.support.ValuesSourceParser.token(ValuesSourceParser.java:119)
    at org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser.parse(DateHistogramParser.java:97)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:198)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:103)
    at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:835)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:655)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:621)
    at org.elasticsearch.search.SearchService.executeDfsPhase(SearchService.java:266)
    at 
```

and

```
...
Caused by: java.lang.IllegalArgumentException: The datetime zone id 'CET' is not recognised
    at org.joda.time.DateTimeZone.forID(DateTimeZone.java:229)
    at org.elasticsearch.search.aggregations.support.ValuesSourceParser.token(ValuesSourceParser.java:119)
    at org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser.parse(DateHistogramParser.java:97)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:198)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:103)
    at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
```
</description><key id="115066710">14524</key><summary>[CI failure] DateHistogramTests, "The datetime zone id 'CET' is not recognised"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>test</label></labels><created>2015-11-04T14:40:24Z</created><updated>2016-05-27T12:51:19Z</updated><resolved>2016-05-27T12:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-04T15:15:19Z" id="153758247">Looking at DateTimeZone#forID, it seems like Joda couldn't look up the zone from the internal Provider class that Joda loads at startup and holds the time zone data. This seems pluggable, but there are two providers in Joda already, UTCProvider which only knows UTC time zone and ZoneInfoProvider, which should be used by default. This one seems to gets loaded using the class loader from "org/joda/time/tz/data" when the first ISOChronology instance (also internal Joda Singleton) gets initialized.
I will try adding logging to both tests to confirm that if this failure happens again, we see which provider is used.
</comment><comment author="cbuescher" created="2015-11-04T15:53:34Z" id="153771671">Another one: http://build-us-00.elastic.co/job/es_core_2x_debian/530/
</comment><comment author="cbuescher" created="2015-11-05T10:35:02Z" id="154023321">Added Logging confirmed that Joda uses the UTCProvider instead of the ZoneInfoProvider that usually should load the time zone data file:

```
[2015-11-04 09:57:06,003][DEBUG][messy.tests              ] Provider for DateTimeZone [class org.joda.time.tz.UTCProvider] 
[2015-11-04 09:57:06,003][DEBUG][messy.tests              ] Available time zone IDs [[UTC]] 
```

UTCProvider is the last fallback when loading the Provider in `DateTimeZone#getDefaultProvider()`, before that it should succeed in loading resources from "org/joda/time/tz/data" in the joda-jar. So either the jar is broken or it doesn't even find it, which is strange given that its a Joda class trying to load it.
Joda seems pretty good at swallowing exceptions, but it looks like it should write something to Std.err if something goes wrong while loading. Unfortunately haven't found anything in the CI output so far.
</comment><comment author="s1monw" created="2015-11-06T08:57:46Z" id="154348338">yet another one - all on 2.x http://build-us-00.elastic.co/job/es_core_2.x_oel_6/1056/
</comment><comment author="s1monw" created="2015-11-06T09:25:01Z" id="154355161">@cbuescher I think I know why this fails... when you look at the latest failure I pasted above you can see that:

```
Suite: org.elasticsearch.script.groovy.GroovySecurityTests
Completed [1/43] on J0 in 2.87s, 1 test
....
....
....
groovy/target/J0/temp/org.elasticsearch.messy.tests.DateHistogramTests_EFD276EEE9F95729-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene54): {date=PostingsFormat(name=Asserting), d=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), dates=PostingsFormat(name=Asserting), _timestamp=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), value=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting)}, docValues:{date=DocValuesFormat(name=Lucene54), d=DocValuesFormat(name=Lucene54), dates=DocValuesFormat(name=Lucene54), value=DocValuesFormat(name=Lucene54), _timestamp=DocValuesFormat(name=Lucene54), _version=DocValuesFormat(name=Lucene54)}, sim=RandomSimilarityProvider(queryNorm=false,coord=crazy): {}, locale=sq_AL, timezone=Africa/Ouagadougou
  2&gt; NOTE: Linux 2.6.32-504.23.4.el6.x86_64 amd64/Oracle Corporation 1.8.0_60 (64-bit)/cpus=8,threads=1,free=275774088,total=501219328
  2&gt; NOTE: All tests run in this JVM: [GroovySecurityTests, TransformOnIndexMapperTests, ContextAndHeaderTransportTests, MinTests, CardinalityTests, GeoDistanceTests, DateHistogramTests]
Completed [20/43] on J0 in 3.19s, 33 tests, 2 errors &lt;&lt;&lt; FAILURES!
```

both run in JVM J0. If you run `GroovySecurityTests` on it's own you can see this in the sys.err:

```
java.io.IOException: Resource not found: "org/joda/time/tz/data/ZoneInfoMap" ClassLoader: sun.misc.Launcher$AppClassLoader@14dad5dc
    at org.joda.time.tz.ZoneInfoProvider.openResource(ZoneInfoProvider.java:210)
    at org.joda.time.tz.ZoneInfoProvider.&lt;init&gt;(ZoneInfoProvider.java:127)
    at org.joda.time.tz.ZoneInfoProvider.&lt;init&gt;(ZoneInfoProvider.java:86)
    at org.joda.time.DateTimeZone.getDefaultProvider(DateTimeZone.java:514)
    at org.joda.time.DateTimeZone.getProvider(DateTimeZone.java:413)
    at org.joda.time.DateTimeZone.forTimeZone(DateTimeZone.java:349)
    at org.joda.time.DateTimeZone.getDefault(DateTimeZone.java:157)
    at org.joda.time.chrono.ISOChronology.getInstance(ISOChronology.java:79)
    at org.joda.time.base.BaseDateTime.&lt;init&gt;(BaseDateTime.java:70)
    at org.joda.time.DateTime.&lt;init&gt;(DateTime.java:171)
    at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:218)
    at a9d18bbccfaebbc0e2a1f5d7d21d068d4a97c59a.run(a9d18bbccfaebbc0e2a1f5d7d21d068d4a97c59a:1)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:296)
    at org.elasticsearch.script.groovy.GroovySecurityTests.doTest(GroovySecurityTests.java:112)
    at org.elasticsearch.script.groovy.GroovySecurityTests.assertFailure(GroovySecurityTests.java:123)
    at org.elasticsearch.script.groovy.GroovySecurityTests.testEvilGroovyScripts(GroovySecurityTests.java:79)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```

it tires to load this without permissions and since it's a static block It will never be re-run. The simple fix would be to load that class before we fire up that test statically, what do you think? maybe @rmuir has an opinion
</comment><comment author="s1monw" created="2015-11-06T09:32:31Z" id="154359270">on 2.x this reproduces the issue:

```
 mvn clean test -Dtests.class=*.GroovySecurityTests\|*.DateHistogramTests -Dtests.jvms=1 -Dtests.seed=7AD385E8819A7321
```
</comment><comment author="cbuescher" created="2015-11-06T09:33:00Z" id="154359523">@simonw I see so when GroovySecurityTests is run before the other it messes up the resource loading? Those DateHistogramTests not using groovy scripting should be moved back to core anyway, I was going to do that but didn't want to cover this test failure by it. Should I move the tests or is this something that should somehow be fixed in the test setup as well?
</comment><comment author="s1monw" created="2015-11-06T09:34:23Z" id="154360267">IMO we should force load that class during bootstrap AND during EsTestCase initialization
</comment><comment author="s1monw" created="2015-11-06T09:38:52Z" id="154361802">This could work but I'd like to consult @rmuir here first

``` diff
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 6a72bb4..620333e 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -59,6 +59,11 @@ final class Bootstrap {
     private final CountDownLatch keepAliveLatch = new CountDownLatch(1);
     private final Thread keepAliveThread;

+    static {
+        // ensure we load all the timezones in the parent classloader with all permissions
+        org.joda.time.DateTimeZone.getDefault();
+    }
+    
     /** creates a new instance */
     Bootstrap() {
         keepAliveThread = new Thread(new Runnable() {
```
</comment><comment author="s1monw" created="2015-11-06T09:39:27Z" id="154362242">@cbuescher can you just put `org.joda.time.DateTimeZone.getDefault();` into the `GroovySecurityTests` for now to make this stop failing... 
</comment><comment author="cbuescher" created="2015-11-06T09:40:25Z" id="154362812">@s1monw before any tests are run?
</comment><comment author="s1monw" created="2015-11-06T09:40:44Z" id="154362949">just put it in a static block for now
</comment><comment author="cbuescher" created="2015-11-06T10:07:32Z" id="154369553">Pushed this to 2.x, although it never failed on master so far your reproduction makes it fail there also. Will put this also on master.
</comment><comment author="rmuir" created="2015-11-06T11:29:52Z" id="154387644">this is really a bug in groovy though: its missing an AccessController.doPrivileged block

Can you open an issue with them so we don't have the hack forever?
</comment><comment author="rmuir" created="2015-11-06T11:30:21Z" id="154387708">sorry, joda-time rather.
</comment><comment author="rmuir" created="2015-11-06T11:30:54Z" id="154387873">I think if we switch to java time, which runs in the jdk, we won't have issues like this.
</comment><comment author="s1monw" created="2015-11-06T11:36:49Z" id="154389900">I opened an issue
</comment><comment author="jodastephen" created="2016-01-28T12:43:22Z" id="176161392">I looked into this, but it is not clear to me what permissions you believe Joda-Time should be requesting.
</comment><comment author="jasontedor" created="2016-05-19T02:18:40Z" id="220210978">I opened JodaOrg/joda-time#375. I've tested a build that contains that fix and it resolves the issue reported here, #18017 and #18328 (and we can remove the hack added in 5563220fe53aae0c5580e05f09606e04668a9da8).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No 'status' response in 2.0.0-beta2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14523</link><project id="" key="" /><description>In ES 1.7 &amp; earlier, a standard request curl `localhost:9200` would return something like:

```
{
  "status" : 200,
  "name" : "Svarog",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.7.3",
    "build_hash" : "xxx",
    "build_timestamp" : "2015-10-15T01:01:01Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```

the 'status' field appears to be dropped in 2.0:

```
{
  "name" : "Svarog",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.0.0-beta2",
    "build_hash" : "xxx",
    "build_timestamp" : "2015-09-1401:01:01Z",
    "build_snapshot" : false,
    "lucene_version" : "5.2.1"
  },
  "tagline" : "You Know, for Search"
}
```

Was this intentional, or unintentional?
If unintentional, will it be reinserted?
</description><key id="115062198">14523</key><summary>No 'status' response in 2.0.0-beta2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luke3141</reporter><labels /><created>2015-11-04T14:23:31Z</created><updated>2015-11-04T14:58:58Z</updated><resolved>2015-11-04T14:58:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-04T14:58:58Z" id="153753752">You should rely on HTTP status code instead.
That's the reason it was removed as it tells the same thing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use fresh index settings for shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14522</link><project id="" key="" /><description>With this commit fresh index settings are used when creating a shard. This
works around a problem with Guice regarding too aggressive caching.

Fixes #14319
</description><key id="115053524">14522</key><summary>Use fresh index settings for shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Core</label><label>bug</label><label>v1.7.4</label></labels><created>2015-11-04T13:40:26Z</created><updated>2015-11-04T15:31:41Z</updated><resolved>2015-11-04T15:31:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-04T13:40:47Z" id="153722170">@bleskes Can you please have a look at the fix for 1.7?
</comment><comment author="jasontedor" created="2015-11-04T14:16:58Z" id="153733531">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.percolator.RecoveryPercolatorIT.testSinglePercolator_recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14521</link><project id="" key="" /><description>Out of memory Error caused the failure:

http://build-us-00.elastic.co/job/es_core_21_strong/198/

reproduce command:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=C3AB6B0AD64016DB -Dtests.class=org.elasticsearch.percolator.RecoveryPercolatorIT -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Des.node.mode=network -Dtests.locale=en_US -Dtests.timezone=Etc/UTC
```

Error:

```
java.lang.OutOfMemoryError: Direct buffer memory
    at __randomizedtesting.SeedInfo.seed([C3AB6B0AD64016DB:E9CBD0B4B13DE8B6]:0)
    at java.nio.Bits.reserveMemory(Bits.java:658)
    at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
    at org.jboss.netty.channel.socket.nio.SocketSendBufferPool$Preallocation.&lt;init&gt;(SocketSendBufferPool.java:156)
    at org.jboss.netty.channel.socket.nio.SocketSendBufferPool.&lt;init&gt;(SocketSendBufferPool.java:42)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.&lt;init&gt;(AbstractNioWorker.java:45)
    at org.jboss.netty.channel.socket.nio.NioWorker.&lt;init&gt;(NioWorker.java:45)
    at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
    at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
    at org.jboss.netty.channel.socket.nio.NioWorkerPool.&lt;init&gt;(NioWorkerPool.java:39)
    at org.jboss.netty.channel.socket.nio.NioWorkerPool.&lt;init&gt;(NioWorkerPool.java:33)

```
</description><key id="115052847">14521</key><summary>org.elasticsearch.percolator.RecoveryPercolatorIT.testSinglePercolator_recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label></labels><created>2015-11-04T13:35:46Z</created><updated>2016-01-19T14:55:22Z</updated><resolved>2016-01-19T14:55:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-19T14:55:22Z" id="172877148">This test no longer fails. (it has been rewritten and is now named RecoveryPercolatorIT#testPercolatorRecovery())
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiple Types for @timestamp field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14520</link><project id="" key="" /><description>Hey,

Since I updated Elasticsearch to 2.0, I get following Error:

```
failed to execute bulk item (index) index {[logstash-2015.11.04](...)java.lang.IllegalArgumentException: Mapper for [@timestamp] conflicts with existing mapping in other types:
[mapper [@timestamp] is used by multiple types. Set update_all_types to true to update [format] across all types.]
```

Hope you can help me guys!
Greetings
</description><key id="115041436">14520</key><summary>Multiple Types for @timestamp field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DKribl</reporter><labels /><created>2015-11-04T12:24:23Z</created><updated>2015-11-05T10:29:04Z</updated><resolved>2015-11-05T10:29:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2015-11-04T19:51:26Z" id="153844331">@DKribl Logstash, by default, will set the `@timestamp` field to be a date. Based on your error, this looks like an expected failure that is _new_ to Elasticsearch 2.0 because in 2.0, ES will enforce more safety controls on conflicting mappings.

This may be easiest to resolve on discuss.elastic.co since I think this is a configuration problem (the way Logstash may be configured? or whatever you have writing into Elasticsearch).
</comment><comment author="DKribl" created="2015-11-05T10:28:28Z" id="154021618">Hey,

thanks for the quick response. Unfortunately the errors increased massively after updateing, so I wasn't able to use my ELK stack anymore. Had to downgrade to 1.7. I will now use a clean ELK Stack to collect new data! This issue can be closed now, thanks :+1:  :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Jboss initiated client node not killed off in Elasticsearch 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14519</link><project id="" key="" /><description>Hi,

My web application is using an ES Node client. The ES server is a single node. Since we are in development stage, it was noticed that when we restarted JBoss for any reason the previous instance of ES Node client still appears in that ES server's cluster view.

Is there a time out that we can set? 

What happens if I use multiple JBoss Instances in a cluster using the same cluster? Do the virtual client nodes then need to know each other as well?
</description><key id="115041044">14519</key><summary>Jboss initiated client node not killed off in Elasticsearch 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-11-04T12:22:06Z</created><updated>2015-11-08T22:43:14Z</updated><resolved>2015-11-08T22:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T22:43:14Z" id="154883306">Hi @nithyanv 

This sounds like a question for the forums: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove guice from the index level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14518</link><project id="" key="" /><description>this is a WIP first cut at removing guice from the index level. With this we will only have a single injector that we create on node startup. No wiring at index creation or so anymore, anything is setup up-front. At this point all tests pass and it's technically ready to merge but I need to integrate a bunch of plugins to ensure all stuff works.
</description><key id="115028809">14518</key><summary>Remove guice from the index level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T11:05:45Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-11-05T14:06:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-04T16:20:38Z" id="153781037">I'll review it in a bit.
</comment><comment author="rjernst" created="2015-11-05T09:54:52Z" id="154010699">This is great! A couple small comments, but LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Receiving "Multiple entries with same key" error when cycling in and out new nodes on elasticsearch 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14517</link><project id="" key="" /><description>Hi,

I'm testing my cluster and cycling in new nodes and cycling out old ones however after waiting a couple of hours shards and replicas haven't sorted themselves out. When listing the status of all shards via GET /cat/_shards I receive...

{
    "error": {
        "root_cause": [
            {
                "type": "illegal_argument_exception",
                "reason": "Multiple entries with same key: [cloudtrail-2015.10.16][0], node[2yaILlgNTX24_lrlf77gIg], [P], v[25], s[STARTED], a[id=wtXiTTw9T4C9KHucOIQKGA]=org.elasticsearch.action.admin.indices.stats.CommonStats@7968034a and [cloudtrail-2015.10.16][0], node[2yaILlgNTX24_lrlf77gIg], [P], v[25], s[STARTED], a[id=wtXiTTw9T4C9KHucOIQKGA]=org.elasticsearch.action.admin.indices.stats.CommonStats@49422abd"
            }
        ],
        "type": "illegal_argument_exception",
        "reason": "Multiple entries with same key: [cloudtrail-2015.10.16][0], node[2yaILlgNTX24_lrlf77gIg], [P], v[25], s[STARTED], a[id=wtXiTTw9T4C9KHucOIQKGA]=org.elasticsearch.action.admin.indices.stats.CommonStats@7968034a and [cloudtrail-2015.10.16][0], node[2yaILlgNTX24_lrlf77gIg], [P], v[25], s[STARTED], a[id=wtXiTTw9T4C9KHucOIQKGA]=org.elasticsearch.action.admin.indices.stats.CommonStats@49422abd"
    },
    "status": 400
}

Any ideas?
</description><key id="115019577">14517</key><summary>Receiving "Multiple entries with same key" error when cycling in and out new nodes on elasticsearch 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels><label>:CAT API</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-11-04T10:18:32Z</created><updated>2015-11-18T13:28:28Z</updated><resolved>2015-11-18T13:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-04T10:44:22Z" id="153681009">can you look at the nodes log file and paste me the stacktrace for this exception?
</comment><comment author="robertsmarty" created="2015-11-04T11:09:23Z" id="153687949">Is this what you're after?

```
[2015-11-04 21:04:30,269][INFO ][rest.suppressed          ] /_cat/shards Params: {}
java.lang.IllegalArgumentException: Multiple entries with same key: [cloudtrail-2015.10.16][0], node[2yaILlgNTX24_lrlf77gIg], [P], v[25], s[STARTED], a[id=wtXiTTw9T4C9KHucOIQKGA]=org.elasticsearch.action.admin.indices.stats.CommonStats@4e720ea0 and [cloudtrail-2015.10.16][0], node[2yaILlgNTX24_lrlf77gIg], [P], v[25], s[STARTED], a[id=wtXiTTw9T4C9KHucOIQKGA]=org.elasticsearch.action.admin.indices.stats.CommonStats@4960652f
  at com.google.common.collect.ImmutableMap.checkNoConflict(ImmutableMap.java:150)
  at com.google.common.collect.RegularImmutableMap.checkNoConflictInBucket(RegularImmutableMap.java:104)
  at com.google.common.collect.RegularImmutableMap.&lt;init&gt;(RegularImmutableMap.java:70)
  at com.google.common.collect.ImmutableMap$Builder.build(ImmutableMap.java:254)
  at org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse.asMap(IndicesStatsResponse.java:65)
  at org.elasticsearch.rest.action.cat.RestShardsAction.buildTable(RestShardsAction.java:172)
  at org.elasticsearch.rest.action.cat.RestShardsAction.access$000(RestShardsAction.java:43)
  at org.elasticsearch.rest.action.cat.RestShardsAction$1$1.buildResponse(RestShardsAction.java:73)
  at org.elasticsearch.rest.action.cat.RestShardsAction$1$1.buildResponse(RestShardsAction.java:70)
  at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
  at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
  at org.elasticsearch.shield.action.ShieldActionFilter$SigningListener.onResponse(ShieldActionFilter.java:193)
  at org.elasticsearch.shield.action.ShieldActionFilter$SigningListener.onResponse(ShieldActionFilter.java:179)
  at org.elasticsearch.action.support.TransportAction$ResponseFilterChain.proceed(TransportAction.java:141)
  at org.elasticsearch.shield.action.ShieldActionFilter.apply(ShieldActionFilter.java:124)
  at org.elasticsearch.action.support.TransportAction$ResponseFilterChain.proceed(TransportAction.java:139)
  at org.elasticsearch.action.support.TransportAction$FilteredActionListener.onResponse(TransportAction.java:166)
  at org.elasticsearch.action.support.TransportAction$FilteredActionListener.onResponse(TransportAction.java:152)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.onCompletion(TransportBroadcastByNodeAction.java:357)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.onNodeResponse(TransportBroadcastByNodeAction.java:326)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction$1.handleResponse(TransportBroadcastByNodeAction.java:300)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction$1.handleResponse(TransportBroadcastByNodeAction.java:292)
  at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:185)
  at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:138)
  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
  at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
  at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
  at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
  at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
  at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="robertsmarty" created="2015-11-06T00:21:37Z" id="154241692">Update on this. Doing more testing I have found that the cluster does eventually go to a green state however the same error still exists when using GET /cat/_shards
</comment><comment author="clintongormley" created="2015-11-09T11:30:13Z" id="155034454">What version(s) of Elasticsearch are you using?
</comment><comment author="robertsmarty" created="2015-11-09T22:09:43Z" id="155214808">2.0.0 and still have the error when accessing GET /cat/_shards so can provide more info. Status of the cluster is green. I deleted the index above to clear the error however I now have it on a different index.
</comment><comment author="s1monw" created="2015-11-09T22:21:04Z" id="155217406">thanks, can you paste the output of `GET localhost:9200/_stats` please
</comment><comment author="robertsmarty" created="2015-11-10T00:24:48Z" id="155242150">![stats](https://cloud.githubusercontent.com/assets/4920375/11050813/72f71aec-879d-11e5-8069-eb1b40596687.jpg)

Trying to upload this txt file as jpg as I don't seem to be able to save to repo. Please rename to txt to open.
</comment><comment author="masaruh" created="2015-11-16T05:26:17Z" id="156919237">It happens when you have Shield and configure alias.
I believe it's fixed by https://github.com/elastic/elasticsearch/pull/14316.
</comment><comment author="GlenRSmith" created="2015-11-16T06:03:31Z" id="156924716">A companion to this is that _settings and _mapping return JSON bodies with duplicate keys.
</comment><comment author="clintongormley" created="2015-11-18T13:28:28Z" id="157713008">Thanks @masaruh. Closing in favour of #14316
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add information about index status (open/closed/etc) to the "Get Index" API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14516</link><project id="" key="" /><description>Since the "Index Status" API has been deprecated in #4854, there is not a convenient way to check whether the index exists, and is closed or open (coming from discussion in the Ruby's client repository, elastic/elasticsearch-ruby#224).

It would be nice to add the information to the ["Get Index"](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-index.html) API, so the user would have a succint way how to check for and existing and open in an expressive language (eg. `client.indices.get(index: 'closed', ignore_unavailable: true)['status'] == ''open`).
</description><key id="115012150">14516</key><summary>Add information about index status (open/closed/etc) to the "Get Index" API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2015-11-04T09:38:09Z</created><updated>2015-11-08T21:04:38Z</updated><resolved>2015-11-08T21:04:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T21:04:38Z" id="154871734">You can use the cluster state API instead:

```
GET _cluster/state/metadata/my_index
```

or even:

```
GET _cluster/state/metadata/my_index?filter_path=metadata.indices.*.state
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update plugin-script.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14515</link><project id="" key="" /><description>It is better not to show lang-groovy plugin in the example, as it is not used since elasticsearch 1.4.0
</description><key id="114998701">14515</key><summary>Update plugin-script.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awislowski</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T08:26:44Z</created><updated>2015-11-05T17:29:11Z</updated><resolved>2015-11-05T17:29:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-05T17:29:11Z" id="154130814">Merged this to master, 2.x, 2.1, and 2.0 branches, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT fails on 2.x and 2.0 on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14514</link><project id="" key="" /><description>Example failures:

http://build-us-00.elastic.co/job/es_core_2x_window-2008/193
http://build-us-00.elastic.co/job/es_core_2x_window-2012/156
http://build-us-00.elastic.co/job/es_core_21_window-2008/70

Reproduce command:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.qa.backwards:current -Dtests.seed=FDFEBB4487D6B7EC -Dtests.class=org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT -Dtests.method="testDeleteRoutingRequired" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Des.node.mode=local -Dtests.bwc.version=2.2.0-SNAPSHOT -Dtests.bwc.path=target/backwards -Dtests.jvm.argline=""-server -XX:+UseG1GC -XX:-UseCompressedOops -Djava.net.preferIPv4Stack=true"" -Dtests.locale=bg -Dtests.timezone=Africa/Lagos
```

Failure message:

```
FAILURE 71.8s | BasicBackwardsCompatibilityIT.testDeleteRoutingRequired &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: ClusterHealthResponse has timed out - returned: [{
   &gt;   "cluster_name" : "SUITE-CHILD_VM=[0]-CLUSTER_SEED=[-257963635650616420]-HASH=[1B66F7F9939D50]-cluster",
   &gt;   "status" : "green",
   &gt;   "timed_out" : true,
   &gt;   "number_of_nodes" : 7,
   &gt;   "number_of_data_nodes" : 7,
   &gt;   "active_primary_shards" : 4,
   &gt;   "active_shards" : 4,
   &gt;   "relocating_shards" : 0,
   &gt;   "initializing_shards" : 0,
   &gt;   "unassigned_shards" : 0,
   &gt;   "delayed_unassigned_shards" : 0,
   &gt;   "number_of_pending_tasks" : 0,
   &gt;   "number_of_in_flight_fetch" : 0,
   &gt;   "task_max_waiting_in_queue_millis" : 0,
   &gt;   "active_shards_percent_as_number" : 100.0
   &gt; }]
   &gt; Expected: is &lt;false&gt;
   &gt;      but: was &lt;true&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([FDFEBB4487D6B7EC:6D1AB79C0D9CADCA]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoTimeout(ElasticsearchAssertions.java:105)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.ensureClusterSizeConsistency(ESIntegTestCase.java:1092)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:600)
   &gt;    at org.elasticsearch.test.ESIntegTestCase.after(ESIntegTestCase.java:1956)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="114988430">14514</key><summary>org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT fails on 2.x and 2.0 on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label></labels><created>2015-11-04T07:16:58Z</created><updated>2015-11-06T09:17:24Z</updated><resolved>2015-11-06T09:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-11-04T07:17:42Z" id="153612445">@nik9000 would you be able to look at these failures? It looks to be only affecting the windows builds
</comment><comment author="nik9000" created="2015-11-04T14:31:55Z" id="153743566">&gt; @nik9000 would you be able to look at these failures? It looks to be only affecting the windows builds

I started yesterday actually. This is caused by #14503. I can reproduce it on linux too using -Djava.net.preferIPv4 or whatever its called. I'm still seeing if I can create a non-bwc recreation for that issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RareClusterStateIT#testDelayedMappingPropagationOnPrimary test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14513</link><project id="" key="" /><description>build url: http://build-us-00.elastic.co/job/es_core_master_strong/5442/

The following reproduce line made the test fail once locally:

```
gradle :core:integTest -Dtests.seed=AA231B6B048C00 -Dtests.class=org.elasticsearch.indices.state.RareClusterStateIT -Dtests.method="testDelayedMappingPropagationOnPrimary" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:-UseCompressedOops -XX:+AggressiveOpts" -Dtests.locale=ar_YE -Dtests.timezone=Brazil/West
```

But now I can make it fail again...

This test also failed yesterday: http://build-us-00.elastic.co/job/es_core_master_small/5436/
Before that I can't find any other failures.
</description><key id="114983095">14513</key><summary>RareClusterStateIT#testDelayedMappingPropagationOnPrimary test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label><label>v5.0.0-alpha1</label></labels><created>2015-11-04T06:43:33Z</created><updated>2015-11-06T09:17:24Z</updated><resolved>2015-11-06T09:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-04T10:47:26Z" id="153681548">I briefly looked at this and can add some pointers to where the problem lies. Basically the publish address is combination of a host of the ipv4 bound address and the port of ipv6 :

```
1&gt; [2015-11-03 05:25:45,601][INFO ][org.elasticsearch.test.transport] [node_t0] publish_address {127.0.0.1:9400}, bound_addresses {[::1]:9400}, {127.0.0.1:9401}
```

The result publish address is actually the address another node is bound to, which causes confusion :)

Somewhere in `NettyTransport#bindServerBootstrap(java.lang.String, java.net.InetAddress, org.elasticsearch.common.settings.Settings)` things go wrong..
</comment><comment author="bleskes" created="2015-11-04T11:38:53Z" id="153694832">it seems we are already tracking this here: https://github.com/elastic/elasticsearch/issues/14503
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CreateIndexIT.testCreateAndDeleteIndexConcurrently test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14512</link><project id="" key="" /><description>Failed today: http://build-us-00.elastic.co/job/es_core_master_centos/8366/
Doesn't reproduce here locally.

Also this failed before recently:
http://build-us-00.elastic.co/job/es_core_master_centos/8341/ (2 nov)
http://build-us-00.elastic.co/job/es_core_master_centos/8306/ (1 nov)

Before that I can't find any failures for this test, so maybe this failure has been introduced recently?
</description><key id="114972171">14512</key><summary>CreateIndexIT.testCreateAndDeleteIndexConcurrently test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label></labels><created>2015-11-04T05:26:04Z</created><updated>2016-03-15T03:30:44Z</updated><resolved>2016-03-15T03:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unable to set query options when using QueryBuilder.setSource(String)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14511</link><project id="" key="" /><description>(Using the Java client version 1.5.2)

``` Java
        return client.prepareSearch(INDEX_NAME)
                .setSource(query)
                .setExplain(debugMode)
                .setVersion(true)
                ;
```

Will render essentially to a MatchAllQuery. This happens because the code paths for the two usages differ. If any of `setExplain`, `setVersion` or the rest of the methods for setting query options are called, the code in `SearchSourceBuilder.innerToXContent(XContentBuilder, Params)` is called and as a consequence invalidates the query that was passed via setSource.

This means there is currently no way of loading a query via setSource while setting up query options (those options that could be passed along with the URL, and I'd expect to be able to set also outside of the context of the actual query).
</description><key id="114968782">14511</key><summary>Unable to set query options when using QueryBuilder.setSource(String)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Java API</label></labels><created>2015-11-04T04:48:06Z</created><updated>2015-11-05T13:08:15Z</updated><resolved>2015-11-04T08:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-04T08:40:13Z" id="153637044">In 1.x or 2.x, the way to achieve what you need is via the `extraSource`, which is what we use internally to pass along options that have been provided as part of the URL.

This is a bit of a flaw of the current java api when it comes to search, as it allows you to provide the whole source as a bytes array (or string, or map). At that point you can't really ask also for it to be modified by providing additional options. In master (3.0) we removed support for setting the source as string or bytes array, you always have to provide a structured `SearchSourceBuilder` object. We did this because parsing happens now on the coordinating node, and if you provide a request using the java api, no parsing should happen anymore as you already provide proper java objects.
</comment><comment author="synhershko" created="2015-11-04T18:06:32Z" id="153811590">Thanks Luca.

Just to make sure, you mean:

``` java
        return client.prepareSearch(INDEX_NAME)
                .setExtraSource(query)
                .setExplain(debugMode)
                .setVersion(true)
                ;
```

Right?

With regards to your plans for 3.0, we do need to have support to load queries from JSON files. This has many use cases, for example allowing other teams to submit PRs for query changes of various parts of the system and only teaching them JSON and a subset of the DSL. Or an easier way to maintain and track changes. Or using Mustache locally for performance reasons. Etc etc. And no, search templates don't really cut it for most of those.

Please provide a way to parse a string query into a SearchSourceBuilder, this is a critical feature I've seen used to many times to want to see it go away. Would be happy to jump on a call to discuss further.
</comment><comment author="javanna" created="2015-11-05T13:08:14Z" id="154055500">Yes @synhershko your code snippet looks good, that is what I meant. Note that the source is not just the query but the whole search request which supports also other sections than the query  (e.g. aggs, highlighting etc.).

As for your request to parse json as part of the java api, there might be a way in the future, we will discuss that but to me it is yet another reason why we should have a java REST client. The Java api should only deal with proper java objects that can be natively serialized over the wire, thus if parsing had to happen there, it would need to happen on the client before sending the request to any node, but for that to happen the parser would need to be aware of all the supported queries, certainly possible for a node client, but it would make less sense for a transport client. /cc @clintongormley 

That said there are a couple of outliers, like the template query, their parsing will still happen on the coordinating node, and the wrapper query, which does allow you to move the parsing to the data nodes by providing the inner query as bytes, but they are only two very particular queries.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Misplaced mapping.date.round_ceil comment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14510</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/current/_setting_changes.html#_log_messages_truncated) we talk about log messages, but then jump into;

&gt; Remove mapping.date.round_ceil setting for date math parsing #8889 (issues: #8556, #8598)

Is this misplaced?
</description><key id="114964582">14510</key><summary>Misplaced mapping.date.round_ceil comment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>v2.0.0</label></labels><created>2015-11-04T03:59:49Z</created><updated>2015-11-08T20:54:36Z</updated><resolved>2015-11-08T20:54:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fixed minor typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14509</link><project id="" key="" /><description>Fixes #14508
</description><key id="114963276">14509</key><summary>Fixed minor typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2015-11-04T03:47:13Z</created><updated>2016-01-18T07:50:08Z</updated><resolved>2015-11-08T20:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-11-04T03:55:38Z" id="153564480">Also found references to `/bin/elasticsearch` which could confuse users.
</comment><comment author="dakrone" created="2015-11-04T16:42:57Z" id="153787375">LGTM
</comment><comment author="clintongormley" created="2015-11-08T20:49:33Z" id="154869202">thanks @markwalkom - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo in parent/child top_children release notes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14508</link><project id="" key="" /><description>Minor typo [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/_parent_child_changes.html#_literal_top_children_literal_query_removed).

&gt; It wasn&#8217;t always faster than the has_child query and the was usually inaccurate
</description><key id="114963179">14508</key><summary>Typo in parent/child top_children release notes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>v2.0.0</label></labels><created>2015-11-04T03:46:01Z</created><updated>2015-11-08T20:48:40Z</updated><resolved>2015-11-08T20:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch API is not RESTful </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14507</link><project id="" key="" /><description>As mentioned in #2688 Elasticsearch HTTP API is not RESTful, but ES manual says something else: https://www.elastic.co/guide/en/elasticsearch/reference/current/api-conventions.html

An example of adding index alias:

This is NOT a restful call:

```
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "remove" : { "index" : "test1", "alias" : "alias1" } }
    ]
}'
```

But this could be RESTful:

```
curl -XPUT 'http://localhost:9200/_aliases/alias1' -d '
{
    "index" : "test1"
}'
```

or something like that could be RESTful too (but we known URI while creating new resource, so example with PUT is far better):

```
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "index" : "test1",
    "alias" : "alias1"
}'
```

I know that API can't be changed easily, so please correct documentation by removing "RESTful" adjective. ElasticSearch provides HTTP API based on JSON documents as a content-type, but this is not a REST. Thanks!
</description><key id="114953089">14507</key><summary>ElasticSearch API is not RESTful </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcinn</reporter><labels /><created>2015-11-04T02:08:18Z</created><updated>2017-06-29T01:50:47Z</updated><resolved>2015-11-08T22:41:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-04T11:05:02Z" id="153686666">When you say it's restfull, I presume you refer to the url structure. Agreed that for the example you gave the structure of the API is not the simplest construction. The reason is that the aliases API is capable of executing multiple API swaps in an _atomic_ fashion. This is important to do stuff like index version where you want switch the alias from the older version your index to the new one with now down time to the external application using the alias.

```
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "index_name_v2",
        "alias": "index_name"
      }
    },
    {
      "remove": {
        "index": "index_name_v1",
        "alias": "index_name"
      }
    }
  ]
}
```
</comment><comment author="marcinn" created="2015-11-04T11:23:46Z" id="153690782">I understand that. Maybe just RESTful principles does not fit ES API requirements. ES API does not have to be RESTful, and then shouldn't be named like that.

On the other side there is room for atomic and RESTful operations. Just create resource with representation of these operations, for example `http://localhost:9200/alias-operations` and PUT a collection of AliasOperation entities, defined as `{action: '&lt;action&gt;', index: '&lt;name&gt;', 'alias': '&lt;name&gt;'}`. ~~ActionOperation may have own unique ID and state of execution, and I should be able to get details about batch via `alias-operations/&lt;id&gt;`.~~ (EDIT) The state of single operation may be available via `alias-operations/&lt;id&gt;`, but this may be not required/useful. PUTting a batch of operations would be RESTful and they can be executed in a transaction.

Thanks.
</comment><comment author="bleskes" created="2015-11-04T11:36:49Z" id="153694543">@marcinn I don't think I follow you and see where this will be structurally different than the current API (also adding IDs to these action is an overkill - these are one off and are executed immediately ). Can you describe more complete requests/interactions?
</comment><comment author="marcinn" created="2015-11-04T11:41:36Z" id="153695226">I didn't expect following a RESTful principles. I'm just asking to remove "RESTful" adj. from website and documentation. ES API is not RESTful and probably never be.
</comment><comment author="clintongormley" created="2015-11-08T22:41:57Z" id="154883219">@marcinn Providing a description that most people will understand is a lot more important than being 100% semantically accurate.  Closing
</comment><comment author="marcinn" created="2015-11-09T00:27:06Z" id="154891102">So, @clintongormley, are you trying to say that people (or maybe only ElasticSearch users) are too dumb to see the difference? Is it o.k. when you're calling the bike a motorcycle? Have you heard about HATEOAS?

Just write "JSON over HTTP". It will be enough and more accurate. If you're not familiar with RESTful please read some about RESTful principles and architecture first, or maybe ask Roy Fielding.  

I've tried to built a RESTful client for ElasticSearch, but this is impossible due to ES API design. 

Thanks. Marcin.
</comment><comment author="marcinn" created="2017-02-23T11:35:13Z" id="281969027">ElasticSearch 5.x still has no REST API. It breaks many REST constraints. Please read carefully these great articles - http://t-code.pl/blog/2016/02/rest-misconceptions-0/

Please update documentation and web pages. Please write clearly that ES uses HTTP/JSON API. You don't need to explain anybody the differences. Just write truth. Thanks.  

&gt; "Adding, updating, retrieving and deleting data through CRUD REST APIs "

For God's sake, REST API is not about CRUD...</comment><comment author="ibnesayeed" created="2017-06-29T01:50:47Z" id="311839820">Roy Fielding, one of the principal authors of the HTTP specification and the originator of the Representational State Transfer (REST) architectural style, wrote on his blog in 2008:

&gt; I am getting frustrated by the number of people calling any HTTP-based interface a REST API. Today&#8217;s example is... ([Read full post](http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven))</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issues with retrieving a long with more that 16 digits.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14506</link><project id="" key="" /><description>Using Elasticsearch version 1.5.2.
1. Create an document.
   curl -XPOST "http://localhost:9200/sports2/_bulk" -d'
   {"index":{"_index":"sports2","_type":"athlete"}}
   {"name":"Michael", "birthdate":"1989-10-1", "sport":"Baseball", "rating": ["5", "4"],  "location":"46.22,-68.45", "long": 111111111111111111}
2. Query it
   curl -XGET "http://localhost:9200/sports2/_search" -d '
   {
   "query": {
       "query_string": { "query": "*" }
   }
   }'
3. Notice the field "long" in the response. One the first 16 significant digits match. The Java API interface (9300) gets the correct value.
   {
   "took": 9,
   "timed_out": false,
   "_shards": {
     "total": 5,
     "successful": 5,
     "failed": 0
   },
   "hits": {
     "total": 1,
     "max_score": 1,
     "hits": [
        {
           "_index": "sports2",
           "_type": "athlete",
           "_id": "AVDP5hc0KF0ooNLJ55I6",
           "_score": 1,
           "_source": {
              "name": "Michael",
              "birthdate": "1989-10-1",
              "sport": "Baseball",
              "rating": [
                 "5",
                 "4"
              ],
              "location": "46.22,-68.45",
              "long": _111111111111111100_
           }
        }
     ]
   }
   }
</description><key id="114944398">14506</key><summary>Issues with retrieving a long with more that 16 digits.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bijaybisht</reporter><labels><label>feedback_needed</label></labels><created>2015-11-04T00:51:38Z</created><updated>2015-11-04T21:27:00Z</updated><resolved>2015-11-04T21:27:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-04T16:52:53Z" id="153790043">@bijaybisht I'm unable to reproduce this:

```
DELETE /sports2
{}

POST /sports2/athlete?refresh
{
  "birthdate": "1989-10-1",
  "location": "46.22,-68.45",
  "long": 111111111111111111,
  "name": "Michael",
  "rating": [
    "5",
    "4"
  ],
  "sport": "Baseball"
}

{"acknowledged":true}
{"_index":"sports2","_type":"athlete","_id":"AVDTaRHyF0ooKpQMJExk","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}
```

---

```
GET /sports2/_search?pretty
{}

{
  "took" : 20,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "sports2",
      "_type" : "athlete",
      "_id" : "AVDTaRHyF0ooKpQMJExk",
      "_score" : 1.0,
      "_source":{
  "birthdate": "1989-10-1",
  "location": "46.22,-68.45",
  "long": 111111111111111111,
  "name": "Michael",
  "rating": [
    "5",
    "4"
  ],
  "sport": "Baseball"
}
    } ]
  }
}
```

What version of Elasticsearch are you using?
</comment><comment author="bijaybisht" created="2015-11-04T17:38:08Z" id="153803152">1.5.2
</comment><comment author="dakrone" created="2015-11-04T17:49:22Z" id="153807274">&gt; 1.5.2

Sorry, I see now that you had it in the original post, my bad for missing it.

I retried the same reproduction (same as above) with 1.5.2, however, the field comes back as `"long": 111111111111111111,`, do you have a client or anything that may be interfering with it?
</comment><comment author="bijaybisht" created="2015-11-04T18:19:02Z" id="153816875">I also have 1.7.3 version installed on the server and I see the same behavior.  I have tried curl, kibana and chrome plugin to retrieve; all exhibit the same thing.
</comment><comment author="dakrone" created="2015-11-04T18:30:20Z" id="153821589">I am testing with a version of your script:

``` bash
# delete index if exists
echo "Deleting..."
curl -XDELETE "http://localhost:9200/sports2"
echo

echo '{"index":{"_index":"sports2", "_type":"athlete", "_id": "1"}}' &gt; /tmp/bulk.json
echo '{"name":"Michael", "birthdate":"1989-10-1", "sport":"Baseball", "rating": ["5", "4"], "location":"46.22,-68.45", "long": 111111111111111111}' &gt;&gt; /tmp/bulk.json

echo "File:"
cat /tmp/bulk.json

echo "Indexing..."
curl -XPOST "http://localhost:9200/_bulk" --data-binary @/tmp/bulk.json
echo

echo "Refreshing..."
curl -XPOST "http://localhost:9200/sports2/_refresh"
echo

echo "Retrieving..."
curl -XGET "http://localhost:9200/sports2/_search?pretty" -d '
{
    "query": {
        "query_string": { "query": "*" }
    }
}'
echo
```

And here is the output I am getting with 1.7.3:

```
&#187; ./repro.sh
Deleting...
{"acknowledged":true}
File:
{"index":{"_index":"sports2", "_type":"athlete", "_id": "1"}}
{"name":"Michael", "birthdate":"1989-10-1", "sport":"Baseball", "rating": ["5", "4"], "location":"46.22,-68.45", "long": 111111111111111111}
Indexing...
{"took":133,"errors":false,"items":[{"index":{"_index":"sports2","_type":"athlete","_id":"1","_version":1,"status":201}}]}
Refreshing...
{"_shards":{"total":10,"successful":5,"failed":0}}
Retrieving...
{
  "took" : 20,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "sports2",
      "_type" : "athlete",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"name":"Michael", "birthdate":"1989-10-1", "sport":"Baseball", "rating": ["5", "4"], "location":"46.22,-68.45", "long": 111111111111111111}
    } ]
  }
}
```

If you run this exact script, do you see the same issue?
</comment><comment author="bijaybisht" created="2015-11-04T18:49:58Z" id="153827980">This works, I don't see the issue. My bad that I mentioned curl. It also works when I do a get from the browser it works well. I think the issue is related with Kibana (Both on safari and chrome) and the chrome plugin.
&lt;img width="1239" alt="screen shot 2015-11-04 at 10 49 20 am" src="https://cloud.githubusercontent.com/assets/4120848/10948292/c750775c-82e1-11e5-8c4b-626400cb130a.png"&gt;
 and the chrome plugin.
</comment><comment author="bijaybisht" created="2015-11-04T18:54:07Z" id="153829195">I also notice that the response from the server is fine (developer tools in chrome), its just the display which is an issue in the case of the chrome plugin, guess both the chrome plugin and the kibana are using the same library.
</comment><comment author="bijaybisht" created="2015-11-04T21:27:00Z" id="153870724">Raised an issue on kibana. Closing this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update upgrade.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14505</link><project id="" key="" /><description>Upgrading requires that your plugins are at the latest version. You should at the very least upgrade them after installing 2.0.0, but probably better to remove them before upgrading then install after the upgrade.

Closes #15389
</description><key id="114938168">14505</key><summary>Update upgrade.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tebriel</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-11-03T23:56:25Z</created><updated>2016-02-07T15:33:04Z</updated><resolved>2016-02-07T15:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T19:39:31Z" id="163368290">@tebriel any feedback for the latest review on this?
</comment><comment author="tebriel" created="2016-02-07T15:04:30Z" id="181027764">@dakrone Many apologies, I totally lost track of this PR. Addressing now.
</comment><comment author="tebriel" created="2016-02-07T15:33:04Z" id="181033258">Closing in favor of https://github.com/elastic/elasticsearch/pull/16491 which is pointed to master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added new community R client, edited old entry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14504</link><project id="" key="" /><description>I maintain two R clients, `elastic` and `elasticdsl`. This PR adds an entry for `elasticdsl` and edited `elastic` entry
</description><key id="114937120">14504</key><summary>Added new community R client, edited old entry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sckott</reporter><labels><label>docs</label></labels><created>2015-11-03T23:47:40Z</created><updated>2015-11-08T23:02:08Z</updated><resolved>2015-11-08T20:41:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T20:46:21Z" id="154869038">Reverted.  

@sckott these docs have moved here instead https://www.elastic.co/guide/en/elasticsearch/client/index.html

Please could you send a new PR

thanks
</comment><comment author="sckott" created="2015-11-08T21:31:42Z" id="154875925">Will do
</comment><comment author="clintongormley" created="2015-11-08T22:26:43Z" id="154881920">@sckott apologies - I got things mixed up. you added it to the right place after all!

I've fixed it and your changes are merged.  thanks
</comment><comment author="sckott" created="2015-11-08T23:02:08Z" id="154884358">No worries, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch publishes an address/port combination to which it isn't bound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14503</link><project id="" key="" /><description>It looks like you can convince Elasticsearch to publish an address/port combination to which it isn't bound. I'm not 100% sure the cause but it produces fun output like:

```
[2015-11-03 16:48:37,979][INFO ][transport                ] [external_0] publish_address {127.0.0.1:9400}, bound_addresses {[::1]:9400}, {127.0.0.1:9402}
```

I checked for a few minutes and didn't see the place where elasticsearch makes sure that its bound both ipv4 and ipv6 under the same port. So maybe it doesnt?
</description><key id="114923786">14503</key><summary>Elasticsearch publishes an address/port combination to which it isn't bound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Network</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T22:21:01Z</created><updated>2015-11-06T09:40:39Z</updated><resolved>2015-11-06T09:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-03T22:21:27Z" id="153507719">Found in 2.2.0. I'm not sure how far back it goes nor how important it is.
</comment><comment author="rmuir" created="2015-11-03T22:24:08Z" id="153508335">the whole point of having a publish address is to lie about what you are bound to. Its a really expert option like that, you do it if you are behind static NAT or something and have that firewall mapping you via a different ip/port.

personally, i'm not sure its needed at all really, if some code is reworked (e.g. i talk to you on the address you talked to me on). but in any case, this is intentional that its not checked.

but i do think its critical that its adjusted in the documentation, almost nobody should be using this parameter unless they are an expert. 
</comment><comment author="rmuir" created="2015-11-03T23:17:44Z" id="153519454">I spoke with @nik9000 about this on always on: there are competing problems. One is that automatic publish address selection is just address section, does not select the port number: which could happen if someone binds to [::1]:9200 and 127.0.0.1:9201 (something running on the last one). So in this case, things are simply wrong.

The other is that publish can be manually specified for expert cases, so we have to be careful. Nik is going to look into fixing the code.

Additionally, going along the path of "publish host is pretty confusing for most", more ideas:
- moving bind.host/publish.host to an expert setting of the docs:  "network.host" is preferred.
- remove warnings about publish host autoselection (in 2.x/master only right now), as we don't want to encourage people specifying it.
</comment><comment author="bleskes" created="2015-11-04T11:39:18Z" id="153694887">FYI we have a related build failure: https://github.com/elastic/elasticsearch/issues/14513
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>crank permgen for qa/bwc (2.x maven)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14502</link><project id="" key="" /><description>with java 7 by default, I get build failures with the permgen we have set. Maybe this stuff can be optimized, or for now we can just crank the value.

```
[INFO] QA: BWC: 2.0.0 ..................................... FAILURE [03:25 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 26:42 min
[INFO] Finished at: 2015-11-03T17:04:26-05:00
[INFO] Final Memory: 92M/2513M
[INFO] ------------------------------------------------------------------------
---------------------------------------------------
constituent[0]: file:/usr/local/apache-maven-3.2.2/lib/aether-impl-0.9.0.M2.jar
constituent[1]: file:/usr/local/apache-maven-3.2.2/lib/maven-settings-3.2.2.jar
constituent[2]: file:/usr/local/apache-maven-3.2.2/lib/org.eclipse.sisu.plexus-0.0.0.M5.jar
constituent[3]: file:/usr/local/apache-maven-3.2.2/lib/aether-api-0.9.0.M2.jar
constituent[4]: file:/usr/local/apache-maven-3.2.2/lib/wagon-file-2.6.jar
constituent[5]: file:/usr/local/apache-maven-3.2.2/lib/maven-aether-provider-3.2.2.jar
constituent[6]: file:/usr/local/apache-maven-3.2.2/lib/guava-14.0.1.jar
constituent[7]: file:/usr/local/apache-maven-3.2.2/lib/plexus-utils-3.0.17.jar
constituent[8]: file:/usr/local/apache-maven-3.2.2/lib/maven-model-builder-3.2.2.jar
constituent[9]: file:/usr/local/apache-maven-3.2.2/lib/maven-compat-3.2.2.jar
constituent[10]: file:/usr/local/apache-maven-3.2.2/lib/commons-cli-1.2.jar
constituent[11]: file:/usr/local/apache-maven-3.2.2/lib/maven-plugin-api-3.2.2.jar
constituent[12]: file:/usr/local/apache-maven-3.2.2/lib/maven-embedder-3.2.2.jar
constituent[13]: file:/usr/local/apache-maven-3.2.2/lib/maven-repository-metadata-3.2.2.jar
constituent[14]: file:/usr/local/apache-maven-3.2.2/lib/maven-core-3.2.2.jar
constituent[15]: file:/usr/local/apache-maven-3.2.2/lib/aether-spi-0.9.0.M2.jar
constituent[16]: file:/usr/local/apache-maven-3.2.2/lib/slf4j-simple-1.7.5.jar
constituent[17]: file:/usr/local/apache-maven-3.2.2/lib/cdi-api-1.0.jar
constituent[18]: file:/usr/local/apache-maven-3.2.2/lib/jsr250-api-1.0.jar
constituent[19]: file:/usr/local/apache-maven-3.2.2/lib/aopalliance-1.0.jar
constituent[20]: file:/usr/local/apache-maven-3.2.2/lib/wagon-provider-api-2.6.jar
constituent[21]: file:/usr/local/apache-maven-3.2.2/lib/org.eclipse.sisu.inject-0.0.0.M5.jar
constituent[22]: file:/usr/local/apache-maven-3.2.2/lib/aether-connector-wagon-0.9.0.M2.jar
constituent[23]: file:/usr/local/apache-maven-3.2.2/lib/wagon-http-2.6-shaded.jar
constituent[24]: file:/usr/local/apache-maven-3.2.2/lib/maven-model-3.2.2.jar
constituent[25]: file:/usr/local/apache-maven-3.2.2/lib/plexus-component-annotations-1.5.5.jar
constituent[26]: file:/usr/local/apache-maven-3.2.2/lib/commons-lang-2.6.jar
constituent[27]: file:/usr/local/apache-maven-3.2.2/lib/jsoup-1.7.2.jar
constituent[28]: file:/usr/local/apache-maven-3.2.2/lib/wagon-http-shared-2.6.jar
constituent[29]: file:/usr/local/apache-maven-3.2.2/lib/commons-io-2.2.jar
constituent[30]: file:/usr/local/apache-maven-3.2.2/lib/slf4j-api-1.7.5.jar
constituent[31]: file:/usr/local/apache-maven-3.2.2/lib/plexus-interpolation-1.19.jar
constituent[32]: file:/usr/local/apache-maven-3.2.2/lib/plexus-cipher-1.7.jar
constituent[33]: file:/usr/local/apache-maven-3.2.2/lib/plexus-sec-dispatcher-1.3.jar
constituent[34]: file:/usr/local/apache-maven-3.2.2/lib/maven-artifact-3.2.2.jar
constituent[35]: file:/usr/local/apache-maven-3.2.2/lib/sisu-guice-3.1.3-no_aop.jar
constituent[36]: file:/usr/local/apache-maven-3.2.2/lib/javax.inject-1.jar
constituent[37]: file:/usr/local/apache-maven-3.2.2/lib/maven-settings-builder-3.2.2.jar
constituent[38]: file:/usr/local/apache-maven-3.2.2/lib/aether-util-0.9.0.M2.jar
constituent[39]: file:/usr/local/apache-maven-3.2.2/conf/logging/
---------------------------------------------------
Exception in thread "main" java.lang.OutOfMemoryError: PermGen space
[2015-11-03 17:04:28,186][INFO ][external-node-service    ] Shutting down
```
</description><key id="114921333">14502</key><summary>crank permgen for qa/bwc (2.x maven)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-03T22:06:17Z</created><updated>2016-02-14T15:23:02Z</updated><resolved>2016-02-14T15:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-03T22:09:45Z" id="153505143">This is a new one for me! I certainly don't see these.

If we wanted to raise the permgen we'd have to fork the ExternalNodeService from maven - we probably should anyway but I couldn't figure out a way for maven's exec plugin to block waiting for the service which is supposed to run in the background during integration steps. Or bump maven's permgen.
</comment><comment author="rmuir" created="2015-11-03T22:10:45Z" id="153505369">yeah i was suggesting to bump maven's permgen, i think its a simple solution? its already bumped in pom.xml, we just bump it more.
</comment><comment author="rmuir" created="2015-11-03T22:11:14Z" id="153505460">hmm i'm wrong, we are just bumping for the tests which do forking...grrr
</comment><comment author="clintongormley" created="2016-02-14T15:23:02Z" id="183904911">This doesn't seem to be happening any more, and we're moving to gradle in master, so closing this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ubuntu init script fails to start the instance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14501</link><project id="" key="" /><description>I had to change this line in /etc/init.d/elasticsearch:114

``` bash
DAEMON_OPTS="$DAEMON_OPTS -Des.max-open-files=true"
```

To

``` bash
DAEMON_OPTS="$DAEMON_OPTS --Des.max-open-files=true"
```

Maybe not the best solution but now I have it working
</description><key id="114904449">14501</key><summary>Ubuntu init script fails to start the instance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">carlitux</reporter><labels /><created>2015-11-03T20:39:18Z</created><updated>2015-11-08T20:34:44Z</updated><resolved>2015-11-03T20:59:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-03T20:40:36Z" id="153481212">Which version of Elasticsearch and which version of Ubuntu?
</comment><comment author="carlitux" created="2015-11-03T20:43:50Z" id="153482198">elasticsearch 2.0.0 and ubuntu trusty 14.04.3
</comment><comment author="nik9000" created="2015-11-03T20:50:28Z" id="153484287">I just checked it again and it works for me. I don't have that line in /etc/init.d/elasticsearch. 114 is a blank line. Is there any chance this is a leftover from a previous install?
</comment><comment author="carlitux" created="2015-11-03T20:59:13Z" id="153486287">@nik9000 sorry I doble checked and it is added by ansible.
</comment><comment author="clintongormley" created="2015-11-08T20:34:44Z" id="154868387">@carlitux btw, changing that option to `--Des...` is just ignoring the option.  A `-Des` option needs to appear at the beginning of the command line.  Alternatively, use `--max-open-files true`.

That is a one-off test anyway, which these days can be replaced by examining the nodes info API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add property permissions so groovy scripts can serialize json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14500</link><project id="" key="" /><description>Allowing read to these properties is not really dangerous, even if
the code surrounding them is.

Closes #14488
</description><key id="114901383">14500</key><summary>Add property permissions so groovy scripts can serialize json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T20:23:23Z</created><updated>2015-11-08T20:36:35Z</updated><resolved>2015-11-03T21:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-03T20:26:20Z" id="153476880">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle build should enforce minimal jvm version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14499</link><project id="" key="" /><description>Its very unintuitive today: this happens often on accident because i switch JAVA_HOME to java 7 when backporting (and sometimes forget to switch it back). 

But in general, users may not be aware java 8 is required. Currently exceptions look like this:

```
:buildSrc:build UP-TO-DATE
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version : 2.8
  JDK Version    : 1.7.0_55-b13 (Oracle Corporation)
  OS Info        : Linux 4.2.0-16-generic (amd64)
:core:compileJava FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:compileJava'.
&gt; invalid flag: -Xdoclint:all/private

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED
```
</description><key id="114900649">14499</key><summary>gradle build should enforce minimal jvm version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-03T20:19:03Z</created><updated>2015-11-06T17:33:28Z</updated><resolved>2015-11-06T17:33:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-06T16:16:10Z" id="154455452">+1 I just fell into the trap and it took me some time to figure out the root problem
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.0.0 fails to start if the Marvel exporter agent is configured.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14498</link><project id="" key="" /><description>Elasticsearch 2.0.0 able to start without these lines in the yml file. Plugins license and marvel agent installed on the ES node. There is no error logs generated either except for gc.log. Monitoring nodes listed are reachable from the ES node and running OK with Kibana/Marvel.

As per documentation (https://www.elastic.co/guide/en/marvel/current/installing-marvel.html) following config was used
marvel.agent.exporters:
  id1:
    type: http
    host: [http://es-thr-mon2-m01:9200,es-thr-mon2-m02:9200]

Correct configuration need to use  with quotes sign as below:
marvel.agent.exporters:
  id1:
    type: http
    host: ["http://es-thr-mon2-m01:9200","es-thr-mon2-m02:9200"]
</description><key id="114898532">14498</key><summary>ES 2.0.0 fails to start if the Marvel exporter agent is configured.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels /><created>2015-11-03T20:06:22Z</created><updated>2015-11-19T09:17:14Z</updated><resolved>2015-11-19T09:17:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dineshbugide" created="2015-11-04T10:40:51Z" id="153680227">The above thing worked for me but I am not able to interact with the server using the URL:
http://localhost:9200/_plugin/marvel

when I try to hit the above URL in my browser it was saying nothing,instead it is showing a blank page
</comment><comment author="clintongormley" created="2015-11-08T22:32:30Z" id="154882223">@dineshbugide Marvel UI is now a Kibana app.  The docs explain how to set it up.

@ajaybhatnagar you need to tell us more about how you installed, configured, and started elasticsearch.
</comment><comment author="mdiehm" created="2015-11-17T18:30:38Z" id="157461451">I had the same issues today. I run debian jessie (8.2). Repos for elastic added to /etc/apt... as described in the documentation. Running elastic through systemd as described in your documentation. When I add the marvel.agent.exporters the way you tell people I get:

systemctl status elasticsearch

[2015-11-17 19:28:17,347][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
        at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
        at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
        at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
        at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
        at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
        at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
        at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
        at org.jboss.netty.channel.Channels.write(Channels.java:725)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:174)
        at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
        at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.finishHim(TransportNodesAction.java:203)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onOperation(TransportNodesAction.java:178)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$600(TransportNodesAction.java:94)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$2.handleResponse(TransportNodesAction.java:155)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$2.handleResponse(TransportNodesAction.java:147)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:185)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:138)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

my config looks like this:
marvel.agent.exporters:

  id1:
    type: http
    host: [http://131.152.2.121:9200, http://131.152.2.122:9200]
</comment><comment author="clintongormley" created="2015-11-18T16:16:46Z" id="157764918">@tlrx any ideas?
</comment><comment author="tlrx" created="2015-11-19T09:17:05Z" id="157997143">The documentation was indeed wrong and has been fixed recently. Now the [Installing Marvel](https://www.elastic.co/guide/en/marvel/current/installing-marvel.html#monitoring-cluster) page shows a configuration sample where the `host` have double quotes:

``` yaml
marvel.agent.exporters:
  id1:
    type: http
    host: ["http://es-mon-1:9200", "http://es-mon2:9200"]
```

In systemd based distributions, like @mdiehm is using, elasticsearch will not start if the `elasticsearch.yml` contains an error. 

The file `elasticsearch.log` remains empty but the command `journalctl -u elasticsearch.service` shows the error:

```
nov. 19 09:59:38 debian elasticsearch[2405]: Exception in thread "main" SettingsException[Failed to load setting
nov. 19 09:59:38 debian elasticsearch[2405]: host: [http:localhost:9200,localhost:9201]
nov. 19 09:59:38 debian elasticsearch[2405]: ^];
nov. 19 09:59:38 debian elasticsearch[2405]: Likely root cause: while scanning a plain scalar
nov. 19 09:59:38 debian elasticsearch[2405]: in 'reader', line 4, column 12:
nov. 19 09:59:38 debian elasticsearch[2405]: Please check http://pyyaml.org/wiki/YAMLColonInFlowContext for deta
nov. 19 09:59:38 debian elasticsearch[2405]: at com.fasterxml.jackson.dataformat.yaml.snakeyaml.scanner.ScannerI
...
org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.
nov. 19 09:59:38 debian elasticsearch[2405]: at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:241)
nov. 19 09:59:38 debian elasticsearch[2405]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.jav
nov. 19 09:59:38 debian elasticsearch[2405]: Refer to the log for complete error details.
nov. 19 09:59:39 debian systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE
nov. 19 09:59:39 debian systemd[1]: Unit elasticsearch.service entered failed state.
```

@ajaybhatnagar I suppose your case falls into the same category. If not, please feel free to reopen this issue and give us more information as @clintongormley suggested.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Java 9 type inference issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14497</link><project id="" key="" /><description>This commit fixes a compilation issue due to modified type inference in
the latest JDK 9 early access builds. We just have to lend a helping
hand to type inference by being explicit about the type.

Closes #14496
</description><key id="114898209">14497</key><summary>Fix Java 9 type inference issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T20:04:32Z</created><updated>2015-11-03T21:56:54Z</updated><resolved>2015-11-03T21:56:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-03T20:31:40Z" id="153478151">+1, thanks
</comment><comment author="nik9000" created="2015-11-03T20:35:29Z" id="153478966">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compilation failing on Java 9 early access build b90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14496</link><project id="" key="" /><description>Core compilation is failing due to modified type inference rules in JDK 9 early access build b90.

```
core/src/main/java/org/elasticsearch/common/collect/Iterators.java:31: error: cannot infer type arguments for ConcatenatedIterator&lt;&gt;
        return new ConcatenatedIterator&lt;&gt;(iterators);
               ^
  reason: cannot infer type-variable(s) T#1
    (varargs mismatch; Iterator&lt;? extends T#2&gt;[] cannot be converted to Iterator&lt;? extends T#1&gt;)
  where T#1,T#2 are type-variables:
    T#1 extends Object declared in class ConcatenatedIterator
    T#2 extends Object declared in method &lt;T#2&gt;concat(Iterator&lt;? extends T#2&gt;...)
```
</description><key id="114897932">14496</key><summary>Compilation failing on Java 9 early access build b90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T20:02:50Z</created><updated>2015-11-03T21:56:49Z</updated><resolved>2015-11-03T21:56:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Awkward Load Average on ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14495</link><project id="" key="" /><description>Hello everyone,

I have already created couple of threads on discuss.elastic.co but nobody replied and still couldn't find the main reason of that much high load average.

Threads are as follows, you can check all scenerio:
https://discuss.elastic.co/t/ngram-analyzer-and-huge-cpu-usage-under-50-req-sec/32797
https://discuss.elastic.co/t/many-slow-queries-and-high-cpu-load-on-load-test-50req-sec/32624

However to explain briefly I have two fields one is analyzed with standard tokenizer with ngram filter and other is only standard tokenizer. 

The problem is revelaed during load test.
When I run a load test on the field with standard tokenized it can reach to 600-700 rps
However, when I tested on with field which is analyzed with ngram filtering I cannot evet see 70 rps.

I was expecting decrease on query performance but load average and cpu load is awkward or I am missing something.

I have 24 cores on the server and when I ran "top" command I can see that %CPU is around 2200.
Is that normal? If not the possible reasons I can think:
1. Mistaken configuration.
2. Complex Query
3. Wrong architecture and Scaling 

**Btw, that is really huge problem that blocks us to make production deploy. I mean due to the fact that I am not sure if query causes this awkward behaviour, I cannot start using that on production. If any additional information is required I can create.**

Mehmet-
</description><key id="114897491">14495</key><summary>Awkward Load Average on ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mehmetgunturkun</reporter><labels /><created>2015-11-03T20:00:33Z</created><updated>2015-11-08T20:29:09Z</updated><resolved>2015-11-08T20:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T20:29:09Z" id="154867933">hi @mehmetgunturkun 

Let's keep this discussion in https://discuss.elastic.co/t/many-slow-queries-and-high-cpu-load-on-load-test-50req-sec/32624/8 for the moment.  We can open an issue here if we find a bug
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delayed allocation can miss a reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14494</link><project id="" key="" /><description>Closes #14445 
Relates to  #14010 and #14011

Implementation note:

The smallest test case I could come up with follows #14445 :
- Create 4 data nodes (and 1 master node, to make the test simpler).
- Create 2 indices with delayed shard allocation set to 10 seconds. Each index has 2 shards (1 primary and 1 replica). Balancer ensures that each data node gets a shard, either a primary or a replica shard.
- Shutdown one node that has a replica shard of first index.
- Delayed allocation kicks in (a task is scheduled to reroute in 10 seconds).
- Wait a second
- Shutdown second node that has a replica shard of second index (no new task is scheduled to reroute in 10 seconds).
- Make replica shard of first index obsolete by setting number_of_replicas to 0 (the task that was scheduled to reroute is still being scheduled).
- Wait until 3 shards (1 for first index and 2 for second index) are available. Wait at least 30 minutes to give a chance to delayed allocation of replica shard of second index to kick in.

Same as in #14010, the issue can be "fixed" by waiting until delayed routing of first shard has kicked in and manually calling reroute. In the given test this works by inserting the following lines before the last line in the test:

```
logger.info("--&gt; waiting after delayed shard assignment should kick in for test1");
Thread.sleep(15000);
logger.info("--&gt; reroute");
client().admin().cluster().prepareReroute().execute().actionGet();
```
</description><key id="114892589">14494</key><summary>Delayed allocation can miss a reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v1.7.4</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T19:35:44Z</created><updated>2015-11-12T16:58:08Z</updated><resolved>2015-11-12T16:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-03T19:38:07Z" id="153464710">@dakrone can you have a look please? Running all tests reveals no issue with removing the lines in RoutingService.
</comment><comment author="dakrone" created="2015-11-04T16:33:52Z" id="153784671">I think this LGTM, it should probably go into 2.1.0 and 2.0.1 since it is a bug fix?
</comment><comment author="ywelsch" created="2015-11-04T22:56:21Z" id="153896061">@dakrone I've added the labels for 2.1.0 and 2.0.1. Thanks for discussing and reviewing this.

@bleskes can you review this as well? (not the test, only the change in RoutingService)
</comment><comment author="bleskes" created="2015-11-05T16:15:52Z" id="154108878">I think the change looks good. I'd love it if we could make a unit test for this instead of an integration test, maybe in RoutingServiceTests .
</comment><comment author="ywelsch" created="2015-11-06T09:47:20Z" id="154365154">@bleskes What kind of unit test do you have in mind? One modelling the scenario of the current integration test? If so, this is not straightforward (I looked into it, and it needs refactoring on RoutingService for the unit test to be timing-independent).
</comment><comment author="bleskes" created="2015-11-09T19:33:57Z" id="155167163">@ywelsch discussed this further and @ywelsch is exploring options for mocking time for the purpose of unit testing.
</comment><comment author="ywelsch" created="2015-11-10T08:49:59Z" id="155360256">Added a unit test. @bleskes @dakrone can you have another look?
</comment><comment author="bleskes" created="2015-11-12T09:19:05Z" id="156042604">test looks good to me. I'm good with pushing one the comments are addressed. Thanks @ywelsch for taking the extra time to write a unit test
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change tar extension back to tar.gz instead of tgz</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14493</link><project id="" key="" /><description>Gradle defaults to tgz extension when tar is compressed. This changes
the tar distribution back to tar.gz. Note that this also means the maven
packaging type is now tar.gz.
</description><key id="114891061">14493</key><summary>Change tar extension back to tar.gz instead of tgz</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T19:27:54Z</created><updated>2015-11-03T20:09:55Z</updated><resolved>2015-11-03T19:41:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-03T19:29:16Z" id="153462537">+1 @jasontedor can you update https://github.com/Homebrew/homebrew/pull/45644 for this once it's in too?
</comment><comment author="nik9000" created="2015-11-03T19:33:55Z" id="153463666">LGTM
</comment><comment author="jasontedor" created="2015-11-03T19:34:20Z" id="153463774">@dakrone Thanks; yes, I'll keep track of this pull request and update Homebrew/homebrew#45644 or open a new pull request when this pull request is integrated.
</comment><comment author="jasontedor" created="2015-11-03T20:09:55Z" id="153473010">@dakrone I've updated Homebrew/homebrew#45644 with jasontedor/homebrew@81a9c3937feac6f0249b989bed95c916c9202b29 to reflect the change here to output to `.tar.gz`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport more ignores for gradle to simplify branch switching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14492</link><project id="" key="" /><description>This adds generated-resources to gitignores, and the build/ dir to
excludes for pattern checks.

closes #14438
</description><key id="114885246">14492</key><summary>Backport more ignores for gradle to simplify branch switching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-11-03T18:57:26Z</created><updated>2015-11-03T19:16:13Z</updated><resolved>2015-11-03T19:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-03T19:03:31Z" id="153455500">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping conflict error upgrading from 1.5 to 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14491</link><project id="" key="" /><description>When trying to start the master node of my cluster, just after the upgrade from 1.5 to 2.0 (yeah, that's a big jump), I get this error log:

```
[2015-11-03 18:15:10,948][ERROR][gateway                  ] [mon-01] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [logstash-2015.10.18], reason: [Mapper for [timestamp] conflicts with existing mapping in other types:
[mapper [timestamp] cannot be changed from type [date] to [string]]]
```

Unfortunately, I don't have the detailed description of the field or the relevant analyzer configuration for the field, but as its name states, it's a timestamp from logstash, so it should look like one of these:
- "dd/MMM/yyyy:HH:mm:ss Z"
- "YYYY-MM-dd HH:mm:ss"
- "HH:mm:ss"
- "YYYY-MM-dd HH:mm:ss,SSS ZZ"
- "YYYY-MM-dd HH:mm:ss,SSSZZ"
- "YYYY-MM-dd HH:mm:ss,SSS"

The log ends with this:

```
[2015-11-03 18:15:11,383][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalStateException: unable to upgrade the mappings for the index [logstash-2015.10.18], reason: [Mapper for [timestamp] conflicts with existing mapping in other types:
[mapper [timestamp] cannot be changed from type [date] to [string]]]
Likely root cause: java.lang.IllegalArgumentException: Mapper for [timestamp] conflicts with existing mapping in other types:
[mapper [timestamp] cannot be changed from type [date] to [string]]
        at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
        at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:345)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:296)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:242)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
        at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:226)
        at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

And the node shuts down..

I have carefully read #13169 and #13345, but none of these helped me, the node keeps failing to start.
</description><key id="114883357">14491</key><summary>Mapping conflict error upgrading from 1.5 to 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilladx</reporter><labels /><created>2015-11-03T18:47:26Z</created><updated>2016-01-05T17:26:04Z</updated><resolved>2015-11-04T14:31:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-04T10:47:23Z" id="153681542">If I read the exception correctly, your `logstash-2015.10.18` index has a `timestamp` field which is mapped as a `string` on one type and as a `date` on another type, which is something that elasticsearch can't cope with. In elasticsearch 1.x this issue would be silent and only trip when trying to search/sort/aggregate on a field. In Elasticsearch 2.0, we detect these inconsistencies more proactively. Unfortunately the only way to fix the issue would be to reindex these data.
</comment><comment author="chilladx" created="2015-11-04T14:31:02Z" id="153743129">Many thanks, @jpountz . So I'll close this issue, as it's not a bug, but a problem in my data. I'll try to downgrade back to 1.5.x, update my mappings, and then upgrade again to 2.0.
</comment><comment author="chilladx" created="2015-11-05T14:22:17Z" id="154071213">Just for the record, here is the fine manual about this: https://www.elastic.co/blog/great-mapping-refactoring

I'm in the "Conflicting field mappings" case, where, as stated by @jpountz , the only option is to reindex the data.
</comment><comment author="veronicacannon" created="2016-01-05T17:26:04Z" id="169070763">I tried to downgrade back to 1.7 (in my case) so I could delete my mappings and indices.  I get the same error when trying to start Elasticsearch in the older version.  This is a new install so I can safely delete the indices, mappings, logstash, etc.  Is there a way to do that without running Elasticsearch? 

Ubuntu 14
downgraded from Elasticsearch 2.1 to 1.7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix download link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14490</link><project id="" key="" /><description>Closes #14487
</description><key id="114875872">14490</key><summary>Fix download link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T18:08:17Z</created><updated>2015-11-05T19:21:19Z</updated><resolved>2015-11-05T19:18:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-03T18:41:17Z" id="153449484">LGTM
</comment><comment author="nik9000" created="2015-11-05T19:21:02Z" id="154161027">Merged to 2.0 and cherry picked to 2.1, 2.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refuse to load fields from _source when using the `fields` option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14489</link><project id="" key="" /><description>Spin-off from #14474. The `fields` option is intended to load stored fields. However it will try to load these fields from the source if they are not stored. This causes confusion because then eg. wildcards don't work.

We should deprecate the ability to load fields from _source and remove it in 3.0.
</description><key id="114875374">14489</key><summary>Refuse to load fields from _source when using the `fields` option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label></labels><created>2015-11-03T18:06:07Z</created><updated>2015-11-30T10:01:21Z</updated><resolved>2015-11-30T10:01:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-03T20:32:59Z" id="153478430">+1

On 3 nov. 2015 6:06 PM +0000, Adrien Grandnotifications@github.com, wrote:

&gt; Spin-off from#14474(https://github.com/elastic/elasticsearch/pull/14474). Thefieldsoption is intended to load stored fields. However it will try to load these fields from the source if they are not stored. This causes confusion because then eg. wildcards don't work.
&gt; 
&gt; We should deprecate the ability to load fields from _source and remove it in 3.0.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/14489).
</comment><comment author="nik9000" created="2015-11-03T20:36:43Z" id="153479591">&gt; We should deprecate the ability to load fields from _source and remove it in 3.0.

It was a great backwards compatibility step when going from 0.90.x to 1.x but I agree, it can go in 3.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>v2.0.0 security manager with groovy scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14488</link><project id="" key="" /><description>Please see this question:
http://stackoverflow.com/questions/33503890/elasticsearch-2-0-0-security-manager-with-groovy-script#

When using `JsonOutput.toJson()` in an update (inside an inline groovy script), I'm getting a security exception:

```
Caused by: java.security.AccessControlException: access denied ("java.util.PropertyPermission" "groovy.json.faststringutils.write.to.final.fields" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkPropertyAccess(SecurityManager.java:1294)
    at java.lang.System.getProperty(System.java:753)
    at groovy.json.internal.FastStringUtils.&lt;clinit&gt;(FastStringUtils.java:37)
    ... 42 more
```

The problem is that the new 2.0.0 security manager does not allow to read the `groovy.json.faststringutils.write.to.final.fields` system property that is used by Groovy's `FastStringUtils` class in its line 37.

I don't want to change or disable the new security manager, but I do need to serialize an object to a JSON string inside my groovy script.

NOT: I've set the following options to allow inline groovy scripting:

```
script.inline: on 
script.indexed: on
```

To reproduce, just invoke `JsonOutput.toJson(someObject)` inside an inline groovy script, such as in an update command.
</description><key id="114875257">14488</key><summary>v2.0.0 security manager with groovy scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">federico-peralta-schaffner</reporter><labels /><created>2015-11-03T18:05:48Z</created><updated>2015-11-03T21:29:25Z</updated><resolved>2015-11-03T21:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-03T18:14:45Z" id="153441431">on the side: you don't want to enable write to final fields (see https://github.com/elastic/elasticsearch/issues/8175)

&gt; but I do need to serialize an object to a JSON string inside my groovy script.

Can you elaborate more on this? It seems strange for a script to have to do this serialization.
</comment><comment author="federico-peralta-schaffner" created="2015-11-03T19:22:30Z" id="153460800">@rmuir Thanks for the prompt reply!

I don't want to enable writes to final fields. The problem is that the `FastStringUtils` Groovy class attempts to check the `groovy.json.faststringutils.write.to.final.fields` system property (this is Groovy code, not mine, please check `FastStringUtils` line 37). There the security error occurs, since the current security policy is not allowing to read system properties, whatever its value is. My point is that this seems to be a bug, since any Groovy class that internally uses the `FastStringUtils` class fails with a security exception (such as `JsonOutput`, which I'm using in my script).

&gt; &gt; but I do need to serialize an object to a JSON string inside my groovy script
&gt; 
&gt; Can you elaborate more on this? It seems strange for a script to have to do this serialization.

Sure, I have an array of objects in my document and my script serializes this array to a string, which is then stored in a new field of type `string`. But this is required as per the functional specifications. Anyways, even if I could change the requirements or implement some work-around, it wouldn't take too long before someone else attempts to indirectly use the Grovvy's `FastStringUtils` class.
</comment><comment author="rmuir" created="2015-11-03T20:25:15Z" id="153476582">Hi, thanks for the report. I opened a PR for this: https://github.com/elastic/elasticsearch/issues/14500

Separately, I'm not sure its the best to do json serialization like this inside a script... but it is not dangerous to allow the properties in question to be read.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0 tar.gz Not in gzip format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14487</link><project id="" key="" /><description>Hi,

I just downloaded 2.0.0 as per the instructions in Elasticsearch Reference [2.0] &#187; Getting Started &#187; Installation. When I go to unzip the file with tar -xvf elasticsearch-2.0.0-rc1.tar.gz, I get the response "tar: This does not look like a tar archive

gzip: stdin: got in gzip format
tar: Child returned status 1
tar: Error is not recoverable:exiting now"

Any idea what might be going wrong?
</description><key id="114872043">14487</key><summary>Elasticsearch 2.0 tar.gz Not in gzip format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dgriff20</reporter><labels><label>docs</label></labels><created>2015-11-03T17:53:58Z</created><updated>2016-01-15T12:40:23Z</updated><resolved>2016-01-15T12:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="reflection" created="2015-11-03T17:59:53Z" id="153434977">I think you might be missing a 'z' option:
`tar -xzvf elasticsearch-2.0.0-rc1.tar.gz`
</comment><comment author="nik9000" created="2015-11-03T18:00:21Z" id="153435100">The link over on https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_installation.html is wrong.
</comment><comment author="dgriff20" created="2015-11-03T18:01:54Z" id="153435719">reflection -  Tried it with the z added and unfortunately, I receive the same error. 
</comment><comment author="dgriff20" created="2015-11-03T18:03:17Z" id="153436836">nik9000 - Do you know the correct link? The one provided did download an elasticsearch-2.0.0-rc1.tar.gz file.
</comment><comment author="nik9000" created="2015-11-03T18:05:24Z" id="153438506">[This](https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.0.0/elasticsearch-2.0.0.tar.gz) should do. What does the file you have look like? I tried it using the link there and it downloaded a little xml file.
</comment><comment author="dgriff20" created="2015-11-03T18:08:02Z" id="153439722">I'll give that version a shot. Closer inspection shows the same for me. Seems like an XML renamed to tar.gz. Strange. Thanks for the help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range bucket in pipeline aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14486</link><project id="" key="" /><description>Hi,

Should be great to be able to perform a grouping of computed values using the pipeline aggregation range bucket.

Thanks,

David
</description><key id="114861958">14486</key><summary>Range bucket in pipeline aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davmrtl</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2015-11-03T17:01:55Z</created><updated>2016-08-17T14:13:27Z</updated><resolved>2016-05-24T10:31:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-11-05T15:42:23Z" id="154097472">What would you like to do with the values in the range buckets?  Do you mainly want to do group metrics like `min`/`max`/`avg`, etc?  Or also "sequential" operations like `moving_avg`?

I ask because the first set would be relatively simple, whereas the second is much more complicated.  Operations like a moving average rely on the previous data to be evenly spaced and contiguous.  We currently have a "gap policy" that helps fill in gaps in a series of data...but I'm not sure how that would work with ranges that can be unequally sized and spaced.

/cc @colings86 
</comment><comment author="nishasingla1224" created="2016-01-30T07:40:59Z" id="177095888">Hi 
I want to group the computed data using pipeline aggregations.Would be great to have a support of this feature.
</comment><comment author="clintongormley" created="2016-02-14T15:21:45Z" id="183904841">@davmrtl @nishasingla1224 I still don't understand what you mean by "grouping" here.  Could you provide a concrete example?
</comment><comment author="clintongormley" created="2016-05-24T10:31:02Z" id="221229733">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment><comment author="phoenixgao" created="2016-08-17T08:38:42Z" id="240349381">Imaging I have indexed events into elasticsearch, each event is like:
{"timestamp" : @timestamp, "event_name": "hit a button", "user_id": 123}

So I'd like to have a first buckets aggregation to count the times a user hit the button group by user_id, then I want to know the range of these counts

Is it possible to do this without a pipeline range aggregation? @clintongormley @polyfractal 
</comment><comment author="polyfractal" created="2016-08-17T13:26:32Z" id="240410014">@phoenixgao: you can do something like this:

``` json
{
   "query": {
      "constant_score": {
         "filter": {
            "term": {
               "event_name": "hit a button"
            }
         }
      }
   },
   "aggs": {
      "users": {
         "terms": {
            "field": "user_id"
         }
      },
      "click_stats": {
         "stats_bucket": {
            "buckets_path": "users._count"
         }
      }
   }
}
```

The query limits the docs to those that were "button click" events, the `terms` agg groups by user_id, and the `stats_bucket` will show you the min/max/avg, etc of the user click counts.

@phoenixgao If you have any more questions, please [make a thread at the discussion forums](https://discuss.elastic.co/).  Github tickets are more for bug reports and feature requests, general questions are better suited for the forums :)
</comment><comment author="phoenixgao" created="2016-08-17T14:13:27Z" id="240423992">@polyfractal Thanks!!
what I was asking might be a duplicate of https://github.com/elastic/elasticsearch/issues/17590 though
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to update the cluster state with the recovered state should make sure it will be recovered later</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14485</link><project id="" key="" /><description>We currently fail to follow the right failure paths.

See http://build-us-00.elastic.co/job/es_core_master_window-2008/2477/
</description><key id="114861536">14485</key><summary>Failure to update the cluster state with the recovered state should make sure it will be recovered later</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T16:59:52Z</created><updated>2015-11-04T09:56:25Z</updated><resolved>2015-11-04T09:43:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-03T20:01:28Z" id="153471016">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>refactory GroovySecurityTests into a unit test.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14484</link><project id="" key="" /><description>This was basically a resurrected form of the tests for the old sandbox.
We use it to check that groovy scripts have some degree of additional containment.

The other scripting plugins (javascript, python) already have this as a unit test,
its much easier to debug any problems that way.
</description><key id="114859637">14484</key><summary>refactory GroovySecurityTests into a unit test.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T16:51:24Z</created><updated>2015-11-03T18:40:06Z</updated><resolved>2015-11-03T18:40:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-03T17:39:24Z" id="153428503">LGTM, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix dangling comma in ClusterBlock#toString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14483</link><project id="" key="" /><description>This commit address some serious health issues that could arise from a
dangerous combination of OCD, string concatenation, and dangling
commas,
</description><key id="114859595">14483</key><summary>Fix dangling comma in ClusterBlock#toString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T16:51:10Z</created><updated>2015-11-08T19:36:55Z</updated><resolved>2015-11-03T16:55:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-03T16:52:26Z" id="153414568">HAHA . LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove InternalLineStringBuilder and InternalPolygonBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14482</link><project id="" key="" /><description>This is a first step in reducing the number of ShapeBuilders before we start making the remaining builders implement Writable for the search refactoring. InternalLineStringBuilder seems to have no use other than in tests, and those tests don't assert anything to begin with, so this removes them. Same for InternalPolygonBuilder which seems to be only there for chaining inside builders.

Relates to #14416
</description><key id="114858430">14482</key><summary>Remove InternalLineStringBuilder and InternalPolygonBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T16:46:11Z</created><updated>2015-11-19T23:06:05Z</updated><resolved>2015-11-19T23:06:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-03T16:47:59Z" id="153413304">@nknize could you have a quick look at this to make sure I didn't remove anything important?
</comment><comment author="cbuescher" created="2015-11-05T15:40:46Z" id="154097022">@nknize also removed the Ring class and replaced it by LineStringBuilder. With that gone, I'm thinking of also merging BaseLineStringBuilder / LineStringBuilder and BasePolygonBuilder / Polygonbuilder because I don't see much value in this abstraction at this point. wdyt? Should I open a separate PR for this?
</comment><comment author="cbuescher" created="2015-11-17T11:32:19Z" id="157343466">Summing up the changes:
- removing InternalLineStringBuilder
  - replaced occurences in Tests by LineStringBuilder
  - replaced MultiLineStringBuilder#linestring() with MultiLineStringbuilder#linestring(LineStringBuilder)
- removing InternalPolygonBuilder
  - replaced with PolygonBuilder/BasePolygonBuilder 
  - replaced MultiPolygonBuilder#polygon with MultiPolygonBuilder#polygon(BasePolygonBuilder)
- removing Ring 
  - replaced with LineStringBuilder 
  - LineStringBuilder gets the close() method which adds starting point as the endpoint
  - replacing BasePolygonBuilder#hole() by BasePolygonBuilder#hole(LineStringBuilder)

In a follow up PR I'd also like to merge BasePolygonBuilder/PolygonBuilder and BaseLineStringBuilder/LineStringBuilder since after this change there is no real reason for having abstract base classes at the moment.
</comment><comment author="nknize" created="2015-11-19T22:23:39Z" id="158219443">@cbuescher This is a really good start to the cleanup. I went ahead and checked out the PR and ran some local validation checks - all passed.

++ to removing the abstract base classes in the next PR. 

LGTM!!
</comment><comment author="cbuescher" created="2015-11-19T22:55:10Z" id="158226446">Thanks a lot, will merge this then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve some logging around master election and cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14481</link><project id="" key="" /><description>Tweaks done while debugging http://build-us-00.elastic.co/job/es_core_master_window-2008/2477/

Most notably we now log cluster state blocks in cluster stat pretty print.

```
version: 1
state uuid: 6uhIU4sUTTGrqm5BgxwAIA
from_diff: false
meta data version: 0
blocks: 
   _global_:
      1,state not recovered / initialized, blocks READ,WRITE,METADATA_READ,METADATA_WRITE,
nodes: 
   {node_t2}{lrWB70f4QyGe1WSlzuZJ-Q}{127.0.0.1}{127.0.0.1:30102}[mode=&gt;network]
   {node_t1}{iOipMnqDRTafJDT-HHPJzQ}{127.0.0.1}{127.0.0.1:30101}[mode=&gt;network]
   {node_t0}{pxYLA6aNSQiEGcL35WI6QQ}{127.0.0.1}{127.0.0.1:30100}[mode=&gt;network]
   {node_t3}{UgliuRPhSz6-FhzaBSlalw}{127.0.0.1}{127.0.0.1:30103}[mode=&gt;network], local, master
routing_table (version 0):
routing_nodes:
-----node_id[lrWB70f4QyGe1WSlzuZJ-Q][V]
-----node_id[UgliuRPhSz6-FhzaBSlalw][V]
-----node_id[pxYLA6aNSQiEGcL35WI6QQ][V]
-----node_id[iOipMnqDRTafJDT-HHPJzQ][V]
---- unassigned
```
</description><key id="114851859">14481</key><summary>Improve some logging around master election and cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T16:17:20Z</created><updated>2015-11-08T21:05:17Z</updated><resolved>2015-11-04T09:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-03T16:23:56Z" id="153404426">LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete only a particular field and value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14480</link><project id="" key="" /><description>Hi,

I want to delete only one particular field and value in elasticsearch using elastic4s library. However, all I see everywhere is different mechanisms to delete the whole document. Can anyone help? is this supported?
</description><key id="114813817">14480</key><summary>Delete only a particular field and value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">208rishabh</reporter><labels /><created>2015-11-03T13:20:11Z</created><updated>2015-11-04T10:52:58Z</updated><resolved>2015-11-04T10:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-04T10:52:58Z" id="153682570">This can be achieved by running a scripted update. It would look something like this:

```
curl -XPOST 'localhost:9200/index/type/id/_update' -d '{
    "script" : "ctx._source.remove(\"name_of_the_field_to_remove\")"
}' 
```

See https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html for more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use plugin name from plugin's properties file rather than plugin's class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14479</link><project id="" key="" /><description>Simple backport for 2.0 branch (already fixed in 2.1, 2.x and master)

closes #14357 
</description><key id="114803204">14479</key><summary>Use plugin name from plugin's properties file rather than plugin's class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.1</label></labels><created>2015-11-03T12:14:18Z</created><updated>2015-11-10T16:37:46Z</updated><resolved>2015-11-10T16:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-11-03T12:15:34Z" id="153335583">@dadoonet Can you have a look please? Thanks
</comment><comment author="dadoonet" created="2015-11-09T16:42:53Z" id="155118514">Sorry it took so long @tlrx. LGTM.
</comment><comment author="tlrx" created="2015-11-10T16:37:46Z" id="155480233">@dadoonet thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Non java bin plugins on 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14478</link><project id="" key="" /><description>The existing support plugin is a bin plugin in bash and powershell. It the plugin-descriptor.properties allows for either site plugins (which require a _site folder) or java plugins with need to provide classes to load. So it appears this breaks the ability to use non-java pin plugins. 
</description><key id="114792646">14478</key><summary>Non java bin plugins on 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jjfalling</reporter><labels /><created>2015-11-03T11:15:10Z</created><updated>2015-11-04T16:12:19Z</updated><resolved>2015-11-04T16:12:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-03T14:36:22Z" id="153373660">Is there really a fourth type of plugin (we already have site, jvm, site+jvm)?

or can maybe this plugin just have a simple site folder that has a single file that says "please use the shell scripts for diagnostic functionality"
</comment><comment author="jjfalling" created="2015-11-04T12:17:24Z" id="153702619">The _site folder would be a potential workaround. In the long term are
moving away from having the diagnostics tool as a plugin so this won't
affect us. However I was under the impression that we intended to have
a bin type of plugin in 1.x.
</comment><comment author="rmuir" created="2015-11-04T12:22:39Z" id="153703440">I am not sure where that impression came from, but i can tell you that adding that folder is 1000x safer and easier than adding another plugin type and complicating all those codepaths.

We already have unnecessary types to support today (e.g. jvm+site) and I don't think we need add any more for any reason.
</comment><comment author="jjfalling" created="2015-11-04T16:12:19Z" id="153777623">Sounds reasonable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support to specify @SuppressLocalMode and @SuppressNetworkMode on a per-method level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14477</link><project id="" key="" /><description>Can help in situations such as #14476
</description><key id="114791405">14477</key><summary>Add support to specify @SuppressLocalMode and @SuppressNetworkMode on a per-method level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>review</label><label>test</label></labels><created>2015-11-03T11:10:16Z</created><updated>2015-11-16T13:34:52Z</updated><resolved>2015-11-16T13:34:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-13T16:14:32Z" id="156476430">@ywelsch do we still need this?
</comment><comment author="ywelsch" created="2015-11-13T16:26:42Z" id="156479485">I'm not passionate about this (anymore). However, if we don't add this, we should at least put `@Target(ElementType.TYPE)` on `SuppressLocalMode` and `SuppressNetworkMode` so that it is clear that the annotation can only be used on a class and not a method.
</comment><comment author="bleskes" created="2015-11-14T10:17:48Z" id="156681199">&gt;  we should at least put @Target(ElementType.TYPE) on SuppressLocalMode and SuppressNetworkMode 

+1 . Just keep it simple for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test with @SuppressLocalMode and discovery.type=local diverges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14476</link><project id="" key="" /><description>Currently there is a test (`ClusterServiceIt.testMasterAwareExecution`) on Jenkins that does not terminate. 

http://build-us-00.elastic.co/job/es_core_master_medium/2568
(reported by @nik9000)

It seems to be not related to cluster state, but more related to test setup, namely interplay of @SuppressLocalMode and put("discovery.type", "local&#8221;). This means that we have combination of node.mode = network and discovery.type = local. Only the tests in ClusterServiceIT are configured like that. Should probably be changed? I reproduced the issue on my machine (but not deterministic even if providing seed...)

Small test case producing the issue sometimes (always on fresh JVM)

```
package org.elasticsearch.action.support.master;

import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.test.ESIntegTestCase;
import org.elasticsearch.test.InternalTestCluster;

import java.util.concurrent.ExecutionException;

import static org.elasticsearch.common.settings.Settings.settingsBuilder;

@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)
@ESIntegTestCase.SuppressLocalMode
public class BlaTest extends ESIntegTestCase {
    public void testFail() throws ExecutionException, InterruptedException {
        Settings settings = settingsBuilder()
                .put("discovery.type", "local")
                .build();

        InternalTestCluster.Async&lt;String&gt; master = internalCluster().startNodeAsync(settings);
        InternalTestCluster.Async&lt;String&gt; nonMaster = internalCluster().startNodeAsync(settingsBuilder().put(settings).put("node.master", false).build());
        master.get();
        nonMaster.get();
        ensureStableCluster(2, TimeValue.timeValueMinutes(2));
    }
}
```

Jstack interesting part:

```
"TEST-BlaTest.testFail-seed#[5531179544489649]" #12 prio=5 os_prio=31 tid=0x00007f83fbafa000 nid=0x5103 waiting on condition [0x0000700001
34d000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  &lt;0x00000006c09db8c0&gt; (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:238)
        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:78)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:42)
        at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:68)
        at org.elasticsearch.test.ESIntegTestCase.ensureClusterSizeConsistency(ESIntegTestCase.java:1094)
        at org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:593)
        at org.elasticsearch.test.ESIntegTestCase.after(ESIntegTestCase.java:1972)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1665)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:922)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
        at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
        at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
        at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
        at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
```

Also noteworthy that often the following warning is displayed in the logs (which shows that it has somewhat to do with discovery and the mix of modes):

```
[2015-11-03 00:19:42,917][WARN ][org.elasticsearch.discovery] [node_t1] waited for 30s and no initial state was set by the discovery
```
</description><key id="114789074">14476</key><summary>Test with @SuppressLocalMode and discovery.type=local diverges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>test</label></labels><created>2015-11-03T10:55:05Z</created><updated>2015-11-06T09:17:24Z</updated><resolved>2015-11-06T09:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-06T09:05:08Z" id="154350097">This is again #14535
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner hits not returning deep fields values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14475</link><project id="" key="" /><description>Hello,

I try to fetch inner hits with specific fields. Everything works fine when the fields are at the root of the nested document, but it doesn't when the field has a composed key with some dots.

I have this with ElasticSearch 1.7.2.

Here is a simple script to illustrate the problem:

``` shell
printf "Delete index:\n\n"
curl -XDELETE localhost:9200/test

printf "\n\nPrepare mappings with some nested object:\n\n"
curl -XPUT localhost:9200/test -d '{
  "mappings": {
    "article": {
      "properties": {
        "comments": {
          "type": "nested"
        }
      }
    }
  }
}'

printf "\n\nInsert an article with nested comment:\n\n"
curl -XPUT localhost:9200/test/article/1 -d '{
  "title": "Inner hits fields bug",
  "comments": [{
    "user": {
      "id": "albanm"
    },
    "content": "What is going on ?"
  }]
}'

printf "\n\nFlush index:\n\n"
curl -XPOST 'http://localhost:9200/test/_flush'

printf "\n\nSearch articles with nested comments in inner hits. In inner hits results the field 'content' IS PRESENT but 'user.id' IS NOT:\n\n"
curl -XPOST localhost:9200/test/article/_search -d '{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "fields": ["content", "user.id"]
      }
    }
  }
}'

printf "\n\n"

```
</description><key id="114784657">14475</key><summary>Inner hits not returning deep fields values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">albanm</reporter><labels><label>:Inner Hits</label><label>bug</label></labels><created>2015-11-03T10:26:13Z</created><updated>2015-11-09T14:05:13Z</updated><resolved>2015-11-09T13:10:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="albanm" created="2015-11-03T10:35:39Z" id="153310701">FYI storing the individual field doesn't seem to change anything.
</comment><comment author="clintongormley" created="2015-11-08T19:21:38Z" id="154859345">There is a discrepancy between how paths are specified for `fields` and `_source`:

`fields` can only return stored fields as inner hits:

```
PUT /test
{
  "mappings": {
    "article": {
      "properties": {
        "comments": {
          "type": "nested",
          "properties": {
            "content": {
              "type": "string",
              "store": true
            },
            "user": {
              "properties": {
                "id": {
                  "type": "string",
                  "store": true
                }
              }
            }
          }
        }
      }
    }
  }
}


PUT /test/article/1
{
  "title": "Inner hits fields bug",
  "comments": [
    {
      "user": {
        "id": "albanm"
      },
      "content": "What is going on ?"
    }
  ]
}
```

The full path must be specified for stored fields:

```
POST /test/article/_search
{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "fields": [
          "comments.user.id",
          "comments.content"
        ]
      }
    }
  }
}
```

But `_source` prepends the path of the nested field:

```
POST /test/article/_search
{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "_source": [
          "user.id",
          "content"
        ]
      }
    }
  }
}
```

@martijnvg looks like a bug to me?
</comment><comment author="martijnvg" created="2015-11-09T09:21:38Z" id="155004777">@clintongormley @albanm The reason why there is a discrepancy between `fields` and `_source` is just because the two are differently implemented. The `fields` requires the field to exist in the mapping and therefor the full path needs to be provided, whereas `_source` doesn't require that and just retrieves the relevant bits from the _source (map of maps).

This discrepancy is noticeable when using inner hits, because the actual source of an inner hit is different than the source of a top level hit. (the _source of the inner hit is a subset of the _source of a top level hit)
So I'm not sure if this is a bug, because the _source is actually just different in the cases.

Another thing is that the `_source` allows to include/exclude also properties that are unmapped. (in cases where object field is configured to be disabled), so that is also a reason that this feature is differently implemented.
</comment><comment author="albanm" created="2015-11-09T10:19:56Z" id="155021240">The behavior that I noticed is still a bug I think. Let's forget _source, and just look at my initial example. The field "content" was successfully extracted from _source and returned without being stored nor prefixed by "comments.", while "user.id" was not.

By the way I switched to using _source filtering as I now see that my problem is with a deprecated functionality:

["This functionality has been replaced by the source filtering parameter."](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-request-fields.html).
</comment><comment author="clintongormley" created="2015-11-09T13:10:31Z" id="155057486">&gt; The behavior that I noticed is still a bug I think. Let's forget _source, and just look at my initial example. The field "content" was successfully extracted from _source and returned without being stored nor prefixed by "comments.", while "user.id" was not.

It's a bit more complicated than that :)  Before 2.0, you could reference fields either with just the name, or with the full path.  `content` was actually being retrieved from the source field, and could be found because you used the short name.  `user.id` however is neither the short name nor the full path.
</comment><comment author="albanm" created="2015-11-09T14:05:13Z" id="155070586">Ok, thanks for the explanations !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a warning about fields vs. source filtering.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14474</link><project id="" key="" /><description>Close #14470
</description><key id="114784619">14474</key><summary>Add a warning about fields vs. source filtering.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T10:25:58Z</created><updated>2015-11-09T10:56:25Z</updated><resolved>2015-11-09T10:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-03T10:49:16Z" id="153312854">LGTM. I wonder if we should also enforce this in code and refuse to load those from source in 3.0 (and deprecate in 2.x)
</comment><comment author="jpountz" created="2015-11-03T10:55:20Z" id="153314126">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add System#exit(), Runtime#exit() and Runtime#halt() to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14473</link><project id="" key="" /><description>I've added System#exit(), Runtime#exit() and Runtime#halt() to the forbidden APIs in all-signatures and added appropriate suppresses.

Can you please have a look @rmuir? Thanks a lot!

Closes #12596
</description><key id="114783358">14473</key><summary>Add System#exit(), Runtime#exit() and Runtime#halt() to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T10:20:40Z</created><updated>2015-11-04T08:31:54Z</updated><resolved>2015-11-04T08:31:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-03T10:29:34Z" id="153309720">left one comment otherwise LGTM
</comment><comment author="danielmitterdorfer" created="2015-11-03T11:47:00Z" id="153328134">Thanks @s1monw. I've reduced the scope now as much as possible. Can you have a short look again, please?
</comment><comment author="rmuir" created="2015-11-03T14:35:07Z" id="153373227">looks great!
</comment><comment author="danielmitterdorfer" created="2015-11-04T07:25:48Z" id="153614618">Thanks @rmuir. I'll merge it to master soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify XContent detection.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14472</link><project id="" key="" /><description>Currently we duplicate the logic for BytesReferences and InputStreams.
</description><key id="114783042">14472</key><summary>Simplify XContent detection.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T10:19:03Z</created><updated>2016-01-22T18:32:59Z</updated><resolved>2015-11-09T11:02:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-03T10:30:04Z" id="153309778">+1 on share the code. I wonder if we should flip this around and use the stream input as the basic implementation and use org.elasticsearch.common.bytes.BytesReference#streamInput  to call it from the bytes reference variant. It's just slighty more efficient as it doesn't read ahead.
</comment><comment author="jpountz" created="2015-11-03T10:49:09Z" id="153312839">I wondered about the same but using the stream input version as a basic implementation had the downside of being a bit harder to read (because it had to check every read against -1 while the BytesReference version can just check the length once) as well as require the stream input returned by every BytesReference to support mark/reset.
</comment><comment author="s1monw" created="2015-11-03T10:57:38Z" id="153314468">lets not over-design this - simplicity is king here
</comment><comment author="bleskes" created="2015-11-03T10:57:54Z" id="153314508">yeah, I hear you. I'm not sure if it's that's easier to read but we're splitting hairs :)  one up side of keeping the stream input variant as the basis is that it might be easier to convert org.elasticsearch.common.xcontent.XContentFactory#xContentType(java.lang.CharSequence)  as well (wrapping the char sequence  with a InputStreams)
</comment><comment author="bleskes" created="2015-11-03T10:58:11Z" id="153314546">so bottom line - up to you :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ignore_unavailable parameter to skip unavailable snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14471</link><project id="" key="" /><description>Catch exception when reading corrupted snapshot.

Single corrupted snapshot file shouldn't prevent listing all other snapshot in repository.
closes #13887
</description><key id="114773219">14471</key><summary>Add ignore_unavailable parameter to skip unavailable snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T09:14:23Z</created><updated>2015-11-19T06:38:31Z</updated><resolved>2015-11-19T06:09:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-11-03T09:15:41Z" id="153294225">@ywelsch my previous PR was created in the ES repo by mistake. - - Create a new one here.

I took your second idea, and add a `ignore_unavailable` parameter to skip unavailable snapshots
</comment><comment author="ywelsch" created="2015-11-03T10:09:24Z" id="153305722">Left some comments. I also prefer `ignore_unavailable` to introducing another snapshot state.
</comment><comment author="xuzha" created="2015-11-03T17:27:09Z" id="153425140">Thanks @ywelsch for the review, I added another commit to address your comments. 
</comment><comment author="ywelsch" created="2015-11-04T09:28:07Z" id="153654530">@xuzha Left a few more comments.
</comment><comment author="xuzha" created="2015-11-04T09:36:15Z" id="153658973"> @ywelsch thanks a lot again. Just squashed my commits and addressed the comments.
</comment><comment author="ywelsch" created="2015-11-04T09:52:13Z" id="153664145">LGTM
</comment><comment author="xuzha" created="2015-11-13T08:08:09Z" id="156358442">Hmm, this PR has been stayed for a while.
If there is no objection, I will merge this PR tomorrow. 
</comment><comment author="clintongormley" created="2015-11-17T14:52:31Z" id="157391577">@ywelsch you want to do a final review?
</comment><comment author="ywelsch" created="2015-11-18T15:05:50Z" id="157741776">@xuzha can you add labels for the versions this should be merged in?
</comment><comment author="xuzha" created="2015-11-18T16:48:18Z" id="157774866">@imotov would you like to take a look ;-) ?? 
</comment><comment author="ywelsch" created="2015-11-18T16:56:44Z" id="157777574">If this goes into 2.2, we need to make the serialisation of GetSnapshotsRequest conditional based on the version to ensure interoperability with 2.1. Note that this is only relevant for the 2.2 port.
</comment><comment author="xuzha" created="2015-11-18T16:58:43Z" id="157778156">Thx, @ywelsch, I will add a check for2.2 port.
</comment><comment author="ywelsch" created="2015-11-18T17:34:35Z" id="157790652">@clintongormley can you review the documentation? If that's done, we're good to go.
</comment><comment author="clintongormley" created="2015-11-18T18:53:52Z" id="157820270">Docs look good to me.
</comment><comment author="xuzha" created="2015-11-19T06:38:31Z" id="157967496">2.x https://github.com/elastic/elasticsearch/commit/bbca80a07938c28a6208cab94d1979e8315285e7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard search for field does not return all fields if store:false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14470</link><project id="" key="" /><description>If fields are not stored (`"store":false` in mapping), it is still possible to query fields using the `"fields":[ "foo", "bar" ]` query syntax as the fields are taken from the `_source`.
If however you use a wildcard : `"fields": [ "*" ]` then no fields are returned.
</description><key id="114769270">14470</key><summary>Wildcard search for field does not return all fields if store:false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>docs</label></labels><created>2015-11-03T08:43:16Z</created><updated>2015-11-09T10:52:58Z</updated><resolved>2015-11-09T10:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-11-03T08:51:02Z" id="153289862">I think we should update the documentation to make it more explicit that `"fields":["*"]` only returns stored fields and won't extract all fields (stored or unstored) from `_source`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using binary type and doc_values=true makes doc() lookups completely unusable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14469</link><project id="" key="" /><description>(ES 2.0 ) 
In the documentation it states that you can fetch binary doc_values by using "doc_values": true, for that type: https://www.elastic.co/guide/en/elasticsearch/reference/current/binary.html.
When trying to fetch this in a java plugin I always get unsupported_operation_exception, thrown by UnsupportedOperationException, BytesBinaryDVAtomicFieldData line 104:
you can reproduce it by creating a simple index and adding some documents with binary data

``` bash
curl -s -XPUT "http://localhost:9200/testindex/" -d '{
    "settings": {
        "index.number_of_shards": 1,
        "index.number_of_replicas": 0
    },
    "mappings": {
    "test":{"_all":{"enabled":false},
    "properties":{
        "qa_data":{"type":"binary", "doc_values": true}
        }
       }
    }
}'

curl -s -XPUT 'http://localhost:9200/testindex/test/3'  -d '{
            "qa_data" : "AB4BNINAgBoQgAQAABQAJIAAEAAAAAAAAAAAACBAAAgAAAAAAIAAABAAgAACIAAQAAAICAAAAAAAgAAgAgAAAAAAAAAAAJAAAHACAAAAAQAAAAAAAAAAQAAAAAAAAAAIEEAAAAAACAAAAAAAAAAAAgIAIAgwICBAAAAAAAAAAAAAAAAAAAAAQAQAAAAgAAAAAAAAIAAAAAAAACAACAAAAAAAAAACAAEAAAEAAAAAAAAAAAAAAAAAAAAAAIAAAAAAAAAQAAAAAACAAAAAAAAAAAAAAAAAgAAAAAAEAAAAAAAAAAAAHgAAPEgAAAAAAAAAIAAAAAAKAAEABAAAABcAAgAEAAAAHAADAAQAAAAhAAEABAAAACMAAQAEAAAAJAABAAQAAAAvAAIACQADADYAAgAEAAAAOAAEAAQAAAA5AAQADgACAD8AAQABAAIATAACAAQAAABfAAEABAAAAGIAAgAJAAMAZQABAAQAAAByAAEAAQACAHQAAgAJAAIA4wACAAQAAAD2AAEABAAAAP0AAgAEAAABDwACAAQAAAEcAAEAAQACATcAAgAEAAABQwACAAkAAgFLAAEAAwADAWQAAwANAAEBdQADAAQAAAF5AAEABAAAAYUAAQAEAAABlwABAAQAAAH5AAEAAgACAgkAAgADAAICFAABAAEAAgIVAAEAAgACAhYAAgAEAAACLAACAAQAAAIvAAIABAAAAmgAAgAEAAACpgACAAIAAQLWAAQADwACAtwAAwAHAAIC4wACAAMAAgMrAAMAAwACA0MAAQAEAAADTQADAAQAAANZAAMABAAAA2EAAwAEAAADpgADAAMAAQOtAAEAAQABA7UAAgAJAAIDvAABAAIAAwO9AAIABAAABBoAAgAEAAAEJgACAAMAAgRFAAIABAAABH0AAgADAAIEjQACAAMAAgT7AAMABAAABRAABAAPAAIFKAACAAQAAAU5AAQADwACBYcAAQABAAMFzAACAAQAAAYfAAIACQACBooAAgADAAIGtwABAAEAAgcbAAEABAAABx4AAgAEAAAHIgACAAQAAAcjAAMABAAAByQAAQAEAAAHJQADAAQAAAc5AAEABAAABzoAAQAEAAAHOwABAAQAAAc8AAEAAQACB10AAwADAAI="
}'
curl -s -XPUT 'http://localhost:9200/testindex/test/4'  -d '{
             "qa_data" : "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"
```

Writing a simple plugin that just calls doc() or calls doc().get('qa_data') will throw exceptions immediately. 

``` java
  @Override
    public float runAsFloat() {
        float finalScore = 0;
        LeafDocLookup doc = doc(); //=&gt; will throw java.lang.UnupportedOperationException 
    }
```
</description><key id="114763402">14469</key><summary>Using binary type and doc_values=true makes doc() lookups completely unusable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jrots</reporter><labels><label>:Fielddata</label><label>discuss</label></labels><created>2015-11-03T07:43:50Z</created><updated>2016-11-22T21:23:23Z</updated><resolved>2016-11-22T21:23:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-03T09:56:43Z" id="153302610">I'm curious what your use-case is, for now the only use-case I knew about for doc values on binary fields was an image plugin for elasticsearch (#5669) and doc values were consumed directly through the plugin, bypassing the scripting layer.
</comment><comment author="jrots" created="2015-11-03T11:06:18Z" id="153315954">Well I have an index that contains +/- 100M documents, the data that is indexed are "persons" and a typically search is :
find users that are around me, but the sorting needs to be done dynamically: 
Each "person" has answered x questions with some answers. 
I need calculate a "matchscore" on the fly for the persons I find, basically the overlap of my questions with the persons I find. (And also some additional checks on the answers of those questions if they are the same). The "matchscore" will give back a percentage between 0 and 100. 
There are about 1 to 1800 questions that can be answered by a person, 
I store the question data packed in 64 bit longs : so I only have to do a bitwise operation to find the "matching" questions.
This goes pretty fast in my benchmarks.
my questionids from 0 to 64 :               100000000000000000000000000000000000010001000000010000000000001 &amp;
the other person questionids from 0 to 64 : 100000000000000000000000000000000000000001000000010000000000001
result will only contain the matching "questionids"

An array of all questions I have answered will look like (encoded in 64 bit ints: )

``` json
[-8989044006797179904,5629656300523520,0,2323857442082914304,36028797287432192,153122456050075656,8388640,144115188075855872,158329681740288,1099511627776,274877906944,34632368128,8796093022208,8623497224,3467807172325277696,0,274945015808,2305843009213693984,8192,576460752303423488,144116287587549184,0,128,4096,2147483648,0,36028797018964992,0,2161728080043835392,536870912
]
```

And for an other person like:

``` json
[-8989040706113233866,5629656858499072,3499296910466940960,2323857992107163712,45036563478904864,1306043995025050154,2199031669792,180143985099014144,562949960761856,36047626155590656,274877906952,34632384512,1225551944202847296,4611967502027857928,3756319573209513984,1477180677777523712,9075601440770,2377900603251622304,134225920,612489549322420544,2449959296801276032,2305843009213693952,128,100732928,576601492006502400,22517998271135872,36028797018963968,288230376151711744,6773554836496449536,3149824
]
```

I first stored the data as a long array in elastic, but you cannot rely on the order as doc_values will be ordered low to high, 
So I created a bytearray that I base64 encode and such store in ES, afterwards decode 

``` java
        byte[] decodeString  = Base64.getDecoder().decode(encodedString);
        ByteBuffer byteBuf = ByteBuffer.wrap( decodeString );
        byteBuf.order( ByteOrder.BIG_ENDIAN );
        LongBuffer longBuf = byteBuf.asLongBuffer();
        long[] questions = new long[numberOfQuestions];
        longBuf.get(questions);
```

I found a workaround for now, to store it as "text" with: 
      "qa_data":{"type":"string", "doc_values": true, "index": "no", "store" : "no"}
and not binary .. seems to work for now.
</comment><comment author="jpountz" created="2015-11-04T11:14:40Z" id="153688849">Thanks for explaining the use-case.

Unrelated to binary doc values but I'm wondering that storing the questions ids directly could be a better option both in terms of storage and runtime. You could take ids from the shortest array and then use galloping search to find common ids in the other array?

Otherwise I agree that we should either document the limitations with doc values on binary fields or add support so that you can at least use them in scripts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expanded on various node roles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14468</link><project id="" key="" /><description>Fixes #14429 
</description><key id="114757715">14468</key><summary>Expanded on various node roles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-11-03T07:08:16Z</created><updated>2016-01-30T17:15:55Z</updated><resolved>2016-01-30T17:15:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T17:41:37Z" id="154852277">@bleskes you mentioned that you wanted to remove documentation of `node.client` in favour of just `node.data: false`, `node.master: false`.
</comment><comment author="clintongormley" created="2015-11-08T17:43:48Z" id="154852398">@markwalkom just on formatting, I'd use a definition list, eg:

```
`node.master`::
+
--
Master nodes ... blah bah 

Another para....
--

`node.data`::
+
--
Data nodes ... blah bah 

Another para....
--
```
</comment><comment author="bleskes" created="2015-11-09T10:37:59Z" id="155024763">&gt;  @bleskes you mentioned that you wanted to remove documentation of node.client in favour of just node.data: false, node.master: false.

@clintongormley yeah, I think we should. I'm good with removing it here, or at least mark them as deprecate and remove the "smart balancers" wording. Note though that it takes more work. We obviously need to change code and also re-write this page (and some more where NodeBuilder is mentioned) https://github.com/elastic/elasticsearch/blob/master/docs/java-api/client.asciidoc 
</comment><comment author="markwalkom" created="2015-11-10T09:16:25Z" id="155365223">@clintongormley updated with the list :)
</comment><comment author="clintongormley" created="2015-11-16T19:27:03Z" id="157143609">I think we should remove mention of `--node.client` and "smart router" as it sounds too attractive.  This isn't something we recommend these days.
</comment><comment author="markwalkom" created="2015-11-17T00:12:30Z" id="157216706">@clintongormley do you specifically mean `node.client` or the concept of a client node? I just want to make sure we're on the same page.
</comment><comment author="clintongormley" created="2015-11-18T14:44:02Z" id="157734161">Specifically `node.client` (just set master and data to false) but also we don't usually recommend setting up client-only nodes any more.  We prefer users to use the transport client instead.
</comment><comment author="clintongormley" created="2016-01-30T17:15:55Z" id="177251303">Closing in favour of c0a7f8889781f342c6c79696de904bc4ef47314b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>About field type change question?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14467</link><project id="" key="" /><description>I found `field type` changed, template setting type is `float`, but now change `long`, what causes? 

```
{
    "test": {
        "mappings": {
            "test": {
                "dynamic_templates": [
                    {
                        "distr_pan": {
                            "mapping": {
                                "properties": {
                                    "v": {
                                        "doc_values": true,
                                        "type": "float"
                                    },
                                    "k": {
                                        "index": "not_analyzed",
                                        "doc_values": true,
                                        "type": "string"
                                    }
                                },
                                "type": "nested"
                            },
                            "match": "distr_pan"
                        }
                    }
                ],
                "_all": {
                    "enabled": false
                },
                "properties": {
                    "distr_pan": {
                        "type": "nested",
                        "properties": {
                            "k": {
                                "type": "string",
                                "index": "not_analyzed",
                                "doc_values": true
                            },
                            "v": {
                                "type": "long",
                                "doc_values": true
                            }
                        }
                    }
                }
            }
        }
    }
}
```
</description><key id="114754210">14467</key><summary>About field type change question?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels /><created>2015-11-03T06:54:22Z</created><updated>2015-11-03T09:19:40Z</updated><resolved>2015-11-03T09:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>upgrade rhino for plugins/lang-javascript</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14466</link><project id="" key="" /><description>the current jar is over 3 years old, we should upgrade it for bugfixes.
the current integration could be more secure: set a global policy and enforce additional (compile-time) checks
</description><key id="114754069">14466</key><summary>upgrade rhino for plugins/lang-javascript</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang JS</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T06:52:25Z</created><updated>2016-01-22T18:43:52Z</updated><resolved>2015-11-03T14:53:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-03T06:56:34Z" id="153262420">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch not accessible after moving from 1.4.4 to 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14465</link><project id="" key="" /><description>Hi,

I removed  elasticsearch 1.4.4. using rpm. (It was installed using rpm package)

&gt; $rpm -e elasticsearch-1.4.4-1.noarch

Then installed 2.0.0 using rpm again and started it as below. 

&gt; $rpm -ivh elasticsearch-2.0.0.rpm
&gt; $sudo chkconfig --add elasticsearch
&gt; $sudo service elasticsearch start

Logs show no error or exceptions

&gt; [2015-11-03 12:12:47,903][INFO ][node                     ] [Atalon] version[2.0.0], pid[4194], build[de54438/2015-10-22T08:09:48Z]
&gt; [2015-11-03 12:12:47,916][INFO ][node                     ] [Atalon] initializing ...
&gt; [2015-11-03 12:12:48,315][INFO ][plugins                  ] [Atalon] loaded [], sites []
&gt; [2015-11-03 12:12:48,781][INFO ][env                      ] [Atalon] using [1] data paths, mounts [[/ (/dev/mapper/vg01-LogVol00)]], net usable_space [37.2gb], net total_space [90.6gb], spins? [possibly], types [ext4]
&gt; [2015-11-03 12:12:59,073][INFO ][node                     ] [Atalon] initialized
&gt; [2015-11-03 12:12:59,073][INFO ][node                     ] [Atalon] starting ...
&gt; [2015-11-03 12:12:59,474][INFO ][transport                ] [Atalon] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
&gt; [2015-11-03 12:12:59,567][INFO ][discovery                ] [Atalon] elasticsearch/RNomvPO4TyyGpRdInlKZ7A
&gt; [2015-11-03 12:13:02,804][INFO ][cluster.service          ] [Atalon] new_master {Atalon}{RNomvPO4TyyGpRdInlKZ7A}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
&gt; [2015-11-03 12:13:02,931][INFO ][gateway                  ] [Atalon] recovered [0] indices into cluster_state
&gt; [2015-11-03 12:13:02,952][INFO ][http                     ] [Atalon] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}
&gt; [2015-11-03 12:13:02,953][INFO ][node                     ] [Atalon] started

However, the curl command gives no response from elasticsearch. 

&gt; curl -XGET http://localhost:9200
&gt; 
&gt; &lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;Network Error&lt;/TITLE&gt;&lt;/HEAD&gt;
&gt; &lt;BODY&gt;
&gt; &lt;FONT face="Helvetica"&gt;&lt;big&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/big&gt;&lt;BR&gt;&lt;/FONT&gt;
&gt; 
&gt; &lt;blockquote&gt;
&gt; &lt;TABLE border=0 cellPadding=1 width="80%"&gt;
&gt; &lt;TR&gt;&lt;TD&gt;
&gt; &lt;FONT face="Helvetica"&gt;&lt;big&gt;Network Error (dns_unresolved_hostname)&lt;/big&gt;&lt;BR&gt;&lt;BR&gt;&lt;/FONT&gt;
&gt; &lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;&lt;FONT face="Helvetica"&gt;Your requested host "localhost" could not be resolved by DNS.&lt;/FONT&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;
&gt; &lt;FONT face="Helvetica"&gt;&lt;/FONT&gt;&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;&lt;FONT face="Helvetica" SIZE=2&gt;&lt;BR&gt;For assistance, contact your network support team.&lt;/FONT&gt;&lt;/TD&gt;&lt;/TR&gt;
&gt; &lt;/TABLE&gt;
&gt; &lt;/blockquote&gt;
&gt; 
&gt; &lt;/FONT&gt;
&gt; &lt;/BODY&gt;&lt;/HTML&gt;
</description><key id="114753935">14465</key><summary>Elasticsearch not accessible after moving from 1.4.4 to 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nithyanv</reporter><labels /><created>2015-11-03T06:51:05Z</created><updated>2015-11-03T07:56:13Z</updated><resolved>2015-11-03T07:00:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-03T07:00:27Z" id="153262855">Try `curl -XGET http://127.0.0.1:9200`

And join us on discuss.elastic.co for questions.
</comment><comment author="nithyanv" created="2015-11-03T07:07:46Z" id="153268271">I tried curl -XGET http://127.0.0.1:9200 and got below 

&gt; &lt;HTML&gt;
&gt; &lt;HEAD&gt;&lt;TITLE&gt;Redirection&lt;/TITLE&gt;&lt;/HEAD&gt;
&gt; &lt;BODY&gt;&lt;H1&gt;Redirect&lt;/H1&gt;&lt;/BODY&gt;
</comment><comment author="nithyanv" created="2015-11-03T07:08:36Z" id="153268650">@dadoonet There are no changes in the log. No connection seems to be getting established there. 
</comment><comment author="dadoonet" created="2015-11-03T07:29:26Z" id="153279542">Try with `curl -XGET http://127.0.0.1:9200/`
</comment><comment author="nithyanv" created="2015-11-03T07:30:52Z" id="153279788">@dadoonet Still the same :-(
</comment><comment author="dadoonet" created="2015-11-03T07:34:56Z" id="153280191">Any chance elasticsearch is not running anymore?
May be you have firewall weird setup?
</comment><comment author="nithyanv" created="2015-11-03T07:45:03Z" id="153281139">I checked that 

&gt; service elasticsearch status
&gt; elasticsearch (pid  4194) is running...

Will check with my team about any firewall issue. 
</comment><comment author="dadoonet" created="2015-11-03T07:48:46Z" id="153281555">There is absolutely no reason if elasticsearch is running and if elasticsearch tells `publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}` that nothing is answering at http://127.0.0.1:9200 (you can run that in a browser BTW)
</comment><comment author="nithyanv" created="2015-11-03T07:53:02Z" id="153281994">I know! This is so strange. It is running fine as a test, in a local windows installation. Already tried through browser. 
</comment><comment author="nithyanv" created="2015-11-03T07:54:28Z" id="153282143">We have set up ETC hosts as well. 

&gt; 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
&gt; ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

1.4.4 was working in the same machine. The rpm installation changes manually creating elasticsearch user and adding to group wheel. http cross origin settings. etc. 

However, in this installation, we did no configuration changes apart from clustername. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Maven to gradle Integ test parity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14464</link><project id="" key="" /><description>A few small nice features of the ant/maven integ tests were lost with the move to gradle, noted here:
https://github.com/elastic/elasticsearch/pull/14460#issuecomment-153227207

This issue is to add these back:
- random tmpdir for cwd
- space in ES home path
- space in plugin files source path
- jps check for already running ES
</description><key id="114752139">14464</key><summary>Maven to gradle Integ test parity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T06:34:50Z</created><updated>2015-11-07T21:09:04Z</updated><resolved>2015-11-07T21:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make Grok-specific classes package-protected.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14463</link><project id="" key="" /><description /><key id="114744912">14463</key><summary>Make Grok-specific classes package-protected.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-11-03T05:28:32Z</created><updated>2015-11-03T05:31:38Z</updated><resolved>2015-11-03T05:31:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-03T05:30:37Z" id="153245712">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to Lucene 5.4 GeoPointField Type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14462</link><project id="" key="" /><description>This PR updates ES 3.0 geo_point field types to use Lucene GeoPointField instead of legacy String and parallel DoubleFields. The following is affected:
- GeoPointFieldMapper - updated to use Lucene 5.4 GeoPointField as default indexing type. lat_lon and geohash are preserved as additional indexing options.
- GeoPointFieldData and GeoPointDV - updated to use Lucene 5.4 GeoPointField instead of 2 parallel Double Arrays.
- GeoQueryBuilder/Parser - updated to use Lucene 5.4 GeoPoint\* Queries.
- Unit and Integration Tests - updated to test based on new indexing format.
#### Notes and Todo:
- Need to revisit/update java docs to remove deprecated query options (e.g., optimize_bbox, coerce)
- This PR is for 3.0 only. It is the same as the split 2.x PRs with the exception that it completely removes backward compatibility with 1.x - 2.1 `geo_point` indexes.
</description><key id="114737662">14462</key><summary>Update to Lucene 5.4 GeoPointField Type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels /><created>2015-11-03T03:59:14Z</created><updated>2015-11-09T14:09:14Z</updated><resolved>2015-11-04T22:56:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-03T06:16:21Z" id="153255908">I quickly glanced at this. It seems like this completely breaks backcompat for geo fields created in 2.x? I think we need to maintain bwc for 2.x in 3.x?
</comment><comment author="nknize" created="2015-11-03T13:58:09Z" id="153362082">&gt; This PR is for 3.0 only. It is the same as the split 2.x PRs with the exception that it completely removes backward compatibility with 1.x - 2.1 geo_point indexes.

It does break bwc per the PR note above. I had an open question/discussion re: geo_point v2 and how long to carry the deprecated `geo_point` functionality. Since geopoint v2 slid from the 2.0 release to 2.2 we're required by convention to support bwc until 4.0, no? That seems like a long time, and removing in 3.0 per the original plan still seemed reasonable but I'm OK carrying bwc if we feel its the right thing to do. /cc @clintongormley 
</comment><comment author="nknize" created="2015-11-04T15:46:18Z" id="153768668">@s1monw For now it may be easier to add GeoPointV2 (backcompat and all) to both master and 2.x, and open a separate issue to discuss backcompat removal in 3.0? /cc @clintongormley 
</comment><comment author="nknize" created="2015-11-04T22:56:28Z" id="153896086">Closing this for now. We can defer the if/when to remove backcompat decision at a later time. /cc @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES type question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14461</link><project id="" key="" /><description>I set a `dynamic template` mapping, a field define as follow

```
{
    distr_pan: {
        mapping: {
            properties: {
                v: {
                    doc_values: true,
                    type: "float"
                },
                k: {
                    index: "not_analyzed",
                    doc_values: true,
                    type: "string"
                }
            },
            type: "nested"
        },
        match: "distr_pan"
    }
}
```

I insert a data (`distr_pan.v` setting a `Integer` value), I found the `dis_pan` type change to `long`

```
distr_pan: {
    type: "nested",
    properties: {
        k: {
            type: "string",
            index: "not_analyzed",
            doc_values: true
        },
        v: {
            type: "long",
            doc_values: true
        }
    }
}
```

Not typecasting  in insert data to ES?
</description><key id="114735001">14461</key><summary>ES type question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels /><created>2015-11-03T03:19:49Z</created><updated>2015-11-03T07:06:49Z</updated><resolved>2015-11-03T07:06:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add back integ tests to distributions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14460</link><project id="" key="" /><description>rpm and deb are still skipped, but this configures rest tests to run for
zip and tgz distributions

closes #14361
</description><key id="114725705">14460</key><summary>Add back integ tests to distributions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-03T01:51:55Z</created><updated>2015-11-03T18:35:16Z</updated><resolved>2015-11-03T18:35:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-03T02:30:15Z" id="153220128">I first tried this out from distribution/zip, maybe a missing dependency?

```
:distribution:zip:integTest#clean UP-TO-DATE
:distribution:zip:integTest#extract

FAILURE: Build failed with an exception.

* What went wrong:
Cannot expand ZIP '/Users/rmuir/workspace/elasticsearch/distribution/zip/build/distributions/elasticsearch-3.0.0-SNAPSHOT.zip' as it does not exist.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED
```

I'm trying from the top-level build now.
</comment><comment author="rmuir" created="2015-11-03T02:54:06Z" id="153227207">There are also quite a few things missing (just from looking at directory structure) that integ tests did before:
1. using a random tmpdir not rooted under ES to ensure no bogus CWD dependencies: https://github.com/elastic/elasticsearch/blob/2.x/dev-tools/src/main/resources/ant/integration-tests.xml#L23-L26
2. using a space in ES run directory to ensure no bogus URL/file handling:
   https://github.com/elastic/elasticsearch/blob/2.x/pom.xml#L121
3. using a space for any plugin dependencies to check for the same in pluginmanager:
   https://github.com/elastic/elasticsearch/blob/2.x/pom.xml#L122

While these may seem "unimportant", I do think they are pretty common problems and these are easy tricks to prevent issues later.
</comment><comment author="rmuir" created="2015-11-03T02:57:47Z" id="153228246">Works for me. I left some comments, all can be followups.
</comment><comment author="rjernst" created="2015-11-03T06:35:18Z" id="153258607">Thanks for the notes @rmuir! I created a follow up for the issues you mentioned: #14464
</comment><comment author="rjernst" created="2015-11-03T06:35:57Z" id="153258643">And I fixed the dependency issue. Running the tests within `distribution/zip` and `distribution/tar` now works.
</comment><comment author="rmuir" created="2015-11-03T07:27:38Z" id="153278912">+1
</comment><comment author="rjernst" created="2015-11-03T18:35:07Z" id="153447940">I also added the environment variables back in this PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian install pointed at 2.0 installs 2.0.0-beta2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14459</link><project id="" key="" /><description>```
# echo "deb http://packages.elastic.co/elasticsearch/2.0/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.0.list
deb http://packages.elastic.co/elasticsearch/2.0/debian stable main
#apt-get update
...

# apt-get install elasticsearch
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 125 not upgraded.
Need to get 28.4 MB of archives.
After this operation, 31.9 MB of additional disk space will be used.
Get:1 http://packages.elastic.co/elasticsearch/2.0/debian/ stable/main elasticsearch all 2.0.0~beta2 [28.4 MB]
```

To install 2.0.0, you have to 2.x in the echo, like so:

```
# echo "deb http://packages.elastic.co/elasticsearch/2.x/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.0.list
#apt-get update
...

# apt-get install elasticsearch
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages will be upgraded:
  elasticsearch
1 upgraded, 0 newly installed, 0 to remove and 125 not upgraded.
Need to get 28.5 MB of archives.
After this operation, 22.5 kB of additional disk space will be used.
Get:1 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main elasticsearch all 2.0.0 [28.5 MB]
```
</description><key id="114715231">14459</key><summary>Debian install pointed at 2.0 installs 2.0.0-beta2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sinneduy</reporter><labels /><created>2015-11-03T00:23:22Z</created><updated>2015-12-19T16:36:19Z</updated><resolved>2015-11-03T00:32:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-11-03T00:32:01Z" id="153203680">Thanks, this has tripped a few people up. Hopefully web searches can help the rest reaching this issue here.

It is covered in the [repositories documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html) and it's been added in #14377 to the list of [packaging changes](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_plugin_and_packaging_changes.html#_repository_naming_structure_changes).
</comment><comment author="sinneduy" created="2015-11-03T00:36:24Z" id="153204323">@jasontedor have you guys considered removing 2.0 as a valid install?  I think that would help a little
</comment><comment author="jasontedor" created="2015-12-19T16:36:19Z" id="166001902">&gt; have you guys considered removing 2.0 as a valid install? I think that would help a little

@sinneduy It has been removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 2.0 install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14458</link><project id="" key="" /><description>Anyone getting problems installing the new elastic search 2.0?

I'm getting the "Exception in thread "main" java.lang.Runtimeexception: don't run as root"
</description><key id="114704422">14458</key><summary>ElasticSearch 2.0 install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">langjai0</reporter><labels /><created>2015-11-02T23:02:59Z</created><updated>2015-12-20T09:08:05Z</updated><resolved>2015-11-03T01:04:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-03T01:04:48Z" id="153208701">Sounds like you are trying to run elasticsearch as root user which is forbidden, security wise?
</comment><comment author="mingfang" created="2015-12-06T17:51:26Z" id="162331833">How do I run elasticsearch as root?
I understand the security considerations.
</comment><comment author="dadoonet" created="2015-12-06T18:12:58Z" id="162333955">You can't run elasticsearch as root.
</comment><comment author="mingfang" created="2015-12-06T18:29:31Z" id="162334786">Why don't you let me decide what I can and can not do.
</comment><comment author="dadoonet" created="2015-12-06T18:44:32Z" id="162336291">Why do you want to run elasticsearch as root?
</comment><comment author="mingfang" created="2015-12-06T18:50:04Z" id="162336555">That's not the point.
If I wanted to run as root then I should be able to.
</comment><comment author="dadoonet" created="2015-12-06T18:53:54Z" id="162336738">That's exactly the point. If you have a problem, we will be happy to understand it and solve it...
We try to protect our users from external attacks. We don't allow to run a query for example with size=1000000000 because you will blow up your memory and cluster.
</comment><comment author="mingfang" created="2015-12-06T18:55:56Z" id="162336822">I do have a problem with you telling me what I can and can not do.
</comment><comment author="rmuir" created="2015-12-06T19:06:34Z" id="162337393">The issue is, although we work hard to improve defaults and security, there are still some very serious traps (like dynamic scripting). 

So its very important that we have at least some basic facilities of the operating system: different user, filesystem permissions, etc.
</comment><comment author="sasauz" created="2015-12-19T21:22:59Z" id="166027606">I would like to start elasticsearch with Linux as autostart and it's means that the elasticseach should be start as root. or? You also write on your site (https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html) that we should do something like this for autostart:

sudo /sbin/chkconfig --add elasticsearch
sudo service elasticsearch start

in this case it will be start as root! What should I do in this case?

p.s. I use SLES
</comment><comment author="jasontedor" created="2015-12-20T02:36:36Z" id="166058289">&gt; in this case it will be start as root!

@sasauz You need superuser privileges so that the process can be started and daemonized under the `elasticsearch` user, but the service is in fact started under the `elasticsearch` user which you can see from

```
    daemon --user $ES_USER...
```

or

```
    start-stop-daemon -d $ES_HOME --start -b --user "$ES_USER -c "$ES_USER"...
```

depending on your system (where, in both cases, the default is for `ES_USER` to be set to `elasticsearch`).
</comment><comment author="nik9000" created="2015-12-20T04:12:30Z" id="166061251">For the most part you should be able to use the deb or rpm distribution if
you want fancy service things. If you can't use those it might be simpler
to see what they do and clone it then to rebuild from first principles.
On Dec 19, 2015 9:36 PM, "Jason Tedor" notifications@github.com wrote:

&gt; in this case it will be start as root!
&gt; 
&gt; @sasauz https://github.com/sasauz You need superuser privileges so that
&gt; the process can be started and daemonized under the elasticsearch user,
&gt; but the service is in fact started under the elasticsearch user which you
&gt; can see from
&gt; 
&gt; ```
&gt; daemon --user $ES_USER...
&gt; ```
&gt; 
&gt; or
&gt; 
&gt;    start-stop-daemon -d $ES_HOME --start -b --user "$ES_USER -c "$ES_USER"...
&gt; 
&gt; depending on your system (where in both cases, the default is for ES_USER
&gt; to be set to elasticsearch).
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14458#issuecomment-166058289
&gt; .
</comment><comment author="sasauz" created="2015-12-20T09:08:05Z" id="166091060">@jasontedor and @nik9000 Thank you! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast hosts port range breaking change in ES 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14457</link><project id="" key="" /><description>There is a breaking change in ES 2.0 where we changed the unicast.hosts specification.

There are 2 issues with this change:

1) This is missed in our breaking changes "network changes" documentation:
https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_network_changes.html

While the new syntax is documented in the [Zen discovery](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-discovery-zen.html#unicast) documentation, it should also be mentioned on the breaking changes page.

2) The change is breaking, but the exception thrown on the failed ES startup is misleading.  Instead of a UnknownHostException, it will be helpful for the error handling to tell the user that the setting specification has changed, and provide an example of the new syntax right in the error message.

Works in 2.0:
discovery.zen.ping.unicast.hosts: ['127.0.0.1:9300-9400']

Works in 1.7, breaks in 2.0 (and server will not start up):
discovery.zen.ping.unicast.hosts: ['127.0.0.1[9300-9400]']

And it throws the exception:

```
[2015-11-02 13:48:57,713][INFO ][watcher.trigger.schedule ] [Starhawk] using [ticker] schedule trigger engine
Exception in thread "main" java.lang.IllegalArgumentException: Failed to resolve address for [127.0.0.1[9300-9400]]
Likely root cause: java.net.UnknownHostException: 127.0.0.1[9300-9400]: nodename nor servname provided, or not known
    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293)
    at java.net.InetAddress.getAllByName0(InetAddress.java:1246)
    at java.net.InetAddress.getAllByName(InetAddress.java:1162)
    at java.net.InetAddress.getAllByName(InetAddress.java:1098)
    at org.elasticsearch.transport.netty.NettyTransport.parse(NettyTransport.java:668)
    at org.elasticsearch.transport.netty.NettyTransport.addressesFromString(NettyTransport.java:620)
    at org.elasticsearch.transport.TransportService.addressesFromString(TransportService.java:398)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.&lt;init&gt;(UnicastZenPing.java:141)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</description><key id="114692690">14457</key><summary>Unicast hosts port range breaking change in ES 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>adoptme</label><label>bug</label><label>docs</label></labels><created>2015-11-02T21:54:49Z</created><updated>2016-01-15T12:40:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Node level total_shards_per_node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14456</link><project id="" key="" /><description>A customer has asked for this to accompany `index.routing.allocation.total_shards_per_node` as they are seeing rather uneven allocation.

While this might sound reasonable for larger systems with a lot of indices and shards (which they have), there are still potential issues with implementing this so I wanted to start a discussion.

(Thanks to @dakrone as well)
</description><key id="114689154">14456</key><summary>Node level total_shards_per_node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>enhancement</label></labels><created>2015-11-02T21:34:58Z</created><updated>2015-11-09T18:23:13Z</updated><resolved>2015-11-09T18:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-11-03T01:06:34Z" id="153208894">Scratch that, I totally misread the docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>min_score doesn't seem to prune irrelevant results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14455</link><project id="" key="" /><description>Scenario: to do significant_terms aggregation on a random subset (10%) of the results. The goal is to speed up computation of the aggregation by taking only a portion of the results.
Issue: setting up score in script_score and applying min_score doesn't seem to influence the final result. The following query has 44k results in the database, and all of them seem to get aggregated.

query:

``` python
call = {
    'aggregations':
    {
        'foo_aggregation':
        {
            'significant_terms':
            {
                'field': 'bar.text',
                'size': 100
            }
        }
    },
    'query':
    {
        'function_score':
        {
            'boost_mode': 'replace',
            'min_score': 0.5,
            'query': 
            {
                'function_score':
                {
                    'query': {
                      'filtered': {
                        'filter': {
                          'bool': {
                            'must': [
                              {'query': { 'match': {'foo_field': {'query': 'foo',
                                                                  'type': 'phrase'}}}},
                              {'query': {'match': {'foo_field': {'query': 'bar',
                                                                 'type': 'phrase'}}}},
                              {'query': {'match': {'foo_field': {'query': 'baz',
                                                                 'type': 'phrase'}}}}]}}}},
                    'random_score': {}
                }
            },
            'script_score':
            {
                'lang': 'expression',
                'script': '(10.0 * _score &gt; 1)?0.0:1.0'
            }
        }
    }
}

.search(body = call, search_type = 'count')
```

Result:

``` python
{u'_shards': {u'failed': 0, u'successful': 6, u'total': 6},
 u'aggregations': {u'foo_aggregation': {u'buckets': [{u'bg_count': 437361,
                                          u'doc_count': 16528,
                                          u'key': u'a',
                                          u'score': 0.16617895961177376},
                                         {u'bg_count': 214256,
                                          u'doc_count': 8869,
                                          u'key': u'b',
                                          u'score': 0.1165279936014176},
                                         {u'bg_count': 20459,
                                          u'doc_count': 1692,
                                          u'key': u'c',
                                          u'score': 0.08204490448968814},
                                         {u'bg_count': 215203,
                                          u'doc_count': 7889,
                                          u'key': u'd',
                                          u'score': 0.07167727779372135},
                                         {u'bg_count': 502079,
                                          u'doc_count': 15660,
                                          u'key': u'e',
                                          u'score': 0.06899975718865733},
                                         {u'bg_count': 24842,
                                          u'doc_count': 1681,
                                          u'key': u'f',
                                          u'score': 0.059883089238118345},
                                         {u'bg_count': 163057,
                                          u'doc_count': 5767,
                                          u'key': u'g',
                                          u'score': 0.0460286671961764},
                                         {u'bg_count': 17804,
                                          u'doc_count': 1208,
                                          u'key': u'h',
                                          u'score': 0.04322160149374382},
                                         {u'bg_count': 56574,
                                          u'doc_count': 2570,
                                          u'key': u'i',
                                          u'score': 0.04263653815448131},
                                         {u'bg_count': 161090,
                                          u'doc_count': 5617,
                                          u'key': u'j',
                                          u'score': 0.04243133744106531}],
                            u'doc_count': 44870}},
 u'hits': {u'hits': [], u'max_score': 0.0, u'total': 44870},
 u'timed_out': False,
 u'took': 1127}
```

Expected:
Aggregation based on ~4400 results, not 44000.

Is this an intended behaviour, and if so, is there a way to exclude results from the final resultset used in aggregation?
</description><key id="114685924">14455</key><summary>min_score doesn't seem to prune irrelevant results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lazymachinist</reporter><labels><label>:Search</label><label>bug</label></labels><created>2015-11-02T21:18:57Z</created><updated>2016-02-14T15:13:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-03T09:47:47Z" id="153301028">I tried it locally and it worked for me with elasticsearch 2.0. Which version are you running?
</comment><comment author="lazymachinist" created="2015-11-03T17:20:55Z" id="153423588">Hi Adrien,

This is the version:
"version" : {
    "number" : "2.0.0",
    "build_hash" : "de54438d6af8f9340d50c5c786151783ce7d6be5",
    "build_timestamp" : "2015-10-22T08:09:48Z",
    "build_snapshot" : false,
    "lucene_version" : "5.2.1"
  },
Can you print en example query that worked (i.e. returned results based on a portion of original matches)? Maybe I am missing some important detail.
</comment><comment author="clintongormley" created="2016-02-14T15:13:38Z" id="183904267">I can replicate this. If you set `size=0`, then the `min_score` condition isn't applied.  This on ES 2.2.0:

```
POST t/t/_bulk
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
{"index":{}}
{"foo_field":"foo bar baz","bar":{"text":"quick brown fox"}}
```

With `size=1`,  works as expected.  With `size=0`, all 14 results are aggregated.

```
GET _search?size=1
{
  "aggregations": {
    "foo_aggregation": {
      "significant_terms": {
        "field": "bar.text",
        "size": 100
      }
    }
  },
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "min_score": 0.5,
      "query": {
        "function_score": {
          "query": {
            "filtered": {
              "filter": {
                "bool": {
                  "must": [
                    {
                      "match": {
                        "foo_field": {
                          "query": "foo",
                          "type": "phrase"
                        }
                      }
                    },
                    {
                      "match": {
                        "foo_field": {
                          "query": "bar",
                          "type": "phrase"
                        }
                      }
                    },
                    {
                      "match": {
                        "foo_field": {
                          "query": "baz",
                          "type": "phrase"
                        }
                      }
                    }
                  ]
                }
              }
            }
          },
          "random_score": {}
        }
      },
      "script_score": {
        "lang": "expression",
        "script": "(10.0 * _score &gt; 1)?0.0:1.0"
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify plugin properties generation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14454</link><project id="" key="" /><description>The gradle task to generate plugin properties files now is a simple copy
with expansion (ie maven filtering). It also no longer depends on
compiling.

closes #14450
</description><key id="114679704">14454</key><summary>Simplify plugin properties generation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T20:42:32Z</created><updated>2015-11-02T21:20:11Z</updated><resolved>2015-11-02T21:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-02T20:56:57Z" id="153154262">looks great! thanks for following up on this one
</comment><comment author="rjernst" created="2015-11-02T21:20:11Z" id="153159774">Relates to #13930
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.0 rpm does not create /home/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14453</link><project id="" key="" /><description>Causes issue if you try to run a cron job as the ES user. Issue on centos 6, package from the repo. 

```
[centos@ip-10-233-237-10 ~]$ sudo su - elasticsearch -s /bin/bash
su: warning: cannot change directory to /home/elasticsearch: No such file or directory

-bash-4.1$ grep elastic /etc/passwd
elasticsearch:x:498:498:elasticsearch user:/home/elasticsearch:/sbin/nologin

-bash-4.1$ ls -l /home/
total 4
drwx------. 3 centos centos 4096 Nov  2 20:23 centos
```
</description><key id="114677803">14453</key><summary>ES 2.0 rpm does not create /home/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">packplusplus</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-11-02T20:30:57Z</created><updated>2016-10-27T08:12:08Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="joshuar" created="2015-11-05T00:44:23Z" id="153917135">The value of HOME used when creating the elasticsearch user should probably be set to something other than /home/elasticsearch that exists and has executable permissions for the elasticsearch user.

In the interim, you can just set HOME to something else, like `HOME=/` to get your cron jobs running again.
</comment><comment author="packplusplus" created="2015-11-05T02:12:30Z" id="153930961">I just have the puppet role create /home/elasticseach. Its a packaging bug, creating a user with the wrong home directory set. Either create the home directory, or like you said,  set home to /var/lib/elasticsearch or whatever.
</comment><comment author="clintongormley" created="2015-11-08T23:10:08Z" id="154885442">On 2.0 it doesn't look like we're setting the home directory, no? https://github.com/elastic/elasticsearch/blob/2.0/distribution/src/main/packaging/scripts/preinst#L67

Is this user not a hangover from a previous installation?
</comment><comment author="joshuar" created="2015-11-08T23:15:34Z" id="154885751">If we don't set the home directory, I think it defaults to whatever the base home directory is in /etc/default/useradd plus the username.  So usually this ends up being `/home/username`, even for system users.
</comment><comment author="pickypg" created="2015-11-10T18:08:29Z" id="155516996">@clintongormley I don't think that the file has changed since 1.6 at least https://github.com/elastic/elasticsearch/blob/1.6/src/packaging/common/scripts/preinst#L67

Before that, it was using the specific scripts: https://github.com/elastic/elasticsearch/blob/1.4/src/rpm/scripts/preinstall

---

I went ahead and installed ES 1.4.3, 1.7.3, and ES 2.0.0 (in reverse order) onto a CentOS 7 VM via `yum`. None of them created a `/home/elasticsearch` entry. I cannot find where we may have created it at some point. Perhaps we never did?
</comment><comment author="tlrx" created="2015-11-10T21:52:33Z" id="155578266">&gt; @clintongormley I don't think that the file has changed since 1.6 at least https://github.com/elastic/elasticsearch/blob/1.6/src/packaging/common/scripts/preinst#L67

In ES 1.6 an effort has been made to unify the behaviour of package &amp; install scripts among the different Linux distributions and I think the change has been made at this time. Elasticsearch does not need any home directory to work so it is not created by the scripts. 

Depending of the distribution, `su -` does a bunch of things and among them it tries to change directory to user's home which does not exist... You're not forced to `su -` in order to run a cron job.
</comment><comment author="packplusplus" created="2015-11-10T22:43:03Z" id="155591394">I don't know what the original version of ES was installed on that cluster, but somewhere along the line /home/elasticsearch was created. It was a cron job failing on a redeploy that kicked out the message. 

If it's not expected to create it, then I guess it's not a bug. If I look at most other "system type" users in RHEL/CentOS they normally set the homedir to / in the passwd file, or in the case of some services that store a lot of data, their var dir. Debian seems to just create a /home/USER dir for non data storing services. 

TLDR; Seems weird to create a user with a home directory that doesn't exist. I'd vote either create the dir (to be consistent with debian), or point it to / to be consistent RHEL. 
</comment><comment author="twigbranch" created="2016-07-07T00:54:25Z" id="230950325">/home/elasticsearch is also useful to house .java.policy files as discussed here: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-security.html#_customising_the_classloader_whitelist
</comment><comment author="JakeFromTheDark" created="2016-09-21T10:38:19Z" id="248574265">Just extend the `useradd` command by `-d /usr/share/elasticsearch`, please.
</comment><comment author="pawelan" created="2016-10-27T08:12:08Z" id="256576280">Guys are you planning first anniversary celebration because it is almost 1 year old!?
I have checked latest version 5.0.0 rpm and it is still making mess in user configuration.
Is this one liner fix so time consuming?

The bug #20599 which was closed as duplicate was actually better titled because the problem is not that home directory is not being created but because the correct one is undefined during user creation in rpm. JakeFromTheDark solution is correct. The other is similar but with /var/lib/elasticsearch set as home dir.

Pretty please with a sugar on top fix it :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup query parsing and remove IndexQueryParserService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14452</link><project id="" key="" /><description>IndexQueryParserService is only a factory for QueryShardContext instances
which are not even bound to a shard. The service only forwards dependencies and even
references node level service directly which makes dependency seperation on shard,
index and node level hard. This commit removes the service entirely, folds the creation
of QueryShardContext into IndexShard which is it's logical place and detaches the
ClusterService needed for index name matching during query parsing with a simple predicate
interface on IndexSettings.
</description><key id="114677500">14452</key><summary>Cleanup query parsing and remove IndexQueryParserService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T20:29:04Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-11-03T16:29:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-02T20:29:13Z" id="153147439">@javanna can you take a look
</comment><comment author="javanna" created="2015-11-03T12:03:23Z" id="153332741">This looks great, thanks a lot @s1monw ! Left a few comments.
</comment><comment author="s1monw" created="2015-11-03T16:14:12Z" id="153401730">@javanna can you take another look
</comment><comment author="javanna" created="2015-11-03T16:18:51Z" id="153402911">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixing run.bat and run.sh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14451</link><project id="" key="" /><description>fixed #14423 

I'm a little new to gradle, so do double check I haven't missed something basic.
</description><key id="114675453">14451</key><summary>fixing run.bat and run.sh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels><label>build</label></labels><created>2015-11-02T20:16:17Z</created><updated>2015-11-09T19:16:06Z</updated><resolved>2015-11-09T19:15:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-11-02T20:48:47Z" id="153152438">I think we should do this right, which means mimicking what we did before with maven. The framework is mostly already there, it just needs to be setup. This means creating a "run" task, which uses ClusterFormationTasks, and also stalls when the task is executed. Unfortunately, this won't work as nice until gradle fixes finalizedBy (https://discuss.gradle.org/t/run-finalizedby-when-task-is-interrupted-ctrl-c-continued-from-old-forum/12036), which means the cluster will remain alive until you run a follow up task... 
</comment><comment author="rjernst" created="2015-11-02T22:47:50Z" id="153182310">Note that the task should also not be under core, but at the root, or under distribution, since it will need to depend on the zip distribution.
</comment><comment author="andrejserafim" created="2015-11-03T05:48:05Z" id="153250927">I was surprised it worked in core to be honest, I only put it there because I had classpath issues with the previous run task.

And now gradle is cheating on my behalf and I didn't even notice - it's gone off to the distribution module. Is there any way to enforce which modules should depend on which?

```
andrej@black:~/dev/elasticsearch/core$ gradle run
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes
:buildSrc:jar
:buildSrc:sourcesJar
:buildSrc:signArchives SKIPPED
:buildSrc:assemble
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version : 2.8
  JDK Version    : 1.8.0_66-b17 (Oracle Corporation)
  OS Info        : Linux 3.19.0-31-generic (amd64)
:core:compileJava UP-TO-DATE
:core:processResources UP-TO-DATE
:core:classes UP-TO-DATE
:core:jar
:distribution:zip:buildZip      &lt;-------------- 
:core:run#setup
:core:run#clean
:core:run#configure
:core:run#start
```
</comment><comment author="rjernst" created="2015-11-03T05:52:20Z" id="153251529">It's not cheating. The cluster formation tasks add a dependency on the zip distribution. It's doing exactly what is expected. I just think it is confusing to have core depend on distribution (since it looks like a circular dep, even though it isn't in this case). So I would move the run task to either the root (my preference for ease of use) or distribution.
</comment><comment author="andrejserafim" created="2015-11-03T05:54:57Z" id="153251859">I've moved it to distribution, since if I move it to the root it can't find the zip.

```
* What went wrong:
Could not resolve all dependencies for configuration ':elasticsearchZip'.
&gt; Cannot resolve external dependency org.elasticsearch.distribution.zip:elasticsearch:3.0.0-SNAPSHOT because no repositories are defined.
  Required by:
      org.elasticsearch:elasticsearch:3.0.0-SNAPSHOT
```

Is there something I can configure to make it work? - first time I'm working with gradle.
</comment><comment author="rjernst" created="2015-11-03T06:01:10Z" id="153252420">It tells you exactly the problem:

&lt;pre&gt;
Cannot resolve ... &lt;b&gt;because no repositories are defined.&lt;/b&gt;
&lt;/pre&gt;

However, rather than adding repositories to the root project, I think defining it under distribution is fine. We can even add a shortcut I think from the root file:

```
task run(dependsOn: ':distribution:runZip')
```
</comment><comment author="andrejserafim" created="2015-11-03T20:10:32Z" id="153473131">rebasing to master, uhh, that generated a lot of commits on the pr.
</comment><comment author="rjernst" created="2015-11-03T21:16:55Z" id="153491957">I think you just need to pull the latest master? Also, can you remove the run.sh and run.bat scripts? I don't think they are useful now (before they were saving from typing a complicated mvn command). 
</comment><comment author="rjernst" created="2015-11-05T23:07:37Z" id="154224960">@andrejserafim This looks good, can you sync up with master and I will merge?
</comment><comment author="andrejserafim" created="2015-11-06T17:55:59Z" id="154485633">And done.
</comment><comment author="rjernst" created="2015-11-06T20:38:05Z" id="154525428">@andrejserafim, sorry to ask one more thing. I hadn't realized how many small noisy commits there are here. Can you rebase on master so there is one commit with a clear commit message?
</comment><comment author="andrejserafim" created="2015-11-09T18:03:42Z" id="155141764">@rjernst anything else?
</comment><comment author="rjernst" created="2015-11-09T19:16:06Z" id="155161185">Thank you @andrejserafim!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup plugin properties generation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14450</link><project id="" key="" /><description>This currently has 2 problems since the switch to gradle:
1. It depends on compiling, so that it can check for the existence of the given classname for jvm plugins. Instead, we should check for the .java file as compile is heavyweight.
2. The generation currently does not use the template file as it did in maven. This was just me not understanding the equivalent of maven filtering for gradle at the time. It is easily fixable.
</description><key id="114668591">14450</key><summary>Cleanup plugin properties generation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label></labels><created>2015-11-02T19:37:50Z</created><updated>2015-11-02T21:19:40Z</updated><resolved>2015-11-02T21:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Adding a link to the guide from the docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14449</link><project id="" key="" /><description>fixed #14257

The guide has nice examples of the json responses to the bulk api.
</description><key id="114662910">14449</key><summary>Adding a link to the guide from the docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-11-02T19:06:21Z</created><updated>2015-11-09T09:47:59Z</updated><resolved>2015-11-09T09:47:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T18:32:59Z" id="154855909">@andrejserafim Let's rather copy the examples into the ref docs?
</comment><comment author="clintongormley" created="2015-11-09T09:47:59Z" id="155013285">Closing. See https://github.com/elastic/elasticsearch/issues/14257#issuecomment-155013221
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit the maximum queue_size for fixed thread pools?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14448</link><project id="" key="" /><description>Dynamically adjustable `queue_size`s for thread pools can in general be a useful feature for users to tune depending on their cluster utilization. However, as `queue_size` tends to `Integer.MAX_VALUE`, the value of this turns negative. So perhaps we should set a upper bound on `queue_size` beyond which it can not be set?

Relates #11511, relates #14367 (cf. [comment](https://github.com/elastic/elasticsearch/pull/14367#discussion_r43549359) from @dakrone)
</description><key id="114661098">14448</key><summary>Limit the maximum queue_size for fixed thread pools?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>adoptme</label><label>low hanging fruit</label><label>resiliency</label></labels><created>2015-11-02T18:56:23Z</created><updated>2016-05-27T14:33:20Z</updated><resolved>2016-05-27T14:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-04T12:21:21Z" id="153703202">I thinks makes sense. 
</comment><comment author="s1monw" created="2015-11-06T10:32:36Z" id="154374473">there is another issue I closed lately related to this: https://github.com/elastic/elasticsearch/issues/3890
I think we can do much better by computing some sensible limit based on the throughput and make that the default?
</comment><comment author="s1monw" created="2015-11-06T10:35:44Z" id="154375077">but hey we maybe should do both.... Lets just do 1024 and be done with it
</comment><comment author="ppf2" created="2015-11-14T21:53:23Z" id="156748576">+1 on at least setting an upper bound.  have seen quite a few cases in the field where users have set queue sizes to unlimited or some ridiculous number like 1 million+
</comment><comment author="nik9000" created="2016-05-20T14:54:14Z" id="220628214">If you set the queue size to less than 0 on a fixed thread pool you'll get an unbounded queue.
</comment><comment author="jasontedor" created="2016-05-27T14:33:17Z" id="222162144">Closed in favor of #18613
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make idea/eclipse project generation build generated resources for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14447</link><project id="" key="" /><description>This adds a generated-resources dir that the plugin properties are
generated into. This must be outside of the build dir, since intellij
has build as "excluded".
</description><key id="114656920">14447</key><summary>Make idea/eclipse project generation build generated resources for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T18:32:56Z</created><updated>2015-11-02T21:20:37Z</updated><resolved>2015-11-02T19:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-02T19:32:30Z" id="153133580">+1, this means plugins unit tests are working, i tested it.

This fixes https://github.com/elastic/elasticsearch/issues/14392

For the future, it would be good if we can avoid a dependency on "compile" for the core module here, its kinda heavy duty especially on laptops.
</comment><comment author="rjernst" created="2015-11-02T19:38:06Z" id="153134981">I created a follow up issue: #14450
</comment><comment author="rjernst" created="2015-11-02T21:20:37Z" id="153160018">Relates to #13930
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DOCFIX:  Shield reference docs list all privileges, but do not define all of them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14446</link><project id="" key="" /><description>https://www.elastic.co/guide/en/shield/current/reference.html

Some items, such as 'exists' are listed as part of 'manage' and are in the complete list of privileges, but do not have specific definitions of their own.  All listed privileges should have definitions as well to avoid ambiguity.
</description><key id="114647123">14446</key><summary>DOCFIX:  Shield reference docs list all privileges, but do not define all of them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jaymode/following{/other_user}', u'events_url': u'https://api.github.com/users/jaymode/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jaymode/orgs', u'url': u'https://api.github.com/users/jaymode', u'gists_url': u'https://api.github.com/users/jaymode/gists{/gist_id}', u'html_url': u'https://github.com/jaymode', u'subscriptions_url': u'https://api.github.com/users/jaymode/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4339958?v=4', u'repos_url': u'https://api.github.com/users/jaymode/repos', u'received_events_url': u'https://api.github.com/users/jaymode/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jaymode/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jaymode', u'type': u'User', u'id': 4339958, u'followers_url': u'https://api.github.com/users/jaymode/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>docs</label></labels><created>2015-11-02T17:48:17Z</created><updated>2016-01-29T20:35:43Z</updated><resolved>2016-01-29T20:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T18:31:25Z" id="154855839">@jaymode could you take a look at this?
</comment><comment author="clintongormley" created="2016-01-29T20:35:43Z" id="176956914">This should be on the shield repo
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delayed allocation can miss a reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14445</link><project id="" key="" /><description>The issue I focus on is described in #14010, #14011. Reproduced it using an integration test and I think I found a program flow leading to this issue.

Assume cluster where setting for delayed allocation is 1 minute. Now a node goes down. Shard becomes unassigned. In `RoutingService.clusterChanged()` (master node), `nextDelaySetting` is 1 minute (= smallest delayed allocation setting). The variable `registeredNextDelaySetting` is initially set to `Long.Max_VALUE`, hence we schedule a new reroute in a minute. The variable `registeredNextDelaySetting` is set to a minute. After half a minute, a second node goes down. A shard of the second node becomes unassigned. In this case, `registeredNextDelaySetting` is still 1 minute, and `nextDelaySetting` is also 1 minute. So nothing happens in `RoutingService.clusterChanged()`. The previous scheduled reroute gets executed at some point. This goes as follows: First, the variable `registeredNextDelaySetting` is again set to `Long.MAX_VALUE` and then we submit a cluster update task to do the reroute. Lets assume that routingResult.changed() yields true (but only because it is set to true in ReplicaShardAllocator due to the second shard still being delayed). Now, after the reroute is successfully applied, `InternalClusterService` calls back to `RoutingService.clusterChanged()`. Here we check if we were the reason for the cluster change event. If yes (and that's the case), we do not schedule a delayed allocation until the next cluster change event. This means that if no new cluster change event happens, the check never gets reevaluated for the remaining delayed shards.
</description><key id="114641081">14445</key><summary>Delayed allocation can miss a reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2015-11-02T17:17:18Z</created><updated>2015-11-12T16:58:08Z</updated><resolved>2015-11-12T16:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-02T17:34:17Z" id="153094498">Let me see if I understand this:
- 00:00 Nodes goes down, 1 shard unassigned (shard A)
- 00:00 Routing service sees a 1 minute delay in assigning the shard and schedules a reroute in 1 minute
- 00:30 Another node goes down, 1 additional shard unassigned (shard B)
- 00:30 Routing service sees a 1 minute configured delay, but does nothing
- 01:00 Scheduled reroute is executed, shard A is assigned (B still unassigned because it has 30 seconds of "delay" left)
- 01:05 New cluster state from the reroute, but ignored by RoutingService because it came from routing service

&gt; we do not schedule a delayed allocation until the next cluster change event. This means that if no new cluster change event happens, the check never gets reevaluated for the remaining delayed shards.

However, assuming shard A is actually assigned there will be a new cluster change event (a shard started event), so that's why this doesn't look like it occurs very frequently.

I think perhaps we can remove the 

``` java
        if (event.source().startsWith(CLUSTER_UPDATE_TASK_SOURCE)) {
            // that's us, ignore this event
            return;
        }
```

From the cluster changed event, what do you think?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] No documentation of response from a bulk index request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14444</link><project id="" key="" /><description>This PR fixes the following issue https://github.com/elastic/elasticsearch/issues/14257.
</description><key id="114639273">14444</key><summary>[DOCS] No documentation of response from a bulk index request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">girirajsharma</reporter><labels /><created>2015-11-02T17:08:23Z</created><updated>2015-11-09T09:47:55Z</updated><resolved>2015-11-09T09:47:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-05T17:38:19Z" id="154133172">@girirajsharma this changes more than just the documentation, it looks like you also added a change to the RPM build, maybe we can separate these two?
</comment><comment author="girirajsharma" created="2015-11-05T18:01:17Z" id="154138986">Oh, my bad.  I just rebased and pushed the change. I will be pushing RPM build issue fix in another PR and as it makes sense. :)
</comment><comment author="clintongormley" created="2015-11-09T09:47:54Z" id="155013273">Closing. See https://github.com/elastic/elasticsearch/issues/14257#issuecomment-155013221
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix auto-generated eclipse try/catch to be less trappy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14443</link><project id="" key="" /><description>When generating a try-catch block, the eclipse default is something like this:

```
try {
  something();
} catch (Exception e) {
  // TODO: auto-generated stub
  e.printStackTrace();
}
```

which is terrible, so the ES eclipse changes this to rethrow a RuntimeException instead.

```
try {
  something();
} catch (Exception e) {
  throw new RuntimeException();
}
```

Unfortunately, this loses the original exception entirely, instead it should be:

```
try {
  something();
} catch (Exception e) {
  throw new RuntimeException(e);
}
```
</description><key id="114635365">14443</key><summary>Fix auto-generated eclipse try/catch to be less trappy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-02T16:49:53Z</created><updated>2015-11-08T18:33:59Z</updated><resolved>2015-11-02T19:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-02T16:50:28Z" id="153080224">note: its been like this in maven for a while, just now seems like a good time to fix it. its unrelated to gradle changes.
</comment><comment author="nik9000" created="2015-11-02T17:04:05Z" id="153084033">LGTM.

I guess eclipse thinks `\n` is for the weak.
</comment><comment author="jpountz" created="2015-11-02T17:05:18Z" id="153084305">LGTM
</comment><comment author="uschindler" created="2015-11-02T17:07:15Z" id="153084806">LGTM
</comment><comment author="rmuir" created="2015-11-02T17:07:16Z" id="153084813">I added a commit fixing some suspicious places of this in the codebase today (mostly tests)
</comment><comment author="rjernst" created="2015-11-02T18:57:49Z" id="153124442">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch _bulk endpoint fails if data contains UTF-8 Byte Order Mark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14442</link><project id="" key="" /><description>This looks like a regression because it worked on 1.x

perl -e 'print "\xEF\xBB\xBF"' &gt; test.dat
echo '{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }' &gt;&gt; test.dat
echo '{ "field1" : "value1" }' &gt;&gt; test.dat

hexdump -C test.dat 

```
00000000  ef bb bf 7b 20 22 69 6e  64 65 78 22 20 3a 20 7b  |...{ "index" : {|
00000010  20 22 5f 69 6e 64 65 78  22 20 3a 20 22 74 65 73  | "_index" : "tes|
00000020  74 22 2c 20 22 5f 74 79  70 65 22 20 3a 20 22 74  |t", "_type" : "t|
00000030  79 70 65 31 22 2c 20 22  5f 69 64 22 20 3a 20 22  |ype1", "_id" : "|
00000040  31 22 20 7d 20 7d 0a 7b  20 22 66 69 65 6c 64 31  |1" } }.{ "field1|
00000050  22 20 3a 20 22 76 61 6c  75 65 31 22 20 7d 0a     |" : "value1" }.|
0000005f
```

Elasticsearch 2.0.0

```
[2015-11-02 17:20:49,328][INFO ][node                     ] [Earthquake] version[2.0.0], pid[77392], build[de54438/2015-10-22T08:09:48Z]
```

curl -XPOST '0:9200/_bulk' --data-binary @test.dat

fails with 

```
{"error":{"root_cause":[{"type":"parse_exception","reason":"Failed to derive xcontent"}],"type":"parse_exception","reason":"Failed to derive xcontent"},"status":400}
```

Server log

```
[2015-11-02 17:21:58,254][INFO ][rest.suppressed          ] /_bulk Params: {}
ElasticsearchParseException[Failed to derive xcontent]
    at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:293)
    at org.elasticsearch.action.bulk.BulkRequest.add(BulkRequest.java:251)
    at org.elasticsearch.rest.action.bulk.RestBulkAction.handleRequest(RestBulkAction.java:88)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:348)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="114631760">14442</key><summary>Elasticsearch _bulk endpoint fails if data contains UTF-8 Byte Order Mark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>:REST</label><label>low hanging fruit</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T16:32:28Z</created><updated>2015-11-09T11:11:40Z</updated><resolved>2015-11-09T11:09:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-03T09:20:03Z" id="153294956">This is due to the fact that detection used to fallback to json when we could not recognize either cbor, smile or yaml. But now we only recognize json if we detect a sequence of spaces followed by a '{'. So I guess we should also accept the common BOMs as first bytes.
</comment><comment author="camilojd" created="2015-11-09T04:03:56Z" id="154912591">Hello,
I have a fix for this, and sending a PR now (all tests run OK).

Do I have to send a separate PR to backport the fix to 2.x?

Thanks!
</comment><comment author="jpountz" created="2015-11-09T10:42:20Z" id="155025529">@camilojd No need to send multiple PRs, we will take care of backporting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast hostname failing to resolve blocks cluster from starting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14441</link><project id="" key="" /><description>Hi folks,

there seems to be a regression from **1.7.3** to **2.0.0** regarding the `gateway.recover_after_time` setting.

``` bash
docker run --name="elasticsearch" -p 9200:9200 -p 9300:9300 -e "SERVICE_ID=$hostname" -e "SERVICE_NAME=elasticsearch" elasticsearch:2.0.0 -Des.index.number_of_shards=3 -Des.index.number_of_replicas=2 -Des.gateway.recover_after_nodes=2 -Des.gateway.expected_nodes=3 -Des.gateway.recover_after_time=5m -Des.discovery.zen.ping.multicast.enabled=false -Des.discovery.zen.ping.unicast.hosts="01.elasticsearch,02.elasticsearch,03.elasticsearch" -Des.cluster.name="logging-test" -Des.network.publish_host="${PUBLIC_IP}"
```

I run Elasticsearch on three nodes (Docker hosts). The DNS names are not immediatly available. In **1.7.3** the cluster stabilizes after a couple of minutes. In **2.0.0** the container crashes directly.

``` java
Exception in thread "main" java.lang.IllegalArgumentException: Failed to resolve address for [02.elasticsearch]
Likely root cause: java.net.UnknownHostException: 02.elasticsearch: unknown error
at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
at java.net.InetAddress.getAllByName(InetAddress.java:1192)
at java.net.InetAddress.getAllByName(InetAddress.java:1126)
at org.elasticsearch.transport.netty.NettyTransport.parse(NettyTransport.java:668)
at org.elasticsearch.transport.netty.NettyTransport.addressesFromString(NettyTransport.java:620)
at org.elasticsearch.transport.TransportService.addressesFromString(TransportService.java:398)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.&lt;init&gt;(UnicastZenPing.java:141)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at &lt;&lt;&lt;guice&gt;&gt;&gt;
at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</description><key id="114624793">14441</key><summary>Unicast hostname failing to resolve blocks cluster from starting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">wkruse</reporter><labels><label>:Network</label><label>blocker</label><label>bug</label><label>discuss</label><label>v5.1.1</label></labels><created>2015-11-02T16:04:17Z</created><updated>2017-07-12T07:17:57Z</updated><resolved>2016-11-22T19:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-02T16:17:49Z" id="153071069">@wkruse it looks like this doesn't have anything to do with the `gateway.recover_after_time` setting. In 2.0.0 the networking is more strict regarding networking settings. In your case, one of your unicast hosts, `02.elasticsearch` fails to resolve to an IP address.
</comment><comment author="meatheadmike" created="2015-11-03T15:51:17Z" id="153395523">Is there an option to turn off the exception that occurs when host resolution fails? In 1.7 I used to set up a list of hosts and then add them as needed (it is "elastic" search after all). But now in 2.0 I have to have all of my hosts preconfigured with DNS or I get an exception and ES wont even start :( 
</comment><comment author="wkruse" created="2015-11-03T16:34:05Z" id="153408571">I've had it similar to @meatheadmike. CoreOS cluster and DNS service discovery per Elasticsearch container. The current solution is to use the hostnames for unicast.
</comment><comment author="dakrone" created="2015-11-03T17:42:05Z" id="153429279">I think the unicast hosts not resolving shouldn't fail the node starting up, what do you think @bleskes ?
</comment><comment author="meatheadmike" created="2015-11-03T17:51:37Z" id="153431730">Here's the current scenario and why it's an issue to me...

I spin up my nodes in Openstack. When I spin up a new node, I name it right then and there. So the DNS name doesn't exist prior. Neither does the IP address.

My current solution is to feed a list of potential node names in to a bash script, cycle through them and remove the ones that don't respond prior to creating the discovery.zen.ping.unicast.hosts entry in elasticsearch.yml. 

It's an ugly hack, but it works. It allows me to spin up new nodes without worrying about what currently exists and what doesn't. But this used to not be necessary :(
</comment><comment author="bleskes" created="2015-11-04T10:56:06Z" id="153683275">&gt; I think the unicast hosts not resolving shouldn't fail the node starting up, what do you think @bleskes ?

I'm a bit wary of this as it may also mean a configuration typo and which will cause the node from joining the cluster and one needs to go grep the logs to find out what's going on... 

Normally the answer would be to make the unicast host dynamically updatable. This doesn't work cleanly here because those settings can not be persisted in the cluster state - they are used before a node joins the cluster and has no access to it.

I'll munch on this some more. Ideas welcome.
</comment><comment author="majormoses" created="2015-11-04T16:07:00Z" id="153776170">This poses a problem for anyone using CM such as chef or puppet. With the way it currently is if a node is rebooting that means that you must deregister it from CM and bootstrap on launch again as well as every node to converge to remove it from its list. Otherwise the ES service on any node cant be (re)started until the machine comes back up. This seems to me at least to be far more common an issue than a misconfiguration of a list of hosts or ips.
</comment><comment author="rmuir" created="2015-11-04T16:33:41Z" id="153784620">The same logic to fail on invalid host was in 1.7, but it was just buggy:

https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java#L141-L143

From what I remember, hostname resolution at a lower level had several bugs, in such a way that unknown hosts wouldn't always deliver exceptions, e.g. be silently treated as a wildcard address (`0.0.0.0`) instead. 
</comment><comment author="rmuir" created="2015-11-04T16:40:03Z" id="153786481">It might have been silent conversion to 127.0.0.1 instead, can't remember which. The problems were something sneaky, like an unexpected null value -&gt; InetAddress.getByName(null) -&gt; 127.0.0.1 or something like that. http://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html#getByName%28java.lang.String%29
</comment><comment author="rmuir" created="2015-11-04T17:03:05Z" id="153792933">The other big problem was use of https://docs.oracle.com/javase/7/docs/api/java/net/InetSocketAddress.html#InetSocketAddress%28java.lang.String,%20int%29, but then not dealing with the 'unresolved' case correctly or at all across the codebase. Then the return method of `null` from the unresolved address would be treated the same as `0.0.0.0` for some methods.

If we want to change the behavior for pings, lets please not go back to using that trappy ctor (which is banned in forbidden apis for a reason).  I would also greatly prefer if we didn't create unresolved InetSocketAddresses in general (so lets ban https://docs.oracle.com/javase/7/docs/api/java/net/InetSocketAddress.html#createUnresolved%28java.lang.String,%20int%29 too).

The reason is, I think we should always explicitly do DNS lookups -&gt; addresses ourselves, this is an important operation, and need not to be "mixed in" with a convenience holder class that combines an IP address with a port number (InetSocketAddress), especially in the very sneaky lenient and trappy way it handles the case.

So for pings, it would mean that we'd just use `String` and `ping` would explicitly do DNS lookup itself if that is what we want to do.
</comment><comment author="jsirex" created="2015-11-16T16:12:31Z" id="157083943">Does anybody now, is it good idea to keep "hostnames" in unicast config instead of IPs?
Is there any performance degradation while ES cluster is up and running? Or host name lookup happens only on start? Does DNS failure affect existing cluster?
</comment><comment author="majormoses" created="2015-11-17T07:18:32Z" id="157297132">it only affects during cluster boot to my knowledge
</comment><comment author="bleskes" created="2015-11-17T08:07:33Z" id="157304371">@jsirex host names look up is currently done only on startup, which is also the source of the trouble reported here - it if fails the node won't start. Note though - this is not fault detection pinging but just the mechanism a node uses to find the cluster once it's starts up/ was isolated from it. You shouldn't worry about performance here.

It seems the consensus is to not resolve the host config on node start but rather keep it as an array of strings. The resolution should be done when starting to ping, ignoring errors  (but WARN log it!). The issue is already marked as adopt me, so people can consider it. I'll add a low hanging fruit.
</comment><comment author="thebenwaters" created="2016-04-12T07:59:37Z" id="208767472">+1 &#128077; 
</comment><comment author="gamykla" created="2016-04-15T15:19:05Z" id="210502115">This needs to be fixed. This really is a problem for my elastic dockerization attempt.
</comment><comment author="harpreet-sawhney" created="2016-04-27T02:05:24Z" id="214944639">+1 
</comment><comment author="mattjtodd" created="2016-04-28T20:45:18Z" id="215556903">&#128077; +1
</comment><comment author="lucacri" created="2016-05-13T00:00:45Z" id="218919189">+1 . Seriously, this is a massive breaking change. I have a cluster down because of this
</comment><comment author="clintongormley" created="2016-05-13T12:04:37Z" id="219024966">&gt;  Seriously, this is a massive breaking change. I have a cluster down because of this

This is not a breaking change.  It was always like this but, because of other networking bugs, might have been hidden. See https://github.com/elastic/elasticsearch/issues/14441#issuecomment-153784620
</comment><comment author="lucacri" created="2016-05-13T16:08:22Z" id="219087960">@clintongormley It might have been intended to work this way before, but it de-facto didn't. Fixing this created a breaking change. And to be fair, the old method made more sense (specially in scalable environments): you tell the node that the other nodes are X, Y, Z. If one is down, go to the next one. 
</comment><comment author="jillesvangurp" created="2016-08-24T15:53:01Z" id="242116054">I just ran into this with docker swarm as well. I'm trying to bootstrap a cluster with two swarm services that use each other to find the master using the swarm registered dns name. The problem is that the DNS names don't exist until after the service finishes starting. So there's a chicken egg problem. Also I noticed that even if there are valid hosts on the unicast list, it still crashes because one of them is not reachable: they all have to be reachable. 

In general DNS entries temporarily not resolving, not being available, or changing over time should all be expected behavior with DNS. In both cases the desirable/expected behavior would be to move on to the next unicast address and keep on trying until something succeeds (would be nice to have an optional timeout on that). Anything that assumes otherwise is fundamentally going to be flaky.

@clintongormley the behavior of exiting with an error code is new I believe even though the previous behavior was hardly correct/desirable. 

IMHO elasticsearch could be a lot more forgiving with networking than it currently is. 

BTW. So far, I've failed to get elasticsearch to cluster when run as a docker swarm. I've so far found no way around the many networking limitations in docker swarm and elasticsearch being very strict about its config.
</comment><comment author="muradm" created="2016-08-29T20:12:58Z" id="243241058">+1 same situation as @jillesvangurp 
</comment><comment author="loren" created="2016-09-02T15:39:38Z" id="244410615">Running into the [same issue](https://discuss.elastic.co/t/waiting-for-unicast-hosts-to-resolve-in-5-0-0-alpha5/58609) on 5.0.0 in a Mesos environment. 

My workaround/hack is to introduce a delay before the elasticsearch process starts up so that the DNS entries have time to get established.
</comment><comment author="loren" created="2016-09-06T16:43:45Z" id="245012367">I see there's already a "blocker" label on the issue, but I'll throw in that it's a blocker for the stuff I'm working on, too.
</comment><comment author="muradm" created="2016-10-02T10:50:44Z" id="250965310">For whom interested. Actually you can start cluster from scratch in dynamic environment. When tried Elasticsearch with docker swarm mode, it worked. The main concern here is resolvability of hosts provided. If you are running in docker swarm mode or consul with registrator etc., after container starts and right before Elasticsearch process is being fired, DNS name already present in related service (swarm/consul/etc.). For instance:

`docker service create --name elasticsearch-cluster1 --mode global \
    --mount src=logs-es-data,dst=/usr/share/elasticsearch/data --network test-net \
    encom/elasticsearch -Ecluster.name=test1 -Ediscovery.zen.ping.unicast.hosts=tasks.elasticsearch-cluster1`

works fine.

In docker swarm mode `tasks.elasticsearch-cluster1` represents DNS entry with list of all containers under the service. For consul and registrator it would be `elasticsearch.service.dc1`. Worth to mention that in docker swarm service also has VIP (depending on your configuration) and related DNS entry `elasticsearch-cluster1`. Since it is VIP and behaves like host, it is bad idea to use it.
</comment><comment author="jasontedor" created="2016-11-23T01:51:37Z" id="262417870">For folks that have been waiting for this, the forthcoming 5.1.0 (no date, sorry) will no longer fail on startup if a host fails to resolve. Instead, during discovery we will ping the hosts that do resolve. If another round of pinging is needed, we will resolve again and so on for each subsequent round of pinging. Please consult the Zen discovery docs for the interplay between this, the JVM DNS cache, and the Java security manager.</comment><comment author="JarenGlover" created="2016-11-23T08:06:30Z" id="262453324">@jasontedor it might be helpful to link to the specific documentation you speaking of while we patiently wait. thanks again! </comment><comment author="jasontedor" created="2016-11-23T12:03:50Z" id="262496266">@JarenGlover At this time, permalinks for the 5.1.0 docs are not available; as soon as such a permalink is available, I will link here.</comment><comment author="jasontedor" created="2016-12-08T18:40:58Z" id="265818970">Version 5.1.1 is released and the aforementioned [docs](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/modules-discovery-zen.html) are available now.</comment><comment author="jgoeres" created="2017-07-12T06:22:53Z" id="314666271">Ran into this issue with my own dockerization attempt. Alas, we are stuck with 2.4.x for the time being.
Any chance that this is backported?
BTW: I ran into a similar issue when I dockerized Zookeeper (where cluster "discovery" works in a similar fashion - by providing the list of cluster member hostnames &amp; ports to each instance).  
Workaround for me was to create a script that goes over the list of hostnames and checks if they are available in DNS and retries until they all are (so basically doing externally what Zookeeper/Elasticsearch should be doing from the start - treating a DNS lookup error like any other network problem and retry...). 
This approach also has the drawback that at least for startup your cluster needs to be fully available.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Allow numeric parameters enclosed in quotes for 'geohash_grid' aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14440</link><project id="" key="" /><description>Originally, only numeric values were allowed for parameters of the
'geohash_grid' aggregation in contrast to other places in the REST 
API. 

With this commit we also allow that parameters are enclosed in quotes (i.e.
as JSON strings). Additionally, with this commit the valid range for
'precision' is enforced for the Java API and the REST API (the latter was
previously missing the check).

Closes #13132
</description><key id="114624322">14440</key><summary>Geo: Allow numeric parameters enclosed in quotes for 'geohash_grid' aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Geo</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T16:02:11Z</created><updated>2015-11-04T08:17:56Z</updated><resolved>2015-11-04T08:08:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-03T08:03:31Z" id="153283595">Can you please have a look @markharwood, @jpountz? The PR basically allows to quote parameters of the geohash_grid aggregation and enforces the documented range of [1,12] for precision as stated in the docs. You can find an example for reproduction in the corresponding ticket #13132.
</comment><comment author="jpountz" created="2015-11-03T08:44:21Z" id="153288973">Thanks @danielmitterdorfer I left some comments. You might want to coordinate with @colings86 on this since he has an open PR to refactor this aggregation to parse on the coordinating node (#14138).
</comment><comment author="danielmitterdorfer" created="2015-11-03T09:34:06Z" id="153298168">Thanks for the heads up on @colings86 work. I'll check with him. @jpountz: Can you look again at my changes?
</comment><comment author="jpountz" created="2015-11-03T09:34:22Z" id="153298200">The PR looks good to me now, thanks!
</comment><comment author="danielmitterdorfer" created="2015-11-03T09:41:53Z" id="153299461">Ok, I'll check next with @colings86 on how to proceed regarding #14138.
</comment><comment author="colings86" created="2015-11-04T07:31:25Z" id="153616738">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Possible typo Apache Spark vs. Apache Storm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14439</link><project id="" key="" /><description>May be it is just a typo, but why the first paragraph here refers to Apache Spark while it is inside Apache Storm part?
https://www.elastic.co/guide/en/elasticsearch/hadoop/current/requirements.html#requirements-storm
</description><key id="114620030">14439</key><summary>[DOCS] Possible typo Apache Spark vs. Apache Storm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2015-11-02T15:41:15Z</created><updated>2015-11-08T18:12:31Z</updated><resolved>2015-11-08T18:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="girirajsharma" created="2015-11-02T16:39:42Z" id="153077245">The following PR https://github.com/elastic/elasticsearch-hadoop/pull/588 fixes above issue.
</comment><comment author="clintongormley" created="2015-11-08T18:12:31Z" id="154854875">This issue was moved to elastic/elasticsearch-hadoop#600
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>check-invalid-patterns in 2.x maven builds should ignore build/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14438</link><project id="" key="" /><description>Otherwise switching git branches from master (on gradle) is very annoying, since some ant build reports in build/reports will have problems.

I'm not sure this task should really be running on `${basedir}` anyway: at least in a blanket way and then excluding build files and stuff. Maybe it is enough to only explicitly run on a few things like `src` and `*.xml` to ensure there are not any nocommits. But otherwise we should add `build/` to the ignore patterns.
</description><key id="114611845">14438</key><summary>check-invalid-patterns in 2.x maven builds should ignore build/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>build</label></labels><created>2015-11-02T15:04:27Z</created><updated>2015-11-08T18:11:13Z</updated><resolved>2015-11-08T18:11:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T18:11:13Z" id="154854812">Closed by #14492
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Updating the Python client docxs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14437</link><project id="" key="" /><description>Adding reference to Pythondsl and updating the versioning information.
</description><key id="114610061">14437</key><summary>[docs] Updating the Python client docxs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>docs</label><label>v1.7.4</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T14:55:20Z</created><updated>2015-11-30T15:57:26Z</updated><resolved>2015-11-30T15:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HonzaKral" created="2015-11-10T11:20:40Z" id="155393381">Updated per comments, thanks!
</comment><comment author="clintongormley" created="2015-11-16T19:28:57Z" id="157144100">LGTM
</comment><comment author="HonzaKral" created="2015-11-30T15:57:26Z" id="160670177">Pushed to `master`, `2.x` and `2.1`

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle cleanEclipse should completely nuke .settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14436</link><project id="" key="" /><description>Today this will only remove `.settings/org.eclipse.jdt.core.prefs`,
leaving a bunch of stale eclipse configuration everywhere.
</description><key id="114606955">14436</key><summary>gradle cleanEclipse should completely nuke .settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-11-02T14:41:54Z</created><updated>2015-11-02T16:31:18Z</updated><resolved>2015-11-02T16:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-11-02T14:43:24Z" id="153036176">@jpountz can you look at this, as it will remove your configuration here (https://github.com/elastic/elasticsearch/commit/1954f770a1415c4ac2cc2f679ab3b68a3c4778d7).

I think we should generate them from the build consistently. I know this will temporarily make things worse (since we havent yet added this to gradle), but today its confusing.
</comment><comment author="jpountz" created="2015-11-02T14:48:34Z" id="153039151">I don't mind merging this fix temporarily until we can get gradle to re-generate those files when running 'gradle eclipse'.
</comment><comment author="rmuir" created="2015-11-02T14:54:34Z" id="153042081">Lemme take a stab at doing the copy in this PR as well. 
</comment><comment author="jpountz" created="2015-11-02T15:01:58Z" id="153043875">I can at least confirm that generating the config and importing into eclipse still works fine with this parth.
</comment><comment author="rmuir" created="2015-11-02T15:46:37Z" id="153058317">@jpountz can you try with my latest commit? it adds all the previous settings from the maven build, so it should properly setup source code formatting, 4-space indent, all that stuff.
</comment><comment author="jpountz" created="2015-11-02T16:27:28Z" id="153073537">:+1: I tried the change, and everything looked fine (license headers, indentation).
</comment><comment author="rmuir" created="2015-11-02T16:31:13Z" id="153074736">thanks for testing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Add match_none query to Query DSL documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14435</link><project id="" key="" /><description>We introduced the MatchNoneQueryBuilder query that does not
return any documents during the query refactoring, mainly as
an internal representation for the NONE option in the IndicesQueryBuilder. 
However, such a query has also been requested for the query dsl, 
and since we already have a parser for it we should document it as 
`match_none` query in the relevant reference docs as well.
</description><key id="114593595">14435</key><summary>Docs: Add match_none query to Query DSL documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T13:29:06Z</created><updated>2015-11-18T13:41:02Z</updated><resolved>2015-11-18T13:41:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-11-02T13:31:40Z" id="153016902">@clintongormley can you have a look, not sure if this is the best place to add this query in the main query-dsl document and if there are other places this should also be documented.
As reference, this was also requested for the DSL in https://github.com/elastic/elasticsearch/issues/13406 and follows a discussion in https://github.com/elastic/elasticsearch/pull/14220.
</comment><comment author="clintongormley" created="2015-11-08T18:03:08Z" id="154853607">@cbuescher Perhaps combine the docs with match_all.  Just add `[float]` in front of the header.
</comment><comment author="synhershko" created="2015-11-15T01:11:48Z" id="156768714">:+1: 
</comment><comment author="cbuescher" created="2015-11-16T10:56:24Z" id="156991686">@clintongormley updated according to your suggestion. Let me know if something is missing.
</comment><comment author="clintongormley" created="2015-11-18T13:33:24Z" id="157714188">One small comment, but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update bool-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14434</link><project id="" key="" /><description /><key id="114579590">14434</key><summary>Update bool-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bogdanvlviv</reporter><labels /><created>2015-11-02T11:50:16Z</created><updated>2015-11-08T17:53:15Z</updated><resolved>2015-11-08T17:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T17:53:15Z" id="154852902">Hi @bogdanvlviv 

Thanks for the PR.  Actually, in 1.4 that form was accepted. In 2.0, the bool filter has been removed in favour of the bool query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for query_binary and filter_binary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14433</link><project id="" key="" /><description>query_binary and filter_binary are unused at this point, as we only parse on the coordinating node and the java api only holds structured java objects for queries and filters, meaning they all implement Writeable and get natively serialized.

Relates to #14308
</description><key id="114574464">14433</key><summary>Remove support for query_binary and filter_binary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T11:15:10Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-11-03T13:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-02T15:51:53Z" id="153059771">LGTM, but should be marked as "breaking" and noted in the migration guide?
</comment><comment author="javanna" created="2015-11-02T15:58:45Z" id="153061613">&gt; should be marked as "breaking" and noted in the migration guide?

not really, we are just removing support for something that is at this time unused and was never documented. Last bit of the actual removal was part of #14384 with the removal of `QuerySourceBuilder`, which used `query_binary` internally and required it to be supported in core.
</comment><comment author="dakrone" created="2015-11-02T16:02:37Z" id="153062636">&gt;  we are just removing support for something that is at this time unused and was never documented

From the code it looks like someone could send `query_binary` or `queryBinary` at the REST layer, is that correct? If so, I definitely think we should mark/document this as breaking, because even though we don't _think_ anyone uses it, someone could be.
</comment><comment author="javanna" created="2015-11-02T16:46:21Z" id="153079083">Sure the feature is exposed to the outside world, I can mark breaking just to make sure, but it feels very weird to document that we removed it given that we never documented before, and we couldn't even deprecate in 2.x for the very same reason. This feature made sense only internally since the java api allowed to provide queries as binary objects, it should be treated as an internal thing as it was up until now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deduplicate cause if already contained in shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14432</link><project id="" key="" /><description>If we have a shard failure on SearchPhaseExecutionException
we can deduplicate the original cause and use the more informative
ShardSearchFailure containing the shard ID etc. but we should deduplicate
the actual cause to prevent stack trace duplication.
</description><key id="114555122">14432</key><summary>Deduplicate cause if already contained in shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T09:18:38Z</created><updated>2015-11-03T11:06:25Z</updated><resolved>2015-11-03T11:06:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-02T10:44:03Z" id="152984737">LGTM
</comment><comment author="jpountz" created="2015-11-03T10:56:11Z" id="153314253">OK, I had missed that this exception overrides getCause(). LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Missing tribe nodes' short description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14431</link><project id="" key="" /><description /><key id="114544291">14431</key><summary>[DOCS] Missing tribe nodes' short description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arinto</reporter><labels><label>docs</label></labels><created>2015-11-02T07:58:22Z</created><updated>2015-11-09T05:51:23Z</updated><resolved>2015-11-08T17:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T17:37:13Z" id="154850162">thanks @arinto - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing tribe nodes' short description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14430</link><project id="" key="" /><description /><key id="114543952">14430</key><summary>Missing tribe nodes' short description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arinto</reporter><labels /><created>2015-11-02T07:54:38Z</created><updated>2015-11-09T05:51:24Z</updated><resolved>2015-11-02T07:57:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arinto" created="2015-11-02T07:57:28Z" id="152945883">I'll resubmit the PR after I signed the iCLA
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update node.(other) docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14429</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-node.html) we only list `node.data` when there is `.master` as well.

We should reflect this in the docs as a I've seen a few people ask about this just recently.
</description><key id="114543145">14429</key><summary>Update node.(other) docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2015-11-02T07:46:42Z</created><updated>2016-01-30T17:14:31Z</updated><resolved>2016-01-30T17:14:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arinto" created="2015-11-02T08:04:00Z" id="152946664">+1. Original discussion thread is [here](https://discuss.elastic.co/t/dedicated-master-nodes-in-elasticsearch-2-0/33485).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated github link and doc for Kafka Consumer client for ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14428</link><project id="" key="" /><description>Updated latest github link and doc for Kafka Consumer client for ES.
</description><key id="114535415">14428</key><summary>Updated github link and doc for Kafka Consumer client for ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reachkrishnaraj</reporter><labels><label>docs</label></labels><created>2015-11-02T06:38:34Z</created><updated>2015-11-08T22:29:13Z</updated><resolved>2015-11-08T22:29:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T22:29:13Z" id="154882048">Hi @reachkrishnaraj 

These docs have moved - please open a new PR adding your changes here instead: https://www.elastic.co/guide/en/elasticsearch/plugins/current/integrations.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct README.textile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14427</link><project id="" key="" /><description>It should be "h3. Upgrading from Elasticsearch 1.x?" in README.textile.
</description><key id="114533656">14427</key><summary>Correct README.textile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">biyuhao</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-11-02T06:23:45Z</created><updated>2015-11-08T22:18:40Z</updated><resolved>2015-11-05T17:40:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-11-05T17:42:54Z" id="154134277">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: Update nested objects directly with the HTTP API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14426</link><project id="" key="" /><description>I have searched during some time for a practical way to update nested objects without having to touch the parent ones. The best I could find so far in the official docs was this link that explains how to [use scripts to make partial updates](https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html#_using_scripts_to_make_partial_updates). This approach has a few downsides:
- You have to enable scripting, which sometimes isn't an option if you are relying on a (cheap) hosted solution (my case), since shared environments aren't reliable enough;
- You have to learn Groovy, which is not so similar to, say, JS, when you are doing something that involves more than a ternary operator.

On the other hand, I think most web developers are so used to REST APIs that something like this would be common sense:
PUT .../index_name/parent_type/123/nested_type/456
- If both the parent object (with ID 123) and the nested object (with ID 456) exist, update the nested object with the request body;
- If only the parent object (with ID 123) exists, insert the nested object (with ID 456) in the array with the other nested objects (in the parent document);
- Otherwise, if the parent object doesn't exist, raise an error

So, what do you think of the idea? Has anyone else proposed something like that before? I would love to contribute myself, but unfortunately Java is not my thing... Do you accept PRs in Scala or Kotlin?
</description><key id="114525880">14426</key><summary>Feature request: Update nested objects directly with the HTTP API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">giovannibonetti</reporter><labels /><created>2015-11-02T05:03:26Z</created><updated>2015-11-08T22:17:35Z</updated><resolved>2015-11-08T22:17:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T22:17:35Z" id="154881197">Hi @giovannibonetti 

Unfortunately, your proposal would be too limited to be generically useful.  We've had discussion about something like this before in #7030, but it hasn't made any progress yet.  Btw, you can store scripts as files in your config directory and use those without enabling dynamic/inline scripting (and going from JS to groovy is not too hard :) )
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark the ingest plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14425</link><project id="" key="" /><description>The ingest plugin should be benchmarked with at least the `grok` and `geoip` processors (likely to be the most used, but are also the processors that are the more complex compared to the `date` or `mutate` processors).

The benchmark should be based on a real world use cases.
</description><key id="114509556">14425</key><summary>Benchmark the ingest plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-11-02T02:02:27Z</created><updated>2016-03-04T15:51:26Z</updated><resolved>2016-03-04T15:51:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-11-02T22:13:49Z" id="153174319">@martijnvg, where would these benchmarks run?

Also, I was thinking of introducing `JMH` micro-benchmarks to help with improvements to the grok processor. Specifically, I was exploring different regex engines to see if it makes sense to stick with Joni. Do you feel this type of micro-benchmarking belongs in the source, or should I keep this outside in a separate project?

Which Data?

would we want to use synthetically generated log lines using randomly generated strings, 
or find a sample dataset of logs that we can use?
</comment><comment author="martijnvg" created="2015-11-03T02:04:01Z" id="153216894">&gt; where would these benchmarks run?

Initially adhoc from our dev machines and maybe later somewhere else but automated.

The idea I had was to start a full cluster (at least one data/master node and an ingest node) and run the benchmark against that. I haven't thought about micro benchmarks, but we add them maybe to a separate repository?

&gt; Which Data?

No idea yet. Would be great if just have a real sample set (~10GB or something like that) of logs.
</comment><comment author="martijnvg" created="2016-03-04T15:51:26Z" id="192330591">I ran a benchmark that indexes apache logs into a single shard index. The ingest overhead is based on comparing the recently added `ingest_took` in the bulk response with the time it took for the bulk API to answer.

I ran several iterators. Indexing 300k, 600k and 1,2M document. For each volume I ran with different pipeles:
- `empty` - A pipeline with no processors.
- `set` - A pipeline that adds the current ingest time as a field to the document being processed.
- `grok` - A pipeline that split up the apache log into several fields. 
- `grok_and_geoip` - Same as `grok`, but also adds geo information based on the client ip.

So in total I ran 12 iterations:

| # docs | pipeline | ingest time overhead |
| --- | --- | --- |
| 300k | empty | 12% |
| 300k | set | 21% |
| 300k | grok | 27% |
| 300k | grok_and_geoip | 37% |
| 600k | empty | 13% |
| 600k | set | 21% |
| 600k | grok | 27% |
| 600k | grok_and_geoip | 37% |
| 1.2M | empty | 13% |
| 1.2M | set | 21% |
| 1.2M | grok | 26% |
| 1.2M | grok_and_geoip | 37% |

Each pipeline has a static tax on top of the indexing time and the time spent increases roughly linearly as more documents are being ingested. While ingesting these apache log with ingest, the used heap space overhead wasn't really noticeable.

The heap memory overhead of ingest is more tied to the size of the documents being pre-processed than to the amount of document being processed in a single bulk request. This is because ingest turns each document into a map of maps and because per bulk request, ingest processes a single document at the time.  So the ingest heap overhead becomes noticeable when preprocessing larger documents. This is why the used heap overhead wasn't noticeable when benchmarking with logs. However when testing with documents that have ~400 fields, I have seen the used heap overhead being 20% to 30% higher then when indexing the same data without ingest enabled.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Homebrew-installed Elasticsearch fails on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14424</link><project id="" key="" /><description>after update to 2.0 did not start anymore now shows:

**es.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.**

i'm using homebrew, someone get the same issue?
</description><key id="114499062">14424</key><summary>Homebrew-installed Elasticsearch fails on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">dcalixto</reporter><labels /><created>2015-11-01T23:03:43Z</created><updated>2015-11-05T22:53:09Z</updated><resolved>2015-11-05T22:53:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ssoulless" created="2015-11-02T00:33:38Z" id="152882032">Yes me I have exact same issue...
</comment><comment author="Sen" created="2015-11-02T03:32:30Z" id="152902693">I have same issue too, any solution?
</comment><comment author="ssoulless" created="2015-11-02T03:37:12Z" id="152903820">Well a quick fix is to downgrade to 1.4, I had an urgency then I did that.

If you are using home-brew do the following:

```
brew uninstall elasticsearch
```

```
brew install homebrew/versions/elasticsearch14
```

It will install the previous version of elastic search that does not have this issue, then you can start elastic search without any problem:

```
launchctl load ~/Library/LaunchAgents/homebrew.mxcl.elasticsearch.plist
```
</comment><comment author="Sen" created="2015-11-02T04:00:55Z" id="152905963">@ssoulless brew just rollbacked to 1.73, fixing local elasticsearch formula file works too
</comment><comment author="zappeee" created="2015-11-02T09:54:28Z" id="152974067">I have the same problem, though I've installed via apt-get / dkpg.
</comment><comment author="nik9000" created="2015-11-02T12:51:06Z" id="153008618">You could just stop using the parameter.... It's no longer supported
because the config dir option gives you as much flexibility in a slightly
different way and it is much simpler for us to support.
On Nov 2, 2015 4:54 AM, "zappeee" notifications@github.com wrote:

&gt; I have the same problem, though I've installed via apt-get / dkpg.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14424#issuecomment-152974067
&gt; .
</comment><comment author="zappeee" created="2015-11-02T12:55:15Z" id="153009329">I would if I just knew where it was used :)

I did a find on my machine and cannot find it anywhere, so any help would be appreciated!
</comment><comment author="jasontedor" created="2015-11-02T13:18:56Z" id="153013607">@dcalixto @ssoulless It's coming from the p-list that the Homebrew formula is building. I'll open up a pull request in the Homebrew project to fix several issues that I see with the formula.
</comment><comment author="nik9000" created="2015-11-02T13:21:24Z" id="153014171">@zappeee, it looks like you aren't using homebrew - if you want to contact me directly I can connect with you outside the ticket and we can track down where the setting comes from for you. My email is in my github profile.
</comment><comment author="jasontedor" created="2015-11-03T19:22:49Z" id="153460885">I've opened Homebrew/homebrew#45644 to address this and other issues. Thank you for bringing them to our attention.
</comment><comment author="jasontedor" created="2015-11-05T22:53:09Z" id="154221547">Closed by Homebrew/homebrew#45644.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Starting checked out elasticsearch cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14423</link><project id="" key="" /><description>According to https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc it can be done by running [run.sh](../blob/master/run.sh) or [run.bat](../blob/master/run.bat), but those guys use mvn.

I see there's been a push to move from maven: #13930. And there's an item there to create a gradle task for this.

I'm new to gradle and the build of elasticsearch, but could folks explain how they startup a node for a little poking around locally - maybe I can script it up. I'm trying to start a node run a few REST calls to get the output for examples in the docs.

My workaround has been to run 
    gradle assemble

Then extract distribution/tar/build/distributions/elasticsearch-3.0.0-SNAPSHOT.tgz and run ./elasticsearch

Thanks
</description><key id="114488147">14423</key><summary>Starting checked out elasticsearch cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels><label>build</label></labels><created>2015-11-01T19:39:46Z</created><updated>2015-11-09T19:15:53Z</updated><resolved>2015-11-09T19:15:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-01T22:34:49Z" id="152870684">We should certainly fix run.sh and run.bat but in what you are doing is
fine.

If you import the project in eclipse you should be able to use the run
configuration sitting in dev-tools. It night also be broken by the move to
gradle, as that was just a few days ago and we had a bazillion things to
make work and certainly missed a few, like run.sh.

Untaring the distribution is the most real thing you can do. In prod you
are more likely to use the deb or rpm but they don't differ much from the
tar save for startup and permissions things.
On Nov 1, 2015 2:39 PM, "Andrej Kazakov" notifications@github.com wrote:

&gt; According to
&gt; https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc it
&gt; can be done by running run.sh http://../blob/master/run.sh or run.bat
&gt; http://../blob/master/run.bat, but those guys use mvn.
&gt; 
&gt; I see there's been a push to move from maven: #13930
&gt; https://github.com/elastic/elasticsearch/issues/13930. And there's an
&gt; item there to create a gradle task for this.
&gt; 
&gt; I'm new to gradle and the build of elasticsearch, but could folks explain
&gt; how they startup a node for a little poking around locally - maybe I can
&gt; script it up. I'm trying to start a node run a few REST calls to get the
&gt; output for examples in the docs.
&gt; 
&gt; My workaround has been to run
&gt; gradle assemble
&gt; 
&gt; Then extract
&gt; distribution/tar/build/distributions/elasticsearch-3.0.0-SNAPSHOT.tgz and
&gt; run ./elasticsearch
&gt; 
&gt; Thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14423.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch not accessed via publicIP:9200</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14422</link><project id="" key="" /><description>hi, we have setup new server, the version 1.7.2 was working perfect and was accessible from public IP address as well.
After upgrading to  version 2.0 i can do
curl -X GET 'http://localhost:9200' vai SSH but 
http://&lt;public-IP&gt;:9200 gives me "unable to connect"
</description><key id="114478641">14422</key><summary>Elasticsearch not accessed via publicIP:9200</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">harpreetsb</reporter><labels /><created>2015-11-01T16:22:22Z</created><updated>2015-11-02T06:53:27Z</updated><resolved>2015-11-01T16:28:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-11-01T16:28:09Z" id="152841789">Elasticsearch now binds on localhost so you can only access it from the local machine.

See network.host settings.
</comment><comment author="harpreetsb" created="2015-11-01T16:45:44Z" id="152843023">could you guide me in specific direction, Links.
</comment><comment author="dadoonet" created="2015-11-01T16:48:21Z" id="152843160">https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-network.html
</comment><comment author="harpreetsb" created="2015-11-02T03:54:49Z" id="152905400">can you tell me the specific settings. i did
'network.host : PUBLIC IP'
but did not work
</comment><comment author="dadoonet" created="2015-11-02T05:55:42Z" id="152921282">What did you do exactly? I'd suggest that you open a thread on discuss.elastic.co with all details and logs.
</comment><comment author="harpreetsb" created="2015-11-02T06:53:27Z" id="152933565">solution found

```
network.host : 0.0.0.0
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using the accept header in the request instead of content-type in _cat API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14421</link><project id="" key="" /><description>Using the accept header in the request instead of content-type.

fixes #14195.
</description><key id="114470244">14421</key><summary>Using the accept header in the request instead of content-type in _cat API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels><label>:CAT API</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-11-01T13:40:53Z</created><updated>2016-02-05T12:52:47Z</updated><resolved>2016-02-05T11:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrejserafim" created="2015-11-01T13:42:08Z" id="152827205">I've tried to squash the multiple revisions several times, but I'm failing to do so, probably just doing something wrong. Perhaps this will do?
</comment><comment author="nik9000" created="2015-11-01T13:52:18Z" id="152827631">For prs you can squash and force push to the branch in your fork with the patch and it'll update the pe. Github can throw out comment history sometimes when you do this so only do it before or after comments are done. Between times just push new patches to the branch. 

That rule isn't hard and fast. Like, if the patch is really out of date you'll want to rebase it eventually and that requires a force push. Usually wait on that if you can. 

I had a look at the patch. Two things:
- do we want to support the old header too?
- immutable map is banned now so just use singletonmap and emptymap.  The build should warn you about that if you have forked from master. 
</comment><comment author="andrejserafim" created="2015-11-01T14:16:34Z" id="152828986">And done - good catch on supporting the old behaviour still. 

Just out of curiosity why ban ImmutableMap? - do you remember? - I didn't get a warning I'd notice (the build has quite a bit of output, so I might have missed it). 

But also if you're referring to https://github.com/elastic/elasticsearch/commit/52f3c89c3b8be4fef24ba1f548b58120f875a7b6#diff-8a460c3fa725e7fce0de794833592128, then only the ImmutableMap.entrySet() seems to be on the list, unless I misinterpreting the usage of dev-tools/src/main/resources/forbidden/all-signatures.txt
</comment><comment author="jasontedor" created="2015-11-01T15:04:39Z" id="152833125">&gt; Just out of curiosity why ban ImmutableMap?

Guava has been removed as a compile-time dependency but it's still available as a test-only dependency which is what you're using it as here. The only reason that it remains as test-only dependency is for Jimfs. I think that our preference would be that we not use Guava (including `ImmutableMap`) within our test source code. At this time, it would take some work to make it possible to ban it as test-only time because the builds will fail if you add something to the forbidden list that isn't on the classpath. I'm not sure if it's worth the effort, so let's just avoid use of it here?
</comment><comment author="nik9000" created="2015-11-01T15:07:03Z" id="152833327">&gt; I think that our preference would be that we not use them in tests either.

Given the time I spent removing it from tests I think we should make sure its banned there too. What we saw was that as of Java 8 there is always a more Java-8 way to do things. Except for having immutability encoded as part of the type of an object.
</comment><comment author="jasontedor" created="2015-11-01T15:08:26Z" id="152833715">&gt; Given the time I spent removing it from tests I think we should make sure its banned there too.

I could be wrong, but I don't think that we have the infrastructure in place to do it today. I think that if we ban it right now, the builds will fail because of the addition to something to the forbidden list that isn't on the compile-time classpath. I think that we'll have to put infrastructure in place to create a test-only forbidden list and apply it at test compilation time.
</comment><comment author="nik9000" created="2015-11-01T15:34:07Z" id="152834828">&gt; I could be wrong, but I don't think that we have the infrastructure in place to do it today.

I thought `test-signatures.txt` would do that but I wouldn't be surprised if I was wrong.
</comment><comment author="jasontedor" created="2015-11-01T15:45:05Z" id="152838487">&gt; I thought test-signatures.txt would do that but I wouldn't be surprised if I was wrong.

I stand corrected. :)
</comment><comment author="andrejserafim" created="2015-11-01T15:54:59Z" id="152840108">&gt; &gt; I thought test-signatures.txt would do that but I wouldn't be surprised if I was wrong.
&gt; 
&gt; I stand corrected. :)

I've tested it locally, it fails the build in the presence of ImmutableMap, so it's effective indeed.
</comment><comment author="andrejserafim" created="2015-11-02T17:08:56Z" id="153085704">I think that's all the comments addressed. Please let me know if there's anything else.
</comment><comment author="ywelsch" created="2015-11-05T10:42:10Z" id="154025989">Using the Accept header with smile, yaml or cbor does not work yet. Unfortunately, the current tests don't cover this.

The reason is that `RestChannel.newBuilder(...)`, which is invoked by `RestTable`, still uses "Content-Type" to determine content-type of result.
</comment><comment author="andrejserafim" created="2015-11-05T19:14:49Z" id="154158586">@ywelsch - good spot, I've added the tests, also made the changes to make cbor and smile work.

Also rebased the fork.
</comment><comment author="ywelsch" created="2015-11-10T17:00:46Z" id="155486805">Thanks for adding the tests. LGTM! @nik9000 or @jasontedor, can you have another look?
</comment><comment author="jasontedor" created="2015-11-10T18:12:18Z" id="155518247">The commit 391a91595e9e1bdbb9fb26687424cddf93308f0d can be removed because the transitive Guava dependency in tests has been removed from the test framework via https://github.com/elastic/elasticsearch/commit/6be9954d28f47b1493389296e3fffdc04a010a00. (And even if it had not been removed, it would be better if it was in a separate PR since it's independent from the issue being addressed with this pull request.)
</comment><comment author="andrejserafim" created="2015-11-10T18:31:13Z" id="155523829">forbidden signature removed.
</comment><comment author="jasontedor" created="2015-11-10T19:08:28Z" id="155534030">@andrejserafim I made some comments regarding the addition to the forbidden list for the testing framework as well as the method-name change / method addition that you made to `XContentType`.

Next, I don't think that with this PR we are handling wildcards appropriately (e.g., `Accept: application/*`).

The last comment I have is that it appears that we are not returning HTTP 406 if the accept header contains a type that we do not handle (e.g., `image/png`). Instead, the request is happily replied to with media type `text/plain; charset=UTF-8`.
</comment><comment author="andrejserafim" created="2015-11-11T19:50:21Z" id="155891909">@jasontedor - case sensitivity and method names changed, I still need to look at wildcards and responding with 406.
</comment><comment author="andrejserafim" created="2015-11-13T17:59:59Z" id="156502115">@jasontedor returning 406 is feels against the grain with the code in it's current shape. I say this because : 
1. there are provisions to try and guess the media type. For example: `org.elasticsearch.rest.RestChannel#newBuilder(org.elasticsearch.common.bytes.BytesReference, boolean)` calls 
   `contentType = XContentFactory.xContentType(autoDetectSource);`
   which uses the body (if any) to determine the content (I understand that the _cat api is GET only).
2. RestChannel has defaults as well, where it will assume JSON if no media type supplied - the RestTable prevents that by checking early and defaulting to TEXT. 

It looks like the _cat API would be one of a small minority returning 406s. It could be a worthwhile thing to do, but it sounds like a much bigger change, probably warrants it's own discussion.

for wildcard media types, do I understand you correctly, that you'd like to default to let's say json if we get `application/*` ?
</comment><comment author="andrejserafim" created="2015-11-16T19:06:13Z" id="157137657">@jasontedor wdyt re 406 and wildcard changes?
</comment><comment author="jasontedor" created="2015-11-17T15:17:40Z" id="157400451">&gt; It looks like the _cat API would be one of a small minority returning 406s. It could be a worthwhile thing to do, but it sounds like a much bigger change, probably warrants it's own discussion.

Agree.

&gt; for wildcard media types, do I understand you correctly, that you'd like to default to let's say json if we get `application/*` ?

Yes.
</comment><comment author="andrejserafim" created="2015-11-17T16:48:14Z" id="157427930">@jasontedor so that's all done then.
</comment><comment author="andrejserafim" created="2015-11-24T17:53:02Z" id="159355038">merged with master.
</comment><comment author="dakrone" created="2015-11-24T18:00:17Z" id="159356955">We should be able to add a REST test for this once https://github.com/elastic/elasticsearch/pull/14973 is merged (and I am merging it now)
</comment><comment author="andrejserafim" created="2015-12-01T19:36:57Z" id="161072620">The REST tests are a little tricky to work with - I can't debug them in my IDE unfortunately, intellij doesn't like RestIT classes in the distribution packages.

Anyhow, I also couldn't test json payloads since they come back as a list rather than a json object, so the test framework can't take them in it's current shape. Ideally we'd just do a json comparison there.

We've done the unit tests to cover all media types, so this is just ensuring that our changes are indeed effective, yaml should suffice for now.
</comment><comment author="andrejserafim" created="2015-12-07T17:38:49Z" id="162602983">any other comments on this one?
</comment><comment author="clintongormley" created="2015-12-15T11:41:03Z" id="164738658">@dakrone could you take another look please?
@andrejserafim I see this PR won't merge cleanly at the moment. Would you mind rebasing against master please?
</comment><comment author="andrejserafim" created="2015-12-15T18:54:25Z" id="164855173">rebased and squashed the commits into 1.
</comment><comment author="clintongormley" created="2016-01-29T18:38:27Z" id="176901884">Apologies for the delay @andrejserafim - any chance you can rebase against master again?
</comment><comment author="andrejserafim" created="2016-01-30T19:05:53Z" id="177278145">rebased with master.
</comment><comment author="clintongormley" created="2016-02-02T13:56:04Z" id="178583382">thanks @andrejserafim. @jasontedor could you take another look please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>resolved #14195</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14420</link><project id="" key="" /><description>Using the accept header in the request instead of content-type.

fixes #14195.
</description><key id="114469014">14420</key><summary>resolved #14195</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels /><created>2015-11-01T13:16:08Z</created><updated>2015-11-01T14:53:51Z</updated><resolved>2015-11-01T13:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-01T13:38:25Z" id="152827020">Next time can you put what you did in the subject line of if the commit/pr?
It makes scanning my email for which patches to review simpler because
github makes it the email subject. The number of the issue resolved isn't
too useful for scanning. The first sentence of the commit message would be
perfect!
On Nov 1, 2015 8:17 AM, "Andrej Kazakov" notifications@github.com wrote:

&gt; Closed #14420 https://github.com/elastic/elasticsearch/pull/14420.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14420#event-451339785.
</comment><comment author="andrejserafim" created="2015-11-01T14:13:24Z" id="152828798">Certainly!

Would it confuse matters too much if I change the pull request subject now?

On Sun, 1 Nov 2015 at 13:39 Nik Everett notifications@github.com wrote:

&gt; Next time can you put what you did in the subject line of if the commit/pr?
&gt; It makes scanning my email for which patches to review simpler because
&gt; github makes it the email subject. The number of the issue resolved isn't
&gt; too useful for scanning. The first sentence of the commit message would be
&gt; perfect!
&gt; On Nov 1, 2015 8:17 AM, "Andrej Kazakov" notifications@github.com wrote:
&gt; 
&gt; &gt; Closed #14420 https://github.com/elastic/elasticsearch/pull/14420.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elastic/elasticsearch/pull/14420#event-451339785.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14420#issuecomment-152827020
&gt; .
</comment><comment author="nik9000" created="2015-11-01T14:53:51Z" id="152831535">No. Go ahead and change.
On Nov 1, 2015 9:13 AM, "Andrej Kazakov" notifications@github.com wrote:

&gt; Certainly!
&gt; 
&gt; Would it confuse matters too much if I change the pull request subject now?
&gt; 
&gt; On Sun, 1 Nov 2015 at 13:39 Nik Everett notifications@github.com wrote:
&gt; 
&gt; &gt; Next time can you put what you did in the subject line of if the
&gt; &gt; commit/pr?
&gt; &gt; It makes scanning my email for which patches to review simpler because
&gt; &gt; github makes it the email subject. The number of the issue resolved isn't
&gt; &gt; too useful for scanning. The first sentence of the commit message would
&gt; &gt; be
&gt; &gt; perfect!
&gt; &gt; On Nov 1, 2015 8:17 AM, "Andrej Kazakov" notifications@github.com
&gt; &gt; wrote:
&gt; &gt; 
&gt; &gt; &gt; Closed #14420 https://github.com/elastic/elasticsearch/pull/14420.
&gt; &gt; &gt; 
&gt; &gt; &gt; &#8212;
&gt; &gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; &gt; https://github.com/elastic/elasticsearch/pull/14420#event-451339785.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; &lt;
&gt; &gt; https://github.com/elastic/elasticsearch/pull/14420#issuecomment-152827020
&gt; &gt; 
&gt; &gt; .
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14420#issuecomment-152828798
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>resolved #14195</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14419</link><project id="" key="" /><description>Using the accept header in the request instead of content-type.

fixes #14195.
</description><key id="114468593">14419</key><summary>resolved #14195</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels /><created>2015-11-01T13:05:53Z</created><updated>2015-11-01T13:07:10Z</updated><resolved>2015-11-01T13:07:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>resolved #14195</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14418</link><project id="" key="" /><description>Using the accept header in the request instead of content-type.

fixes #14195.
</description><key id="114468554">14418</key><summary>resolved #14195</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrejserafim</reporter><labels /><created>2015-11-01T13:04:24Z</created><updated>2015-11-01T13:05:31Z</updated><resolved>2015-11-01T13:05:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ES 2.0.0. Marvel not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14417</link><project id="" key="" /><description>Yes there was #14349 for ES 2.0.0-rc1 however it's identical for ES 2.0.0
# curl localhost:9200

{
  "name" : "Deadpool",
  "cluster_name" : "kibanadev",
  "version" : {
    "number" : "2.0.0",
    "build_hash" : "de54438d6af8f9340d50c5c786151783ce7d6be5",
    "build_timestamp" : "2015-10-22T08:09:48Z",
    "build_snapshot" : false,
    "lucene_version" : "5.2.1"
  },
  "tagline" : "You Know, for Search"
}

[root@vm000530 bin]# ./plugin install marvel --verbose
-&gt; Installing marvel...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel/2.0.0/marvel-2.0.0.zip ...
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel/2.0.0/marvel-2.0.0.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel/2.0.0/marvel-2.0.0.zip];
</description><key id="114410427">14417</key><summary>ES 2.0.0. Marvel not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcinkubica</reporter><labels /><created>2015-10-31T13:07:08Z</created><updated>2015-11-29T14:03:52Z</updated><resolved>2015-10-31T13:12:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-10-31T13:12:56Z" id="152734695">You should run:

```
bin/plugin install license
bin/plugin install marvel-agent
```

The documentation may help you: https://www.elastic.co/guide/en/marvel/current/getting-started.html
</comment><comment author="marcinkubica" created="2015-10-31T13:53:13Z" id="152738311">Cheers mate.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ShapeBuilder impls to implement Writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14416</link><project id="" key="" /><description>At the GeoShapeQueryBuilder holds the ShapeBuilder as a bytes reference, meaning that it won't be parsed on the coordinating node but on each data node. This was done to speed up the query refactoring but should be revisited at this point. Parsing on the coordinating node would be better, it means that ShapeBuilder implementation would need to implement `Writeable` and override `equals` and `hashcode` for testing.
</description><key id="114409058">14416</key><summary>ShapeBuilder impls to implement Writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-31T12:35:06Z</created><updated>2016-01-12T18:17:11Z</updated><resolved>2016-01-12T18:17:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] test query and search source parsing from different XContent types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14415</link><project id="" key="" /><description>We used to test only json parsing as we relied on QueryBuilder#toString which uses the json format. This commit makes sure that we now output the randomly generated queries using a random format, and that we are always able to parse them correctly.

This revealed a couple of issues with binary objects that haven't been migrated yet to be structured Writeable objects. We used to keep them in the format they were sent while parsing, which led to problems when printing them out as we expected them to always be in json format. Also we can't compare different BytesReference objects that hold the same content but in different formats (unless we want to parse them as part of equal and hashcode, doesn't seem like a good idea)  and verify that we have parsed the right objects if they can be different formats. The fix is to always keep binary objects in json format. Best fix would be not to have binary objects, which we'll get to once we are done with the search refactoring.
</description><key id="114405682">14415</key><summary>[TEST] test query and search source parsing from different XContent types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>review</label><label>test</label></labels><created>2015-10-31T11:45:03Z</created><updated>2015-11-09T14:29:23Z</updated><resolved>2015-11-09T14:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-31T11:45:49Z" id="152726523">@colings86 @cbuescher can you have a look please?
</comment><comment author="cbuescher" created="2015-11-02T11:16:31Z" id="152990936">LGTM, added a minor comment. 
</comment><comment author="javanna" created="2015-11-02T13:26:46Z" id="153016030">I have rebased and updated, will wait for @colings86 to have a look though just to make sure that he agrees on the changes made to `SearchSourceBuilder`
</comment><comment author="colings86" created="2015-11-09T11:38:05Z" id="155038933">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expand testing of queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14414</link><project id="" key="" /><description>Similar to what we attempted with #11608, we should find a way to expand the range of queries that we test against in our query tests. Our random query generator currently generates a query out of a very limited set of supported queries. Also those queries are always leaf queries. See https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java#L41 .

We should expand the set of random queries, at the same time without going nuts otherwise or we'll end up with too complicated queries that make test failures super tricky to debug. For instance it's important to set a reasonable maximum depth in case we introduce compound queries.

This might end up being a big-ish change to the query test infra given that at the moment our tests depend on each other (e.g. to generate a random term on any test query we do `new TermQueryBuilderTests().createTestQueryBuilder()`). Maybe we should extract that logic from the tests like I tried to do in #11608, not sure.

Also, we should do the same for span queries. At the moment we only create span_term queries, but span queries can be combined together (e.g. span_or that contains span_not etc.) and we never test that (besides few specific tests that have been written).

Also, while we make changes we should take into account some todos that are currently in `SearchSourceBuilderTests` (see https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java#L214). Seems like we should find a way to test against random queries when generating a random search source too. Maybe we just need to make the query generation a little more generic.
</description><key id="114401471">14414</key><summary>Expand testing of queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>adoptme</label><label>test</label></labels><created>2015-10-31T10:03:19Z</created><updated>2017-01-12T10:47:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-31T10:04:03Z" id="152719569">@cbuescher ping I thought this might interest you as you've been working on query testing, or maybe @MaineC as well.
</comment><comment author="clintongormley" created="2016-05-03T12:41:15Z" id="216515257">@cbuescher can this be closed?
</comment><comment author="cbuescher" created="2016-05-03T12:48:26Z" id="216516669">@clintongormley I think this is still on the wantlist. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deduplicate search sections parsing code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14413</link><project id="" key="" /><description>When we moved query parsing on the coordinating node (see #10217), we added a new `SearchSourceBuilder#fromXContent` method that given an `XContentParser` and the current `QueryParseContext` parses and creates a new `SearchSourceBuilder` object. The parse code ends up duplicating what we already have in our `SearchParseElement` classes. We made a choice not to refactor these initially as they depend on `SearchContext` and there are still apis that use them to parse lucene queries straight-away and set them to the search context rather than going through our intermediate query representation.

This solution can only be temporary though, as we are currently duplicating parsing code. What we should do ideally instead, is move over parsing code to the new solution (either `SearchSourceBuilder#fromXContent` or an updated version of `SearchParseElement` that doesn't depend on `SearchContext`) and get rid of the previous parsing code. I realize how this becomes hard to do given that we have not moved yet all of the search sections over. Maybe we should wait till the search refactoring is completed, but after that  we should clean this up and have only one way to parse search sections, with code in one single place, otherwise it becomes hard to understand how each api parses search sections.
</description><key id="114399536">14413</key><summary>Deduplicate search sections parsing code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>enhancement</label></labels><created>2015-10-31T09:10:42Z</created><updated>2016-09-08T19:04:15Z</updated><resolved>2016-09-08T08:50:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T12:40:55Z" id="216515190">@javanna @cbuescher does this still need to happen?
</comment><comment author="javanna" created="2016-05-03T13:16:06Z" id="216523667">I don't know anymore, better ask @colings86  :)
</comment><comment author="colings86" created="2016-05-09T07:23:54Z" id="217793272">The `ParseElement` classes should be obselete now as all the search request parsing happens on the coordinating node and doesn't use `ParseElement` so we should now be able to remove `ParseElement` and all its subclasses from the codebase.
</comment><comment author="javanna" created="2016-09-06T17:41:16Z" id="245029643">&gt; The ParseElement classes should be obselete now as all the search request parsing happens on the coordinating node and doesn't use ParseElement so we should now be able to remove ParseElement and all its subclasses from the codebase.

I double checked and I think we are not using `SearchParseElement` ourselves as each of our search sections has its own `fromXContent` method now, but plugins still need the interface to provide their parsing code for the ext section. We may want to revise a bit this code though and make it clear that it's needed only for plugins.
</comment><comment author="colings86" created="2016-09-07T07:47:31Z" id="245202383">Yep, the `SearchParseElement` is now only used for plugins which add a custom fetch sub phase. We should work on a way to replace this with the builder system we have for the rest of the search request. When I looked at doing that before I think it got tricky but unfortunately I don't remember what made it tricky
</comment><comment author="javanna" created="2016-09-08T08:50:53Z" id="245534293">I opened #20382 to clean things up a bit. Closing this one then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`mvn clean package -DskipTests` fails due to forbidden-apis check on elasticsearch master branch.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14412</link><project id="" key="" /><description>Following stacktrace is logged:

```
[INFO] --- forbiddenapis:2.0:check (check-forbidden-apis) @ elasticsearch ---
[INFO] Scanning for classes to check...
[INFO] Reading bundled API signatures: jdk-unsafe-1.8
[INFO] Reading bundled API signatures: jdk-deprecated-1.8
[INFO] Reading bundled API signatures: jdk-system-out
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Build Tools and Resources ......................... SUCCESS [  0.894 s]
[INFO] Rest API Specification ............................ SUCCESS [  0.219 s]
[INFO] Elasticsearch: Parent POM ......................... SUCCESS [  3.978 s]
[INFO] Elasticsearch: Core ............................... FAILURE [ 38.082 s]
[INFO] Distribution: Parent POM .......................... SKIPPED
[INFO] Distribution: TAR ................................. SKIPPED
       ***[content trimmed]***
[INFO] QA: Smoke Test Client ............................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 44.293 s
[INFO] Finished at: 2015-10-31T12:37:22+05:30
[INFO] Final Memory: 43M/708M
[INFO] ------------------------------------------------------------------------
```

```
[ERROR] Failed to execute goal de.thetaphi:forbiddenapis:2.0:check (check-forbidden-apis) on project elasticsearch: IO problem while reading files with API signatures. /home/giriraj/elastic/elasticsearch/core/target/dev-tools/forbidden/core-signatures.txt (No such file or directory) 
```

It seems that the check `[forbidden-apis] Reading API signatures` is failing,

```
&#10140; ~ sudo update-alternatives --config java
[sudo] password for giriraj: 
There are 2 choices for the alternative java (providing /usr/bin/java).
  Selection    Path                                            Priority           Status
---------------------------------------------------------------------------------------
  0            /usr/lib/jvm/java-8-oracle/jre/bin/java   2         auto mode
* 1            /usr/lib/jvm/java-7-oracle/jre/bin/java   1         manual mode
  2            /usr/lib/jvm/java-8-oracle/jre/bin/java   2         manual mode
```

forbidden-apis check fails in case of both java-8-oracle as well as java-7-oracle.
</description><key id="114394870">14412</key><summary>`mvn clean package -DskipTests` fails due to forbidden-apis check on elasticsearch master branch.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">girirajsharma</reporter><labels /><created>2015-10-31T07:37:43Z</created><updated>2015-10-31T07:44:15Z</updated><resolved>2015-10-31T07:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-31T07:44:15Z" id="152708966">We just moved to gradle as the build management system for elasticsearch. See #13930. We recently removed also pom files I believe, so at this point maven should not even try and build the project as it is not a maven project anymore and the error should be clearer. Have a look [here](https://github.com/elastic/elasticsearch/blob/master/README.textile#building-from-source) as well for more info.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix eclipse (again)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14411</link><project id="" key="" /><description>Don't recommend the horrible gradle integration built into eclipse:
- it ignores our configuration when it feels
- it sneakily starts up a daemon (which is a cache corrupter, nobody wants that)

Fix output paths to not be "bin", which just litters the source tree. Use build/eclipse/N for each source directory. This is better as e.g. src and tests aren't all smashed together.
</description><key id="114391392">14411</key><summary>fix eclipse (again)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-31T06:41:43Z</created><updated>2015-10-31T06:45:45Z</updated><resolved>2015-10-31T06:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-31T06:44:33Z" id="152699904">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add document-oriented completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14410</link><project id="" key="" /><description>Adds document-oriented completion suggester (see https://github.com/elastic/elasticsearch/issues/10746), replacing the previous completion suggester implementation from master.

This PR contains the changes from `feature/completion_suggester_v2` branch (https://github.com/elastic/elasticsearch/pull/11740 and https://github.com/elastic/elasticsearch/pull/13659).  Apart from code simplifications and bug fixes, the only functional change this adds is a collector which collects top N documents instead of top N suggestions. This was needed to ensure suggestion with multiple contexts would count as one suggestion. The change is contained in [CompletionSuggester.java](https://github.com/elastic/elasticsearch/compare/elastic:master...areek:completion_suggester_master?expand=1#diff-635ea967237e27a266c4b76ff1041016).

The [2.x branch](https://github.com/areek/elasticsearch/tree/completion_suggester_2x) and [2.1 branch](https://github.com/elastic/elasticsearch/tree/feature/completion_suggester_v2) for this feature implements the new completion suggester in a bwc way and allows the old suggester to be used if 
created in a previous version.

closes https://github.com/elastic/elasticsearch/issues/10746
</description><key id="114390498">14410</key><summary>Add document-oriented completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-31T06:15:53Z</created><updated>2016-01-22T10:56:35Z</updated><resolved>2015-11-07T23:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-04T11:23:51Z" id="153690799">LGTM, thanks @areek!

Nice to see we can now remove two forked Lucene classes, the custom postings format, and cutover to Lucene's impls instead.
</comment><comment author="s1monw" created="2015-11-04T12:09:52Z" id="153701439">I left two minor comments on the latest commits, this LGTM too and doesn't need another review. Please go ahead and merge after the comments are fixed
</comment><comment author="djschny" created="2015-11-09T19:31:17Z" id="155166519">Was this prematurely merged? I'm still pretty concerned about removing the payloads ability the users have now without concrete numbers that show the speed is the same. See comments on https://github.com/elastic/elasticsearch/issues/10746
</comment><comment author="clintongormley" created="2016-01-22T10:56:34Z" id="173878338">This was originally merged into 2.2, but has been backed out in https://github.com/elastic/elasticsearch/pull/16170 and so is in master only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add os.allocated_processors stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14409</link><project id="" key="" /><description>Current processors setting is not reflected in nodes info API
("os.available_processors"). Add os.allocated_processors to shows
actual number of processors that we are using.

closes #13917
</description><key id="114389689">14409</key><summary>Add os.allocated_processors stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-31T05:50:15Z</created><updated>2015-11-04T23:03:17Z</updated><resolved>2015-11-03T17:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-31T15:58:40Z" id="152745205">Left some minor comments. LGTM. I think this can go all the way down to 2.1, do you agree?
</comment><comment author="xuzha" created="2015-11-03T23:29:01Z" id="153523091">Merged in master. 

Will back port this to 2.1 and 2.x tonight.
</comment><comment author="xuzha" created="2015-11-04T22:55:57Z" id="153895987">Has been backported to 
2.1 : https://github.com/elastic/elasticsearch/commit/ba95190fdb1a633799aef98c6513ba3d4876bcaa
2.x : https://github.com/elastic/elasticsearch/commit/35e41d3d2ec055f6b17c867553cc38d1c8f9b8c8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0 configuration publish host doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14408</link><project id="" key="" /><description>Hi,
In my configuration, I added following entries in elasticsearch.yml based on https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-http.html

http.publish_host: xx.xx.xx.xx

http.publish_port: 9200

However, it doesn't work as in ES 1.7.

Only setting network.host: xx.xx.xx.xx allows for remote http connection.
</description><key id="114385223">14408</key><summary>Elasticsearch 2.0 configuration publish host doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">gujason721</reporter><labels><label>:Network</label><label>adoptme</label><label>docs</label></labels><created>2015-10-31T03:55:37Z</created><updated>2015-12-14T19:58:36Z</updated><resolved>2015-12-14T19:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="harpreetsb" created="2015-11-02T03:52:22Z" id="152904908">yes same problem, cat access it via public ip
</comment><comment author="nik9000" created="2015-11-02T13:49:58Z" id="153021771">Sorry, I don't understand what you mean. Could you rephrase it?
</comment><comment author="rmuir" created="2015-11-02T13:52:45Z" id="153022537">I think the confusion stems from `publish_XXX` not being listed in the documentation separately as an "advanced option". Setting the publish host does not change binding at all, it only changes what the node "advertises" and is only intended for special proxy/firewall situations. In 99% of cases people should not be changing it.
</comment><comment author="nik9000" created="2015-11-02T13:56:56Z" id="153023307">&gt; In 99% of cases people should not be changing it.

To make this as clear as possible: in almost all cases what you should change is the `host` setting. Not `bind_host` or `publish_host`.

Honestly https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html could do with a rewrite to mention `host` first and way lower down mention that you can, in extraordinary situations, bind to a different host than you publish.
</comment><comment author="rmuir" created="2015-11-02T13:58:20Z" id="153023616">+1
</comment><comment author="avisri" created="2015-12-08T15:32:49Z" id="162916680">Config:
#rpm -qa  | grep elastic
elasticsearch-2.0.0-1.noarch
#grep -v ^# /etc/elasticsearch/elasticsearch.yml| sed  /^$/d| grep publish_host | sed -s "s/10.*/10.x.x.x/"
network.publish_host: 10.x.x.x

Result:
#netstat -planet | grep 9300   | less
tcp        0      0 ::ffff:127.0.0.1:9300       :::\*                        LISTEN      495        30774459   26642/java

I was wondering whether it should bind to  10.x.x.x address instead ? 
</comment><comment author="nik9000" created="2015-12-08T18:32:25Z" id="162972303">&gt; network.publish_host: 10.x.x.x

Use `network.host` instead.
</comment><comment author="avisri" created="2015-12-14T17:04:40Z" id="164495970">Need to ensure 
Elastic listen locally only (9200) but talk to other nodes on the named interface ( 9300) 

Sent from my iPhone

&gt; On Dec 8, 2015, at 10:33 AM, Nik Everett notifications@github.com wrote:
&gt; 
&gt; network.publish_host: 10.x.x.x
&gt; 
&gt; Use network.host instead.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2015-12-14T19:58:36Z" id="164542404">Closed by https://github.com/elastic/elasticsearch/pull/15360

New docs can be seen here: https://www.elastic.co/guide/en/elasticsearch/reference/2.x/modules-network.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make gradle eclipse always run cleanEclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14407</link><project id="" key="" /><description>Otherwise the 'merging' gets really trappy. it basically never works without a clean.

See 38.4.1.1. Disabling merging with a complete rewrite:
https://docs.gradle.org/current/userguide/eclipse_plugin.html
</description><key id="114383640">14407</key><summary>make gradle eclipse always run cleanEclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-31T03:01:24Z</created><updated>2015-11-08T21:51:55Z</updated><resolved>2015-10-31T03:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-31T03:05:17Z" id="152688892">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forwardport BWC test fixes to gradle branches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14406</link><project id="" key="" /><description>Once gradle has stabilized on master we'll need to forward port https://github.com/elastic/elasticsearch/pull/13477 to those branches. Normally forwardporting/backporting doesn't merit its own issue but in this case we'll be committing to the maven branches long before we'll be able to commit to the gradle branches.
</description><key id="114359734">14406</key><summary>Forwardport BWC test fixes to gradle branches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label></labels><created>2015-10-30T21:32:11Z</created><updated>2017-01-12T10:46:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-08T15:52:15Z" id="162924720">To make this clear:
Even though 3.0.0 doesn't have to be wire compatible with any released projects we can forward port this to master and only create the "current" tests. These tests validate the BWC tests by running them against the current version.
</comment><comment author="brwe" created="2016-01-18T17:13:38Z" id="172594824">Discussed with @rjernst and @nik9000 and we decided that best would be to first figure out how many of these actually need the infrastructure with the ExternalNodeService etc. because alternatively we might also just spin up a mixed version cluster and run the rest tests against that. It might even give us more coverage. Some test explicitly test rolling upgrade though. I'll make a list and then we can discuss further.
</comment><comment author="brwe" created="2016-01-20T18:56:13Z" id="173324055">I looked at all bwc tests and asked around what they actually test etc and here is a summary:

We can replace most of the tests with REST tests although we would lose a little bit of randomness. However, there are some things that cannot be testes with REST tests alone:
- transport clients that connect to mixed version clusters (see TransportClientBackwardsCompatibilityIT.testSniffMode)
- rolling upgrades, see BasicBackwardsCompatibilityIT.testIndexRollingUpgrade/testIndexUpgradeSingleNode
- full cluster restarts and expected behavior afterwards, see RecoveryBackwardsCompatibilityIT.testReusePeerRecovery

These tests attempt to simulate real world behavior but the question is how successful this is. For example, with the current setup we cannot install plugins. Also, while we simulate some things these tests don't actually stress the cluster much. For example, we have a test that indexes, upgrades a node, indexes, upgrades a node and so forth but none that indexes while nodes are upgraded. 
It seems wrong to to have these kind of tests in our current setup and we should rather have a dedicated way to simulate an actual cluster and check things like indexing while we upgrade, recovery, etc. As to how this should look like I do not know yet.

In any case, while I believe we would profit from a separation of stress tests and just mixed version cluster tests it will be a lot of work and we might also decide to postpone this and simply port the current mechanism to master (which will be much less work). 

Below is a list with all test and what I think we should do with them. 

Let me know what you think!

Detailed list of all current bwc tests and what I think should be done with them:

## The following tests can be replaced by REST tests:

BasicBackwardsCompatibilityIT:
    public void testExternalVersion() throws Exception {
    public void testInternalVersion() throws Exception {
    public void testIndexAndSearch() throws Exception {
    public void testNoRecoveryFromNewNodes() throws ExecutionException, InterruptedException {
    public void testUnsupportedFeatures() throws IOException {
    public void testExistsFilter() throws IOException, ExecutionException, InterruptedException {
    public void testDeleteRoutingRequired() throws ExecutionException, InterruptedException, IOException {
    public void testIndexGetAndDelete() throws ExecutionException, InterruptedException {
    public void testUpdate() {
    public void testAnalyze() {
    public void testExplain() {
    public void testGetTermVector() throws IOException {
    public void testIndicesStats() {
    public void testMultiGet() throws ExecutionException, InterruptedException {
    public void testScroll() throws ExecutionException, InterruptedException {
    public void testSimpleFunctionScoreParsingWorks() throws IOException, ExecutionException, InterruptedException {

ClusterStateBackwardsCompatIT:
    public void testClusterState() throws Exception {
    public void testClusterStateWithBlocks() {

GetIndexBackwardsCompatibilityIT:
    public void testGetAliases() throws Exception {
    public void testGetMappings() throws Exception {
    public void testGetSettings() throws Exception {
    public void testGetWarmers() throws Exception {

NodesStatsBasicBackwardsCompatIT:
    public void testNodeStatsSetIndices() throws Exception {
    public void testNodeStatsSetRandom() throws Exception {

SignificantTermsBackwardCompatibilityIT:
    public void testAggregateAndCheckFromSeveralShards() throws IOException, ExecutionException, InterruptedException {

These will be tricky to have as REST tests but the [repository hdfs plugin](https://github.com/elastic/elasticsearch/tree/master/plugins/repository-hdfs/src/test/resources/rest-api-spec/test/hdfs_repository) has some snapshot/restore REST tests already that we can make use of:

SnapshotBackwardsCompatibilityIT
public void testBasicWorkflow() {
public void testSnapshotMoreThanOnce() throws Exception {

## We need a dedicated test that connects clients to a mixed version cluster to replace 

TransportClientBackwardsCompatibilityIT:
    public void testSniffMode() throws ExecutionException, InterruptedException {
UnicastBackwardsCompatibilityIT:
    public void testUnicastDiscovery() {

## We need dedicated stresstests for:

UpgradeIT:
    public void testUpgrade() throws Exception {

RecoveryBackwardsCompatibilityIT
    public void testReusePeerRecovery() throws Exception {

BasicBackwardsCompatibilityIT:
    public void testIndexUpgradeSingleNode() throws Exception {
    public void testIndexRollingUpgrade() throws Exception {
    public void testRecoverFromPreviousVersion() throws ExecutionException, InterruptedException {

For the latter we should also add a regular Integration tests that indexes while a whole index with replicas is moved around.

## I am unsure what to do with this one:

BasicAnalysisBackwardCompatibilityIT:
public void testAnalyzerTokensAfterUpgrade() throws IOException, ExecutionException, InterruptedException {
</comment><comment author="javanna" created="2016-01-21T11:04:43Z" id="173538922">I think we can get rid of `UnicastBackwardsCompatibilityIT`, it was meant to test unicast discovery when we didn't use it by default. If we create a mixed cluster we will use unicast which gets then tested indirectly. Other way around, not sure if we need to write some bw comp test for multicast at this point.

`TransportClientBackwardsCompatibilityIT` could be part of some different test infra for the java api (which would need more tests if we care). This specific test is about sniff mode, as we don't test sniffing elsewhere. If we move to a REST test infrastructure, we would also need a test for transport client without sniff, which is now tested indirectly by any of our bw comp tests as we use it to connect to the external cluster.
</comment><comment author="clintongormley" created="2016-04-29T16:17:05Z" id="215788148">Comment copied from #18065:

&gt; The 2.x branch has a bunch of backwards compatibility tests and we need to examine each one and decide if we need to port it to the master branch (and then 5.x branch, if we've cut such a branch before doing this issue.)
&gt; 
&gt; Many of them just want to run features against a mixed version cluster. We should get good enough coverage for those by running all the REST tests. Right now we run the core REST tests. I think all we have to do is run all the plugin REST tests for those tests.
&gt; 
&gt; Others assert that things work during a rolling restart. Those are going to be harder to port.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ClusterInfo in nodes' stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14405</link><project id="" key="" /><description>Currently the only way to diagnose/debug issues with the gathered cluster info (disk and shard sizes) is to turn on verbose logging for the `cluster` namespace.

We should expose this in the nodes stats API so people can see the amount of free/used disk that is measured by Elasticsearch.
</description><key id="114356910">14405</key><summary>Expose ClusterInfo in nodes' stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Cluster</label><label>enhancement</label></labels><created>2015-10-30T21:12:26Z</created><updated>2016-07-12T22:26:59Z</updated><resolved>2016-07-12T22:26:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-22T11:54:25Z" id="227721021">Are you still planning on exposing disk usage stats somewhere?
</comment><comment author="dakrone" created="2016-06-22T15:20:34Z" id="227778452">@clintongormley I'm on the fence about this, on one hand, it'd be nice to have whatever the calculated disk usage is available in the cluster state, on the other hand, I'm not sure it's worth creating a new cluster state every 30 seconds due to the disk changing every time. I wonder if this is less necessary now that the cluster allocation explain API will hopefully give more information about disk usage when a shard fails to be assigned.

What do you think? Is it worth trying to add this even though it will cause new cluster states to be sent every thirty seconds?
</comment><comment author="ywelsch" created="2016-06-22T18:43:17Z" id="227839258">@dakrone Why is exposing disk usage and shard size related to cluster state? `/_nodes/stats` just queries each node directly to gather stats?
</comment><comment author="dakrone" created="2016-06-22T19:37:16Z" id="227853405">@ywelsch this would be exposing the master node's _idea_ of the disk state by adding the `InternalClusterInfoService`'s `ClusterInfo` to the state, not the actual disk used (which you are right, could be gathered from the nodes directly)
</comment><comment author="ywelsch" created="2016-06-23T07:19:34Z" id="227969797">@dakrone If we're interested in what the master node thinks, we can use an action that fetches it from the master (`TransportMasterNodeAction`). Still doesn't require to add it to the cluster state ;-)
</comment><comment author="bleskes" created="2016-06-23T07:26:08Z" id="227971024">@ywelsch @dakrone I'm a bit confused - the ticket isn't about adding the disk usage to the cluster state but rather expose them through an API? I'm -1 on adding the to the cluster state internals - they have nothing to do with it and it will be indeed a shame to publish it for nothing.

Since we only use those for allocations, how about adding a flag/section to the allocation explain API to return the current cluster info in it's entirety?
</comment><comment author="dakrone" created="2016-06-23T15:12:33Z" id="228082132">&gt; I'm a bit confused - the ticket isn't about adding the disk usage to the cluster state but rather expose them through an API? I'm -1 on adding the to the cluster state internals

Sorry, there's some history here that wasn't communicated clearly, this comes from things like https://github.com/elastic/elasticsearch/pull/11643 where the cluster info was available to all the nodes as part of the state, which would allow them to make judgments about which path to put a shard on, or shard weight, etc. For what it's worth I'm -1 on adding it to the cluster state also (which is why that PR was closed and not merged).

&gt; Since we only use those for allocations, how about adding a flag/section to the allocation explain API to return the current cluster info in it's entirety?

This does sound reasonable to me and would be very easy to do, what do you think @ywelsch?
</comment><comment author="ywelsch" created="2016-06-23T15:29:48Z" id="228088053">&gt; &gt; Since we only use those for allocations, how about adding a flag/section to the allocation explain API to return the current cluster info in it's entirety?
&gt; 
&gt; This does sound reasonable to me and would be very easy to do, what do you think @ywelsch?

sounds good to me!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_field_stats returns date values as strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14404</link><project id="" key="" /><description>The `max_value` and `min_value` provided by the `/_field_stats` api currently provide strings values for date fields rather than milliseconds from epoch.

This feels like a bug to me, I expect them to resemble the results of a `max` or `min` aggregation:

`/_field_stats`

``` js
{
  // ...
  max_value: "2015-10-29T23:58:20.015Z"
  // ...
}
```

max aggregation:

``` js
{
  value: 1446163100015
  value_as_string: "2015-10-29T23:58:20.015Z"
}
```
</description><key id="114354993">14404</key><summary>/_field_stats returns date values as strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">spalger</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-10-30T21:00:04Z</created><updated>2015-11-23T08:02:24Z</updated><resolved>2015-11-23T08:02:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2015-10-30T21:03:20Z" id="152650578">This is really important for post request sorting. ISO8601 will sort fine, but users that have alternative formats may find their strings don't sort as expected
</comment><comment author="martijnvg" created="2015-10-31T09:37:09Z" id="152717910">@spalger @rashidkpc Right, the data format being returned depends on the configuration in the date field in the mapping, but that isn't good for sorting, so lets change the format slightly to also include the milliseconds from epoch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move logging for the amount of free disk to TRACE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14403</link><project id="" key="" /><description>Since this can potentially be logged for every `canRemain`, it's nicer
to put it in TRACE rather than DEBUG.

Resolves #12843
</description><key id="114354632">14403</key><summary>Move logging for the amount of free disk to TRACE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T20:57:37Z</created><updated>2016-02-29T16:37:52Z</updated><resolved>2015-10-30T21:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-30T21:01:23Z" id="152650204">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move RR gradle plugin files to match external repo paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14402</link><project id="" key="" /><description>The RR gradle plugin is at
https://github.com/randomizedtesting/gradle-randomized-testing-plugin.
However, we currently have a copy of this, since the plugin is still in
heavy development. This change moves the files around so they can be
copied directly from the elasticsearch fork to that repo, for ease of
syncing.
</description><key id="114349782">14402</key><summary>Move RR gradle plugin files to match external repo paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T20:25:34Z</created><updated>2015-10-30T22:20:11Z</updated><resolved>2015-10-30T22:20:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-30T22:18:49Z" id="152663425">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back -Dtests.output support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14401</link><project id="" key="" /><description>This change fixes the test logging for randomized testing to support
tests.output like maven/ant do.
</description><key id="114346714">14401</key><summary>Add back -Dtests.output support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T20:07:58Z</created><updated>2015-10-30T20:14:57Z</updated><resolved>2015-10-30T20:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-30T20:13:19Z" id="152640961">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException logged as WARN</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14400</link><project id="" key="" /><description>Test case:
- start a 1.7.2 node with `cluster.name: es172`
- then start a 2.0.0 node on the same machine with `cluster.name: es200`

The 2.0 node will start properly and work properly but during startup a NullPointerException is logged:  [es200.txt](https://github.com/elastic/elasticsearch/files/24532/es200.txt)
</description><key id="114341329">14400</key><summary>NullPointerException logged as WARN</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>:Discovery</label><label>:Network</label><label>adoptme</label><label>bug</label></labels><created>2015-10-30T19:38:09Z</created><updated>2017-03-17T23:41:21Z</updated><resolved>2017-03-17T23:41:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-30T20:21:57Z" id="152642565">This does reproduce (with 1.7.3 and 2.0.0)
</comment><comment author="majormoses" created="2015-11-03T19:50:17Z" id="153468230">I saw this when I tried an upgrade a dev cluster. I no longer have that cluster around though so I cant provide any more insight other than I was able to see the same issue.
</comment><comment author="krisb78" created="2015-11-09T14:57:50Z" id="155086318">In my case (as described under https://github.com/elastic/elasticsearch/issues/13676), the new client node upgraded to 2.0.0 fails to join the 1.7.2 cluster (i.e, I'm not trying to run two nodes on the same machine):

Im getting a number of these:

```
[2015-11-09 14:46:01,505][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0x449bf305, /10.0.1.231:40601 =&gt; /10.0.1.237:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

...

And then eventually:

```
[2015-11-09 14:46:01,525][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0xc143077d, /10.0.1.231:46567 =&gt; /10.0.1.251:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-09 14:46:03,526][DEBUG][action.admin.cluster.health] [cubitsearch-client-1] no known master node, scheduling a retry
```
</comment><comment author="dakrone" created="2015-11-09T16:16:25Z" id="155111398">&gt; the new client node upgraded to 2.0.0 fails to join the 1.7.2 cluster

This is expected, 2.x and 1.x nodes are not compatible, you'll need to do a full cluster upgrade to use 2.0.

This issue is the NullPointerException logged when the two try to communicate.
</comment><comment author="jasontedor" created="2017-03-17T23:41:21Z" id="287497121">This does not occur between 2.x and 5.x, and I do not think we need to worry about this between 1.x and 2.x as 1.x is out of maintenance mode, and 2.x is in maintenance mode for critical bug fixes and this not a critical bug.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increased load and later es crash after update to 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14399</link><project id="" key="" /><description>I have 3 data nodes on Ubuntu 14.04 (KVM), Oracle Java 8, 2 Cores and 8 GB of RAM. 
2 Replicas. About 20 million documents. Heap size 4 GB.

After updating from 1.7.3 to 2.0 my load increases from around 0.5 to 3 - 4.

In the beginning it was even worse because of relocation but now it should more or less idle but the load is much bigger as with 1.7.3
</description><key id="114340320">14399</key><summary>Increased load and later es crash after update to 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monotek</reporter><labels /><created>2015-10-30T19:34:06Z</created><updated>2015-10-31T19:47:03Z</updated><resolved>2015-10-31T19:33:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="monotek" created="2015-10-31T11:47:04Z" id="152726558">After some time one of the nodes has even higher load (&gt; 20), the other 2 nodes get load 0.1 and elasticsearch does not work anymore. If i try to stop elasticsearch on the node with the high load, it becomeas a zombie.

Syslog for the node with the high load says:

Oct 30 21:13:03 es1 kernel: [22200.884124] INFO: task jbd2/vdb1-8:307 blocked for more than 120 seconds.
Oct 30 21:13:03 es1 kernel: [22200.892344]       Not tainted 3.13.0-66-generic #108-Ubuntu
Oct 30 21:13:03 es1 kernel: [22200.898723] "echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs" disables this message.
Oct 30 21:13:03 es1 kernel: [22200.904917] jbd2/vdb1-8     D ffff88023fd13180     0   307      2 0x00000000
Oct 30 21:13:03 es1 kernel: [22200.904922]  ffff8802322bdcb0 0000000000000046 ffff8802322ab000 ffff8802322bdfd8
Oct 30 21:13:03 es1 kernel: [22200.904925]  0000000000013180 0000000000013180 ffff8802322ab000 ffff8802322bdd98
Oct 30 21:13:03 es1 kernel: [22200.904931]  ffff8800370480b8 ffff8802322ab000 ffff88013eef2800 ffff8802322bdd80
Oct 30 21:13:03 es1 kernel: [22200.904934] Call Trace:
Oct 30 21:13:03 es1 kernel: [22200.904969]  [&lt;ffffffff81728499&gt;] schedule+0x29/0x70
Oct 30 21:13:03 es1 kernel: [22200.9ies completly and the oth04988]  [&lt;ffffffff8128ac67&gt;] jbd2_journal_commit_transaction+0x287/0x1ab0
Oct 30 21:13:03 es1 kernel: [22200.905001]  [&lt;ffffffff810a3234&gt;] ? update_curr+0xe4/0x180
Oct 30 21:13:03 es1 kernel: [22200.905007]  [&lt;ffffffff810ab390&gt;] ? prepare_to_wait_event+0x100/0x100
Oct 30 21:13:03 es1 kernel: [22200.905014]  [&lt;ffffffff8107484b&gt;] ? lock_timer_base.isra.35+0x2b/0x50
Oct 30 21:13:03 es1 kernel: [22200.905017]  [&lt;ffffffff810756ff&gt;] ? try_to_del_timer_sync+0x4f/0x70
Oct 30 21:13:03 es1 kernel: [22200.905020]  [&lt;ffffffff812905fd&gt;] kjournald2+0xbd/0x250
Oct 30 21:13:03 es1 kernel: [22200.905023]  [&lt;ffffffff810ab390&gt;] ? prepare_to_wait_event+0x100/0x100
Oct 30 21:13:03 es1 kernel: [22200.905025]  [&lt;ffffffff81290540&gt;] ? commit_timeout+0x10/0x10
Oct 30 21:13:03 es1 kernel: [22200.905033]  [&lt;ffffffff8108b7d2&gt;] kthread+0xd2/0xf0
Oct 30 21:13:03 es1 kernel: [22200.905036]  [&lt;ffffffff8108b700&gt;] ? kthread_create_on_node+0x1c0/0x1c0
Oct 30 21:13:03 es1 kernel: [22200.905041]  [&lt;ffffffff81734ba8&gt;] ret_from_fork+0x58/0x90
Oct 30 21:13:03 es1 kernel: [22200.905044]  [&lt;ffffffff8108b700&gt;] ? kthread_create_on_node+0x1c0/0x1c0

Nothing for this time in elasticsearch.log

The elasticsearch log of the other nodes say things like:

[2015-10-31 03:50:02,299][INFO ][rest.suppressed          ] //otrs-2015.10/tickets/19088107 Params: {index=otrs-2015.10, id=19088107, type=tickets}
RemoteTransportException[[es1][192.168.10.61:9300][indices:data/write/index]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.action.support.replication.TransportRep
licationAction$PrimaryPhase$1@134e54bc on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1205638d[Running, pool size = 2, active t
hreads = 2, queued tasks = 200, completed tasks = 42298]]];
Caused by: EsRejectedExecutionException[rejected execution of org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1@134e54bc on EsThreadPoolExecutor[index, queue 
capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@1205638d[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 42298]]]

[2015-10-31 12:51:03,567][WARN ][discovery.zen.ping.unicast] [es3] [12] failed send ping to {es1}{egOv1fGiRvmxV8BrvXfpnQ}{192.168.10.61}{192.168.10.61:9300}{max_local_storage_nodes=1}
java.lang.IllegalStateException: can't add nodes to a stopped transport
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:833)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:828)
        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:243)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:379)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</comment><comment author="bleskes" created="2015-10-31T15:49:46Z" id="152744759">In 2.0 we change the default behavior of the translog to fsync on every operation. Before it was fsync-ing every 5 seconds (by default), in order to strike a good balance between performance and durability (i.e., data loss when all shard copies loose power). We recently measured the impact fsync has on bulk operation and have discovered that it was greatly reduced in the years since the 5 second decision was made. As such, we decided to increase safety change the default. Note that we do this smartly and try to share fsyncs among as many operations as we can.

From your rejection exception I see you are using the single doc indexing operation, which means that the fsync cost is not well amortized. I suggest you move to the _bulk api if possible. If not you can disable the new behavior by setting `index.translog.durability` to `async` (see https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html#_translog_settings  )
</comment><comment author="monotek" created="2015-10-31T19:33:37Z" id="152765742">thanks for the hint. i will have a look into bulk indexing of the php library i use.

i also had some help from the forum:
https://discuss.elastic.co/t/elasticsearch-crashes-after-update-to-2-0/33437/8

indeed it seems that it was an io problem.

my path.data dir was already an own partition which mounts a glusterfs volume via libgfapi to prevent context switches of fuse mounts. it looks like this was still not fast enough for the new fsync implementation in 2.0.

the mount options "data=writeback,relatime,nobarrier" took a lot of io pressure from the system now.
load seems to be normal again.

closing this bug for now...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update to lucene-5.4.x-snapshot-1711508</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14398</link><project id="" key="" /><description>`gradle clean check` passes after removing local maven repo
</description><key id="114337054">14398</key><summary>update to lucene-5.4.x-snapshot-1711508</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T19:16:13Z</created><updated>2015-11-08T21:43:06Z</updated><resolved>2015-10-30T19:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-30T19:21:14Z" id="152623558">LGTM, just one tiny comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script_field returns different array value in the 2.0 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14397</link><project id="" key="" /><description>This behaviour is reproducible in v1.7.3 and 2.0.0.
Request:
    GET /my_index/object/_search
    {
      "script_fields": {
        "last_updates": {
          "script": "list=[];list.push(key:'test');list"
        }
      }
    }

Response:
1.7.3
  "fields": { "last_updates": [[ {"key": "test"}]]}
2.0
  "fields": {"last_updates": [ {"key": "test"}]}
</description><key id="114315578">14397</key><summary>Script_field returns different array value in the 2.0 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mediaon</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-10-30T17:20:34Z</created><updated>2016-01-29T20:24:27Z</updated><resolved>2016-01-29T20:24:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T21:34:18Z" id="154876058">This looks to me like something that changed in Groovy, but leaving it open for further comments.
</comment><comment author="clintongormley" created="2016-01-29T20:24:27Z" id="176952923">No further comments. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update mapping PUT should use /_mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14396</link><project id="" key="" /><description>Instead of /mapping
</description><key id="114315073">14396</key><summary>Update mapping PUT should use /_mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bradvido</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T17:17:48Z</created><updated>2015-10-30T17:58:40Z</updated><resolved>2015-10-30T17:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-30T17:58:38Z" id="152602910">Cherry-picked this to master, 2.x, 2.1, and 2.0, thanks @bradvido!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Find file containing the keyword</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14395</link><project id="" key="" /><description>Is there any module in elasticsearch to find the file containing the search keyword and get the contents of the file with the keyword
</description><key id="114314047">14395</key><summary>Find file containing the keyword</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jst2cu</reporter><labels /><created>2015-10-30T17:12:51Z</created><updated>2015-10-30T17:26:46Z</updated><resolved>2015-10-30T17:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ksujeet" created="2015-10-30T17:16:51Z" id="152591560">Take a look at this, 
https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-attachment-type.html
https://github.com/elastic/elasticsearch-mapper-attachments
</comment><comment author="dadoonet" created="2015-10-30T17:26:46Z" id="152593946">+1 and ask questions on discuss.elastic.co
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES v1.7.1 Returning almost similar results on every page </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14394</link><project id="" key="" /><description>```
GET _search
{
  "from": 0,
  "size": 10,
  "query": {  
        "term": {
          "artifactName": {
            "value": "rosa parks"
          }
        }
  }
}
```

This a sample query which I am trying to run on ES and Its running well but when I select the from as 1 the results of this are similar to what I got the previous set the only exception is that the first result is excluded.

Example of my issue:
Page    Results
0          1-10
2          2-11 (Expecting 11-20)

Could you please point me in the direction where I am going wrong?
@kimchy , @s1monw , @elasticdog 
</description><key id="114307355">14394</key><summary>ES v1.7.1 Returning almost similar results on every page </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ksujeet</reporter><labels /><created>2015-10-30T16:38:10Z</created><updated>2015-10-30T17:58:57Z</updated><resolved>2015-10-30T17:58:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[DOCS] document replacement for search exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14393</link><project id="" key="" /><description>Search exists api was removed from master and deprecated in 2.x. With this PR we document how the same can be achieved using the search api.

Relates to #13910 
</description><key id="114306477">14393</key><summary>[DOCS] document replacement for search exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">javanna</reporter><labels><label>docs</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T16:33:54Z</created><updated>2015-11-09T14:07:46Z</updated><resolved>2015-11-09T14:07:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-30T16:34:29Z" id="152580052">@clintongormley can you have a look please? also, I am not sure where this new section should go, probably not where I added it :)
</comment><comment author="javanna" created="2015-10-31T08:16:39Z" id="152712785">@jpountz pushed a new commit, thanks for having a look.
</comment><comment author="clintongormley" created="2015-11-08T22:01:31Z" id="154879126">Yeah, its current position feels too important.  How about adding a section called `terminate_after` in the request body section, and putting the docs on there (along with the docs for terminate after that are currently on https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html)
</comment><comment author="javanna" created="2015-11-09T13:21:52Z" id="155059666">@clintongormley I pushed a new commit, can you double check please?
</comment><comment author="clintongormley" created="2015-11-09T13:54:32Z" id="155068355">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing gradle test-resources for plugin metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14392</link><project id="" key="" /><description>About half plugins' tests won't work from IDEs at the moment, because the plugin metadata is not configured as a test resource.

In maven we created a separate test resource folder for plugin metadata:
https://github.com/elastic/elasticsearch/blob/2.x/plugins/pom.xml#L253-L257

Then we copied two resources to it:
- plugin descriptor file (with resource filtering populating it)
- plugin security policy (if it exists)

https://github.com/elastic/elasticsearch/blob/2.x/plugins/pom.xml#L266-L284

At least with maven, the copy would not be triggered if you did it 'the way that makes sense', which is in process-resources, so it ran at an awkward time in generate-resources, which all the IDE tasks invokved.
</description><key id="114304895">14392</key><summary>Add missing gradle test-resources for plugin metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-30T16:27:09Z</created><updated>2015-11-02T19:38:25Z</updated><resolved>2015-11-02T19:38:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add community plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14391</link><project id="" key="" /><description /><key id="114299439">14391</key><summary>Add community plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jurgc11</reporter><labels><label>docs</label></labels><created>2015-10-30T16:05:10Z</created><updated>2015-11-08T21:12:26Z</updated><resolved>2015-11-08T21:12:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Filter path refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14390</link><project id="" key="" /><description>Jackson has been updated to version 2.6 in #13344, we can now reimplement the `filter_path` feature using the new Jackson streaming support.

No more custom hand-made logic (made in #10980), we can now implement a Jackson's `TokenFilter` and have a more readable and more maintainable code.

Bonus points:
- less code
- better performance
- closes #11560 since JSON fields that contains dots (like `.marvel-es-XX`) are now correctly filtered
- allows to filter `_source` in search response hits / get document response / multi get document response
</description><key id="114294914">14390</key><summary>Filter path refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:REST</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T15:42:52Z</created><updated>2015-12-11T13:18:28Z</updated><resolved>2015-11-16T16:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-08T21:11:27Z" id="154872668">@tlrx note that fields can no longer contain embedded dots
</comment><comment author="tlrx" created="2015-11-09T08:06:04Z" id="154992417">@clintongormley Yes I know, but indices still can have dot in names and the response filtering must be able to filter them correctly
</comment><comment author="danielmitterdorfer" created="2015-11-09T17:39:12Z" id="155135683">I left a few comments but apart from that LGTM. I am not sure whether somebody else wants to check so I am leaving the 'review' label for now. @tlrx Feel free to update it.
</comment><comment author="tlrx" created="2015-11-10T20:38:42Z" id="155558891">@danielmitterdorfer Thanks for your review! I updated the code according to your comments about static/final. I'd like to keep the null check for now if that's fine for you.

I'll look for someone else to review it.
</comment><comment author="jpountz" created="2015-11-16T08:59:34Z" id="156959385">I left some comments but I like the change in general, eg. how it handles raw fields.
</comment><comment author="tlrx" created="2015-11-16T10:24:17Z" id="156980731">@jpountz I updated the code following your comments, can you give it another look please?
</comment><comment author="jpountz" created="2015-11-16T15:59:33Z" id="157078633">LGTM
</comment><comment author="tlrx" created="2015-11-16T16:30:41Z" id="157089086">@jpountz thanks!
</comment><comment author="reinaldoluckman" created="2015-12-07T17:15:30Z" id="162597035">At this point in ES 2.1.0, there is some way to make a search like

http://localhost:9200/twitter/tweet/_search?q=message:elastic

And receive an array of _source's without any metadata like

[{ 
   "user" : "yoyo", 
   "postDate" : "2009-11-15T14:12:12", 
   "message" : "Elastic is fun" 
}, { 
   "user" : "bulgogi", 
   "postDate" : "2009-11-15T14:12:12", 
   "message" : "Elastic Search is cool" 
}, { 
   "user" : "kimchy", 
   "postDate" : "2009-11-15T14:12:12", 
   "message" : "trying out Elastic Search" 
}]
</comment><comment author="tlrx" created="2015-12-07T19:33:05Z" id="162634594">For now, a `GET /_search?filter_path=hits.hits._source` will return only the _source of your document, containing all the fields and without metadata fields like `_id`, `_index` etc. Note that the structure of the JSON will be unchanged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get eclipse working easier with gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14389</link><project id="" key="" /><description>This prevents recursive dependencies between 'core' and 'test-framework' and updates the docs.
</description><key id="114294511">14389</key><summary>Get eclipse working easier with gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-30T15:40:36Z</created><updated>2015-10-30T18:32:31Z</updated><resolved>2015-10-30T18:32:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-30T15:41:38Z" id="152559904">+1
</comment><comment author="s1monw" created="2015-10-30T15:43:33Z" id="152560409">that's awesome for all eclipse users! small fix but I assume that took a while to figure out :)
</comment><comment author="rjernst" created="2015-10-30T15:50:50Z" id="152563194">lgtm
</comment><comment author="rmuir" created="2015-10-30T15:51:24Z" id="152563406">Well, its still rough and generates some broken projects:
- smoke-test-client
- smoke-test-plugins

I think ideally we would just inhibit those projects (and probably distribution/ projects like `tar`) from being added to IDE configurations in general? So try to ignore those.

Tests also work from elasticsearch core and plugins (some)

Note that there are problems with plugins with a security.policy, but thats also not unique to eclipse and we just need to fix the resource filtering in general there for any IDE.
</comment><comment author="s1monw" created="2015-10-30T15:53:11Z" id="152563845">what I care usually is to run simple unittests in the IDE for fast iteration I think those compromises are ok.
</comment><comment author="jpountz" created="2015-10-30T16:00:21Z" id="152565785">I don't have the gradle plugin, so I tested your patch by running "gradle -Declipse eclipse" and then I could import the project into eclipse without the circular dependency error. I'm wondering if we could automatically add this property if someone runs "gradle eclipse"?
</comment><comment author="rjernst" created="2015-10-30T16:10:58Z" id="152568263">ok, i think we can try a hack to remove the prop. Try using this:

```
if (gradle.startParameter.taskNames.contains('eclipse') == false) {
```
</comment><comment author="ywelsch" created="2015-10-30T16:14:13Z" id="152569793">That's very syntactic, I would prefer something along the lines of `gradle.taskGraph.whenReady { gradle.taskGraph.hasTask(...) }`
</comment><comment author="jpountz" created="2015-10-30T16:15:59Z" id="152570291">@rjernst it worked when generating files from the command line using 'gradle eclipse'
</comment><comment author="rjernst" created="2015-10-30T16:16:03Z" id="152570309">@ywelsch i dont think that works? by the time the task graph is ready, i think it is too late to add dependency substitutions?
</comment><comment author="ywelsch" created="2015-10-30T16:40:24Z" id="152581416">@rjernst I tried it (because I was in doubt suddenly), and it worked

```
// hack so that eclipse doesn't have circular references
// the downside is, if you hack on test-framework, you have to gradle install
subprojects {
  project.gradle.taskGraph.whenReady { taskGraph -&gt;
    configurations {
      all {
        resolutionStrategy {
          dependencySubstitution {
            if (taskGraph.allTasks.any { it.name == 'eclipseClasspath' }) {
              substitute module("org.elasticsearch:test-framework:${version}") with project("${projectsPrefix}:test-framework")
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="rjernst" created="2015-10-30T16:42:55Z" id="152582094">@ywelsch I tried it as well, and it "works", but it will not for long:

```
Changed strategy of configuration ':core:compile' after task dependencies have been resolved. This behaviour has been deprecated and is scheduled to be removed in Gradle 3.0
```
</comment><comment author="rjernst" created="2015-10-30T16:43:54Z" id="152582558">I don't think we should worry about the task graph here, we certainly wont' be adding something that depends on the eclipse task (which is the only reason the task graph would be better than the listed tasks).
</comment><comment author="rmuir" created="2015-10-30T17:03:17Z" id="152587624">@rjernst the main thing i see is, that logic does not work when invoked from eclipse itself... But the sysprop always works consistently from the command line or the IDE. I think its a good step for now?
</comment><comment author="rmuir" created="2015-10-30T17:11:08Z" id="152589376">`gradle.startParameter.taskNames` is empty when called via eclipse, that's why that logic doesnt work.
</comment><comment author="rmuir" created="2015-10-30T18:22:04Z" id="152608335">I removed the need to specify a sysprop. eclipse itself sets a ton, its easy to detect.
</comment><comment author="jpountz" created="2015-10-30T18:26:31Z" id="152609334">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore the mapping transform docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14388</link><project id="" key="" /><description>Mapping transform was deprecated in 2.0.0 but the docs were removed. This
adds them back with deprecation warnings.

Relates to #13657
</description><key id="114294252">14388</key><summary>Restore the mapping transform docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-30T15:39:10Z</created><updated>2015-10-30T16:40:29Z</updated><resolved>2015-10-30T16:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-30T15:42:10Z" id="152560042">LGTM
</comment><comment author="nik9000" created="2015-10-30T16:35:38Z" id="152580294">@adichad - this covers what you asked for in https://github.com/elastic/elasticsearch/pull/13657#issuecomment-152444861 . Thanks for calling us out on this.
</comment><comment author="nik9000" created="2015-10-30T16:40:29Z" id="152581434">Merged to 2.0 branch and cherry-picked to 2.1 and 2.x branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards relocating during rolling restarts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14387</link><project id="" key="" /><description>This behaviour is reproducible in v1.6.0 through 2.0.0.  

Expected during rolling restarts that no shard relocations will occur, however there is shard movement occurring while the cluster is in a yellow health state.  

Steps to reproduce:
1. Create a cluster with at least 3 nodes, 1 index with 2 shards + 1 replica (4 shards total), and index some data.
2. Stop all indexing
3. Set allocation: none
4. _all/_flush/synced
5. restart a single node
6. reenable allocation

At step 6, shards are observed to be relocating, in addition to any recovery by sync_id that has occurred.  After recoveries and relocations, the cluster will change to green state.  This was tested in slow motion by limiting bandwidth to one of the nodes in the cluster.

Relocations are **not** observed in a 2 node cluster, or when restarting the entire cluster.
</description><key id="114284824">14387</key><summary>Shards relocating during rolling restarts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Allocation</label><label>v1.7.4</label><label>v2.1.0</label></labels><created>2015-10-30T14:52:00Z</created><updated>2016-10-27T13:05:41Z</updated><resolved>2015-11-10T11:06:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-05T15:01:48Z" id="154085273">Hi @PhaedrusTheGreek 

Could you add the exact commands etc that you used to test.  I'm on a poor network and can't view the video.

thanks
</comment><comment author="PhaedrusTheGreek" created="2015-11-05T15:36:44Z" id="154095820">The relocating shards seems to be recoveries, not rebalances.   I infer this because when i set the following, I see them all happening at once.  

```
"cluster.routing.allocation.node_concurrent_recoveries" : 10
```

This is what i'm seeing after restarting a node - shards moving on and off.

```
index shard prirep state      docs   store ip           node                                                
big   0     r      RELOCATING 1026 605.7kb 192.168.0.2  Max -&gt; 192.168.0.25 gUii9aw4QTW_CRP4Akg_Nw Scrier   
big   0     p      STARTED    1026 605.7kb 192.168.0.25 Arclight                                            
big   1     r      RELOCATING 1018 854.4kb 192.168.0.2  Max -&gt; 192.168.0.25 8y8i2N0oQpag6zsVPm323g Arclight 
big   1     p      STARTED    1018 854.4kb 192.168.0.25 Scrier                                              
big2  2     r      STARTED     413 278.4kb 192.168.0.25 Arclight                                            
big2  2     p      RELOCATING  413 278.4kb 192.168.0.25 Scrier -&gt; 192.168.0.2 m3eFHPt0QyqEdiXUnk59Yg Max    
big2  0     r      RELOCATING  405 270.6kb 192.168.0.2  Max -&gt; 192.168.0.25 8y8i2N0oQpag6zsVPm323g Arclight 
big2  0     p      STARTED     405 270.6kb 192.168.0.25 Scrier                                              
big2  3     p      RELOCATING  405   269kb 192.168.0.25 Arclight -&gt; 192.168.0.2 m3eFHPt0QyqEdiXUnk59Yg Max  
big2  3     r      STARTED     405   269kb 192.168.0.25 Scrier                                              
big2  1     r      RELOCATING  410 426.8kb 192.168.0.2  Max -&gt; 192.168.0.25 gUii9aw4QTW_CRP4Akg_Nw Scrier   
big2  1     p      STARTED     410 426.8kb 192.168.0.25 Arclight                                            
big2  4     p      RELOCATING  411 443.7kb 192.168.0.25 Arclight -&gt; 192.168.0.2 m3eFHPt0QyqEdiXUnk59Yg Max  
big2  4     r      STARTED     411 443.6kb 192.168.0.25 Scrier                                              
big3  2     r      STARTED     407 344.6kb 192.168.0.2  Max                                                 
big3  2     p      STARTED     407 344.6kb 192.168.0.25 Scrier                                              
big3  0     r      STARTED     406 319.2kb 192.168.0.25 Arclight                                            
big3  0     p      RELOCATING  406 319.3kb 192.168.0.25 Scrier -&gt; 192.168.0.2 m3eFHPt0QyqEdiXUnk59Yg Max    
big3  3     r      STARTED     413 402.1kb 192.168.0.2  Max                                                 
big3  3     p      STARTED     413 402.1kb 192.168.0.25 Scrier                                              
big3  1     r      STARTED     411 276.8kb 192.168.0.2  Max                                                 
big3  1     p      STARTED     411 276.8kb 192.168.0.25 Arclight                                            
big3  4     r      STARTED     407 342.5kb 192.168.0.2  Max                                                 
big3  4     p      STARTED     407 342.5kb 192.168.0.25 Arclight                    
```

TRACE logs show a lot of this:

```
[2015-11-05 10:22:00,938][TRACE][indices.recovery         ] [Max] [big3][0] recovery completed from [Scrier][gUii9aw4QTW_CRP4Akg_Nw][Jasons-MacBook-Pro-3.local][inet[/192.168.0.25:9300]], took[2.6m]
   phase1: recovered_files [7] with total_size of [319.2kb], took [2.5m], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [19ms]
         : recovered [0] transaction log operations, took [0s]
   phase3: recovered [0] transaction log operations, took [1ms]
```
</comment><comment author="PhaedrusTheGreek" created="2015-11-05T15:41:19Z" id="154097169">As for the exact command for testing, all that i am doing is starting up 3 nodes, and restarting one with 

```
CTRL-C; bin/elasticsearch
```

Then watching things move around with

```
GET /_cat/shards?v
```
</comment><comment author="s1monw" created="2015-11-06T10:49:06Z" id="154377367">I assigned it to @ywelsch we will look into this and come back to you shortly. In the meanwhile can you show all the commands you are executing especially the one that: `Set allocation: none`
</comment><comment author="PhaedrusTheGreek" created="2015-11-06T15:16:21Z" id="154434838">This is the exact command I used:

```
PUT /_cluster/settings
{
        "persistent" : {
            "cluster.routing.allocation.enable" : "none"
        }
}
```

And I would see something like this on all nodes:

```
[2015-10-30 10:54:51,429][INFO ][cluster.routing.allocation.decider] [Humus Sapien] updating [cluster.routing.allocation.enable] from [ALL] to [NONE]
```

Shard relocations / recoveries begin after relocation is reenabled like this:

```
PUT /_cluster/settings
{
        "persistent" : {
            "cluster.routing.allocation.enable" : "all"
        }
}
```
</comment><comment author="bleskes" created="2015-11-06T15:28:46Z" id="154438130">A short update - @clintongormley and I researched this. It has to do with a race condition between the gateway allocator and the cluster balancer. When the node comes back/allocation is enabled the gateway allocator goes and asks the node for information about it's shard store. This is done async. While that request is in flight, the balanced allocator thinks the node is empty and assigns shards to it. Only later when the gateway allocator assigns the missing shard back to node does the cluster rebalances again. Our idea for a fix was to disable balancing while there are in flight data fetching requests... 
</comment><comment author="s1monw" created="2015-11-06T18:57:21Z" id="154501103">@bleskes makes sense to me - I will take a look at implementing this.
</comment><comment author="PhaedrusTheGreek" created="2015-11-06T19:21:55Z" id="154507784">Tested these workarounds with good results:

1.x

```
 "cluster.routing.allocation.balance.threshold" : "100.0f" (During Node Restart)
 "cluster.routing.allocation.balance.threshold" : "1.0f" (Return to Default)
```

2.0

```
"cluster.routing.rebalance.enable" : "none" (During Node Restart)
"cluster.routing.rebalance.enable" : "all" (Return to Default)
```
</comment><comment author="astefan" created="2015-11-10T07:49:45Z" id="155348627">Was this present in ES versions before 1.6?
</comment><comment author="s1monw" created="2015-11-10T08:09:11Z" id="155353293">&gt; Was this present in ES versions before 1.6?

no I don't think so since back then we fetched data synchronously so this couldn't happen.
</comment><comment author="bittusarkar" created="2016-10-27T11:04:16Z" id="256611792">@s1monw Is this issue fixed in Elasticsearch 2.x?
</comment><comment author="s1monw" created="2016-10-27T11:14:50Z" id="256613656">@bittusarkar yes see #14652
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for Lucene 5.4 GeoPoint queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14386</link><project id="" key="" /><description>This is GeoPointV2 PR part 3. It adds query support for the new Lucene 5.4 GeoPointField type along with backward compatibility support for (soon to be) "legacy" geo_point indexes.
</description><key id="114281232">14386</key><summary>Add support for Lucene 5.4 GeoPoint queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Query Refactoring</label><label>review</label></labels><created>2015-10-30T14:33:00Z</created><updated>2015-11-08T22:57:40Z</updated><resolved>2015-11-04T22:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-04T12:18:58Z" id="153702833">I am missing a tests on this one and I think it should really have one. In master this is much simpler and maybe the entire PR should go first in to master?
</comment><comment author="nknize" created="2015-11-04T22:53:50Z" id="153895551">closing in favor of #14537 /cc @s1monw I've addressed bwc, norelease, and tests in the new PR. Its ready for next round of review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add duration field to /_cat/snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14385</link><project id="" key="" /><description>Follows suggestion from https://github.com/elastic/elasticsearch/pull/14247/files#r43038922 to add duration field to `/_cat/snapshots` output.
</description><key id="114280099">14385</key><summary>Add duration field to /_cat/snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T14:26:32Z</created><updated>2015-11-14T19:56:48Z</updated><resolved>2015-11-04T09:34:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-10-30T14:27:23Z" id="152540476">@dakrone you reviewed the previous PR, can you have a look at this one as well?
</comment><comment author="dakrone" created="2015-10-30T15:33:12Z" id="152557394">LGTM, thanks @ywelsch 
</comment><comment author="ywelsch" created="2015-11-02T12:56:16Z" id="153009468">@drewr Does this look good to you?
</comment><comment author="drewr" created="2015-11-14T19:56:48Z" id="156742446">LGTM thanks! :+1:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate query api: move query parsing to the coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14384</link><project id="" key="" /><description> Similarly to what we did with the search api and the explain api we can now also move query parsing on the coordinating node for the validate query api. This makes it more realistic as query parsing happens now in the coordinating node rather than on the data nodes. 

As a result, the java api only accepts the query in `QueryBuilder` format rather than `String` or bytes array. This is because the java api works now only with query objects that are already parsed. Note that if a query needs to be provided in string format through the java api this can be done using `WrapperQueryBuilder` but in that case the parsing of the wrapped query will happen on each data node instead.

The response body returned in case the query parsing phase fails is slightly different compared to before: it will not contain any reference to an index nor shards header, as parsing is now independent from indices/shards.
</description><key id="114273374">14384</key><summary>Validate query api: move query parsing to the coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T13:49:31Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-11-02T10:46:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-02T10:09:31Z" id="152976618">@s1monw can you have a look please?
</comment><comment author="s1monw" created="2015-11-02T10:18:15Z" id="152978044">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrading fields with only `search_analyzer` specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14383</link><project id="" key="" /><description>When upgrading from 1.x to 2.0, a field that has only a `search_analyzer` specified will throw an exception and prevent the node from starting.  Really, the field does have an `index_analyzer`, set to `default`.  We should be able to migrate this mapping to a 2.0 compatible version.

Relates to #14313
</description><key id="114261027">14383</key><summary>Upgrading fields with only `search_analyzer` specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label><label>low hanging fruit</label><label>v2.0.1</label></labels><created>2015-10-30T12:32:21Z</created><updated>2015-11-18T14:05:42Z</updated><resolved>2015-11-18T14:05:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-18T14:05:42Z" id="157722602">Closed by https://github.com/elastic/elasticsearch/pull/14677
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High heap memory consumption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14382</link><project id="" key="" /><description>Need help with memory consumption. 

This is our cluster:
![2015-10-30 17-00-36 kopf elasticsearch](https://cloud.githubusercontent.com/assets/146705/10845214/c656fa96-7f27-11e5-834f-65e440a94415.png)

On start (about 3 or more hours) we have green line (heap memory usage). After two or more houre we can see red line (heap memory usage)
![2015-10-30 17-02-58](https://cloud.githubusercontent.com/assets/146705/10845270/45a76b46-7f28-11e5-8965-099fbdc7d664.png)

After that node crased: 
GC prints messages in logs like this.

```
[2015-10-28 20:37:42,134][WARN ][monitor.jvm              ] [process8] [gc][old][1312][37] duration [21s], collections [1]/[21.7s], total [21s]/[7.3m], memory [19.4gb]-&gt;[18.1gb]/[19.9gb], all_pools {[young] [231.3mb]-&gt;[114.6mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [19.1gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:38:07,962][WARN ][monitor.jvm              ] [process8] [gc][old][1315][38] duration [23.2s], collections [1]/[23.7s], total [23.2s]/[7.7m], memory [19.2gb]-&gt;[18gb]/[19.9gb], all_pools {[young] [8.5mb]-&gt;[16.1mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [19.1gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:38:34,725][WARN ][monitor.jvm              ] [process8] [gc][old][1322][39] duration [20.6s], collections [1]/[20.7s], total [20.6s]/[8m], memory [19.7gb]-&gt;[18.1gb]/[19.9gb], all_pools {[young] [479.9mb]-&gt;[60.6mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [19.2gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:39:00,554][WARN ][monitor.jvm              ] [process8] [gc][old][1325][40] duration [23.1s], collections [1]/[23.8s], total [23.1s]/[8.4m], memory [19.4gb]-&gt;[18.1gb]/[19.9gb], all_pools {[young] [491.6mb]-&gt;[87.7mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [18.9gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:39:25,743][WARN ][monitor.jvm              ] [process8] [gc][old][1333][41] duration [18s], collections [1]/[18.1s], total [18s]/[8.7m], memory [19.5gb]-&gt;[18.1gb]/[19.9gb], all_pools {[young] [461.7mb]-&gt;[49.5mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [19gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:39:52,107][WARN ][monitor.jvm              ] [process8] [gc][old][1343][42] duration [16.9s], collections [1]/[17.3s], total [16.9s]/[9m], memory [19.4gb]-&gt;[18.1gb]/[19.9gb], all_pools {[young] [429.8mb]-&gt;[65.4mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [18.9gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:40:19,758][WARN ][monitor.jvm              ] [process8] [gc][old][1349][43] duration [22.3s], collections [1]/[22.6s], total [22.3s]/[9.4m], memory [19.2gb]-&gt;[18.1gb]/[19.9gb], all_pools {[young] [265.9mb]-&gt;[87.5mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [18.9gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 20:40:44,749][WARN ][monitor.jvm              ] [process8] [gc][old][1352][44] duration [22.7s], collections [1]/[22.9s], total [22.7s]/[9.7m], memory [19.2gb]-&gt;[18gb]/[19.9gb], all_pools {[young] [284.5mb]-&gt;[5.2mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [18.9gb]-&gt;[18gb]/[19.3gb]}
[2015-10-28 21:57:08,889][INFO ][monitor.jvm              ] [process8] [gc][old][5920][279] duration [15.1s], collections [2]/[15.7s], total [15.1s]/[10.3m], memory [19.3gb]-&gt;[18.5gb]/[19.9gb], all_pools {[young] [209.6mb]-&gt;[70.2mb]/[532.5mb]}{[survivor] [66.5mb]-&gt;[0b]/[66.5mb]}{[old] [19gb]-&gt;[18.4gb]/[19.3gb]}
```

We have some custom settings in elasticsearch.yml

```
bootstrap.mlockall: true
index.query.bool.max_clause_count: 100000
threadpool.bulk.queue_size: 300
http.max_content_length: 500mb
transport.tcp.compress: true
index.load_fixed_bitset_filters_eagerly: false
```

Here screenshot of dump analyzer (we have memory dump too 26GB )

| Class Name | Shallow Heap | Retained Heap | Percentage |
| --- | --- | --- | --- |
| org.elasticsearch.common.cache.LocalCache$LocalManualCache @ 0xdee6ad78 | 16 | 11,148,875,400 | 42.23% |

---

![2015-10-30 16-56-06 eclipse memory analyzer](https://cloud.githubusercontent.com/assets/146705/10845143/3cb5a422-7f27-11e5-93de-2eb27524072c.png)
</description><key id="114258415">14382</key><summary>High heap memory consumption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kiryam</reporter><labels /><created>2015-10-30T12:13:56Z</created><updated>2015-10-30T13:45:16Z</updated><resolved>2015-10-30T13:45:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T13:45:16Z" id="152529915">Hi @kiryam 

Please ask questions like these on the forum https://discuss.elastic.co/

I'd start with removing this from your config.  What you have set here is abusive to Elasticsearch:

```
index.query.bool.max_clause_count: 100000
threadpool.bulk.queue_size: 300
http.max_content_length: 500mb
index.load_fixed_bitset_filters_eagerly: false
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster doesn't scale beyond ~400 requests/sec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14381</link><project id="" key="" /><description>I raised a question on the forums, didn't get much input, so raising an issue to see if it catches any input from others.

I have a 3 node cluster, 8gb each.  It maxes out at ~400 requests per second, regardless of what query we give it. We add a 4th node, still it maxes out in the same area (I think we get an extra 20 req/s tops).

Is this normal? Any settings we can look at to tweak this further? We're on ES 1.5.2.
</description><key id="114249630">14381</key><summary>Cluster doesn't scale beyond ~400 requests/sec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels /><created>2015-10-30T11:09:56Z</created><updated>2015-10-30T13:59:22Z</updated><resolved>2015-10-30T13:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T13:42:18Z" id="152529139">Hi @johnament 

This is definitely a question for the forums.  Please continue the conversation there, perhaps by providing more info.
</comment><comment author="johnament" created="2015-10-30T13:59:22Z" id="152532848">Please do let me know what more info I could provide, as best as we can tell with a default install this issue is happening: https://discuss.elastic.co/t/query-not-scaling/33032
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/indices: JSON output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14380</link><project id="" key="" /><description>The `_cat/indices` api seems to only output plain text, JSON output would be nice, eg instead of

```
health status index                       pri rep docs.count docs.deleted store.size pri.store.size 
yellow open   my_index_1               5   1      10179            0      8.4mb          8.4mb 
yellow open   my_index_2               5   1       5111          344      5.1mb          5.1mb
```

it would be nice to get

```
[  
    {  
        "health": "yellow",
        "status": "open",
        "index": "my_index_1",
        "pri": 5,
        "rep": 1,
        "docs_count": 10179,
        "docs_deleted": 0,
        "store_size": "8.4mb",
        "pri_store_size": "8.4mb"
    },
    {  
        "health": "yellow",
        "status": "open",
        "index": "my_index_2",
        "pri": 5,
        "rep": 1,
        "docs_count": 5111,
        "docs_deleted": 344,
        "store_size": "5.1mb",
        "pri_store_size": "5.1mb"
    }
]
```
</description><key id="114248308">14380</key><summary>_cat/indices: JSON output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">j0hnsmith</reporter><labels /><created>2015-10-30T10:59:54Z</created><updated>2015-10-30T11:28:31Z</updated><resolved>2015-10-30T11:28:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-10-30T11:15:03Z" id="152498461">Try adding `?format=json` , for example `_cat/indices?format=json`.

@clintongormley This should probably be documented.
</comment><comment author="j0hnsmith" created="2015-10-30T11:24:41Z" id="152499761">Hmm, `?format=json` works (thanks for letting me know). 

My original request was made with with `Accept: application/json`, this returns `Content-Type: text/plain; charset=UTF-8`, is there a reason why the `Accept` header is ignored?
</comment><comment author="ywelsch" created="2015-10-30T11:28:31Z" id="152500264">Yes, the Accept header is currently ignored, which is a bug.
There is an open issue for that (#14195).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Redis input plugin host as an array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14379</link><project id="" key="" /><description>Is there a reason the redis output can use an array but the redis input cannot.

I would like to use my domain name (a DNS entry for many IP's with the same domain name) in the redis input so that I can spin up new nodes as needed without changing any of my configurations. If I understand the current setup correctly I need to have an input statement that includes each redis nodes IP address on each of my indexers such as:

input {
redis {
host =&gt; [ "x.x.x.x" ]
data_type =&gt; "list"
key =&gt; "bro"
}
redis {
host =&gt; [ "x.x.x.x" ]
data_type =&gt; "list"
key =&gt; "bro"
}
redis {
host =&gt; [ "x.x.x.x" ]
data_type =&gt; "list"
key =&gt; "bro"
}
redis {
host =&gt; [ "x.x.x.x" ]
data_type =&gt; "list"
key =&gt; "bro"
}
}

To help my current setup is I have multiple IDS sensors sending logs to a local redis queue on the sensor, that local queue sends data to a central redis queue with 8 redis nodes (each on their own vm's) I have 4 logstash indexers reading from that queue and sending into a multi node ES cluster. As stated earlier I think having the redis input able to read a hostname as an array or from a domain name that has multiple IP addresses would be helpful in a setup like mine because it would allow me to spin up a new node when needed and just add that nodes IP address to my DNS server and not have to change any config in my ELK stack.

What I would like to see is one statement on each of my indexer nodes that says:

input {
redis {
host =&gt; [ "domain.name" ]
data_type =&gt; "list"
key =&gt; "bro"
}
}

When redis failed to connect to one node in the array it would move onto the next, and recycle the list when completed.  This would be greatly beneficial when scaling a cluster because it would require no interaction with the logstash configs on any node and only adding nodes to a DNS entry.

Just a thought, and please if I have this information wrong correct me.
</description><key id="114247609">14379</key><summary>Redis input plugin host as an array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tdesrochers</reporter><labels /><created>2015-10-30T10:54:34Z</created><updated>2015-10-30T13:40:13Z</updated><resolved>2015-10-30T13:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T13:40:13Z" id="152528738">This issue was moved to elastic/logstash#4121
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move randomized runner gradle plugin to external dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14378</link><project id="" key="" /><description>The randomized testing gradle code belongs with the randomized testing
library. This moves it to its own repository, which is now published as
a snapshot in nexus.

See #13930
</description><key id="114221806">14378</key><summary>Move randomized runner gradle plugin to external dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>stalled</label><label>v5.4.2</label></labels><created>2015-10-30T08:10:39Z</created><updated>2017-06-09T06:14:45Z</updated><resolved>2017-06-09T06:14:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-30T14:50:21Z" id="152545999">FYI, eventually the gradle plugin can move inside the randomizedtesting library directly. But this plugin is still young and immature. So in the meantime, it has its own repository right next to randomizedtesting.
</comment><comment author="rjernst" created="2015-10-30T14:51:05Z" id="152546164">And the repo is here:
https://github.com/randomizedtesting/gradle-randomized-testing-plugin
</comment><comment author="jpountz" created="2015-11-02T09:00:30Z" id="152960806">LGTM
</comment><comment author="rjernst" created="2015-11-02T22:33:06Z" id="153178920">I'm holding off on pushing this to ease the ability for others to contribute to cleanups as we stabilize the gradle build.
</comment><comment author="dakrone" created="2016-04-06T20:47:14Z" id="206559450">@rjernst do we still need to hold off on this, or can it be merged?
</comment><comment author="dakrone" created="2016-09-12T21:20:39Z" id="246497684">@rjernst do we still need to hold off on merging this?
</comment><comment author="rjernst" created="2016-09-12T21:25:01Z" id="246498985">I need to get back around to making a real release of the randomized testing gradle project. Right now it is just a very old snapshot.
</comment><comment author="dakrone" created="2016-09-12T21:32:45Z" id="246501117">Okay, I'll mark this as stalled waiting on a release of the randomized testing gradle project then.
</comment><comment author="dakrone" created="2017-04-07T23:11:12Z" id="292672896">@rjernst is this still stalled?</comment><comment author="rjernst" created="2017-04-07T23:56:01Z" id="292678109">Unfortunately yes. It is stalled in that I have had no time to work on documenting everything in there (eg all the settings, how to use it, etc).</comment><comment author="rjernst" created="2017-06-09T06:14:45Z" id="307304853">I'm closing this PR and will reopen when I have time to prepare the gradle randomized testing plugin for release to maven.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mention changes to repos.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14377</link><project id="" key="" /><description>Fixes #14376
</description><key id="114219082">14377</key><summary>Mention changes to repos.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2015-10-30T07:47:52Z</created><updated>2015-10-30T13:26:01Z</updated><resolved>2015-10-30T09:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-30T08:29:05Z" id="152459233">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add repo updates to breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14376</link><project id="" key="" /><description>We've moved from having repos for each major+minor version to a major only.

Need to update the docs.
</description><key id="114218878">14376</key><summary>Add repo updates to breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-10-30T07:45:53Z</created><updated>2015-10-30T13:26:12Z</updated><resolved>2015-10-30T13:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T13:26:12Z" id="152525565">Closed by #14377
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove maven pom files and supporting ant files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14375</link><project id="" key="" /><description>This change removes the leftover pom files. A couple files were left for
reference, namely in qa tests that have not yet been migrated (vagrant
and multinode). The deb and rpm assemblies also still exist for
reference when finishing their setup in gradle.

See #13930
</description><key id="114213760">14375</key><summary>Remove maven pom files and supporting ant files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T06:54:44Z</created><updated>2015-10-31T01:59:13Z</updated><resolved>2015-10-31T01:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-30T08:11:30Z" id="152456269">Please don't merge this one yet until mapper attachments plugin has been moved to Gradle.
</comment><comment author="rjernst" created="2015-10-30T08:15:35Z" id="152457193">@dadoonet This has no impact on mapper attachments. I think this needs to merge soon, because until it is, maven still "works" and someone may modify a pom and try checking in a change. If there is a need to look at parent poms in order to get mapper attachments working, then that is exactly what source control history is for. Also, this won't affect the already published parent poms snapshots.
</comment><comment author="rmuir" created="2015-10-31T01:57:57Z" id="152686230">+1, looks great! 

@dadoonet mapper attachments is currently tracking 2.1 branch (https://github.com/elastic/elasticsearch-mapper-attachments/blob/master/pom.xml#L30) so its not impacted by the change.

On the other hand this is causing a lot of mistakes and confusion. its too easy to accidentally type 'mvn clean' and for it to seem to have worked, but it really didn't, etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Move test framework files to their new location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14374</link><project id="" key="" /><description>The test jar was previously built in maven by copying class files. With
gradle we now have a proper test framework artifact. This change moves
the classes used by the test framework into the test-framework module.

See #13930
</description><key id="114211627">14374</key><summary>Build: Move test framework files to their new location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T06:27:04Z</created><updated>2015-10-30T15:10:35Z</updated><resolved>2015-10-30T08:34:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-30T08:33:04Z" id="152459777">LGTM
</comment><comment author="jpountz" created="2015-10-30T10:51:20Z" id="152493196">Unfortunately this causes integration issues with Eclipse, since Eclipse builds both source and test files at once, and so it detects a cyclic dependency between core and the test-framework.

I wouldn't like to revert this change just because of Eclipse, yet it seems important to me to have good eclipse integration (not as much for us as for external contributors). The only work-around I can think of right now would be to generate a single project that contains all modules like Lucene does (and Lucene has the same kind of core &lt;-&gt; test-framework dependency). I can look into it if this looks reasonable and if nobody can't think of an easier option.
</comment><comment author="jpountz" created="2015-10-30T14:55:32Z" id="152547332">Maybe just ignoring this error as described at http://stackoverflow.com/a/17208409/675589 is a sufficient workaround
</comment><comment author="rmuir" created="2015-10-30T15:10:35Z" id="152551032">I think a better fix is to modify our groovy build to configure eclipse properly? You can get access to the generated eclipse data and modify it before it is written:

```
eclipse {
  classpath {
    file {
      //closure executed after .classpath content is loaded from existing file
      //but before gradle build information is merged
      beforeMerged { classpath -&gt;
        //you can tinker with the Classpath here
      }
   }
}  
```

https://docs.gradle.org/current/dsl/org.gradle.plugins.ide.eclipse.model.EclipseClasspath.html

We could e.g. hackishly remove the project dependency on test-framework and replace it with an equivalent ordinary classpath entry pointing to test-framework/src/java, and i think things would mostly work as before? There might be better solutions here too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin install [url] unable to validate cert path for local https repos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14373</link><project id="" key="" /><description>Hi,

Our deployment automation has an internal repo for all elasticsearch related artifacts. I'm currently receiving a "unable to find valid certification path to requested target" when accessing this internal repo over https.

Is there any way to configure a trust store for plugin downloads?
</description><key id="114190382">14373</key><summary>bin/plugin install [url] unable to validate cert path for local https repos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels><label>:Plugins</label><label>feedback_needed</label></labels><created>2015-10-30T03:08:21Z</created><updated>2015-11-09T21:27:19Z</updated><resolved>2015-11-09T10:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="robertsmarty" created="2015-10-30T03:09:11Z" id="152398102">I'm using elasticsearch 2.0.0
</comment><comment author="s1monw" created="2015-11-06T10:54:06Z" id="154379063">can you try passing the truststore as a cmd argument like this:

```
-Djavax.net.ssl.trustStore=path/to/trustStore.jks
```

I didn't try this out though just an idea
</comment><comment author="robertsmarty" created="2015-11-09T10:55:32Z" id="155027866">Works perfectly!
</comment><comment author="s1monw" created="2015-11-09T10:56:32Z" id="155028057">awesome! @clintongormley should we add this to some documentation
</comment><comment author="clintongormley" created="2015-11-09T13:34:34Z" id="155063922">@s1monw added in https://github.com/elastic/elasticsearch/commit/fe473ed24ed240f2edbe7c71b3f3b5e8e5f58f7d
</comment><comment author="s1monw" created="2015-11-09T21:27:19Z" id="155201938">awesome thanks @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>startup scripts should pass jvm -Djava.io.tmpdir=$ES_HOME/tmp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14372</link><project id="" key="" /><description>Today we use the system tempdir, but this can cause several problems (e.g. https://discuss.elastic.co/t/elasticsearch-2-0-warning-jna-is-not-available/33320/1). It would also just be better in general as far as isolating the elasticsearch process.

Instead we should just set our own: scripts should also create it, if it does not exist. It should not and cannot be configurable: it needs to be passed to the jvm initially and not set after-the-fact. Otherwise we end out with a mess (e.g. 2 temp directories to contend with).
</description><key id="114188022">14372</key><summary>startup scripts should pass jvm -Djava.io.tmpdir=$ES_HOME/tmp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-10-30T02:37:40Z</created><updated>2016-03-09T10:58:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-23T15:36:29Z" id="158971177">&gt; Instead we should just set our own: scripts should also create it, if it does not exist. It should not and cannot be configurable: it needs to be passed to the jvm initially and not set after-the-fact. Otherwise we end out with a mess (e.g. 2 temp directories to contend with).

I suspect there are plenty of distros that'd be upset if we didn't let them override that to dump it where they want it. If its just something that defaults to $ESHOME/tmp but can be changed with a shell variable that'd be fine.
</comment><comment author="rmuir" created="2015-11-23T15:39:38Z" id="158972019">Yeah: shell variable is fine. Its more that it wont really work as a e.g. typical elasticsearch.yml configuration option. 

We need to pass java.io.tmpdir into the JVM up front, in JAVA_OPTS, otherwise it only works "partially" and we will be stuck with two temporary directories, which is bad.
</comment><comment author="nellicus" created="2016-03-09T06:10:18Z" id="194129334">another user hit this today @rmuir @nik9000 
</comment><comment author="rmuir" created="2016-03-09T10:58:41Z" id="194241219">Thats's ok: Like i said its essentially unfixable until we decide to cleanup how elasticsearch configuration works.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add gradle version check and some build info that is always output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14371</link><project id="" key="" /><description>The build info looks like the following:

```
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version : 2.7
  JDK Version    : 1.8.0_60-b27 (Oracle Corporation)
  OS Info        : Mac OS X 10.10.5 (x86_64)
```
</description><key id="114172988">14371</key><summary>Build: Add gradle version check and some build info that is always output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-30T00:14:15Z</created><updated>2015-10-30T00:15:08Z</updated><resolved>2015-10-30T00:15:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-30T00:14:43Z" id="152362665">looks good. this is important for debugging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StackOverflowError when parsing deeply nested mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14370</link><project id="" key="" /><description>Have a scenario where ES is throwing a StackOverflowError when the data node receives a cluster state published by the master.  It is failing when parsing the mappings.

```
2015-10-27 00:23:50,561 [DEBUG] [service] [node_name] set local cluster state to version 575475 
2015-10-27 00:23:50,646 [WARN] [cluster] [node_name] [index_name] failed to add mapping [log], source [{"log":{"dynamic_templates":[{"facet_string_template":{"mapping":{"type":"multi_field","fields": 
... 
&lt;a TON of mappings&gt; 
... 
java.lang.StackOverflowError 
at org.elasticsearch.common.Strings.toUnderscoreCase(Strings.java:959) 
at org.elasticsearch.index.mapper.core.TypeParsers.parseField(TypeParsers.java:164) 
at org.elasticsearch.index.mapper.core.StringFieldMapper$TypeParser.parse(StringFieldMapper.java:153) 
at org.elasticsearch.index.mapper.core.TypeParsers.parseMultiField(TypeParsers.java:284) 
at org.elasticsearch.index.mapper.core.StringFieldMapper$TypeParser.parse(StringFieldMapper.java:184) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:189) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:290) 
at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:214) 
```

In this particular case, there is an explosion in mapped fields due to dynamically created fields where a single field ends up having many many levels of nesting (recursion).  As a result, StackOverflowError shows up whenever the mappings has to be parsed so that the nodes cannot process the updates.

It will probably be helpful if we have a sensible default to limit the number of depth levels for a field, etc..
</description><key id="114171347">14370</key><summary>StackOverflowError when parsing deeply nested mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-10-29T23:57:47Z</created><updated>2015-11-08T21:50:03Z</updated><resolved>2015-11-08T21:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-30T20:27:11Z" id="152643597">@ppf2 it sounds reasonable to me to have a sensible default to the limit. Do you know how deep these mappings went? Could we reasonably pick "100" as the limit for nesting?
</comment><comment author="ppf2" created="2015-10-30T20:30:29Z" id="152644255">@dakrone Will send you a sample offline.  I think we can choose a default, and also expose this as a configurable setting?
</comment><comment author="rjernst" created="2015-10-30T20:41:36Z" id="152646430">100 seems crazy high to me. `foo.bar.baz.a.b.c.d.e.f.g` is already 10 levels deep and that is crazy...
</comment><comment author="dakrone" created="2015-10-30T20:43:32Z" id="152646796">@ppf2 We don't need a sample, I was just curious the number of levels it went down (not the actual mapping)
</comment><comment author="ppf2" created="2015-10-30T22:44:20Z" id="152667982">@dakrone In this case, easily &gt; 500 levels :open_mouth: 
</comment><comment author="clintongormley" created="2015-11-08T21:50:03Z" id="154877674">Closing in favour of #11511
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Update gitignore to keep maven targets on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14369</link><project id="" key="" /><description>See #14364
</description><key id="114164791">14369</key><summary>Build: Update gitignore to keep maven targets on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T22:58:01Z</created><updated>2015-10-30T01:15:12Z</updated><resolved>2015-10-29T23:27:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-29T23:18:16Z" id="152354370">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Move the real forbidden api signature files to their new location within buildSrc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14368</link><project id="" key="" /><description>The gradle branch used copies of the forbidden api signature files. This
moves the files to their correct location.

closes #14363
</description><key id="114162540">14368</key><summary>Build: Move the real forbidden api signature files to their new location within buildSrc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T22:42:08Z</created><updated>2015-10-29T22:56:42Z</updated><resolved>2015-10-29T22:56:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-29T22:43:30Z" id="152348328">+1 thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid changing thread pool types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14367</link><project id="" key="" /><description>This commit forbids the changing of thread pool types for any thread
pool. The motivation here is that these are expert settings with
little practical advantage.

Closes #14294, relates #2509, relates #2858, relates #5152
</description><key id="114148834">14367</key><summary>Forbid changing thread pool types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>breaking</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T21:13:35Z</created><updated>2016-01-31T13:44:08Z</updated><resolved>2015-11-03T02:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-30T20:42:56Z" id="152646694">Left some comments but I like this! The only thing I wonder, however, is how a new 2.1.0 node joining a 2.0.0 node that already has custom threadpool settings will behave, once it sees the new settings it will try to apply them but fail, so you'll end up with different nodes with different configurations. This seems kind of dangerous, what do you think about deprecate in 2.x and remove in 3.0 only?
</comment><comment author="nik9000" created="2015-10-30T20:50:37Z" id="152648125">&gt; This seems kind of dangerous, what do you think about deprecate in 2.x and remove in 3.0 only?

I'd be ok if the 2.1 nodes just failed to start entirely if the cluster has this configuration and huffily printed out a message to the logs "detected dangerous threadpool configuration. Please change blah blah blah." So long as the parameter can be changed without bouncing the whole cluster this is something people can work around I think.

Folks set these configuration variables like this are setting them in they pre-prod environments too hopefully. So it _shouldn't_ be a surprise during upgrade for most folks.
</comment><comment author="dakrone" created="2015-10-30T20:54:25Z" id="152648844">&gt; I'd be ok if the 2.1 nodes just failed to start entirely if the cluster has this configuration and huffily printed out a message to the logs

+1 only if it's easy to implement the failing to start. Since these settings are previously dynamic, someone can change their config and then start the 2.1.0 node up again.
</comment><comment author="jasontedor" created="2015-11-02T13:51:44Z" id="153022351">@dakrone I pushed a refactoring cleanup in 32b06108eda10d48e3b767f719e3e3d25485353b and validated dynamic settings attempts in fb30fc2775a700757c1169fc083b9ce650d10633. Would you mind taking a look?
</comment><comment author="jasontedor" created="2015-11-02T14:37:14Z" id="153033422">&gt; This seems kind of dangerous, what do you think about deprecate in 2.x and remove in 3.0 only?

@dakrone As @jpountz remarked in the original [pull request](https://github.com/elastic/elasticsearch/pull/14336#issuecomment-152036897) (when the idea was only to forbid changing thread pool types to type "cached"), this is a very expert setting so there will probably be very little if any real-world breakage. I'm fine adding something clear to the `migrate_2_1.asciidoc` and possibly implementing @nik9000's [suggestion](https://github.com/elastic/elasticsearch/pull/14367#issuecomment-152648125), or just leaving this for 3.0. Any final thoughts @dakrone @jpountz @nik9000?
</comment><comment author="nik9000" created="2015-11-02T14:44:02Z" id="153036547">&gt; Any final thoughts @dakrone @jpountz @nik9000?

I'd certainly prefer it complain loudly in the logs if it gets a now invalid setting but I'm fine if ES doesn't fail to start. Or if it does. Either way.

I like the idea of putting this in the 2.x line somewhere (2.1, 2.2, whatever) because it makes things less likely to crash.
</comment><comment author="jasontedor" created="2015-11-02T15:35:01Z" id="153054482">&gt; +1 only if it's easy to implement the failing to start.

It's easy, but it'd be a hack. Right now we are very lenient about failure to apply settings. We'd basically have to special case this setting and I'm not a fan of that.

&gt; Since these settings are previously dynamic, someone can change their config and then start the 2.1.0 node up again.

The problem here is that today we still do not have way to delete persistent cluster settings. That gets us into situations like #3670 which is still waiting on #6732 to be addressed. In this case, someone could have these settings applied on a 2.0.x cluster, they will no longer apply on a 2.1.y cluster, will see warning messages in the logs, but not have a clean way to remove the setting.

@nik9000 @jpountz @dakrone I'm starting to lean towards 3.0 only.
</comment><comment author="dakrone" created="2015-11-02T15:58:31Z" id="153061556">&gt; It's easy, but it'd be a hack. Right now we are very lenient about failure to apply settings. We'd basically have to special case this setting and I'm not a fan of that.

Not worth it for a hack. Let's not do that then.

&gt; I'm starting to lean towards 3.0 only.

+1 for 3.0 only, this isn't so important we need to jump through tons of hoops to get it in for 2.1. I think we can remove the docs for setting it in 2.1 though, just to reduce the chance of the type being set.
</comment><comment author="jasontedor" created="2015-11-02T16:32:36Z" id="153075055">@dakrone Removed the the byte serialization of `ThreadPoolType` in d4e015d6945392090b2833276381fde7c2ae1cf2 and returned to the string serialization.
</comment><comment author="dakrone" created="2015-11-02T16:39:52Z" id="153077284">LGTM, thanks @jasontedor !
</comment><comment author="jasontedor" created="2015-11-02T16:42:24Z" id="153077980">&gt; +1 for 3.0 only

Removed the [v2.1.0](https://github.com/elastic/elasticsearch/labels/v2.1.0) and [v2.2.0](https://github.com/elastic/elasticsearch/labels/v2.2.0) labels. That said, we still have the issue that if someone has this set on a 2.x cluster and they migrate to 3.y, they are going to have the problem of having this setting stuck in the cluster state until a solution to #3670 comes along.
</comment><comment author="jpountz" created="2015-11-02T16:49:14Z" id="153079834">So maybe in this PR we should keep accepting the type parameter if the value is equal to the right threadpool type and leave a comment saying that this hack can be removed when #3670 is in? Otherwise I'm afraid that this might get forgotten and that we will get failed upgrades when releasing 3.0.
</comment><comment author="jasontedor" created="2015-11-02T17:56:34Z" id="153100117">&gt; So maybe in this PR we should keep accepting the type parameter if the value is equal to the right threadpool type and leave a comment saying that this hack can be removed when #3670 is in? Otherwise I'm afraid that this might get forgotten and that we will get failed upgrades when releasing 3.0.

@jpountz Agree. Done in 843a394ea40e7a41081c245e0be9fa42661dec2e.
</comment><comment author="jpountz" created="2015-11-02T19:17:02Z" id="153129152">LGTM. It should be backportable to 2.x under the current form, right? (not especially pushing for this, just wondering as what seemed to be the major concern against backporting to 2.x does not seem to apply anymore?)
</comment><comment author="jasontedor" created="2015-11-02T19:19:31Z" id="153130021">&gt; It should be backportable to 2.x under the current form, right? 

Yes.

&gt; (not especially pushing for this, just wondering as what seemed to be the major concern against backporting to 2.x does not seem to apply anymore?)

Correct.
</comment><comment author="jpountz" created="2015-11-03T10:56:42Z" id="153314333">@jasontedor I'm not seeing the commit on the 2.1/2.x branches, do I miss anything?
</comment><comment author="jasontedor" created="2015-11-03T11:09:43Z" id="153316791">@jpountz It was a messy integration into those branches (from among other things, Guava still being used, lambdas not able to be used); I have it ready locally and will be getting it into origin soon. 
</comment><comment author="jasontedor" created="2015-11-03T17:04:54Z" id="153419073">@jpountz This is integrated into 2.1 and 2.x, and I've backported your test bug fix from a7bf06ee3a14fc1464fa295eede3898f2f105ce6 as well.
</comment><comment author="jpountz" created="2015-11-03T18:00:25Z" id="153435111">Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add back manifest info to jars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14366</link><project id="" key="" /><description>closes #14365
</description><key id="114145990">14366</key><summary>Build: Add back manifest info to jars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T21:00:50Z</created><updated>2015-10-29T21:07:03Z</updated><resolved>2015-10-29T21:06:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-29T21:06:26Z" id="152326885">looks good, i added a comment for a followup.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>gradle loses ALL JAR METADATA</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14365</link><project id="" key="" /><description>we have empty manifests now (e.g. unzip elasticsearch.zip and you will see the elasticsearch jar file in it has nothing).

maven populates these jars with important metadata that we verify at runtime to detect incompatibilities.
</description><key id="114131952">14365</key><summary>gradle loses ALL JAR METADATA</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-29T19:50:05Z</created><updated>2015-10-29T21:06:57Z</updated><resolved>2015-10-29T21:06:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>.gitignore needs to be updated for gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14364</link><project id="" key="" /><description>running gradle check dirties up my checkout with tons of stuff.
</description><key id="114130606">14364</key><summary>.gitignore needs to be updated for gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-29T19:41:57Z</created><updated>2015-10-30T01:17:40Z</updated><resolved>2015-10-30T01:17:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-29T19:43:17Z" id="152298414">I think we should just backport the gradle changes to all 2.x branches, whether or not they are using gradle. And the ignores used by maven need to be in master too (even though master does not use maven).

Otherwise, changing git branches is a disaster.
</comment><comment author="nik9000" created="2015-10-29T20:07:31Z" id="152303868">&gt; Otherwise, changing git branches is a disaster.

++++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>recent forbidden apis changes are lost in gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14363</link><project id="" key="" /><description>Stuff like this is missing:

```

@defaultMessage this should not have been added to lucene in the first place
org.apache.lucene.index.IndexReader#getCombinedCoreAndDeletesKey()

@defaultMessage this method needs special permission
java.lang.Thread#getAllStackTraces()
```

We should review any of these files moved around/changed in the commit and make sure we did not lose any other recent changes.
</description><key id="114129969">14363</key><summary>recent forbidden apis changes are lost in gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-10-29T19:38:15Z</created><updated>2015-10-29T22:56:40Z</updated><resolved>2015-10-29T22:56:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add gradle build script to plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14362</link><project id="" key="" /><description>Gradle build scripts were introduced into Elasticsearch!

This PR contains the necessary gradle build scripts to enable gradle building for IngestPlugin.
</description><key id="114127682">14362</key><summary>add gradle build script to plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>build</label></labels><created>2015-10-29T19:25:59Z</created><updated>2015-10-29T22:26:22Z</updated><resolved>2015-10-29T22:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-10-29T20:41:48Z" id="152313286">@martijnvg  @rjernst do you mind taking a look at this?
</comment><comment author="rjernst" created="2015-10-29T21:17:26Z" id="152330883">If you want to make this equivalent, you should add securemock as a dependency in PluginBuildPlugin (see buildSrc). However, I think it makes more sense as a dep of test-framework?
</comment><comment author="talevy" created="2015-10-29T21:38:02Z" id="152335563">@rjernst I thought other plugins also used it, but that doesn't seem to be the case. Only IngestPlugin uses it.
I think how it is now is better for now. what do you think?
</comment><comment author="rjernst" created="2015-10-29T22:24:53Z" id="152345076">Sure. LGTM.
</comment><comment author="talevy" created="2015-10-29T22:26:16Z" id="152345321">thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch integration tests are not run in gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14361</link><project id="" key="" /><description>cd distribution
gradle check

does nothing... we need to get the rest tests going ASAP or the risk of breaking ES via refactorings in such a way we cannot recover is very high. 

plugin integ tests are working already, i will take a look at distribution/zip and see if i can figure out how to fix it as a first step. any help is appreciated.
</description><key id="114126146">14361</key><summary>elasticsearch integration tests are not run in gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T19:17:09Z</created><updated>2015-11-03T18:35:12Z</updated><resolved>2015-11-03T18:35:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-30T16:54:44Z" id="152585431">@rmuir - do you want some help looking at this? I assigned it to you because you said you were looking at it but if you haven't yet I can grab it.
</comment><comment author="rmuir" created="2015-10-30T16:55:31Z" id="152585625">please!
</comment><comment author="nik9000" created="2015-11-02T14:14:34Z" id="153028114">Ryan told me he'd grab this so I'm assigning it to him.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Properly set indices and indicesOptions on subrequest made by /_cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14360</link><project id="" key="" /><description /><key id="114109899">14360</key><summary>Properly set indices and indicesOptions on subrequest made by /_cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:CAT API</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T18:00:08Z</created><updated>2015-10-30T13:37:35Z</updated><resolved>2015-10-30T10:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-29T21:17:39Z" id="152330974">fix looks good, can we write a REST test that makes sure this is solved and doesn't happen again?
</comment><comment author="javanna" created="2015-10-30T09:51:23Z" id="152476019">hey @ywelsch thanks for fixing this, I see that this can't really be tested through REST tests as it doesn't change the the end result, but only how things work internally. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't start ES after upgrading from 1.7.1 to 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14359</link><project id="" key="" /><description>I'm getting the following in our test cluster when I try to start up:

`Exception in thread "main" java.lang.IllegalStateException: unable to upgrade the mappings for the index [dev-note-index], reason: [Field name [listing.requestAmount] cannot contain '.']`

Any ideas?
</description><key id="114102440">14359</key><summary>Can't start ES after upgrading from 1.7.1 to 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">futurekill</reporter><labels /><created>2015-10-29T17:22:42Z</created><updated>2015-11-06T20:34:37Z</updated><resolved>2015-10-29T18:51:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="futurekill" created="2015-10-29T17:40:07Z" id="152260643">I upgraded using the ES yum repo...here's what's installed now:

Name        : elasticsearch
Version     : 2.0.0
Release     : 1
Architecture: noarch
Install Date: Thu 29 Oct 2015 05:08:13 PM UTC
Group       : Application/Internet
Size        : 31905528
License     : (c) 2009
Signature   : RSA/SHA1, Thu 22 Oct 2015 08:41:40 AM UTC, Key ID d27d666cd88e42b4
Source RPM  : elasticsearch-2.0.0-1.src.rpm
Build Date  : Thu 22 Oct 2015 08:41:37 AM UTC
Build Host  : vagrant-ubuntu-trusty-64
Relocations : /usr
Packager    : Elasticsearch
Summary     : Distribution: RPM
</comment><comment author="clintongormley" created="2015-10-29T18:51:44Z" id="152285433">It's in the breaking changes docs: https://www.elastic.co/guide/en/elasticsearch/reference/current/_mapping_changes.html#_field_names_may_not_contain_dots

We recommend using the migration plugin to detect these issues before upgrading: https://github.com/elastic/elasticsearch-migration/
</comment><comment author="aanm" created="2015-11-06T17:00:06Z" id="154472380">@clintongormley But why? Is it difficult to solve that? I mean, if elasticsearch is a document oriented database why can't save json documents since json supports dots in field names?
</comment><comment author="rjernst" created="2015-11-06T17:20:47Z" id="154477498">@aanm Json does not support dots in field names (at least not so far as not using quotes); using a dot implies the part before the dot is an object. The problem with this in elasticsearch is ambiguity, and indeterminism. When a user passes say "foo.bar" as a field name, should that be an object field "foo", with a "bar" field, or a single field "foo.bar"? What happens when a field "foo.bar" exists at the same time an object field "foo" with a subfield "bar" exists? These are the problems that led to removing dot as a valid character in field names.
</comment><comment author="aanm" created="2015-11-06T19:50:32Z" id="154514565">@rjernst Thanks for the reply. Exactly! It supports as far I use quotes, which I do. I think, if a user says 'foo.bar' it's wrong and should cause an exception but if the user says '"foo.bar"' (see the double quote) elastic should consider a single field "foo.bar".

&gt; What happens when a field "foo.bar" exists at the same time an object field "foo" with a subfield "bar" exists?

Nothing, they are different things ' "foo.bar" ' is a field.
</comment><comment author="rjernst" created="2015-11-06T19:54:46Z" id="154515558">&gt; &gt; What happens when a field "foo.bar" exists at the same time an object field "foo" with a subfield "bar" exists?
&gt; 
&gt; Nothing, they are different things ' "foo.bar" ' is a field.

But now you have two fields, with the same path. Remember that fields in 2.0 must be specified using their full path. The field "foo.bar" has the same path as that with object field "foo" and sub field "bar". 
</comment><comment author="diranged" created="2015-11-06T19:57:20Z" id="154516194">Hey guys this is a pretty amazingly painful change. We have 30 days worth of indexed data from our Flume pipeline ... and you guessed it, all 30 indexes have a `flume.syslog.status` field ([FLUME-2838](https://issues.apache.org/jira/browse/FLUME-2838)).

We're going to get around this temporarily by actually striping this header out with our custom ElasticSearch Sink ... but even that is going to delay our upgrade to ES 2.0 by 30 days whlie we wait for our indexes to roll over.

Is there any way that this change can be made optional rather than a completely blocking change?
</comment><comment author="rjernst" created="2015-11-06T20:01:14Z" id="154517124">&gt; Is there any way that this change can be made optional rather than a completely blocking change?

@diranged Sorry, it cannot. The major improvements to mappings in 2.0 were to eliminate the possibility of ambiguity in fields. That cannot be made optional as the inner workings of mappings now depend on that uniqueness.
</comment><comment author="diranged" created="2015-11-06T20:04:30Z" id="154517992">@rjernst and without dumping and re-indexing ~9 billion events, we can't _change_ the field name of previously indexed data.. right?
</comment><comment author="s1monw" created="2015-11-06T20:07:15Z" id="154518694">&gt; Is there any way that this change can be made optional rather than a completely blocking change?

this can't be done differently. You guys ran into a long standing bug that we fixed. Waiting 30 days is not the end of the world. Changes like this must be done at some point, if you make it lenient you just end up with more trouble. If you wait 30 days you will be able to go to 2.1 even which might be even more stable.
</comment><comment author="rjernst" created="2015-11-06T20:08:27Z" id="154518992">@diranged Yes, you will need to reindex.
</comment><comment author="aanm" created="2015-11-06T20:20:58Z" id="154521625">&gt; But now you have two fields, with the same path. Remember that fields in 2.0 must be specified using their full path. The field "foo.bar" has the same path as that with object field "foo" and sub field "bar".

@rjernst Okay, can provide me a path example?
</comment><comment author="rjernst" created="2015-11-06T20:24:45Z" id="154522393">@aanm "path" is the term used for the full canonical name for a field. This includes any object fields it is part of, from the root of the mappings. "foo.bar" is the path (ie the full canonical name) for an object field called "foo", with a subfield "bar".
</comment><comment author="aanm" created="2015-11-06T20:27:42Z" id="154523017">@rjernst Okay "foo.bar" is not supported, how about `foo\.bar`?
</comment><comment author="rjernst" created="2015-11-06T20:34:31Z" id="154524642">Escaping is not supported at this time. I won't say it will never be supported, but it is sufficiently complicated enough with all the places we access/use field names that it would be a largish change IMO.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding US-Gov-West</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14358</link><project id="" key="" /><description>Adds support for EC2 and S3 gov end points: http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/using-govcloud-endpoints.html
- ec2.us-gov-west-1.amazonaws.com
- s3-us-gov-west-1.amazonaws.com
</description><key id="114099637">14358</key><summary>Adding US-Gov-West</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">amos6224</reporter><labels><label>:Plugin Discovery EC2</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T17:09:42Z</created><updated>2015-11-02T13:48:47Z</updated><resolved>2015-11-02T13:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="amos6224" created="2015-10-29T17:10:13Z" id="152251883">Please review and merge if possible 
</comment><comment author="dadoonet" created="2015-11-02T13:48:47Z" id="153021309">Merged. Thanks!

BTW I added another commit for S3 repositories as well.

Pushed in master, 2.x and 2.1 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0.0 don't start if marvel-agent is set in plugin.mandatory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14357</link><project id="" key="" /><description>Running elasticsearch 2.0.0 on ubuntu 14.04, marvel-agent is installed:

```
/usr/share/elasticsearch/bin/plugin list
Installed plugins in /usr/share/elasticsearch/plugins:
    - marvel-agent
    - license
```

However if I set marvel-agent in the list of mandatory plugins in elasticsearch.yml I get the following Exception:

```
[2015-10-29 16:53:52,138][INFO ][node                     ] [schinf-0000-prd1] version[2.0.0], pid[5239], build[de54438/2015-10-22T08:09:48Z]
[2015-10-29 16:53:52,140][INFO ][node                     ] [schinf-0000-prd1] initializing ...
[2015-10-29 16:53:52,404][ERROR][bootstrap                ] Exception
ElasticsearchException[Missing mandatory plugins [marvel-agent]]
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:149)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:144)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

If I don't set marvel-agent in plugin.mandatory, elasticsearch start correctly and list marvel-agent as installed.
</description><key id="114096815">14357</key><summary>Elasticsearch 2.0.0 don't start if marvel-agent is set in plugin.mandatory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">rvrignaud</reporter><labels><label>:Plugins</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-10-29T16:55:38Z</created><updated>2016-01-29T20:22:03Z</updated><resolved>2016-01-29T20:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T12:44:49Z" id="152513736">As a workaround, specify `plugin.mandatory: marvel`
</comment><comment author="tlrx" created="2015-11-03T12:15:21Z" id="153335544">The offending bug is located [here](https://github.com/elastic/elasticsearch/blob/v2.0.0/core/src/main/java/org/elasticsearch/plugins/PluginsService.java#L132) where it used the name defined in the plugin's class rather than the name defined in the plugin's properties file.

This is already fixed in 2.x, 2.1 and [master](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/PluginsService.java#L129) branch of elasticsearch. We don't need to rename anything in Marvel for now.

I created #14479 to backport the fix in 2.0 too, in case 2.0.1 is released.
</comment><comment author="clintongormley" created="2016-01-29T20:22:03Z" id="176952419">2.1.0 has been released. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Field Mapping Support for GeoPointV2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14356</link><project id="" key="" /><description>This is GeoPointV2 PR part 2. It adds the abstraction layer to GeoPointFieldMapper needed to cut over to Lucene 5.4's new GeoPointField type while maintaining backward compatibility with existing `geo_point` indexes.
</description><key id="114093145">14356</key><summary>Add Field Mapping Support for GeoPointV2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Mapping</label><label>review</label></labels><created>2015-10-29T16:41:36Z</created><updated>2015-11-08T22:56:06Z</updated><resolved>2015-11-04T22:51:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-11-03T14:01:28Z" id="153362779">@rjernst If you wouldn't mind having a look at this PR (and any others in this merge series) it maintains BWC with 1.x - 2.1 created indexes.
</comment><comment author="s1monw" created="2015-11-04T12:15:57Z" id="153702390">@rjernst if you have the time can you please prioritize the review of this PR
</comment><comment author="nknize" created="2015-11-04T22:51:07Z" id="153894936">closing in favor of PR #14536
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify Analysis registration and configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14355</link><project id="" key="" /><description>This change moves all the analysis component registration to the node level
and removes the significant API overhead to register tokenfilter, tokenizer,
charfilter and analyzer. All registration is done without guice interaction such
that real factories via functional interfaces are passed instead of class objects
that are instantiated at runtime.

This change also hides the internal analyzer caching that was done previously in the
IndicesAnalysisService entirely and decouples all analysis registration and creation
from dependency injection.
</description><key id="114075107">14355</key><summary>Simplify Analysis registration and configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Analysis</label><label>:Plugins</label><label>breaking-java</label><label>enhancement</label><label>PITA</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T15:28:38Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-30T11:01:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-29T15:36:05Z" id="152217948">I think @uschindler will like this too
</comment><comment author="s1monw" created="2015-10-29T15:51:53Z" id="152222670">https://github.com/elastic/elasticsearch/issues/8961 is related to this
</comment><comment author="uschindler" created="2015-10-29T16:41:12Z" id="152242358">I like this more, but I am not yet fully happy :-) You know why: Use Lucene's factories! (although this would break users's config, unless we have some legacy mapping)

Although this is another issue, here my thoughts: Maybe provide a way to configure Analyzers using Lucene's CustomAnalyzer (since 5.0, https://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.html) in addition to the old way (using different names for components and settings, unfortunately), and deprecate the "old way"...
</comment><comment author="rjernst" created="2015-10-29T18:04:04Z" id="152268024">This is fantastic! LGTM, Just cosmetic comments. I agree with @uschindler that we should work towards using the lucene factories, but that work can continue in follow up issues.
</comment><comment author="brusic" created="2015-10-29T19:15:39Z" id="152291568">3.0?
</comment><comment author="s1monw" created="2015-10-30T07:46:17Z" id="152452259">&gt; 3.0?

@brusic can you be more verbose? :)
</comment><comment author="s1monw" created="2015-10-30T09:41:59Z" id="152474100">&gt; I like this more, but I am not yet fully happy :-) You know why: Use Lucene's factories! (although this would break users's config, unless we have some legacy mapping)

this PR isn't about changing what we use but rather how it's specified and configured and registered. Note that this removes the Guice dependency entirely which is a huge step forward. I agree we should reuse more code from lucene here but lets do that step by step.
</comment><comment author="brusic" created="2015-10-30T16:41:48Z" id="152581742">And to think I am a very talkative person, both in speech and in writing!

I noticed that the PR is tagged 3.0.0. Since 2.0 is not officially out, I
was surprised this change is getting pushed out.

&gt; &gt; 3.0?
</comment><comment author="nik9000" created="2015-10-30T16:45:32Z" id="152583204">&gt; I noticed that the PR is tagged 3.0.0. Since 2.0 is not officially out, I
&gt; was surprised this change is getting pushed out.

Ah! 2.0 was feature frozen a while back, maybe a month or so. This change, and changes it rely on need Java 8 or they become nasty. And 3.0 is the first release which will have Java 8 as a minimum requirement.
</comment><comment author="brusic" created="2015-10-30T17:40:09Z" id="152597445">&gt; Ah! 2.0 was feature frozen a while back, maybe a month or so. This change, and changes it rely on need Java 8 or they become nasty. And 3.0 is the first release which will have Java 8 as a minimum requirement.

Completely understandable that this change would not make it into 2.0. What I meant to say is that I am surprised that any feature is getting tagged 3.0 and not 2.x. Clinton had this feature marked as 2.1.

I only looked at part of the code, since I had to implement something with a similar AnalysisRegistry a long time ago and wanted to see how it was done. I always considered it a hack and wanted to move toward something more official with the services. Did not read enough of the change to see Java 8 specific changes.

Anyways, good job. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update search-template.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14354</link><project id="" key="" /><description>Felt the documentation for the conditional clauses was not clear and invited questions like http://stackoverflow.com/questions/26606853/elasticsearch-search-templates. I had answered the question in stackoverflow. Also thought it is appropriate to propose improvisation of the documentation.
</description><key id="114038124">14354</key><summary>Update search-template.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">Iswaryar</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-10-29T12:22:38Z</created><updated>2016-03-10T12:37:00Z</updated><resolved>2016-03-10T12:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T09:59:37Z" id="152478407">Hi @Iswaryar 

Thanks for the PR.  Please could I ask you to sign the CLA?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="Iswaryar" created="2015-10-30T10:44:57Z" id="152490068">done
</comment><comment author="clintongormley" created="2015-10-30T11:59:51Z" id="152506083">Hi @Iswaryar 

I've had a look at the changes and I think they introduce more confusion, by adding too many examples, and in the wrong place.  Let's rethink this.  

What parts did you find confusing?  If we start there, then finding the right solution will be easier.
</comment><comment author="Iswaryar" created="2015-10-30T12:14:37Z" id="152508301">It is mentioned that "the template should either be stored in a file or, when used via the REST API, should be written as a string". And the way to execute the stored template is also given. But it is not mentioned that only the
 {
    "query":{ whatever query }
}
part should be placed in the file in config&gt;scripts with .mustache extension.
I din't find it straight forward. Seeing the question on stackexchange, I knew I was not alone. Also when using the registered template, we use the "id". For the ones stored in the file, we use just the template property. 
</comment><comment author="clintongormley" created="2016-03-10T12:37:00Z" id="194821280">Hi @Iswaryar 

Sorry it has taken a while to get back to this.  I've taken another look and still feel that the existing docs are clearer than the proposed changes, so I'm going to close this PR.

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add threadgroup isolation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14353</link><project id="" key="" /><description>Code with `modifyThread` and `modifyThreadGroup` may only modify
its own threadgroup (or an ancestor of that). This enforces
what is intended by the ThreadGroup class.

Today there are only two thread groups: "system" (Java) and "main" (ES). 

Adding this isolation has two new immediate implications:
1. Code without permissions (scripts) may not create or mess with threads at all.
2. ES application threads cannot mess with Java system threads at all.

ES puts all application threads in one single group today, but in the future
this can be organized better, and we will have more isolation in the system.

NOTE: implementation-wise, this means our SM impl needs to be privileged, so that
it can properly do access checks. I organized this here (https://github.com/rmuir/securesm/), we can move it to an elastic repository. It has its own tests. It needs to be a separate jar just because how security checks work in java, and being completely separate means no problems with IDEs, "reactor builds" or what have you. See the javadocs there for more details.
</description><key id="114027391">14353</key><summary>Add threadgroup isolation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-29T11:08:33Z</created><updated>2015-10-30T12:53:29Z</updated><resolved>2015-10-29T18:15:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-29T13:49:53Z" id="152184344">LGTM

&gt; we can move it to an elastic repository

+1
</comment><comment author="jasontedor" created="2015-10-29T13:56:51Z" id="152188644">Because of the POM changes, the addition of the securesm artifact and the [imminent Gradle transition](https://github.com/elastic/elasticsearch/issues/13930), I think that @rjernst will want to coordinate with you on integrating to master?

LGTM. This is incredibly important work.
</comment><comment author="rjernst" created="2015-10-29T17:38:17Z" id="152260148">This should be easy to merge into the gradle branch right away. LGTM, push away.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename the `allow_primary` param of the _reroute API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14352</link><project id="" key="" /><description>The allocate command of the [_reroute API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html?q=rero#cluster-reroute) takes an `allow_primary` parameter, which allows to force assign a primary shard. Using this parameter is dangerous as it will always result in data loss and the new shard will be created empty. This is well documented, but I still see people use it in an attempt to recover from a scenario where ES does not assign the primary to a node that _has_ shard data on disk. The hope there is that that will trigger an allocation and the data will be salvaged. Sadly this means the data is deleted and any hope of salvaging it in another way (something _is_ wrong with it) is lost.

I suggest we rename the parameter to something that makes the weight of the decision clearer. I can't come up with a concise name though. Some suggestions I had is 'assign_primary_as_empty`,`force_clean_primary` .
</description><key id="114025287">14352</key><summary>Rename the `allow_primary` param of the _reroute API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-10-29T10:54:03Z</created><updated>2016-01-26T10:23:50Z</updated><resolved>2016-01-26T10:23:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T10:15:04Z" id="152482688">A suggestion made in FixItFriday was to remove the parameter entirely.  Essentially, you have lost a shard, and the only thing to do is to reindex.

Looking for opinions on this: when is it useful to assign an empty primary shard?
</comment><comment author="bleskes" created="2015-10-30T11:27:41Z" id="152500149">If people find themselves in the unhappy place where a shard is red, they can't index into that index any more because of the missing shard. Even if you don't need to index into that index any more, the cluster status is red which means you have to disable monitoring of it until the index is deleted. The allow primary flag allows you to accept your loss and go back to normal. I see it used frequently after multi nodes failures where primary and replica are gone.

On 30 okt. 2015 11:15 AM +0100, Clinton Gormleynotifications@github.com, wrote:

&gt; A suggestion made in FixItFriday was to remove the parameter entirely. Essentially, you have lost a shard, and the only thing to do is to reindex.
&gt; 
&gt; Looking for opinions on this: when is it useful to assign an empty primary shard?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/14352#issuecomment-152482688).
</comment><comment author="dakrone" created="2015-10-30T20:27:43Z" id="152643688">What about `force_empty_primary` to indicate that the newly created shard will in fact be empty?
</comment><comment author="nik9000" created="2015-10-30T20:32:09Z" id="152644579">`accept_data_loss`?
</comment><comment author="bleskes" created="2015-10-31T16:03:03Z" id="152745429">I like the fact that `force_empty_primary` takes ownership of the implication and is clear about what's going to happen. `accept_data_loss` is more lenient and says I'm willing, if needed to run the risk. IMHO it's too subtle. 
</comment><comment author="clintongormley" created="2016-01-26T10:23:50Z" id="174944908">Closed by #15708
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>typo error: adress -&gt; address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14351</link><project id="" key="" /><description>typo error: adress -&gt; address
</description><key id="114016419">14351</key><summary>typo error: adress -&gt; address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmferrer</reporter><labels><label>docs</label></labels><created>2015-10-29T10:01:10Z</created><updated>2015-10-30T09:45:18Z</updated><resolved>2015-10-30T09:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T09:45:18Z" id="152474602">thanks @jmferrer - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bad git tag for version 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14350</link><project id="" key="" /><description>it seems that the git tag for the v2.0.0 release is bad. After a git clone and checkout, the pom.xml indicates 2.0.0-SNAPSHOT as you can see in https://github.com/elastic/elasticsearch/blob/v2.0.0/pom.xml

Is the root directory has changed in this version ? This is bad because I have a build system using directly the sources from github.
</description><key id="114011885">14350</key><summary>Bad git tag for version 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raphz</reporter><labels><label>build</label><label>discuss</label></labels><created>2015-10-29T09:32:58Z</created><updated>2015-12-14T11:48:22Z</updated><resolved>2015-12-14T11:48:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-30T20:31:30Z" id="152644450">+1, it seems bad that we never actually had a commit on the 2.0 branch where the version was 2.0.0, it went from 2.0.0-SNAPSHOT to 2.0.01-SNAPSHOT.

@clintongormley I feel like this should be part of the release process, to actually have a "Release 2.0.0" commit that immediately precedes the tagging. Someone should always be able to do `git checkout v2.0.0` and build elasticsearch-2.0.0.tar.gz
</comment><comment author="clintongormley" created="2015-11-08T21:46:17Z" id="154876667">These days we do an internal release candidate before putting the release live. While we're checking it, the (eg) 2.0 branch can move on.  I think what we can do is to add a commit with the new version and tag that, even though it will be one commit off the (eg) 2.0 branch.
</comment><comment author="raphz" created="2015-11-25T08:00:15Z" id="159528669">There is always a release problem. The same thing has occured again with the version 2.1.0 the version is still 2.1.0-SNAPSHOT instead of 2.1.0.
</comment><comment author="clintongormley" created="2015-11-28T17:12:10Z" id="160321424">Why is this a problem?  Part of the release process is to update the version to not include -SNAPSHOT (which happens in dev-tools/prepare_release_candidate.py).  So we're tagging the SHA that is used to build the internal release candidate which gets chosen to be made public.  

What we could do is to extract the code which changes the version into a standalone script.
</comment><comment author="dadoonet" created="2015-11-28T17:24:30Z" id="160322061">It is a problem to me. If you want to build yourself a version you basically get the tagged version and build.
Here you end up getting a -SNAPSHOT version which is incorrect.
</comment><comment author="clintongormley" created="2015-11-28T17:27:54Z" id="160322647">@dadoonet not if part of the build process is to do the version change.
</comment><comment author="dadoonet" created="2015-11-28T17:41:49Z" id="160323548">As a end user, building a version is running mvn package. 

It generates here a wrong artifact (wrong name).
</comment><comment author="sibay" created="2015-12-14T09:28:14Z" id="164388080">Any news?
I have the same problem. I wanted to build the source jar of version 2.0.0. 

```
git checkout vx.y.z
mvn clean source:jar install
```

works flawless for (eg) v1.7.3 but not for v2.0.0. I can remove the SNAPSHOT from the version number manually, but still don't know if i am building the 2.0 release or something "near" this release.

When i do a 

```
git checkout v2.0.0 
```

what is the next step to build the 2.0.0 version?
</comment><comment author="clintongormley" created="2015-12-14T11:48:22Z" id="164417822">&gt; but still don't know if i am building the 2.0 release or something "near" this release.

This is exactly the problem.  Even if you build from exactly the same commit (including the version changes) you cannot be sure that you are building Elasticsearch in the same way.  You may have some significant differences in your build environment which changes the end result.

We will not longer be committing the non -SNAPSHOT versions to the repository.  Dropping -SNAPSHOT is part of the release process only, and all it does is:
- update the POM's to remove -SNAPSHOT
- update Version.java to set the snapshot flag to false on the appropriate version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel 2.0 install error, FileNotFound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14349</link><project id="" key="" /><description>Hi, 
I follow the [Marvel guide page](https://www.elastic.co/downloads/marvel) to install Marvel 2.0. when I run the command: `bin/plugin install marvel-agent`, I got the error as follow: 

```
&#10140;  elasticsearch-2.0.0-rc1  bin/plugin install marvel-agent --verbose
-&gt; Installing marvel-agent...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.0.0-rc1/marvel-agent-2.0.0-rc1.zip ...
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.0.0-rc1/marvel-agent-2.0.0-rc1.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.0.0-rc1/marvel-agent-2.0.0-rc1.zip];
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```

Please help. Thanks.
</description><key id="113972581">14349</key><summary>Marvel 2.0 install error, FileNotFound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhaozhiming</reporter><labels /><created>2015-10-29T03:38:13Z</created><updated>2015-10-29T04:36:19Z</updated><resolved>2015-10-29T04:36:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-10-29T04:36:19Z" id="152073874">Hi,
We didn't release Marvel for ES 2.0.0-rc1. You should upgrade elasticsearch 2.0.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>jar hell in test classpath when running with JDK 1.8.0_66 on OS X (ant-javafx.jar, packager.jar)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14348</link><project id="" key="" /><description>Trying to run a test that extends `ESIntegTestCase` with Elasticsearch 2.0.0, getting this:

```
java.lang.RuntimeException: found jar hell in test classpath
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:63)
    at org.elasticsearch.test.ESTestCase.&lt;clinit&gt;(ESTestCase.java:106)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$1.run(RandomizedRunner.java:573)
Caused by: java.lang.IllegalStateException: jar hell!
class: jdk.packager.services.UserJvmOptionsService
jar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/ant-javafx.jar
jar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/packager.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:267)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:185)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:86)
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:61)
    ... 4 more
```

Not sure I can do much about that.
</description><key id="113961444">14348</key><summary>jar hell in test classpath when running with JDK 1.8.0_66 on OS X (ant-javafx.jar, packager.jar)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinst</reporter><labels /><created>2015-10-29T01:48:09Z</created><updated>2016-07-20T15:13:21Z</updated><resolved>2015-10-29T01:53:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-29T01:53:04Z" id="152049922">You have to fix your IDE config: https://github.com/elastic/elasticsearch/pull/13465
</comment><comment author="robinst" created="2015-10-29T07:02:17Z" id="152098655">Thanks. I removed "ant-javafx.jar" from the configured JRE classpath, works now.

By the way, I wouldn't be surprised if you guys got more bug reports about this, because that's really weird (and the first time a library forced me to do that).
</comment><comment author="nik9000" created="2015-10-29T12:22:46Z" id="152162461">Yeah, I suspect we will. Jarhell checks are too useful to give up though.
They detect all kinds of "fun" ways we've seen things broken.

I could see some extra help on these checks because they are much more
likely to be hit by new contributors, exactly the people who we want to
encourage and the people who are going to have the hardest time figuring
out the issue on their own.
On Oct 29, 2015 3:02 AM, "Robin Stocker" notifications@github.com wrote:

&gt; Thanks. I removed "ant-javafx.jar" from the configured JRE classpath,
&gt; works now.
&gt; 
&gt; By the way, I wouldn't be surprised if you guys got more bug reports about
&gt; this, because that's really weird (and the first time a library forced me
&gt; to do that).
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14348#issuecomment-152098655
&gt; .
</comment><comment author="rmuir" created="2015-10-29T14:52:10Z" id="152204130">Everyone loves intellij, but nobody submits bugs to them for their clearly broken configuration.

So I think its good if it confuses people, at some point it will encourage someone to fix the damn thing. I don't care about intellij, so it will not be me :)
</comment><comment author="jprante" created="2015-12-10T13:40:09Z" id="163621325">FYI same config works here flawlessly

```
Jorg-Prantes-MacBook-Pro:~ joerg$ uname -a
Darwin Jorg-Prantes-MacBook-Pro.local 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64
Jorg-Prantes-MacBook-Pro:~ joerg$ java -version
java version "1.8.0_66"
Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
Jorg-Prantes-MacBook-Pro:~ joerg$ ls -l  /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/ant-javafx.jar
-rwxrwxr-x  1 root  wheel  1165611  6 Okt 22:59 /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/ant-javafx.jar
Jorg-Prantes-MacBook-Pro:~ joerg$ ls -l  /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/packager.jar
-rwxrwxr-x  1 root  wheel  4646  6 Okt 22:59 /Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/lib/packager.jar
Jorg-Prantes-MacBook-Pro:~ joerg$ cd ~es/elasticsearch-2.1.0
Jorg-Prantes-MacBook-Pro:elasticsearch-2.1.0 joerg$ ./bin/elasticsearch
[2015-12-10 14:37:28,445][INFO ][node                     ] [Kiden Nixon] version[2.1.0], pid[83532], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-12-10 14:37:28,446][INFO ][node                     ] [Kiden Nixon] initializing ...
[2015-12-10 14:37:28,495][INFO ][plugins                  ] [Kiden Nixon] loaded [], sites []
[2015-12-10 14:37:28,517][INFO ][env                      ] [Kiden Nixon] using [1] data paths, mounts [[/ (/dev/disk0s2)]], net usable_space [336.9gb], net total_space [931gb], spins? [unknown], types [hfs]
[2015-12-10 14:37:30,217][INFO ][node                     ] [Kiden Nixon] initialized
[2015-12-10 14:37:30,218][INFO ][node                     ] [Kiden Nixon] starting ...
[2015-12-10 14:37:30,274][INFO ][transport                ] [Kiden Nixon] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[fe80::1]:9300}, {[::1]:9300}
[2015-12-10 14:37:30,280][INFO ][discovery                ] [Kiden Nixon] elasticsearch/sr2IKPk7QnizIRE-2yquZQ
[2015-12-10 14:37:33,305][INFO ][cluster.service          ] [Kiden Nixon] new_master {Kiden Nixon}{sr2IKPk7QnizIRE-2yquZQ}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-12-10 14:37:33,317][INFO ][http                     ] [Kiden Nixon] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[fe80::1]:9200}, {[::1]:9200}
[2015-12-10 14:37:33,317][INFO ][node                     ] [Kiden Nixon] started
[2015-12-10 14:37:33,344][INFO ][gateway                  ] [Kiden Nixon] recovered [1] indices into cluster_state
```
</comment><comment author="rjernst" created="2015-12-10T19:13:21Z" id="163722163">@jprante That is because java (which bin/elasticsearch invokes) does not just put all of its jars in jdk and jre onto the bootclasspath. Intellij does, which is wrong.
</comment><comment author="jprante" created="2015-12-10T21:28:28Z" id="163754632">After some digging, I found this error in IntelliJ when selecting "Run" from the "Run" menu with the green triangle. What a strange way to start ES.

It's not the boot class path, but the ordinary class path, which can be configured in "File -&gt; Project Structure -&gt; SDKs -&gt; 1.8 -&gt; Classpath".

After editing configuration in _Run&gt;Edit configurations_ by setting VM options to `-Des.path.home=/var/tmp` and program argument to `start` it is indeed possible to select _Run&gt;Run 'Elasticsearch'_ and execute a node without error ... 

How good that I'm used to console and Maven/Gradle, which IntelliJ supports very well.
</comment><comment author="bonitao" created="2015-12-29T02:19:14Z" id="167700816">I have been trying to upgrade to elasticsearch 2.0, but the jar hell check is preventing me from doing so. From #13404, it seems that there are no plans to make it optional.

However, I fail to see how it is possible to write tests using ESIntegTestCase in complex environment with the jar hell check enabled. In production I am usually deploying an uber jar, so everything works fine, since only one instance of a class goes in the flattened jar.

But when running tests from maven, there isn't much I can do to avoid the jar hell, because the duplicated classes are in the dependency jars and the classloader is deciding at runtime which class to pick (as maven shade, which I use to build the uber jar, has decided offline). It seems that others in stackoverflow are having the same problem: http://stackoverflow.com/questions/33975807/elasticsearch-jar-hell-error.

I used maven duplicate finder plugin to take a deep look in my duplicated jars to see what I could do, and although I could get rid of many of them, there are still some that seem impossible to fix. See below just for a glimpse of the problem:

```
mvn clean duplicate-finder:check
...
[WARNING] Found duplicate and different classes in [au.com.bytecode:opencsv:2.4, net.sf.opencsv:opencsv:2.3]:
[WARNING]   au.com.bytecode.opencsv.CSVReader
[WARNING]   au.com.bytecode.opencsv.CSVWriter
[WARNING]   au.com.bytecode.opencsv.bean.HeaderColumnNameTranslateMappingStrategy
[WARNING] Found duplicate and different classes in [com.google.guava:guava:18.0, org.apache.spark:spark-network-common_2.10:1.6.0]:
[WARNING]   com.google.common.base.Absent
[WARNING]   com.google.common.base.Function
[WARNING]   com.google.common.base.Optional
[WARNING]   com.google.common.base.Present
[WARNING]   com.google.common.base.Supplier
...
```

Unfortunately, the reality is that upstream dependencies can be packaged in some crazy ways (sometimes for good reasons, sometimes not), and as far as I know, when in maven or intellij, different from deploy jars where I can use maven-shade or others, there isn't an "offline" way for me to tell the system how to get rid of the jar hell. 

After struggling this for quite some time now, the only way I out I see is to keep my own locally hacked elasticsearch, which is a solution I really don't like. Any other suggestions on how to work around this? 

If there is a true solution, I will be happy to hear, but right now I have 45 dependencies contributing to the jar hell, and I have no idea how to fix them.
</comment><comment author="jprante" created="2015-12-29T08:27:59Z" id="167744228">@bonitao I think you should take one step back and take a fresh breath. Wow, ES gives you the opportunity to clean up the dependencies of your project.

Are you authoring a plugin? Or do you just need an embedded ES client?

Beside wrestling with Maven, you could use Gradle for your project. There is a learning curve but the classpath configuration abilities are more flexible than in Maven. The ES team is switching to Gradle (again) because Maven is not flexible enough.

For an example, see my integration test at https://github.com/jprante/elasticsearch-langdetect/blob/master/build.gradle which can test plugin loading- it does not require the ES test framework.

Before thinking about a "hacked" Elasticsearch (which is a good thing), you could also try to just change the Elasticsearch build. From above it seems Google Guava has conflict with Spark. So it may be an alternative to shadow dependencides, either in your project, or in Spark, or in Elasticsearch. For example, there is a great Gradle shadow plugin at https://github.com/johnrengelman/shadow
</comment><comment author="bonitao" created="2015-12-29T12:23:40Z" id="167779612">Hi @jprante,

Thanks for the response. A month ago, when first tried to upgrade to ES 2.x, I have actually tried to approach this as you said: as an opportunity to clean up my dependencies. But it did not work out, and I do not believe it will. Let me try to explain why.

I am not authoring a plugin. I am building a restful service that has an embedded ES client and an extract/load/transform pipeline, which also talks with ES. The code does a bunch of things, like doing search queries, doing snapshot/restores through the admin api, and doing bulk indexing with elasticsearch-hadoop-mr. This all works with ES 1.7.x, and from what I have seen, it also works in ES 2.x after I build an uber jar. The only thing that fails are my unit tests in ES 2.x, because in the test environment my classpath is built from the individual dependency jars.

The code is built with maven, but I also have a SBT setup for it. I have a passing knowledge of gradle, but as I understand, none of these tools really support the idea of creating a test classpath by unzipping and manipulating dependencies jars. Yes, they all have tricks like the one you did in https://github.com/jprante/elasticsearch-langdetect/blob/master/build.gradle#L121 to remove full jars from the classpath, but not for manipulating the contents of the individual classpaths for tests.

It is of course possible, after all, both the maven shade plugin and the gradle shadow plugin you linked, solve that exact problem.  But these are tied to the release steps of the build, and as far as I know, there is not a natural way to run my unit tests with a classpath pointing solely to an uber jar. So, all the jar hell check is buying is forcing me into moving my unit tests into integration tests, and forcing the integration tests to work out of the flat jar (which I still need to figure out how to do).

As for fixing the jar hell in the dependencies, it is just not possible. I have already fixed many simple cases, like common-beanutils and netty, which is brought as a uber jar and individual deps in my build, and for smaller libraries, I have worked on upstream fixes to their pom, but I still have dozens of conflicts (all easily seen with duplicate finder), and some  are really beyond my league. I would love to see anyone that has been able to run unit tests with ES 2.x and spark together. Direct shadowing is of no help here, because the libraries are being used directly in the tests. I would need to shadow guava within spark, and depend on the resulting jar instead of the original spark libraries. 

And the spark issue is just one of many. For example, hadoop has some fake conflicts with itself: 

```
[WARNING] Found duplicate and different classes in [org.apache.hadoop:hadoop-yarn-client:2.7.1, org.apache.hadoop:hadoop-yarn-common:2.7.1]:
[WARNING]   org.apache.hadoop.yarn.client.api.impl.package-info
[WARNING]   org.apache.hadoop.yarn.client.api.package-info
[WARNING] Found duplicate and different classes in [org.apache.hadoop:hadoop-yarn-api:2.7.1, org.apache.hadoop:hadoop-yarn-common:2.7.1]:
[WARNING]   org.apache.hadoop.yarn.factories.package-info
[WARNING]   org.apache.hadoop.yarn.factory.providers.package-info
[WARNING]   org.apache.hadoop.yarn.util.package-info
[
```

And so does spark: 

```
[WARNING] Found duplicate and different classes in [org.apache.spark:spark-catalyst_2.10:1.5.0, org.apache.spark:spark-core_2.10:1.5.0, org.apache.spark:spark-graphx_2.10:1.5.0, org.apache.spark:spark-launcher_2.10:1.5.0, org.apache.spark:spark-mllib_2.10:1.5.0, org.apache.spark:spark-network-common_2.10:1.5.0, org.apache.spark:spark-network-shuffle_2.10:1.5.0, org.apache.spark:spark-repl_2.10:1.5.0, org.apache.spark:spark-sql_2.10:1.5.0, org.apache.spark:spark-streaming_2.10:1.5.0, org.apache.spark:spark-unsafe_2.10:1.5.0, org.spark-project.spark:unused:1.0.0]:
[WARNING]   org.apache.spark.unused.UnusedStubClass
```

And scala-lang bundles stuff it probably shouldn't:

```
[WARNING] Found duplicate and different classes in [org.fusesource.jansi:jansi:1.4, org.scala-lang:jline:2.10.4]:
[WARNING]   org.fusesource.jansi.Ansi
[WARNING]   org.fusesource.jansi.AnsiConsole
[
```

And my dependencies bring conflicting opencsv versions:

```
[WARNING] Found duplicate and different classes in [au.com.bytecode:opencsv:2.4, net.sf.opencsv:opencsv:2.3]:
[WARNING]   au.com.bytecode.opencsv.CSVReader
[WARNING]   au.com.bytecode.opencsv.CSVWriter
[WARNING]   au.com.bytecode.opencsv.bean.HeaderColumnNameTranslateMappingStrategy
```

So, I will need to solve each one of those to be able to run elasticsearch tests, even though I already have a solution for production, through the creation of uber jar. So, all I really need is a flag to disable the jar hell check in unit tests. With that flag, I could even choose to run the tests again in the integration phase with the flattened jar, but that is later in the development cycle, and slower.

I hope that exposition of the problem helps. ES is a great product, and I will definitively hack the source to get my tests to run if needed, but it seems to me that many people will face similar issues, and it would be best to leave a system property toggle for when fixing the jar hell is not feasible. Again, in production there is nothing to fix if you are running a flattened jar (damage is already done), and if I understand it right, people not running tests won't even hit the check. 
</comment><comment author="jprante" created="2015-12-29T12:44:39Z" id="167782110">@bonitao I know this is the wrong place to discuss further because it may be going off-topic. You can email me privately (see my github profile).  The only thing I like to understand is why it is not possible to assemble a cleaned up uberjar of your project by exploding all the various messy dependencies you have mentioned here, except the ES dependencies, and pass the uberjar later to the classpath of an ES integration test.
</comment><comment author="cff3" created="2015-12-31T14:17:46Z" id="168202559">@jprante I would really appreciate if you could continue the discussion publicly. I'm facing the same issues as @bonitao does (especially the issue with org.apache.spark.unused.UnusedStubClass) and I would be glad to see whether there is a good down-to-earth solution for the problem. 
Fighting jar hell is certainly worth a lot of efforts and it's really great that elastic offers the JarHell checker. It would be event greater if people could decide on their own whether it is more important for them to cope with jar hell or to concentrate on writing and running tests. I would argue that this decision is highly context specific.
Currently I'm hacking around this problem by overriding org.elasticsearch.bootstrap.BootstrapForTesting.ensureInitialized().
I do this by implementing org.elasticsearch.bootstrap.BootstrapForTesting.java in my source path 

&lt;pre&gt;
package org.elasticsearch.bootstrap;
public class BootstrapForTesting {
    public static void ensureInitialized(){}
}
&lt;/pre&gt;

which basically means I'm fighting jar hell fighting with jar hell. Far from ideal.
</comment><comment author="PeterLappo" created="2016-01-05T15:02:03Z" id="169026215">@jprante I'm facing the same issue as @bonitao and @cff3 (as are others looking at other issues raised on this project) and its taking a lot of time to sort out and all we are trying to do is write an integration test.

Surely the obvious thing is to have a system property to disable the check with a warning if duplicates exist. After all there is no guarantee that the duplicate class may actually be used by the code. The default can still be crash and burn. And if even if a duplicate was used and the "wrong" class was loaded isn't it a case of caveat emptor ("let the buyer beware" ) ?
</comment><comment author="rjernst" created="2016-01-05T23:10:50Z" id="169164243">@PeterLappo The test framework is meant to check elasticsearch works, and we have done a lot of work to ensure we know exactly which classes are being loaded (ie the jarhell work). If you are just using elasticsearch as a service, a much better integration test is using a real elasticsearch node (and the test setup already allows this, see the many QA tests or integration tests for plugins in the elasticsearch repo, and the discussion in #11932). If you are embedding elasticsearch, then it is better for you to have your own test base classes which initializes an embedded node as you would (and constructing a Node directly does not do a jarhell check).
</comment><comment author="s1monw" created="2016-01-06T08:25:38Z" id="169265234">&gt; Surely the obvious thing is to have a system property to disable the check with a warning if duplicates exist. After all there is no guarantee that the duplicate class may actually be used by the code. The default can still be crash and burn. And if even if a duplicate was used and the "wrong" class was loaded isn't it a case of caveat emptor ("let the buyer beware" ) ?

I think 2 years ago I would have agreed with you. Now after all the things that i have seen working on es core for a long time I can tell you any kind of opt-out and or leniency is the root of all evil. The only thing that I am considering as a temporary solution until we have a good and released test-fixture for a real integ test is to add an opt-out to the jar hell check in the test bootstrap. On the real node I am sorry we won't go there to protect us and others from even getting to the point where this can be trouble.
</comment><comment author="PeterLappo" created="2016-01-06T10:24:51Z" id="169286483">@rjernst @s1monw 
Hi Guys,
I understand that you may want to use JarHell to ensure you have a clean build to test ES or ES plugins, but my use case is different. 

I have some client code written with Jest that writes to ES and I want to create a test that proves my code works. I have tests working but they relied on a local instance of ES running which is not ideal as I'd need to ensure the tests run on integration servers or other peoples machines. Our code is not so easy to clean as there are several conflicts that JarHell detects which are beyond my control, e.g. sl4j in dependent libraries. So I would like to spin up an instance of ES and test my Jest client logic and verify the data exists as I expect in ES. At the end of the test I then want to close down ES and clean up any data. 

Ideally I want
- an in-memory version of ES rather than storing data on disk; 
- as Jest uses http:port a random unused port;
- I don't want to inherit from anything as I'm using Scala test which has its own base class that I inherit from (its not a trait which I must admit is sub-optimal).

Obviously I can start a ES in a separate process during my test and do all the setup myself but I was hoping something may exist already and maybe an example that I can copy to get me started? 
</comment><comment author="rjernst" created="2016-01-06T20:21:15Z" id="169450096">@PeterLappo Having external services for integration tests is exactly the point of us adding test fixtures (in master). See #15561. For 2.x, you can look at how the [client tests](https://github.com/elastic/elasticsearch/tree/2.x/qa/smoke-test-client) start an ES cluster.
</comment><comment author="PeterLappo" created="2016-01-06T23:01:43Z" id="169489761">Thanks will take a look
</comment><comment author="anjithp" created="2016-01-14T10:12:35Z" id="171594875">I'm also facing the same problem as @bonitao.  Is there any solution - other than modifying the elasticsearch source code - to this issue? Your help will be much appreciated.
</comment><comment author="PeterLappo" created="2016-01-14T13:29:47Z" id="171645189">We gave up and wrote a script that started a ES instance, ran the tests and then shutdown ES

&gt; On 14 Jan 2016, at 10:13, Anjith Kumar Paila notifications@github.com wrote:
&gt; 
&gt; I'm also facing the same problem as @bonitao https://github.com/bonitao. Is there any solution - other than modifying the elasticsearch source code - to this issue? Your help will be much appreciated.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub https://github.com/elastic/elasticsearch/issues/14348#issuecomment-171594875.
</comment><comment author="sawyercade" created="2016-04-07T22:40:37Z" id="207122904">Echoing what others have said here - we build an uber jar of ES with some shaded/relocated dependencies, mostly due to version conflicts between ES and our project's dependencies. We would LOVE to be able to use ESIntegTestCase to test our mappings, transforms, index requests, and queries, but at the moment are unable to do so without shading the entire test-jar, which raises a whole host of other issues.

@s1monw any chance of adding that opt-out to the jar hell check in the test bootstrap soon?
</comment><comment author="ndtreviv" created="2016-07-20T14:50:02Z" id="233972968">I stumbled across this today, and am now also fighting jar hell in transient dependencies.

I get the sentiment re: jar hell, I really do, and I want to be on board. At the moment, I can't use elasticsearch with Apache Storm and Flux, because Flux has commons-cli as a dependency, and is "overriding" class behaviour with it's own jar hell a la @cff3:

```
Caused by: java.lang.IllegalStateException: jar hell!
class: org.apache.commons.cli.AlreadySelectedException
jar1: /Users/me/.m2/repository/org/apache/storm/flux-core/1.0.1/flux-core-1.0.1.jar
jar2: /Users/me/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar
```

&lt;img width="364" alt="screen shot 2016-07-20 at 15 40 17" src="https://cloud.githubusercontent.com/assets/1530653/16990580/60fff344-4e90-11e6-91d4-36298fe70432.png"&gt;
&lt;img width="418" alt="screen shot 2016-07-20 at 15 40 32" src="https://cloud.githubusercontent.com/assets/1530653/16990585/63b8adb0-4e90-11e6-925e-b2dc2fe7068b.png"&gt;

I can't do anything about this. It's holding up my development. 
Like I said - I agree with the sentiment, but it's totally knee-capping my work, it's not my fault, and I'm powerless to do anything about it. This makes me sad. Very sad.

How would others solve this?
</comment><comment author="dadoonet" created="2016-07-20T15:13:21Z" id="233980577">@ndtreviv May be you could find some ideas here: https://www.elastic.co/blog/to-shade-or-not-to-shade. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0 fails to start after install Marvel 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14347</link><project id="" key="" /><description>Hi

I've got an odd issue, I'm running a vanilla install on Elasticsearch 2.0 pulled from the deb repository, it start after the initial installation, but then I installed Marvel and now seeing the following

[2015-10-28 23:11:13,025][INFO ][bootstrap                ] es.default.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.

The only config file I have for Elasticsearch is /etc/elasticsearch/elasticsearch.yml, I've also tried uninstall Marvel and still seeing the same issue.

Cheers
</description><key id="113945844">14347</key><summary>Elasticsearch 2.0 fails to start after install Marvel 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">projx</reporter><labels /><created>2015-10-28T23:16:16Z</created><updated>2016-06-10T16:09:31Z</updated><resolved>2015-10-30T09:46:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-10-29T08:43:27Z" id="152113590">I can't reproduce the issue on Ubuntu 14.04, can you please tell us which OS you use?

Also, this message indicates that a deprecated setting `es.default.config` has been configured but elasticsearch 2.0 does not use it anymore and will refuse to start. Maybe a `grep` on your configuration files (/etc/elasticsearch/elasticsearch.yml /etc/default/elasticsearch)may help to locate where this setting comes from.
</comment><comment author="vandinhchuong" created="2015-10-29T10:29:56Z" id="152138759">Hi, I have also the same issue, I use Ubuntu 15.04 and elasticsearch 2.0 with the following log

[2015-10-29 17:26:40,411][INFO ][bootstrap                ] es.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.

Another question: I don't see node.master &amp; node.data in config/elasticsearch.yml, Can I use them in elasticsearch 2.0?
</comment><comment author="clintongormley" created="2015-10-30T09:46:58Z" id="152474888">@vandinhchuong @projx It sounds like you have some legacy configuration files lying around.

&gt; Another question: I don't see node.master &amp; node.data in config/elasticsearch.yml, Can I use them in elasticsearch 2.0?

Yes.  We just reduced the config file to show only the most important bits.
</comment><comment author="ssoulless" created="2015-11-02T00:45:48Z" id="152883734">Hi I just can't start elastic search, I did a new fresh install with home-brew and when I try to start with the command:

```
elasticsearch --config=/usr/local/opt/elasticsearch/config/elasticsearch.yml
```

it show the message: es.config is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.

Is there a quick fix I can use for just run elasticsearch?? I need it quickly help.
</comment><comment author="tlrx" created="2015-11-02T08:06:02Z" id="152946882">You can use 

```
bin/elasticsearch --path.conf=/tmp/config/
```

where `--path.conf` refers to a directory that contains the `elasticsearch.yml` file and all other  logging, plugins etc configurations files.

With elasticsearch 1.x, it is possible to start elasticsearch and set the configuration _file_ with parameters like `--config`, `-Des.config`, `-Delasticsearch.config` but it is also possible to configure the configuration _directory_ with `--path.conf`, `-Des.path.conf` or `CONF_DIR`... It requires a complex logic to manage which parameter takes the precedence and often leads to mistakes.

That's why it has been removed in 2.0. If you want to know more, you can have a look at #13772.
</comment><comment author="ericsync" created="2015-12-02T16:34:23Z" id="161357139">Hi, we have an Elasticsearch (v1.7) cluster with  multiple nodes instances on each physical machines (3 box). We're under CentOS 7. Each ES instances had its own configuration file and environments variables. The conf file to use is specified in the service section of each instances startup script (/usr/lib/systemd/system/elasticsearch-node1.service, node2.service, etc.) by using the es.config parameter. We had to tweak a bit the systemd scripts but it does the job.

Now ,we are settings up a test environment to migrate under 2.0. How can we have multiple instances running on the same system with different conf files with 2.0 if we cannot specify a conf file? Does it mean that only one instance of ES is supported on a single box? At this point, it sounds that our migration is compromised. Thanks you for any help!
</comment><comment author="rjernst" created="2015-12-02T16:40:45Z" id="161358961">&gt; Does it mean that only one instance of ES is supported on a single box?

No, it means that you need to have separate config dirs for each instance.
</comment><comment author="ericsync" created="2015-12-02T17:10:01Z" id="161367442">Thank you for the answer. It works.
I now have a /etc/elasticsearch-node{n} and /var/log/elasticsearch-node{n} directories for each instances.
Had to chown elasticsearch. on these directories and create a service startup script for each instances in /usr/lib/systemd/system, and tweak the service startup scripts settings to point on the right dir.
Trivial actually. 
Sorry for not thinking about using different directory. It's the end of the day, I'm getting tired I guess...
</comment><comment author="rjernst" created="2015-12-02T17:12:57Z" id="161368231">@ericsync No problem!
</comment><comment author="coreation" created="2016-06-10T15:57:41Z" id="225222573">@clintongormley my error was (after adding a log_daemon in the init.d file for elasticsearch) was that apt-get apparently had updated my elasticsearch from 2.3.2 to 2.3.3 which broke my delete-by-query plugin. At this point I'd think that "just doing an apt-get update" is kind of dangerous in this regard? What's the best way to handle updates in this situation?

edit: does apt-get update throw away your entire elasticsearch data files as well... After the update everything's gone. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContent Builder doesn't check for null keys in maps. It should raise an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14346</link><project id="" key="" /><description>There should be a null check in https://github.com/elastic/elasticsearch/blob/148265bd164cd5a614cd020fb480d5974f523d81/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java#L1237

It should probably raise a unique exception in this event. We encountered this in Logstash with some users running custom plugins.

Jackson also does not perform a null check, leading to a confusing NPE in some Jackson codes.
</description><key id="113932064">14346</key><summary>XContent Builder doesn't check for null keys in maps. It should raise an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">andrewvc</reporter><labels><label>:REST</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-10-28T21:46:24Z</created><updated>2015-12-16T16:04:34Z</updated><resolved>2015-12-16T16:04:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-30T09:41:54Z" id="152474081">@javanna could you look at this one please
</comment><comment author="andrewvc" created="2015-12-11T23:12:33Z" id="164074122">@javanna mind taking a look here?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update GeoPoint FieldData for GeoPointV2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14345</link><project id="" key="" /><description>This is GeoPointV2 PR part 1. It adds an abstraction layer to GeoPoint FieldData for supporting the new GeoPointField type in Lucene 5.4 along with backwards compatibility for legacy geo_point field types. This abstraction is necessary for 2.x only and will not be carried forward in 3.0.
</description><key id="113924037">14345</key><summary>Update GeoPoint FieldData for GeoPointV2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Fielddata</label><label>:Geo</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T21:18:50Z</created><updated>2016-01-22T18:32:14Z</updated><resolved>2015-11-09T17:52:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-11-04T12:15:25Z" id="153702309">@mikemccand @colings86 can you both please take a look at this?
</comment><comment author="colings86" created="2015-11-04T14:59:27Z" id="153753886">@nknize Not sure how feasible it is but it would be good to have a test which ensures that geopoint doc values work correctly under the following scenarios:
- pre-2.2 index with only pre 2.2 segments
- pre 2.2 index with a mix of pre and post 2.2. segments
- post 2.2 index

Maybe we could make a backwards compatibility test work to do this?
</comment><comment author="nknize" created="2015-11-05T17:22:20Z" id="154128037">Addressed review comments. Once these current 3 PRs are merged I will have a final PR that cuts over to the GeoPointFieldV2. That PR will finish adding backcompat testing to existing unit and integration tests. I've also opened a new issue #14562 to address @colings86 point for having better backcompat testing for FieldData.
</comment><comment author="nknize" created="2015-11-09T15:15:26Z" id="155092344">@colings86 @mikemccand this should be ready. Let me know if you have any objections to the updates. If there are no objections I'd like to cut over to GeoPointV2 at some point today.
</comment><comment author="colings86" created="2015-11-09T15:47:12Z" id="155101039">This LGTM although I'll leave the final LGTM to @mikemccand :)
</comment><comment author="mikemccand" created="2015-11-09T16:03:36Z" id="155106420">LGTM2!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>+ in delimiter not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14344</link><project id="" key="" /><description>Trying to index and find the token "c++" in text. 
In settings I've tried escaping the +, using unicode, and doing a double ++. Its still returning everything that has a "C" in it as a matching results, as if the + was not considered an alphanum.

See my settings below: 

```
settings: {
  analysis: {
    filter: {
      symbol_filter: {
        type: "word_delimiter",
        type_table: ["# =&gt; ALPHANUM", "@ =&gt; ALPHANUM", "/ =&gt; ALPHANUM", "$ =&gt; ALPHANUM", "&amp; =&gt; ALPHANUM", "+ =&gt; ALPHANUM", "- =&gt; ALPHANUM"]
      }
    },
    analyzer: {
      symbol_analyzer: {
        type: "custom",
        tokenizer: "standard",
        filter: ["lowercase", "symbol_filter"]
      }
    }
  }
}
```
</description><key id="113909593">14344</key><summary>+ in delimiter not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bigterminal</reporter><labels /><created>2015-10-28T19:59:09Z</created><updated>2015-10-29T14:40:25Z</updated><resolved>2015-10-29T14:40:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-10-29T14:40:25Z" id="152200660">Hi @bigterminal, 
Filters, like the `word_delimiter` in your example, are working on a stream of tokens from the tokenizer. Since you are using the standard tokenizer, this will already remove most punctuation, including the `+` character. The whitespace tokenizer should help. Please ask questions about usage in the forums at https://discuss.elastic.co/ in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High warmer time and threads reported when no warmers exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14343</link><project id="" key="" /><description>This is happening on an elasticsearch cluster with a relatively large number of indices (~2500), where **zero** indices have warmers defined. I most recently noticed that on a node that had only been running for 4 hours, its node stats reported 3.8h of processing time spent on warmers. Is there some other operation other than true index warmers that are being reported in this category? On this same node, the thread pool stats reported `completed` values of about 1000:1 for warmer threads vs other operations (search, generic, etc).

Relevant sections from the _indices_ and _thread_pool_ sections of `_nodes/stats`:

``` json
"warmer": {
      "current": 0,
      "total": 34628,
      "total_time": "3.8h",
      "total_time_in_millis": 13805427
    }
```

[full indices section of _nodes/stats](https://gist.github.com/allthedrones/f27b1e1471c031fe7c52#file-node-stats-indexes-json)

``` json
"warmer": {
      "threads": 4,
      "queue": 0,
      "active": 0,
      "rejected": 0,
      "largest": 4,
      "completed": 24963774
    }
```

[full thread_pool section of _nodes/stats](https://gist.github.com/allthedrones/f27b1e1471c031fe7c52#file-node-stats-threadpool-json)
</description><key id="113905766">14343</key><summary>High warmer time and threads reported when no warmers exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">allthedrones</reporter><labels /><created>2015-10-28T19:38:05Z</created><updated>2016-03-26T17:49:59Z</updated><resolved>2015-10-28T21:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-28T21:57:37Z" id="152007517">This is likely from the top warmer. If you want to verify, you enable trace logging for `index.warmer` and look for `top warming` in the logs.

Have you verified that this is harming your performance in any meaningful way? If you have, you can disable this by setting `index.warmer.enabled` to `false`.You can set this cluster wide, or on an individual index (it sounds like you want this cluster wide). Be sure to verify that turning it off doesn't have a negative impact on the typical queries that hit your system! You should be able to verify that this had the desired effect against the warmer thread pool by requesting

```
_cat/thread_pool\?v\&amp;h=warmer.type,\
warmer.active,\
warmer.size,\
warmer.queue,\
warmer.queueSize,\
warmer.rejected,\
warmer.largest,\
warmer.completed,\
warmer.min,\
warmer.max,\
warmer.keepAlive
```

before changing the setting, then changing the setting, indexing some documents, and then checking the output of the above request again. After `warmer.keepAlive` expires, all of the threads in this thread pool should be terminated.
</comment><comment author="allthedrones" created="2015-10-29T14:29:01Z" id="152197170">Thanks for the clarification. We've noticed some _correlation_ between high warmer activity and very high (~90%) heap, with virtually all of the heap usage promoted up to the "old" pool. Obviously this doesn't necessarily imply a causal link. Is there any documentation on what the top warmer does? That might lead us to some _other_ behavior or activity which might be a more likely culprit in the heap growth. I couldn't find anything by searching for `elasticsearch "top warmer"`. Thanks again for the quick response.
</comment><comment author="allthedrones" created="2015-11-05T21:05:35Z" id="154191633">We were able to verify that disabling warmers (even with no warmers defined) caused heap usage to drop from ~90% to ~30%, with no discernable negative impact to query or indexing performance ... so there is definitely a bug or undocumented performance consideration here. This was cluster wide for 8 data nodes with 28G of heap space, so it's an enormous impact.
</comment><comment author="clintongormley" created="2015-11-09T10:59:12Z" id="155028541">The top warmer is used to warm up data structures that are per shard, not per segment.  eg parent-child global ordinals, any fields with global_ordinals set to eager, etc.

If you're not seeing any performance impact, then perhaps you have mappings like the above which aren't actually being used?
</comment><comment author="ispringer" created="2016-03-26T17:49:59Z" id="201905191">We are running ES 1.7.2 and saw the exact same issue as @allthedrones - unexplained abnormally high CPU and heap usage. All of the hot threads were warmer threads. We turned on trace logging for `index.warmer` and saw that the top warmer was running 2-3 times per second. When we set `index.warmer.enabled` to false globally, the CPU and heap usage quickly dropped to normal levels, and there was no discernible impact on query or indexing performance. In my opinion, this is a very serious bug, and I think this issue should be reopened.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to randomizedtesting 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14342</link><project id="" key="" /><description>This is just released (https://github.com/randomizedtesting/randomizedtesting/blob/master/CHANGES.txt)

Most important is it solves a long running issue (https://issues.apache.org/jira/browse/LUCENE-6478) which prevented us from being able to use `java.security.debug` in tests... this is really important to be able to do!
</description><key id="113897882">14342</key><summary>Upgrade to randomizedtesting 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T18:59:53Z</created><updated>2015-10-30T09:37:38Z</updated><resolved>2015-10-28T19:30:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-28T19:01:28Z" id="151956783">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create custom datatype in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14341</link><project id="" key="" /><description>I haven't found a way to create a custom datatype to reuse in mapping definition.

I would like to have a way to define custom_datatype in the mapping like that :

``` json
{
     "settings" : {
        "number_of_shards" : 1,
        "custom_datatypes": [{
            "string_with_raw": {
              "mapping": {
                "index": "analyzed",
                "omit_norms": true,
                "type": "string",
                "fields": {
                  "raw": {
                    "ignore_above": 256,
                    "index": "not_analyzed",
                    "type": "string"
                  }
                }
              }
            }
         }],
    },
    "mappings" : {
      "my_type": {
        "properties": {
          "field1": {
                "type": "string_with_raw"             
            },
           "field2": {
                "type": "string_with_raw"             
            },
            "field3": {
                "type": "string"             
            }

        }
      }
   }
}
```

For now I have to duplicate for every field : 

``` json
 "mappings": {
      "my_type": {      
        "properties": {
          "message": {
               "index": "analyzed",
                "omit_norms": true,
                "type": "string",
                "fields": {
                  "raw": {
                    "ignore_above": 256,
                    "index": "not_analyzed",
                    "type": "string"
                  }
                }     
            },
           "field2": {
                "index": "analyzed",
                "omit_norms": true,
                "type": "string",
                "fields": {
                  "raw": {
                    "ignore_above": 256,
                    "index": "not_analyzed",
                    "type": "string"
                  }
                }             
            },
            "field3": {
                "type": "string"             
            }
          }
     }
  }
}
```

It could also be used in dynamic mappings:

``` json
{
     "settings" : {
        "number_of_shards" : 1,
        "custom_datatypes": [{
            "string_with_raw": {
              "mapping": {
                "index": "analyzed",
                "omit_norms": true,
                "type": "string",
                "fields": {
                  "raw": {
                    "ignore_above": 256,
                    "index": "not_analyzed",
                    "type": "string"
                  }
                }
              }
            }
         }],
    },
    "mappings" : {
      "my_type": {
         "dynamic_templates": [{
            "message_field": {
              "mapping": {
                "omit_norms": false,
               "type": "string_with_raw"
              },
              "match_mapping_type": "string",
              "match": "*_with_raw"
            }
        }],
        "properties": {
          "field1": {
                "type": "string_with_raw"             
            },
           "field2": {
                "type": "string_with_raw"             
            },
            "field3": {
                "type": "string"             
            }            
        }
      }
   }
}
```

For the dynamic_templates part, the value set in the mapping should override equivalent field defined in the custom datatype.

Is there a way to achieve that at the moment ?
</description><key id="113869569">14341</key><summary>Create custom datatype in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmaynier</reporter><labels /><created>2015-10-28T17:05:06Z</created><updated>2017-07-01T18:20:15Z</updated><resolved>2015-10-28T17:48:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-28T17:48:59Z" id="151928626">There is indeed a way to achieve this. What you want is to create a plugin that registers your custom data type. Currently this means creating a plugin that registers your custom field type.

See an example from the tests [here](https://github.com/elastic/elasticsearch/blob/148265bd164cd5a614cd020fb480d5974f523d81/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapperPlugin.java). Follow through to `ExternalIndexModule` and `ExternalMapper`.

Questions like this are better asked on http://discuss.elastic.co
</comment><comment author="jmaynier" created="2015-10-28T17:56:13Z" id="151932204">Using the plugin to create a completely different datatype like geopoint or ip, for which you need to create custom method to handle range search, etc. make sense.
But I don't think having to create a plugin each time you want to reuse a set of definition for a type is the way to go.  

@rjernst  don't you think ?
</comment><comment author="sciros" created="2016-06-03T03:37:54Z" id="223483337">It does seem like we could benefit from the notion of a reusable type. If we map type "my_type" in the JSON, then it would be handy to be able to say "type":"my_type" in cases where we replicate this mapping.
</comment><comment author="jmaynier" created="2016-06-03T10:04:57Z" id="223541025">@sciros, yes that was the point. ES devs did not take the time to answer so it seems that this requierement is of low importance to them.
</comment><comment author="pszynk" created="2016-11-23T15:14:12Z" id="262541668">+1</comment><comment author="Hronom" created="2017-07-01T18:20:15Z" id="312447820">+1 what the status?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Voting Only Master Node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14340</link><project id="" key="" /><description>The current ideal setup for master nodes is to have 3 master nodes, thus giving high availability (HA) with a quorum of 2 master nodes at any given time.

However, in some cloud regions (and more frequently in internal data centers), this creates a problem because there may only be two available zones to place your nodes. In these scenarios, you do not want to give up HA just to maintain 3 master nodes (to be clear: it's better than not doing it, but it's less than ideal) because it means that you have a lopsided HA environment where one "half" cannot actually survive without the other.

The only real solution to this problem is to have a third zone, even if that third zone is a _slightly_ higher latency (across the globe is not going to work), then plunk a master node into that zone to give a single master node per zone -- thus enabling the actual survivability of any single zone, but not of two zones.

Now, you could do this today with a master node and things would be generally okay except that the master node with the added latency may become the elected master node, which is obviously not ideal.

This is where a voting-only master node improves this setup significantly. If you have a master node that can itself not be elected, but that participates in elections only, then you can have a _tiny_ node (careful with VM size due to automatic network throttling in cloud ecosystems!) that help to survive the loss of any single zone without seriously risking performance.
</description><key id="113866161">14340</key><summary>Voting Only Master Node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Discovery</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2015-10-28T16:50:01Z</created><updated>2016-11-02T15:39:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-28T17:05:01Z" id="151911158">This is definitely on our horizon. We need to do some infra work before we get there though... on it :)
</comment><comment author="bertrandfalguiere" created="2016-06-09T12:48:24Z" id="224884207">I'm new to ES, but won't it be better and maybe technically easier to give "points" to the master eligible nodes? Those points would give priority to the best ranked node to become master.
If zones A and B have high availability, but not zone C, you could give:
"node.master_eligibility" : 3  for your master eligible node A,
"node.master_eligibility" : 2  for your master eligible node B,
"node.master_eligibility" : 1  for your laggy master eligible node C.
If you lose A, B is elected if needed, and vice versa.
It would have have the same effect of avoiding C to become master.

And (new case) if you have a fast A and slow B and C, it means that B and C can take over if the cluster lost A, but as soon as the fast A node is back online, it takes its mastership back.

It probably means changing the election process, but no new node status is needed.

Does it make sense? Again i'm new to all this.
</comment><comment author="bleskes" created="2016-06-09T13:16:44Z" id="224891041">@bertrandfalguiere no worries and welcome to the repo.  

If I read you correctly, you want to have some mechanics to influence master election and indicate you prefer one master over another. Putting all the trickiness of a distributed master election aside (it's not easy to add these things correctly),  the main thing to realize here that you need to have a good working master at any given moment. Otherwise your cluster is in a big risk. So have a slow master and a fast master doesn't make sense - you need two fast masters etc.

PS. We try to keep github for concrete issues. For more discussion-y things we ask people to go to https://discuss.elastic.co
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor Geo utilities to Lucene 5.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14339</link><project id="" key="" /><description>This PR removes all local lucene/XGeo\* classes and refactors to Lucene 5.4 Geo Utility classes.
</description><key id="113865363">14339</key><summary>Refactor Geo utilities to Lucene 5.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>release highlight</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T16:46:33Z</created><updated>2015-10-29T20:29:10Z</updated><resolved>2015-10-28T16:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-10-28T16:50:12Z" id="151906957">LGTM, wonderful!
</comment><comment author="jpountz" created="2015-10-28T17:05:11Z" id="151911207">Wonderful indeed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>marking and sending shard failed due to [failed recovery]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14338</link><project id="" key="" /><description>Getting the following exception after startup in a 2 node cluster using version elasticsearch-1.7.2. Can someone help me understand what these messages really mean and how to resolve it ? 

```
[10/28/15 11:31:57:364 EDT] 00000056 org.elasticsearch.indices.cluster                            W [NODE2] [[xtttenantmaster][0]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [xtttenantmaster][0]: Recovery failed from [NODE1][......][host1][inet[/*.*.*.*:9600]] into [NODE2][....][host2][inet[/*.*.*.*:9600]]
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70)
        at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:567)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(Thread.java:785)
Caused by: org.elasticsearch.transport.RemoteTransportException: [NODE1][inet[/*.*.*.*:9600]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.transport.NotSerializableTransportException: [org.elasticsearch.indices.recovery.DelayRecoveryException] source node does not have the shard listed in its state as allocated on the node;
```
</description><key id="113855567">14338</key><summary>marking and sending shard failed due to [failed recovery]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nihilson</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2015-10-28T16:01:17Z</created><updated>2016-01-29T20:21:29Z</updated><resolved>2016-01-29T20:21:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T20:26:29Z" id="152308414">@bleskes any ideas here?
</comment><comment author="bleskes" created="2015-11-02T11:18:48Z" id="152991438">Recovery of a replica is triggered by the master publishing a cluster state. Sometimes that  cluster state is processed by the target node first  and only then by the source node. The target node then initiates the recovery by sending a message to the source node. If that message arrives before the source node has processed the cluster state, it will respond with a `DelayRecoveryException`. This should only tell the target node to retry in a few seconds and is just part of the normal process.

Sadly in your case something goes wrong with the serialization of the DelayRecoveryException , which causes a different `NotSerializableTransportException` to be sent. The target node then cancels the recovery as it doesn't know what went wrong. Can you check if there is anything in the source node logs that would hint what wasn't serializable? Sadly we don't do a really good job indicating this in 1.7. In 2.0 exception serialization was completely rewritten to help with these issues.
</comment><comment author="clintongormley" created="2016-01-29T20:21:29Z" id="176952294">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unable to start elasticsearch on btrfs subvolume</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14337</link><project id="" key="" /><description>Hello,

When I start elasticsearch, the logs give:

```
Oct 28 09:17:18 lunielkes03 elasticsearch[1305]: {1.7.3}: Initialization Failed ...
Oct 28 09:17:18 lunielkes03 elasticsearch[1305]: - ElasticsearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/srv/elasticsearch/lunielkes]]
Oct 28 09:17:18 lunielkes03 elasticsearch[1305]: IOException[failed to obtain lock on /srv/elasticsearch/lunielkes/nodes/49]
Oct 28 09:17:18 lunielkes03 elasticsearch[1305]: IOException[Mount point not found]
Oct 28 09:17:18 lunielkes03 systemd[1]: elasticsearch.service: main process exited, code=exited, status=3/NOTIMPLEMENTED
Oct 28 09:17:18 lunielkes03 systemd[1]: Unit elasticsearch.service entered failed state.
```

It says that it cannot find the mountpoint. I was wondering if this was not related to the fact that:
- / is mounted as a btrfs
- /srv/elasticsarch is a subvolume of /

```
df -h /srv/elasticsearch/
Filesystem      Size  Used Avail Use% Mounted on
-               100G  2.4G   96G   3% /srv/elasticsearch
```

```
btrfs subvol list /
ID 260 gen 3050 top level 5 path srv/elasticsearch
ID 264 gen 3046 top level 260 path srv/elasticsearch/.snapshots
ID 265 gen 1147 top level 264 path srv/elasticsearch/.snapshots/1/snapshot
ID 270 gen 1634 top level 264 path srv/elasticsearch/.snapshots/6/snapshot
ID 271 gen 1731 top level 264 path srv/elasticsearch/.snapshots/7/snapshot
&#8230;
```
</description><key id="113834830">14337</key><summary>unable to start elasticsearch on btrfs subvolume</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">briner</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2015-10-28T14:34:23Z</created><updated>2017-03-23T07:11:55Z</updated><resolved>2016-02-13T22:24:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T20:20:41Z" id="176952080">Hi @briner 

Did you manage to resolve this issue?  One thing that is curious is that you failed to obtain a lock on `/srv/elasticsearch/lunielkes/nodes/49` which implies that you were running 50 nodes on this box?
</comment><comment author="briner" created="2016-02-03T07:20:08Z" id="179060453">Hi @clintongormley,
I did remove btrfs and use instead a ext4 filesystem.
</comment><comment author="clintongormley" created="2016-02-13T22:24:34Z" id="183766101">Thanks @briner - not sure we have enough info to take this any further.  Closing
</comment><comment author="torrancew" created="2016-02-22T17:31:18Z" id="187280904">FYI I was able to reproduce this issue precisely. As far as I know, I'm only running a single node on this box, via systemd. However, all attempts to run it, interactively, via the systemd unit, etc, result in precisely the error the OP received. In my case, /var is a BTRFS subvolume, and my data path is /var/lib/elasticsearch/data.
</comment><comment author="torrancew" created="2016-02-22T17:46:12Z" id="187287879">I have found at least one workaround so far: mounting the subvolume directly. In my case, I was able to mount the subvolume at exactly its original path, which is either **really** hacky or **really** elegant, depending on your perspective.

Example:

```
[sysadmin@caribou ~]$ sudo journalctl -u elasticsearch -f
Feb 22 07:58:37 caribou systemd[1]: Started Elasticsearch.
Feb 22 07:58:41 caribou elasticsearch[5140]: [2016-02-22 07:58:41,401][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: 'arm' architecture unsupported
Feb 22 07:58:43 caribou elasticsearch[5140]: [2016-02-22 07:58:43,324][INFO ][node                     ] [caribou] version[2.2.0], pid[5140], build[8ff36d1/2016-01-27T13:32:39Z]
Feb 22 07:58:43 caribou elasticsearch[5140]: [2016-02-22 07:58:43,325][INFO ][node                     ] [caribou] initializing ...
Feb 22 07:58:47 caribou elasticsearch[5140]: [2016-02-22 07:58:47,183][INFO ][plugins                  ] [caribou] modules [lang-expression, lang-groovy], plugins [], sites []
Feb 22 07:58:47 caribou elasticsearch[5140]: Exception in thread "main" java.lang.IllegalStateException: Failed to obtain node lock, is the following location writable?: [/srv/elasticsearch/elk]
Feb 22 07:58:47 caribou elasticsearch[5140]: Likely root cause: java.io.IOException: Mount point not found
Feb 22 07:58:47 caribou elasticsearch[5140]: at sun.nio.fs.LinuxFileStore.findMountEntry(LinuxFileStore.java:91)
Feb 22 07:58:47 caribou elasticsearch[5140]: at sun.nio.fs.UnixFileStore.&lt;init&gt;(UnixFileStore.java:65)
Feb 22 07:58:47 caribou elasticsearch[5140]: at sun.nio.fs.LinuxFileStore.&lt;init&gt;(LinuxFileStore.java:44)
Feb 22 07:58:47 caribou elasticsearch[5140]: at sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:51)
Feb 22 07:58:47 caribou elasticsearch[5140]: at sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:39)
Feb 22 07:58:47 caribou elasticsearch[5140]: at sun.nio.fs.UnixFileSystemProvider.getFileStore(UnixFileSystemProvider.java:368)
Feb 22 07:58:47 caribou elasticsearch[5140]: at java.nio.file.Files.getFileStore(Files.java:1461)
Feb 22 07:58:47 caribou systemd[1]: elasticsearch.service: Main process exited, code=exited, status=1/FAILURE
Feb 22 07:58:47 caribou systemd[1]: elasticsearch.service: Unit entered failed state.
Feb 22 07:58:47 caribou systemd[1]: elasticsearch.service: Failed with result 'exit-code'.
Feb 22 07:58:47 caribou systemd[1]: elasticsearch.service: Service hold-off time over, scheduling restart.
Feb 22 07:58:47 caribou systemd[1]: Stopped Elasticsearch.
```

```
[sysadmin@caribou ~]$ df -h | grep /srv
[sysadmin@caribou ~]$ sudo btrfs subvolume list /
ID 257 gen 768 top level 5 path home
ID 258 gen 787 top level 5 path usr
ID 259 gen 53 top level 258 path usr/local
ID 260 gen 787 top level 5 path var
ID 261 gen 782 top level 5 path opt
ID 262 gen 766 top level 5 path srv
ID 264 gen 19 top level 260 path var/lib/machines
[sysadmin@caribou ~]$ sudo mount -t btrfs -o subvolid=262,ssd,discard,space_cache,compress=lzo /dev/mmcblk0p2 /srv
[sysadmin@caribou ~]$ df -h | grep /srv
/dev/mmcblk0p2   30G  1.9G   28G   7% /srv
```

```
[sysadmin@caribou ~]$ sudo systemctl start elasticsearch
[sysadmin@caribou ~]$ sudo journalctl -u elasticsearch -f
Feb 22 18:55:15 caribou systemd[1]: Started Elasticsearch.
Feb 22 18:55:19 caribou elasticsearch[7316]: [2016-02-22 18:55:19,582][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: 'arm' architecture unsupported
Feb 22 18:55:21 caribou elasticsearch[7316]: [2016-02-22 18:55:21,526][INFO ][node                     ] [caribou] version[2.2.0], pid[7316], build[8ff36d1/2016-01-27T13:32:39Z]
Feb 22 18:55:21 caribou elasticsearch[7316]: [2016-02-22 18:55:21,527][INFO ][node                     ] [caribou] initializing ...
Feb 22 18:55:25 caribou elasticsearch[7316]: [2016-02-22 18:55:25,420][INFO ][plugins                  ] [caribou] modules [lang-expression, lang-groovy], plugins [], sites []
Feb 22 18:55:25 caribou elasticsearch[7316]: [2016-02-22 18:55:25,568][INFO ][env                      ] [caribou] using [1] data paths, mounts [[/srv (/dev/mmcblk0p2)]], net usable_space [27gb], net total_space [29.2gb], spins? [no], types [btrfs]
Feb 22 18:55:25 caribou elasticsearch[7316]: [2016-02-22 18:55:25,569][INFO ][env                      ] [caribou] heap size [123.7mb], compressed ordinary object pointers [unknown]
Feb 22 18:55:38 caribou elasticsearch[7316]: [2016-02-22 18:55:38,263][INFO ][node                     ] [caribou] initialized
Feb 22 18:55:38 caribou elasticsearch[7316]: [2016-02-22 18:55:38,264][INFO ][node                     ] [caribou] starting ...
Feb 22 18:55:38 caribou elasticsearch[7316]: [2016-02-22 18:55:38,693][INFO ][transport                ] [caribou] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
Feb 22 18:55:38 caribou elasticsearch[7316]: [2016-02-22 18:55:38,753][INFO ][discovery                ] [caribou] elk/XZf5Q2LFTjaFA3wu3JEfUQ
Feb 22 18:55:41 caribou elasticsearch[7316]: [2016-02-22 18:55:41,997][INFO ][cluster.service          ] [caribou] new_master {caribou}{XZf5Q2LFTjaFA3wu3JEfUQ}{127.0.0.1}{127.0.0.1:9300}{type=raspi2}, reason: zen-disco-join(elected_as_master, [0] joins received)
Feb 22 18:55:42 caribou elasticsearch[7316]: [2016-02-22 18:55:42,102][INFO ][http                     ] [caribou] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
Feb 22 18:55:42 caribou elasticsearch[7316]: [2016-02-22 18:55:42,105][INFO ][node                     ] [caribou] started
Feb 22 18:55:42 caribou elasticsearch[7316]: [2016-02-22 18:55:42,303][INFO ][gateway                  ] [caribou] recovered [0] indices into cluster_state
```

**Update:**
Added example of problem and workaround
</comment><comment author="TJC" created="2017-03-23T07:11:55Z" id="288635461">Can we re-open this? It's still an issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid cached thread pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14336</link><project id="" key="" /><description>This commit forbids the &#8220;cached&#8221; thread pool setting for any thread
pool other than the generic thread pool (which we continue to require
to be of type &#8220;cached&#8221;).

Closes #14294, relates #2858, relates #5152
</description><key id="113831424">14336</key><summary>Forbid cached thread pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label></labels><created>2015-10-28T14:20:19Z</created><updated>2015-10-30T13:02:45Z</updated><resolved>2015-10-29T21:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-28T14:21:03Z" id="151859693">@kimchy Would you mind opining on whether or not you think this is okay to forbid?
</comment><comment author="jpountz" created="2015-10-29T00:09:23Z" id="152036897">This sounds reasonable to me. The PR is only tagged with 3.0 for now but if we decide to merge it I think it would make sense to have it on 2.x as well (I wouldn't consider it a breaking change as this is a very expert setting).
</comment><comment author="jasontedor" created="2015-10-29T21:14:01Z" id="152329793">Closing in favor of #14367 which takes this pull request further by forbidding changing the thread pool type for any thread pool.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize query parsing based on how a query will be used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14335</link><project id="" key="" /><description>Today if you ask the `terms` query parser to generate a query, it will create a `TermsQuery`. However a `DocValuesTermsQuery` would match exactly the same documents, and would perform worse when used for iterating over matches, but better when used to check whether some documents match. We could make the parser a bit smarter and sometimes generate a `DocValuesTermsQuery` instead of a `TermsQuery`.

I think there are at least two use-cases when using a `DocValuesTermsQuery` instead of a `TermsQuery` would be an easy win:
- excluded clauses (`must_not` in a `bool`)
- in `filter`/`filters` aggregation

I used `TermsQuery` as an example, but it applies more generally to all multi-term queries that can also be executed via doc values such as ranges.

Ideally this would be a Lucene feature, but I'm not sure how this could work given that Lucene doesn't have a schema. So I think elasticsearch is the right place to put it?
</description><key id="113830148">14335</key><summary>Optimize query parsing based on how a query will be used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>discuss</label><label>enhancement</label></labels><created>2015-10-28T14:14:41Z</created><updated>2017-04-26T15:42:09Z</updated><resolved>2017-04-26T09:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-28T14:23:03Z" id="151860296">I'm concerned about this optimization because i think it can easily open a can of worms where things start to look like they used to.

I also think it only really applies for more rare / abuse type cases: its _extraordinarily_ risky and in the worst case acts like a DBMS full table scan.

So, I don't think we should do it.
</comment><comment author="polyfractal" created="2015-11-06T16:44:04Z" id="154466654">At least for the aggregation `filter` case, can we just default to `DocValuesTermsQuery` on the Elasticsearch side?  It seems like the agg filter will always satisfy the case of "just matching" instead of iterating?

I've been running into a lot of slow aggs due to `filter` agg buckets lately.  Rewriting it is usually non-trivial (have to move some filters to the query, convert some to `terms` aggs and then exclude terms you aren't interested in, etc).

I don't really know enough Adrien's suggestion to comment intelligently, but it's a noticeable performance problem for aggs anyhow.  If aggs could be a bit smarter it'd be a nice win for users.
</comment><comment author="jpountz" created="2015-11-09T14:49:58Z" id="155084216">I'm a bit torn here. I certainly agree with Robert that I don't want to get back to the previous situation where we were trying to do too much on top of Lucene and often ended up making worse decisions than the ones that Lucene would have made by itself. Given that the new query improvements (improved cost API, two-phase iteration, etc.) only landed into the hands of our users two weeks ago, I suggest that we wait for a couple months before reevaluating this issue so that we have better perspective.
</comment><comment author="polyfractal" created="2015-11-09T15:09:57Z" id="155091050">Works for me, I certainly don't know enough about how all the components fit together to have a well-educated opinion.  :)
</comment><comment author="colings86" created="2017-04-26T09:34:39Z" id="297319254">Closing as this is no longer relevant</comment><comment author="jpountz" created="2017-04-26T15:42:08Z" id="297452561">Superseded by https://issues.apache.org/jira/browse/LUCENE-7055</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add cache deadlock test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14334</link><project id="" key="" /><description>This commit adds a unit test for a deadlock issue that existed prior to
commit 1d0b93f76667e4af1bad7b0e522f1a4e6c8b3fbc. While commit
1d0b93f76667e4af1bad7b0e522f1a4e6c8b3fbc seems to have addressed the
deadlock issue it would be more robust to have a unit test for it and a
unit test will reduce the risk that future maintenance on Cache will
reintroduce the deadlock issue. This test reliably fails prior to but
passes after commit 1d0b93f76667e4af1bad7b0e522f1a4e6c8b3fbc.
</description><key id="113810005">14334</key><summary>Add cache deadlock test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T12:31:22Z</created><updated>2015-10-28T14:31:13Z</updated><resolved>2015-10-28T14:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-28T14:28:30Z" id="151861823">LGTM
</comment><comment author="jasontedor" created="2015-10-28T14:31:13Z" id="151862611">Thank you for reviewing @jpountz. I've squashed the two commits in this pull request and integrated into master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace processor builder and builder factory for a simple processor factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14333</link><project id="" key="" /><description>Instead of having a factory builder that created a builder and a builder that creates the actual processor, have just a factory that known how to build a processor from a map of maps. If processor need to be created programatically then the constructor of a processor should just be directly used.

Based on comments from:
- https://github.com/elastic/elasticsearch/pull/14132#discussion_r43151138
- https://github.com/elastic/elasticsearch/pull/14132#discussion_r43151271
</description><key id="113799127">14333</key><summary>Replace processor builder and builder factory for a simple processor factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-10-28T11:35:57Z</created><updated>2015-11-03T06:30:37Z</updated><resolved>2015-11-03T06:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-28T11:38:10Z" id="151810787">@talevy and @s1monw would you like to take a look at this?
</comment><comment author="talevy" created="2015-10-28T20:04:16Z" id="151974314">simpler, I like it.

LGTM
</comment><comment author="martijnvg" created="2015-11-02T01:57:15Z" id="152888533">@uboness @talevy I updated the PR. I'll merge this is after the `grok` and `geoip` processors are merged in.
</comment><comment author="martijnvg" created="2015-11-03T05:51:15Z" id="153251462">@talevy I've rebased this pr with feature/ingest and cut over the grok and geoip processors from the builder to the factory.
</comment><comment author="talevy" created="2015-11-03T06:24:35Z" id="153257406">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correction : configuration file syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14332</link><project id="" key="" /><description>Correction of syntax in configuration file.
</description><key id="113783472">14332</key><summary>Correction : configuration file syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeremie70</reporter><labels /><created>2015-10-28T10:05:56Z</created><updated>2015-10-29T20:13:11Z</updated><resolved>2015-10-29T20:13:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T20:13:11Z" id="152305167">Hi @jeremie70 

Thanks for the PR, but both formats are acceptable. This is a YAML file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove some deprecations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14331</link><project id="" key="" /><description>Remove support for request elements/values that have been deprecated in 2.0 (in some cases even 1.0).
</description><key id="113772149">14331</key><summary>Remove some deprecations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T09:00:47Z</created><updated>2015-10-28T10:20:06Z</updated><resolved>2015-10-28T10:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-28T09:30:44Z" id="151779287">LGTM
</comment><comment author="javanna" created="2015-10-28T10:20:06Z" id="151791932">Merged into master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchContextMissingException: No search context found | Node Shutting down</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14330</link><project id="" key="" /><description>I know this issue has been raised lot many times. Referring  to issue #9726 where in @clintongormley mentioned that if one of the shard takes a longer time to respond, the search context gets expired on other shards which results in this exception. The reason for slower shard could be gc running on that particular node.

Firstly ours is not a scan and scroll request and this is a normal search request. 
We have a 3 node cluster and the exception stack trace was found on the master node where first SearchContextMissingException exception we got was for the master node itself and then within same second we got another exception for the 3rd node.

The master node printed this exception for 15-20 times and after that logs vanished. Somewhere nearly 12 hours the master got shut down with other nodes in the cluster reporting error "reason: zen-disco-master_failed".

The logs are currently in the debug mode and none of the node reported any kind of garbage collection. 
Its been nearly 2 years since we are using ES in production. This issue of node shutting down is very frequent in past one month.
</description><key id="113749873">14330</key><summary>SearchContextMissingException: No search context found | Node Shutting down</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">piyushGoyal2</reporter><labels /><created>2015-10-28T05:54:23Z</created><updated>2016-10-14T10:12:26Z</updated><resolved>2015-10-29T20:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-28T08:20:22Z" id="151760507">&gt; I know this issue has been raised lot many times. Referring to issue #9726 where in @clintongormley mentioned that if one of the shard takes a longer time to respond, the search context gets expired on other shards which results in this exception. The reason for slower shard could be gc running on that particular node.

we are talking about 5 minutes here I don't think this is what's happening here....

&gt; The master node printed this exception for 15-20 times and after that logs vanished. Somewhere nearly 12 hours the master got shut down with other nodes in the cluster reporting error "reason: zen-disco-master_failed".

does the exception happen when the node is shutting down? Why is your node shutting down at all? Which version are you using and are you using the shutdown API anywhere? Is it possible that you are shutting down the cluster via the _shutdown API due to some developer using it locally?
</comment><comment author="piyushGoyal2" created="2015-10-28T09:24:27Z" id="151778181">If it's not the above stated issue, then what could be the cause of the Exception. My Java client asks for lot of documents from a single query. And this query is fired after approximately 10 seconds each with different parameters. All I can see in my logs are these exceptions for different ID's that ES is trying to locate at the very same instance of time.
For the other questions of yours
does the exception happen when the node is shutting down?
The exception does not happen at the time of node shut down. In fact, there are actually no logs registered on the node which is getting shut down at the time of shutting down. Other nodes just mention that a node has been removed from cluster.

Why is your node shutting down at all? 
That is what a million dollar question for us why ES process running on the node gets itself killed when nothing is reported.

Which version are you using?
1.3.2
 you using the shutdown API anywhere? Is it possible that you are shutting down the cluster via the _shutdown API due to some developer using it locally?
No, we are not using shutdown API anywhere using the java API. It's a production instance where in we have blocked all these API through sense so no case of some developer shutting it down via _shutdown API. And always it's a random node which gets shutdown and not a specific node.
</comment><comment author="s1monw" created="2015-10-28T09:29:02Z" id="151779024">&gt; Which version are you using?
&gt; 1.3.2

I strongly recommend to upgrade there are millions of bugs fixed since they. Please report back if the issue still exists on the latest release
</comment><comment author="piyushGoyal2" created="2015-10-28T09:35:27Z" id="151780282">We are waiting for the stable release of 2.0. I guess it would be better for me to reopen the defect if it still persist on the newer version of ES. Thanks for the help @s1monw meanwhile . :)
</comment><comment author="clintongormley" created="2015-10-29T20:12:04Z" id="152304919">@piyushGoyal2 you're in luck! https://www.elastic.co/downloads/past-releases/elasticsearch-2-0-0 :)

I'm going to close this for now, but please feel free to reopen if you see the same thing in 2.0
</comment><comment author="OnlyGit" created="2016-10-14T10:12:26Z" id="253761542">The ElasticSearch version we use is 2.4.1, this issue also appears, but we don't use the scan or scroll request
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove uncaught exception handler in tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14329</link><project id="" key="" /><description>This is not needed: full mvn verify passes.

Furthermore, there are all kinds of checks for this case
(rejected while shutting down) in the actual code, so there
is no need to have it here. If its supposed to be non-fatal,
then we add the missing places to the actual code, not globally to all threads.
</description><key id="113737406">14329</key><summary>Remove uncaught exception handler in tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T03:40:08Z</created><updated>2015-10-29T20:17:18Z</updated><resolved>2015-10-28T12:05:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-28T05:12:56Z" id="151727013">LGTM
</comment><comment author="s1monw" created="2015-10-28T08:15:33Z" id="151759893">LGTM! the times where this was needed where pretty crazy in terms of uncaught exceptions etc. for node shutdown. Now we have AbstractRunnable etc. and it's time to fix the remaining places! good to clean this up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch depends on spatial test jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14328</link><project id="" key="" /><description>```
[INFO] +- org.apache.lucene:lucene-spatial:jar:5.4.0-snapshot-1708254:compile
[INFO] |  +- org.apache.lucene:lucene-spatial3d:jar:5.4.0-snapshot-1708254:compile
[INFO] |  \- com.spatial4j:spatial4j:jar:tests:0.5:compile
```

and make a .zip distribution and you see this tests jar in lib/

it should not be there...
</description><key id="113736619">14328</key><summary>elasticsearch depends on spatial test jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>build</label></labels><created>2015-10-28T03:34:50Z</created><updated>2016-01-29T20:18:17Z</updated><resolved>2016-01-29T20:18:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-28T08:48:22Z" id="151768357">I think this is a lucene issue see the difference between 5.x and 5.3
`http://svn.apache.org/repos/asf/lucene/dev/branches/branch_5x/lucene/spatial/ivy.xml` vs. `http://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_5_3/lucene/spatial/ivy.xml` I will open a lucene issue I think it's caused by this revision: http://svn.apache.org/viewvc?view=revision&amp;revision=1704760 / https://issues.apache.org/jira/browse/LUCENE-6810
</comment><comment author="s1monw" created="2015-10-28T08:51:34Z" id="151768977">I opened this https://issues.apache.org/jira/browse/LUCENE-6860
</comment><comment author="clintongormley" created="2016-01-29T20:18:17Z" id="176951538">Fixed in Lucene 5.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>support symlinked .m2/repository in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14327</link><project id="" key="" /><description>Some jenkins servers have this, but our codebase normalization doesn't
follow symlinks. Add this so that its correct.

Only really impacts tests, i suppose it helps if someone has a symlinked plugins/
but that is not recommended :)

Easy to see the current problems on e.g. jenkins servers configured this way:

```
mvn clean verify -Dmaven.repo.local=/some/symlink/to/the/real/m2/repository
```
</description><key id="113723816">14327</key><summary>support symlinked .m2/repository in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-28T01:29:02Z</created><updated>2015-10-29T20:06:42Z</updated><resolved>2015-10-28T04:18:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-28T01:52:52Z" id="151695556">This passes full `mvn verify` with symlink'd .m2 locally.
</comment><comment author="rjernst" created="2015-10-28T04:15:54Z" id="151717309">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14326</link><project id="" key="" /><description /><key id="113721088">14326</key><summary>2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dajposkakac</reporter><labels /><created>2015-10-28T01:05:29Z</created><updated>2015-10-28T01:19:38Z</updated><resolved>2015-10-28T01:19:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-28T01:19:38Z" id="151689900">This appears to be a mistakenly opened pull request which I am closing. If I've been too hasty in my assessment, I apologize, but in which case could you help us understand the intent?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Circuit breaker to manage native script plugin memory usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14325</link><project id="" key="" /><description>Not sure if this goes into #11070 or not. 
Users can pretty do anything in native script plugins (for various reasons), eg. building large arraylists, using java's regex pattern matcher, etc.. that can end up using a ton of memory.  It will be difficult for the request circuit breaker to estimate the amount of memory the native plugin will use and prevent it from running.  So it can be useful to provide a circuit breaker that allows for a memory cap that kills off queries that may have a native plugin using a lot of heap once it hits a user defined memory limit.
</description><key id="113717843">14325</key><summary>Circuit breaker to manage native script plugin memory usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Circuit Breakers</label><label>discuss</label><label>enhancement</label></labels><created>2015-10-28T00:32:02Z</created><updated>2016-01-29T20:24:27Z</updated><resolved>2016-01-29T20:17:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-28T20:38:18Z" id="151983502">@ppf2 I'm not sure this is feasible right now, with the way the circuit breakers work there is a requirement that requests for additional memory are funneled through a single point (where we can track usage and freeing), we already have this for fielddata and requests (ie BigArrays), but for native scripts, they are allocating memory directly on the JVM heap, so I don't think there is a hook we can use to increment/decrement memory for.

Native scripts could always use our circuit-breaking BigArrays instance, however, to generate new arrays of different types, in which case it will go through the requests breaker.
</comment><comment author="clintongormley" created="2016-01-29T20:17:48Z" id="176951436">Once users are in native script territory, it's pretty much up to them what they do.  I don't think we can provide any limits here.  Closing
</comment><comment author="ppf2" created="2016-01-29T20:24:27Z" id="176952925">Ok sounds good, thanks for the tip on BigArrays btw @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add support for array/list access in path </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14324</link><project id="" key="" /><description>The ingest plugin's [Data](https://github.com/elastic/elasticsearch/blob/feature/ingest/plugins/ingest/src/main/java/org/elasticsearch/ingest/Data.java#L46-L53) class currently supports retrieval of a value using a path to a field specified in dot-notation (e.g. `foo.bar`). For more complex insertions, updates, and retrievals of nested fields within the document, it would be useful to be able to access fields through array/list typed fields.

The notation proposed here is to introduce a path element in the specified path that is an integer and is to be evaluated as an index into the parent array/list object.

```
# example paths
foo.bar.1.first
foo.4
foo.4.3.6.last
```

Here is an example: 

```
data = new Data("index", "type", "id",
                new HashMap&lt;String, Object&gt;() {{
                    put("foo", Arrays.asList("a", "b", "c");
                    put("fizz", new HashMap&lt;String, Object&gt;() {{
                        put("buzz", "hello world");
                    }});
                }});
assertThat(data.getProperty("foo.1"), equalTo("b"))
```

`addField` is also meant to support this behavior. more functions will have to be introduced to distinguish between an append to an array, and a full replace.
</description><key id="113711474">14324</key><summary>[Ingest] Add support for array/list access in path </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-10-27T23:31:51Z</created><updated>2015-12-22T15:24:22Z</updated><resolved>2015-12-22T15:24:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-27T16:54:32Z" id="160174646">Read operations are clear in terms of syntax and expected results. The syntax for setting values to lists remains to be figured out. At the moment the add/set processor doesn't support appending values, if we set `"list":"item"` it will simply replace the list with a string `"item"`. Shall we move to adding by default (through set processor) if we find a list? Or shall we have a specific append processor?
And what if we get `{ "set" : {"list.1" : "value"}}`? 
</comment><comment author="martijnvg" created="2015-11-27T20:42:27Z" id="160201500">@javanna I lean towards having a separate `append` processor. Embedding the append a value into the `set` processor complicates things. For example your example, what if there is a field name with a `.` in it (`list.1` field name)? I also considered using an array syntax like this: `"list" : ["value"]`, but then I thought what if someone just wants to replace an array field field an array with a single element. 

If we would have a separate `append` processor then we have more control over how it can be used and certain behaviour in specific scenarios. For example an option to tell in what slot of the array a value needs to be appended. If the field doesn't exits yet when can just append an array with a single element and if the field already exists but isn't array we can fail with a clear error or maybe be lenient and append the existing value and new value into a new array. 

Downside is that we end up with another extra processor to kind of do what `set` processor does, but just for arrays...
</comment><comment author="javanna" created="2015-11-27T21:07:32Z" id="160203574">@mvg I think the dot within a field name is not a concern as not supported anymore in es, but I totally agree that we should have a specific processor to deal with arrays explicitly.
</comment><comment author="uboness" created="2015-11-27T23:56:26Z" id="160227884">I think setting a value in a specific index within an array is taking things a bit too far for the moment. I understand that this scenario theoretically exists, but I would love to hear an example for it. IMO, in the vast majority of use cases, ppl simply want the following three basic ops:
- set a field value (completely override)
- set a field value if doesn't already exist
- set a field value if doesn't exist or append/add it to an array if already exists.

These three cases can be covered by:

``` js
{
  "set" : {
    "field_1" : 42, // default behaviour is to override or add
    "field_2" : { "value" : 42, "if_exists" : "override" }, // same as above
    "field_3" : { "value" : 42, "if_exists" : "skip" }, // will only set the value if "field_3" doesn't already exist
    "field_3" : { "value" : 42, "if_exists" : "append" } // will set the value if field doesn't exist, otherwise append it to the existing one/s
  }
}
```

I might be complete off with my assessment here, but if I'm not, I think this should vold into a simple `set` processor. (would love to hear @talevy's opinion here) 
</comment><comment author="javanna" created="2015-11-30T13:11:12Z" id="160623363">With #15078 we have added the support for specifying indices of items within list. What's left to do is decide how to expose the append semantics. We now have an `IngestDocument#appendFieldValue` but it's not exposed to any processor yet. We have to discuss if we want to add a new append processor, or add an option to the set processor. Do we also need the "set if it doesn't exist" semantic? We don't have that at the moment, so that would be an additional "feature" to implement. Also, when we say "append it to the existing one/s" we mean only for arrays right? 
@clintongormley what do you think this should look like in terms of api?
</comment><comment author="clintongormley" created="2015-11-30T13:13:06Z" id="160623679">Hmmm  I'd say that if somebody is OK with having a list of items, then it probably makes sense to always make it a list (even if it doesn't already exist).  So i'd have a dedicated `append` or `push` processor which will:
- append the specified value(s) to an existing array
- create an array with the specified value(s) if the field doesn't exist
- convert an existing scalar to an array then append the specified value(s)

It should be possible to specify multiple values as well:

```
{ "push": { "tags": "my_tag" }}
{ "push": { "tags": ["one", "two", "three"] }}
```

For the option of: set-scalar-if-doesn't-exist, i think that is unlikely to be a common use case, but it could be implemented as a `create` processor if we really want to.
</comment><comment author="uboness" created="2015-11-30T13:29:10Z" id="160626764">just an idea... now that we have `set` support paths like `field.0`, what if we added support for `field._last` that effectively indicates:
- if there's no field, create an array and add the value
- if there's an array, push/append it to the the end
- if there's a single value, create an array with that single value and push/append the new value to it

we could also support `field._first` in which case it'll prepend the value instead of append. The advantage is that we're still using a single processor (`set`) for this as it already handles random access to arrays.
</comment><comment author="clintongormley" created="2015-11-30T14:04:03Z" id="160639070">`_last` to me means the last element, not the last element plus one.  I find it confusing.  I'd much rather have a simple clean dedicated processor.  Also, I don't think we need to support a prepend operation as values in an array in ES are essentially unordered.
</comment><comment author="uboness" created="2015-11-30T14:30:20Z" id="160645518">&gt; _last to me means the last element, not the last element plus one. I find it confusing. I'd much rather have a simple clean dedicated processor. 

fair enough. I personally feel we're starting to have a bit of a proliferation of processors with semi-overlapping goals. I'd much rather have a single processor that handles setting values. Now we have 2:
- one is to set single values on scalar **and** array fields, but if they don't exist it actually adds the value
- another one to add values to existing scalars and turn them into arrays, or add values to existing arrays, but if the field doesn't exist it adds the values into a single value array.

that ^ I find a bit messy and confusing. Personally, I much rather have a single processor with simple rules that enable you to customize behaviour based on exceptions (and its defaults need to apply to +90% of the use cases.

&gt; Also, I don't think we need to support a prepend operation as values in an array in ES are essentially unordered.

yes and now... in the source they're still ordered... and `set` does take order into account as it allows to set items in specific indices (a la `field1.array.3`). Though I agree... I've yet seen a real use case for this random access into arrays and as mentioned in other tickets... would love to get @talevy's opinion here.
</comment><comment author="clintongormley" created="2015-11-30T15:38:50Z" id="160663767">&gt; one is to set single values on scalar and array fields, but if they don't exist it actually adds the value
&gt; another one to add values to existing scalars and turn them into arrays, or add values to existing arrays, but if the field doesn't exist it adds the values into a single value array.

No.  One (`set`) sets the value of a scalar variable, whether that variable is named `foo`, or `foo.bar`, or `foo.0`, or `foo.0.bar`.  The other (`push`) appends values to an array.   (It can also upgrade a scalar to an array if it needs to, although typically people will just use `push` and so the variable will be an array from the outset)
</comment><comment author="martijnvg" created="2015-12-09T19:09:50Z" id="163360327">I'm kind of leaning towards having the `if_exists` option in the `set` processor instead of new processor, because now `set` processor only operates on one field and it feels clean to add another setting now because of how the settings are specified. So it would look like this:

```
{
   "set" : {
       "field" : "_field",
       "value" : "_value", // this can also be an array or an object
       "if_exists" : "append|overwrite|skip"
   }
}
```
</comment><comment author="javanna" created="2015-12-21T17:20:39Z" id="166364257">After discussing this internally, we went for adding a specific processor as described by Clint above:

&gt; append the specified value(s) to an existing array
&gt; create an array with the specified value(s) if the field doesn't exist
&gt; convert an existing scalar to an array then append the specified value(s)

Opened #15577 for it.
</comment><comment author="javanna" created="2015-12-22T15:24:22Z" id="166645764">Closed https://github.com/elastic/elasticsearch/commit/46f99a11a02db3ddedc86f92f13ad33d4d2acf7e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Server not started on Ubuntu after upgrade to 2.0.0-rc1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14323</link><project id="" key="" /><description>Exception

```
1) Error injecting constructor, RepositoryException[[testrepo] location [/tmp/backups/backup2] doesn't match any of the locations specified by path.repo because this setting is empty]
  at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.repositories.fs.FsRepository
  while locating org.elasticsearch.repositories.Repository

1 error
        at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:137)
        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:56)
        at org.elasticsearch.repositories.RepositoriesService.createRepositoryHolder(RepositoriesService.java:404)
        ... 7 more
Caused by: RepositoryException[[testrepo] location [/tmp/backups/backup2] doesn't match any of the locations specified by path.repo because this setting is empty]
        at org.elasticsearch.repositories.fs.FsRepository.&lt;init&gt;(FsRepository.java:85)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:865)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:858)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        ... 11 more
```

After create directory and add path.repo: ["/tmp/backups"] to elasticsearch.yml it start working, but in logs have similar messages about another directories: /tmp/test_repository, /tmp/test_register

Also interesting in logs:

[2015-10-28 00:24:47,331][INFO ][cluster.routing.allocation.decider] [Paige Guthrie] low disk watermark [85%] exceeded on [8qeuTUZdQ0SuYLZTspuvjw][Paige Guthrie][/var/lib/elasticsearch/elasticsearch/nodes/0] free: 11.9gb[10.9%], replicas will not be assigned to this node

ES installed just to localhost without cluster and so on.
</description><key id="113710936">14323</key><summary>Server not started on Ubuntu after upgrade to 2.0.0-rc1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ewgRa</reporter><labels /><created>2015-10-27T23:27:01Z</created><updated>2015-10-28T01:44:30Z</updated><resolved>2015-10-28T01:43:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-28T01:43:07Z" id="151693672">I guess you upgraded from a version &lt; 1.6?

See https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-1.6.html
</comment><comment author="dadoonet" created="2015-10-28T01:44:29Z" id="151694352">Add all your repo dir to path.repo
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange limits when using Java API (array of objects) (SOLVED)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14322</link><project id="" key="" /><description>It seems that there is some kind of limit in ES when creating an array of objects. 

I created a small example which you can easily try. Just don't forget to set the correct cluster name.

import org.elasticsearch.ElasticsearchException;
import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;
import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;
import org.elasticsearch.client.Client;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentFactory;

public class ESTests {

```
public static final String INDEX = "tempindex";
public static final int SIZE1 = 3;
public static final int SIZE2 = 3;

public static void main(String[] args) throws Exception {

    Client client = new TransportClient(ImmutableSettings.builder().put("cluster.name", "ncluster").build())
            .addTransportAddress(new InetSocketTransportAddress("localhost", 9300));
    System.out.println("Client initialized.");
    try {
        //first delete the old index, if exists.
        client.admin().indices().delete(new DeleteIndexRequest(INDEX)).actionGet();
        System.out.println("Index deleted.");
    } catch (ElasticsearchException e) {
        //ignore
    }

    Settings settings = ImmutableSettings.settingsBuilder().build();
    System.out.println("Settings built.");
    //Then create a new-clean index.
    client.admin().indices().create(new CreateIndexRequest(INDEX, settings)).actionGet();
    System.out.println("Index created.");

    XContentBuilder cb = XContentFactory.jsonBuilder();
    int id = 1;
    cb.startObject();
    cb.field("name", "test document");

    {


        cb.startArray("testArray");
        String str = "";
        for(int i = 0; i &lt; SIZE1; i++) str += "0";

        for(int i = 0; i &lt; SIZE2; i++) {
            cb.startObject();
            cb.field("counter", i);
            cb.field("str", str);
            cb.endObject();
        }
        cb.endArray();

    }


    cb.endObject();

    client.prepareIndex(INDEX, "document", Integer.toString(id)).setSource(cb.string()).execute();

}
```

}

When I change the constants SIZE1 and SIZE2, to values 100 and 1000 respectively, the document is not being created! Though, you might need to try bigger numbers on your pc.

To retrieve the document, I use the Sense plugin for Chrome and the following command: 
POST /tempindex/document/_search?pretty=1
{
     "query" : {
        "match_all" : {}
    }
}

I have posted a similar question here: https://discuss.elastic.co/t/strange-limits-when-using-java-api/32914 but nobody answered so far.

I am under a deadline so you reply soon, it will help me a lot.

Thanks!
</description><key id="113698892">14322</key><summary>Strange limits when using Java API (array of objects) (SOLVED)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">ane18s</reporter><labels /><created>2015-10-27T21:58:11Z</created><updated>2015-10-28T22:27:44Z</updated><resolved>2015-10-28T22:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-10-28T14:25:41Z" id="151861122">Hi @ane18s,
sorry but I cannot reproduce this. Assuming you use ES 1.7, I can reliably create large arrays of objects with large fields (just tried it with 10000 array entries, 10000 chars), although Sense for me freezes on such large documents, but curl works just fine. I think there are two problems with the test code:
- just calling `execute()` on the IndexRequestBuilder will return a Future, but that only starts the index operation. If your test now aborts, the cluster might not have received it. Instead, like when issuing an index request via REST, you should wait for the response using `execute().actionGet()`. there's even a handy shortcut for this with the `get()` method on all ActionRequestBuilders. 
- also close the client after you are done with it

Please let me know if this solves your issue.
</comment><comment author="ane18s" created="2015-10-28T22:27:08Z" id="152016661">THANK YOU!!! The "execute().actionGet()" solved the problem. Again, THANK YOU!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use System.nanoTime() to initialize Engine.lastWriteNanos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14321</link><project id="" key="" /><description>On _lastWriteNanos_ we use System.nanoTime() to initialize this since:
- we use the value for figuring out if the shard / engine is active so if we startup and no write has happened yet we still consider it active
  for the duration of the configured active to inactive period. If we initialize to 0 or Long.MAX_VALUE we either immediately or never mark it
  inactive if no writes at all happen to the shard.
- we also use this to flush big-ass merges on an inactive engine / shard but if we we initialize 0 or Long.MAX_VALUE we either immediately or never
  commit merges even though we shouldn't from a user perspective (this can also have funky sideeffects in tests when we open indices with lots of segments
  and suddenly merges kick in.
</description><key id="113698400">14321</key><summary>Use System.nanoTime() to initialize Engine.lastWriteNanos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T21:55:01Z</created><updated>2015-10-30T12:39:21Z</updated><resolved>2015-10-29T15:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-27T21:55:08Z" id="151657120">@mikemccand can you look
</comment><comment author="mikemccand" created="2015-10-27T21:57:44Z" id="151657695">LGTM, thanks @s1monw!
</comment><comment author="bleskes" created="2015-10-28T08:29:21Z" id="151762905">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.4-snapshot-1710880.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14320</link><project id="" key="" /><description /><key id="113697544">14320</key><summary>Upgrade to lucene-5.4-snapshot-1710880.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T21:49:22Z</created><updated>2015-10-29T20:18:16Z</updated><resolved>2015-10-28T13:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-10-27T21:51:30Z" id="151656288">LGTM
</comment><comment author="s1monw" created="2015-10-27T21:56:31Z" id="151657414">cool push it so I can clean up IndexSearcherWrapper
</comment><comment author="jpountz" created="2015-10-28T08:37:06Z" id="151765525">I rebased and updated the PR to adapt to #14311. @rmuir Can you look?
</comment><comment author="rmuir" created="2015-10-28T12:56:26Z" id="151835846">+1 I just went to do this (sorry, i intended to before falling asleep) and saw you already did it. looks great.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards out of sync when setting  refresh_interval &amp; number_of_replicas at the same time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14319</link><project id="" key="" /><description>If you create an index number_of_replicas to 0 and refresh_interval to -1 (using one API call) &amp; add some documents. Then set number_number_of_replicas to 1 and refresh_interval to "1s" and add some more documents. The primary and replicas are out of sync, it looks like the replica doesn't have the new documents. See this gist for a repro:

https://gist.github.com/msimos/869292265208852fbee4

If you run the match_all query multiple times you'll see the different documents returned. If you use preference=_primary then all documents are returned. If the query hits the replica, then only the first documents are returned when replicas were set to 0. Also using _cat/shards you can see the size difference between the shards.

It seems you can workaround this by doing sending this in two separate calls:

```
PUT /index/_settings
{
  "number_of_replicas": 1
}

PUT /index/_settings
{
  "refresh_interval": "1s"
}
```

Also seems the ordering matters, that "number_of_replicas": 1 is set before "refresh_interval": "1s". This was on Elasticsearch 1.7.2.
</description><key id="113691352">14319</key><summary>Shards out of sync when setting  refresh_interval &amp; number_of_replicas at the same time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msimos</reporter><labels><label>:Core</label><label>bug</label></labels><created>2015-10-27T21:17:45Z</created><updated>2016-01-10T10:21:22Z</updated><resolved>2016-01-07T21:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T20:00:59Z" id="152302430">Hi @msimos 

Thanks for the report and the nice clear recreation.  It is not that the shards are out of sync, but that the shard holding the first doc thinks that it doesn't need to refresh.

This is fixed in 2.0 already
</comment><comment author="mikemccand" created="2015-10-30T14:54:34Z" id="152547085">A forced refresh (`/index2/_refresh`) resolves the issue, so somehow the replica is failing to refresh after 1s (but did in fact index the 2nd document).

Curiously, if I pass `"number_of_shards": 1` when creating the index, the bug doesn't happen, but with 2 shards, it does.  Hmm.
</comment><comment author="mikemccand" created="2015-10-30T15:34:47Z" id="152558070">OK I think I see what's happening:

The new replica is created (because of `"number_of_replicas": 1`), but because this is async change, the additional `"refresh_interval": "1s"` in the same request is never applied to the newly created replica shards since they have not yet set up listeners (`IndexShard.ApplyRefreshSettings`) for settings changes.

So the replica shards still have the old `refresh_interval` (-1).

Reversing the two settings works because the newly created replica shards will inherit the latest index settings when they are created, including the change just made to `refresh_interval`.

I'm not sure how to cleanly fix this. @bleskes?

Maybe we could simply move `number_of_replicas` to the end of all settings updated in one request?
</comment><comment author="s1monw" created="2015-10-30T15:36:45Z" id="152558645">@mikemccand can you try and verify if that still happens on master / 3.0?
</comment><comment author="mikemccand" created="2015-10-30T15:46:19Z" id="152561108">@s1monw I'll test master.  @clintongormley already checked that 2.0 fixes it (I'm not sure why/how!) ...
</comment><comment author="mikemccand" created="2015-10-30T15:58:54Z" id="152565412">@s1monw master doesn't have the bug either ... I'm not sure why.
</comment><comment author="bleskes" created="2015-11-01T10:06:16Z" id="152814192">I did some digging and this has to do with the fact that index shards are created with old index settings. IndexShard get's it's index setting using guice and the `@IndexSettings` annotation. This is supposed be supplied by [IndexSettingsProvider](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/index/settings/IndexSettingsProvider.java) , which in turn asks `IndexSettingsService` for the latest setting. For some unknown reason, Guice decides to cache those settings and thus IndexShard gets a stale copy. This means that if Guice captures an old settings, the settings are changed and then a shard is created, the shard will see the old setting. It does register a setting change listener but as far as `IndexSettingService` is concerned, no settings have changed since the last iteration. This doesn't happen in 2.0 because we don't inject the index setting to the shard anymore but rather ask for a live copy in the IndexShard constructor. Master is of course totally rewritten to another mechanism.

I've put a test reproducing on https://gist.github.com/bleskes/1e336a206a52cb4f1b0f . (based on 1.7 code)

I think we can do one of the following:
- Fix the Guice issue and make the provider be called every time (I did some looking and this looks like a rabbit hole - unless somebogy knows what needs to be done)
- Change 1.7 to do what 2.0 does and not inject the index setting to the IndexShard. This is a local fix and we might suffer other simillar issues in the future, so if we go down this route I suggest we remove the index settings annotation and use this mechanism all over the place (in 2.x and 1.7).

Thoughts?
</comment><comment author="mikemccand" created="2015-11-01T12:33:11Z" id="152822794">Option 1 sounds scary (I don't like rabbit holes).  Option 2 seems like a biggish change to make on a bug fix branch?  We could simply leave this bug unfixed in 1.x and encourage users who hit it to upgrade to 2.x?
</comment><comment author="jasontedor" created="2015-11-01T15:09:56Z" id="152833775">&gt; We could simply leave this bug unfixed in 1.x and encourage users who hit it to upgrade to 2.x?

Since a workaround exists in the 1.x line, this seems to me to be a sensible option.
</comment><comment author="bleskes" created="2015-11-01T15:13:17Z" id="152833958">note that the work around is likely to to work but not guaranteed. Sometime shard creation is throttled/disabled (see test) and  then it doesn't.

There is the option of just doing the on liner change in 1.7 to make index shard get a new setting every time and only the bigger change (i.e., remove the provider and the annotation) in 2.x .
</comment><comment author="s1monw" created="2015-11-02T21:00:56Z" id="153155163">&gt; @s1monw master doesn't have the bug either ... I'm not sure why.

@mikemccand in master I trashed this Provider @bleskes is talking about entirely. New shards are guaranteed to see the latest settings.
</comment><comment author="bleskes" created="2015-11-03T14:28:00Z" id="153370962">Unless someone objects soon, I'll assume we're going with:

1) One line fix to index shard in 1.7 to _not_ have an injected index settings.
2) removal of the provider in 2.x.
</comment><comment author="jasontedor" created="2015-11-03T14:30:17Z" id="153371585">&gt; Unless someone objects soon, I'll assume we're going with:
&gt; 
&gt; 1) One line fix to index shard in 1.7 to _not_ have an injected index settings.
&gt; 2) removal of the provider in 2.x.

+1
</comment><comment author="mikemccand" created="2015-11-03T14:37:59Z" id="153374041">&gt; @mikemccand in master I trashed this Provider @bleskes is talking about entirely. 

OK wonderful!

&gt; Unless someone objects soon, I'll assume we're going with:
&gt; 
&gt; 1) One line fix to index shard in 1.7 to not have an injected index settings.
&gt; 2) removal of the provider in 2.x.

+1
</comment><comment author="danielmitterdorfer" created="2015-11-04T13:42:37Z" id="153722490">I've implemented the fix for 1.7 (see PR #14522). I am about to work on removing the provider in 2.x.
</comment><comment author="jasontedor" created="2016-01-07T21:16:43Z" id="169807462">This was closed by #14522.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid usage of Thread#getAllThreadGroups()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14318</link><project id="" key="" /><description>This method needs special permission and can cause all kinds of other problems
if we are creating lots of theads. Also the reason why we added this are fixed
long ago, no need to maintain this code.
</description><key id="113677710">14318</key><summary>Remove and forbid usage of Thread#getAllThreadGroups()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T20:15:50Z</created><updated>2015-10-27T20:56:15Z</updated><resolved>2015-10-27T20:56:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-27T20:17:42Z" id="151630206">Nice cleanup, +1

In the future if we need to enumerate threads for debugging reasons like this, we should just enumerate our own threadgroup which is allowed. We'd see all the ES threads and thats all that is needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Caching Weight wrappers should propagate the BulkScorer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14317</link><project id="" key="" /><description>If we don't, then we will use the default bulk scorer, which can be
significantly slower in some cases (eg. disjunctions).

Related to https://issues.apache.org/jira/browse/LUCENE-6856
</description><key id="113648177">14317</key><summary>Caching Weight wrappers should propagate the BulkScorer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T18:16:20Z</created><updated>2015-10-29T19:51:48Z</updated><resolved>2015-10-27T20:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-27T18:55:00Z" id="151609186">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deduplicate concrete indices after indices resolution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14316</link><project id="" key="" /><description>This commit fixes a regression introduced with #12058 as we are not deduplicating concrete indices after indices resolution anymore. This causes failures with the delete index api when providing the same index name multiple times in the request, or aliases/wildcard expressions that end up pointing to the same concrete index. The bug was revealed after merging #11258 as we delete indices in batch rather than one by one. The master node will expect too many acknowledgements based on the number of indices that it's trying to delete, hence the request will never be acknowledged by all nodes.
</description><key id="113635680">14316</key><summary>Deduplicate concrete indices after indices resolution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>regression</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T17:18:53Z</created><updated>2015-10-27T18:06:28Z</updated><resolved>2015-10-27T18:06:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-27T17:20:26Z" id="151575732">LGTM left a minor - no need to review again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix issues with failed cache loads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14315</link><project id="" key="" /><description>This commit fixes two issues that could arise when a loader throws an
exception during a load in Cache#computeIfAbsent.

The underlying issue is that if the loader throws an exception,
Cache#computeIfAbsent would attempt to remove the polluted entry from
the cache. However, this cleanup was performed outside of the segment
lock. This means another thread could race and expire the polluted
entry (leading to NPEs) or get a polluted entry out of the cache before
the loading thread had a chance to cleanup (leading to ISEs).

The solution to the initial problem of correctly handling failed cached
loads is to check for failed loads in all places where entries are
retrieved from the map backing the segment. In such cases, we treat it
as if there was no entry in the cache, and we clean up the cache on a
best-effort basis. All of this is done outside of the segment lock to
avoid reintroducing the deadlock that was initially a problem when
loads were executed under a segment lock.
</description><key id="113631338">14315</key><summary>Fix issues with failed cache loads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T16:58:16Z</created><updated>2015-10-28T18:37:45Z</updated><resolved>2015-10-28T18:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-27T16:59:25Z" id="151569045">Would you mind taking a look @jpountz?
</comment><comment author="jpountz" created="2015-10-27T17:45:22Z" id="151583899">Doesn't this change bring back the issue that you can run into a deadlock if you load dependent keys that end up in the same segment?
</comment><comment author="jasontedor" created="2015-10-27T18:48:30Z" id="151607511">@jpountz I've pushed df972d2c95b948fcca6d3299d7ad6656fa79d15f to avoid reintroducing the deadlock problem that was initially solved by not executing loads while holding a segment lock.
</comment><comment author="jpountz" created="2015-10-27T19:48:22Z" id="151622578">I _think_ this is good, but to be honest the logic is quite complex so I could easily miss something. Could you add tests for the two issues that you are trying to fix, maybe as well as the potential deadlock issue in case of dependent keys?
</comment><comment author="jasontedor" created="2015-10-27T21:05:24Z" id="151643934">@jpountz Adding a test caught an issue with the pollution handling in racing threads. I've added a fix in c1d8ace818ef819d39826f6678685d7a93c3d5c4 and a test in 1b2b71bcd93e124ab6a0456924e934d6a38babb3 that currently fails on master and does not fail after c1d8ace818ef819d39826f6678685d7a93c3d5c4.
</comment><comment author="jpountz" created="2015-10-28T08:46:53Z" id="151768131">Can you also add a test for the deadlock problem with dependent keys? I might be wrong but it should be easy to test with an object that generates bad hashcodes?
</comment><comment author="jasontedor" created="2015-10-28T12:33:01Z" id="151829538">@jpountz I opened #14334 to add a unit test for the deadlock issue. This test fails prior to but passes after commit 1d0b93f76667e4af1bad7b0e522f1a4e6c8b3fbc (the commit to address the deadlock issue) and also passes against this current pull request.
</comment><comment author="jpountz" created="2015-10-28T17:12:15Z" id="151913283">LGTM
</comment><comment author="jasontedor" created="2015-10-28T18:29:47Z" id="151943057">Thanks again for another careful review @jpountz. Squashed and integrated into master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ghosts of discovery-ec2 in 2.x branches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14314</link><project id="" key="" /><description>Some files moved to discovery-ec2 when backporting some changes from master to 2.x.
We should never have any file in `discovery-*` in 2.x branches but only in `cloud-*`.

Closes #14127.
</description><key id="113626231">14314</key><summary>ghosts of discovery-ec2 in 2.x branches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>v2.2.0</label></labels><created>2015-10-27T16:35:35Z</created><updated>2015-11-20T08:54:07Z</updated><resolved>2015-11-20T08:53:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-27T16:40:00Z" id="151562836">is this a bug? If not I think it should not go into `2.0.1`
</comment><comment author="dadoonet" created="2015-10-27T16:50:54Z" id="151565877">Agreed. Targeted 2.0.1 version removed.
</comment><comment author="s1monw" created="2015-10-27T16:51:50Z" id="151566155">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to 2.0 on Found from 1.7.3 fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14313</link><project id="" key="" /><description>I tried to upgrade from 1.7.3 yesterday with Alex help and there was a problem even though my cluster passed the migration assistant with all greens except for a not red issue with a boolean.

Here is the exception.  

 [2015-10-26 22:40:44,272][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalStateException: unable to upgrade the mappings for the index [sa_tps_reports], reason: [analyzer on field [raw] must be set when search_analyzer is set]
</description><key id="113625382">14313</key><summary>Upgrade to 2.0 on Found from 1.7.3 fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smayzak</reporter><labels><label>:Mapping</label></labels><created>2015-10-27T16:31:28Z</created><updated>2015-10-30T13:00:26Z</updated><resolved>2015-10-30T12:57:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rborer" created="2015-10-29T11:27:08Z" id="152151963">I had the same issue, on a single node with kibana 4.1.2.

I found the following exception in ES logs:

```
[2015-10-29 11:18:09,862][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalStateException: unable to upgrade the mappings for the index [kibana-int], reason: [Mapper for [_index] conflicts with existing mapping in other types:
[mapper [_index] cannot be changed from type [_index] to [string]]]
Likely root cause: java.lang.IllegalArgumentException: Mapper for [_index] conflicts with existing mapping in other types:
[mapper [_index] cannot be changed from type [_index] to [string]]
        at org.elasticsearch.index.mapper.FieldTypeLookup.checkCompatibility(FieldTypeLookup.java:117)
        at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:345)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:296)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:242)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:329)
        at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:112)
        at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:226)
        at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:85)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</comment><comment author="clintongormley" created="2015-10-30T12:33:39Z" id="152512033">@rborer This is a different error.  Please could you upload your mapping for the kibana index?  I think this might be an old index.
</comment><comment author="clintongormley" created="2015-10-30T12:35:14Z" id="152512259">@smayzak I've changed the migration tool to report this mapping as a "red" error, and I've opened #14383 to make it possible to upgrade with this mapping
</comment><comment author="rborer" created="2015-10-30T12:39:14Z" id="152512857">@clintongormley Thanks, here is my current Kibana mapping:

```
{
  "kibana-int" : {
    "mappings" : {
      "temp" : {
        "properties" : {
          "dashboard" : {
            "type" : "string"
          },
          "group" : {
            "type" : "string"
          },
          "title" : {
            "type" : "string"
          },
          "user" : {
            "type" : "string"
          }
        }
      },
      "dashboard" : {
        "properties" : {
          "_index" : {
            "type" : "string"
          },
          "_source" : {
            "properties" : {
              "dashboard" : {
                "type" : "string"
              },
              "group" : {
                "type" : "string"
              },
              "title" : {
                "type" : "string"
              },
              "user" : {
                "type" : "string"
              }
            }
          },
          "_type" : {
            "type" : "string"
          },
          "_version" : {
            "type" : "long"
          },
          "dashboard" : {
            "type" : "string"
          },
          "exists" : {
            "type" : "boolean"
          },
          "group" : {
            "type" : "string"
          },
          "title" : {
            "type" : "string"
          },
          "user" : {
            "type" : "string"
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-10-30T12:57:38Z" id="152516317">Thanks @rborer.  That index is from kibana3, which won't work with kibana4 anyway, so you can go ahead and delete it.
</comment><comment author="rborer" created="2015-10-30T13:00:26Z" id="152516763">@clintongormley fool me, I completely forgot about that kibana3 index! Thanks a lot, I'll get rid of it asap.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Out of memory error with elasticsearch-2.0.0-rc1 on windows server 2012</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14312</link><project id="" key="" /><description>I'm currently evaluating ELK for my company. I played successfully with logstash-2.0.0-betaX and elasticsearch-2.0.0-betaX and upgraded to elasticsearch-2.0.0-rc1 a few days ago.

Since then, I get a "java.lang.OutOfMemoryError: GC overhead limit exceeded" after a few minutes of elasticsearch running.

Here is my setup:
- ELK running on windows server 2012 with 8GB Ram
- logstash-2.0.0-beta3 indexing log files (accessible through network shared file) and logs in oracle databases (logstash-input-jdbc)
- elasticsearch-2.0.0-rc1 with shield plugin

elasticsearch.yml configuration major changes:

```
threadpool.index.queue_size: 1000
threadpool.search.queue_size: 10000
threadpool.bulk.queue_size: 100
```

ES_HEAP_SIZE

```
set ES_MIN_MEM=2g
set ES_MAX_MEM=2g
```

**The exact same set up with elasticsearch-2.0.0-beta1 works fine.**

Here is the complete stacktrace:

```
[2015-10-27 15:07:15,107][INFO ][monitor.jvm              ] [node_test_dc_1] [gc][old][489][24] duration [5.3s], collections [1]/[5.4s], total [5.3s]/[1.5m], memory [1.5gb]-&gt;[1.5gb]/[1.9gb], all_pools {[young] [243.1mb]-&gt;[245.5mb]/[268.5mb]}{[survivor] [0b]-&gt;[0b]/[205mb]}{[old] [1.3gb]-&gt;[1.3gb]/[1.3gb]}
[2015-10-27 15:09:11,562][INFO ][monitor.jvm              ] [node_test_dc_1] [gc][old][521][62] duration [7s], collections [1]/[7s], total [7s]/[3.4m], memory [1.5gb]-&gt;[1.5gb]/[1.9gb], all_pools {[young] [258.3mb]-&gt;[259.7mb]/[268.5mb]}{[survivor] [0b]-&gt;[0b]/[205mb]}{[old] [1.3gb]-&gt;[1.3gb]/[1.3gb]}
[2015-10-27 15:11:52,501][WARN ][index.engine             ] [node_test_dc_1] [etl-2015.10.14][0] Failed to close SearcherManager
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.lang.SecurityManager.checkPackageAccess(SecurityManager.java:1563)
    at java.lang.Class.checkPackageAccess(Class.java:2372)
    at java.lang.Class.checkMemberAccess(Class.java:2351)
    at java.lang.Class.getMethod(Class.java:1783)
    at org.apache.lucene.store.MMapDirectory$2$1.run(MMapDirectory.java:289)
    at org.apache.lucene.store.MMapDirectory$2$1.run(MMapDirectory.java:286)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.apache.lucene.store.MMapDirectory$2.freeBuffer(MMapDirectory.java:286)
    at org.apache.lucene.store.ByteBufferIndexInput.freeBuffer(ByteBufferIndexInput.java:378)
    at org.apache.lucene.store.ByteBufferIndexInput.close(ByteBufferIndexInput.java:357)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:96)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:83)
    at org.apache.lucene.codecs.lucene50.Lucene50CompoundReader.close(Lucene50CompoundReader.java:120)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:96)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:83)
    at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:152)
    at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:169)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:359)
    at org.apache.lucene.index.FilterDirectoryReader.doClose(FilterDirectoryReader.java:134)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:130)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:274)
    at org.apache.lucene.search.ReferenceManager.swapReference(ReferenceManager.java:62)
    at org.apache.lucene.search.ReferenceManager.close(ReferenceManager.java:146)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:96)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:83)
    at org.elasticsearch.index.engine.InternalEngine.closeNoLock(InternalEngine.java:954)
    at org.elasticsearch.index.engine.Engine.failEngine(Engine.java:517)
    at org.elasticsearch.index.engine.Engine.maybeFailEngine(Engine.java:556)
    at org.elasticsearch.index.engine.InternalEngine.maybeFailEngine(InternalEngine.java:886)
```

Heap dump analyze:
![es_leak_suspects](https://cloud.githubusercontent.com/assets/10754469/10764138/081edfd2-7ccd-11e5-8197-91484126b011.png)
</description><key id="113620331">14312</key><summary>Out of memory error with elasticsearch-2.0.0-rc1 on windows server 2012</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raphaele81</reporter><labels /><created>2015-10-27T16:07:58Z</created><updated>2015-10-27T16:14:47Z</updated><resolved>2015-10-27T16:14:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-27T16:14:30Z" id="151554926">Hi, thanks for testing. this is an issue with that RC with filterreaders not evicting properly, fixed by:
- https://github.com/elastic/elasticsearch/pull/14070
- https://github.com/elastic/elasticsearch/pull/14071
- https://github.com/elastic/elasticsearch/pull/14084
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup plugin security</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14311</link><project id="" key="" /><description>- plugin authors can use full policy syntax, including codebase substitution
  properties like core syntax.
- simplify test logic.
- move out test-framework permissions to separate file.
</description><key id="113614074">14311</key><summary>Cleanup plugin security</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T15:41:21Z</created><updated>2015-10-28T00:07:43Z</updated><resolved>2015-10-28T00:07:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-27T19:55:50Z" id="151624768">this looks awesome! I love the cleanups related to test framework! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Registering Streams for SignificantTerms and MovAvg is broken for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14310</link><project id="" key="" /><description>this is a spin-off from #14306 Registering streams requires modifying a static map which is broken for instance if we start/stop a node that loads a plugin that tries to register the same stream then multiple times. We really need to clean up this API or close it down to build-in only.
</description><key id="113613603">14310</key><summary>Registering Streams for SignificantTerms and MovAvg is broken for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>adoptme</label><label>PITA</label></labels><created>2015-10-27T15:39:11Z</created><updated>2017-02-14T14:33:34Z</updated><resolved>2017-02-14T14:33:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2017-02-14T14:33:33Z" id="279722844">I fixed this six months ago.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing: Ensure port binding does not interfere with other JVMs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14309</link><project id="" key="" /><description>The netty tests currently check for an unused port and use that one to not
bind anything on it, and then check again if nothing is listening on that port.
The problem with this approach is, that other JVMs can bind on this in the meanwhile
making that check useless.

This commit changes the approach to try to bind to a privileged port for all the ports
that should not be used. So in case a wrong port is used, an exception will be thrown because
the binding wont be allowed by the operating system and the test thus fails.

TODO for the reviewer: Does this approach also work under windows? Are the ports privileged there as well and cannot be bound to?

Closes #14230
</description><key id="113609545">14309</key><summary>Testing: Ensure port binding does not interfere with other JVMs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label></labels><created>2015-10-27T15:21:02Z</created><updated>2016-03-10T14:55:14Z</updated><resolved>2016-03-10T14:55:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-19T13:09:22Z" id="158052841">@spinscale The CLA check fails. It seems you have messed up your Git configuration.
</comment><comment author="rmuir" created="2015-11-19T13:36:39Z" id="158058522">&gt; TODO for the reviewer: Does this approach also work under windows? Are the ports privileged there as well and cannot be bound to?

Well the bigger issue is the test wants to get "connection refused" but even that is not guaranteed everywhere. For example on freebsd with net.inet.tcp.blackhole = 2, you won't get an RST even for "localhost" in a jail.
</comment><comment author="spinscale" created="2016-03-10T14:55:13Z" id="194886673">stale. closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove query_binary, filter_binary &amp; aggs_binary </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14308</link><project id="" key="" /><description>We support `query_binary`, `filter_binary` and `aggs_binary` so a query, filter (post_filter actually) or aggregations can be provided in binary base64 format as part of the search request (plus other apis that support those sections).

This feature is undocumented and untested, at the point that we partially ended up removing it as part of the search refactoring (we currently don't parse query_binary anymore on the coordinating node).

I am opening this up for discussion, I never saw this feature used in real life, but maybe it is effectively used, anyways we should figure out whether we want to remove it or maintain it (then document it and test it too).
</description><key id="113594783">14308</key><summary>Remove query_binary, filter_binary &amp; aggs_binary </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T14:23:25Z</created><updated>2016-02-15T15:22:38Z</updated><resolved>2016-02-15T15:22:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-27T14:31:12Z" id="151517632">+1 to remove (I also haven't ever seen it and this is first time I've ever heard of it as well)
</comment><comment author="jpountz" created="2015-10-27T14:40:46Z" id="151522574">+1
</comment><comment author="javanna" created="2015-10-28T18:06:00Z" id="151935572">I did some more digging and I think this binary thing is only used internally for situations where we don't parse on the coordinating node and the request source is streamed as a bytes array. This is all going away with the search refactoring so we can safely remove. The query and filter part is already almost obsolete (last usage is in validate query but I am working on it), the aggs_binary will stay around till the aggs refactoring is completed I believe.
</comment><comment author="javanna" created="2015-11-03T13:23:31Z" id="153350884">`query_binary` and `filter_binary` have been removed as part of #14433. Only `aggs_binary` is left, which will be removed as part of the aggs refactoring that is being worked on. assigning @colings86 who is on it.
</comment><comment author="colings86" created="2016-02-15T15:22:38Z" id="184251447">Done in https://github.com/elastic/elasticsearch/pull/14136
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stopwords are ignored in aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14307</link><project id="" key="" /><description>Hi,
I have setup my index with the following setting and afterwards aggreations on the index. But the settings seems to be ignored, is that how it should be?

``` JSON
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_stop": {
                    "type":       "stop",
                    "stopwords":  "_danish_"
                }
            }
        }
    },
    "mappings": {
        "job": {
            "dynamic_templates": [ 
                {
                    "template_1": {
                        "match": "*",
                        "match_mapping_type": "string",
                        "mapping": {
                            "type": "string",
                            "analyzer": "my_stop"
                        }
                    }
                }
            ]
        }
    }
}
```
</description><key id="113588472">14307</key><summary>Stopwords are ignored in aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Eightyplus</reporter><labels /><created>2015-10-27T13:54:42Z</created><updated>2016-02-03T10:26:09Z</updated><resolved>2015-10-29T19:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Eightyplus" created="2015-10-28T08:44:31Z" id="151767548">I tried adding a stopword filter to my "aggs" query and it that didn't work either.

A way to obtain what I want, but certainly not optimal:
("content" has to be replace with the field I'm doing aggs on)

``` JSON
       "query": {
            "filtered": {
                "query": {
                    "match_all": {}
                },
                "filter": {
                    "bool": {
                        "must_not": [
                            { "term": { "content": "a"}},
                            { "term": { "content": "s"}},
                            { "term": { "content": "k"}},
                            { "term": { "content": "og"}},
                            { "term": { "content": "v"}},
                        ]
                    }
                }
            }
        }
```

which actually does not work, since it removes all index object of that type and not just the word :-1: 
</comment><comment author="clintongormley" created="2015-10-29T19:45:46Z" id="152298973">Hi @Eightyplus 

Please join us in the forums instead: http://discuss.elastic.co/.  This issues list is for bug reports and feature requests.
</comment><comment author="mpenet" created="2016-02-03T09:09:39Z" id="179109234">Did you find a solution? I hit the same wall, this was feasible on 1.x using `exclude` in the terms aggregation (using a list of words). However you can no longer mix `exclude` and `include` of different types (`include` in my case has to be a regex).
</comment><comment author="clintongormley" created="2016-02-03T09:17:36Z" id="179113296">@mpenet just convert the list of words to a regex:

```
[ foo,bar,baz ]  -&gt; (foo|bar|baz)
```
</comment><comment author="Eightyplus" created="2016-02-03T09:19:49Z" id="179114124">No I did not find a solution. I realized that the filter removes any document match which contain a "not" word and thought it would require either
1) new code to handle the filter (add request in discuss.elastic.co ..)
2) filter the words before indexing (the text could still be indexed as "not analyzed" to be able to query the entire text)

I thought it was to much trouble and have since moved on.
/Morten
</comment><comment author="mpenet" created="2016-02-03T09:25:53Z" id="179118183">@clintongormley that's what I have at the moment, but it's super slow compared to the 1.x mixed regex + list in include exclude. I am thinking about inverting the logic to mitigate the problem since exclude are executed first, but I doubt I ll reach the perf of 1.x with that. 
</comment><comment author="mpenet" created="2016-02-03T10:26:09Z" id="179150166">to give you an idea, on the same dataset, same query (minus the change for exclude + new aggreg format)
1.7    : 0.7s 
2.1.2 : 2.8s 

There are 500ish terms in the "exclude" clause 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't be lenient in PluginService#processModule(Module)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14306</link><project id="" key="" /><description>We today catch the exception and move on - this is no good, we should just fail
all the way if something can't be loaded or processed
</description><key id="113588131">14306</key><summary>Don't be lenient in PluginService#processModule(Module)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T13:52:48Z</created><updated>2015-10-27T15:54:45Z</updated><resolved>2015-10-27T15:52:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-27T13:54:46Z" id="151503378">+1
</comment><comment author="jaymode" created="2015-10-27T13:54:49Z" id="151503391">+1 on failing here. LGTM!
</comment><comment author="jasontedor" created="2015-10-27T14:02:41Z" id="151505257">I left a nitty comment about a spelling error but otherwise LGTM. :)
</comment><comment author="rjernst" created="2015-10-27T14:21:21Z" id="151511489">LGTM too
</comment><comment author="s1monw" created="2015-10-27T15:32:21Z" id="151542317">@jaymode @rmuir @rjernst @jasontedor i pushed some fixes - of course a test failed
</comment><comment author="rjernst" created="2015-10-27T15:35:20Z" id="151543202">Still LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: aggregations are not supported with search_type=scan</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14305</link><project id="" key="" /><description>Hi

I am getting an 

`elasticsearchIllegalArgumentException aggregations are not supported with search_type=scan`

error when using the java API. 

```
SearchResponse searchResponse = client.prepareSearch(_INDEX)
        .setSearchType(SearchType.SCAN)
    .setTypes(_TYPE)
        .setQuery(qb)
        .setScroll(ScrollTimeout.createScrollTimeoutValue(wait,tu))
        .setSize(SCROLL_SIZE)
        .execute()
        .actionGet();
```

I understand this is a feature that is coming in release 2. In the meantime, is there an interim solution someone can recommend? The number of docs I need to process are in order of 50MM and growing at rate of 1MM/day. The solution I can think of right now is to use scan and create a gazetter (on disc) using Lucene's FST. 

Any help would be much appreciated
</description><key id="113581909">14305</key><summary>Java API: aggregations are not supported with search_type=scan</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">montyhall</reporter><labels /><created>2015-10-27T13:21:36Z</created><updated>2015-10-29T19:43:43Z</updated><resolved>2015-10-29T19:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T19:43:43Z" id="152298496">Aggs are not supported with scan in 2.0.  In 1.x, they were supported, but were recalculated for each subsequent request and returned incorrect values.  Aggs results are calculated over ALL hits, not just the hits that are returned in a single request.

Why are you wanting to use aggs with scanning?  I suggest you make two requests: one with size zero for the aggs, and another with scroll/scan to scroll through all the docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove "query" query and fix related parsing bugs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14304</link><project id="" key="" /><description>We have two types of parse methods for queries: one for the inner query, to be used once the parser is positioned within the query element, and one for the whole query source, including the query element that wraps the actual query.

With the search refactoring we ended up using the former in count, cat count and delete by query, whereas we should have used the former.  It ends up working properly given that we have a registered (deprecated) query called "query", which used to allow to wrap a filter into a query, but this has the following downsides:
1) prevents us from removing the deprecated "query" query
2) we end up supporting a top level query that is not wrapped within a query element (pre 1.0 syntax iirc that shouldn't be supported anymore)

This commit finally removes the "query" query and fixes the related parsing bugs. We also had some tests that were providing queries in the wrong format, those have been fixed too.

Closes #13326
</description><key id="113567137">14304</key><summary>Remove "query" query and fix related parsing bugs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T12:00:39Z</created><updated>2015-10-29T19:46:48Z</updated><resolved>2015-10-27T14:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-27T12:36:45Z" id="151478273">This looks great! I didn't know we supported calling the `_search` API and providing the query without nesting it into a `query` object. I'm wondering if we want to keep supporting that... (out of the scope of this PR)
</comment><comment author="javanna" created="2015-10-27T12:49:26Z" id="151480788">&gt; I didn't know we supported calling the _search API and providing the query without nesting it into a query object

We don't, we used to pre 1.0 I think, but maybe my explanation above was confusing. I just meant that depending on where we are in the parsing process, we use one of the two methods, think of the difference between count for instance, which only supports a query element, and search, which supports all the `SearchParseElement`s (`aggs`, `size` etc.). Hope this clarifies things. Thanks for the review!
</comment><comment author="jpountz" created="2015-10-27T12:51:25Z" id="151481251">ok, great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Open up QueryCache and SearcherWrapper extension points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14303</link><project id="" key="" /><description>This commit makes QueryCache and SearcherWrappoer registration public
otherwise plugins can't access those extension points due to security restrictions.
</description><key id="113566300">14303</key><summary>Open up QueryCache and SearcherWrapper extension points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T11:54:47Z</created><updated>2015-10-27T13:19:42Z</updated><resolved>2015-10-27T13:19:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-27T12:15:36Z" id="151472233">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete by query to not wrap the inner query into an additional query element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14302</link><project id="" key="" /><description>The delete by query plugin used to set the provided body as the query of the SearchSourceBuilder, which means it was going to be wrapped into an additional query element. This ended up working properly only because we have a registered "query" query that makes all the parsing work anyway. That said, this has a side effect: we ended up supporting a query that is not wrapped into a query element on the REST layer, something that should not be supported. Also, we want to remove the deprecated "query" query from master as it is deprecated in 2.x, but it is not possible as long as we need it to properly parse the delete_by_query body.

This is what caused #13326 in the first place, but master has changed in the meantime and will need different changes.

Relates to #13326
</description><key id="113560474">14302</key><summary>Delete by query to not wrap the inner query into an additional query element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugin Delete By Query</label><label>bug</label><label>review</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-27T11:18:05Z</created><updated>2015-10-27T12:15:57Z</updated><resolved>2015-10-27T12:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-10-27T11:29:17Z" id="151462208">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field stats query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14301</link><project id="" key="" /><description>Currently, I use a pattern based indices in order to segregate my data. I use one per day per data set, and when searching supply the set of indices I want either as an index query (as a filter) or on the URL. This can be a little unwieldy with large non-month-aligned ranges due to the simple nature of the pattern matching available.

The fields API would allow me to index based on something other than a date pattern, but would mean I now required two lookups instead of one, i.e.
1/ Use field API to ascertain which indices I need for a given query
2/ Pass the index names in the URI for the search or within an indices query in the DSL.

What would be great would be if there was a field stats query that could be used as a filter in a search DSL order to perform the above steps within a single query. That would mean I neither had to rely on patterns nor pass index names in URLs or Filters. (My assumption is that the field stats api call to ascertain the minimum set of indices is far cheaper than searching without limiting the set of indices in the first place). I would have thought this might represent a useful approach for Kibana to use internally, too, unless it's caching the results for the field stats api requests separately to elasticsearch.

This suggestion was based upon a discussion in #10523 
</description><key id="113557675">14301</key><summary>Field stats query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pjcard</reporter><labels /><created>2015-10-27T10:59:28Z</created><updated>2015-10-30T09:37:28Z</updated><resolved>2015-10-29T19:37:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T19:37:52Z" id="152297030">Hi @pjcard 

Elasticsearch would need to perform two requests, just like the client does today.  In fact, this is what Kibana does as well.  The field stats request is very fast, so I believe that Kibana doesn't cache the results but issues a fresh call before each search request (I may be mistaken here though).

Sorry, but i don't want to further complicate the already complicated search API by adding more options for what is essentially sugar.
</comment><comment author="pjcard" created="2015-10-30T09:37:28Z" id="152473405">Hi @clintongormley,

Thanks for your reply. On reading about it, it was a slight surprise to me that the implication of it was that the default way to use elasticsearch has changed to making two RESTful calls, instead of one; or to put it another way, that the only part of the process of searching that has to be performed separately is the initial step of ascertaining which indices should be queried. (Given that it is fast, could it be applied implicitly based on the search DSL, instead? Or explicitly as a filter without options?)

In any case, thanks for the response. The information is much appreciated.
Paul
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function_score's min_score option does not play well with caching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14300</link><project id="" key="" /><description>`function_score` has a `min_score` option that allows to reject documents whose score is not above a given threshold. The problem is that the produced scores depend on the query norm/boost so scores might be different depending on where the `function_score` query is in the query tree, which makes it impossible to cache.

Here are the options I can think of:
- only allow to use `min_score` in combination with `boost_mode=replace` (which does not take normalization factors into account)
- stop supporting `min_score`
- add a hack to disable caching when `min_score` is used
</description><key id="113538956">14300</key><summary>function_score's min_score option does not play well with caching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>bug</label><label>discuss</label></labels><created>2015-10-27T09:22:46Z</created><updated>2016-01-29T20:01:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T19:34:14Z" id="152296217">@jpountz just to give some context: imagine your query is a has_child query which is using this technique to score parents https://github.com/elastic/elasticsearch/issues/2917, you can use `min_score` to exclude parents that don't have a high enough score.
</comment><comment author="jpountz" created="2015-10-30T09:53:32Z" id="152476455">Right, I understand that this can be useful. I'm just concerned that if you use this feature in a sub-tree of the query that doesn't need scores (ie. either a filter or the top-level query if you don't sort on the score) then you will hit bugs.
</comment><comment author="clintongormley" created="2016-01-29T20:01:12Z" id="176944397">With the changes of boost -&gt; boost query and the switch in Lucene 6 to BM25, which doesn't use query norm, would it be possible to cache this query in those circumstances?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>usage quota on every data path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14299</link><project id="" key="" /><description>Currently, cluster.routing.allocation.disk.watermark.low and cluster.routing.allocation.disk.watermark.high , these two parameters control the disk allocation. But both of them calculate the usage and free space of the whole disk not the specified path. It works only when there is only one elasticsearch instance running on the disk and it will not work when there are two or more instances running on the same disk. Does elasticsearch have a setting to control the data path usage or path disk quota? If we have this setting, then we can deploy two or more elasticsearch instances using same disks. 
</description><key id="113525887">14299</key><summary>usage quota on every data path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yiguolei</reporter><labels /><created>2015-10-27T07:54:41Z</created><updated>2017-04-11T17:30:02Z</updated><resolved>2015-10-29T19:31:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T19:31:21Z" id="152295343">@yiguolei I don't understand why would you want to run two instances on the same disks?  You're splitting the I/O between two instances.  

Also, a path doesn't have a max amount of space.  Only the disk has that.  
</comment><comment author="simpleopen" created="2017-04-11T17:30:02Z" id="293337675">Hi, @clintongormley ,

Tentatively,  I think I have a user case of @yiguolei 's proposal. The feature of disk usage quota per data path can help prevent a single point of issue from causing all-cluster data loss incident. Do you think we can maybe revisit this discussion, and get your feedbacks?

We have a self-hosted Elasticsearch cluster, accepting log feeds from multiple sources. Due to a Logstash configuration error, one feeding source suddenly went crazy by sending a huge amount of data within a short period. We have a very big disk buffer, but it was still eaten up. All the nodes got a disk full, and the cluster&#8217;s functionality came into a halt. It took some time to fix the problem, and incoming data is lost during the gap.

Feels like, a single point of issue should not fail the entire cluster, and we want to look into possibly restricting total disk usage of indices from each source.

On our cluster, each source creates one index per day, with naming pattern source_name-YYYY-MM-DD, and we can get total disk usage per source by command &#8220;du -chs /indices/source_name-* | grep total&#8221;.
Tentatively thinking, maybe we can have a watchdog script to check it, and close or delete the indices that exceed the quota. I am wondering if there is any existing tool for it?

I am also wondering how Elasticsearch Cloud handles disk usage issue, assuming there are also multiple clients on their shared host. Is it deploying one virtual machine for each client or something similar?

Feedbacks and pointers will be highly appreciated.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use NFS as FsRepository for snapshot meet java.nio.file.NoSuchFileException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14298</link><project id="" key="" /><description>My ES version is 1.5.2.
When I use NFS as FsRepo for snapshot, while different shards of same index in different host and username(or even same username but in diffrent uid\gid), 
I got the partial error:

&gt; {
&gt;   "snapshots": [
&gt;      {
&gt;         "snapshot": "test",
&gt;         "repository": "test_snapshot",
&gt;         ...
&gt;         "indices": {
&gt;            "demo": {
&gt;               "shards_stats": {
&gt;                  "initializing": 0,
&gt;                  "started": 0,
&gt;                  "finalizing": 0,
&gt;                  "done": 3,
&gt;                  "failed": 2,
&gt;                  "total": 5
&gt;               },
&gt;               ...
&gt;               "shards": {
&gt;                  "0": {
&gt;                     "stage": "FAILURE",
&gt;                     "stats": {
&gt;                        "number_of_files": 0,
&gt;                        "processed_files": 0,
&gt;                        "total_size_in_bytes": 0,
&gt;                        "processed_size_in_bytes": 0,
&gt;                        "start_time_in_millis": 0,
&gt;                        "time_in_millis": 0
&gt;                     },
&gt;                     "reason": "IndexShardSnapshotFailedException[[demo][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/nfs/test_snapshot/indices/demo/0/__0 (Permission denied)]; "
&gt;                  },
&gt;                  "1": {
&gt;                     "stage": "DONE",
&gt;                     "stats": {
&gt;                        "number_of_files": 4,
&gt;                        "processed_files": 4,
&gt;                        "total_size_in_bytes": 2712,
&gt;                        "processed_size_in_bytes": 2712,
&gt;                        "start_time_in_millis": 1445929403979,
&gt;                        "time_in_millis": 9
&gt;                     }
&gt;                  },
&gt;                  ...

java.nio.file.NoSuchFileException below in ES log:

&gt; Caused by: java.nio.file.NoSuchFileException: /home/nfs/test_snapshot/indices/demo/2
&gt;       at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
&gt;        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
&gt;        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
&gt;        at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:426)
&gt;        at java.nio.file.Files.newDirectoryStream(Files.java:411)
&gt;        at java.nio.file.Files.newDirectoryStream(Files.java:468)
&gt;        at
&gt; org.elasticsearch.common.blobstore.fs.FsBlobContainer.listBlobsByPrefix(FsBlobContainer.java:65)
&gt;        at org.elasticsearch.common.blobstore.fs.FsBlobContainer.listBlobs(FsBlobContainer.java:56)
&gt;        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapsh&gt;ot(BlobStoreIndexShardRepository.java:438)

Should we need a write permission for other users to free user set the NFS user id mapping or make sure both client and server has same username.

Thanks 
</description><key id="113521694">14298</key><summary>Use NFS as FsRepository for snapshot meet java.nio.file.NoSuchFileException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuanyuanking</reporter><labels /><created>2015-10-27T07:16:24Z</created><updated>2015-11-06T08:12:36Z</updated><resolved>2015-10-27T07:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-27T07:33:14Z" id="151402038">Unless you set the repository to read only, then you need a permission to access it in write mode on every node.
</comment><comment author="xuanyuanking" created="2015-10-27T07:45:44Z" id="151403826">default permission of ${index_name}/${shard_num} is 775, should we add a attr param in FsBlobStore createDirectories?   
</comment><comment author="dadoonet" created="2015-10-27T08:57:04Z" id="151418258">I don't think so. May be we should document that in https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html?

@imotov WDYT?
</comment><comment author="imotov" created="2015-11-02T23:08:44Z" id="153187247">@dadoonet it seems to be a common problem with NFS mounts. Not really sure how to deal with it though. This problem is now detected during repository validation, but it's still somewhat confusing. 
</comment><comment author="xuanyuanking" created="2015-11-03T05:48:36Z" id="153251135">@imotov @dadoonet 
My solution of this problem in production is only give all the ${index_name}/${shard_num} dir 777 permission in FsBlobStore, that make different shards in different user can write BlobStoreRepo.
Should we add a config item like "repositories.fs.open_write_permission" and control this? If you think this is nessisary, I can give a PR on it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add aws canned acl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14297</link><project id="" key="" /><description>This PR add Canned ACL support.

User could set cannedACL property, S3 client would use this cannedACL to create s3 object and bucket. 
CannedACL list :  "private", "public-read","public-read-write", "authenticated-read", "log-delivery-write", "bucket-owner-read", "bucket-owner-full-control"

Related to #14103 
</description><key id="113520086">14297</key><summary>Add aws canned acl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T06:59:37Z</created><updated>2016-03-18T03:56:11Z</updated><resolved>2015-10-29T19:19:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-10-27T07:02:09Z" id="151397516">I'm not sure if this is what we want. Please take a look. Still need document changes.
</comment><comment author="dadoonet" created="2015-10-27T07:30:40Z" id="151401660">Thank you so much for doing this!

I left some comments. As you mentioned, please also add documentation.
</comment><comment author="xuzha" created="2015-10-28T01:54:32Z" id="151696032">@dadoonet  I updated the PR and addressed your comments. Would you please take a look if you have time. 
</comment><comment author="dadoonet" created="2015-10-28T02:04:34Z" id="151697482">It looks good to me. :)
</comment><comment author="xuzha" created="2015-10-29T20:19:49Z" id="152306883">Thanks David, merged and : 
2.1 https://github.com/elastic/elasticsearch/commit/754a0db178d612e3d017cbf9636bc0fa682daceb
2.x https://github.com/elastic/elasticsearch/commit/993aae5421dbe91ffd3f4ee5f9507e1559230a34
</comment><comment author="dadoonet" created="2015-10-29T20:25:10Z" id="152308119">@xuzha I left 2 comments on your latest commits.
</comment><comment author="xuzha" created="2015-10-29T20:52:51Z" id="152319114">@dadoonet thanks. Push another commit to 2.1 and 2.x

2.1: https://github.com/elastic/elasticsearch/commit/1fa92ce5875565b745bc41a31034c673c6e45030
2.x: https://github.com/elastic/elasticsearch/commit/0e3418c727f107a81df7a03a21c7ff49dd5d1289
</comment><comment author="JamesBromberger" created="2016-03-17T08:50:17Z" id="197768708">I am using ES 2.2.0 , and setting up a cross account trust where the receiving bucket (in another AWS account0 requires "bucket-owner-full-control", Using AWS CLi confirms I can put objects with the canned ACl into the prefix I have reserved for this; I can list the objects back, and I can read the content back.

However, when trying to set up a backup location in elasticsearch, it fails the verification. My config is:
{ "type": "s3", "settings": { "bucket": "XXX-logs", "region": "ap-southeast-2", "server_side_encryption": "True", "base_path": "XXX/Dev/Elasticsearch/", "buffer_size": "100mb", "canned_acl": "bucket-owner-full-control" } }

However when submitting this I get:
{"error":{"root_cause":[{"type":"repository_verification_exception","reason":"[s3backupsec] path [XXX][Dev][Elasticsearch] is not accessible on master node"}],"type":"repository_verification_exception","reason":"[s3backupsec] path [XXX][Dev][Elasticsearch] is not accessible on master node","caused_by":{"type":"i_o_exception","reason":"Unable to upload object XXX/Dev/Elasticsearch/tests-DJJ3cNL_ScqiNn6P-LQOAw/master.dat-temp","caused_by":{"type":"amazon_s3_exception","reason":"Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: CBD309137D130093)"}}},"status":500}

Which from the S3 logs on the target S3 bucket I see a  403 AccessDenied. The only condition that is required by S3 in my case is the bucket-owner-full-control ACL. It appears that the check in index/snapshots/blobstore/BlobStoreIndexShardRepository.java calling org.elasticsearch.repositories.blobstore.BlobStoreRepository.testBlobPrefix() may not have the canned ACL being added, and thus fails to get set up.

This maybe somewhere around startVerification() in repositories/blobstore/BlobStoreRepository.java.

In my use case, the target bucket is to be write once (no deletes; escrow copy of the backup, so the corresponding "ensure moves are supported" is also likely to fail....
</comment><comment author="clintongormley" created="2016-03-17T13:21:38Z" id="197876911">@JamesBromberger please open a new issue for this
</comment><comment author="xuzha" created="2016-03-17T20:57:28Z" id="198079536">@clintongormley I will take a look this week.
</comment><comment author="JamesBromberger" created="2016-03-18T03:56:11Z" id="198196373">Logged issue: https://github.com/elastic/elasticsearch/issues/17179
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing misspelled words in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14296</link><project id="" key="" /><description>Few words found misspelled throughout the documentation.  
</description><key id="113488887">14296</key><summary>Fixing misspelled words in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonodonnell</reporter><labels><label>docs</label></labels><created>2015-10-27T01:59:05Z</created><updated>2015-10-29T19:26:06Z</updated><resolved>2015-10-29T19:24:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T19:26:06Z" id="152294109">thanks @Dwaligon - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add listener mechanism for failures to send shard failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14295</link><project id="" key="" /><description>This commit adds a listener mechanism for executing callbacks when
exceptional situations occur sending a shard failure message to the
master. The two types of exceptional situations that can occur are if
the master is not known and if the transport request exception handler
is invoked for any reason after sending the shard failed request to the
master. This commit only adds the infrastructure for executing
callbacks when one of these exceptional situations occur; no effort is
made to properly handle the exceptional situations. Some unit tests are
added for ShardStateAction to test that the listener infrastructure is
correct.

Relates #14252
</description><key id="113484042">14295</key><summary>Add listener mechanism for failures to send shard failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2015-10-27T01:14:56Z</created><updated>2015-10-28T10:39:46Z</updated><resolved>2015-10-28T10:39:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-27T08:30:12Z" id="151412989">Great start. Left some comments.
</comment><comment author="jasontedor" created="2015-10-27T14:59:23Z" id="151530506">I've pushed commits responding to your comments. Thanks for the initial review @bleskes.
</comment><comment author="bleskes" created="2015-10-28T08:34:33Z" id="151765093">LGTM. Thx @jasontedor 
</comment><comment author="jasontedor" created="2015-10-28T10:39:46Z" id="151796388">Rebased on master, squashed, and merged into master. Thanks for reviewing @bleskes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove threadpool type cached setting from documentation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14294</link><project id="" key="" /><description>We don't recommend changing the size of the thread pools, and certainly not recommend setting them to unbounded.  Seems like documenting the configuration to modify our thread pools to [cached](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html#_literal_cache_literal) without sufficient warnings will do more harm than good.
</description><key id="113483403">14294</key><summary>Remove threadpool type cached setting from documentation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-10-27T01:11:28Z</created><updated>2015-11-03T02:17:24Z</updated><resolved>2015-11-03T02:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-28T14:21:41Z" id="151859930">I think that we should go further than merely removing it from the documentation, but actually forbidding any thread pool other than the generic thread pool of being of type "cached". The generic thread pool should remain as "cached" because operations that are submitted to this thread pool can not block nor be rejected; we remain very careful about which operations are submitted to this thread pool. I've opened #14336 to address.
</comment><comment author="bleskes" created="2015-10-28T14:31:56Z" id="151862990">I wonder if we should allow to configure the thread pools types at all these days. Might be the simplest to just remove the option and fix the defaults we have now.

On 28 okt. 2015 3:22 PM +0100, Jason Tedornotifications@github.com, wrote:

&gt; I think that we should go further than merely removing it from the documentation, but actually forbidding any thread pool other than the generic thread pool of being of type "cached". The generic thread pool should remain as "cached" because operations that are submitted to this thread pool can not block nor be rejected; we remain very careful about which operations are submitted to this thread pool. I've opened#14336(https://github.com/elastic/elasticsearch/pull/14336)to address.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/14294#issuecomment-151859930).
</comment><comment author="jasontedor" created="2015-10-28T14:38:30Z" id="151866587">I've added the [discuss label](https://github.com/elastic/elasticsearch/labels/discuss) to open up if we want to take this further than merely forbidding the `cached` thread pool type and actually forbidding changing any thread pool type.
</comment><comment author="jpountz" created="2015-10-29T00:06:51Z" id="152036559">+1 to not allow to configure threadpool types at all
</comment><comment author="s1monw" created="2015-10-29T09:03:34Z" id="152118542">+1
</comment><comment author="jasontedor" created="2015-10-29T21:14:41Z" id="152330086">I've closed #14336 in favor of #14367 which takes the original pull request further by forbidding the changing of the thread pool type for any thread pool.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fold IndexCacheModule into IndexModule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14293</link><project id="" key="" /><description>This commit brings all the registration etc. from IndexCacheModule into
IndexModule. As a side-effect to remove a circular dependency between
IndicesService and IndicesWarmer this commit also cleans up IndicesWarmer and
separates the Engine from the warmer.
</description><key id="113454898">14293</key><summary>Fold IndexCacheModule into IndexModule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cache</label><label>:Internal</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-26T21:27:03Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-27T10:45:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-26T23:32:14Z" id="151313878">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parallel snapshot deletes fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14292</link><project id="" key="" /><description>Executing parallel snapshot deletes on a repository can lead to failures.

Flaky test:

```
curl -XPOST localhost:9200/_snapshot/repo1 -d '{
  "type": "fs",
  "settings": {
    "location": "repo1_loc"
  }
}'

curl -XPUT localhost:9200/_snapshot/repo1/snap1?wait_for_completion=true -d '{
}'

curl -XPUT localhost:9200/_snapshot/repo1/snap2?wait_for_completion=true -d '{
}'

curl -XDELETE localhost:9200/_snapshot/repo1/snap1 &amp;  #&#160;parallel
curl -XDELETE localhost:9200/_snapshot/repo1/snap2 &amp;
```

Output:

```
{"acknowledged":true}
{"snapshot":{"snapshot":"snap1","version_id":3000099,"version":"3.0.0-SNAPSHOT","indices":[],"state":"SUCCESS","start_time":"2015-10-26T21:10:21.636Z","start_time_in_millis":1445893821636,"end_time":"2015-10-26T21:10:21.638Z","end_time_in_millis":1445893821638,"duration_in_millis":2,"failures":[],"shards":{"total":0,"failed":0,"successful":0}}}
{"snapshot":{"snapshot":"snap2","version_id":3000099,"version":"3.0.0-SNAPSHOT","indices":[],"state":"SUCCESS","start_time":"2015-10-26T21:10:21.654Z","start_time_in_millis":1445893821654,"end_time":"2015-10-26T21:10:21.655Z","end_time_in_millis":1445893821655,"duration_in_millis":1,"failures":[],"shards":{"total":0,"failed":0,"successful":0}}}
{"acknowledged":true}
{"error":{"root_cause":[{"type":"repository_exception","reason":"[repo1] failed to list snapshots in repository"}],"type":"repository_exception","reason":"[repo1] failed to list snapshots in repository","caused_by":{"type":"no_such_file_exception","reason":"/Users/ywelsch/dev/elasticsearch/distribution/zip/target/integ tests/elasticsearch-3.0.0-SNAPSHOT/repo/repo1_loc/snap-snap1.dat"}},"status":500}
```
</description><key id="113453843">14292</key><summary>Parallel snapshot deletes fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>bug</label></labels><created>2015-10-26T21:20:59Z</created><updated>2017-06-30T10:27:54Z</updated><resolved>2017-06-30T10:27:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2017-06-30T10:27:54Z" id="312233362">Fixed by #22313 (concurrent snapshot deletions are disallowed now)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing misspelled words in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14291</link><project id="" key="" /><description /><key id="113448381">14291</key><summary>Fixing misspelled words in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonodonnell</reporter><labels /><created>2015-10-26T20:49:37Z</created><updated>2015-10-27T01:58:24Z</updated><resolved>2015-10-27T01:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasonodonnell" created="2015-10-27T01:55:01Z" id="151341632">I signed the CLA - check is still failing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple lang-groovy test fails presumably due to classloader issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14290</link><project id="" key="" /><description>this is a 2.2 upwards issue which is likely due to classloader problems. The issue occurs upon cherry-picking a test like this `git cherry-pick d752d69ad1a5d1c5eaec1c3291b97b17360bb1e3`  which basically adds a simple test using a closure:

``` diff
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
@@ -83,6 +83,8 @@ public class GroovySecurityTests extends ESIntegTestCase {
         assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\"value\", v)");
         // Times
         assertSuccess("def t = Instant.now().getMillis()");
+        // groovy time
+        assertSuccess("use(groovy.time.TimeCategory) { new Date(123456789).format('HH') }");
         // GroovyCollections
         assertSuccess("def n = [1,2,3]; GroovyCollections.max(n)");
```

it causes this exception:

```
Started J0 PID(42388@panthor.local).
Suite: org.elasticsearch.script.groovy.GroovySecurityTests
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.plugin:lang-groovy -Dtests.seed=3257189F4994B662 -Dtests.class=org.elasticsearch.script.groovy.GroovySecurityTests -Dtests.method="testEvilGroovyScripts" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=it_CH -Dtests.timezone=America/Belize
FAILURE 6.47s | GroovySecurityTests.testEvilGroovyScripts &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: Unexpected ShardFailures: [shard [[wbu9YNtHQoenb6TcMBSRoA][test][3]], reason [RemoteTransportException[[node_t0][local[1]][indices:data/read/search[phase/query/id]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ScriptException[failed to run inline script [use(groovy.time.TimeCategory) { new Date(123456789).format('HH') }; doc['foo'].value + 2] using lang [groovy]]; nested: IllegalAccessException[Class org.codehaus.groovy.reflection.CachedMethod can not access a member of class edb3a80dafbe8a36227baf55e0419d03a36948bc$_run_closure1 with modifiers "public"]; ], cause [ScriptException[failed to run inline script [use(groovy.time.TimeCategory) { new Date(123456789).format('HH') }; doc['foo'].value + 2] using lang [groovy]]; nested: IllegalAccessException[Class org.codehaus.groovy.reflection.CachedMethod can not access a member of class edb3a80dafbe8a36227baf55e0419d03a36948bc$_run_closure1 with modifiers "public"];
   &gt;    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:301)
   &gt;    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.runAsDouble(GroovyScriptEngineService.java:317)
   &gt;    at org.elasticsearch.search.sort.ScriptSortParser$2$1.get(ScriptSortParser.java:195)
   &gt;    at org.elasticsearch.index.fielddata.NumericDoubleValues$1.get(NumericDoubleValues.java:47)
   &gt;    at org.apache.lucene.search.FieldComparator$DoubleComparator.copy(FieldComparator.java:211)
   &gt;    at org.apache.lucene.search.TopFieldCollector$SimpleFieldCollector$1.collect(TopFieldCollector.java:206)
   &gt;    at org.apache.lucene.search.AssertingLeafCollector.collect(AssertingLeafCollector.java:53)
   &gt;    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:218)
   &gt;    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:169)
   &gt;    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
   &gt;    at org.apache.lucene.search.AssertingBulkScorer.score(AssertingBulkScorer.java:70)
   &gt;    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:800)
   &gt;    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:514)
   &gt;    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:334)
   &gt;    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:125)
   &gt;    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:340)
   &gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:354)
   &gt;    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:351)
   &gt;    at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
   &gt;    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)
   &gt; Caused by: java.lang.IllegalAccessException: Class org.codehaus.groovy.reflection.CachedMethod can not access a member of class edb3a80dafbe8a36227baf55e0419d03a36948bc$_run_closure1 with modifiers "public"
   &gt;    at java.lang.reflect.AccessibleObject.slowCheckMemberAccess(AccessibleObject.java:296)
   &gt;    at java.lang.reflect.AccessibleObject.checkAccess(AccessibleObject.java:288)
   &gt;    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)
   &gt;    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)
   &gt;    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294)
   &gt;    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019)
   &gt;    at groovy.lang.Closure.call(Closure.java:426)
   &gt;    at groovy.lang.Closure.call(Closure.java:420)
   &gt;    at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:112)
   &gt;    at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.access$400(GroovyCategorySupport.java:68)
   &gt;    at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:252)
   &gt;    at org.codehaus.groovy.runtime.DefaultGroovyMethods.use(DefaultGroovyMethods.java:406)
   &gt;    at org.codehaus.groovy.runtime.dgm$755.doMethodInvoke(Unknown Source)
   &gt;    at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:218)
   &gt;    at edb3a80dafbe8a36227baf55e0419d03a36948bc.run(edb3a80dafbe8a36227baf55e0419d03a36948bc:1)
   &gt;    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:296)
   &gt;    ... 22 more
   &gt; ]]
   &gt; Expected: &lt;0&gt;
   &gt;      but: was &lt;1&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([3257189F4994B662:26BC7BE015A09D8A]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures(ElasticsearchAssertions.java:294)
   &gt;    at org.elasticsearch.script.groovy.GroovySecurityTests.assertSuccess(GroovySecurityTests.java:129)
   &gt;    at org.elasticsearch.script.groovy.GroovySecurityTests.testEvilGroovyScripts(GroovySecurityTests.java:87)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /Users/simon/projects/elasticsearch/plugins/lang-groovy/target/J0/temp/org.elasticsearch.script.groovy.GroovySecurityTests_3257189F4994B662-002
  2&gt; NOTE: test params are: codec=Lucene53, sim=RandomSimilarityProvider(queryNorm=false,coord=no): {}, locale=it_CH, timezone=America/Belize
  2&gt; NOTE: Mac OS X 10.11 x86_64/Oracle Corporation 1.8.0_40 (64-bit)/cpus=4,threads=1,free=435997328,total=514850816
  2&gt; NOTE: All tests run in this JVM: [GroovySecurityTests]
Completed [1/1] in 7.51s, 1 test, 1 failure &lt;&lt;&lt; FAILURES!
```

for better readability also apply this:

``` diff
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 93da5ac..8655052 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -588,6 +588,10 @@ public abstract class StreamInput extends InputStream {
                     return (T) readStackTrace(new LockObtainFailedException(readOptionalString(), readThrowable()), this);
                 case 18:
                     return (T) readStackTrace(new InterruptedException(readOptionalString()), this);
+                case 19:
+                    return (T) readStackTrace(new IllegalAccessException(readOptionalString()), this);
+                case 20:
+                    return (T) readStackTrace(new IllegalAccessError(readOptionalString()), this);
                 default:
                     assert false : "no such exception for id: " + key;
             }
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 71558ff..7d83ce1 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -562,6 +562,12 @@ public abstract class StreamOutput extends OutputStream {
             } else if (throwable instanceof InterruptedException) {
                 writeVInt(18);
                 writeCause = false;
+            } else if (throwable instanceof IllegalAccessException) {
+                writeVInt(19);
+                writeCause = false;
+            } else if (throwable instanceof IllegalAccessError) {
+                writeVInt(20);
+                writeCause = false;
             } else {
                 ElasticsearchException ex;
                 if (throwable instanceof ElasticsearchException &amp;&amp; ElasticsearchException.isRegistered(throwable.getClass())) {
```
</description><key id="113442084">14290</key><summary>simple lang-groovy test fails presumably due to classloader issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-10-26T20:16:31Z</created><updated>2016-02-01T08:39:08Z</updated><resolved>2016-02-01T08:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Disk Based Allocation by Node/Node Tag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14289</link><project id="" key="" /><description>Currently today, we have disk based allocation that supports a cluster wide setting. This works great for most deployments where heterogeneous hardware is common, but it often fails when you have a hot/cold deployment with vastly different hardware for the two segments.

For example, if you have hot nodes with 3 TB of super fast storage and cold nodes with 20 TB of slower storage, then you may want to have very different rules for how much space should be available between the two groups.

Depending on your deployment, you may also be able to take advantage of certain properties. For example, the cold nodes may never have to deal with merging due to forced merges occurring on the hot nodes. Conversely, hot nodes then do have to deal with normal and final merging to a single segment. And cold nodes may also benefit in some setups where "summary" indices get created in the place of full data sets at a dramatically reduced size.

For this to work, either the setting would have to allow any node to override the cluster-wide setting, or to allow it to be set based on node tags, just as nodes get grouped that way already.
</description><key id="113426801">14289</key><summary>Disk Based Allocation by Node/Node Tag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Allocation</label><label>discuss</label><label>enhancement</label></labels><created>2015-10-26T18:58:29Z</created><updated>2015-10-29T19:30:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-29T19:14:41Z" id="152291378">And using percentages isn't sufficient?
</comment><comment author="pickypg" created="2015-10-29T19:30:29Z" id="152295145">Any percentage on 3 TB versus 20 TB is going to be a massive difference in absolute terms, when generally you can get by with a smaller percentage on such large drives.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix plugin list command error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14288</link><project id="" key="" /><description>Closes #14287
</description><key id="113415386">14288</key><summary>Fix plugin list command error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-26T18:04:00Z</created><updated>2015-10-26T18:11:22Z</updated><resolved>2015-10-26T18:08:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-26T18:06:18Z" id="151229867">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin tells you to use --list when plugin not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14287</link><project id="" key="" /><description>```
Plugin watcher not found. Run plugin --list to get list of installed plugins.
```

But when you try to use `bin/plugin --list`:

```
$ bin/plugin --list
ERROR: unknown command [--list]. Use [-h] option to list available commands
```

It should show `Run plugin list to get list of installed plugins`, or something of that nature.
</description><key id="113414181">14287</key><summary>bin/plugin tells you to use --list when plugin not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">untergeek</reporter><labels><label>bug</label></labels><created>2015-10-26T17:57:48Z</created><updated>2015-10-28T18:38:36Z</updated><resolved>2015-10-26T18:08:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="untergeek" created="2015-10-26T18:05:18Z" id="151229408">Beat me by 60 seconds :smile: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: run base query tests for more than one random query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14286</link><project id="" key="" /><description>Currently we only run the base tests in AbstractQueryTestCase for one randomly generated query. In order to extend coverage on the query builders and parsers we should run those test on more than one random query, especially since this comes at almost no cost and will increase the chance that when something breaks the query builders or parsers, this will more likely cause an immediate test failure when running `mvn test` locally instead of having to wait several CI cycles.

This PR also contains small change in GeoShapeQueryBuilderTest random query setup. The combination of having a `MULTILINESTRING` random shape together with `SpatialStrategy.TERM` causes very huge lucene queries (often &gt; 8000 terms) and therefore slows down the test quiete significantly, so we try to avoid this combination.
</description><key id="113400957">14286</key><summary>Tests: run base query tests for more than one random query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-26T16:54:19Z</created><updated>2015-10-28T14:40:21Z</updated><resolved>2015-10-28T14:40:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-10-28T09:55:26Z" id="151784800">@javanna could you have a look at this? Added loops to the four base tests shared for all query builders. Also removed one parameter combination in the GeoShapeQueryBuilder test setup that incidentally was making the created query very huge and thus slowed down the test. 
</comment><comment author="javanna" created="2015-10-28T10:04:33Z" id="151788601">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove circular dependency between IndicesService and IndicesStore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14285</link><project id="" key="" /><description>This commit introduces a new IndexStoreConfig that is passed to
IndexStore instances instead it's pretty messy parent service.
</description><key id="113360829">14285</key><summary>Remove circular dependency between IndicesService and IndicesStore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>:Store</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-26T13:59:03Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-26T20:32:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-26T14:04:13Z" id="151143524">Suggested a wording change but otherwise looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fold SimilarityModule into IndexModule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14284</link><project id="" key="" /><description>IndexModule is the central extension point we should centralize the extension mechanism
towards removing guice entirely on the index level.
</description><key id="113354126">14284</key><summary>Fold SimilarityModule into IndexModule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-26T13:25:28Z</created><updated>2015-10-26T13:47:11Z</updated><resolved>2015-10-26T13:47:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-26T13:29:25Z" id="151133406">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport #8201 to 2.0 and disable by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14283</link><project id="" key="" /><description>This relates to #14273
</description><key id="113342016">14283</key><summary>Backport #8201 to 2.0 and disable by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Scripting</label><label>bug</label><label>review</label><label>v2.0.1</label></labels><created>2015-10-26T12:09:46Z</created><updated>2015-10-26T14:29:20Z</updated><resolved>2015-10-26T13:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-26T12:10:20Z" id="151114200">@rmuir @pickypg can you take a look? once this is in I will add that test to the other branches as well
</comment><comment author="rmuir" created="2015-10-26T13:34:03Z" id="151134470">+1. looks great.
</comment><comment author="pickypg" created="2015-10-26T14:29:20Z" id="151155856">+1 Looked GTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Kerberos/SPNEGO Shield custom realm to documentation (also to master and 2.1/2.x)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14282</link><project id="" key="" /><description /><key id="113341981">14282</key><summary>Add Kerberos/SPNEGO Shield custom realm to documentation (also to master and 2.1/2.x)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">salyh</reporter><labels /><created>2015-10-26T12:09:34Z</created><updated>2015-10-26T12:53:22Z</updated><resolved>2015-10-26T12:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added IMAP/POP3 Mailimporter to documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14281</link><project id="" key="" /><description /><key id="113340773">14281</key><summary>Added IMAP/POP3 Mailimporter to documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">salyh</reporter><labels /><created>2015-10-26T12:00:31Z</created><updated>2015-10-26T12:01:56Z</updated><resolved>2015-10-26T12:01:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-26T12:01:56Z" id="151112174">Closing in favour of #14280
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added IMAP/POP3 Mailimporter to documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14280</link><project id="" key="" /><description /><key id="113339876">14280</key><summary>Added IMAP/POP3 Mailimporter to documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">salyh</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-10-26T11:56:07Z</created><updated>2015-10-26T12:45:47Z</updated><resolved>2015-10-26T12:45:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-26T12:01:06Z" id="151111966">Hi @salyh 

Happy to merge this, just wondering if it wouldn't be better to rename the repository to no longer include the word "river"? 
</comment><comment author="salyh" created="2015-10-26T12:12:55Z" id="151114844">done: https://github.com/salyh/elasticsearch-imap
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove guice injection from IndexStore and friends</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14279</link><project id="" key="" /><description>This commit replaces dependency injection from IndexStore and subclasses
and replaces it with dedicated set of dependencies.
</description><key id="113336506">14279</key><summary>Remove guice injection from IndexStore and friends</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-26T11:33:22Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-26T13:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-26T12:43:59Z" id="151121793">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix AckClusterUpdateSettingsIT.testClusterUpdateSettingsAcknowledgement()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14278</link><project id="" key="" /><description>See http://build-us-00.elastic.co/job/es_core_master_oracle_6/3089/

After changes in #14259, "cluster.routing.allocation.cluster_concurrent_rebalance" is now working properly. It needs to be explicitly set to a higher value for this test.
</description><key id="113320085">14278</key><summary>Fix AckClusterUpdateSettingsIT.testClusterUpdateSettingsAcknowledgement()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>review</label><label>test</label></labels><created>2015-10-26T09:51:44Z</created><updated>2015-10-26T10:03:01Z</updated><resolved>2015-10-26T10:03:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-26T10:01:10Z" id="151087542">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeout settings (default to 5 minutes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14277</link><project id="" key="" /><description>When using azure repository, we should set default timeout to 5 minutes. Retry default is 3 times.

``` java
client.getDefaultRequestOptions().setTimeoutIntervalInMs(5 * 60 * 1000);   // 5 minutes
```

See original commit: https://github.com/craigwi/elasticsearch-cloud-azure/commit/69261347ce69d1a3a05e0d672aa962bbe0904811
</description><key id="113311834">14277</key><summary>Add timeout settings (default to 5 minutes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>enhancement</label></labels><created>2015-10-26T09:10:05Z</created><updated>2015-12-29T11:02:33Z</updated><resolved>2015-12-29T11:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mark shard as recovering on the cluster state thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14276</link><project id="" key="" /><description>#13766 removed the previous async trigger of shard recovery with the goal of making it easier to test. The async constructs were folded into IndicesClusterStateService. The change is good but it also made the state managment on the shard level async, while it was previously done on the cluster state thread, before triggering the recoveries (but it was hard to see that). This caused state related race conditions and some build failures (#14115). To fix, the shard state management is now pulled out of the recovery code and made explicit in IndicesClusterStateService, and runs on the cluster state update thread.

 Closes #14115
</description><key id="113235126">14276</key><summary>Mark shard as recovering on the cluster state thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-10-25T16:30:53Z</created><updated>2015-10-29T11:01:46Z</updated><resolved>2015-10-29T11:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-26T10:43:14Z" id="151096847">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flush big merges automatically if shard is inactive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14275</link><project id="" key="" /><description>Today if a shard is marked as inactive after a heavy indexing period
large merges are very likely. Yet, those merges are never committed today
since time-based flush has been removed and unless the shard becomes active
again we won't revisit it.
Yet, inactive shards have very likely been sync-flushed before such that we need
to maintain the sync id if possible which this change tries on a per shard basis.
</description><key id="113182348">14275</key><summary>Flush big merges automatically if shard is inactive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-24T20:49:40Z</created><updated>2015-10-26T11:15:50Z</updated><resolved>2015-10-26T10:41:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-24T20:49:53Z" id="150849733">@bleskes @mikemccand can you take a look
</comment><comment author="mikemccand" created="2015-10-25T10:02:32Z" id="150908727">I left a few comments ... I think this is an important change, to promptly free up possibly large amounts of transient disk tied up by "ghost segments" stuck in between worlds.
</comment><comment author="s1monw" created="2015-10-26T09:38:13Z" id="151078964">@mikemccand do you wanna take another look I applied some changes based on your comments
</comment><comment author="mikemccand" created="2015-10-26T10:01:03Z" id="151087521">LGTM, thanks @s1monw
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add workaround for JDK-8014008</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14274</link><project id="" key="" /><description>We should not implement this method, it is a real problem to do that. 

But I think it is ok to workaround the JDK bug (https://bugs.openjdk.java.net/browse/JDK-8014008). We can just give it new empty mutable permissions (no permissions at all) and otherwise return UNSUPPORTED which is the right thing to do. Under normal operation of ES this method never even gets called, so its not a performance issue.

This allows jconsole/visualvm to work in the meantime.

I would like to see progress on the jdk bug before pushing though, it should not have to come from me. The track record/attitude around RMI on security is not good (e.g.: https://bugs.openjdk.java.net/browse/JDK-8035404) and this is their bug to fix, we can't maintain these workarounds forever.
</description><key id="113129710">14274</key><summary>Add workaround for JDK-8014008</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>jvm bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-24T02:28:39Z</created><updated>2015-11-05T13:34:59Z</updated><resolved>2015-11-05T13:24:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-10-24T06:30:56Z" id="150767146">LGTM.
</comment><comment author="otisg" created="2015-10-27T17:48:48Z" id="151584864">@rmuir any chance of this getting into 2.0 or is 2.0.x or maybe even 2.1 more likely? Thanks.
</comment><comment author="ywelsch" created="2015-11-05T10:50:24Z" id="154028492">Tested it / LGTM. We should add this for 2.0.1 / 2.1.
</comment><comment author="rmuir" created="2015-11-05T11:40:35Z" id="154037957">I am still waiting on something to happen on https://bugs.openjdk.java.net/browse/JDK-8014008

See the issue description. If i push the change, then it becomes my responsibility to get the hack removed.
</comment><comment author="rmuir" created="2015-11-05T13:23:38Z" id="154058129">@ywelsch has nicely taken on the pain of researching the situation in openjdk so we can remove our hack in the future :) Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy script in aggregation not working in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14273</link><project id="" key="" /><description>Here's the aggregation expressed in elasticsearch-dsl-py (which works fine in 1.x):

```
s.aggs \
  .bucket('hour', 'terms', script= \
          'use(groovy.time.TimeCategory) { \
            new Date(doc["timestamp"].value).format("HH") \
          }', size=0, order={'_term': 'asc'}) \
  .metric('bandwidth', 'sum', field='size') \
    .bucket('action', 'terms', field='action', size=0) \
    .metric('bandwidth', 'sum', field='size')
```

In 2.0 we get this error:

```
RemoteTransportException[[Longshot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: 
QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: 
GroovyScriptExecutionException[failed to run inline script 
[use(groovy.time.TimeCategory) {                 new Date(doc["timestamp"].value).format("HH")               }] using lang [groovy]]; nested: 
NoClassDefFoundError[Could not initialize class org.codehaus.groovy.vmplugin.v7.IndyInterface];
```

If we can no longer use groovy.time.TimeCategory, is there an alternative way to get hour buckets coalesced across multiple days from timestamps?  Thanks in advance!
</description><key id="113108661">14273</key><summary>Groovy script in aggregation not working in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">reflection</reporter><labels><label>:Scripting</label><label>bug</label><label>v2.0.1</label></labels><created>2015-10-23T22:09:20Z</created><updated>2015-10-26T14:10:45Z</updated><resolved>2015-10-26T14:10:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-10-23T22:27:17Z" id="150707140">Can you check the log of the node for the cause of the `Could not initialize class org.codehaus.groovy.vmplugin.v7.IndyInterface` error? That's not the root error, rather something else failed to let that happen.
</comment><comment author="reflection" created="2015-10-23T22:38:13Z" id="150708868">This is the full error from a recent run: https://gist.github.com/reflection/a62d981f9f1e499e7c8e
</comment><comment author="pickypg" created="2015-10-24T01:12:07Z" id="150730643">Hi @reflection, can you try to find the original run of the script? Once that kind of error happens, it repeats? _If_ you can restart the node, then run it to find the first occurrence (if it's difficult without restarting), then that would be good.
</comment><comment author="rmuir" created="2015-10-24T02:53:47Z" id="150745502">I think its a legit bug: see http://www.groovy-lang.org/indy.html

Even though we have not enabled indy (https://github.com/elastic/elasticsearch/pull/8201), we ship an indy jar in 2.0, hence it still will be used for core groovy classes. 

To fix this for 2.0, we'd need to either backport that change completely, or at least backport policy file changes from that PR. And of course a test that is like @reflection script that uses one of these classes so we know its working.
</comment><comment author="clintongormley" created="2015-10-24T08:47:53Z" id="150777269">Would a workaround be to delete the indy jar for now?
</comment><comment author="rmuir" created="2015-10-24T12:38:33Z" id="150806340">no clue. first things first, we need a failing unit test.
</comment><comment author="rmuir" created="2015-10-24T12:41:37Z" id="150807444">i also don't think we should encourage users to manually mess with jars like that. we should just fix the bug....
</comment><comment author="pickypg" created="2015-10-24T15:56:54Z" id="150828948">Yeah, that's why I was trying to find the core error. Curious if it's a security manager error that trips up the `IndyInterface` (as we found with #8201) or if it's something else.
</comment><comment author="rmuir" created="2015-10-24T16:35:38Z" id="150831877">I'm sure its probably that. You probably get NoClassDefFound because there was an earlier security exception before when trying to do the static init, as the indy stuff will not work.

So I think we just need to have a test that uses standard groovy function to ensure invokeDynamic is working.
</comment><comment author="pickypg" created="2015-10-24T17:19:39Z" id="150835098">I just tried this in a script field in 2.0 RC1 and it did fail:

``` http
PUT /test/type/1
{
  "message" : "this is text",
  "timestamp" : "2015-10-24T17:12:00.000Z"
}

GET /test/_search
{
  "script_fields": {
    "hours": {
      "script": "use(groovy.time.TimeCategory) { new Date(doc['timestamp'].value).format('HH') }"
    }
  }
}
```

This generated the `ExceptionInInitializerError`:

```
Caused by: java.lang.ExceptionInInitializerError
        at org.codehaus.groovy.vmplugin.v7.Java7.invalidateCallSites(Java7.java:66)
        at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.newScope(GroovyCategorySupport.java:78)
        at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:109)
        at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.access$400(GroovyCategorySupport.java:68)
        at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:252)
        at org.codehaus.groovy.runtime.DefaultGroovyMethods.use(DefaultGroovyMethods.java:406)
        at org.codehaus.groovy.runtime.dgm$755.invoke(Unknown Source)
        at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoMetaMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:251)
        at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.callCurrent(PogoMetaMethodSite.java:59)
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:174)
        at 6500c935a9b77886d78797d577be9c2be71f03d0.run(6500c935a9b77886d78797d577be9c2be71f03d0:1)
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:248)
        ... 10 more
Caused by: java.security.AccessControlException: access denied ("java.util.PropertyPermission" "groovy.indy.logging" "read")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkPropertyAccess(SecurityManager.java:1294)
        at java.lang.System.getProperty(System.java:717)
        at org.codehaus.groovy.vmplugin.v7.IndyInterface.&lt;clinit&gt;(IndyInterface.java:76)
        ... 24 more
```

Core caused by:

```
 java.security.AccessControlException: access denied ("java.util.PropertyPermission" "groovy.indy.logging" "read")
```
</comment><comment author="pickypg" created="2015-10-24T17:21:27Z" id="150835601">Related to https://github.com/apache/incubator-groovy/pull/119
</comment><comment author="rmuir" created="2015-10-24T17:27:51Z" id="150835859">To me the safest option for a 2.0.x fix would be:
- add test to GroovySecurityIT
- backport https://github.com/elastic/elasticsearch/pull/8201 in its entirety but just change default to false. it can be enabled in 2.1
</comment><comment author="s1monw" created="2015-10-26T12:04:00Z" id="151112597">@reflection can you tell me what what JVM you are using?
</comment><comment author="s1monw" created="2015-10-26T14:10:45Z" id="151145287">this is fixed by #14283 for `2.0.1` the first time the class is accessed we get a security excepiton and all subsequent tries get a shadowing `NoClassDefFoundError`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update indices-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14272</link><project id="" key="" /><description>The current description of the query seemed confusing to some people (e.g. https://twitter.com/wielinde/status/654582620630687744), this is just a proposal to simplify it.
</description><key id="113082869">14272</key><summary>Update indices-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels /><created>2015-10-23T19:15:09Z</created><updated>2016-01-18T07:50:08Z</updated><resolved>2015-10-23T21:00:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T19:59:53Z" id="150675690">LGTM but I wonder if we really need that query? it's not related to this PR and you should merge as it is but I wonder what the real usecases are and if it can't be build with other tools? @clintongormley do you know what the history is here?
</comment><comment author="javanna" created="2015-10-23T20:21:14Z" id="150680048">thanks for picking this up @cbuescher !
</comment><comment author="cbuescher" created="2015-10-23T21:00:16Z" id="150691587">@s1monw thanks, I think I saw this query beeing used when searching an alias that covers indices with different schemas, so you need two slightly different queries for either of these. Not sure if there is a good replacement for this usecase.
</comment><comment author="clintongormley" created="2015-10-24T08:42:05Z" id="150777086">We plan to remove this query in the future: https://github.com/elastic/elasticsearch/issues/12017
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo Location Context suggester misses entities within the precision distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14271</link><project id="" key="" /><description>Hi,

We use the Completion/context suggester with geo hashes to power our autocomplete experience. ES client and server version is 1.3.4. We use 3 precisions: 4, 5 and 6. Neighbours are also enabled. 

However we noticed that some entities within the precision distance are not returned by the suggester. 

Eg: Precision 5 is approx. 3 miles x 3 miles.
We make a suggestion request from this lat/lng: 37.297304, -122.044645, for a safeway that is located at 37.281433, -122.031138. But it does not show up for precision 5 suggestion request.

The safeway shows up for precisions 4 and 5 requests from a lat/lng very close to it. So it is a a valid entity.

This is consistently noticeable for all 3 precisions. Since neighbours are enabled we expect this to work correctly.

Is this a known issue? Has this been fixed in the newer versions?

Here is the completion suggester definition:

{
  "entity" : {
    "properties" : {
      "name_suggest" : {
        "type" : "completion",
        "index_analyzer" : "standard_synonym_analyzer",
        "search_analyzer" : "standard_synonym_analyzer",
        "context": {
          "location": {
            "type": "geo",
            "precision": [4,5,6],
            "neighbours": true
          },
          "type" : {
            "type" : "category",
            "path" : "radius_type",
            "default" : ["local"]
          }
        }
      }
    }
  }
}

Thanks,
Lakshmi
</description><key id="113081802">14271</key><summary>Geo Location Context suggester misses entities within the precision distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lakshmithyagarajan</reporter><labels><label>:Suggesters</label><label>feedback_needed</label></labels><created>2015-10-23T19:08:12Z</created><updated>2016-01-29T19:11:25Z</updated><resolved>2016-01-29T19:11:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-24T08:34:16Z" id="150775642">Hi @lakshmithyagarajan 

Could you give us some concrete data points, ie some docs to index, and the completion requests that aren't working correctly?

thanks
</comment><comment author="clintongormley" created="2016-01-29T19:11:25Z" id="176917324">Nothing further. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain api: move query parsing to the coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14270</link><project id="" key="" /><description>Similarly to what we did with the search api, we can now also move query parsing on the coordinating node for the explain api. Given that the explain api is a single shard operation (compared to search which is instead a broadcast operation), this doesn't change a lot in how the api works internally. The main benefit is that we can simplify the java api by requiring a structured query object to be provided rather than a bytes array that will get parsed on the data node. Previously if you specified a QueryBuilder it would be serialized in json format and would get reparsed on the data node, while now it doesn't go through parsing anymore (as expected), given that after the query-refactoring we are able to properly stream queries natively.
</description><key id="113062144">14270</key><summary>Explain api: move query parsing to the coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-23T17:15:27Z</created><updated>2015-10-28T09:47:31Z</updated><resolved>2015-10-28T09:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-23T18:31:56Z" id="150657317">Looks right to me.
</comment><comment author="s1monw" created="2015-10-28T09:34:55Z" id="151780189">LGTM thanks @javanna 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to spatial4j 0.5 for correct Multi-Geometry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14269</link><project id="" key="" /><description>A long time coming this upgrades to Spatial4J 0.5 which includes the fix for calculating a Multi-geometry bounding box. Duplicate code is removed

closes #9904
</description><key id="113061653">14269</key><summary>Update to spatial4j 0.5 for correct Multi-Geometry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>review</label><label>upgrade</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-23T17:12:06Z</created><updated>2015-11-20T14:07:29Z</updated><resolved>2015-10-28T02:35:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T20:01:06Z" id="150675912">this PR is missing updates for license files can you add it?
</comment><comment author="nknize" created="2015-10-26T13:26:22Z" id="151132518">signature and license files updated.
</comment><comment author="s1monw" created="2015-10-26T13:27:28Z" id="151132798">LGTM - @clintongormley does the notice file etc look good to? why didn't we have one before?
</comment><comment author="nknize" created="2015-10-26T13:30:54Z" id="151133725">@s1monw Spatial4j moved to locationtech.org which is under the Eclipse Foundation. The notice file is the eclipse SUA and needs to be provided along with the "unless otherwise indicated" license file (in this case ASL2.0).  
</comment><comment author="clintongormley" created="2015-10-26T13:48:01Z" id="151138658">NOTICE file looks good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security Manager breaks JConsole and VisualVM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14268</link><project id="" key="" /><description>Hi,

Short version: with security manager enabled one cannot connect to ES with JConsole or VisualVM - permission denied.  I'm assuming this is not a known issue because it seems like a pretty significant problem.

See https://discuss.elastic.co/t/security-policy-in-2-0-breaks-jconsole-and-visualvm/32861
</description><key id="113051378">14268</key><summary>Security Manager breaks JConsole and VisualVM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels /><created>2015-10-23T16:11:09Z</created><updated>2015-12-05T09:25:24Z</updated><resolved>2015-10-23T16:16:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-23T16:16:50Z" id="150623078">Looks like JConsole/VisualVM are the broken ones, not the other way around.

`attempt to add a Permission to a readonly Permissions object` in that stack indicates bugs in their code, not ours. See javadocs for Policy: 

`Applications are discouraged from calling this method since this operation may not be supported by all policy implementations. Applications should solely rely on the implies method to perform policy checks.`

They are getting this value because we don't implement `getPermissions(CodeSource)/getPermission(ProtectionDomain)` nor should we since its not necessary. And they are not checking the result value for whatever it is they are doing to see if its http://docs.oracle.com/javase/8/docs/api/java/security/Policy.html#UNSUPPORTED_EMPTY_COLLECTION for optional methods.
</comment><comment author="s1monw" created="2015-10-23T19:56:56Z" id="150675103">&gt; Looks like JConsole/VisualVM are the broken ones, not the other way around.

I agree with this observation we can't and shouldn't do anything on our side to compensate for problems like these in 3rd party applications. It's just not maintainable and sacrificing security for problems like this is not an option given all the history along those lines. I really think this should be communicated to the folks that maintain these apps. @otisg do you wanna help here communicating this? you seem to be a user of this and I guess you can push them a bit?
</comment><comment author="rmuir" created="2015-10-23T23:59:46Z" id="150720208">There is already a bug report for it: https://bugs.openjdk.java.net/browse/JDK-8014008
</comment><comment author="samcday" created="2015-12-05T08:14:25Z" id="162160291">We were using JMX + Datadog JMX integration to get heap + gc stats for Elasticsearch into Datadog. Is there any workaround or are we 100% screwed here? :)
</comment><comment author="rmuir" created="2015-12-05T09:24:52Z" id="162165029">You gotta at least do your part and use the latest version of elastisearch: #14274
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Aggregation for Bucketing on Value Change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14267</link><project id="" key="" /><description>A recent requirement to solve the gaps and island problem using Elasticsearch.

https://www.simple-talk.com/sql/t-sql-programming/the-sql-of-gaps-and-islands-in-sequences

Currently, this is being solved with an inefficient aggregation script.

This ticket proposes we support the ability to create buckets when the value for a field changes.  This assumes data is sorted appropriately prior to the aggregation.

For example, consider the following data.  The documents are sorted by the "doc_id" field. We create buckets on changes on the "value" field.

{ "index": { "_index": "test", "_type": "test", "_id": "1" } }
{ "value": "A"}
{ "index": { "_index": "test", "_type": "test", "_id": "2" } }
{ "value": "A" }
{ "index": { "_index": "test", "_type": "test", "_id": "3" } }
{ "value": B"}
{ "index": { "_index": "test", "_type": "test", "_id": "4" } }
{ "value": "A"}
{ "index": { "_index": "test", "_type": "test", "_id": "5" } }
{ "value": "A" }

Here we end up with 3 buckets. "A" - 2, "B" -1, "A" - 2

This would require us to label buckets with appropriately distinct names - possibly with the sequence range.
The sort field above could easily be a timestamp and the field to detect change on a numeric.

_What constitutes a change and thus new bucket?_

I think this largely depends on type.  If a string field is used, the change should be discrete values i.e. any change in value.  For other types we could support different models with a "model" parameter. For starters,  numerics could bucket based on the differential - the user would be required to specify the desired interval as a rate of change e.g. if they expect a continous gap &gt; 0, 0 if they expect values to not change.  
Like any aggregation metrics/sub aggregations could be applied as desired.

Proposed syntax:

```
GET /test/_search
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "islands": {
      "change": {
        "model":"linear",
        "rate":0,
        "field":"value"
      }
    }
  }
}

```

Thanks
Dale
</description><key id="113041575">14267</key><summary>Support Aggregation for Bucketing on Value Change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-10-23T15:21:10Z</created><updated>2017-05-12T13:24:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gingerwizard" created="2015-10-23T15:22:39Z" id="150606555">@markharwood @polyfractal Please comment.
</comment><comment author="gingerwizard" created="2015-10-23T18:01:05Z" id="150648387">Correction: The proposal is to do this as a pipeline aggregation.  The above syntax above is therefore incorrect. The issue discussing @HonzaKral is that to make this useful its unlikely people will want to "sample" buckets and will want to inspect each value - and thus lots of buckets, which will be expensive.
</comment><comment author="markharwood" created="2015-10-23T18:24:39Z" id="150654767">Trying to understand the motivation more. 
Is it possible that these pre-collapsed buckets would have child aggs? Would this bucket-collapsing exercise be expected to fuse their results? 
</comment><comment author="polyfractal" created="2015-10-23T18:39:37Z" id="150658993">Hmm, I think I misunderstood the problem originally.  I didn't realize it needed to be sorted in the context of one field (e.g. an `id`) but checking for gaps in another field.  I think that is going to make it tricky to do as a regular aggregation.  

IIRC, aggregations can only evaluate a single ValueSource currently, so they can only inspect the values of one field.  I'm also not sure how sorting plays into it, since I think the sorting is done _while_ documents are collecting, which is also when aggregations are being collected.  So the agg won't see any sorted values, just a random stream of documents.

If this was for a island/gapping on a single, numeric field (e.g. on the ID itself) it would be less of a problem because ordering isn't important, just tracking numerical gaps.

&gt; Correction: The proposal is to do this as a pipeline aggregation. 

I don't think it makes sense to implement this as a pipeline.  Pipeline aggs only operate on the results of buckets produced by other aggs, which means all the documents have long since been lost to the query.  So you'd be finding islands/gaps in bucketed data, which I don't think makes sense?
</comment><comment author="yaccob" created="2015-10-24T04:49:14Z" id="150754047">I think that the **sorting criteria needs to be evident** to enable sorting the resulting buckets. I'm trying to demonstrate this with a practical use case. 

## Simple Use Case

Below you can see a use case that demonstrates why awareness of the sorting criteria may be important.
In this use case the user wants to see an aggregated view of his transactions per account and the **chronological balance development** across accounts. Since this use case is presenting a chronological view, the time field is the sorting criteria for this use case. 

### Transaction Documents

Each transaction for a particular user creates a document that contains a timestamp, the account ID, the amount spent in this transaction and the overall balance across all this user's accounts after the transaction.

| time | account | amount spent | overall balance after transaction |
| --- | --- | --- | --- |
| 2015-10-24 05:50:03 | A | 10 | 10 |
| 2015-10-24 05:51:18 | A | 5 | 15 |
| 2015-10-24 05:53:25 | B | 3 | 18 |
| 2015-10-24 05:57:11 | A | 17 | 35 |
| 2015-10-24 05:57:28 | A | 2 | 37 |

### Aggregated View

The aggregated view should look like demonstrated in the below table. 
The time column shows max(time) of each bucket, so I suppose it would make sense to tag the buckets with the result of an aggregate function of the sorting criteria. Other use cases may require different aggregate functions for the sorting criteria (min, avg, ...). 

| time | account | number of transactions | amount spent | overall balance after account transactions |
| --- | --- | --- | --- | --- |
| 2015-10-24 05:51:18 | A | 2 | 15 | 15 |
| 2015-10-24 05:53:25 | B | 1 | 3 | 18 |
| 2015-10-24 05:57:28 | A | 2 | 19 | 37 |

## More Complex Use Case

A more complex use case may require considering more than one discrimination criteria.
There is still only one sorting criteria. I'm not aware of a use case where more than one sorting criteria would make sense. 

### Transactions:

| time | account | sub-account | amount spent | overall balance after transaction |
| --- | --- | --- | --- | --- |
| 2015-10-24 05:50:03 | A | A1 | 10 | 10 |
| 2015-10-24 05:51:18 | A | A2 | 5 | 15 |
| 2015-10-24 05:53:25 | B | B1 | 3 | 18 |
| 2015-10-24 05:57:11 | A | A1 | 17 | 35 |
| 2015-10-24 05:57:28 | A | A1 | 2 | 37 |

### Aggregated View:

| time | account | sub-account | number of transactions | amount spent | overall balance after account transactions |
| --- | --- | --- | --- | --- | --- |
| 2015-10-24 05:50:03 | A | A1 | 1 | 10 | 10 |
| 2015-10-24 05:51:18 | A | A2 | 1 | 5 | 15 |
| 2015-10-24 05:53:25 | B | B1 | 1 | 3 | 18 |
| 2015-10-24 05:57:28 | A | A1 | 2 | 19 | 37 |
</comment><comment author="yaccob" created="2015-10-24T22:51:44Z" id="150865177">+1
</comment><comment author="HonzaKral" created="2015-10-25T15:14:01Z" id="150934097">&gt; I don't think it makes sense to implement this as a pipeline. Pipeline aggs only operate on the results of buckets produced by other aggs, which means all the documents have long since been lost to the query. So you'd be finding islands/gaps in bucketed data, which I don't think makes sense?

The idea is to use something like a (date) histogram with another nested aggregation (let's say `terms` with limit of 1 or `avg`) and then essentially grouping date intervals (days, months, minutes, ...) together based on continuous values of the inner aggregations (== days where XYZ is the most popular brand, hours when we are selling opposed to byuing, ....).

The problem is that people might be tempted to use overly fine aggregations (date_histogram over a year with a minute precision with a terms agg inside) to simulate the raw base aggregation. But that is a problem not specific to this use case.
</comment><comment author="jariza" created="2015-10-29T16:51:46Z" id="152246489">+1
</comment><comment author="castorm" created="2015-11-03T08:46:31Z" id="153289262">+1
</comment><comment author="nik9000" created="2015-11-03T14:05:23Z" id="153364967">This is neat!

I see how doing this as a pipeline buys you sorting - and how it "automatically" solves the problem of merging the results from multiple shards. I guess the cost of doing it as a pipeline aggregation is that:
1. Its more confusing. @gingerwizard's use cases are all about documents. Doing it as a pipeline would actually have you detecting gaps in buckets.
2. It pushed lots of work to the coordinating node.

I bet we could make point 2 not so bad by doing stuff like streaming the results from the data nodes to the coordinating node rather than building them all at once (maybe we already do that). We could also push the calculations to the data node if we knew that all the data required was on the data node - say we used routing based on customer id or something.

I wonder about the temptation to use really really small aggregations under the pipeline - if you really do just want the documents maybe we can just do the search and read the data from doc_values somehow? That feels like it'd be much less "heavy". Still lots of data though.

I wonder if there are tricks we can play with statistics that let us push more stuff to the data nodes.
</comment><comment author="bikeholik" created="2015-11-05T14:41:05Z" id="154079056">+1
</comment><comment author="gschjetne" created="2017-05-12T13:23:45Z" id="301075098">This feature would be immensely useful for me, especially if one could specify bucketing criteria as a filter, ie. the aggregation returns buckets alternating between documents that match the filter and documents that do not.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java api: move to ordinary getters and setters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14266</link><project id="" key="" /><description>We've been talking for some time about whether we should start using ordinary getters and setters like any other java project rather than our current methods that don't use the `get` and `set` prefix. We have also been moving to ordinary getters and setters as we go, but for the java api it would be beneficial to move over all of the objects at this point, otherwise we just introduce or keep inconsistencies.
</description><key id="113039400">14266</key><summary>Java api: move to ordinary getters and setters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>adoptme</label><label>breaking</label><label>discuss</label><label>enhancement</label><label>v6.0.0</label></labels><created>2015-10-23T15:09:24Z</created><updated>2017-05-03T06:55:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T20:01:45Z" id="150676035">hell yeah!!!
</comment><comment author="MaineC" created="2016-04-21T12:23:38Z" id="212892236">@javanna what's our preferred strategy here: 
- Simply switch over in v5.0.0 (cheapest, IDE will warn anyway),
- Deprecate in 2.x, and remove in v5.x (means adding a dependency to deprecate everything that should be removed before 2.x is no longer supported),
- Deprecate in 5.x and remove one version later (means carrying the old syntax along for one more release)?

I have forgotten how we've handled similar cases in the past, I'm afraid - I believe it was option one, but am not entirely certain.
</comment><comment author="javanna" created="2016-04-21T12:28:40Z" id="212895628">I think that we can go and remove in 5.0 directly given that our java users have a compiler which will tell them what they have to change. This is the same rationale followed with the search refactoring. Let's document the changes though. Seems overkill to introduce deprecations that will stick around for the whole 5.x series, and it's too late to introduce deprecations in 2.x.
</comment><comment author="MaineC" created="2016-04-21T12:29:23Z" id="212896096">+1 Thanks for the clarification.
</comment><comment author="nik9000" created="2016-04-21T12:29:53Z" id="212896428">Maybe we don't should just do it in on the REST java api and not bother with the native one?
</comment><comment author="javanna" created="2016-04-21T12:39:02Z" id="212900073">I think that we should clean things up regardless of whether the java api will be replaced with something else or not. The request builders are the ones that may go at some point, but they only have proper setters so nothing to do there. I think this is about request objects, which will stay around for internal use anyways, and should be cleaned up at this point. My 2 cents though, we can discuss :)
</comment><comment author="MaineC" created="2016-04-21T13:25:35Z" id="212918532">tl;dr: Both good points, me personally I'm in favour of getting rid of inconsistencies. It makes it easier to add new code that is in line with what we want to see by looking around yourself.

Longer version in addition to the above: While having consistent code is a nice goal, tidying stuff up that will be deleted in a few days or weeks isn't particularly satisfying. Remaining open questions might be:
- Which classes should be included in the cleanup - stuff that gets kicked out soon probably isn't worth looking at. 
- Should we use a "switch as we go" strategy like we did before?
</comment><comment author="javanna" created="2016-04-21T13:36:34Z" id="212922018">&gt; I think this is about request objects

By that I meant classes that extend ActionRequest. Those will stay anyways and are the ones with most inconsistencies I think. 
</comment><comment author="javanna" created="2016-04-21T13:59:13Z" id="212932482">Also classes that extend `ActionResponse` should be targeted I think. Do we want to discuss this before we start, just so we agree we should go ahead? We can mark discuss and discuss it in the next fixit friday if needed.
</comment><comment author="MaineC" created="2016-04-21T14:04:12Z" id="212934700">&gt; Do we want to discuss this before we start, just so we agree we should go ahead?

Yeah - that actually was why instead of pinging people privately I put my initial question about how to proceed to this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add community plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14265</link><project id="" key="" /><description>Add a link to a community plugin which allows a WebSocket client to connect to Elasticsearch and receive a feed of changes being made in the database
</description><key id="113035408">14265</key><summary>Add community plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jurgc11</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-23T14:51:39Z</created><updated>2016-03-10T12:29:35Z</updated><resolved>2016-03-10T12:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-23T18:42:37Z" id="150659717">Hi @jurgc11 

Please could i ask you to sign the CLA http://www.elasticsearch.org/contributor-agreement/ and to send the PR against the master branch of the docs.

thanks
</comment><comment author="clintongormley" created="2016-03-10T12:29:34Z" id="194819695">Hi @jurgc11 

Sorry this dropped off the radar, but I see your plugin is already listed https://www.elastic.co/guide/en/elasticsearch/plugins/current/api.html so I'll close this PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move to lucene BoostQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14264</link><project id="" key="" /><description>Latest version of lucene deprecated Query#setBoost and Query#getBoost which made queries effectively immutable. Those methods need to be replaced with `BoostQuery` that wraps any query that needs boosting.

This commit replaces usages of setBoost with BoostQuery and adds it to forbidden-apis for prod code.

Usages of `getBoost` are only partially removed, as some will have to stay for backwards compatibility.
</description><key id="113017601">14264</key><summary>Move to lucene BoostQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-23T13:20:15Z</created><updated>2015-11-09T14:04:50Z</updated><resolved>2015-11-09T14:04:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-26T09:02:20Z" id="151067187">Thanks for doing this work! I left some comments but things look good to me in general. As highlighted in ones of the comments, I think we should forbid usage of Query.setBoost to prevent it from coming back.
</comment><comment author="javanna" created="2015-10-28T16:22:02Z" id="151899071">@jpountz pushed a new commit, can you have another look please?
</comment><comment author="javanna" created="2015-11-03T17:37:16Z" id="153427913">I rebased and left a few inline comments for @jpountz , I think it's getting closer. Also removed the WIP label, this is not work in progress anymore.
</comment><comment author="javanna" created="2015-11-06T17:30:06Z" id="154479509">@jpountz I addressed your comments, left the last question about manually multiplying boosts vs letting lucene do it and maintain the original wrapped structure. Thanks for your time!
</comment><comment author="jpountz" created="2015-11-09T11:32:50Z" id="155035144">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DuelScrollIT is flaky</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14263</link><project id="" key="" /><description>```
Suite: org.elasticsearch.search.scroll.DuelScrollIT
  2&gt; REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=509C33B10AD7FB5B -Dtests.class=org.elasticsearch.search.scroll.DuelScrollIT -Dtests.method="testDuelIndexOrderQueryThenFetch" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=pt_BR -Dtests.timezone=Atlantic/South_Georgia
FAILURE 1.80s J2 | DuelScrollIT.testDuelIndexOrderQueryThenFetch &lt;&lt;&lt;
   &gt; Throwable #1: org.junit.ComparisonFailure: expected:&lt;[31]&gt; but was:&lt;[72]&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([509C33B10AD7FB5B:58D94811A90F5A55]:0)
   &gt;    at org.elasticsearch.search.scroll.DuelScrollIT.testDuelIndexOrder(DuelScrollIT.java:283)
   &gt;    at org.elasticsearch.search.scroll.DuelScrollIT.testDuelIndexOrderQueryThenFetch(DuelScrollIT.java:309)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/rmuir/workspace/elasticsearch/core/target/J2/temp/org.elasticsearch.search.scroll.DuelScrollIT_509C33B10AD7FB5B-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene53): {nested.field3=PostingsFormat(name=Asserting), _routing=PostingsFormat(name=Asserting), nested.field4=PostingsFormat(name=Asserting), field1=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), foo=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), field2=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting)}, docValues:{nested.field3=DocValuesFormat(name=Asserting), field1=DocValuesFormat(name=Asserting), foo=DocValuesFormat(name=Asserting), _version=DocValuesFormat(name=Asserting)}, sim=RandomSimilarityProvider(queryNorm=false,coord=yes): {}, locale=pt_BR, timezone=Atlantic/South_Georgia
  2&gt; NOTE: Linux 3.19.0-28-generic amd64/Oracle Corporation 1.8.0_45 (64-bit)/cpus=8,threads=1,free=459413528,total=510132224
  2&gt; NOTE: All tests run in this JVM: [ClusterSettingsIT, DuelScrollIT]
Completed [12/287] on J2 in 10.60s, 4 tests, 1 failure &lt;&lt;&lt; FAILURES!
```
</description><key id="113014709">14263</key><summary>DuelScrollIT is flaky</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scroll</label><label>:Search</label><label>adoptme</label><label>jenkins</label><label>test</label></labels><created>2015-10-23T13:03:09Z</created><updated>2017-04-26T20:12:01Z</updated><resolved>2017-04-26T20:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2017-04-26T09:18:44Z" id="297313050">@jimczi this test seems to pass with this seed for me on master now. Any reason not to close this?</comment><comment author="jimczi" created="2017-04-26T20:12:01Z" id="297527252">I tested this seed on master, 5.x, 5.4, 5.3 and 2.4. They all pass so I think it's ok to close this.
Thanks @colings86 </comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when combining field and script in aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14262</link><project id="" key="" /><description>Using a script in the percentile aggregation in Kibana 4, this results in an Elasticsearch NullPointerException. [Example](https://www.dropbox.com/s/op3kzha6qwjlbpe/Screenshot%202015-10-23%2014.08.17.png?dl=0). This seems to be caused by Kibana including both the field and script property. It is not possible to exclude the field property from Kibana.

For the sake of simplicity I narrowed this down to a plain simple ES query. Elasticsearch 1.7.3 on a bare CentOS 7.1 box installed using the yum repo. No changes have been made to the ES config.

Be sure to have one document in an index with a numerical field.

```
$ curl -XDELETE localhost:9200/test

$ curl -XPUT localhost:9200/test/test/1?pretty=true -d '{
    "metric": 100
}'

{
  "_index" : "test",
  "_type" : "test",
  "_id" : "1",
  "_version" : 1,
  "created" : true
}
```

Mapping has become a long value, which is fine for percentile agg

```
$ curl -XGET localhost:9200/test/_mapping?pretty=true

{
  "test" : {
    "mappings" : {
      "test" : {
        "properties" : {
          "metric" : {
            "type" : "long"
          }
        }
      }
    }
  }
}
```

Running a query with both the field and script properties supplied. Results in a NullPointerException. I expected that the script would have precedence.

```
$ curl -XPOST localhost:9200/test/test/_search?pretty=true -d '{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "some-agg": {
      "percentiles": {
        "field": "metric",
        "lang": "expression",
        "script": "1"
      }
    }
  }
}'

{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 4,
    "failed" : 1,
    "failures" : [ {
      "index" : "test",
      "shard" : 2,
      "status" : 500,
      "reason" : "QueryPhaseExecutionException[[test][2]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "
    } ]
  },
  "hits" : {
    "total" : 0,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "some-agg" : {
      "values" : {
        "1.0" : "NaN",
        "5.0" : "NaN",
        "25.0" : "NaN",
        "50.0" : "NaN",
        "75.0" : "NaN",
        "95.0" : "NaN",
        "99.0" : "NaN"
      }
    }
  }
}
```
</description><key id="113007534">14262</key><summary>NullPointerException when combining field and script in aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jappievw</reporter><labels><label>:Expressions</label><label>:Scripting</label><label>adoptme</label><label>bug</label></labels><created>2015-10-23T12:09:42Z</created><updated>2016-03-14T20:22:36Z</updated><resolved>2016-03-14T20:22:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jappievw" created="2015-10-23T14:30:15Z" id="150590407">PS; this is what is included in the Elasticsearch log file:

```
[2015-10-23 16:29:19,142][DEBUG][action.search.type       ] [Pip the Troll] [test][2], node[YWWDb9UsQEGefx8R1f_Urw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@35c01a59] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [test][2]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:301)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:312)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.script.expression.ExpressionScript.setNextVar(ExpressionScript.java:109)
    at org.elasticsearch.search.aggregations.support.ValuesSource$Numeric$WithScript$DoubleValues.setDocument(ValuesSource.java:513)
    at org.elasticsearch.search.aggregations.metrics.percentiles.AbstractPercentilesAggregator.collect(AbstractPercentilesAggregator.java:83)
    at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
    at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:117)
    ... 8 more
```
</comment><comment author="clintongormley" created="2015-10-23T18:40:38Z" id="150659221">Note: this script works correctly with Groovy
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>min_doc_freq missing suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14261</link><project id="" key="" /><description>I have an index(1 shard 0 replica's) with the following 6 documents:

&lt;pre&gt;
&lt;code&gt;
POST /test_suggest/type
{
  "word": "laptop"
}
POST /test_suggest/type
{
  "word": "laptop"
}
POST /test_suggest/type
{
  "word": "laptop"
}
POST /test_suggest/type
{
  "word": "laptup"
}
POST /test_suggest/type
{
  "word": "laptup"
}
POST /test_suggest/type
{
  "word": "laptip"
}
&lt;/code&gt;
&lt;/pre&gt;


When i do the following suggest query:

&lt;pre&gt;
&lt;code&gt;
GET /test_suggest/_suggest
{
  "Suggest": {
    "term": {
      "field": "word",
      "suggest_mode": "missing",
      "size": 5,
      "min_doc_freq": "0",
      "prefix_len": 0,
      "analyzer": "default",
      "sort": "frequency"
    },
    "text": "laptep"
  }
}
&lt;/code&gt;
&lt;/pre&gt;


I get the following suggestions:

&lt;pre&gt;
&lt;code&gt;
{
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "Suggest": [
      {
         "text": "laptep",
         "offset": 0,
         "length": 6,
         "options": [
            {
               "text": "laptop",
               "score": 0.8333333,
               "freq": 3
            },
            {
               "text": "laptup",
               "score": 0.8333333,
               "freq": 2
            },
            {
               "text": "laptip",
               "score": 0.8333333,
               "freq": 1
            }
         ]
      }
   ]
}
&lt;/code&gt;
&lt;/pre&gt;


That's exactly what i expected to get back. When i change the minimum document frequency to 1, it filters out the last document with "laptip".

But when i do the following query:

&lt;pre&gt;
&lt;code&gt;
GET /test_suggest/_suggest
{
  "Suggest": {
    "term": {
      "field": "word",
      "suggest_mode": "missing",
      "size": 5,
      "min_doc_freq": "1",
      "prefix_len": 0,
      "analyzer": "default",
      "sort": "frequency"
    },
    "text": "laptip"
  }
}
&lt;/code&gt;
&lt;/pre&gt;


I do not get any suggestions back. Normally it's logical that there are no suggestions because sugest_mode missing is used. But in this case i added the option min_doc_freq = 1, so i expected the following response:

&lt;pre&gt;
&lt;code&gt;
{
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "Suggest": [
      {
         "text": "laptep",
         "offset": 0,
         "length": 6,
         "options": [
            {
               "text": "laptop",
               "score": 0.8333333,
               "freq": 3
            },
            {
               "text": "laptup",
               "score": 0.8333333,
               "freq": 2
            }
         ]
      }
   ]
}
&lt;/code&gt;
&lt;/pre&gt;


Is this something that can be added to the functionality of the min_doc_freq parameter, or as a new option of the suggester?
</description><key id="112999512">14261</key><summary>min_doc_freq missing suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">Ard-Jan</reporter><labels><label>:Suggesters</label><label>discuss</label></labels><created>2015-10-23T11:07:54Z</created><updated>2015-10-23T18:34:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Delete document type in index with routing in 2.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14260</link><project id="" key="" /><description>How can you delete a document type from a index which uses routing in version 2.0.0 or later. I can delete a single document or the whole index, but not the type, when routing (or parent) is used.

Greetings Damien
</description><key id="112988440">14260</key><summary>Delete document type in index with routing in 2.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbod</reporter><labels /><created>2015-10-23T09:52:10Z</created><updated>2015-10-23T10:24:14Z</updated><resolved>2015-10-23T10:08:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-23T10:08:13Z" id="150534475">Was removed. See https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_mapping_changes.html#_types_may_no_longer_be_deleted

Please ask questions on discuss.elastic.co
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check rebalancing constraints when shards are moved from a node they can no longer remain on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14259</link><project id="" key="" /><description>When a shard can no longer remain on a node (because disk is full or some exclude filters are set in place), it is moved to a different node. Currently, rebalancing constraints are not taken into consideration when this move takes place. An example for this is #14057: `cluster.routing.allocation.cluster_concurrent_rebalance` is set to 1 but 80 shards are moved off the node in one go.

This PR checks rebalancing constraints when shards are moved from a node they can no longer remain on. The constraints that are affected by this are the following:
- cluster.routing.allocation.cluster_concurrent_rebalance
- cluster.routing.allocation.allow_rebalance
- cluster.routing.rebalance.enable
- index.routing.rebalance.enable
- rebalance_only_when_active

Closes #14057
</description><key id="112977505">14259</key><summary>Check rebalancing constraints when shards are moved from a node they can no longer remain on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-23T08:39:11Z</created><updated>2015-10-26T08:27:05Z</updated><resolved>2015-10-26T08:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T09:11:59Z" id="150524114">this looks awesome! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update  facets reference to aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14258</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/1.7//query-dsl.html) we point to facets, we should update it to aggs, I guess [this page](https://www.elastic.co/guide/en/elasticsearch/reference/1.7//search-aggregations.html)?
</description><key id="112942415">14258</key><summary>Update  facets reference to aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2015-10-23T03:38:22Z</created><updated>2015-10-23T18:26:51Z</updated><resolved>2015-10-23T18:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-23T18:26:51Z" id="150655278">Already fixed in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>