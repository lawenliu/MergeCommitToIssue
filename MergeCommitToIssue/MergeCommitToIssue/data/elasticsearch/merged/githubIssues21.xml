<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Simplify nodes stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4347</link><project id="" key="" /><description>First, this breaks backwards compatibility!
- Removed /_cluster/nodes/stats endpoint
- Excpect the stats types not as parameters, but as part of the URL
- Returning all indices stats by default, returning all nodes stats by default
- Supporting groups &amp; types in nodes stats now as well
- Updated documentation &amp; tests accordingly

Closes #4057
</description><key id="23790612">4347</key><summary>Simplify nodes stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T14:40:12Z</created><updated>2014-06-15T06:04:05Z</updated><resolved>2014-01-06T07:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-08T22:05:10Z" id="30094218">LGTM, I would love @clintongormley to try it out as well...
</comment><comment author="clintongormley" created="2013-12-10T11:59:28Z" id="30219798">Heya

This doesn't seem to support the indices-metrics, eg:

```
/_nodes/stats/indices/indexing
```

Also doesn't support the `level` parameter
</comment><comment author="spinscale" created="2013-12-11T10:55:23Z" id="30310857">Fixed the indices metrics.

Regarding supporting the `level` parameter: The aggregation of information happens to early right now, preventing the stats from being returned per shard at all (for the curious: `InternalIndicesService.stats()` loops through the index shards and aggregates there already.

Requires some refactoring around the `NodeIndicesStats` class to support shard level stats and aggregate them later.
</comment><comment author="spinscale" created="2013-12-11T21:09:21Z" id="30364200">@clintongormley added the level parameter for "indices" and "shards", however "cluster" does not make a lot of sense in a response, which is per node...

will squash it into one commit after review
</comment><comment author="clintongormley" created="2013-12-11T21:10:38Z" id="30364295">True... perhaps should be node, indices, shards ?
</comment><comment author="spinscale" created="2013-12-11T21:15:33Z" id="30364693">I like it, wait with reviewing, will put that in first
</comment><comment author="bleskes" created="2013-12-12T08:12:51Z" id="30395914">Hi,

I've had a quick look to see how things will impact the cluster stats end point I work on now. Things look good from that perspective (it will even allow for removing some code in the cluster stats end point).

Two comments:
- the indices stats API return the index level stats both if `level=indices` (default) &amp; `level=shards` , which is different than here.
- Wording wise - `level=indices` implies to me a per-index stats (which we don't expose now at all). I like Clint's suggestion of having `node` (what we do now in 0.90), `indices` (per index) and `shards` (per shard).

Perhaps we want to make level multi-valued. If we do so, we allow people to get both aggregates and detailed level if need be.
</comment><comment author="spinscale" created="2013-12-12T11:02:59Z" id="30406844">@bleskes I will also create the additive behaviour here when the `level` parameter is specified. Wording wise I will go with `node` (default as it is right now), `indices` (per index) and `shards`

thx for the input
</comment><comment author="spinscale" created="2013-12-12T13:20:14Z" id="30419583">Pushed to support `node`, `indices` and `shards` as `level` parameter (documentation still pending) - would be happy about another enduser test
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolate API : is size really mandatory for highlight ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4346</link><project id="" key="" /><description>Documentation found at http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-percolate.html#_percolate_api doesn't tell that that "size" parameter is mantadory when using "highlight" parameter. 

Howhever, when using "highlight" parameter :

```
 curl -XGET 'localhost:9200/my-index/message/_percolate' -d '{
     "highlight":{"fields":{"message":{}}},"doc" : {
         "message" : "A new bonsai tree in the office"
     }
 }'
```

Elasticsearch server (1.0.0.Beta2) throws errors telling that "Can't highlight if size isn't specified" :

```
 {
      "took":3,
      "_shards":{
           "total":5,
           "successful":4,
           "failed":1,
           "failures":[
                {
                     "index":"my-index",
                     "shard":2,
                     "reason":"BroadcastShardOperationFailedException[[my-index][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticSearchIllegalArgumentException[Can't highlight if size isn't specified]; "
                }]
           },
      "total":0
 }
```

Documentation should tell about that. By the way, I can't understand why size should be mandatory for highlight to be performed. **Maybe the documentation is right, and code needs to be changed.**
</description><key id="23788411">4346</key><summary>Percolate API : is size really mandatory for highlight ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pdesoyres</reporter><labels /><created>2013-12-05T14:01:21Z</created><updated>2014-07-09T13:01:54Z</updated><resolved>2014-07-09T13:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-07T21:35:06Z" id="30065436">The idea behind making size required for percolating is performance. If `size` wouldn't be specified, then highlighting would happen on all matches, this can make the percolate request slow if many queries match, so by making the `size` option required with highlighting, the percolate request with highlighting can't be unexpectedly slow. It just depends on the specified `size`. The documentation should have mentioned this... I'll update it. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>word delimiter token filter for numbers: unexpected search results since 0.90.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4345</link><project id="" key="" /><description>As from version 0.90.6 elasticsearch returns unexpected search results for a query with 'AND' operator and analyzer using word delimiter token filter with 'generate_number_parts = true' and 'catenate_numbers = true'.

how to reproduce: https://gist.github.com/abronner/9e3d9806eece2a99ab6c
</description><key id="23777965">4345</key><summary>word delimiter token filter for numbers: unexpected search results since 0.90.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abronner</reporter><labels /><created>2013-12-05T10:19:59Z</created><updated>2014-12-24T17:14:54Z</updated><resolved>2014-12-24T17:14:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:14:54Z" id="68064901">Hi @abronner 

Sorry it has taken so long to look at this issue.  It was this commit which changed the behaviour: https://github.com/elasticsearch/elasticsearch/commits/dcef69b6d49351668cd74f83fb43890ddbf30c52

It changed the query from:

```
"+text:12 +text:01 +text:2013 +text:12012013"
```

to:

```
"+(text:12 text:12012013) +text:01 +text:2013"
```

(the order of emitted tokens has also changed, but the change in the query resulted from the way stacked tokens are treated)

Stacked tokens are considered to be alternatives for each other, as in synonyms.  

i think the only way around this would be to set `generate_number_parts` to `false`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add IO operation stats to the File System statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4344</link><project id="" key="" /><description>The `GET _cluster/nodes/stats/?clear&amp;fs` endpoint currently gives metrics for read and write operations (both number and bytes). It's handy to add another metric which sums writes and reads for a total number of ops.
</description><key id="23777951">4344</key><summary>Add IO operation stats to the File System statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T10:19:37Z</created><updated>2013-12-10T18:14:39Z</updated><resolved>2013-12-05T11:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/fs/FsStats.java</file></files><comments><comment>Added a FsStats.total section that sums up all stats for the different path.dirs. Added `disk_io_size` and `disk_io_op` to the toXContent output, summing up `disk_read_size+disk_write_size` and `disk_writes+disk_reads` respectively.</comment></comments></commit></commits></item><item><title>Add a total section to file system stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4343</link><project id="" key="" /><description>The `GET _cluster/nodes/stats/?clear&amp;fs` endpoint currently gives file system stats for every data path the node is configure for. It's handy to have a `total` section which adds all the per path stats into a global overview.
</description><key id="23777405">4343</key><summary>Add a total section to file system stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T10:08:43Z</created><updated>2013-12-10T18:14:51Z</updated><resolved>2013-12-05T11:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/fs/FsStats.java</file></files><comments><comment>Added a FsStats.total section that sums up all stats for the different path.dirs. Added `disk_io_size` and `disk_io_op` to the toXContent output, summing up `disk_read_size+disk_write_size` and `disk_writes+disk_reads` respectively.</comment></comments></commit></commits></item><item><title>Allow to disable sending a refresh-mapping to master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4342</link><project id="" key="" /><description>When a node processed an index request, which caused it to update its own mapping, then it sends that mapping to the master. While the master process it, that node receives a state that includes an older version of the mapping. Now, there is a conflict, its not bad (i.e. the cluster state will _eventually_ have the correct mapping), but we send for a refresh just in case form that node to the master.

With a system that has extreme cases of updates and frequent mapping changes, it might make sense to disable this feature. The `indices.cluster.send_refresh_mapping` setting can be introduced to support that (note, this setting need to be set on the data nodes)

Note, sending refresh mapping is more important when the reverse happens, and for some reason, the mapping in the master is ahead, or in conflict, with the actual parsing of it in the actual node the index exists on. In this case, the refresh mapping will result in warning being logged on the master node.
</description><key id="23757432">4342</key><summary>Allow to disable sending a refresh-mapping to master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T00:13:56Z</created><updated>2013-12-10T18:14:58Z</updated><resolved>2013-12-05T00:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Allow to disable sending a refresh-mapping to master node</comment><comment>When a node processed an index request, which caused it to update its own mapping, then it sends that mapping to the master. While the master process it, that node receives a state that includes an older version of the mapping. Now, there is a conflict, its not bad (i.e. the cluster state will eventually have the correct mapping), but we send for a refresh just in case form that node to the master.</comment></comments></commit></commits></item><item><title>has_child query can yield in wrong results if parent type has nested inner objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4341</link><project id="" key="" /><description>If a parent type has nested inner objects then the `has_child` query (with score_mode) can return wrong results. This is caused by an internal short circuiting mechanism and the fact that hidden nested Lucene document have the same uid as the main document. The internal short circuiting mechanism sees the hidden nested Lucene document as a hit and starts the countdown for short circuiting from the wrong hits. This results in too less hits being returned.

Relates to #4210 and is the reason why with ES version 1.0 beta2 too less results are being returned.
</description><key id="23755031">4341</key><summary>has_child query can yield in wrong results if parent type has nested inner objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>enhancement</label><label>regression</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-04T23:27:13Z</created><updated>2013-12-17T09:24:34Z</updated><resolved>2013-12-04T23:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdsFilter.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Fixed incorrect results with `has_child` query with score mode if the parent type has nested object types. The inner objects (Separate Lucene docs) are also emitted as hits, which incorrectly decreased the count down short circuit mechanism in the `has_child` query.</comment></comments></commit></commits></item><item><title>Remove hard-coded "ok": true from REST responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4340</link><project id="" key="" /><description>Fixes #4310.

I chose to remove only the `{"ok": true}` messages where the `true` was hard-coded, there are still 3 places where the `ok` field could be different: RestDeleteByQueryAction, RestExplainAction, and RestClearScrollAction. In these the actual success of the action is returned.

I'm happy to remove those if we decide we don't want them.
</description><key id="23740138">4340</key><summary>Remove hard-coded "ok": true from REST responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-04T19:35:24Z</created><updated>2014-06-14T03:39:26Z</updated><resolved>2014-01-07T16:26:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-08T21:46:52Z" id="30093793">looks good, I would also remove the ok from the other 3 rest actions.
</comment><comment author="dakrone" created="2013-12-09T16:42:01Z" id="30148026">@kimchy I've removed the other 3 `ok`s.
</comment><comment author="dakrone" created="2013-12-26T18:38:34Z" id="31230673">This requires either https://github.com/elasticsearch/elasticsearch-rest-api-spec/pull/2 or https://github.com/elasticsearch/elasticsearch/pull/4541 to be merged in order to avoid test failures.
</comment><comment author="dakrone" created="2014-01-03T22:56:25Z" id="31560949">Updated this after the REST spec has been pulled into the main repo, please take another look.
</comment><comment author="s1monw" created="2014-01-07T16:17:13Z" id="31751503">LGTM +1 to push
</comment><comment author="dakrone" created="2014-01-07T16:26:29Z" id="31752464">Merged in d23f640c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support 'yaml' as a format for the Analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4339</link><project id="" key="" /><description>Fixes #4311.

Copies the format of the `detailed` format, because putting the `text` format into yaml doesn't make much sense to me.
</description><key id="23736672">4339</key><summary>Support 'yaml' as a format for the Analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels /><created>2013-12-04T18:46:03Z</created><updated>2014-12-12T16:27:46Z</updated><resolved>2013-12-08T22:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-04T20:22:39Z" id="29841644">I would simply remove the format to be honest, it collides with our regular `format` parameter across all the other REST APIs. Just have it output the "detailed" version always, and thats it?
</comment><comment author="dakrone" created="2013-12-04T21:24:12Z" id="29846974">@kimchy I agree (I think the `text` format is harder to read than the detailed format). Removing it, however, would be breaking, so okay if I do that and merge this only to the master branch?
</comment><comment author="kimchy" created="2013-12-04T21:27:10Z" id="29847221">ok to do it on master. I don't think anybody uses the text format in the analyze API, so potentially its ok to change it also in 0.90, but the safest is master.
</comment><comment author="kimchy" created="2013-12-08T21:47:17Z" id="30093806">LGTM
</comment><comment author="dakrone" created="2013-12-08T22:20:21Z" id="30094604">Merged in bc9698a.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update the Wonderdog repo URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4338</link><project id="" key="" /><description>I discovered that the Wonderdog repo was relocated; I didn't try the rest of the repo links, so I can't attest to their accuracy.
</description><key id="23735369">4338</key><summary>Update the Wonderdog repo URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mdaniel</reporter><labels /><created>2013-12-04T18:25:55Z</created><updated>2014-07-16T21:50:53Z</updated><resolved>2013-12-06T19:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-12-06T19:06:52Z" id="30020134">Fixed in a1d4731
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix TRACE logs being hidden when DEBUG logging is also enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4337</link><project id="" key="" /><description>Fixes #4323.

I chose to log additionally in two places, and switch the precedence of logging in one other.
</description><key id="23734713">4337</key><summary>Fix TRACE logs being hidden when DEBUG logging is also enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-12-04T18:15:56Z</created><updated>2014-07-02T13:28:06Z</updated><resolved>2014-02-13T20:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-05T07:49:28Z" id="29877632">might be reading this wrong, but wont two of these three changes still not output two log lines if the log level is set to TRACE, containing redundant information?
</comment><comment author="dadoonet" created="2013-12-05T07:58:39Z" id="29878019">@spinscale I agree. Or I'm misreading it as well. :-)
</comment><comment author="dakrone" created="2013-12-05T15:46:02Z" id="29908553">@spinscale @dadoonet:
So, for IndexShardGatewayService I chose to log both the DEBUG message and TRACE message if TRACE is enabled, since I would consider both formats to be helpful (the debug one is better for grepping, the trace one better for reading).

For MapperService I chose to check `isTraceEnabled()` first and only print _either_ the trace message or the debug message, because they're basically the same message, just with more details in the trace version.

For RecoveryTarget I again chose to log both messages (when trace is enabled) because I thought both would be beneficial.

Does that clear up your question?
</comment><comment author="javanna" created="2014-02-13T19:47:49Z" id="35017852">ops ... @dakrone I just realized I merged a similar PR a few days ago, sorry about that! Can you have a look if that's ok and eventually close this one?
</comment><comment author="dakrone" created="2014-02-13T20:04:52Z" id="35019680">Closing this as it's been fixed already, thanks @javanna 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify usage of nodes info API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4336</link><project id="" key="" /><description>Important: This breaks backwards compatibility with 0.90
- Removed endpoints: /_cluster/nodes, /_cluster/nodes/nodeId1,nodeId2
- Disallow usage of parameters, but make them required part of URI instead
- Changed NodesInfoRequest to return everything by default
- Fixed NPE in NodesInfoResponse

Closes #4055

**Note**: Before this should is going to be pushed, #4057 must be changed as well, because some URLs overlap and are not returned correctly
</description><key id="23721267">4336</key><summary>Simplify usage of nodes info API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-04T15:11:40Z</created><updated>2014-06-14T17:50:24Z</updated><resolved>2014-01-08T09:03:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-08T22:05:04Z" id="30094216">LGTM, I would love @clintongormley to try it out as well...
</comment><comment author="clintongormley" created="2013-12-10T11:58:42Z" id="30219764">Heya

The `{nodes}` part of the URL should accept nodes, `_all` or blank.  Currently it doesn't accept a blank if I want to specify a metric, eg:

```
`/_nodes/fs`
```

I realise that `fs` could potentially clash with a node name, but unlikely to do so and we have `_all` for that case
</comment><comment author="spinscale" created="2014-01-06T08:38:50Z" id="31633983">Didnt push this yet, because I needed to change the rest api spec files - and make them version dependent (1.0 has different URLs than 0.90) - not sure if there is a decision how to handle this yet.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fail geohash_cell filter if geohash prefix is not enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4335</link><project id="" key="" /><description>While we are at it, cleanup also the Java code to conform to the correct naming and conventions we use.
</description><key id="23711627">4335</key><summary>Fail geohash_cell filter if geohash prefix is not enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-04T12:16:29Z</created><updated>2013-12-10T18:15:53Z</updated><resolved>2013-12-04T12:17:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java</file></files><comments><comment>Fail geohash_cell filter if geohash prefix is not enabled</comment><comment>closes #4335</comment></comments></commit></commits></item><item><title>Node deadlock on shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4334</link><project id="" key="" /><description>ES Version: 0.90.7, Java version: 1.7 update 45 64 bit Server VM.

I have a 7 node cluster with 5 master nodes and 2 client nodes. 
When I was shutting down all nodes to do a full cluster restart, one node did not die and looks there is a deadlock.

Stack Trace:

2013-12-03 22:07:50
Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.45-b08 mixed mode):

"Attach Listener" daemon prio=10 tid=0x00007f8ed4028000 nid=0x5d32 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"Thread-1" prio=10 tid=0x00007f8e88698000 nid=0x5c6e waiting on condition [0x00007f8e7e861000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1461)
    at org.elasticsearch.threadpool.ThreadPool.awaitTermination(ThreadPool.java:249)
    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:342)
    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:73)

"SIGTERM handler" daemon prio=10 tid=0x00007f8ed4042000 nid=0x5c6b in Object.wait() [0x00007f8ee7915000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on &lt;0x00000005fd3d76c8&gt; (a org.elasticsearch.bootstrap.Bootstrap$1)
    at java.lang.Thread.join(Thread.java:1280)
    - locked &lt;0x00000005fd3d76c8&gt; (a org.elasticsearch.bootstrap.Bootstrap$1)
    at java.lang.Thread.join(Thread.java:1354)
    at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
    at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
    at java.lang.Shutdown.runHooks(Shutdown.java:123)
    at java.lang.Shutdown.sequence(Shutdown.java:167)
    at java.lang.Shutdown.exit(Shutdown.java:212)
    - locked &lt;0x00000005fd340058&gt; (a java.lang.Class for java.lang.Shutdown)
    at java.lang.Terminator$1.handle(Terminator.java:52)
    at sun.misc.Signal$1.run(Signal.java:212)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#7]" daemon prio=10 tid=0x00007f8e84112800 nid=0x799a waiting on condition [0x00007f8ee7c62000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#12]" daemon prio=10 tid=0x00007f8e8c11f800 nid=0x7999 waiting on condition [0x00007f8ee7ca3000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#3]" daemon prio=10 tid=0x00007f8e8011e000 nid=0x7997 waiting on condition [0x00007f8ee7d25000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#2]" daemon prio=10 tid=0x00007f8e8c11d800 nid=0x7996 waiting on condition [0x00007f8ee7d66000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#4]" daemon prio=10 tid=0x00007f8e84111000 nid=0x7995 waiting on condition [0x00007f8ee7da7000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#8]" daemon prio=10 tid=0x0000000001fa0800 nid=0x7991 waiting for monitor entry [0x00007f8ee7f7c000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.terminated(EsThreadPoolExecutor.java:64)
    - waiting to lock &lt;0x00000005fdbaaf50&gt; (a java.lang.Object)
    - locked &lt;0x00000005fae03ef0&gt; (a org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:704)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1006)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#6]" daemon prio=10 tid=0x0000000001f9f000 nid=0x7990 waiting on condition [0x00007f8ee7fbd000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"elasticsearch[AG 8][search][T#1]" daemon prio=10 tid=0x00007f8e8410f000 nid=0x798f waiting on condition [0x00007f8ee7ffe000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"DestroyJavaVM" prio=10 tid=0x00007f8f1000a800 nid=0x775c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"elasticsearch[AG 8][clusterService#updateTask][T#1]" daemon prio=10 tid=0x00007f8e84107800 nid=0x7798 waiting on condition [0x00007f8eee056000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.interruptIdleWorkers(ThreadPoolExecutor.java:781)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:695)
    at java.util.concurrent.ThreadPoolExecutor.shutdown(ThreadPoolExecutor.java:1397)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(EsThreadPoolExecutor.java:56)
    - locked &lt;0x00000005fdbaaf50&gt; (a java.lang.Object)
    at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:395)
    at org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:656)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:417)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

"Service Thread" daemon prio=10 tid=0x00007f8f10113800 nid=0x7769 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread1" daemon prio=10 tid=0x00007f8f10111000 nid=0x7768 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread0" daemon prio=10 tid=0x00007f8f1010e800 nid=0x7767 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"Signal Dispatcher" daemon prio=10 tid=0x00007f8f1010c800 nid=0x7766 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"Surrogate Locker Thread (Concurrent GC)" daemon prio=10 tid=0x00007f8f10102000 nid=0x7765 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

"Finalizer" daemon prio=10 tid=0x00007f8f100eb800 nid=0x7764 in Object.wait() [0x00007f8f0c1bd000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on &lt;0x00000005fce11a08&gt; (a java.lang.ref.ReferenceQueue$Lock)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
    - locked &lt;0x00000005fce11a08&gt; (a java.lang.ref.ReferenceQueue$Lock)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
    at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)

"Reference Handler" daemon prio=10 tid=0x00007f8f100e7800 nid=0x7763 in Object.wait() [0x00007f8f0c1fe000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on &lt;0x00000005fce13cb0&gt; (a java.lang.ref.Reference$Lock)
    at java.lang.Object.wait(Object.java:503)
    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
    - locked &lt;0x00000005fce13cb0&gt; (a java.lang.ref.Reference$Lock)

"VM Thread" prio=10 tid=0x00007f8f100e5000 nid=0x7762 runnable 

"Gang worker#0 (Parallel GC Threads)" prio=10 tid=0x00007f8f1001c000 nid=0x775d runnable 

"Gang worker#1 (Parallel GC Threads)" prio=10 tid=0x00007f8f1001e000 nid=0x775e runnable 

"Gang worker#2 (Parallel GC Threads)" prio=10 tid=0x00007f8f1001f800 nid=0x775f runnable 

"Gang worker#3 (Parallel GC Threads)" prio=10 tid=0x00007f8f10021800 nid=0x7760 runnable 

"Concurrent Mark-Sweep GC Thread" prio=10 tid=0x00007f8f100a2000 nid=0x7761 runnable 
"VM Periodic Task Thread" prio=10 tid=0x00007f8f1011e800 nid=0x776a waiting on condition 

JNI global references: 284
# Found one Java-level deadlock:

"Thread-1":
  waiting for ownable synchronizer 0x00000005fdba9278, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by "elasticsearch[AG 8][search][T#8]"
"elasticsearch[AG 8][search][T#8]":
  waiting to lock monitor 0x00007f8e980a33a8 (object 0x00000005fdbaaf50, a java.lang.Object),
  which is held by "elasticsearch[AG 8][clusterService#updateTask][T#1]"
"elasticsearch[AG 8][clusterService#updateTask][T#1]":
  waiting for ownable synchronizer 0x00000005fdba9278, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by "elasticsearch[AG 8][search][T#8]"
# Java stack information for the threads listed above:

"Thread-1":
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1461)
    at org.elasticsearch.threadpool.ThreadPool.awaitTermination(ThreadPool.java:249)
    at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:342)
    at org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:73)
"elasticsearch[AG 8][search][T#8]":
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.terminated(EsThreadPoolExecutor.java:64)
    - waiting to lock &lt;0x00000005fdbaaf50&gt; (a java.lang.Object)
    - locked &lt;0x00000005fae03ef0&gt; (a org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:704)
    at java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1006)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
"elasticsearch[AG 8][clusterService#updateTask][T#1]":
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000005fdba9278&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.ThreadPoolExecutor.interruptIdleWorkers(ThreadPoolExecutor.java:781)
    at java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:695)
    at java.util.concurrent.ThreadPoolExecutor.shutdown(ThreadPoolExecutor.java:1397)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(EsThreadPoolExecutor.java:56)
    - locked &lt;0x00000005fdbaaf50&gt; (a java.lang.Object)
    at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:395)
    at org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:656)
    at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:417)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)

Found 1 deadlock.
</description><key id="23691904">4334</key><summary>Node deadlock on shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">aganapat</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-04T03:16:55Z</created><updated>2013-12-23T02:06:16Z</updated><resolved>2013-12-15T03:13:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-12-12T21:40:56Z" id="30464725">@aganapat do you remember if you tried to update thread pool settings before shutting nodes down. Did you update any settings at all before shutdown? 
</comment><comment author="aganapat" created="2013-12-12T23:57:41Z" id="30474632">I am not exactly sure but yes I was playing with update settings for threadpool.
</comment><comment author="aganapat" created="2013-12-19T21:20:06Z" id="30967512">Thanks for fixing this. I have a related question, I observed faster bulk insert times when I increased the threadpool size to 50 for bulk. The thread pool is of type fixed. What is your recommendation on this ? 
</comment><comment author="imotov" created="2013-12-23T02:06:16Z" id="31101509">It's hard to recommend something here without knowing details of your index and hardware. Moreover, we are tying to use github issues for feature requests and bug reporting. Could you ask your question on the mailing list?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java</file></files><comments><comment>Resolve potential deadlock state during EsThreadPoolExecutor shutdown</comment></comments></commit></commits></item><item><title>Shard replica recovering from remote shard although data is identical (incomplete checksum ?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4333</link><project id="" key="" /><description>It looks like a bug to me, please tell me if I misunderstand something

After having a look in ES source code and user mailing list post (https://groups.google.com/forum/?fromgroups#!searchin/elasticsearch/checksum/elasticsearch/9uF-a5vqfkQ/19jIIMpho8UJ) it seems recovery of a shard replica copies data from a remote primary shard only if local data is different from the remote one.

I experience here a strange behavior, as replica shard recovery copies data from remote primary shard even when files on the disk are identical.
I had a look at trace level logs, and it seems some files does not have any checksum associated with them

&lt;pre&gt;
2013-12-04 10:52:40.995 [elasticsearch[index3][generic][T#6]] TRACE org.elasticsearch.indices.recovery - [index3] [logst_20130902][3] recovery [phase1] to [index][8zAoqm0QTJy6ZZxr6hxcKg][inet[/172.28.2.124:9300]]{server=index, local=false}: recovering [_4m7.fdt], exists in local store, but is different: remote [name [_4m7.fdt], length [5055878], checksum [null]], local [name [_4m7.fdt], length [5055878], checksum [null]]&lt;/pre&gt;

I then had a look at the _checksums-###### file in the index and it seems that some files are actually not registered inside.

list of files:
&lt;pre&gt;
PS E:\...\nodes\0\indices\logst_20130902\3\index&gt; ls


    ディレクトリ: E:\...\nodes\0\indices\logst_20130902\3\index


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
-a---        2013/12/03     18:06        149 segments_2o
-a---        2013/12/04     11:21          0 write.lock
-a---        2013/12/03     18:06   24851477 _3wf.fdt
-a---        2013/12/03     18:06       8588 _3wf.fdx
-a---        2013/12/03     18:06       1523 _3wf.fnm
-a---        2013/12/03     18:06        416 _3wf.si
-a---        2013/11/21     19:10         40 _3wf_1.del
-a---        2013/12/03     18:06     228022 _3wf_es090_0.blm
-a---        2013/12/03     18:06    6242130 _3wf_es090_0.doc
-a---        2013/12/03     18:06         34 _3wf_es090_0.pay
-a---        2013/12/03     18:06    1902714 _3wf_es090_0.pos
-a---        2013/12/03     18:06   52196457 _3wf_es090_0.tim
-a---        2013/12/03     18:06     383286 _3wf_es090_0.tip
-a---        2013/12/03     18:06    5055878 _4m7.fdt
-a---        2013/12/03     18:06       1620 _4m7.fdx
-a---        2013/12/03     18:06       1523 _4m7.fnm
-a---        2013/12/03     18:06        416 _4m7.si
-a---        2013/11/21     19:10         41 _4m7_1.del
-a---        2013/12/03     18:06      46454 _4m7_es090_0.blm
-a---        2013/12/03     18:06     993440 _4m7_es090_0.doc
-a---        2013/12/03     18:06         34 _4m7_es090_0.pay
-a---        2013/12/03     18:06     387284 _4m7_es090_0.pos
-a---        2013/12/03     18:06   11199156 _4m7_es090_0.tim
-a---        2013/12/03     18:06      95699 _4m7_es090_0.tip
-a---        2013/12/03     17:42    3644643 _5y3.fdt
-a---        2013/12/03     17:42       1360 _5y3.fdx
-a---        2013/12/03     17:42       1523 _5y3.fnm
-a---        2013/12/03     17:42        414 _5y3.si
-a---        2013/12/03     17:42      33334 _5y3_es090_0.blm
-a---        2013/12/03     17:42     696249 _5y3_es090_0.doc
-a---        2013/12/03     17:42         34 _5y3_es090_0.pay
-a---        2013/12/03     17:42     277744 _5y3_es090_0.pos
-a---        2013/12/03     17:42    8093228 _5y3_es090_0.tim
-a---        2013/12/03     17:42      70340 _5y3_es090_0.tip
-a---        2013/12/03     18:06        256 _checksums-1386061599167
&lt;/pre&gt;


content of _checksums-1386061599167

&lt;pre&gt;
              _5y3.fnm jp3l_5y3_es090_0.tim19lvp16_4m7_1.del 16ngfcf_5y3_es090_0.blm 1z3shz_5y3_es090_0.pay 1oc8igi_5y3_es090_0.doc ug5ct1_5y3.fdx 148mb8r_5y3.si o5uult_5y3_es090_0.tip 1rjuwi0_5y3_es090_0.pos ccqsms
_3wf_1.del 14p3tnl_5y3.fdt 1ttm03z
&lt;/pre&gt;


I am using ES 0.90.1. I had a look at recent release notes but could not find anything related.

Note: for some reason when there is only one segment/shard, checksum file seems to be complete
</description><key id="23690649">4333</key><summary>Shard replica recovering from remote shard although data is identical (incomplete checksum ?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martin-a</reporter><labels /><created>2013-12-04T02:39:19Z</created><updated>2014-12-24T16:47:44Z</updated><resolved>2014-12-24T16:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T16:47:44Z" id="68063240">Hi @martin-a 

Sorry it has taken so long to look at this ticket.  All of the code involved here has changed so much since 0.90.1 that I don't think it is still worth investigating this issue.  Obviously, if you see anything like this on newer versions, please open another issue.  thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updates to Docs for Fuzzy matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4332</link><project id="" key="" /><description>Rework of the Fuzzy query docs to fix some errors, make certain aspects more clear, and promote the use of integer, not float, fuzziness values.
</description><key id="23690056">4332</key><summary>Updates to Docs for Fuzzy matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewvc</reporter><labels /><created>2013-12-04T02:21:06Z</created><updated>2014-06-17T12:03:25Z</updated><resolved>2014-05-06T12:07:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrewvc" created="2013-12-11T23:36:46Z" id="30376065">Curious if I submitted this correctly, or if there is something lacking in this PR since I haven't heard anything on it. I know you guys are busy, but feedback would be appreciated!
</comment><comment author="andrewvc" created="2013-12-12T18:52:21Z" id="30450153">@nik9000 Updated the pull request to take into account your various suggestions. Thanks for the constructive comments!
</comment><comment author="clintongormley" created="2013-12-13T09:12:51Z" id="30495630">Hi @andrewvc 

Thanks for the patch to the docs. I'm hesitating this because I'm not sure that the description of the fuzzy query is still accurate - the way it has works has changed dramatically in Lucene 4.

I need some time to investigate. Will come back to you

ta

clint
</comment><comment author="andrewvc" created="2013-12-13T15:33:45Z" id="30517682">@clintongormley thanks for looking into it. I'm sure you guys are quite busy with the 1.0 release! 

I do think, however, that these docs should be accurate for Lucene 4. I'm quite familiar with the new changes to the FuzzyQuery class. I've also read through the relevant ES source code, and tested a few things related to it as well. These docs accurately reflect that floating-point fuzziness values are now deprecated and that integer values of either 1 or 2  are to be expected. I have also verified that max_expansions/prefix works as I described. 

Let me know if there's anything else I can do to here!
</comment><comment author="andrewvc" created="2013-12-13T15:36:06Z" id="30517865">Now that I think about it, it might be nice to add a link to http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html . Though I'm not sure if that's too much detail for the ES docs.
</comment><comment author="andrewvc" created="2013-12-18T15:10:18Z" id="30848801">@clintongormley I've also reworked some of the 'match' query docs to better describe fuzzy queries.
</comment><comment author="clintongormley" created="2013-12-18T15:11:11Z" id="30848877">Sorry for the delay @andrewvc - this is still on my list :)
</comment><comment author="andrewvc" created="2014-01-04T19:50:28Z" id="31586927">@clintongormley any interest in merging in this patch?
</comment><comment author="clintongormley" created="2014-05-06T12:07:51Z" id="42294435">Hi @andrewvc 

Apologies for the late response on this PR.  I'd completely forgotten about it.  In the meantime, we've renamed the `min_similarity` parameter to `fuzziness` across all fuzzy APIs, and improved the docs.  I've read through the changes you made above and I think that all of the relevant points have been incorporated in the new docs, so I'm going to close this.

Please feel free to send a new PR if you think we've forgotten anything. Thanks, and sorry for the delay
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix implementation of currentValueHash in FieldDataSource.Bytes.SortedAndUnique</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4331</link><project id="" key="" /><description>Close #4330
</description><key id="23682724">4331</key><summary>Fix implementation of currentValueHash in FieldDataSource.Bytes.SortedAndUnique</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-03T23:25:04Z</created><updated>2014-07-11T08:20:04Z</updated><resolved>2013-12-03T23:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>FieldDataSource.Bytes.SortedAndUnique returns wrong hashes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4330</link><project id="" key="" /><description>The implementation of `currentValueHash` in FieldDataSource.Bytes.SortedAndUnique is not correct and returns hash codes that are always equal to 0, which makes BytesRefHash lookup perform in linear time instead of constant time.
</description><key id="23682603">4330</key><summary>FieldDataSource.Bytes.SortedAndUnique returns wrong hashes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-03T23:22:50Z</created><updated>2013-12-10T18:16:08Z</updated><resolved>2013-12-03T23:43:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java</file></files><comments><comment>Fix implementation of currentValueHash in FieldDataSource.Bytes.SortedAndUnique.</comment></comments></commit></commits></item><item><title>API to list running requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4329</link><project id="" key="" /><description>I searched for but couldn't find a request for an api to list running queries an updates so I'm going to put together my wish list.
1.  Only really need queries, indexes, updates, bulk updates, and multi search.
2.  Suggest requests would be nice as well but are not required.
3.  Should be able to get full request body.
4.  The request should have free form fields that I can add to the request so I can recognize requests more easily.  I'd put things in there like the ip that caused me to issue the request and the my own identifier for the request type.
5.  The requests should have some unique identifier on them that lasts for the life of the request and won't be repeated soon.
6.  I don't need to search this stuff though it might help.
7.  Needs to return all queries running on the cluster.
8.  I won't issue the command frequently but when I do I need it to be serviced even if the cluster is under heavy load.  In fact, that is when I'll need it the most.
9.  This API is useful even without the ability to kill requests but, one day, killing would be nice.
</description><key id="23663707">4329</key><summary>API to list running requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Cluster</label></labels><created>2013-12-03T18:39:42Z</created><updated>2015-07-10T16:04:06Z</updated><resolved>2015-07-10T16:04:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ralgara" created="2015-05-29T04:31:57Z" id="106681964">I second @nik9000 in his wish list for APIs to list and kill queries. Much needed. The referenced https://github.com/elastic/kibana/issues/1173 and this #4329 reference each other and both are closed. Any light at the end of the tunnel?
</comment><comment author="clintongormley" created="2015-06-26T11:04:55Z" id="115648437">Today the search context is stored in a thread local. It would be feasible to add an API that would iterate through all of these values (representing currently running searches) and return them, but it would mean changing the data structure to allow locking.  We'd also need a way of capturing other types of requests (like bulk etc).

The information would have to be per-node to be of any use.

It might be worth investigating, although I'm leaning towards directing effort towards making changes that prevent abusive requests, things like switching from fielddata to doc values (as we've done in master), improvements to the circuit breakers, and safeguarding users from making silly mistakes (eg #11511).
</comment><comment author="nik9000" created="2015-07-09T13:16:45Z" id="119960185">&gt; It might be worth investigating, although I'm leaning towards directing effort towards making changes that prevent abusive requests, things like switching from fielddata to doc values (as we've done in master), improvements to the circuit breakers, and safeguarding users from making silly mistakes (eg #11511).

This is probably for the best. I wonder if you could do current request listing without something fancy - just keeping a list per node. But meh, just not being able to make bad queries is more likely I think.

The lack of a kill feature is still a big whole when you compare it to relational databases - they always have the feature and I've always used it. Its a bit wish list-y though. PostgreSQL's kill works because each connection has process isolation. It makes locking harder but killing easier. With Elasticsearch you'd have to do something like add `Thread.interrupted` all over the place or something. Including to Lucene.
</comment><comment author="clintongormley" created="2015-07-10T16:04:06Z" id="120445326">Closing in favour of #12187
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.90.7: Get API returns string value for field of type "byte" when using ?fields=</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4328</link><project id="" key="" /><description>Here's a gist reproducing the problem: https://gist.github.com/tikitu/7773227

The value for `byte_field` (which the mapping defines as `"type": "byte"`) is returned as a numerical value when using the plain get API, but as a string when using `?fields=`. The integer and long fields, on the other hand, are unaffected.

I would sort of suspect it's related to #3490 -- hope it's possible to fix this one directly on 0.90 though.
</description><key id="23658634">4328</key><summary>0.90.7: Get API returns string value for field of type "byte" when using ?fields=</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tikitu</reporter><labels /><created>2013-12-03T17:23:19Z</created><updated>2014-12-24T16:42:20Z</updated><resolved>2014-12-24T16:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaddison" created="2014-06-04T17:22:43Z" id="45122270">I am encountering this issue with 0.90.13 as well. I am planning to upgrade to at least 1.1.x in the near future, so I presume #3490 addresses this upon my switch.

Edit: sounds like this should be closed as "won't fix", after reading comments in #3490 more closely.
</comment><comment author="clintongormley" created="2014-12-24T16:42:20Z" id="68062951">This appears to be fixed in master.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elasticsearch std_deviation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4327</link><project id="" key="" /><description>Hi,

I opened a ticket for kibana a while back asking about being able to graph std deviation.  

https://github.com/elasticsearch/kibana/issues/541 

I was wondering if this could be a possibility in a future release or if it just doesn't fit on the road map.

Thanks for your insight.

Geoff
</description><key id="23654246">4327</key><summary>elasticsearch std_deviation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oweng</reporter><labels /><created>2013-12-03T16:25:26Z</created><updated>2014-12-24T16:40:09Z</updated><resolved>2014-12-24T16:40:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T16:40:08Z" id="68062818">The `extended_stats` aggregation would now support this.  Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix _all boosting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4326</link><project id="" key="" /><description>_all boosting used to rely on the fact that the TokenStream doesn't eagerly
consume the input java.io.Reader. This fixes the issue by using binary search
in order to find the right boost given a token's start offset.

Close #4315
</description><key id="23652822">4326</key><summary>Fix _all boosting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-03T16:07:28Z</created><updated>2014-07-16T21:50:55Z</updated><resolved>2013-12-05T22:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolate API request parsing issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4325</link><project id="" key="" /><description>I experiment strange behaviour using percolate API using fresh 1.0.0-Beta2 elasticsearch :

First, I index a new percolate query :

```
curl -XPUT 'localhost:9200/my-index/.percolator/1' -d '{
    "query" : {
        "match" : {
            "message" : "bonsai tree"
        }
    }
}'
```

Then when I use Percolate API :

```
 curl -XGET 'localhost:9200/my-index/message/_percolate' -d '{
     "doc" : {
         "message" : "A new bonsai tree in the office"
     },
     "score":true
 }'
```

**note the usage of `score:true`**

I get a result without score :

```
{
    "took": 5,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "total": 1,
    "matches": [
        {
            "_index": "my-index",
            "_id": "1"
        }
    ]
}
```

But if I change the location of the parameter `score` in the query : 

```
 curl -XGET 'localhost:9200/my-index/message/_percolate' -d '{
     "score":true,
     "doc" : {
         "message" : "A new bonsai tree in the office"
     }
 }'
```

Then, I get the expected result :

```
{
    "took": 39,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "total": 1,
    "matches": [
        {
            "_index": "my-index",
            "_id": "1",
            "_score": 1
        }
    ]
}
```

I experiment the same behaviour with the `highlight` and `size` parameters.
</description><key id="23650043">4325</key><summary>Percolate API request parsing issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pdesoyres</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-03T15:32:41Z</created><updated>2013-12-04T09:00:14Z</updated><resolved>2013-12-04T00:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-04T00:12:28Z" id="29765571">@pdesoyres Thanks for reporting this bug, I've pushed a fix for this, which will be included in the next 1.0 release.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file></files><comments><comment>Fixed parsing bug in percolator, where everything after the `doc` object was skipped.</comment></comments></commit></commits></item><item><title>Fixes #4323 - Debug logging can hide trace logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4324</link><project id="" key="" /><description>I swapped the order of the `isTraceEnabled` and `isDebugEnabled` checks to ensure that the trace log message is used if trace is enabled.
</description><key id="23632482">4324</key><summary>Fixes #4323 - Debug logging can hide trace logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tstibbs</reporter><labels /><created>2013-12-03T10:11:11Z</created><updated>2014-06-18T14:58:22Z</updated><resolved>2014-02-07T10:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-06T17:29:37Z" id="34348012">Thanks for the PR @tstibbs, would you mind signing the [elasticsearch CLA](http://www.elasticsearch.org/contributor-agreement/) so that we can merge it in?
</comment><comment author="tstibbs" created="2014-02-07T08:25:43Z" id="34416810">@javanna oops, sorry, agreement now signed.
</comment><comment author="javanna" created="2014-02-07T10:21:49Z" id="34424137">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>In some places trace logging is hidden by isDebugEnabled check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4323</link><project id="" key="" /><description>In three places the code does something like this:

```
if (logger.isDebugEnabled()) {
    logger.debug(...);
} else if (logger.isTraceEnabled()) {
    logger.trace(...);
}
```

In most (all?) cases, if trace is enabled, debug is also enabled, meaning that even if trace is enabled, it will still enter the debug block and will therefore log the debug message not the trace message.

I suggest the blocks should be in the opposite order (check if trace is enabled - if not, check if debug is enabled)

Minor issue, but annoying if you needed to see the information that that particular trace log would have logged.
</description><key id="23632395">4323</key><summary>In some places trace logging is hidden by isDebugEnabled check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">tstibbs</reporter><labels /><created>2013-12-03T10:09:34Z</created><updated>2014-07-02T13:28:40Z</updated><resolved>2014-07-02T13:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-02T13:28:40Z" id="47775284">Closing this as it was already fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wrong link to Lowercase Token Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4322</link><project id="" key="" /><description /><key id="23629902">4322</key><summary>Wrong link to Lowercase Token Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YousefED</reporter><labels /><created>2013-12-03T09:20:10Z</created><updated>2014-07-16T21:50:56Z</updated><resolved>2013-12-03T09:40:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-12-03T09:40:02Z" id="29695426">Merged - many thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>org.elasticsearch.common.util.concurrent.EsRejectedExecutionException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4321</link><project id="" key="" /><description>org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution of [org.elasticsearch.transport.TransportService$2]
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:35) ~[elasticsearch-0.90.1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821) ~[na:1.7.0_25]
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372) ~[na:1.7.0_25]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:200) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:171) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:147) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.sendExecuteFirstPhase(TransportSearchCountAction.java:78) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:206) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:193) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:142) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchCountAction.doExecute(TransportSearchCountAction.java:60) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchCountAction.doExecute(TransportSearchCountAction.java:50) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:116) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:214) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:841) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:62) ~[elasticsearch-0.90.1.jar:na]
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:57) ~[elasticsearch-0.90.1.jar:na]

what is the cause of this exception? what is the solution of this exception?
</description><key id="23623886">4321</key><summary>org.elasticsearch.common.util.concurrent.EsRejectedExecutionException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">desaxena</reporter><labels /><created>2013-12-03T06:21:24Z</created><updated>2013-12-10T05:45:16Z</updated><resolved>2013-12-04T18:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-12-04T18:21:15Z" id="29830484">Hi @desaxena, looking at the code for 0.90.1, this exception is thrown when a delayed task is scheduled but the thread pool has already been shut down (if the node is shutting down for instance).

If you have any further questions, please don't hesitate to ask them in the [mailing list](http://www.elasticsearch.org/help/). The mailing list is the perfect place to ask questions like this one. Please use github issues for bug reports and enhancements requests. 
</comment><comment author="desaxena" created="2013-12-10T05:45:16Z" id="30201348">thanx
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistent query behavior with multi_field and string value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4320</link><project id="" key="" /><description>I am seeing inconsistent behavior in ES 0.90.7 when storing a string directly in a multi_field, versus as a hash of attributes.  Relevant gists:

https://gist.github.com/azell/7746233
https://gist.github.com/azell/7746253

The only difference in the above two gists is the JSON document published to ES.  In the former, the string is directly associated with its value:
  "name" : "test me"

In the latter, the string is associated with a hash which contains its value:
    "name" : { "value" : "test me" }

Performing a search against the multi_field field without the default name ("name.untouched") returns 1 result in the former, but 0 in the latter.  Is this intended?  Ref http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#string :

The string type also support custom indexing parameters associated with the indexed value. For example:

{
    "message" : {
        "_value":  "boosted value",
        "_boost":  2.0
    }
}

The mapping is required to disambiguate the meaning of the document. Otherwise, the structure would interpret "message" as a value of type "object". The key _value (or value) in the inner document specifies the real string content that should eventually be indexed. The _boost (or boost) key specifies the per field document boost (here 2.0).
</description><key id="23622080">4320</key><summary>Inconsistent query behavior with multi_field and string value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">azell</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>discuss</label><label>low hanging fruit</label></labels><created>2013-12-03T05:25:59Z</created><updated>2015-12-15T11:36:02Z</updated><resolved>2015-12-15T11:36:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="azell" created="2013-12-08T07:12:49Z" id="30076548">The culprit looks to be MultiFieldMapper.parse.  In theory, the code should rewind the ParserContext to the START_OBJECT token before calling Mapper.parse.  As it is, the second and later Mapper objects receive a ParserContext stuck at the END_OBJECT token, as the first mapper has consumed additional tokens.

I will work on a fix. 
</comment><comment author="clintongormley" created="2015-09-21T19:41:16Z" id="142088059">This is still broken in 2.0
</comment><comment author="jimczi" created="2015-12-07T15:25:21Z" id="162556680">The quick and easy solution would be what @azell proposed: " the code should rewind the ParserContext to the START_OBJECT token before calling Mapper.parse". Unfortunately the json parser is forward only and there is no way to rewind it. This is a clear blocker which makes the low hanging fruit higher ;). We could add the object to the context and check in all the field mappers that accept multi fields (or copy_to) if the value is populated or not but this would complicate the mapping even more. 
</comment><comment author="clintongormley" created="2015-12-15T11:36:01Z" id="164737096">thanks for looking at this @jimferenczi. i think we should close in favour of https://github.com/elastic/elasticsearch/issues/15388
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem rebuild_index on haystack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4319</link><project id="" key="" /><description>before using elasticsearch, i using solr for make indexing data for my application (Base from Django). 2 days ago i migrate to elasticsearch. i using haystack for making indexing data. i try rebuild_index without prepare method, is success. But when i try add prepare method, and try again rebuild_index haystack aborted process indexing data.
![screenshot from 2013-12-03 11 41 47](https://f.cloud.github.com/assets/2105984/1661183/84894c12-5bd5-11e3-8513-7d0acd644f7b.png)
</description><key id="23621034">4319</key><summary>Problem rebuild_index on haystack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drayanaindra</reporter><labels /><created>2013-12-03T04:42:48Z</created><updated>2014-08-08T18:59:35Z</updated><resolved>2014-08-08T18:59:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:59:35Z" id="51643924">Hi @drayanaindra 
I guess you should open this on the haystack list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Range filter execution fielddata should work on non-numeric type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4318</link><project id="" key="" /><description>Hey,

You can sort on non-numeric type (string in our case) so why cant you do range filter using the same fielddata?  It seems currently the range filter option fielddata is just a rename of the numeric_range range filter.  Also the current docs dont mention the fact it only works on numeric.

Currently with the 1.0.0Beta2 it fails if you try a range fielddata query with the following

```
QueryParsingException[[index] [range] filter field [date.untouched] is not a numeric type]; }]"
```

Thanks
Zuhaib

Edit:
I see lucene has "FieldCacheRangeFilter" for this support but that currently ES uses TermRangeFilter.
</description><key id="23613285">4318</key><summary>Range filter execution fielddata should work on non-numeric type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zsiddique</reporter><labels><label>adoptme</label></labels><created>2013-12-03T00:37:45Z</created><updated>2015-08-26T14:25:07Z</updated><resolved>2015-08-26T14:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-05T11:01:20Z" id="54611617">Agreed, this could even be done quite efficiently using ordinals. I'm upgrading this issue to an `adoptme`.
</comment><comment author="jpountz" created="2015-08-26T14:25:07Z" id="135038899">Closing: we removed the execution option in 2.0. Range execution through fielddata might come back, but with automatic detection of whichever is more efficient as opposed to using an option.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improved percolation api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4317</link><project id="" key="" /><description>Hey guys,

I was working with the percolator earlier today, and I thought of a nice way to improve its api. So, right now, if I register a query using the percolator:

```
curl -XPUT localhost:9200/_percolator/test/kuku -d '{
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
```

and I percolate a matching document

```
curl -XGET localhost:9200/test/type1/_percolate -d '{
    "doc" : {
        "field1" : "value1"
    }
}'
```

I get something of the effect back:

```
{"ok":true, "matches":["kuku"]}
```

Which a fine way of doing things, but I think it'd be nicer to be able to register metadata along with the query. So, something of the effect:

```
curl -XPUT localhost:9200/_percolator/test/ -d '{
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    },
    "meta": {
        "campaign": 1,
        "user_id": 1,
        "user_name": kuku
    }
}'
```

If a document matched, I'd get something like this back:

```
{"ok":true, "matches":[{"_id": "BzA9dR7OSqGylX-0y80IJA", "campaign": 1, "user_id": 1, "user_name": kuku}]}
```

Currently, if I need to store more information about a query, I need to do hacks, such as concatenating strings with all the information I want to get back from that match (example: "campaign:1-id:1-name:kuku") to use as the id of that document, but that has drawbacks.

What do you guys think?
</description><key id="23605631">4317</key><summary>Improved percolation api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felipellrocha</reporter><labels><label>:Percolator</label><label>adoptme</label><label>enhancement</label></labels><created>2013-12-02T22:09:05Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-09T10:24:17Z" id="30120515">@felipellrocha With the current and the redesigned percolator register you can add metadata with the query just as regular fields since it is treated as a document. Since the percolator returns ids, you can just execute a subsequent get or multi get request to ES to get the documents with that metadata, which just gets what you want?
</comment><comment author="felipellrocha" created="2013-12-13T17:47:49Z" id="30528688">That's fine and all, it's just a little roundabout. Why do it in two api calls, when you can in one? I just feel that would be a more elegant way of doing things. If es already knows which document matched, why not return the meta information that goes along with it, you know?
</comment><comment author="martijnvg" created="2013-12-17T09:38:33Z" id="30737307">Yes, including the meta fields of a match make sense. I guess the percolate api should have the kind of `fields` support as is available in the search api.
</comment><comment author="felipellrocha" created="2013-12-19T17:25:29Z" id="30948074">I like that. It'd save a lot of network traffic in cases you know which fields you're looking for, and it'd make for a consistent api throughout.
</comment><comment author="julianhille" created="2014-08-20T20:28:43Z" id="52838919">we currently solve that through the percolator id. We store there joined by "_" the information we need. Would be awesome to return that within the response and without parsing the id.
</comment><comment author="jeantil" created="2015-02-16T10:38:45Z" id="74488716">I am not sure what the "adoptme" label means, but we would definitely welcome a "fields" param on percolation requests to get additional information instead of encoding them in the id or doing additional requests to get them 
</comment><comment author="martijnvg" created="2015-02-16T21:30:23Z" id="74575302">@jeantil It means that this issue has been discussed and that it is going to be added to ES. This is not really evident from the comments... but I've heard this feature request often, so it should just be added to the percolator, so I'm adding the 2.0 label to the issue, just to make it more visible and likely to get it in sooner.
</comment><comment author="jeantil" created="2015-02-17T07:20:21Z" id="74626807">Thanks, your answer is crystal clear !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/apache/lucene/index/memory/ExtendedMemoryIndex.java</file><file>core/src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexService.java</file><file>core/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>core/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolateStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorFieldMapper.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhase.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCache.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueryCacheStats.java</file><file>core/src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQuery.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryBuilder.java</file><file>core/src/main/java/org/elasticsearch/index/query/PercolatorQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>core/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateException.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorModule.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorHighlightSubFetchPhaseTests.java</file><file>core/src/test/java/org/elasticsearch/index/percolator/PercolatorQueryCacheTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/PercolatorQueryTests.java</file><file>core/src/test/java/org/elasticsearch/index/query/QueryShardContextTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorAggregationsIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/PercolatorServiceTests.java</file><file>core/src/test/java/org/elasticsearch/percolator/RecoveryPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/percolator/TTLPercolatorIT.java</file><file>core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/percolator/PercolatorQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/rescore/QueryRescoreBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolStatsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java</file><file>modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>percolator: Replace percolate api with the new percolator query</comment></comments></commit></commits></item><item><title>Fixing #4308 - Minor issue about Geojson</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4316</link><project id="" key="" /><description>Fixes #4308 - all test pass.
</description><key id="23603843">4316</key><summary>Fixing #4308 - Minor issue about Geojson</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2013-12-02T21:40:00Z</created><updated>2014-07-16T21:50:57Z</updated><resolved>2013-12-04T17:09:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-04T17:09:27Z" id="29824180">Closing as well. Not needed due to discussion resolved at #4308 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Per-field boosting of the _all field is broken unless very specific conditions are met</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4315</link><project id="" key="" /><description>The _all field uses payloads in order to be able to store per-field boosts in a single index field. However, the way it is implemented relies on the fact that the token stream doesn't eagerly consume the input `java.io.Reader` (see `AllEntries.read`). So in practice, boost on the _all field doesn't work when under any of these circumstances:
- there is a char filter,
- the tokenizer is not the `standard` tokenizer,
- any token filter has read-ahead logic.
</description><key id="23600894">4315</key><summary>Per-field boosting of the _all field is broken unless very specific conditions are met</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-02T20:53:30Z</created><updated>2013-12-05T23:04:29Z</updated><resolved>2013-12-05T17:41:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-12-02T22:37:12Z" id="29665402">Could you also consider a wider scope of
1. Per field boost in multified see #4108
2. Infrastructure for boosting fragments of input text at index time. This would allow to have some sort of markup in the indexed json to supply boost to fragments of text. Common use case is finding and boosting fragments of importance as a part of indexing
</comment><comment author="jpountz" created="2013-12-03T11:02:53Z" id="29700644">@roytmana The two issues you are mentioning are actually quite tough to implement, so I would like to concentrate on just fixing boosting on the _all field for now.
</comment><comment author="roytmana" created="2013-12-03T14:38:17Z" id="29714270">@jpountz isn't #1 quite similar to _all?
I understand _all is searched in a special way taking per field boosts stored as postings into account. Could not the same to be done for multifields?
</comment><comment author="jpountz" created="2013-12-03T14:48:32Z" id="29715113">@roytmana a similar method could be applied indeed. But I'm not fully happy with the way per-field boosting works for the _all field so I would like that we consider improving it before applying the same logic to other places. In particular, this doesn't work with all queries (eg. phrase queries) and is quite wasteful storage-wise (4 bytes per occurrence of a term whose field has a boost which is not 1: I wouldn't be surprised to see that it sometimes almost doubles the size of the inverted index for the _all field).
</comment><comment author="roytmana" created="2013-12-03T14:57:35Z" id="29715950">@jpountz Great thank you for the info. I just wanted to bring these two cases up so you could consider them as you work on _all implementation. Hopefully multifield will follow soon :-) and an arbitrary snippet boosting after that 
</comment><comment author="roytmana" created="2013-12-05T17:48:50Z" id="29920165">@jpountz do you mind if I create another ticket with expanded scope as discussed in my first reply toy your post as I feel ability to boos individual text fragments and particularly multifields is very powerful feature?
Or maybe you would rather write it up yourself?
</comment><comment author="jpountz" created="2013-12-05T23:03:55Z" id="29948003">@roytmana please open a ticket. I do think the ability to boost individual text fragments is very interesting!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/all/AllEntries.java</file><file>src/main/java/org/elasticsearch/common/lucene/all/AllTokenStream.java</file><file>src/test/java/org/elasticsearch/common/lucene/all/SimpleAllTests.java</file></files><comments><comment>Fix _all boosting.</comment></comments></commit></commits></item><item><title>starting elasticsearch on solaris fails (1.0.0.Beta2-SNAPSHOT)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4314</link><project id="" key="" /><description>When executing bin/elasticsearch on solaris, I got this error:

./elasticsearch: syntax error at line 92: `JAVA=$' unexpected

Had to change this line:
     `JAVA=$(which java)`
to:
    `JAVA=`which java``

This also makes it consistent with the rest of the file.

Version: 1.0.0.Beta2-SNAPSHOT
</description><key id="23586168">4314</key><summary>starting elasticsearch on solaris fails (1.0.0.Beta2-SNAPSHOT)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ah85</reporter><labels /><created>2013-12-02T17:04:21Z</created><updated>2014-03-11T15:55:21Z</updated><resolved>2013-12-03T09:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-03T09:25:52Z" id="29694492">Thanks a lot for notifying! Took the opportunity and removed any `$(whatever)` bashism by using backticks.
</comment><comment author="tmzullinger" created="2014-03-11T15:55:21Z" id="37312128">FWIW, $() is not a bashism.  It is defined in POSIX¹.  If /bin/sh on Solaris doesn't accept this, it's because it's an ancient shell (not surprising, given how ancient most of the tools on Solaris are).  

That might still be a reason to use backticks, but it's incorrect to call this a bashism.  When working around old shells on systems like Solaris, it's worth noting that's the problem so the work around can be removed when those ancient systems are no longer supported.  (As I understand it, even Solaris has a proper posix sh, it just isn't always /bin/sh.)

¹ http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_03
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove bash specific calls</comment></comments></commit></commits></item><item><title>has_parent query can yields inconstant results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4313</link><project id="" key="" /><description>When `has_parent` query get wrapped into a `filtered` query, it can yield wrong results.
</description><key id="23580385">4313</key><summary>has_parent query can yields inconstant results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-12-02T15:44:24Z</created><updated>2013-12-02T15:46:04Z</updated><resolved>2013-12-02T15:44:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-02T15:44:33Z" id="29627978">Fixed via: https://github.com/elasticsearch/elasticsearch/commit/079ac79617c199756404b30cd7955ba88f8039f8
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Test framework documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4312</link><project id="" key="" /><description>The java test framework using randomized testing is explained with a couple of code samples.
</description><key id="23570225">4312</key><summary>[DOCS] Test framework documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-02T12:51:15Z</created><updated>2014-07-16T21:50:57Z</updated><resolved>2013-12-02T17:03:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-02T13:23:12Z" id="29616930">LGTM Thanks alex
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support YAML as output format of the analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4311</link><project id="" key="" /><description>It would be nice to be able to get the result of an analyzer in YAML instead of JSON. Currently I am not able to get it as YAML. Neither via `format=yaml` nor by setting the accept header to `application\yaml`.
</description><key id="23568557">4311</key><summary>Support YAML as output format of the analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">obfischer</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-02T12:13:30Z</created><updated>2013-12-08T22:20:47Z</updated><resolved>2013-12-08T22:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-12-04T18:43:30Z" id="29832413">I looked into this, it looks like setting `?format=yaml` does work for the XBuilder, but fails to produce any output because of https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java#L122

I'll work on a fix for this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java</file></files><comments><comment>Support 'yaml' as a format for the Analyze API</comment></comments></commit></commits></item><item><title>Remove "ok" : true from successful REST responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4310</link><project id="" key="" /><description>Its pretty meaningless, especially with correct REST codes. Also, we are not consistent in returning it in all APIs. I suggest we simply remove it.
</description><key id="23567246">4310</key><summary>Remove "ok" : true from successful REST responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-02T11:44:08Z</created><updated>2014-10-02T21:26:40Z</updated><resolved>2014-01-07T16:26:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2013-12-02T17:11:20Z" id="29636785">+1
</comment><comment author="dakrone" created="2014-01-07T16:26:23Z" id="31752451">Merged in d23f640c
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] updated json responses after #4310 and #4480</comment></comments></commit></commits></item><item><title>Change mapping of a single type without having to re-create the whole index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4309</link><project id="" key="" /><description>Currently it's not possible to change the mapping of a type; a new index has to be created and populated. If my system has many types and I only want to change the mapping of one type, nevertheless I've to re-index everything.

The idea could be the type first has to be removed (thus loosing all data) and then created that type with it's mapping.
</description><key id="23557088">4309</key><summary>Change mapping of a single type without having to re-create the whole index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfn</reporter><labels /><created>2013-12-02T07:57:13Z</created><updated>2013-12-02T21:00:18Z</updated><resolved>2013-12-02T21:00:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-02T08:11:56Z" id="29600547">Why not using http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-delete-mapping.html ?
</comment><comment author="mfn" created="2013-12-02T21:00:18Z" id="29656912">I somehow overlooked it. It's exactly what I needed. Sorry for the noise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Minor issue about Geojson</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4308</link><project id="" key="" /><description>Currently the geojson is returned in lower case:

```
{
  type: polygon
  coordinates: [ ]
}
```

But geojson is case sensitive - at least leaflet needs this. To workaround it one has to do:

```
if(bounds["type"] == "polygon")
    bounds.type = "Polygon";
else if(bounds["type"] == "mulipolygon")
    bounds.type = "MultiPolygon";
else if(bounds["type"] == "linestring")
    bounds.type = "LineString";
...
```
</description><key id="23539343">4308</key><summary>Minor issue about Geojson</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2013-12-01T16:57:51Z</created><updated>2013-12-04T11:27:50Z</updated><resolved>2013-12-04T11:27:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-01T18:32:00Z" id="29579588">I guess we can fix this for 1.0 and break BWcompat?
</comment><comment author="karussell" created="2013-12-01T19:36:03Z" id="29581062">Yes, would be good. But before please verify in the specs: http://geojson.org/geojson-spec.html that I'm not wrong
</comment><comment author="karussell" created="2013-12-01T19:36:38Z" id="29581078">Ah, here it is: "The value of the type member must be one of: "Point", "MultiPoint", "LineString", "MultiLineString", "Polygon", "MultiPolygon", "GeometryCollection", "Feature", or "FeatureCollection". **The case of the type member values must be as shown here**."
</comment><comment author="s1monw" created="2013-12-01T19:37:18Z" id="29581092">got it. Do you want to prepare a pull-request?
</comment><comment author="karussell" created="2013-12-01T19:49:32Z" id="29581386">ok
</comment><comment author="karussell" created="2013-12-02T16:43:19Z" id="29633898">I'll rename Envelope and Circle too although I do not see this in the spec, ok?

(And then there seems to GeometryCollection which is absent in ES)
</comment><comment author="spinscale" created="2013-12-02T17:13:50Z" id="29637034">Circle was part of a spec proposal which was never accepted by the geojson people... cant find the link to the proposal right now, sorry
</comment><comment author="karussell" created="2013-12-02T17:17:02Z" id="29637386">Ah, ok. + I think I've found something: http://geopriv.dreamhosters.com/geojson/geojson-spec.html

BTW: is it normal that ES tests do fail if I'm disconnected?
</comment><comment author="spinscale" created="2013-12-02T17:23:39Z" id="29637992">should not be the case: try `ES_TEST_LOCAL=true mvn test` - iirc the tests annotated with `@Network` are skipped by default. try setting `tests.network` explicitely to false.
</comment><comment author="karussell" created="2013-12-02T21:40:23Z" id="29660497">Please have a look!
</comment><comment author="s1monw" created="2013-12-04T10:58:12Z" id="29795827">@spinscale can you look at the PR looks good to me!
</comment><comment author="spinscale" created="2013-12-04T11:11:38Z" id="29796573">@karussell I am unsure I got the whole issue right, so excuse me for jumping in lately here.

What your change is doing at a first glance is to ensure, that you uppercase the first char of the geo shapes on parsing time - in order to be GeoJSON compliant. However if you indexed the shapes already like that, you would be geo json compliant anyway, but still be able to index shapes with lowercase names.

This more sounds like a strict option in the mapping to me, if at all needed. But maybe I am missing something important here and need to be enlightened before commenting more. Thanks!
</comment><comment author="karussell" created="2013-12-04T11:27:50Z" id="29797491">&gt; This more sounds like a strict option in the mapping to me, if at all needed.

Oh, no! This was indeed the case that I index them lower case ... sorry! I thought it was automatically done as I was sure that I was using the camel case thing.

I think Elasticsearch allows indexing everything so it should not force this here too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shingle filter should expose `filler_token`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4307</link><project id="" key="" /><description>Since Lucene 4.4 release enable_position_increment settings on token filters cannot be set to false which results in underscores appearing for filtered tokens in shingle filters.
</description><key id="23537462">4307</key><summary>Shingle filter should expose `filler_token`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">MrHash</reporter><labels><label>enhancement</label></labels><created>2013-12-01T14:39:31Z</created><updated>2015-08-12T09:44:47Z</updated><resolved>2014-02-26T21:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-01T18:33:43Z" id="29579630">I agree that shingle filter needs to have some options here but as a workaround you can specify a `lucene_version` with your `stop` filter to still set the `enable_position_increment`
</comment><comment author="MrHash" created="2013-12-01T19:39:37Z" id="29581167">OK thanks for the tip - that works for now. Presumably if position increment disabling has been removed from the Lucene core then the only workaround is to modify the shingle filter. I had a look at the Lucene source and it looks there is currently no method to override the filler token to an empty string which is currently hardcoded as an underscore.
</comment><comment author="s1monw" created="2013-12-01T19:40:47Z" id="29581195">@MrHash yes that is correct. I think there needs to be one. I hope I will be able to open an issue and fix that in lucene soon. Feel free to beat me!
</comment><comment author="s1monw" created="2013-12-01T20:59:08Z" id="29582949">here is an issue https://issues.apache.org/jira/browse/LUCENE-5353
</comment><comment author="MrHash" created="2013-12-01T21:07:24Z" id="29583149">Nice. I noticed also that the remove_trailing=false option of the stop filter also generates a filler token, despite enable_position_increments being set to false. Hopefully this update should also take care of this issue.
</comment><comment author="s1monw" created="2014-02-19T11:15:44Z" id="35488162">with lucene 4.7 we will be able to make the filler token configurable. 
</comment><comment author="MrHash" created="2014-02-24T08:56:46Z" id="35867545">Good stuff.
</comment><comment author="alup" created="2014-10-17T13:17:50Z" id="59510850">By using `filler_token = ""` you may end up with duplicate tokens in the token stream. This is not the same behavior as it was by using `enable_position_increment`. Is there any way to bypass this problem?
</comment><comment author="lalitkapoor" created="2014-10-27T21:55:58Z" id="60676475">+1
</comment><comment author="clintongormley" created="2014-10-28T10:06:49Z" id="60733054">@alup Could you open a new issue and provide more detail of the problem please?
</comment><comment author="alup" created="2014-10-29T15:29:29Z" id="60944541">Ok, when I find some time, I will come up with an example to denote the problem.
</comment><comment author="apanimesh061" created="2015-08-11T22:12:47Z" id="130093243">@alup Did you find the work around for this? I did post a question [here](https://discuss.elastic.co/t/are-these-es-bugs-or-just-wrong-implementations-by-me/27228) here as well.
</comment><comment author="clintongormley" created="2015-08-12T09:44:47Z" id="130239252">@apanimesh061 could you open a new issue for this, and include a full JSON recreation?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactoryTests.java</file></files><comments><comment>Expose `filler_token` via ShingleTokenFilterFactory</comment></comments></commit></commits></item><item><title>has_child filter and query yield inconsistent results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4306</link><project id="" key="" /><description>Document marked as deleted are not taken into account:
- The deleted docs are not being applied in the has_child query, which they should and were taken into account in has_child filter which isn't necessary.
- The short circuit mechanism needs to know know about deleted docs, but it doesn't and therefor short circuits the has_child query execution too early.
- The above was amplified when the `has_child` query was wrapped into a `filtered` query. 

Relates to #4297 and #4210.
</description><key id="23524239">4306</key><summary>has_child filter and query yield inconsistent results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>regression</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-30T18:15:00Z</created><updated>2013-12-02T09:14:38Z</updated><resolved>2013-11-30T19:06:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file></files><comments><comment>The short circuit mechanism always needs to be wrapped with the live docs. In certain scenarios the live docs isn't passed down with acceptedDocs.</comment><comment>For example when a has_child is wrapped in a filtered query as query and the wrapped filter is cached.</comment><comment>The short circuit mechanism in that case counts down based on deleted docs, which then yields lower results than is expected.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ConstantScorer.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file></files><comments><comment>Fixes related to accepted docs not taken into account:</comment><comment>* Removed the applyAcceptedDocs in ChildrenConstantScoreQuery, they need to be applied at all times. (because of short circuit mechanism)</comment><comment>* Moved ParentDocSet to FilteredDocIdSetIterator, because it fits better than MatchDocIdSet.</comment><comment>* Made similar changes to ParentConstantScoreQuery for consistency between the two queries. The bug accepted docs bug didn't occur in the ParentConstantScoreQuery.</comment><comment>* Updated random p/c tests to randomly update parent or child docs during the test run.</comment></comments></commit></commits></item><item><title>Cat API: Improve help and execute help before sending server requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4305</link><project id="" key="" /><description>Just a quick afternoon try to not send the actual requests when you only want to get the help for an endpoint. Still need to take a closer look at the whole API and its design on source code level to be sure, this is the correct approach. @drewr maybe you can have a look...
- Force the cat API classes to have simple help available by extending from a AbstractCatAction
- Use a set binder on guice creation to create the help on node start up
- Make sure that the help/field info is returned without querying any data
</description><key id="23522298">4305</key><summary>Cat API: Improve help and execute help before sending server requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-30T15:55:25Z</created><updated>2014-06-29T18:07:43Z</updated><resolved>2013-12-03T23:32:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-02T23:25:24Z" id="29669086">this is great!
</comment><comment author="dakrone" created="2013-12-03T16:38:55Z" id="29726131">This works great, thanks @spinscale! :+1:
</comment><comment author="drewr" created="2013-12-04T01:08:30Z" id="29768636">Ugh, horrible merge job. :-1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Corrupt ElasticSearch index - why the index won't restart?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4304</link><project id="" key="" /><description>I had to restart my ElasticSearch instance, and when it tried to restart I see these errors. I have 3 indices, and 2 of them were able to restart properly, but the last one, which has 5 shards, only 1 of the shard was able to recover.

I see these errors in the logs.

What could be wrong? Is this a corrupt index?

Caused by: java.io.IOException: Input/output error: NIOFSIndexInput(path="/home/sshadmin/elasticsearch-0.90.2/data/buzzsumo/nodes/0/indices/articles/4/index/_1nmus_es090_0.blm")
        at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:191)
        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:272)
        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:51)
        at org.apache.lucene.store.DataInput.readInt(DataInput.java:84)
        at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:181)
        at org.apache.lucene.store.DataInput.readLong(DataInput.java:131)
        at org.apache.lucene.store.BufferedIndexInput.readLong(BufferedIndexInput.java:194)
        at org.elasticsearch.index.codec.postingsformat.BloomFilter.deserialize(BloomFilter.java:160)
        at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.&lt;init&gt;(BloomFilterPostingsFormat.java:136)
        at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:100)
        at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat.fieldsProducer(ElasticSearch090PostingsFormat.java:81)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:194)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:233)
        at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:127)
        at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:56)
        at org.apache.lucene.index.ReadersAndLiveDocs.getReader(ReadersAndLiveDocs.java:121)
        at org.apache.lucene.index.ReadersAndLiveDocs.getReadOnlyClone(ReadersAndLiveDocs.java:218)
        at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:100)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:377)
        at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111)
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1427)
        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:275)
        ... 6 more
    Caused by: java.io.IOException: Input/output error
        at sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.pread(Unknown Source)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(Unknown Source)
        at sun.nio.ch.IOUtil.read(Unknown Source)
        at sun.nio.ch.FileChannelImpl.read(Unknown Source)
        at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:176)
        ... 28 more
    [2013-11-29 22:23:59,788][WARN ][indices.memory           ] [BuzzSumo Search 1] failed to set shard [articles][4] index buffer to [59.4mb]
</description><key id="23514598">4304</key><summary>Corrupt ElasticSearch index - why the index won't restart?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HenleyChiu</reporter><labels><label>discuss</label></labels><created>2013-11-30T03:27:35Z</created><updated>2014-10-17T09:17:34Z</updated><resolved>2014-10-17T09:17:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T09:17:34Z" id="59487873">Hi @HenleyChiu 

Sorry for the late reply.  This error looks like you have suffered from some hardware failure at the disk level (or network level, if you're using a network attached disk).  The error message is just what the JVM receives from the kernel, so we can't debug this further.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>minScore is not inclusive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4303</link><project id="" key="" /><description>See #3934 
</description><key id="23506491">4303</key><summary>minScore is not inclusive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-29T19:28:22Z</created><updated>2013-11-29T20:44:11Z</updated><resolved>2013-11-29T20:44:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/MinimumScoreCollector.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Changed the minScore comparator from &gt; to &gt;=</comment></comments></commit></commits></item><item><title>CAT api: Minor changes for consistent behaviour</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4302</link><project id="" key="" /><description>- Added cat help documentation for /recovery/{index}
- Made behaviour on specifying non-existing indices consistent to always return 404 if one index does not exist
- Returning '-' in NodesAction if neither client nor data node in order to prevent an empty column (makes it hard to parse)
</description><key id="23501115">4302</key><summary>CAT api: Minor changes for consistent behaviour</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-29T16:40:41Z</created><updated>2014-07-16T21:50:58Z</updated><resolved>2013-12-09T16:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-08T21:50:05Z" id="30093869">if its still relevant, can we re-apply these changes to master?
</comment><comment author="spinscale" created="2013-12-09T16:00:06Z" id="30143840">Everything here included in the meantime... closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Issue with BulkProcessor api and ConcurrentRequests &gt; 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4301</link><project id="" key="" /><description>I have switched my river to use `org.elasticsearch.action.bulk.BulkProcessor` in the last release using the following default values: 

``` java
  public final static int DEFAULT_CONCURRENT_REQUESTS = 50;
  public final static int DEFAULT_BULK_ACTIONS = 1000;
  public final static TimeValue DEFAULT_FLUSH_INTERVAL = TimeValue.timeValueMillis(10);
  public final static ByteSizeValue DEFAULT_BULK_SIZE = new ByteSizeValue(5, ByteSizeUnit.MB);
```

Unfortunately it seems there is an issue with this api when a value of ConcurrentRequests different from the default (1) - probably greater than the number of CPUs available. Number of documents send to bulkprocessor does not match the number of documents indexed (all documents are unique).
I have created a small project to reproduce the issue [1].
Run with conccurentRequest = 50 return:
java.lang.RuntimeException: Number of documents indexed 357841 does not match the target 1000000
java.lang.RuntimeException: Number of documents indexed 257381 does not match the target 1000000
Run with conccurentRequest = 8 the results are correct.

I am using ES 0.90.7

[1] - https://github.com/richardwilly98/test-elasticsearch-bulk-processor
</description><key id="23493653">4301</key><summary>Issue with BulkProcessor api and ConcurrentRequests &gt; 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">richardwilly98</reporter><labels><label>non-issue</label></labels><created>2013-11-29T14:01:39Z</created><updated>2014-05-27T10:50:14Z</updated><resolved>2013-12-02T09:35:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-29T14:54:18Z" id="29520701">Are you sure you cleaned the index before starting your test? I cloned the project, ran it multiple times and never saw the exception at the end but I noticed a different count within your counter variable. `indexedDocumentCount` is written by separate threads that concurrently call `afterBulk` and needs proper synchronization, an `AtomicInteger` would fix it.

Beyond that, a few pointers on using `BulkProcessor`: I wouldn't throw exception in `afterBulk` (see #3761 too), I would check if there are failures in each `BulkResponse` in the `afterBulk`. Also, I would wait for green after creating the index.

I think the issue is in your code and not in the `BulkProcessor`, let me know what you think.
</comment><comment author="javanna" created="2013-11-29T14:56:35Z" id="29520842">By the way, if you set a too high concurrent requests, you might get this back: `EsRejectedExecutionException[rejected execution (queue capacity 50)`, but you only see it if you check failures within the `BulkResponse`.
</comment><comment author="richardwilly98" created="2013-11-29T15:40:34Z" id="29523272">Yes I did start from a new index at each iteration.

Sent via BlackBerry by AT&amp;T

-----Original Message-----
From: Luca Cavanna notifications@github.com
Date: Fri, 29 Nov 2013 06:54:53 
To: elasticsearch/elasticsearchelasticsearch@noreply.github.com
Reply-To: elasticsearch/elasticsearch reply@reply.github.com
Cc: Richard Louaprerichard.louapre@gmail.com
Subject: Re: [elasticsearch] Issue with BulkProcessor api and
 ConcurrentRequests &gt; 1 (#4301)

Are you sure you cleaned the index before starting your test? I cloned the project, ran it multiple times and never saw the exception at the end but I noticed a different count within your counter variable. `indexedDocumentCount` is written by separate threads that concurrently call `afterBulk` and needs proper synchronization, an `AtomicInteger` would fix it.

Beyond that, a few pointers on using `BulkProcessor`: I wouldn't throw exception in `afterBulk` (see #3761 too), I would check if there are failures in each `BulkResponse` in the `afterBulk`. Also, I would wait for green after creating the index.

I think the issue is in your code and not in the `BulkProcessor`, let me know what you think.

---

Reply to this email directly or view it on GitHub:
https://github.com/elasticsearch/elasticsearch/issues/4301#issuecomment-29520701
</comment><comment author="richardwilly98" created="2013-11-29T15:46:41Z" id="29523631">I have a couple of users reporting missing documents sine I tried to use BulkPorcessor API.

Sent via BlackBerry by AT&amp;T

-----Original Message-----
From: richard.louapre@gmail.com
Date: Fri, 29 Nov 2013 15:38:56 
To: elasticsearch/elasticsearchreply@reply.github.com
Reply-To: richard.louapre@gmail.com
Subject: Re: [elasticsearch] Issue with BulkProcessor api and ConcurrentRequests &gt; 1 (#4301)

Yes I did start from a new index at each iteration.

Sent via BlackBerry by AT&amp;T

-----Original Message-----
From: Luca Cavanna notifications@github.com
Date: Fri, 29 Nov 2013 06:54:53 
To: elasticsearch/elasticsearchelasticsearch@noreply.github.com
Reply-To: elasticsearch/elasticsearch reply@reply.github.com
Cc: Richard Louaprerichard.louapre@gmail.com
Subject: Re: [elasticsearch] Issue with BulkProcessor api and
 ConcurrentRequests &gt; 1 (#4301)

Are you sure you cleaned the index before starting your test? I cloned the project, ran it multiple times and never saw the exception at the end but I noticed a different count within your counter variable. `indexedDocumentCount` is written by separate threads that concurrently call `afterBulk` and needs proper synchronization, an `AtomicInteger` would fix it.

Beyond that, a few pointers on using `BulkProcessor`: I wouldn't throw exception in `afterBulk` (see #3761 too), I would check if there are failures in each `BulkResponse` in the `afterBulk`. Also, I would wait for green after creating the index.

I think the issue is in your code and not in the `BulkProcessor`, let me know what you think.

---

Reply to this email directly or view it on GitHub:
https://github.com/elasticsearch/elasticsearch/issues/4301#issuecomment-29520701
</comment><comment author="javanna" created="2013-11-29T15:51:45Z" id="29523940">@richardwilly98 as I said the problem is in your code and the way you use the `BulkProcessor`, as far as I can see from your testcase. I gave you suggestions on how to fix it, could you please try that out?
</comment><comment author="richardwilly98" created="2013-11-30T10:53:40Z" id="29550231">@javanna 
I have implemented your suggestions:
- Use `AtomicInteger` for `indexedDocumentCount` variable.
- Look for failures in `afterBulk` method
- Wait for green status of the cluster after creating index and flushing bulkprocessor

The first suggestion helps to get the correct value but I still missing documents if concurrent request is greater then the number of cores available. 
I have updated my test to ramp-up the number of concurrent requests. On my laptop with 8 cores I start to get this behavior with 11 concurrent request. I understand that using a number of concurrent request greater than the number of cores may not make sense.
But API should not miss document to index because if this value. Do you agree?
</comment><comment author="javanna" created="2013-11-30T18:46:14Z" id="29558295">If there are errors you are notified in the `afterBulk` methods. It can either be a failure or exceptions returned for specific documents within the `BulkResponse`. If you do get back errors, which happens when you set a too high concurrent requests for instance, well then the number of indexed documents is differerent from what you expected, but you are notified that something went wrong.

Can you reproduce the case where you get no errors through `afterBulk` and the number of indexed documents is less than what you expect? 
</comment><comment author="richardwilly98" created="2013-12-01T10:53:30Z" id="29571399">@javanna 
That's exactly what's was happening: I got failures in the `afterBulk` methods.
Basically I will need to find the _sweet_ spot for concurrent request parameter.
Is there a way from the API to tell when ES queue capacity is close to the max?
Thanks again for your help!
</comment><comment author="javanna" created="2013-12-02T09:35:04Z" id="29604375">Ok then I don't see any bug in the `BulkProcessor`, as it does notify that there are issues. If one of the documents is not properly indexed you know that something went wrong. I'm going to close this issue.

I suspect you are exceeding the size of the bulk thread pool, which is per shard, on each node. That means that if you have a single node with 10 shards, and a single bulk request touches documents that are on all those 10 shards, you are potentially going to have 10 parallel threads (one per shard) doing the work. That way it's easy to reach the maximum queue size which is 50 by default.

You can see thread pools sizes and current values using the nodes info api [http://localhost:9200/_nodes?thread_pool](http://localhost:9200/_nodes?thread_pool). You can monitor how they are doing using the nodes stats api [http://localhost:9200/_nodes/stats?thread_pool](http://localhost:9200/_nodes/stats?thread_pool). You can also change the [thread pools](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html) types and queue size if needed using the cluster update settings api, but be careful there...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>External versioning does not persist versions of deletes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4300</link><project id="" key="" /><description>Elasticsearch only keeps a record of the version of deletes for a short time (the 'index.gc_deletes' setting, default 60s). I think this can cause issues if you restart a cluster, as the record of deletes will have been lost during the restart. I'm using external versioning.

It’s easy to try out:
1. Create an index with the default of 1 replica per primary shard, spread across two nodes, then shutdown both nodes.
2. Start node 1.
3. Send a document with external version 1.
4. Shutdown node 1.
5. Startup node 2.
6. Startup node 1.

In step 5 node 2 becomes the master, so when node 1 starts in step 6, node 1 removes the document you’ve just sent to it, because the master didn’t have it. If elasticsearch kept records of deletes, it would have known that the reason it didn’t exist on node 2 was because it hasn’t received it yet (it wouldn't have an entry in its 'deletes cache', so would know that it hadn't deleted it).

And there’s a variation on the above which enables a delete to be ‘undone’, despite the fact that the version of the delete is greater than the version of the insert.

So, because it doesn't keep a persistent record of the deletes, when resolving differences between nodes it doesn’t know if a node simply hasn’t received an update yet, or if it has actually deleted it since the most recent update.
My feeling is that this is wrong. If two nodes disagree about whether a document should exist or not, then I feel like the versioning system should be used, rather than using the master as the version of truth. 

One the face of it it seems like a fairly simple change - persist the contents of RobinEngine.versionMap to disk (another lucene index?), and read the contents of this into memory on startup. There's clearly performance issues though if we write/delete a record to some kind of disk store every time a document is index.
</description><key id="23487649">4300</key><summary>External versioning does not persist versions of deletes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">tstibbs</reporter><labels /><created>2013-11-29T11:32:26Z</created><updated>2014-08-08T18:53:42Z</updated><resolved>2014-08-08T18:53:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-01-31T14:41:36Z" id="33798909">I think there is a potential issue here of not honouring GC deletes, but it only happens in the case that you bring all the copies of a shard down, either one by one during an upgrade or together (in which case the cluster status becomes RED). If just one node falls off and comes back it's OK (assuming you have replicas). Given the temporary nature of delete retention, I'm not sure it's worth the effort to persist them all the time, just for these use cases.
</comment><comment author="kimchy" created="2014-01-31T15:00:03Z" id="33800534">Indeed, this is by design.
</comment><comment author="kimchy" created="2014-01-31T17:43:49Z" id="33824544">btw, the example you gave is not related to deletes. 2 nodes tests are, well, tricky in distributed systems :). For those case, you can require that both copies will be available on writes with write consistency, for example. On more than 2 nodes, the story is different, and again, if you care more about HA, then you might want to have 3 copies of the data, or require quorum/all copies of the data be available.
</comment><comment author="tstibbs" created="2014-02-03T09:18:04Z" id="33934555">I can see why you need an odd number of copies of data when you've got no versioning system, but I think my point is that when you're using _external_ versioning, I don't see why it should matter whether it's two, three or x nodes.

In any system, given a set of actions, if every action in that set has a version, then it does not matter what order the actions arrive in, or whether all parts of the system agree on which action is the latest - you can always determine the most recent action by simply looking at the versions.

In elasticsearch the set of actions is just an ordered collection of puts and deletes. Even if I only have two nodes in my system, and even if they don't agree on the what was the latest action, it's (conceptually) trivial to work out which action was the latest - it's the one with the highest version. 

If only ever doing puts, we already have enough information to do this (the version is stored in the index) - it's only because of the deletes that there's a problem.
</comment><comment author="clintongormley" created="2014-08-08T18:53:42Z" id="51643167">Elasticsearch doesn't have an eventual consistency model. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix computation of explanations for AllTermQuery.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4299</link><project id="" key="" /><description>With this commit, numMatches is now updated in `setFreqCurrentDoc` and `sloppyFreq` is used instead of `freq` to compute the score of the explanation.
</description><key id="23484545">4299</key><summary>Fix computation of explanations for AllTermQuery.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-11-29T10:20:29Z</created><updated>2014-07-16T21:50:58Z</updated><resolved>2013-12-02T16:00:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>AllTermQuery explanations are not accurate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4298</link><project id="" key="" /><description>AllTermQuery explanations return scores that are different from the scores that are assigned to documents. There are two issues:
- `numMatches` is not updated in `setFreqCurrentDoc`,
- `freq()` is used instead of `sloppyFreq()` to compute the score of the explanation.
</description><key id="23484389">4298</key><summary>AllTermQuery explanations are not accurate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-29T10:17:18Z</created><updated>2013-11-29T18:29:15Z</updated><resolved>2013-11-29T18:29:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>src/test/java/org/elasticsearch/common/lucene/all/SimpleAllTests.java</file></files><comments><comment>Fix computation of explanations for AllTermQuery.</comment></comments></commit></commits></item><item><title>has_child filter yields inconsistent results when the number of child docs are high</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4297</link><project id="" key="" /><description>Using 0.90.7 we've experienced the following: We have ~100K parent docs and ~1M child docs. We're essentially doing what's described here: http://joelabrahamsson.com/grouping-in-elasticsearch-using-child-documents/

When searching for parent docs and filtering using has_child it works as expected when the has_child filter contains a filter that limits the possible child docs to ~200K. The same goes when the number of child docs in the index is lower. 

When we we however use the exact same query/filters with the exception that the has_child filter only limits the possible child docs to  &gt; ~500K we start seeing results that can't possibly be right. Also, when we run the same query again it produces a different result.

So, it seems to me that when the number of child docs become large the search results become inconsistent. The result does not however contain any indication that something has gone wrong (10 shards, 10 successful).

We're using five small nodes where ES has 1GB memory on each. The relevant index has 10 shards and 1 replica.
</description><key id="23479677">4297</key><summary>has_child filter yields inconsistent results when the number of child docs are high</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">joelabrahamsson</reporter><labels /><created>2013-11-29T08:20:43Z</created><updated>2013-12-04T22:46:16Z</updated><resolved>2013-12-04T22:46:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-29T08:52:29Z" id="29503092">@joelabrahamsson Can you share the query that produces correct results and the query that produces incorrect results?
</comment><comment author="joelabrahamsson" created="2013-11-29T10:22:16Z" id="29507458">One example would be, this produces consistent results when issued multiple times:

```
curl -XPOST "http://localhost:9200/pageviews/user/_search" -d'
{
   "query": {
      "filtered": {
         "query": {
            "match_all": {}
         },
         "filter": {
            "has_child": {
               "type": "userpageview",
               "filter": {
                   "range": {
                      "timestamp": {
                         "gte": "2013-11-29T07:15:00.000Z",
                         "lt": "2013-11-29T08:15:00.000Z"
                      }
                   }
               }
            }
         }
      }
   }
}'
```

While this produces different results each time:

```
curl -XPOST "http://localhost:9200/pageviews/user/_search" -d'
{
   "query": {
      "filtered": {
         "query": {
            "match_all": {}
         },
         "filter": {
            "has_child": {
               "type": "userpageview",
               "filter": {
                   "range": {
                      "timestamp": {
                         "gte": "2013-11-29T05:15:00.000Z",
                         "lt": "2013-11-29T08:15:00.000Z"
                      }
                   }
               }
            }
         }
      }
   }
}'
```
</comment><comment author="martijnvg" created="2013-11-29T12:29:50Z" id="29513647">I've been digging around and I want to check if you're executing delete or update requests on the same index as well for the parent documents?
</comment><comment author="joelabrahamsson" created="2013-11-29T13:25:33Z" id="29515920">Yes, we're: 
- Continuously indexing child documents, about 100/second.
- The child documents are indexed with _ttl so they are also continuously removed.
- Continously indexing parent documents using _update.

However, no child documents that match the above filters are indexed or removed.
</comment><comment author="martijnvg" created="2013-11-29T21:19:44Z" id="29538022">After some more debugging it seems that parent document marked as deleted where not handled properly. I will push a fix for this soon.

The indexing on the parent docs via the _update executes under the hood a delete and then an add, so this was triggering the bug.
</comment><comment author="martijnvg" created="2013-12-02T09:14:38Z" id="29603314">@joelabrahamsson I made several fixes to the has_child filter and query via #4306, the issues you're reporting should have been fixed.
</comment><comment author="martijnvg" created="2013-12-02T16:41:45Z" id="29633726">@joelabrahamsson The fixes are included in the just released beta2 release, if you can it would be great if you can verify if the `has_child` is working as expected in your environment.
</comment><comment author="joelabrahamsson" created="2013-12-02T17:04:57Z" id="29636186">Great! To be sure I'd have to try in production. Is it a viable to upgrade one node at a time from 0.90.7 to beta 2 while keeping the cluster running?
</comment><comment author="martijnvg" created="2013-12-02T21:27:27Z" id="29659260">Upgrading from 0.90.7 to 1.0.beta2 can't be done in a rolling manner (node at the time). You'll need to stop all the nodes in the cluster, upgrade all the nodes and then start all the nodes again. Also this is a beta release, this release doesn't provide any backward compatibility guarantees, in worst case you may need to reindex your data when upgrading to the next 1.0 release (and in best case scenario you can just do a rolling upgrade). I was hoping that you could verify the bug fixes in a development or pre-production environment.

I have also back ported the fixes to the 0.90 branch, so when the next 0.90.x version is released these fixes are also included. Perhaps for production you can build a 0.90.8-SNAPSHOT and run that? (once 0.90.8 is released you can then just upgrade in a rolling manner. I expect 0.90.8 to be released soon)
</comment><comment author="joelabrahamsson" created="2013-12-03T14:56:01Z" id="29715815">Just an update: we've upgraded to beta 2 in production and we'll evaluate either tomorrow or thursday when we have good enough data. So, we'll definitely get back on this :)
</comment><comment author="martijnvg" created="2013-12-03T14:59:31Z" id="29716133">@joelabrahamsson Cool, thanks for sharing this!
</comment><comment author="joelabrahamsson" created="2013-12-04T15:55:36Z" id="29817132">The fix resolved our issue. Thanks!
</comment><comment author="martijnvg" created="2013-12-04T22:46:16Z" id="29853975">Thanks for confirming, I'll close this issue. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cat/indices waits for cluster health timeout on missing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4296</link><project id="" key="" /><description>We're not using `concreteIndices` right after creating it. 
</description><key id="23473558">4296</key><summary>_cat/indices waits for cluster health timeout on missing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label><label>v1.0.0.Beta2</label></labels><created>2013-11-29T03:06:02Z</created><updated>2013-11-29T03:14:12Z</updated><resolved>2013-11-29T03:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file></files><comments><comment>Should be limiting health request to concreteIndices.</comment></comments></commit></commits></item><item><title>index.routing.allocation not working as documented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4295</link><project id="" key="" /><description>I have 2 nodes and I created an index and trying to use routing allocation so that I can keep indexes on specific set of nodes. We need this for multitenancy and isolation. But it doesn't seem to work. I see my index is getting created on both the nodes.

I added to the yaml file for node .86. I only want this index to be stored on .86 node but it also is getting stored in .242 node. I have 2 nodes so far.

 node.group1: group2

Then I added this setting:

``` sh
curl -XPUT 10.80.140.86:9200/test4/_settings -d '{
 "index.routing.allocation.include.group1" : "group2",
 "index.routing.allocation.exclude.group1" : "group1"
}'
```

Head shows this for 2 nodes:

``` javascript
{ name: 10.80.140.242
transport_address: inet[/10.80.140.242:9300]
 version: 0.90.6
http_address: inet[/10.80.140.242:9200]
 attributes: {group1: group1
 }
} 
{ name: 10.80.140.86
transport_address: inet[/10.80.140.86:9300]
 version: 0.90.6
http_address: inet[/10.80.140.86:9200]
 attributes: {group1: group2
 }
} 


{ state: open
settings: { index.number_of_replicas: 1
index.number_of_shards: 5
 index.version.created: 900699
index.uuid: Bqd1gRznTl6U5IUFDevLqA
 index.routing.allocation.include.group1: group2
index.routing.allocation.exclude.group1: group1
 }
mappings: { }
aliases: [ ]
}
```
</description><key id="23470400">4295</key><summary>index.routing.allocation not working as documented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mohitanchlia</reporter><labels /><created>2013-11-28T23:46:14Z</created><updated>2014-03-14T08:31:17Z</updated><resolved>2014-03-13T16:16:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-29T10:30:36Z" id="29507888">What gives the following?

``` sh
curl -XGET "http://localhost:9200/test4/_status?pretty"
```
</comment><comment author="mohitanchlia" created="2013-11-30T02:24:02Z" id="29544226">{
   "ok":true,
   "_shards":{
      "total":10,
      "successful":8,
      "failed":0
   },
   "indices":{
      "test4":{
         "index":{
            "primary_size":"495b",
            "primary_size_in_bytes":495,
            "size":"732b",
            "size_in_bytes":732
         },
         "translog":{
            "operations":0
         },
         "docs":{
            "num_docs":0,
            "max_doc":0,
            "deleted_docs":0
         },
         "merges":{
            "current":0,
            "current_docs":0,
            "current_size":"0b",
            "current_size_in_bytes":0,
            "total":0,
            "total_time":"0s",
            "total_time_in_millis":0,
            "total_docs":0,
            "total_size":"0b",
            "total_size_in_bytes":0
         },
         "refresh":{
            "total":5,
            "total_time":"0s",
            "total_time_in_millis":0
         },
         "flush":{
            "total":3136,
            "total_time":"729ms",
            "total_time_in_millis":729
         },
         "shards":{
            "0":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":false,
                     "node":"uVJnmV0aRpyMH1kXQ02caQ",
                     "relocating_node":null,
                     "shard":0,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"79b",
                     "size_in_bytes":79
                  },
                  "translog":{
                     "id":1385070793627,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"57ms",
                     "total_time_in_millis":57
                  }
               },
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"kzSLZgACQOOskCjdt_H6EA",
                     "relocating_node":null,
                     "shard":0,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"99b",
                     "size_in_bytes":99
                  },
                  "translog":{
                     "id":1385070793627,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":1,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"97ms",
                     "total_time_in_millis":97
                  }
               }
            ],
            "1":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"uVJnmV0aRpyMH1kXQ02caQ",
                     "relocating_node":null,
                     "shard":1,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"99b",
                     "size_in_bytes":99
                  },
                  "translog":{
                     "id":1385070803635,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":1,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"98ms",
                     "total_time_in_millis":98
                  }
               },
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":false,
                     "node":"kzSLZgACQOOskCjdt_H6EA",
                     "relocating_node":null,
                     "shard":1,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"79b",
                     "size_in_bytes":79
                  },
                  "translog":{
                     "id":1385070803635,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"108ms",
                     "total_time_in_millis":108
                  }
               }
            ],
            "2":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"kzSLZgACQOOskCjdt_H6EA",
                     "relocating_node":null,
                     "shard":2,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"99b",
                     "size_in_bytes":99
                  },
                  "translog":{
                     "id":1385070793641,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":1,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"85ms",
                     "total_time_in_millis":85
                  }
               }
            ],
            "3":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"uVJnmV0aRpyMH1kXQ02caQ",
                     "relocating_node":null,
                     "shard":3,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"99b",
                     "size_in_bytes":99
                  },
                  "translog":{
                     "id":1385070803647,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":1,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"78ms",
                     "total_time_in_millis":78
                  }
               },
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":false,
                     "node":"kzSLZgACQOOskCjdt_H6EA",
                     "relocating_node":null,
                     "shard":3,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"79b",
                     "size_in_bytes":79
                  },
                  "translog":{
                     "id":1385070803647,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"102ms",
                     "total_time_in_millis":102
                  }
               }
            ],
            "4":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"kzSLZgACQOOskCjdt_H6EA",
                     "relocating_node":null,
                     "shard":4,
                     "index":"test4"
                  },
                  "state":"STARTED",
                  "index":{
                     "size":"99b",
                     "size_in_bytes":99
                  },
                  "translog":{
                     "id":1385070793656,
                     "operations":0
                  },
                  "docs":{
                     "num_docs":0,
                     "max_doc":0,
                     "deleted_docs":0
                  },
                  "merges":{
                     "current":0,
                     "current_docs":0,
                     "current_size":"0b",
                     "current_size_in_bytes":0,
                     "total":0,
                     "total_time":"0s",
                     "total_time_in_millis":0,
                     "total_docs":0,
                     "total_size":"0b",
                     "total_size_in_bytes":0
                  },
                  "refresh":{
                     "total":1,
                     "total_time":"0s",
                     "total_time_in_millis":0
                  },
                  "flush":{
                     "total":392,
                     "total_time":"104ms",
                     "total_time_in_millis":104
                  }
               }
            ]
         }
      }
   }
}
</comment><comment author="mohitanchlia" created="2013-12-02T17:53:43Z" id="29640711">Did you see anything glaring from the _status output?
</comment><comment author="dadoonet" created="2013-12-02T21:39:19Z" id="29660374">Really strange. Your index `test4` seems to have bad allocation.
2 replica shards are not allocated.

I extracted that here:

``` javascript
{
   "ok":true,
   "_shards":{
      "total":10,
      "successful":8,
      "failed":0
   },
   "indices":{
      "test4":{
         "shards":{
            "0":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":false,
                     "node":"uVJnmV0aRpyMH1kXQ02caQ"
                  }
               },
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"kzSLZgACQOOskCjdt_H6EA"
                  }
               }
            ],
            "1":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"uVJnmV0aRpyMH1kXQ02caQ"
                  }
               },
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":false,
                     "node":"kzSLZgACQOOskCjdt_H6EA"
                  }
               }
            ],
            "2":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"kzSLZgACQOOskCjdt_H6EA"
                  }
               }
            ],
            "3":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"uVJnmV0aRpyMH1kXQ02caQ"
                  }
               },
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":false,
                     "node":"kzSLZgACQOOskCjdt_H6EA"
                  }
               }
            ],
            "4":[
               {
                  "routing":{
                     "state":"STARTED",
                     "primary":true,
                     "node":"kzSLZgACQOOskCjdt_H6EA"
                  }
               }
            ]
         }
      }
   }
}
```

It seems that something goes wrong here. Could you create a new index from scratch, check that without any allocation it is well allocated on both nodes (run the same `_status` query) and only then reallocate the index?
</comment><comment author="mohitanchlia" created="2013-12-03T18:20:30Z" id="29736274">Not sure why this is happening? I create couple of indexes and all show the
same. It appears that there is a issue somewhere where if you try to
allocate replicas &gt; available nodes then it ends up in this situation. So
if I say replica of 2 and if I just have 1 node then it goes in this state.
If I have 2 nodes then it works fine.

On Mon, Dec 2, 2013 at 1:39 PM, David Pilato notifications@github.comwrote:

&gt; Really strange. Your index test4 seems to have bad allocation.
&gt; 2 replica shards are not allocated.
&gt; 
&gt; I extracted that here:
&gt; 
&gt; {
&gt;    "ok":true,
&gt;    "_shards":{
&gt;       "total":10,
&gt;       "successful":8,
&gt;       "failed":0
&gt;    },
&gt;    "indices":{
&gt;       "test4":{
&gt; 
&gt; ```
&gt;      "shards":{
&gt;         "0":[
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":false,
&gt;                  "node":"uVJnmV0aRpyMH1kXQ02caQ"
&gt;               }
&gt;            },
&gt; 
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":true,
&gt;                  "node":"kzSLZgACQOOskCjdt_H6EA"
&gt;               }
&gt;            }
&gt;         ],
&gt; 
&gt;         "1":[
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":true,
&gt;                  "node":"uVJnmV0aRpyMH1kXQ02caQ"
&gt;               }
&gt;            },
&gt; 
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":false,
&gt;                  "node":"kzSLZgACQOOskCjdt_H6EA"
&gt;               }
&gt;            }
&gt;         ],
&gt; 
&gt;         "2":[
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":true,
&gt;                  "node":"kzSLZgACQOOskCjdt_H6EA"
&gt;               }
&gt;            }
&gt;         ],
&gt; 
&gt;         "3":[
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":true,
&gt;                  "node":"uVJnmV0aRpyMH1kXQ02caQ"
&gt;               }
&gt;            },
&gt; 
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":false,
&gt;                  "node":"kzSLZgACQOOskCjdt_H6EA"
&gt;               }
&gt;            }
&gt;         ],
&gt; 
&gt;         "4":[
&gt;            {
&gt;               "routing":{
&gt;                  "state":"STARTED",
&gt;                  "primary":true,
&gt;                  "node":"kzSLZgACQOOskCjdt_H6EA"
&gt;               }
&gt;            }
&gt; 
&gt;         ]
&gt;      }
&gt;   }
&gt; ```
&gt; 
&gt;    }}
&gt; 
&gt; It seems that something goes wrong here. Could you create a new index from
&gt; scratch, check that without any allocation it is well allocated on both
&gt; nodes (run the same _status query) and only then reallocate the index?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4295#issuecomment-29660374
&gt; .
</comment><comment author="julienadam" created="2014-03-13T14:20:25Z" id="37537904">I'm having a similar issue. I have 3 nodes, tagged with a "zone" attribute A,B,C respectively. Then I created an index and set "index.routing.allocation.require.zone": "A". But then when I add data it gets spread on multiple nodes, including nodes not tagged with A at all.

However if I setup 2 nodes on zone A, 2 on zone B and 2 on zone C, when I add data, it gets spread only on zone A which is what I need. It means I have to have 2 nodes in each zone to do it however.

Basically what I would like is to store data produced in zone A and B locally, and have everything replicated to zone C. What would the best way to do that ?
</comment><comment author="clintongormley" created="2014-03-13T16:16:04Z" id="37552498">&gt; I'm having a similar issue. I have 3 nodes, tagged with a "zone" attribute A,B,C respectively. Then I created an index and set "index.routing.allocation.require.zone": "A". But then when I add data it gets spread on multiple nodes, including nodes not tagged with A at all.

This is not a bug. It tries to allocate to the tagged nodes, but it won't put replicas on the same node as primaries. Because you only have one node labelled `A`, it'll put some one set of shards on there, but then will use another node for the others.

&gt; Basically what I would like is to store data produced in zone A and B locally, and have everything replicated to zone C. What would the best way to do that ?

What you are looking for is awareness: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-cluster.html#allocation-awareness
</comment><comment author="julienadam" created="2014-03-14T08:31:17Z" id="37625825">Indeed, that makes a lot of sense. Though for beginners and for more clarity it would help if this was mentioned in the docs.

Thanks for your help !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Configurable minimum document count for bucket aggregators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4294</link><project id="" key="" /><description>Enabling configuring per bucket aggregation a threshold for the document counts on the buckets, so only buckets that contain more than `n` (defaults to `1`) documents will be returned.

When `n &gt; 1`, this may be only implementable in the reduce phase (because of the distributed nature of Elasticsearch), so there won't be any speed-up or slow-down compared to `n == 1`.

The `n = 0` case is interesting because it could be a generalization of the `"all_terms": true` setting of terms facets[1] and the `empty_bucets` of the `histogram` aggregation

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_all_terms
</description><key id="23465813">4294</key><summary>Configurable minimum document count for bucket aggregators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-11-28T20:12:16Z</created><updated>2014-01-22T12:01:28Z</updated><resolved>2014-01-22T12:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-22T12:01:28Z" id="33015459">Duplicate of #4662
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OOM when building with java6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4293</link><project id="" key="" /><description>The maven-compiler-plugin upgrade from 2.3.2 to 3.1 (see #4279) could cause out of memory issue when building the project with Maven and JDK6 and default memory settings (no `MAVEN_OPTS`).

This issue does not appear with JDK7.
</description><key id="23460953">4293</key><summary>OOM when building with java6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-28T17:21:14Z</created><updated>2013-11-28T17:24:46Z</updated><resolved>2013-11-28T17:24:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-28T17:24:46Z" id="29477137">Fix pushed in master: https://github.com/elasticsearch/elasticsearch/commit/fa762f09fae571e0477931bf2c2acc65cc9840cd

And in 0.90: https://github.com/elasticsearch/elasticsearch/commit/c9bed132ca0b75e5ecbf180a8851d2a4513323a8
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi_fields cannot contain objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4292</link><project id="" key="" /><description>When creating a mapping with a multi_field containing a field of type `object` the request returns OK, but there is an error in the ES logs, eg:

```
curl -XPUT "http://localhost:9200/test" -d'
{
   "mappings": {
      "profile": {
         "properties": {
            "foo": {
               "type": "multi_field",
               "fields": {
                  "foo": {
                     "type": "object",
                     "properties": {
                        "id": {
                           "type": "integer",
                           "index": "not_analyzed"
                        },
                        "date": {
                           "type": "date",
                           "index": "not_analyzed",
                           "format": "date_time_no_millis"
                        }
                     }
                  }
               }
            }
         }
      }
   }
}'
```

shows this in the logs:

```
[test] failed to add mapping [profile], source [{"profile":{"properties":{"foo":{"type":"multi_field","fields":{"foo":{"properties":{"date":{"type":"date","format":"date_time_no_millis"},"id":{"type":"integer"}}}}}}}}]
org.elasticsearch.index.mapper.MapperParsingException: No type specified for property [foo]
        at org.elasticsearch.index.mapper.multifield.MultiFieldMapper$TypeParser.parse(MultiFieldMapper.java:123)
        at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:263)
        at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parse(ObjectMapper.java:219)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:176)
        at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:314)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:193)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:417)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:381)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:414)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
```

We should throw an error instead of returning OK
</description><key id="23454581">4292</key><summary>Multi_fields cannot contain objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-11-28T15:09:23Z</created><updated>2014-08-08T18:50:56Z</updated><resolved>2014-08-08T18:50:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:50:56Z" id="51642834">Multi-fields have been removed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>has_child query with score_mode=avg can emit infinity as score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4291</link><project id="" key="" /><description /><key id="23444601">4291</key><summary>has_child query with score_mode=avg can emit infinity as score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-28T11:31:12Z</created><updated>2013-11-28T16:23:37Z</updated><resolved>2013-11-28T16:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file></files><comments><comment>Fixed positive infinity bug that can occur in specific scenarios when score mode average is used.</comment></comments></commit></commits></item><item><title>Add support for term filtering on terms aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4290</link><project id="" key="" /><description>Enable filtering the terms that should be aggregated. This should be based on `include`/`exclude` regexps:

```
{
    "terms" : {
        "field" : "tags",
        "include" : "[tag1 | tag2 | tag_*]",
        "exclude" : "[tag_10 | tag_2*]"
    }
}
```

The `exclude` takes precedence over the `include`, meaning, the `include` is evaluated first, and then the `exclude` will do the last filtering
</description><key id="23442529">4290</key><summary>Add support for term filtering on terms aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">uboness</reporter><labels><label>feature</label><label>v1.0.0.Beta2</label></labels><created>2013-11-28T10:46:13Z</created><updated>2013-11-28T10:51:34Z</updated><resolved>2013-11-28T10:51:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-11-28T10:51:34Z" id="29455461">Duplicate #4267 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify de-duplication and optimize/merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4289</link><project id="" key="" /><description>This contribution is based on the feedback given in issue 4254 and issue 4255 .
</description><key id="23438375">4289</key><summary>Clarify de-duplication and optimize/merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">mfn</reporter><labels /><created>2013-11-28T09:28:00Z</created><updated>2014-07-16T21:50:59Z</updated><resolved>2013-12-05T10:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-28T13:45:31Z" id="29464389">I think this looks good except of updates are reflected as additions but deletes are not removed immediately
</comment><comment author="spinscale" created="2013-12-05T10:14:28Z" id="29886024">Pushed in master/0.90, see https://github.com/elasticsearch/elasticsearch/commit/2da0611dfb44cbb13a395959e9283dc2843067bf. Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES java very high CPU usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4288</link><project id="" key="" /><description>I am having similar issues as described on this thread. https://github.com/elasticsearch/elasticsearch/issues/1940
Although leap second already happened in June but I tried resetting the time anyway. Didn't work. 
This is causing a lot of frustration. Any help is much appreciated. 

I have upgraded ES to the latest (elasticsearch-0.90.7-1). Have restarted ES, front end, logstash. no joy at all. 

java -version
java version "1.7.0_45"
OpenJDK Runtime Environment (rhel-2.4.3.2.el6_4-x86_64 u45-b15)
OpenJDK 64-Bit Server VM (build 24.45-b08, mixed mode)

After the upgrade I am seeing these in the logs, which seems like a totally different issues. 

[2013-11-28 03:48:32,796][WARN ][discovery.zen            ] [LA Logger] received a join                    request for an existing node [[Night Thrasher][ecqZvZhDSTGiVkrjj6G_hw][inet[/192.168.1                   28.146:9300]]{client=true, data=false}]
[2013-11-28 03:48:36,006][WARN ][discovery.zen            ] [LA Logger] received a join                    request for an existing node [[Night Thrasher][ecqZvZhDSTGiVkrjj6G_hw][inet[/192.168.1                   28.146:9300]]{client=true, data=false}]

Hot thread output: https://gist.github.com/ydnitin/7687098
</description><key id="23429444">4288</key><summary>ES java very high CPU usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ydnitin</reporter><labels /><created>2013-11-28T04:02:46Z</created><updated>2016-11-03T15:12:02Z</updated><resolved>2014-02-22T16:13:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-28T08:55:14Z" id="29447959">Hey there,

checking out your hot threads output, the last third of the output is the important part, as you can clearly see, that a lot of queries are eating up your CPU.

Do you run some complex queries including fuzzy support. Is your system acting different if you do not query it at all?

And last question to get a better overview: What is your setup? number of nodes, etc? Looks like the cluster state might not be up to date on that node, where the client is trying to rejoin (due to load).. but this is just a wild assumption, which is not backed by facts.
</comment><comment author="s1monw" created="2013-11-28T08:57:12Z" id="29448052">I agree with @spinscale this seems very much like you shooting some queries against elasticsearch and they cause load? Can you identify if that is abnormal?
</comment><comment author="ydnitin" created="2013-11-28T20:17:44Z" id="29483626">I suspected the same so I dropped all traffic to ES and the load came down right away and then enabled the traffic again load jumped up. My users query ES via kibana interface. I am not familiar with fuzzy query concept. ES is totally new for me. 
Load came down after I posted this last night. I guess user closed his kibana session? 
Is there a tool to look at the running queries, like pgmonitor?
Is there a simplified query syntax documentation out there? http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-queries.html this one contains too much information for my users. 

I am running just one node, two shards.
Server: Physical 
RAM: 24G
HeapSize set: 8G
CPU: 8

I am to add another node to ES. Any suggestions as to how I should profile my ES setup to order new hardware?
</comment><comment author="spinscale" created="2013-11-29T08:13:38Z" id="29501551">Hey,

there is a slow search log mechanism in elasticsearch. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-slowlog.html#search-slow-log

Furthermore you should talk to the kibana users and see what kind of queries they execute (almost every kibana widget can reveal the query being executed). Also make sure, that they are not reloading their data in kibana too often, maybe that already helps a bit (without getting to the cause of the problem, maybe it is just one query, you would have to try out the queries one by one).

There is no possibility to give any hardware recommendations without knowing data, configuration, mapping, documents, query load, index load etc... 

Adding more nodes and replicated the same data to them most likely will allow to at least spread the load.
</comment><comment author="s3k" created="2014-02-12T12:55:04Z" id="34865830">I have same problem with high cpu usage.
(mb pro, osx, standard java 7, 2 core, 2.5Ghz, i5)

Here some tips:
On my local machine i set in config/elasticsearch.yml

   index.number_of_shards: 1
   index.number_of_replicas: 0

For 1 index with 185k docs my cpu load is 2.5-5% for ES java process

Also plugins makes HUGE performance reduce. 
For me that is Marvel and analysis-morphology plugins. When i turned it off, that has given me additional cpu resources for 2 times, maybe 4.

I think that yml config is the solution.

P.S Also i have server (Ubuntu 12.04, openjdk 7, 2 xeon cores for 2.4Ghz and 2Gb ram) with default config, with plugins (marvel and analysis-morphology) which works fine for 1 500 000 docs on index.
</comment><comment author="davidgraca" created="2014-07-11T13:17:44Z" id="48728631">Don't forget that if you installed Marvel to delete all indexes after you removed, since it doesn't do that.
</comment><comment author="gentunian" created="2014-12-09T17:46:06Z" id="66324793">Same issue with marvel plugin. I kept getting out of heap space at a minute of starting (1GB heap). I removed the plugin and all indices and now cpu is fine, here a graph showing the improved cpu USER time:
![screenshot - 12092014 - 02 41 11 pm](https://cloud.githubusercontent.com/assets/1090113/5362471/06426490-7fb2-11e4-9647-19947aa2f99f.png)
</comment><comment author="bleskes" created="2014-12-10T00:44:54Z" id="66386503">@gentunian did you have the errors while the marvel UI was open in a browser, or also without. Would be great to get the out put of http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html#cluster-nodes-hot-threads to see what the node is doing. 
</comment><comment author="gentunian" created="2014-12-10T01:10:41Z" id="66388984">@bleskes on both cases I ran out of heap space. I could see in `elasticsearch.log` every 2 or 3 seconds what I assume was GC calls (I can revise the log tomorrow at work if you want to). 

`curl -s -XGET 'localhost:9200/_cat/thread_pool?v'` was all 0s for every column.

I had a lot of shards, maybe more than 500 (logstash defaults to 5 shards for every index and I have more than 100).

In the marvel stats, the marvel indices were on top of each column (index rate, search query, etc. Sorry if the names are wrong, I'm typing this from memory).

All this in QA. Tomorrow after finding out the CPU usage, I'm planning a production deployment.
</comment><comment author="bleskes" created="2014-12-11T14:21:24Z" id="66624784">@gentunian I can think of a couple of things that _may_ be going on. Maybe open a new issue with your details, as your primary concern is memory and not CPU?
</comment><comment author="ACV2" created="2015-05-19T18:15:28Z" id="103621012">I'm using elasticsearch 1.5 

and it is working perfectly the most part of the time, but everyday at the same time it becomes crazy, CPU % goes to ~70% when the average is around 3-5% there are SUPER servers with 32GB reserved for lucene, swap it is lock and clearing the cache doesn't solve the problem (it doesn't take down the heap mem) 

Settings: 

3 servers (nodes) 32 cores and 128GB RAM each 
2 buckets (indices) one with ~18 million documents (this one doesn't receive updates pretty often just indexing new docs) the other one have around 7-8 million documents but we are constantly bombarding it with updates search delete and indexing as well 

The best distribution for our structure, was to have only 1 shard per node with not replicas, we can afford to have a % of the data off for few seconds, that will be back as soon as the server get online again, and this process is fast enough since it doesn't need to relocate anything. previously we used to have 3 shards with 1 replica, but the issue mentioned above occurs as well, so is easy to figure it out that the problem is not related with the distribution. 

Things that I already tried, 

Merging, i try to use the Optimize API trying to give less load to the schedule merge, but actually the merging process takes a lot of R/W of the disk but it doesn't affect substantially the mem or the CPU load. 

Flushing, I tried to flush with long and shot intervals, and the results were the same nothing 
![pic1-1](https://cloud.githubusercontent.com/assets/6947506/7710304/c5b74bc6-fe2c-11e4-819f-fc5c46bcd742.PNG)
![pic2-1](https://cloud.githubusercontent.com/assets/6947506/7710313/d4861d12-fe2c-11e4-8577-06628fb7966a.PNG)
![pic2-2](https://cloud.githubusercontent.com/assets/6947506/7710314/d489d43e-fe2c-11e4-9de4-649a9ac93a56.PNG)
![pic1-2](https://cloud.githubusercontent.com/assets/6947506/7710315/d48cffce-fe2c-11e4-81ac-f199eb91405e.PNG)

changed, since flushing affects directly the merging process and as mentioned above, merging process doesn't takes that much of the CPU or mem usage. 

managing the cache, clearing it manually but it doesn't seems to take the cpu load to normal state not even for a moment. 

Here is the most of the elasticsearch.yml configs 

# Force all memory to be locked, forcing the JVM to never swap

bootstrap.mlockall: true

## Threadpool Settings

# Search pool

threadpool.search.type: fixed
threadpool.search.size: 20
threadpool.search.queue_size: 200

# Bulk pool

threadpool.bulk.type: fixed
threadpool.bulk.size: 60
threadpool.bulk.queue_size: 3000

# Index pool

threadpool.index.type: fixed
threadpool.index.size: 20
threadpool.index.queue_size: 1000

# Indices settings

indices.memory.index_buffer_size: 30%
indices.memory.min_shard_index_buffer_size: 12mb
indices.memory.min_index_buffer_size: 96mb

# Cache Sizes

indices.fielddata.cache.size: 30%
#indices.fielddata.cache.expire: 6h #will be depreciated &amp; Dev recomend not to use it
indices.cache.filter.size: 30%
#indices.cache.filter.expire: 6h #will be depreciated &amp; Dev recomend not to use it

# Indexing Settings for Writes

index.refresh_interval: 30s
#index.translog.flush_threshold_ops: 50000
#index.translog.flush_threshold_size: 1024mb
index.translog.flush_threshold_period: 5m
index.merge.scheduler.max_thread_count: 1

here is the stats when the server is in a normal state: 
node_stats_normal.txt

Node stats during the problem. 
node_stats.txt

I will appreciate any help or discussion that can point me in the right direction to get rid of this behavior 

thanks in advance.. 

Regards, 

Daniel
</comment><comment author="clintongormley" created="2015-05-25T12:34:02Z" id="105225543">Hi @ACV2 

This issue is closed.  I suggest you follow the advice given above (ie check the hot threads output, enable the slow query log), and see what happens at the time of day when your cluster goes wild.  If the answer isn't immediately obvious, I'd suggest asking about it in the forum: discuss.elastic.co
</comment><comment author="travisbell" created="2015-06-07T20:56:50Z" id="109799144">@AVC2 Did you learn anything since your post about this? We're seeing more or less, the same problem. Sometimes daily, sometimes only one a week (it's that random) one of our ES hosts decides to go crazy. We usually restart the instance when we see this because it affects our end users response times but it's starting to be a problem.

In our situation, these hosts aren't idle, they're trucking along with 25-30% CPU and then BAM, 70-100%. _Sometimes_ they correct themselves after 24-48 hours, other times they don't.

I was just curious is you learned anything that could help point us in the right direction. I'll see about checking the hot threads output and slowlogs.
</comment><comment author="ACV2" created="2015-06-12T11:08:53Z" id="111453868">Hey there, @Travis. Actually it was a journey... Elastic Search is everything but human friendly, after this post where I actually checked every single configuration trying to optimize the performance to avoid this behavior, with not success... I reduce the query time from 12 sec to 2-3 in a period of 10-15 minutes still huge and unacceptable, however we are working to optimize our back end to stabilize the performance, that being said.. My findings:

-merge process
-flush process
-background process (script) bombarding the server with queries slowing it down 

Can you post your server configurations so I can help you a little bit more :)

Regards
Daniel 

&gt; On 7/6/2015, at 16:27, Travis Bell notifications@github.com wrote:
&gt; 
&gt; @AVC2 Did you learn anything since your post about this? We're seeing more or less, the same problem. Sometimes daily, sometimes only one a week (it's that random) one of our ES hosts decides to go crazy. We usually restart the instance when we see this because it affects our end users response times but it's starting to be a problem.
&gt; 
&gt; I was just curious is you learned anything that could help point us in the right direction. I'll see about checking the hot threads output and slowlogs.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="travisbell" created="2015-06-24T15:32:22Z" id="114912502">Hi @ACV2 

Thanks for the reply. We noticed this was _usually_ tied to when we ran some big re-indexes, when the indexes were merging lots. During the re-indexes, we still have a full request load.

At the end of the re-index I added a task to run an optimize the index and low and behold, we haven't seen this problem crop up in the past ~12 days. Forcing an optimize to fix the runaway CPU problem doesn't feel like an actual fix but it does alleviate the issue for us. ES seems to get stuck in some kind of a weird state after these re-indexes and the optimize seems to kick it out of that state.
</comment><comment author="suminda123" created="2015-09-08T05:16:54Z" id="138436550">I had a same issue, high cpu and high memory, for me it was marvel creating a new big index everyday.
so I deleted all marvel indices and ran _flush, it cleared my memory and cpu came down fro 90% to 4%.
</comment><comment author="travisbell" created="2015-09-08T16:54:44Z" id="138632515">Yes, the daily optimize reduced our CPU in a similar amount (~60% ➞ ~10%). We still run our daily optimize task and haven't seen this issue since enabling it at the beginning of June.
</comment><comment author="Alino" created="2016-06-02T09:14:59Z" id="223238333">have someone tried to upgrade to ES 2.+ ? does it fix this cpu issue?
</comment><comment author="travisbell" created="2016-06-02T14:28:49Z" id="223309013">Hi @Alino, for us the problem was actually the fact we were using G1GC. As soon as we switched to CMS the problem went away (our ES cluster has been up for 51 days now!) What I mentioned in my last post ^ was still true, it greatly _reduced_ the times this happened but the runaway CPU problems continued to happen every ~7 days or so.

There's a long and interesting thread of some other users and I discussing this on the Elastic discussion board here: https://discuss.elastic.co/t/indexing-performance-degrading-over-time/40229
</comment><comment author="speedplane" created="2016-06-23T18:09:28Z" id="228134389">@travisbell Hi Travis, I've been seeing very similar patters. Everything runs perfectly for a full week, then all of a sudden we get really high CPU usage, and elsaticsearch is unresponsive. Most operations fail. Can you expand a bit on what you did to fix this issue? How do you switch from G1GC to CMS?

I have a feeling that this is caused by a node running out of java heap space. But I would imagine that would lead to a crash, not 100% CPU and I don't see OutOfMemory errors in the logs, so I'm not sure.
</comment><comment author="speedplane" created="2016-06-23T18:12:24Z" id="228135357">I see the following in the logs, right before I get 100% CPU, which is why I suspect it's an out of memory issue.

```
[2016-06-23 17:44:23,621][WARN ][monitor.jvm] 
[elasticsearch1-elasticsearch-5-vm] [gc][old][218473][270] 
duration [1.2m], collections [1]/[1.2m], total [1.2m]/[17.7m], 
memory [25.2gb]-&gt;[25.2gb]/[25.9gb], 
all_pools {[young] [196.5mb]-&gt;[141.6mb]/[532.5mb]}{[survivor] 
[66.5mb]-&gt;[0b]/[66.5mb]}{[old] [25gb]-&gt;[25gb]/[25.3gb]}
```

(formatted for easier reading)
</comment><comment author="jasontedor" created="2016-06-23T18:25:52Z" id="228140237">Not quite, but very close.

```
[...] duration [1.2m] [...] memory [25.2gb]-&gt;[25.2gb]/[25.9gb] [...]
```

This indicates a completely ineffective garbage collection cycle lasting for over a minute. The JVM is consuming CPU trying to collect when collecting is ineffective. You'll likely hit GC overhead limits soon at which point you will see `OutOfMemoryError`s.

It's best to open a new post on the [Elastic Discourse forum](https://discuss.elastic.co).
</comment><comment author="speedplane" created="2016-06-23T19:12:12Z" id="228154184">@jasontedor I've started [a discussion here](https://discuss.elastic.co/t/elasticsearch-high-cpu-usage-gc-not-working/53816?u=michael_sander). Any help on the matter would be appreciated.
</comment><comment author="magicalbanana" created="2016-11-03T15:12:02Z" id="258171259">@acv2 what tool are you using for the visual monitoring data?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.19.12: query containing OR &amp; AND ParseException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4287</link><project id="" key="" /><description>Hi everyone,

I need your thoughts on the following issue.
The following query failed due to the "OR" in caps, if I search for "or" it works.

```
{
  "from" : 0,
  "size" : 100,
  "query":{
      "bool":{
         "must":[
            {
               "dis_max":{
                  "tie_breaker":0.7,
                  "queries":[
                     {
                        "field":{
                           "name":"OR"
                        }
                     }
                  ]
               }
            }
         ]
      }
   }
}
```

I got the following error
    at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:211)
    at org.elasticsearch.index.query.FieldQueryParser.parse(FieldQueryParser.java:168)
    ... 19 more
Caused by: org.apache.lucene.queryParser.ParseException: Encountered " &lt;OR&gt; "OR "" at line 1, column 0.

The same thing happens with the AND.

is it an issue related to the old version of ES?

thanks for your help
</description><key id="23415032">4287</key><summary>0.19.12: query containing OR &amp; AND ParseException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dbaq</reporter><labels /><created>2013-11-27T21:03:41Z</created><updated>2013-11-28T08:41:27Z</updated><resolved>2013-11-28T08:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-28T08:41:12Z" id="29447295">Hey,

please ask questions like this on the mailinglist/google group, as we try to use github issues for bugs only.

1) Upgrade, please :-)
2) You are using the lucene query parser, where OR is a reserved iirc, thus it must be used like 'foo OR bar', see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-field-query.html - stating that this is a query_string query
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Question on quering nested items</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4286</link><project id="" key="" /><description>Hi I'm still learning ES's API and have a quick question.

I'm trying to figure if it's possible to search on subfields. Let's say the schema is that we index `users` who have many `posts` each.

Is it possible to query each post in `posts` of each user and also return the post instead of the user?

Thanks
</description><key id="23413571">4286</key><summary>Question on quering nested items</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Diolor</reporter><labels /><created>2013-11-27T20:32:45Z</created><updated>2013-11-27T21:58:28Z</updated><resolved>2013-11-27T21:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-27T21:26:45Z" id="29421477">Please use the mailing list for questions. Issues are only for issues, feature requests, pull requests...
Thanks!
</comment><comment author="Diolor" created="2013-11-27T21:58:28Z" id="29423656">@dadoonet  Thank you for the answer and sorry.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add allocate_all_primaries to cluster reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4285</link><project id="" key="" /><description>From the docs:
`allocate_all_primaries`::
    Allocate all unallocated primaries to any node that can take them.
    Accepts no parameters.  Each allocation is similar to running `allocate`
    with `allow_primary` so this can cause data loss.  This is useful in the
    same cases as `allocate` with `allow_primary` but doesn't require looking
    up the `index` or `shard` or guessing an appropriate `node`.

Closes #4206
</description><key id="23401509">4285</key><summary>Add allocate_all_primaries to cluster reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>adoptme</label><label>feature</label></labels><created>2013-11-27T17:21:10Z</created><updated>2016-03-08T13:03:25Z</updated><resolved>2016-03-08T13:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-11-27T17:25:18Z" id="29402947">I've confirmed this works using the local gateway:
1.  Start two nodes
2.  Execute:

``` bash
curl -XDELETE "http://localhost:9200/test?pretty" -s
curl -XPOST "http://localhost:9200/test?pretty" -s -d '{
  "settings": {
    "index": {
      "number_of_shards": 5,
      "number_of_replicas": 0
    }
  }
}'
for i in {1..100}; do 
  curl -XPOST "http://localhost:9200/test/test?pretty" -d '{"foo": "1"}' -s
done
```
1.  Shut down the node at localhost:9201.  Wait for a few seconds.
2.  Execute the below and notice the timeouts. ctrl-c it when you are bored.

``` bash
for i in {1..100}; do 
  curl -XPOST "http://localhost:9200/test/test?pretty" -d '{"foo": "1"}' -s
done
```
1.  Execute this:

``` bash
curl -XPOST 'localhost:9200/_cluster/reroute' -d '{
  "commands" : [
    {
      "allocate_all_primaries" : {}
    }
  ]
}'
```
1.  Now this will work without timeouts:

``` bash
for i in {1..100}; do 
  curl -XPOST "http://localhost:9200/test/test?pretty" -d '{"foo": "1"}' -s
done
```

The data is lost but at least you don't have timeouts.

Github's markup is making a mess of this....
</comment><comment author="kimchy" created="2013-12-07T19:24:19Z" id="30062591">It would be interested to check somehow if the primary allocation is just being throttled from being allocated to a node, and in which case, not force the allocation.... . This will require to shard knowledge somehow with `LocalGatewayAllocator` (in case of local gateway, somehow, we need to take into account the gateway abstraction, maybe have a method that will give the node for a primary shard, and then check the decider on it).
</comment><comment author="nik9000" created="2013-12-09T15:07:34Z" id="30138929">@kimchy, I understand what you are saying but I'm not sure how I'd go about it.  It does make me think of something else:  will this force allocation and ignore throttling?  Is that OK if we're allocating thousands of shards?

I can have a look at implementing what you mention sometime in the next few days.
</comment><comment author="kimchy" created="2013-12-09T15:41:36Z" id="30142019">@nik9000 this force allocation will not end up ignoring throttling, it will just come back to being allocated and respect throttling.
</comment><comment author="nik9000" created="2013-12-09T16:04:25Z" id="30144277">That, at least, is great news.  I can imagine folks in a disaster repeatedly trying this over and over again which won't help.  I'll make sure that it refuses to do anything if all the unallocated primaries are throttled.  I'll see about spitting out a different error message in that case so people know that all shards are in the process of being allocated. 
</comment><comment author="nik9000" created="2013-12-10T20:54:36Z" id="30266853">&gt; (in case of local gateway, somehow, we need to take into account the gateway abstraction, maybe have a method that will give the node for a primary shard, and then check the decider on it).

So I had a look at this and I'm not really sure how to do this because the decision about which node to assign the shard comes after allocation commands are run.  I wonder if it'd be simpler to store the list of throttled shards in the cluster state and dig it back out again during the allocation command....
</comment><comment author="kimchy" created="2013-12-10T20:56:08Z" id="30266993">To be honest, I don't have a good idea about how to do it yet as well :), I will try and spend some time thinking about it and provide feedback soonish (sorry!).
</comment><comment author="nik9000" created="2013-12-10T21:35:36Z" id="30270309">I thought I could get this from the AllocationExplanation on ClusterState but that always seems to be empty.  I actually can't find any code that sets it.
</comment><comment author="nik9000" created="2013-12-10T22:06:46Z" id="30273077">Pushed revised version - doesn't do what @kimchy wanted yet but is a bit nicer any way.
</comment><comment author="nik9000" created="2014-03-05T16:53:46Z" id="36764869">I haven't looked at this in a long while.  I imagine this would still be useful but don't have much time to think about it recently.  Any interest in me resurrecting this?
</comment><comment author="manologarciagarcia" created="2014-04-24T09:21:06Z" id="41259794">I have exactly this problem, I have just one shard and sometimes when I restart and look at the health of my cluster, I get this for one of my indexes:

http://pastebin.com/Tq08vep1

I know that if I delete the index, the problem will go away, but that's not the optimal solution.

Is there a solution for this problem? Are this changes here a solution for my problem?

Thanks
</comment><comment author="d1nsh" created="2014-06-05T00:12:08Z" id="45167955">Any plans of merging this? We run into issues with "unassigned shards" occasionally and it would be great to have a feature like this.
</comment><comment author="martijnvg" created="2014-08-08T11:13:18Z" id="51589115">@nik9000 Is this still on your radar? I think this new allocation command is useful.

Just thinking out loud here about how to detect if a node is throttling the primary shard allocation:
1) The LocalGatewayAllocator#buildShardStates() logic can be moved to a public helper class, on top of this there can be an additional method that just returns the DiscoveryNode that has the highest shard version.
2) Then in AllocateAllPrimariesAllocationCommand#execute() there can be somewhat of the following logic:

``` java
boolean found = false;
for (MutableShardRouting routing : allocation.routingNodes().unassigned()) {
    DiscoveryNode nodeHoldingHigestShardVersion = newHelper.findNodeWithHighestShardVersion();
    Decision decision = Decision.YES;
    if (nodeHoldingHigestShardVersion != null) {
        RoutingNode routingNode = allocation.routingNodes().node(nodeHoldingHigestShardVersion.id());
        decision = allocation.deciders().canAllocate(routing, routingNode, allocation);
    }
    if (decision.type() != Decision.Type.THROTTLE &amp;&amp; routing.primary()) {
        found = true;
        // Just clear the post allocation flag to the shard so it'll assign itself.
        allocation.routingNodes().addClearPostAllocationFlag(routing.shardId());
    }
}

if (!found) {
    throw new ElasticsearchIllegalArgumentException("[allocate_all_primaries] no unassigned primaries");
}
```

This way throttled primary allocation will not be affected by the new command.
</comment><comment author="nik9000" created="2014-09-04T22:59:17Z" id="54557477">This has sunk pretty low on my radar.  So low I haven't actually been checking the status and the ping must have slipped by me.  I can pick it up at some point but if you want it quickly maybe you can grab it?  If my code is a good starting point you can have it.  Or start over - I won't be offended - the pull request is really stale.
</comment><comment author="s1monw" created="2014-09-05T07:23:43Z" id="54593061">@nik9000 I labeled it accordingly such that it won't get forgotten and will be picked up at some point thanks for pinging again.
</comment><comment author="clintongormley" created="2014-10-10T09:48:09Z" id="58634559">The pain in allocating many primary shards is finding a place to put them, so suggestion:
- remove the `allow_primary` flag from allocation
- add an `allocate_primary` action where:
- `node` is optional - if not specified then it chooses the node automatically
</comment><comment author="martijnvg" created="2014-10-10T12:01:00Z" id="58646130">+1 This plan looks good.

On 10 October 2014 11:48, Clinton Gormley notifications@github.com wrote:

&gt; The pain in allocating many primary shards is finding a place to put them,
&gt; so suggestion:
&gt; - remove the allow_primary flag from allocation
&gt; - add an allocate_primary action where:
&gt; - node is optional - if not specified then it chooses the node
&gt;   automatically
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/4285#issuecomment-58634559
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="joestump" created="2014-10-10T15:35:56Z" id="58672890">@nik9000 please allow me to buy you a :beer: or :coffee: next time you're in Portland, OR. Great little improvement to ES right here. :+1: 
</comment><comment author="soundofjw" created="2015-07-21T20:39:54Z" id="123471058">+1 this would still be great ;)
</comment><comment author="damm" created="2015-07-21T21:32:21Z" id="123483700">+1 really needed.
</comment><comment author="clintongormley" created="2015-07-23T10:22:22Z" id="124048988">@soundofjw @damm what version of Elasticsearch are you using?  I asked our support team just a few days ago if they still think that this functionality would be useful.  Their response was that, with recent versions, the need for this has pretty much disappeared.
</comment><comment author="damm" created="2015-07-23T18:53:39Z" id="124210070">@clintongormley I'm using 1.7.0; I still have issues where I break out the bash scripts in this pull request. Single node recently; but a month ago on a cluster actually.  

Not common but it happens enough that I don't forget it.
</comment><comment author="soundofjw" created="2015-07-23T20:45:40Z" id="124235412">@clintongormley Pretty much same - 1.7.0 as well. There are few times that we need to do this, but it usually happens when setting up a cluster for the first time, or making big changes.
</comment><comment author="damm" created="2015-07-23T23:39:06Z" id="124269680">+1 to making big changes; I had to break this out when I had a cluster that was not allocating based on available space and it was making one node run out of space.

Had to re-route a bunch of data quickly while waiting for Elasticsearch to balance itself out once there was enough free space.
</comment><comment author="clintongormley" created="2015-07-27T10:55:42Z" id="125165055">@soundofjw why would you need this when setting up a cluster for the first time, or making big changes?  The only time you should need this is when you lose ALL copies of many shards (primaries and replicas) - and you want to force allocation of new empty shard copies.
</comment><comment author="ofir-petrushka" created="2015-07-27T12:27:36Z" id="125187471">@clintongormley It's like a bricked phone with no factory reset button. (no new index creation, no inserts, no fix button...)
When you put up a new cluster you might not have all the settings right yet, and might reset all nodes at once and/or have no replica set... (also data copy is delayed a lot by default and moves slow..)

For example installing the nodes with a deployment system (ex. chef, puppet, andsible..), you might deploy to all nodes at once since you don't care yet about down times etc. somehow it reaches such a state..

I had that multiply times doing a new cluster setup (redeploying nodes again and again) + after a few hours of work, not sure why.

It should just be a loop of existing commands...
</comment><comment author="soundofjw" created="2015-07-30T22:14:50Z" id="126507543">@clintongormley +1 to what @ofir-petrushka and @damm are saying.

One issue I've seen more than once is when the cluster resets state due to all masters resetting - and then data nodes come and recover shards which are no longer recognized.

You'll see a lot of "# of documents mismatch" in this case.
</comment><comment author="clintongormley" created="2015-08-05T10:41:57Z" id="127953040">@soundofjw 

&gt; One issue I've seen more than once is when the cluster resets state due to all masters resetting - and then data nodes come and recover shards which are no longer recognized.

This issue should be fixed in 2.0 with https://github.com/elastic/elasticsearch/pull/9952
</comment><comment author="soundofjw" created="2015-08-05T17:03:41Z" id="128071100">@clintongormley Awesome! That's great news :+1: 
</comment><comment author="damm" created="2015-11-28T08:11:44Z" id="160263971">@clintongormley just hit this with 2.1 :/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi-percolate should respect the rest.action.multi.allow_explicit_index setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4284</link><project id="" key="" /><description>See http://github.com/elasticsearch/elasticsearch/issues/3636
</description><key id="23397960">4284</key><summary>Multi-percolate should respect the rest.action.multi.allow_explicit_index setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-11-27T16:30:26Z</created><updated>2014-01-08T10:20:22Z</updated><resolved>2014-01-08T10:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file></files><comments><comment>Multi-percolate respects the `rest.action.multi.allow_explicit_index` setting</comment></comments></commit></commits></item><item><title>PR for all maven plugins update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4283</link><project id="" key="" /><description>I wrote a branch with all commits in it for an easier merge if it could help:

Related to #4274, #4275, #4276, #4277, #4278, #4279, #4280, #4281, #4282 
</description><key id="23394800">4283</key><summary>PR for all maven plugins update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-11-27T15:46:23Z</created><updated>2014-06-25T02:15:37Z</updated><resolved>2013-11-28T14:12:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-28T14:12:50Z" id="29465908">Merged in master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade RPM Maven Plugin to 2.1-alpha-3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4282</link><project id="" key="" /><description /><key id="23394642">4282</key><summary>Upgrade RPM Maven Plugin to 2.1-alpha-3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:43:37Z</created><updated>2014-06-14T12:47:33Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-27T15:46:59Z" id="29394447">@spinscale Could you please have a look at this one and test that there is no regression with RPM?
</comment><comment author="spinscale" created="2013-11-28T13:48:41Z" id="29464556">my vagrant test script built and started the RPM just fine, when using your branch

+1
</comment><comment author="dadoonet" created="2013-11-28T13:49:12Z" id="29464581">Thanks @spinscale! 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade RPM Maven Plugin to 2.1-alpha-3</comment></comments></commit></commits></item><item><title>Upgrade Maven Jar Plugin to 2.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4281</link><project id="" key="" /><description /><key id="23394618">4281</key><summary>Upgrade Maven Jar Plugin to 2.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:43:15Z</created><updated>2014-06-14T02:11:23Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Jar Plugin to 2.4</comment></comments></commit></commits></item><item><title>Upgrade Maven Resources Plugin to 2.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4280</link><project id="" key="" /><description /><key id="23394602">4280</key><summary>Upgrade Maven Resources Plugin to 2.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:42:58Z</created><updated>2014-06-14T09:32:49Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Resources Plugin to 2.6</comment></comments></commit></commits></item><item><title>Upgrade Maven Compiler Plugin to 3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4279</link><project id="" key="" /><description /><key id="23394585">4279</key><summary>Upgrade Maven Compiler Plugin to 3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:42:47Z</created><updated>2014-06-19T00:40:13Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>OOM when building with java6</comment></comments></commit><commit><files /><comments><comment>Upgrade Maven Compiler Plugin to 3.1</comment></comments></commit></commits></item><item><title>Upgrade Maven Assembly Plugin to 2.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4278</link><project id="" key="" /><description /><key id="23394571">4278</key><summary>Upgrade Maven Assembly Plugin to 2.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:42:35Z</created><updated>2014-06-19T11:19:29Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Assembly Plugin to 2.4</comment></comments></commit></commits></item><item><title>Upgrade Maven Eclipse Plugin to 2.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4277</link><project id="" key="" /><description /><key id="23394531">4277</key><summary>Upgrade Maven Eclipse Plugin to 2.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:41:59Z</created><updated>2014-06-14T12:29:14Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Eclipse Plugin to 2.9</comment></comments></commit></commits></item><item><title>Upgrade Maven Source Plugin to 2.2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4276</link><project id="" key="" /><description /><key id="23394517">4276</key><summary>Upgrade Maven Source Plugin to 2.2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:41:50Z</created><updated>2014-06-14T06:52:52Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Source Plugin to 2.2.1</comment></comments></commit></commits></item><item><title>Upgrade Maven Surefire Plugin to 2.16</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4275</link><project id="" key="" /><description /><key id="23394489">4275</key><summary>Upgrade Maven Surefire Plugin to 2.16</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:41:32Z</created><updated>2014-06-16T16:23:54Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Surefire Plugin to 2.16</comment></comments></commit></commits></item><item><title>Upgrade Maven Dependency Plugin to 2.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4274</link><project id="" key="" /><description /><key id="23394468">4274</key><summary>Upgrade Maven Dependency Plugin to 2.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T15:41:15Z</created><updated>2014-06-22T20:47:18Z</updated><resolved>2013-11-28T14:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade Maven Dependency Plugin to 2.8</comment></comments></commit></commits></item><item><title>Fetch / Count might fail if executed on a relocated shard.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4273</link><project id="" key="" /><description>When we relocate a shard we might still have pending SearchContext
instances hanging around that will be used in "in-flight" searches
on the already relocated shard. This is a valid operation but if
we have already closed the underlying directory which happens during
cleanup concurrently the close call on the IndexReader can trigger
an AlreadyClosedException when the NRT reader tries to cleanup files
via the IndexWriter. This kind of smells like a bug in Lucene, a close should never throw that exception IMO
</description><key id="23391700">4273</key><summary>Fetch / Count might fail if executed on a relocated shard.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T14:59:33Z</created><updated>2013-11-27T15:07:07Z</updated><resolved>2013-11-27T15:07:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-27T15:03:49Z" id="29390735">Here is an example exception showing the issue

```
[2013-11-27 13:58:18,425][DEBUG][action.search.type       ] [node_1] [21276] Failed to execute fetch phase
org.apache.lucene.store.AlreadyClosedException: this Directory is closed
    at org.apache.lucene.store.BaseDirectory.ensureOpen(BaseDirectory.java:66)
    at org.elasticsearch.index.store.Store$StoreDirectory.deleteFile(Store.java:370)
    at org.apache.lucene.index.IndexFileDeleter.deleteFile(IndexFileDeleter.java:584)
    at org.apache.lucene.index.IndexFileDeleter.deletePendingFiles(IndexFileDeleter.java:407)
    at org.apache.lucene.index.IndexWriter.deletePendingFiles(IndexWriter.java:4559)
    at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:371)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:231)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:111)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:253)
    at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearcher.release(RobinEngine.java:1559)
    at org.elasticsearch.test.engine.MockRobinEngine$AssertingSearcher.release(MockRobinEngine.java:128)
    at org.elasticsearch.search.internal.SearchContext.release(SearchContext.java:210)
    at org.elasticsearch.search.SearchService.freeContext(SearchService.java:514)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:426)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:662)
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/test/java/org/elasticsearch/search/basic/SearchWhileRelocatingTests.java</file></files><comments><comment>Catch AlreadClosedException in RobinSearcher#release()</comment></comments></commit></commits></item><item><title>Allow per index routing values in the same query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4272</link><project id="" key="" /><description>I have a single query that spans over a index that has per client private data that uses routing, as well as, a global index with shared data with no routing. 

I would like to combine this into a single query, but end up needing to set different routing values per index, eg the private index has the clientid routing value and the global index does not have a routing value. 

My current workaround is to simply run two queries and merge those results. This approach doesn't work great with relevancy since the scores are computed in isolation and also make pagination painful. 

Heres a brief discussion that might help if there is confusion:
https://groups.google.com/forum/#!topic/elasticsearch/FwsOAHz80MU

This would be a nice to have, but definitely, not a priority. 
</description><key id="23391655">4272</key><summary>Allow per index routing values in the same query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>discuss</label></labels><created>2013-11-27T14:58:45Z</created><updated>2014-10-17T09:15:26Z</updated><resolved>2014-10-17T09:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:50:32Z" id="51642792">Hi @ppearcy 

Given that you're hitting all shards anyway, how does it help using a routing value for part of the query?    The routing just determines to which shards the search request is sent.  If you're trying to limit the impact of the search, just filtering on the client ID (or whatever you use) would help there, as it would be a cached filter.  
</comment><comment author="ppearcy" created="2014-08-08T19:57:57Z" id="51650092">I am only hitting all shards on the global index, I want the per tenant one on the private. 

To re-summarize, I have two indexes that tenants submit documents to, a private one and a global one with two different query patterns, the private one is queried most of the time to a single tenant (ie, routing of tenantid + filter of tenantid) and the global is queried most of the time across all tenants.

I have a secondary query pattern, when I want to run a query to return all documents for a given tenant from both indexes, I want to use separate routing parameters per index to allow the query on the private index to span a single shard only while the global index doesn't use routing and just has the tenanantid filter applied across all shards. 
</comment><comment author="clintongormley" created="2014-08-09T09:21:30Z" id="51681886">OK -I'll reopen this for more discussion. 
</comment><comment author="jpountz" created="2014-09-05T10:59:54Z" id="54611468">Would aliases work in your case? An alias can wrap several indices with different per-index routing keys.
</comment><comment author="clintongormley" created="2014-09-06T17:52:46Z" id="54722482">++ @jpountz nice solution
</comment><comment author="ppearcy" created="2014-09-08T16:12:03Z" id="54844805">It's a good suggestion and viable workaround since it would address the key relevancy and paging concerns by providing a unified result. 

However, you would end up managing 1000s of aliases (one per routing value) and would either need to have a process to create these or do a lazy creation of them on search failures (or always confirm it exists before running the search). 

I still think per index routing settings on the query would be helpful, but won't fuss if this is closed out. 
</comment><comment author="clintongormley" created="2014-10-17T09:15:26Z" id="59487641">Hi @ppearcy 

We've had a long discussion about this, and all solutions other than the suggested one using aliases look really messy.  Given that this is a bit of a corner case, we're going to leave things as they are.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup of IndexFieldDataService.getForField.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4271</link><project id="" key="" /><description>This method has 2 signatures and one of them is dangerous since it allows to
discard fielddata configuration of the field mapper. This commit changes the
percolator so that it uses fielddata configuration of the _id field mapper
instead of forcing the paged_bytes format.
</description><key id="23385998">4271</key><summary>Cleanup of IndexFieldDataService.getForField.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-11-27T13:15:21Z</created><updated>2014-07-16T21:51:03Z</updated><resolved>2013-11-27T13:57:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Clean up calls to IndexFieldDataService.getForField</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4270</link><project id="" key="" /><description>This method has two signatures: one that takes a mapper and the other one that
takes parameters that are normally retrieved from the field mapper. This second
one is bad because it allows to potentially discard field data configuration of
the field mapper.

For example, the percolator discards the field mapper configuration and
retrieved field data by forcing the "paged_bytes" format even though the field might have been configured to use doc values.
</description><key id="23385655">4270</key><summary>Clean up calls to IndexFieldDataService.getForField</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T13:07:59Z</created><updated>2013-11-27T13:56:55Z</updated><resolved>2013-11-27T13:56:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/test/java/org/elasticsearch/benchmark/fielddata/LongFieldDataBenchmark.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTest.java</file></files><comments><comment>Cleanup of IndexFieldDataService.getForField.</comment></comments></commit></commits></item><item><title>Allow to get a specific transport client inside of tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4269</link><project id="" key="" /><description>Adding functionality to call cluster().transportClient() in tests in order
to get an arbitrary TransportClient object back, independently if the
transport client ratio in returning the normal clients is configured.
</description><key id="23384866">4269</key><summary>Allow to get a specific transport client inside of tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-27T12:50:15Z</created><updated>2014-07-16T21:51:04Z</updated><resolved>2013-12-09T15:55:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-09T14:31:08Z" id="30135899">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Made hole character in XAnalyzingSuggester part of Postingsformat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4268</link><project id="" key="" /><description>- Hole charactor now can change with new releases
- Fixed bug where the SEP_LABEL constant was used instead of the sepLabel instance variable
- Replaced if- with switch-statement
</description><key id="23383707">4268</key><summary>Made hole character in XAnalyzingSuggester part of Postingsformat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-27T12:24:08Z</created><updated>2014-07-16T21:51:04Z</updated><resolved>2013-11-28T13:36:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-27T16:40:57Z" id="29399200">LGTM push it
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms aggregator should support filtering terms by a regex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4267</link><project id="" key="" /><description>Enable filtering the terms that should be aggregated. This should be based on `include`/`exclude` regexps:

```
{
    "terms" : {
        "field" : "tags",
        "include" : "[tag1 | tag2 | tag_*]",
        "exclude" : "[tag_10 | tag_2*]"
    }
}
```

The `exclude` takes precedence over the `include`, meaning, the `include` is evaluated first, and then the `exclude` will do the last filtering
</description><key id="23382022">4267</key><summary>Terms aggregator should support filtering terms by a regex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T11:48:20Z</created><updated>2013-11-29T12:47:34Z</updated><resolved>2013-11-29T12:47:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-27T12:02:20Z" id="29379293">Ha thanks! I thought it was already the case as we discussed in #2109 
It's the same use case right?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/UnmappedTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/BucketPriorityQueue.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>- Added support for term filtering based on include/exclude regex on the terms agg</comment><comment>- Added javadoc to the TermsBuilder</comment></comments></commit></commits></item><item><title>Update to shade plugin 2.2 to shade test artifact as well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4266</link><project id="" key="" /><description>When we want to use test artifact in other projects, dependencies
are not shaded as for core artifact.

Issue opened in maven shade project: [MSHADE-158](http://jira.codehaus.org/browse/MSHADE-158)

**Don't merge this PR until shade plugin 2.2 is released!**

Vote in progress: http://markmail.org/message/pg565rgqhxwai56u. Released will probably happen on friday november, 29th.

When using it in other projects, you basically need to change your `pom.xml` file:

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
    &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
    &lt;type&gt;test-jar&lt;/type&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
```

You can also define some properties:

``` xml
&lt;properties&gt;
    &lt;tests.jvms&gt;1&lt;/tests.jvms&gt;
    &lt;tests.shuffle&gt;true&lt;/tests.shuffle&gt;
    &lt;tests.output&gt;onerror&lt;/tests.output&gt;
    &lt;tests.client.ratio&gt;&lt;/tests.client.ratio&gt;
    &lt;es.logger.level&gt;INFO&lt;/es.logger.level&gt;
&lt;/properties&gt;
```
</description><key id="23380918">4266</key><summary>Update to shade plugin 2.2 to shade test artifact as well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-27T11:22:56Z</created><updated>2014-06-13T03:09:43Z</updated><resolved>2013-11-28T10:01:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-28T09:11:31Z" id="29448774">it looks fine from looking at the changes. If this doesn't break our packageing which I think it doesn't I am +1. Can you check if the packages are still ok?
</comment><comment author="dadoonet" created="2013-11-28T10:02:51Z" id="29452453">@s1monw Tested and pushed. Should we backport it to 0.90 branch as well? I guess so.
</comment><comment author="s1monw" created="2013-11-28T10:59:04Z" id="29455890">+1 to backport
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove randomizedtesting-runner explicit dependency</comment><comment>Was added by mistake with #4266</comment></comments></commit><commit><files /><comments><comment>Update to shade plugin 2.2 to shade test artifact as well</comment></comments></commit></commits></item><item><title>BulkProcessor process every n+1 docs instead of n</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4265</link><project id="" key="" /><description>When you set a BulkProcessor with a bulk actions size of 100, it executes the bulk after 101 documents.

``` java
BulkProcessor.builder(client(), listener).setBulkActions(100).setConcurrentRequests(1).setName("foo").build();
```

Same for size. If you set the bulk size to 1024 bytes, it will actually execute the bulk after 1025 bytes.

This patch fix it.
</description><key id="23378971">4265</key><summary>BulkProcessor process every n+1 docs instead of n</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">EvanYellow</reporter><labels><label>:Java API</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-11-27T10:40:44Z</created><updated>2015-06-08T00:06:48Z</updated><resolved>2014-03-15T13:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-15T13:53:16Z" id="37726203">@EvanYellow Thanks! Fix applied as well to BulkSize and test added.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file></files><comments><comment>BulkProcessor process every n+1 docs instead of n</comment></comments></commit></commits></item><item><title>0.90.7 deleteByQuery shard failure when shards are relocating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4264</link><project id="" key="" /><description>May still be a failing case leftover from #3854.

I am seeing deleteByQuery failures occur on shards that are relocating.

&gt; Nov 27 02:00:38 esm1.global.search.sat.wordpress.com [2013-11-27 02:00:38,109][WARN ][cluster.action.shard     ] [esm1.global.search.sat.wordpress.com] [global-2-40m-50m][16] received shard failed for [global-2-40m-50m][16], node[pNG_x12bQVi0OMsGyICrrQ], [R], s[INITIALIZING], indexUUID [_na_], reason [Failed to perform [deleteByQuery/shard] on replica, message [RemoteTransportException[[es3.global.search.dfw.wordpress.com][inet[/192.0.82.43:9300]][deleteByQuery/shard/replica]]; nested: NullPointerException; ]]
&gt; Nov 27 02:00:38 es3.global.search.dfw.wordpress.com [2013-11-27 02:00:38,724][WARN ][action.deletebyquery     ] [es3.global.search.dfw.wordpress.com] Failed to perform deleteByQuery/shard on replica [global-2-40m-50m][0]
&gt; Nov 27 02:00:38 es3.global.search.dfw.wordpress.com org.elasticsearch.transport.RemoteTransportException: [es1.global.search.iad.wordpress.com][inet[/66.155.9.190:9300]][deleteByQuery/shard/replica]
&gt; Nov 27 02:00:38 es3.global.search.dfw.wordpress.com Caused by: java.lang.NullPointerException

Once the errors start the relocating shards go from a "RELOCATING" state to "INITIALIZING". One of those shards required me to restart the node in order to get the shard back to a working state.

The cluster is currently working on rebalancing itself, so I can't yet verify that the problem only ever occurs when relocating shards, but its very clear that:
- the errors occurred on shards that were relocating
- those shards are now also initializing rather than relocating
- No problems occur when deleteByQuery calls are disabled.
</description><key id="23363121">4264</key><summary>0.90.7 deleteByQuery shard failure when shards are relocating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels /><created>2013-11-27T02:16:44Z</created><updated>2013-11-27T16:00:12Z</updated><resolved>2013-11-27T15:59:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-27T08:19:01Z" id="29366872">hey,

can you attach the stack trace of the NPE from the es3 node here as well - or are we unlucky and the system is running that long, that is has optimized the stack trace away (not kidding, thats a JIT compilation feature)?

Thanks!
</comment><comment author="kimchy" created="2013-11-27T12:47:38Z" id="29381730">I believe we fixed this problem just recently in #4232.
</comment><comment author="pierrre" created="2013-11-27T14:48:16Z" id="29389437">@kimchy will you release 0.90.8 today? (my cluster is unstable)
</comment><comment author="kimchy" created="2013-11-27T14:50:28Z" id="29389610">@pierrre no, 0.90.8 doesn't have a concrete release date currently, potentially next week
</comment><comment author="pierrre" created="2013-11-27T14:53:25Z" id="29389857">@kimchy should I rollback to 0.90.6?
</comment><comment author="gibrown" created="2013-11-27T15:59:43Z" id="29395559">@kimchy yep, looks like the same bug, thanks.

@pierrre in my case I am avoiding this issue by disabling node rebalancing in the cluster:

&gt; curl -XPUT  "http://localhost:9200/_cluster/settings" -d '{ "transient" : { "cluster.routing.allocation.cluster_concurrent_rebalance" : 0 } }'

Not a good permanent solution since you need to be able to disable your deleteByQuery operations if you need any rebalancing of the cluster, but it seems to be avoiding the problem in my case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added aggregation support to the percolate api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4263</link><project id="" key="" /><description>Relates #4245
</description><key id="23351492">4263</key><summary>Added aggregation support to the percolate api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-26T22:03:36Z</created><updated>2015-05-18T23:33:34Z</updated><resolved>2013-11-28T20:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-28T20:23:10Z" id="29483817">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cat API: Add h parameter to apis, allowing to return columns and descriptions for them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4262</link><project id="" key="" /><description /><key id="23347961">4262</key><summary>Cat API: Add h parameter to apis, allowing to return columns and descriptions for them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.0.0.Beta2</label></labels><created>2013-11-26T21:13:44Z</created><updated>2013-11-26T21:14:21Z</updated><resolved>2013-11-26T21:14:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Table.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestTable.java</file></files><comments><comment>Cat API: Add h parameter to apis, allowing to return columns and descriptions for them</comment><comment>closes #4262</comment></comments></commit></commits></item><item><title>Add field data memory circuit breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4261</link><project id="" key="" /><description>This adds the field data circuit breaker, which is used to estimate
the amount of memory required to load field data before loading it. It
then raises a CircuitBreakingException if the limit is exceeded.

It is configured with two parameters:

`indices.fielddata.cache.breaker.limit` - the maximum number of bytes
of field data to be loaded before circuit breaking. Defaults to
`indices.fielddata.cache.size` if set, unbounded otherwise.

`indices.fielddata.cache.breaker.overhead` - a contast for all field
data estimations to be multiplied with before aggregation. Defaults to
1.03.

Both settings can be configured dynamically using the cluster update
settings API.
</description><key id="23345283">4261</key><summary>Add field data memory circuit breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label></labels><created>2013-11-26T20:29:21Z</created><updated>2016-10-25T23:08:08Z</updated><resolved>2014-01-02T22:08:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T20:39:24Z" id="29331709">Cool stuff @dakrone 
</comment><comment author="dakrone" created="2013-12-06T22:30:42Z" id="30035296">I realized that the FieldDataEstimator class is no longer needed, as estimations have been moved into their respective field data loading classes, so I'll remove it.
</comment><comment author="dakrone" created="2013-12-11T21:07:13Z" id="30364026">Updated code and force-pushed another squashed commit (because there were going to be merge conflicts regardless, and I'd rather rebase and deal with them now rather than after reviews).

Changes:
- Move all the files into better/more-applicable packages
- Breaker stats are now under the key `fielddata_breaker` and the class is called FieldDataBreakerStats
- Use constants instead of strings for reused field data filter settings
- TermsEnum is wrapped in a filter if BlockTreeStats can't be used (for people using custom postings formats)
- Fix "unwinding" of breaker in the event a different exception occurs while loading field data
- De-interface-ify MemoryAggregatingCircuitBreaker to become concrete MemoryCircuitBreaker
- Logger passed through to MemoryCircuitBreaker to preserve which area is using the breaker
- Remove "field data" from strings in MemoryCircuitBreaker to make it a bit more generic (reflecting the package move to `common.breaker`)

I may have forgotten other changes that went in, so more reviews welcome :)
</comment><comment author="dakrone" created="2013-12-19T19:38:47Z" id="30958993">Pushed a new version of the circuit breaker that addresses @imotov's comments.
</comment><comment author="s1monw" created="2014-01-02T21:13:52Z" id="31485147">LGTM please squash and push :+1: 
</comment><comment author="dakrone" created="2014-01-02T22:08:12Z" id="31489147">Merged in https://github.com/elasticsearch/elasticsearch/commit/a7542247516d594f612044f33b95db39d27c5393, closing #4592
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Integration with Apache Mesos for distributed resource management</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4260</link><project id="" key="" /><description>When running a cluster of elasticsearch nodes it's not always that each node is being fully used for indexing / searching. In addition, if you're wanting to optimize the resource usage of other components, hadoop cassandra on the same node a good resource manager would be great to have. 

The Apache Mesos framework allows for cluster management. Is this something on the roadmap to integrate elastic with this framework?
http://mesos.apache.org
</description><key id="23343199">4260</key><summary>Integration with Apache Mesos for distributed resource management</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hvandenb</reporter><labels /><created>2013-11-26T19:59:05Z</created><updated>2014-08-08T18:47:52Z</updated><resolved>2014-08-08T18:47:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:47:52Z" id="51642468">Hi @hvandenb 

No, there are no plans for anything like that.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>AllocationDeciders should be executed in order, starting at "cheap execution" and "most likely to return no"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4259</link><project id="" key="" /><description>The different AllocationDeciders are more or less expensive processing wise. They are also more or less likely to return a Decision.NO or Decision.THROTTLE. For large clusters this can result in a 10-15% speedup in recalculating the cluster state (tested with https://github.com/geidies/elasticsearch/blob/optimize_RoutingNodes/src/test/java/org/elasticsearch/cluster/routing/allocation/MassiveClusterRebalanceRoutingTests.java - 3.7 seconds on my test machine compared to 4.2 seconds without the optimization).

ConcurrentRebalanceAllocationDecider - loops over all ShardRoutings, O(n), optimization to O(1) for that in separate PR
DisableAllocationDecider - is O(1) 
ClusterRebalanceAllocationDecider - is O( #shards_unassigned ) + O( #shards ) or O( 1 ) + O( #shards ) - optimization for that in separate PR, making it O( 2 )
DiskThresholdDecider - O( 2 )
SnapshotInProgressAllocationDecider - O( 1 )
ReplicaAfterPrimaryActiveAllocationDecider and RebalanceOnlyWhenActiveAllocationDecider - O( #shards ) with current implementation of RoutingNodes.getShardsRoutingFor( MutableShardRouting ) - optimization in separate PR, making it O( #shards in replica set )
ShardsLimitAllocationDecider is O( # shards on node ) + O( 1 ).
AwarenessAllocationDecider is O( # shards in cluster ) \* # awareness attributes, making it the most expensive, but least likely to be turned on.
SameShardAllocationDecider is O( # shards on node ) \* # nodes on host
ThrottlingAllocationDecider, which is O( #shards_per_node ) + O( #shards_per_node )

In addition to the re-ordering, instead of applying all AllocationDeciders, skip the rest of one return a Decision.NO. This logic is ported from the Decision.Multi class.
</description><key id="23328491">4259</key><summary>AllocationDeciders should be executed in order, starting at "cheap execution" and "most likely to return no"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">geidies</reporter><labels /><created>2013-11-26T16:54:37Z</created><updated>2014-06-22T19:56:05Z</updated><resolved>2013-12-13T11:32:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-26T17:09:55Z" id="29311179">Quick note on multi allocation deciders:

the idea of the multi decision logic was to allow to easily debug which ones returned NO (and not just the first one). Though we can potentially only enable it when we build the feature of debugging the "trace" of how allocation decision is done.

In that case, I would not use the multi allocation decider at all now, and only use it when in the future we have the ability to enable trace logging of allocation.
</comment><comment author="geidies" created="2013-11-26T20:35:07Z" id="29331350">That's what I thought, that it was a maintainability vs. performance trade-off. Thanks for the insight!

P.S:. Here's a patch on top of my PR to enable trace logging of the Decisions.

```
diff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.j
index 13b2838..766bb25 100644
--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java
+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java
@@ -73,13 +73,16 @@ public class AllocationDeciders extends AllocationDecider {
         for (AllocationDecider allocationDecider : allocations) {
             Decision decision = allocationDecider.canRebalance(shardRouting, allocation);
             // short track if a NO is returned.
-            if ( decision == Decision.NO ) {
+            if ( !logger.isTraceEnabled() &amp;&amp; decision == Decision.NO ) {
                 return decision;
             }
             else if ( decision != Decision.ALWAYS ) {
-                ret.add( decision );
+                ret.add( decision, allocationDecider.getClass() );
             }
         }
+        if ( logger.isTraceEnabled() ) {
+            logger.trace( "canRebalance decision: " + ret.toString() );
+        }
         return ret;
     }

@@ -92,15 +95,18 @@ public class AllocationDeciders extends AllocationDecider {
         for (AllocationDecider allocationDecider : allocations) {
             Decision decision = allocationDecider.canAllocate(shardRouting, node, allocation);
             // short track if a NO is returned.
-            if ( decision == Decision.NO ) {
+            if ( !logger.isTraceEnabled() &amp;&amp; decision == Decision.NO ) {
                 return decision;
             }
             // the assumption is that a decider that returns the static instance Decision#ALWAYS
             // does not really implements canAllocate
             else if (decision != Decision.ALWAYS) {
-                ret.add(decision);
+                ret.add( decision, allocationDecider.getClass() );
             }
         }
+        if ( logger.isTraceEnabled() ) {
+            logger.trace( "canAllocate decision: " + ret.toString() );
+        }
         return ret;
     }

@@ -113,13 +119,16 @@ public class AllocationDeciders extends AllocationDecider {
         for (AllocationDecider allocationDecider : allocations) {
             Decision decision = allocationDecider.canRemain(shardRouting, node, allocation);
             // short track if a NO is returned.
-            if ( decision == Decision.NO ) {
+            if ( !logger.isTraceEnabled() &amp;&amp; decision == Decision.NO ) {
                 return decision;
             }
             else if (decision != Decision.ALWAYS) {
-                ret.add(decision);
+                ret.add( decision, allocationDecider.getClass() );
             }
         }
+        if ( logger.isTraceEnabled() ) {
+            logger.trace( "canRemain decision: " + ret.toString() );
+        }
         return ret;
     }
 }
diff --git a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java
index 406ac95..7cbaaa0 100644
--- a/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java
+++ b/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java
@@ -113,17 +113,23 @@ public abstract class Decision {
     public static class Multi extends Decision {

         private final List&lt;Decision&gt; decisions = Lists.newArrayList();
+        private final List&lt;Class&gt;    deciders  = Lists.newArrayList();

         /**
          * Add a decission to this {@link Multi}decision instance
          * @param decision {@link Decision} to add
          * @return {@link Multi}decision instance with the given decision added
          */
-        public Multi add(Decision decision) {
+        public Multi add(Decision decision, Class&lt;? extends AllocationDecider&gt; klass ) {
             decisions.add(decision);
+            deciders.add( klass );
             return this;
         }

+        public Multi add( Decision decision ) {
+            return this.add( decision, AllocationDecider.class );
+        }
+
         @Override
         public Type type() {
             Type ret = Type.YES;
@@ -141,8 +147,11 @@ public abstract class Decision {
         @Override
         public String toString() {
             StringBuilder sb = new StringBuilder();
-            for (Decision decision : decisions) {
-                sb.append("[").append(decision.toString()).append("]");
+            for ( int i = 0; i &lt; decisions.size(); i++ ) {
+                String klassName = deciders.get(i).getName();
+                sb.append("[").append(decisions.get(i).toString())
+                  .append("(").append( klassName.substring( klassName.lastIndexOf("." ) + 1 ) ).append(")")
+                  .append("]");
             }
             return sb.toString();
         }
```
</comment><comment author="s1monw" created="2013-12-06T09:49:27Z" id="29975486">The commit here looks pretty close. Yet, sometimes the output of the Multi decision is useful. can we maybe have a realtime setting that triggers this behavior. something like `cluster.routing.allocation.decision.verbose=true|false` I think it should be disabled by default and always on if the logger has trace enabled? @kimchy what do you think?
</comment><comment author="kimchy" created="2013-12-06T10:32:36Z" id="29978392">regarding the Multi decision, I am ok with doing it in another change, we need to infrastructure to report on it (view reroute potentially) on the decisions made anyhow before this becomes useful.
</comment><comment author="s1monw" created="2013-12-08T19:29:30Z" id="30089947">@geidies can you fix the code style violations here and the stuff I commented about. I think other than that it's ready. We will fix the logging case in a different issue.
</comment><comment author="geidies" created="2013-12-09T21:47:49Z" id="30177036">rebased on top of master.
</comment><comment author="s1monw" created="2013-12-13T11:32:01Z" id="30503246">pushed!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixing CompletionFieldMapper.isReservedChar() to take all relevant chars in account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4258</link><project id="" key="" /><description>This fixes a randomized occuring test where the lookup key contained a unit separator character U+001F
</description><key id="23328149">4258</key><summary>Fixing CompletionFieldMapper.isReservedChar() to take all relevant chars in account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-26T16:50:32Z</created><updated>2014-07-16T21:51:06Z</updated><resolved>2013-11-26T19:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T19:44:41Z" id="29325790">pushed! thanks!
</comment><comment author="s1monw" created="2013-11-26T19:45:07Z" id="29325862">for the reference: https://github.com/elasticsearch/elasticsearch/commit/2a456e571646344ffac4c7f8394562fc0988421e
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shouldn't be necessary to loop over ShardRoutings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4257</link><project id="" key="" /><description>As several other classes can change the internal state of the RoutingNodes data structure, inefficient looping over nodes and assigned shards was necessary in the AllocationDeciders.

With larger clusters, reallocation gets too slow. In our current case, we have 5 years of data with daily indices, 6 shards per index, replication factor 1. Recalculating cluster state can take minutes, with the master sitting at 100% CPU in RoutingNodes.shardsRoutingFor( MutableShardRouting ).

The taken approach is
a) making RoutingNodes a singleton, since only one active instance should ever exist anyhow,
b) notifying RoutingNodes of changes in MutableShardRouting instances state.

This certainly is not the most elegant approach and adds complexity instead of removing it, but is what can be done without a major refactoring of allocation.

In the supplied test case execution of the final reallocation is sped up from 22 seconds on my test machine to 4.2 seconds.
</description><key id="23326174">4257</key><summary>Shouldn't be necessary to loop over ShardRoutings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">geidies</reporter><labels /><created>2013-11-26T16:24:43Z</created><updated>2014-07-05T11:26:21Z</updated><resolved>2013-12-16T11:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T20:13:32Z" id="29329495">hey thanks for opening this pullrequest. I think we should not allow calling back into `RoutingNodes` but rather allow for a new statistics class or so that we can pass along with the other information down the `AllocationDecider` path. I think we should make this somehow private to the `ShardAllocator` and it's alg to update the statistics. I can think of adding something like a `ShardRoutingMutator` that has methods like:

```
ShardRoutingMutator#deassignNode(MutableShardRouting routing, ShardRoutingStats)
```

that is used instead of calling `MutableShardRouting#deassignNode()` that way we can keep the code clean?  Does this sound like a good first iteration? What do you think?
</comment><comment author="geidies" created="2013-11-26T21:13:03Z" id="29334408">Sounds definitely better. What worried me most when looking into this is the ShardsAllocators modifying the unassigned Array of RoutingNodes directly. So I thought along the lines of a ShardRoutingManager (or ShardRoutingMutator) to be the only one allowed to change MutableShardRoutings and the internal state of RoutingNodes, keeping statistics at the same time to be used by the AllocationDeciders. 

So this ShardRoutingMutator should keep track of the counts which are used by the AllocationDeciders - I think the RoutingNodes itself keeping a Map ShardId-&gt; replica sets is a good idea, though; maybe RoutingNodes should be renamed though, then, to reflect it keeps track of more than RoutingNode's. 

As I said, this is getting complex pretty quickly if not done as a dirty hack. However, the speed improvement and along that the scalability gain is probably worth it.
</comment><comment author="geidies" created="2013-11-27T03:19:16Z" id="29357293">I ended up calling the class managing routing `RoutingManager`.

Looks better?
</comment><comment author="s1monw" created="2013-11-29T11:59:15Z" id="29512273">I had a quick look and I think we are going into the right direction. I wonder if we can merge RoutingManager into RoutingNodes? This would make things simpler? I also still see some `RountingNodes.getInstance(...)` can you remove them?
</comment><comment author="geidies" created="2013-11-29T14:42:59Z" id="29520041">I'm not sure whether adding the `RoutingManager` functionality to `RoutingNodes` would make things simpler. It certainly would make the code less readable. And since conceptually, the `RoutingManager` manages both RoutingNodes (book-keeping) and ShardRoutings (state changes), I think it is a cleaner approach to keep it its own class.

The last commit removes all necessity to have `RoutingNodes` be a singleton. This means that not only `MutableShardRouting` state is only managed through the `RoutingManager`, but also the `RoutingNode` state may no longer be changed by the ShardsAllocators or a command.
</comment><comment author="kimchy" created="2013-11-29T15:05:46Z" id="29521344">I agree with @s1monw  that with how it looks today, its cleaner to have this functionality in RoutingNodes, otherwise, why are there retrieval methods (shard...For) in RoutingManager, and not only mutator ones.

I like where this is headed, making sure that all state mutations happen through a single place will mean that we can more easily introduce caching data structures where we can make sure they remain consistent with state changes and introduce the relevant speedups.

The test included in this change is not really a unit/integration test? Its really a perf/bench test. We can move it to the bench/perf package we have, but it shouldn't remain a test.

Few untreated notes, but I want to make sure we won't loose this:
- Once this is done, it would be nice, in future changes, to consolidate state mutations on unassigned lists, and nodes removals / additions.
- I would love to see the performance test also for awareness deciders. They can potentially benefits from some perf love as well.
</comment><comment author="geidies" created="2013-11-29T15:32:05Z" id="29522798">Either way, with `RoutingManager` or its functionality integrated in `RoutingNodes`, we achieve better performance for cluster state recalculation, so I'm good with either approach :)

For completeness: the retrieval method `#replicaSetFor(MutableShardRouting` returns an unmodifiable list, to encourage working with smaller data sets, but not having them modified outside. @kimchy you're completely right this belongs into RoutingNodes really.

The test indeed should be moved - or rather re-done in the right package by someone who knows more about testing than I.

The `unassigned` array I think, too, we should treat in a separate issue.

What do you guys think - for readability make `RoutingManager` a sub-class of `RoutingNodes`, and add its public methods to `RoutingNodes` itself?
</comment><comment author="s1monw" created="2013-11-29T21:08:53Z" id="29537692">bq. What do you guys think - for readability make RoutingManager a sub-class of RoutingNodes, and add its public methods to RoutingNodes itself?

I think it should be in `RoutingNodes` keep the change simple we can refactor later!
</comment><comment author="geidies" created="2013-11-29T21:21:22Z" id="29538070">Making it at subclass (see 8665cf5) to `RoutingNodes` was easiest, and looks pretty clean to me. Let me know if we can review that.
</comment><comment author="s1monw" created="2013-11-29T21:33:27Z" id="29538420">oh you mean an `inner class` ok yeah that makes more sense. I will review it over the weekend or early next week.
</comment><comment author="geidies" created="2013-11-29T21:37:43Z" id="29538535">Oh, indeed, I meant inner class. Sorry for the confusion!
</comment><comment author="geidies" created="2013-12-05T16:35:53Z" id="29913267">To add some information, the current performance characteristics of a recalculation of the cluster state seem to be:

```
O( 
  (# indices) * (
    (# shards)  // ConcurrentRebalanceAllocationDecider
    + ( (# unassigned shards ) + (# shards) ) // ClusterRebalanceAllocationDecider
    + ( 2 ) // DiskThresholdDecider
    + ( 1 ) // SnapshotInProgressAllocationDecider
    + (# shards) // ReplicaAfterPrimaryActiveAllocationDecider
    + (# shards) // RebalanceOnlyWhenActiveAllocationDecider
    + (# shards per node) // ShardsLimitAllocationDecider
    + ((# shards) * (# awareness attributes)) // AwarenessAllocationDecider
    + ((# shards per node) * (# nodes on host)) // SameShardAllocationDecider
    + ((# shards per node) * (#shards per node)) // ThrottlingAllocationDecider
  )
)
```

With this patch and #4259 this can be reduced to mostly `O( # shards )`, which makes maintaining large(r) clusters easier. Would be awesome if someone can take a further look at it.
</comment><comment author="geidies" created="2013-12-06T15:46:41Z" id="30003862">@s1monw, I hope I have removed all foo coding style I introduced. `RoutingManager` is no longer there, folded into `RoutingNodes`
</comment><comment author="s1monw" created="2013-12-08T19:44:06Z" id="30090698">I think we are super close here. I left some more comments but I think this looks great in general! Let fix these last cleanups and then get it in!
</comment><comment author="s1monw" created="2013-12-09T16:08:01Z" id="30144628">hey can you squash all your commits into one and rebase with master so I can merge it in?
</comment><comment author="geidies" created="2013-12-09T18:09:15Z" id="30156274">@s1monw - thanks very much for your guidance and help!
</comment><comment author="s1monw" created="2013-12-16T11:12:31Z" id="30652880">pushed to master and 0.90 via https://github.com/elasticsearch/elasticsearch/commit/8d321530de539bfa3e09d10e4fd15d3ab05b0c05  

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not start packages on installation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4256</link><project id="" key="" /><description>The reason to not start packages on installation is to allow to configure
them before starting up (setting heap, cluster.name etc)

Also the documentation was updated in order to show, which statements need
to be executed to start up elasticsearch and configure it to fire up automatically on boot.
In addition, these statements are also printed out when the package is
installed, depending on whether chkconfig, system or update-rc.d is used.

Closes #3722
</description><key id="23325003">4256</key><summary>Do not start packages on installation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-26T16:09:59Z</created><updated>2014-06-25T21:29:57Z</updated><resolved>2014-01-03T16:41:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-11-27T10:16:50Z" id="29373362">I will definitely walk though this with someone who is more into RHEL/Fedora spec files on our side than I am. Thanks a lot!
</comment><comment author="drewr" created="2014-01-02T16:59:37Z" id="31466224">:+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion only retrieves one result when multiple documents share same output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4255</link><project id="" key="" /><description>When I create multiple documents which have the same output value in the completion field, a suggest completion request only retrieves one object.
I guess it is a feature, however, once we may set different payloads to those documents, it would make sense to retrieve multiple suggestions with the same output.

My environment settings:
ElasticSearch version number: `1.0.0.Beta1`
Lucene version: `4.5.1`

In the following example, I create an index with two documents. Each document has a different value in the payload, but the same output and input.
Performing a suggestion completion request, only one document is retrieved.

Scripts:
## Create and populate index

``` sh
curl -XDELETE 'localhost:9200/notebookindex'

curl -XPUT localhost:9200/notebookindex

curl -XPUT localhost:9200/notebookindex/friend/_mapping -d '{
  "friend" : {
        "properties" : {
            "name" : { "type" : "string" },
            "suggestField" : { "type" : "completion", "payloads" : true }
        }
    }
}'

curl -XPUT 'localhost:9200/notebookindex/friend/1' -d ' {
  "name": "james smith",
  "suggestField": {
    "input": ["james", "smith", "james smith"],
    "output": "james smith",
    "payload": {"id": "1", "phone": "555-55555"}
  }
}'

curl -XPUT 'localhost:9200/notebookindex/friend/2' -d '{
  "name": "james smith",
  "suggestField": {
    "input": ["james", "smith", "james smith"],
    "output": "james smith",
    "payload": {"id": "2", "phone": "444-44444"}
  }
}'
```
## Search 1: look for friends starting with `j`

``` sh
curl -XPOST 'localhost:9200/notebookindex/_suggest?pretty' -d '{
  "my-friends-suggest": {
    "text": "j",
    "completion": {
      "field": "suggestField"
    }
  }
}'
```

Only one `James` is found, even though there are two documents matching the suggestion.

``` JSON
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "my-friends-suggest" : [ {
    "text" : "j",
    "offset" : 0,
    "length" : 1,
    "options" : [ {
      "text" : "james smith",
      "score" : 1.0, "payload" : {"id":"2","phone":"444-44444"}
    } ]
  } ]
}
```
## Removal and Search 2: remove the `James` (`id 2`) previously found and look for friends starting with `j`

``` sh
curl -XDELETE 'localhost:9200/notebookindex/friend/2'

curl -XPOST 'localhost:9200/notebookindex/_suggest?pretty' -d '{
  "my-friends-suggest": {
    "text": "j",
    "completion": {
      "field": "suggestField"
    }
  }
}'
```

The other `James` is found.

``` JSON
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "my-friends-suggest" : [ {
    "text" : "j",
    "offset" : 0,
    "length" : 1,
    "options" : [ {
      "text" : "james smith",
      "score" : 1.0, "payload" : {"id":"1","phone":"555-55555"}
    } ]
  } ]
}
```
</description><key id="23316469">4255</key><summary>Completion only retrieves one result when multiple documents share same output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">danilomr</reporter><labels><label>enhancement</label></labels><created>2013-11-26T14:16:37Z</created><updated>2014-12-11T20:38:10Z</updated><resolved>2014-12-11T20:38:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T15:47:19Z" id="29303017">I am feeling bad to close these issues but this is expected and rather a feature than a bug. We de-duplicate under the hood and only return one if the score is the same as well as the output. Thanks for writing this up.
</comment><comment author="mfn" created="2013-11-26T17:32:29Z" id="29313242">Is there any chance to revisit this behavior? Just because the output is the same doesn't mean it's a different "thing", e.g. the payload could be different.

A workaround would probably be to just add "something" unique to the output and put the actual desired output as part of the payload I guess.

thanks
</comment><comment author="facundoolano" created="2014-01-22T13:54:28Z" id="33023645">I think the de-duplication should be optional, there are times when you can't construct the differentiated output at index time, and rather you have to do it based on the user input.
</comment><comment author="s1monw" created="2014-01-23T12:39:00Z" id="33120265">I took a look at this yesterday and i think I can make this work. There is still some work left since I think I ran into a bug in lucene that I wanted to fix ages ago but haven't yet... not sure when I will be able to get this done but there is hope!
</comment><comment author="clintongormley" created="2014-07-11T10:08:23Z" id="48714262">@areek assigning this to you
</comment><comment author="petard" created="2014-08-12T10:01:27Z" id="51895170">Is de-duplication now optional? In the docs it says "The result is de-duplicated if several documents have the same output, i.e. only one is returned as part of the suggest result. This is optional." but it doesn't say how to turn it off actually.
</comment><comment author="areek" created="2014-08-12T15:08:16Z" id="51927279">No de-dup is not yet optional. I can see how the doc can be confusing on this (will fix), its the output that is optional.

There has been some progress made related to this issue (check out #7133). After this issue is committed (should be very soon), then same outputs will be appropriately stored and hence it will be easier to support optional de-duplication in completion &amp; context suggesters.
</comment><comment author="smithatlanta" created="2014-08-27T13:10:38Z" id="53569525">I'm running into the same issue.  I was using this to do quick searches for movie titles and was scratching my head when we only received one document back for "Shaft" and "Titanic".  I guess I'll have to go back to using match.  It's great that you will have the de-dup option pretty soon.
</comment><comment author="gacarrillor" created="2014-09-05T21:50:17Z" id="54686108">I'm another ES user eager to use the de-duplication option. As @mfn mentioned, the payload is sufficient to disambiguate suggestions, like here (place names with their corresponding place types): 

![completion_suggester_same_output](https://cloud.githubusercontent.com/assets/652785/4172108/8cdfc7d0-3544-11e4-8349-f973e16cc63b.png)
</comment><comment author="kgujral" created="2014-10-15T06:29:07Z" id="59163237">+1
Please make this feature optional and differentiate on the basis of both output and payload.
Thanks
</comment><comment author="areek" created="2014-12-11T20:38:10Z" id="66684508">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Completion suggest: Clarify de-duplication, optimize/merge</comment></comments></commit></commits></item><item><title>Completion field is not removed from the completion-suggester index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4254</link><project id="" key="" /><description>Hello to all!

I have a problem with documents holding completion suggestion fields: at some point, a document is removed, but its completion field remains.
This problem doesn't happen to all document inserted in the index, it only happened to me when I created documents with high ids.

My environment settings:
ElasticSearch version number: `1.0.0.Beta1`
Lucene version: `4.5.1`

In the following example, I create an index and insert two documents. One with a high id (`id 3567245`) and other with a low id (`id 103`). When I remove the 
document with low id, its completion suggestion field still remains.

Note: if I change the high id from `3567245` to, for example, `700`. The problem reported doesn't happen.
## Create index, populate and remove one of the documents

``` sh
curl -XDELETE 'localhost:9200/myindex'

curl -X PUT localhost:9200/myindex

curl -X PUT localhost:9200/myindex/mytype/_mapping -d '{
  "mytype": {
      "properties": {
        "name": {
          "type": "string"
        },
        "suggest-mytype": {
          "type": "completion",
          "payloads": true
        }
      }
    }
}'

curl -XPOST 'localhost:9200/_bulk' -d '
{"index":{"_index":"myindex","_type":"mytype","_id":3567245}}
{"name":"Johnny","suggest-mytype":{"input":["Johnny"],"output":"Johnny","payload":{"_id":3567245}}}
{"index":{"_index":"myindex","_type":"mytype","_id":103}}
{"name":"Anderson","suggest-mytype":{"input":["Anderson"],"output":"Anderson","payload":{"_id":103}}}
'

curl -XDELETE 'localhost:9200/myindex/_query' -d '
{"bool":{"should":[{"term":{"_id":{"value":103}}}]}}
'
```
## search 1: show everything from the index

``` sh
curl -XPOST 'localhost:9200/myindex/_search?pretty' -d '{}'
```

Everything looks ok, the document with `id 103` was removed:

``` JSON
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "myindex",
      "_type" : "mytype",
      "_id" : "3567245",
      "_score" : 1.0, "_source" : {"name":"Johnny","suggest-mytype":{"input":["Johnny"],"output":"Johnny","payload":{"_id":3567245}}}
    } ]
  }
}
```
## search 2: look for suggestions

``` sh
curl -XPOST 'localhost:9200/myindex/_suggest?pretty' -d '{
  "completion1": {
    "text": "Jo",
    "completion": {
      "field": "suggest-mytype"
    }
  },
  "completion2": {
    "text": "a",
    "completion": {
      "field": "suggest-mytype"
    }
  }
}'
```

Output for search 2: 

``` JSON
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "completion1" : [ {
    "text" : "Jo",
    "offset" : 0,
    "length" : 2,
    "options" : [ {
      "text" : "Johnny",
      "score" : 1.0, "payload" : {"_id":3567245}
    } ]
  } ],
  "completion2" : [ {
    "text" : "a",
    "offset" : 0,
    "length" : 1,
    "options" : [ {
      "text" : "Anderson",
      "score" : 1.0, "payload" : {"_id":103}
    } ]
  } ]
}
```

The entry with the `id 103` was **not** supposed to show up here, because we removed the document.
</description><key id="23312483">4254</key><summary>Completion field is not removed from the completion-suggester index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danilomr</reporter><labels /><created>2013-11-26T13:00:34Z</created><updated>2013-11-26T15:48:32Z</updated><resolved>2013-11-26T15:48:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T15:48:32Z" id="29303129">this is also expected. you need to do an optimize / merge to get rid of them the suggest dictionaries might not reflect deletions immediately
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Completion suggest: Clarify de-duplication, optimize/merge</comment></comments></commit></commits></item><item><title>Completion suggest mechanism: Update lucene trunk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4253</link><project id="" key="" /><description>The analyzing suggester and the fuzzysuggester got a couple of updates on lucene trunk, which have been incorporated here.

This required a backwards compatible postings format, so a new version has been created as well as tests checking for this.

Also the CompletionSuggestSearchTests now are more randomized now (would have helped me finding a bug faster).
</description><key id="23299933">4253</key><summary>Completion suggest mechanism: Update lucene trunk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-26T08:29:56Z</created><updated>2014-07-16T21:51:08Z</updated><resolved>2013-11-26T12:11:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T09:24:06Z" id="29277767">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use tokenizer's settings in a setting of SynonymTokenFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4252</link><project id="" key="" /><description>In the following index setting, synonymTest uses own settings, not bigramTokenizer's one (synonymTest does not use min_gram and max_gram). I think that it's better to use bigramTokenizer's setting.

```
curl -XPUT localhost:9200/test -d '{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "bigram_analyzer" : {
          "type" : "custom",
          "tokenizer" : "bigramTokenizer",
          "filter" : ["synonymTest"]
        }
      },
      "tokenizer" : {
          "bigramTokenizer" : {
          "type" : "ngram",
          "min_gram" : 2,
          "max_gram" : 2
        }
      },
      "filter" : {
        "synonymTest" : {
          "type" : "synonym",
          "synonyms_path" : "synonym.txt",
          "tokenizer" : "bigramTokenizer"
        }
      }
    }
  }
}'
```
</description><key id="23298323">4252</key><summary>Use tokenizer's settings in a setting of SynonymTokenFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marevol</reporter><labels><label>feedback_needed</label></labels><created>2013-11-26T07:41:32Z</created><updated>2014-08-08T08:46:18Z</updated><resolved>2014-08-08T08:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T18:18:59Z" id="51510797">Hi @marevol 

I'm sorry but I don't understand this at all.  Why are you trying to access the tokenizer's setting from within a token filter.  The token filter just receives a token stream from the tokenizer - it is independent code.
</comment><comment author="johtani" created="2014-08-08T06:38:02Z" id="51568191">&gt; @marevol 

Do you try the following setting? 
It works if you append the tokenizer setting, e.g. min_gram/max_gram ,  in `synonymTest`.

```
curl -XPUT localhost:9200/test -d '{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "bigram_analyzer" : {
          "type" : "custom",
          "tokenizer" : "bigramTokenizer",
          "filter" : ["synonymTest"]
        }
      },
      "tokenizer" : {
          "bigramTokenizer" : {
          "type" : "ngram",
          "min_gram" : 2,
          "max_gram" : 2
        }
      },
      "filter" : {
        "synonymTest" : {
          "type" : "synonym",
          "synonyms_path" : "synonym.txt",
          "tokenizer" : "bigramTokenizer",
          "min_gram" : 2,
          "max_gram" : 2
        }
      }
    }
  }
}'
```

Or is it suggestion of the improvement, using the tokenizer' settings instead of appending to the token filter settings?

&gt; @clintongormley 

I think `tokenizer` in `synonymTest` that he explain is the tokenizer that will be used to tokenize the synonym,.
</comment><comment author="clintongormley" created="2014-08-08T08:46:18Z" id="51577283">Ah OK - I understand now.  After discussing, we feel that a cleaner solution would be to use the same analysis chain up to the point where the synonym token filter appears, so that synonyms are analyzed in the same way as other text.

Closing in favour of #7199
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add cat API for pending tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4251</link><project id="" key="" /><description>would be useful to have the pending tasks API also exposed as cat API
</description><key id="23297062">4251</key><summary>Add cat API for pending tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.0.0.Beta2</label></labels><created>2013-11-26T06:56:44Z</created><updated>2013-12-02T08:45:26Z</updated><resolved>2013-11-29T07:09:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestHelpAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestPendingClusterTasksAction.java</file></files><comments><comment>Add _cat/pending_tasks.</comment></comments></commit></commits></item><item><title>Deciders shouldn't loop over all shards.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4250</link><project id="" key="" /><description>Instead of using loops over all ShardRoutings, do accounting in RoutingNodes.

Speeds up recalculating cluster state on large clusters. This is the background of the patch - we have 20k+ shards in the cluster and the master sits at 100% CPU for minutes trying to apply cluster state changed events.

routing.allocation related tests pass. Compilation problem with head pulled into my master to apply changes on top? (org.elasticsearch.common.inject.Inject)
There is certainly not much elegance with the approach of using a Singleton - but this class is instantiated in one place only anyhow. There is even less elegance of using the notification from MutableShardRouting to RoutingNodes. but i guess this is what can be done without a major refactoring. 
</description><key id="23274432">4250</key><summary>Deciders shouldn't loop over all shards.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">geidies</reporter><labels /><created>2013-11-25T21:23:26Z</created><updated>2014-07-09T06:02:20Z</updated><resolved>2013-11-26T16:10:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T15:56:53Z" id="29303970">can you split out "Skipping execution of remaining Deciders if one of them returns a Dec.." into a separate PullRequest?
</comment><comment author="s1monw" created="2013-11-26T15:58:10Z" id="29304083">I think this also counts for the this commit: 43d808a
I think we should get them in case by case and they are easier to review and to track.
</comment><comment author="geidies" created="2013-11-26T16:10:44Z" id="29305420">@s1monw , 43d808a really should have been squashed into https://github.com/geidies/elasticsearch/commit/f67d4537a19a92e9d104f590bd3ed6c0b0ed1cb1 - I'll close this pull request and create two new ones.
</comment><comment author="geidies" created="2013-11-26T16:56:34Z" id="29309862">@s1monw - https://github.com/elasticsearch/elasticsearch/pull/4257 and https://github.com/elasticsearch/elasticsearch/pull/4259 to replace this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document multi term vector api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4249</link><project id="" key="" /><description>Documentation for multi term vectors. Also this pull request makes parsing of rest requests consistent with single term vector api.

Closes #3998
</description><key id="23264423">4249</key><summary>Document multi term vector api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-11-25T18:59:18Z</created><updated>2014-07-16T21:51:09Z</updated><resolved>2013-11-26T16:05:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T15:53:34Z" id="29303638">I think this looks good? I think we can push this!
</comment><comment author="brwe" created="2013-11-26T16:05:34Z" id="29304901">pushed to master (3be5f33, dbef640)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Possible improvement for orderly shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4248</link><project id="" key="" /><description>Last week I spent a few hours manually restarting nodes to upgrade to 0.90.7.  Since we want to keep our configured level of redundancy at all non-emergency times I did it by using the shard allocation api to disallow shard from the node being shutdown, waiting until there were no indexes on the node, restarting it, then using the shard allocation api to allow shards back on the node, then waiting for the cluster to re-balance shards.

I wonder if this process could be automated beyond the bash/curl/awk mess that I've been using and if that might let the re-balance operation proceed more quickly.  Would it be possible to:
1.  Ask the cluster to prepare a node for shutdown.
2.  The cluster will not allocate replicas to that node until it has finished shutting down.
2.  The cluster allocates an extra copy of all replicas that that node is hosting.
3.  Once this is done the node goes through the normal shutdown process.
4.  When the node comes back up it should rejoin the cluster and announce that replicas that it sill holds.  At this point I think you can let the standard startup and re-balance logic take hold.  The replicas that didn't change will stay on the restarted node and be removed from the node to which they were recently replicated.  Those that did change will be removed from the restarted node and cluster re-balancing will balance the shards again.

I think this strikes a nice balance between the exclude._ip way of shutting down and the disable_allocation way of shutting down.

At some point it'd be nice if there were some kind of log that could be replayed against out of date replicas so they could recover even if changes had been made.  I wonder if something could be synthesized from the _timestamp field....
</description><key id="23254011">4248</key><summary>Possible improvement for orderly shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2013-11-25T16:31:42Z</created><updated>2014-11-28T09:50:20Z</updated><resolved>2014-11-28T09:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-11-26T17:37:43Z" id="29313656">See also #4043 please
</comment><comment author="nik9000" created="2013-11-27T17:32:37Z" id="29403566">@roytmana #4043 looks like it is for cluster restarts rather than rolling restarts.  Still a problem, but right now I'm more interested in rolling restarts:)  Still, if there is something that makes sense in both cases then I'd love to do it.
</comment><comment author="roytmana" created="2013-11-27T17:40:39Z" id="29404310">yes, but with a rolling restart you at least know why you go through the pain where with cluster restart (say for full upgrade) the pain is unjustified :-) 

It would be nice if restarting a node or entire cluster on purpose were slightly different from what ES does on node crash and optimized for quick recovery and ease of use. 

We develop but we do not manage production ES and I feel the "average" sys admins are  honestly not ready for the touchiness and complexity of current operational support. they would need to spend days on the mailing lists and bag their heads against problems before they can support ES with confidence
</comment><comment author="nik9000" created="2013-11-27T17:52:03Z" id="29405248">Yeah, I'm the primary maintainer of our cluster but I wish it were easier for other folks in the organization to do stuff.  I kind of like this idea because it is somewhat analogous to apache's graceful shutdown.

I suppose it could also be used for removing nodes cleanly - just don't restart the node.  It isn't any _better_ than using the cluster allocation api to push shards off the node but it would be more like other stuff, especially if it could be achieved with an `/etc/init.d/elasticsearch graceful`.
</comment><comment author="clintongormley" created="2013-11-27T18:57:18Z" id="29410270">Hi @nik9000 

What's wrong with:
- disable allocation 
- shut down a node
- start up the node
- reenable allocation

I realise you're trying to ensure that the cluster will be green even while the node is down, but is this really required?  It's certainly a lot faster to go through the above process than to copy your shards around the cluster for every node. 
</comment><comment author="nik9000" created="2013-11-27T19:38:59Z" id="29414176">I want to be able to handle losing another node unexpectedly while I'm in the process of doing rolling restarts:
1.  If I do lose a node while while restarting with allocation off I have to remember to turn it on in what has suddenly become a very exciting situation.  This becomes scary if it isn't me, but someone less familiar with Elasticsearch doing the restart.  Doubly scary again if they are using some kind of maintenance script to automate execution across many nodes.
1.  If the node I lose happens to also contain a replica of the one of the indexes on a restarting node I now have two fewer replicas.  This is bad because I (at least theoretically) have sized the number of replicas I need to handle my search traffic plus one redundant replica.  Two redundant replicas is more than I can afford.  I believe the window of opportunity is longer than the restart for indexes that must be restored to the restarted node.

I do admit to having another motive to keeping the state green as much as possible:  I don't want to excite my entire ops team just because a few indexes are taking a while copying over.  Most folks monitor the health status and nagios will start ringing bells if it stays yellow for long.
</comment><comment author="clintongormley" created="2014-11-28T09:50:20Z" id="64874907">Hi @nik9000 

We discussed this in FixItFriday today and there seem to be two issues:
1. making redundant copies of the shards on the node being restarted
2. reusing the replicas on the restarted node

The ability to reuse the replicas on the started node either requires all segments on that node to be the same as those on the primary, or sequence IDs (#6069).  The addition of sequence IDs (which we are actively working on) will speed up recovery dramatically.  This pretty much obviates the need for number 1 - the redundant replicas, as recovery will be much faster.

I think, with that plan, we can close this issue in favour of sequence IDs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added `execution` option to `range` filter and deprecated `numeric_range` filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4247</link><project id="" key="" /><description>Relates to #4034
</description><key id="23244391">4247</key><summary>Added `execution` option to `range` filter and deprecated `numeric_range` filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-25T14:12:53Z</created><updated>2015-05-18T23:33:36Z</updated><resolved>2013-11-25T22:46:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-25T17:05:28Z" id="29219832">looks good to me, I would say we can push it to 0.90 as well....
</comment><comment author="martijnvg" created="2013-11-25T17:12:43Z" id="29220529">Makes sense since it only deprecates the `numeric_range` filter.
</comment><comment author="martijnvg" created="2013-11-25T22:46:28Z" id="29250542">pushed to master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Explanation for non-green health status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4246</link><project id="" key="" /><description>Would it be possible for some api (probably the health api) to explain why the cluster is red or yellow?  Something like:
"foo_bar_index has unassigned shards that are being initialized"
"foo_bar_index has unassigned shards that cannot be initialized because the XXXXX" (where XXXXX is some kind of message from the allocator)
"foo_bar_index has unassigned master shards that cannot be reassigned without data loss"
"the cluster cannot contact enough master eligible nodes and must wait for one to come back"
would be superb.

Some of this could live in an external monitoring program but it'd get out of date and it wouldn't have the kind of access to the reasons behind decisions that could make this super useful.
</description><key id="23242589">4246</key><summary>Explanation for non-green health status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-11-25T13:39:02Z</created><updated>2014-08-08T18:46:20Z</updated><resolved>2014-08-08T18:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-12-30T09:17:11Z" id="31338225">+1
</comment><comment author="clintongormley" created="2014-08-08T18:46:20Z" id="51642273">The cluster health API can return index/shard level info with the `level` param:

```
GET /_cluster/health?level=shards
```

 and the `cat-health` API gives you more useful info by index:  http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-health.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add aggregation support in the percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4245</link><project id="" key="" /><description>Add aggregation support in the percolate api, just like the facet support.
</description><key id="23237390">4245</key><summary>Add aggregation support in the percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta2</label></labels><created>2013-11-25T11:48:22Z</created><updated>2014-01-16T15:32:16Z</updated><resolved>2013-11-28T16:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="benneic" created="2014-01-14T02:36:54Z" id="32234062">Hey @martijnvg can you explain how an aggregating percolator works? Does it retain running totals as you fire in documents in successive percolator get requests for aggregations like sum? 

For example:

With an aggregating percolator I am expecting that I could filter a timestamp on a post by previous 2 days, then aggregate sum the likes on all posts, then filter again when sum_likes &gt; 100.

So when first doc containing a post.likes=70 is queried against percolator, it has no match, then a subsequent post.likes=40 within the 2 day time range is tested against percolator and it matches. As new docs are tested against percolator and old posts are older than 2 days their post.likes are removed from the running sum_likes total?
</comment><comment author="martijnvg" created="2014-01-16T15:32:16Z" id="32478830">@beichhor Aggregations (and facets) in percolate api operate on the matching queries for a single percolate request. This can be useful if your percolate queries have extra metadata and in the percolate api you can then get analytics for the matching queries based on aggregations on this metadata. 

&gt; Does it retain running totals as you fire in documents in successive percolator get requests for aggregations like sum?

No, it doesn't.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorFacetsAndAggregationsTests.java</file></files><comments><comment>Added aggregation support to the percolate api.</comment><comment>Closes #4245</comment></comments></commit></commits></item><item><title>node client got a shard  (possible bug)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4244</link><project id="" key="" /><description>i have a 3 data and 1 client node cluster setup in a dev environment.
i started to shutdown the data nodes one by one and left only the client node open.

When i opened the farm back i saw that one shard was missing and it got assigned to the client node. (of course no data was there)

im using the latest es build.
</description><key id="23226913">4244</key><summary>node client got a shard  (possible bug)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alens</reporter><labels /><created>2013-11-25T08:04:10Z</created><updated>2013-11-25T08:06:07Z</updated><resolved>2013-11-25T08:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested document not visible from facet script?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4243</link><project id="" key="" /><description>My aim is to create an auto complete for author names, and return their full name and screen name..

From what I can tell from all the documentation, this should work, but I just can't get the script to see the "sources" nested field of the "link" document type

This gist contains the index setup and testing shell scripts

https://gist.github.com/brightbits/7637501

Many thanks
</description><key id="23225641">4243</key><summary>Nested document not visible from facet script?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikebaldry</reporter><labels /><created>2013-11-25T07:19:54Z</created><updated>2013-12-16T03:35:20Z</updated><resolved>2013-12-16T03:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-30T18:04:13Z" id="29557483">@brightbits Nested documents are indexed internally as separate documents. So, they are not accessible using doc lookup in the facet that you run on the top documents unless you set `include_in_root` or `include_in_parent` to true See [Nested Type Mapping](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-nested-type.html) page of documentation for more details. You can also access the source of the document (with all nested documents) using source lookup (`_source`), or switch to a nested facet, which works on nested documents. 

If you have any further questions, please don't hesitate to ask them in the [mailing list](http://www.elasticsearch.org/help/). The mailing list is the perfect place to ask questions like this one. You can use github issues for bug reports and enhancements requests. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add support for `shard_size` to terms aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4242</link><project id="" key="" /><description>add support for `shard_size` parameter (next to `size`) in the terms aggregations - will help increase accuracy on high cardinality fields.

similar to the `shard_size` support in terms facets - #3821 
</description><key id="23216190">4242</key><summary>add support for `shard_size` to terms aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label><label>v1.0.0.Beta2</label></labels><created>2013-11-24T23:59:07Z</created><updated>2013-11-27T13:51:25Z</updated><resolved>2013-11-27T13:51:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/TermsFacetParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsTests.java</file></files><comments><comment>Added support for shard_size in terms agg</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 4.6.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4241</link><project id="" key="" /><description>This upgrade would include the following improvements:
- Remove XIndexWriter in favor of the fixed IndexWriter
- Removes patched XLuceneConstantScoreQuery
- Now uses Lucene passage formatters contributed from Elasticsearch in PostingsHighlighter
- Upgrades to Lucene46 Codec from Lucene45 Codec
- Fixes problem in CommonTermsQueryParser where close was never called.

here are the changes in Lucene 4.6:

http://lucene.apache.org/core/4_6_0/changes/Changes.html
</description><key id="23211892">4241</key><summary>Upgrade to Lucene 4.6.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-24T20:07:54Z</created><updated>2013-11-24T20:11:30Z</updated><resolved>2013-11-24T20:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/index/XIndexWriter.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/search/XLuceneConstantScoreQuery.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XDefaultPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/apache/lucene/store/RateLimitedFSDirectory.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferDirectory.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorFields.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java</file><file>src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/OnGoingMerge.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/IndexUpgraderMergePolicy.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/search/facet/query/QueryFacetExecutor.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighterTests.java</file><file>src/test/java/org/elasticsearch/index/cache/filter/FilterCacheTests.java</file><file>src/test/java/org/elasticsearch/index/codec/CodecTests.java</file><file>src/test/java/org/elasticsearch/index/codec/postingformat/DefaultPostingsFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/search/suggest/phrase/NoisyChannelSpellCheckerTests.java</file></files><comments><comment>Upgrade to Lucene 4.6</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 4.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4240</link><project id="" key="" /><description>This commit upgrades to Lucene 4.6 and contains the following improvements:
- Remove XIndexWriter in favor of the fixed IndexWriter
- Removes patched XLuceneConstantScoreQuery
- Now uses Lucene passage formatters contributed from Elasticsearch in PostingsHighlighter
- Upgrades to Lucene46 Codec from Lucene45 Codec
- Fixes problem in CommonTermsQueryParser where close was never called.
</description><key id="23211119">4240</key><summary>Upgrade to Lucene 4.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-24T19:26:10Z</created><updated>2014-07-16T21:51:09Z</updated><resolved>2013-11-24T20:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-11-24T19:44:12Z" id="29163916">+1!
</comment><comment author="s1monw" created="2013-11-24T20:03:14Z" id="29164431">I agree with all the comments. Yet, I added back the check for constant score and I will remove it in a different issue. I will fix and push!
</comment><comment author="s1monw" created="2013-11-24T20:12:06Z" id="29164675">pushed 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error: "No mapping found for value_field" uses key_field instead of value_field in range facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4239</link><project id="" key="" /><description>Assume you have a valid key_field - data.field3 and a non existing value_field - data.field5 and you issue a query like:
{"facets":{"facet_result":{"range":{"key_field":"data.field3","value_field":"data.field5","ranges":[{"to":18},{"from":18,"to":19},{"from":19}]}}}}

Then you will get an error like:
... No mapping found for value_field [data.field3]]; 

instead of 
... No mapping found for value_field [data.field5]]; 
</description><key id="23204502">4239</key><summary>Error: "No mapping found for value_field" uses key_field instead of value_field in range facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">ronsher</reporter><labels><label>bug</label><label>v0.90.8</label></labels><created>2013-11-24T12:36:54Z</created><updated>2013-11-24T13:57:15Z</updated><resolved>2013-11-24T13:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-11-24T13:52:09Z" id="29155712">thanks... it was already fixed in master... will push a fix for 0.90
</comment><comment author="uboness" created="2013-11-24T13:57:15Z" id="29155811">fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Local primaries should be preferred to relocating primaries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4238</link><project id="" key="" /><description>To reproduce the bug use -Dtests.seed=5AB62524C9AB0489
Fixes #4237
</description><key id="23195093">4238</key><summary>Local primaries should be preferred to relocating primaries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-11-23T22:05:56Z</created><updated>2014-06-25T15:05:28Z</updated><resolved>2013-11-25T02:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-24T20:20:32Z" id="29164875">LGTM, nice one!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>During node startup local primaries should be preferred to relocating primaries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4237</link><project id="" key="" /><description>To reproduce 
- set `cluster.routing.allocation.node_initial_primaries_recoveries` to any value lower than `cluster.routing.allocation.node_concurrent_recoveries`
- start cluster with two nodes (node1 and node2)
- create an index with 1000 shards and 0 replicas:

```
curl -XPUT localhost:9200/test -d '{"settings": {"number_of_shards":1000, "number_of_replicas":0}}' 
```
- shutdown node2
- enable allocation filtering to exclude node1:

```
curl -XPUT localhost:9200/_cluster/settings -d '{"transient": {"cluster.routing.allocation.exclude._id": "...id of node1..."}}'
```
- at this time half of the shards should be `STARTED` and another half should be `UNASSIGNED`
- start node2
- observe that instead of initializing `node_initial_primaries_recoveries` shards on node2 first, elasticsearch is moving shards from node1 to node2

Expected behavior: until all local shards are initialized, all `node_initial_primaries_recoveries` shards should be initializing locally and the rest (`node_concurrent_recoveries` - `node_initial_primaries_recoveries`) can be used for relocation.

Impact: as a result of this bug, sometimes relocating primaries from another node can take over local recovery and a cluster may take very long time to get to green status
</description><key id="23194977">4237</key><summary>During node startup local primaries should be preferred to relocating primaries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-23T21:59:02Z</created><updated>2013-11-25T02:32:45Z</updated><resolved>2013-11-25T02:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java</file></files><comments><comment>Local primaries should be preferred to relocating primaries</comment></comments></commit></commits></item><item><title>Fixed file-based template loading via config/templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4236</link><project id="" key="" /><description>When parsing the json file, the first field is ignored as
parser.nextToken() seems to be called too often.

Closes #4235
</description><key id="23185087">4236</key><summary>Fixed file-based template loading via config/templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-23T10:17:46Z</created><updated>2014-06-29T14:25:21Z</updated><resolved>2013-11-27T09:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-26T16:23:14Z" id="29306657">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Loading templates via templates/ directory is not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4235</link><project id="" key="" /><description>Seems that with 0.90.7/master loading files via the `config/templates`directory does not work.

The problem is, that the parsed file jumps over the first while parsing, which results in the template not being set, if it is the first.

Going to send a PR in a second.
</description><key id="23185068">4235</key><summary>Loading templates via templates/ directory is not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-23T10:15:58Z</created><updated>2014-01-17T13:51:18Z</updated><resolved>2013-11-27T09:05:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arr-dev" created="2014-01-17T13:51:18Z" id="32606937">This still doesn't seem to work for me.

I've used example given in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#config but after restart, `curl -XGET http://localhost:9200/_template` is empty and doesn't seem to be applied.

Elasticsearch 0.9.10.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Fix loading templates in config/ directory</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Fix parsing of file based template loading</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Fixed file-based template loading via config/templates</comment></comments></commit></commits></item><item><title>Preference only_node with unknown nodeId returns useful exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4234</link><project id="" key="" /><description>When the search preference is set to only node, but this node is not a
data (or does not exist), we return a search exception, which indicates,
that this is actually a server problem.

However specifying a non-existing node id is a client problem
and should return a more useful error message than
{"error":"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed]","status":503}

I am not a hundred percent sure, if this introduces some sort of special handling again and we should drop it therefore, but I think it improves the user experience by knowing what is going on.
</description><key id="23184185">4234</key><summary>Preference only_node with unknown nodeId returns useful exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-23T08:47:06Z</created><updated>2014-06-16T13:29:48Z</updated><resolved>2013-12-10T19:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-10T13:48:04Z" id="30227859">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added mlockall setting to process info output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4233</link><project id="" key="" /><description>Added with PR https://github.com/elasticsearch/elasticsearch/pull/4226
</description><key id="23183028">4233</key><summary>Added mlockall setting to process info output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-23T06:48:54Z</created><updated>2013-11-23T06:48:58Z</updated><resolved>2013-11-23T06:48:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in RobinEngine.acquireSearcher()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4232</link><project id="" key="" /><description>There appears to be a NullPointerException in the RobinEngine class on version 0.90.7.

[2013-11-23 01:10:04,745][WARN ][cluster.action.shard     ] [Thing] [indexName][18] sending failed shard for [indexName][18], node[W71dMRgfR3iZuVI77KIsdg], relocating [cV9T2EC9SyaYxhigk4_N6Q], [R], s[RELOCATING], indexUUID [_na_], reason [Failed to perform [deleteByQuery/shard] on replica, message [RemoteTransportException[[Meld][inet[/192.168.1.1:9300]][deleteByQuery/shard/replica]]; nested: NullPointerException; ]]
[2013-11-23 01:10:08,315][WARN ][action.deletebyquery     ] [Thing] Failed to perform deleteByQuery/shard on replica [indexName][18]
org.elasticsearch.transport.RemoteTransportException: [Baron Blood][inet[/192.168.1.1:9300]][deleteByQuery/shard/replica]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.engine.robin.RobinEngine.acquireSearcher(RobinEngine.java:744)
        at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:601)
        at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnReplica(TransportShardDeleteByQueryAction.java:132)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:245)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:225)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
</description><key id="23180127">4232</key><summary>NPE in RobinEngine.acquireSearcher()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">feangulo</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-23T02:43:00Z</created><updated>2013-11-27T12:47:38Z</updated><resolved>2013-11-25T10:25:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="feangulo" created="2013-11-23T02:43:37Z" id="29124105">This is in 0.90.7.
</comment><comment author="s1monw" created="2013-11-23T09:44:01Z" id="29129256">do you see any other errors in your log files by any chance?
</comment><comment author="feangulo" created="2013-11-23T14:50:24Z" id="29133657">Yes. I am running a configuration with multiple "workhorses", "coordinators" and "load balancers". The error message above is seen in the "workhorse" log. Then I also see this error message in the "coordinator":

[2013-11-23 05:38:50,052][WARN ][cluster.action.shard     ] [Coordinator 1] [indexName][10] received shard failed for [indexName][10], node[1-iPub6-Tt-Czl9UmNWQlA], relocating [gK2UZwMOSpOYBYPFjoQwiQ], [R], s[RELOCATING], indexUUID [PBCHvDiWQAKy-eYsRmGXrw], reason [Failed to perform [deleteByQuery/shard] on replica, message [RemoteTransportException[[Thunderbolt][inet[/192.168.1.1:9300]][deleteByQuery/shard/replica]]; nested: NullPointerException; ]]

It also appears that it crashes the workhorse node, but I'm still trying to reproduce this.
</comment><comment author="kimchy" created="2013-11-25T10:21:54Z" id="29190413">its a state management issue, going to fix it shortly
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>acquireSearcher should fail with state failure when not set/closed</comment><comment>also fixes #4232</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>acquireSearcher should fail with state failure when not set/closed</comment><comment>also fixes #4232</comment></comments></commit></commits></item><item><title>Elasticsearch can contain regexp dsl query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4231</link><project id="" key="" /><description>can elasticsearch have the query like this ?

```
{
  "from": 0,
  "size": 10,
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "must": [
            {
              "term": {
                "first_name": "elasticsearch"
              }
            },
            {
              "regexp": {
                "last_name": "server*"
              }
            }
          ]
        }
      },  
```

I have tried it many times but does not yield any result.
I have tried putting filter and other combinations
</description><key id="23140802">4231</key><summary>Elasticsearch can contain regexp dsl query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">desaxena</reporter><labels /><created>2013-11-22T14:24:29Z</created><updated>2013-11-23T06:59:50Z</updated><resolved>2013-11-23T06:59:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-23T06:59:50Z" id="29127375">Hey,

please ask questions like this one the google group at https://groups.google.com/forum/#!forum/elasticsearch, as the github issues tracker is intended to be used for issues only. Follow http://elasticsearch.org/help to create a useful example, of what you already did (including how you indexed your documents), what you expected and what did not work out as expected.

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Docs] Add documentation for _source filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4230</link><project id="" key="" /><description /><key id="23131098">4230</key><summary>[Docs] Add documentation for _source filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-11-22T11:16:44Z</created><updated>2014-07-16T21:51:12Z</updated><resolved>2013-11-27T06:49:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-27T06:49:15Z" id="29363628">Committed to master: c63d8c4fb566882bd4655ddc7dec13b4e8719244

@polyfractal thx for reviewing!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactored put mapping api to make use of the new recently introduced generic ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4229</link><project id="" key="" /><description>Refactored put mapping api to make use of the new recently introduced generic ack mechanism

Closes #4228
</description><key id="23101642">4229</key><summary>Refactored put mapping api to make use of the new recently introduced generic ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-21T22:11:17Z</created><updated>2014-06-15T15:04:43Z</updated><resolved>2013-11-29T10:43:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Move put mapping api to new acknowledgement mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4228</link><project id="" key="" /><description>Move put mapping api to new acknowledgement mechanism introduced in #3786 .
</description><key id="23098949">4228</key><summary>Move put mapping api to new acknowledgement mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-21T21:28:31Z</created><updated>2013-11-29T10:43:06Z</updated><resolved>2013-11-29T10:43:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeMappingCreatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Refactored put mapping api to make use of the new recently introduced generic ack mechanism</comment></comments></commit></commits></item><item><title>Consistency between `_search` and `_msearch`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4227</link><project id="" key="" /><description>The `_msearch` API doesn't accept the same header parameters than `_search`'s query parameters.
For instance `version` is both valid in the search body and query parameters for `_search`, but is ignored in the query parameters and header in `_msearch`.

`MultiSearchRequest.add()` should rely on `RestSearchAction.parseSearchSource()` or even `RestSearchAction.parseSearchRequest()` to parse the subrequests, for maximum compatibility.

IMHO, a clean bulk API should use the header parameters as query parameters on the target endpoint, which would imply that they keep in sync no matter the code changes in the target endpoints.
</description><key id="23083082">4227</key><summary>Consistency between `_search` and `_msearch`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2013-11-21T17:33:06Z</created><updated>2016-12-23T10:08:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-29T23:33:15Z" id="29541472">yea, agreed, that would be nice to have
</comment><comment author="clintongormley" created="2016-12-23T10:08:38Z" id="268967389">Should _search be implemented internally via _msearch, in which case the two would never go out of sync?  @christoph and @martijnvg what do you think?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added mlockall setting to process info output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4226</link><project id="" key="" /><description>Right now we only log a 'configuration error' in the logfile upon start up if mlockall is not configured. I would like see this in the infos as well and added it to the process info in order to get hold of it more easily.

Not the most elegant way, definately open for nicer solutions.
</description><key id="23068152">4226</key><summary>Added mlockall setting to process info output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-21T14:02:40Z</created><updated>2014-06-25T21:21:57Z</updated><resolved>2013-11-23T06:46:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-21T15:11:51Z" id="28991926">I think this is good?
</comment><comment author="martijnvg" created="2013-11-21T15:15:54Z" id="28992311">Nice one! How would you want to do this differently? To me to approach looks good.
</comment><comment author="kurtado" created="2013-11-22T08:28:53Z" id="29056472">Would be great to minimize this common source of frustration for users. 
</comment><comment author="dadoonet" created="2013-11-22T08:35:55Z" id="29056798">+1 nice to have this.
</comment><comment author="bleskes" created="2013-11-22T08:56:54Z" id="29057879">+1 on adding this functionality. The more we know via the info/stats API about what people did the better.

One little comment that may help with your feeling of "not elegant" - right now the processinfo class is purely informational and all the hairy stuff happens in `org.elasticsearch.monitor.process.JmxProcessProbe` &amp; `org.elasticsearch.monitor.process.SigarProcessProbe` . Maybe add the mlockall boolean to the constructor of `ProcessInfo` and make the probes get the value from natives and pass it to the ProcessInfo class?

Other than that - don't see why it's not elegant. Looks like a fairly straight forward solution to me.
</comment><comment author="spinscale" created="2013-11-23T06:46:45Z" id="29127233">Merged into master with https://github.com/elasticsearch/elasticsearch/commit/fdc4f72e8afd753099992a34ffe64fd2656ddc1d and into 0.90 with https://github.com/elasticsearch/elasticsearch/commit/3145ffd3728fcd24e8b05a34d157d6eedbc5368c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SloppyMath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4225</link><project id="" key="" /><description>Added copy of SloppyMath.java from lucene 4.6+
and setup GeoDistance for new haversin method

closes #3862
</description><key id="23067274">4225</key><summary>SloppyMath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-11-21T13:47:11Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-12-11T09:22:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-11-21T14:08:22Z" id="28986683">SloppyMath.asin says it has an error around 1E-7, which translates to an error of 1.3m in haversine if I'm not mistaken. I don't think this is an issue, but this makes me wonder whether ARC distance computation should fall back to PLANE on short distances (to minimize the error) or maybe if we should have both a precise ARC and a sloppy SLOPPY_ARC GeoDistances (probably with SLOPPY_ARC as the default since I think the trade-off is better)?
</comment><comment author="s1monw" created="2013-11-21T15:08:49Z" id="28991670">I agree with @jpountz I think we should fall back if we have short distances. Generally when we port stuff from Lucene as an XClass we don't port the tests. I think in this case it's ok but we should also put a check for `4.6` in the `XSloppyMath` class itself. We usually do this as a static block like:

``` Java
static {
  assert Version.CURRENT.luceneVersion.compareTo(org.apache.lucene.util.Version.LUCENE_45) == 0 : "Remove XSloppyMath once 4.6 is out";
}
```
</comment><comment author="chilling" created="2013-11-22T07:35:39Z" id="29053691">You're right. I worked the whole day on getting the error out. I hope you agree that the error doesn't matter for _great_ distances.
</comment><comment author="s1monw" created="2013-11-22T08:05:32Z" id="29055284">Yeah for great distances this really doesn't matter. We might want to think about making it configurable in the mapping such that you can opt out entirely?
</comment><comment author="chilling" created="2013-11-22T10:30:17Z" id="29063081">@s1monw what do you want to make configurable in the mapping?
In the latest version I managed to reduce the error less 1.0E-4 meters in `haversin`. But I need to calculate the plane distance every time and branch on the result. Another problem I got is a different value of the earth major axis used in the Lucene implementation. I left a comment on jira to move this to the signature of the `haversin` method. If robert agrees, we are able to calculate not only with WGS84 compatible distances but also use a more accurate unit (mm) to calculate the distance and make the results more accurate this way.
The current version is a little buggy and I need to find out, if my accurate version of haversin is really accurate.
</comment><comment author="s1monw" created="2013-11-22T10:38:29Z" id="29063544">sorry @chilling I think mapping is not the right place for it. I really just wanted to make sure that people can opt out of the sloppy calculation so we maybe expose it as a query param `"sloppy" : [true|false|"xmiles"|"xmeters"]` something like that?
</comment><comment author="chilling" created="2013-11-22T10:41:44Z" id="29063751">@s1monw I see. This is actually a good idea.
</comment><comment author="jpountz" created="2013-11-29T10:36:05Z" id="29508180">Florian, thanks for the updates. I think it looks good, but it would be easier for me to review if you could rebase against master (so that we don't need XSloppyMath anymore) and squash the commits (since they all apply to the same lines of code). Thanks!
</comment><comment author="s1monw" created="2013-11-29T11:49:31Z" id="29511800">hmm seems like the latest push still has XSloppyMath...
</comment><comment author="chilling" created="2013-11-29T13:27:36Z" id="29516020">@jpountz thanks for your review, I'm going to set this things up tomorrow. @s1monw for this update I let in there, how about backporting?
</comment><comment author="jpountz" created="2013-11-29T13:40:30Z" id="29516610">@chilling I don't think there is an issue with backporting? (the 0.90 branch is on Lucene 4.6 as well)
</comment><comment author="jpountz" created="2013-11-29T18:19:36Z" id="29530983">I disabled the fallback to plane and all accuracy tests still passed. I need to spend some more time to understand when we can expect the highest errors and if we actually need this fallback. If you have already investigated it, please share. :-)
</comment><comment author="jpountz" created="2013-12-02T15:36:15Z" id="29627170">I've been looking into this a bit further and I don't think we need the fallback on short distances: the approximate functions use taylor series at 0 and are thus **very** precise on small data (and short distances mean that cos and arcsin have been computed against small data).

So I would vote to just remove the fallback to plane and change the tests to make sure the delta is within 1‰ instead of 1mm (I just patched the test case to make sure it passes with such a test).
</comment><comment author="jpountz" created="2013-12-09T08:48:42Z" id="30113863">+1 to squash and push
</comment><comment author="jpountz" created="2013-12-09T19:28:38Z" id="30164779">For the record, `GeoDistanceSearchBenchmark` seems to be **very** happy with this change. It reports times which are 3.2x to 4.5x faster as with accurate arc distance computations.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException in TransportShardBulkAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4224</link><project id="" key="" /><description>This happened on elasticsearch 0.90.6 with JVM 1.7.0_25

Found this in logs, cannot tell what triggered this. The only thing I know is, there are lots of index/search operations going and there seems to be some cluster instability.

```
[2013-XX-YY 08:45:22,518][DEBUG][action.bulk              ] [myNode] [logstash-2013.XX.YY][3], node[vXE6ojncQUG-foPsMjVY_w], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@28c7fa4f]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:138)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:75)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```
</description><key id="23053552">4224</key><summary>NullPointerException in TransportShardBulkAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-21T09:27:11Z</created><updated>2014-08-08T18:44:02Z</updated><resolved>2014-08-08T18:44:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="avleen" created="2013-11-26T08:29:04Z" id="29274705">This just bit us tonight, too, somewhat out of the blue.

Immediately before this I see:

&lt;pre&gt;
[2013-11-26 02:41:52,659][INFO ][discovery.zen            ] [myDataNode] master_left [[logstash01.ny4.etsy.com][swmFFvkEQHaBDyYtBSeSbA][inet[/ip.add.re.ss:9300]]{tag=archive, data=false, master=true}], reason [do not exists on master, act as master failure]
&lt;/pre&gt;

The master, and other nodes in the cluster, were just fine.

About 8 seconds later, it found the master again.
</comment><comment author="DenisUspenskiy" created="2013-12-24T15:55:03Z" id="31176010">Hello,
Also have the same problem. Below is the exception stack trace:

[2013-12-24 14:56:53,082][DEBUG][action.bulk] [Trinity] [agentsmith][10], node[gYnN_-hxQly-2bctN2IVkg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@37daa067]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:138)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:75)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
</comment><comment author="spinscale" created="2013-12-25T13:12:15Z" id="31199000">@DenisUspenskiy did you have some cluster instability as well? Do you have master reelections around that time? Can you reproduce it?
</comment><comment author="spinscale" created="2014-01-13T08:29:51Z" id="32151660">Clinton managed to reproduce it in #4693:

This is reproducible by deleting an index, not waiting for the response, then trying to bulk index into that index, (ie the requests were run in parallel):
</comment><comment author="ashpynov" created="2014-05-29T14:36:16Z" id="44538459">Same call stack here.
On data node such backlog while bulk indexing during master had been restarted. After this some shards on affected index on data node became and stay Unassigned while other is OK. (Index allocation rule is only on data node, no replica, 10 shards). Data node restart do not help. Only index drop.
version is 0.90.7
</comment><comment author="cdmicacc" created="2014-06-18T14:16:14Z" id="46440881">I think we're seeing this, as well.  We get it when we close an index (using ElasticSearch 1.1.1): I have a process that is reindexing to a new index using the bulk API.  While that is happening, my live system is still writing to the old index.  Eventually, the reindex completes and the alias is changed so that writes are directed to the new index.  Just after that, I close the old index.  ElasticSearch's logs get filled with this for a short time (presumably while the threadpools drain):

```
[2014-06-17 17:28:30,986][INFO ][cluster.metadata         ] [es11] closing indices [[idx-2014-06-12-11]]
[2014-06-17 17:29:02,154][DEBUG][action.bulk              ] [es11] [idx-2014-06-12-11][4], node[1r5Z85J8TM2e1Tp3KO3NAA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@7e39e699]
java.lang.NullPointerException
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:139)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:76)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```
</comment><comment author="clintongormley" created="2014-08-08T18:44:02Z" id="51641990">This NPE has been fixed in recent versions. The bulk API can still fail briefly with an index-does-not-exist exception, but this should be fixed by #6790.

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException in FsTranslog when reverting transient log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4223</link><project id="" key="" /><description>This happens on elasticsearch 0.90.6 with JVM 1.7.0_25

Found this in logs, cannot tell what triggered this. The only thing I know is, there are lots of index/search operations going and there seems to be some cluster instability.

```
[2013-XX-YY 12:51:22,939][WARN ][index.translog           ] [myNode] [logstash-2013.XX.YY][1] failed to flush shard on translog threshold
java.lang.NullPointerException
        at org.elasticsearch.index.translog.fs.FsTranslog.revertTransient(FsTranslog.java:302)
        at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:908)
        at org.elasticsearch.index.shard.service.InternalIndexShard.flush(InternalIndexShard.java:559)
        at org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
```
</description><key id="23053483">4223</key><summary>NullPointerException in FsTranslog when reverting transient log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-21T09:25:44Z</created><updated>2013-11-29T23:30:34Z</updated><resolved>2013-11-29T23:30:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-29T23:29:30Z" id="29541399">This happens when reverting the trans transaction log on failure, and when that happens, actually we might have failed on the transient translog creation to being with....
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file></files><comments><comment>NullPointerException in FsTranslog when reverting transient log</comment><comment>This happens when reverting the trans transaction log on failure, and when that happens, actually we might have failed on the transient translog creation to being with....</comment><comment>fixes #4223</comment></comments></commit></commits></item><item><title>Add line feed for prettified main REST action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4222</link><project id="" key="" /><description /><key id="23026248">4222</key><summary>Add line feed for prettified main REST action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2013-11-20T21:39:59Z</created><updated>2014-06-22T15:51:07Z</updated><resolved>2013-11-29T15:44:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-29T15:44:09Z" id="29523475">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation inconsistent on thread pool defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4221</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html

Top of page seems to indicate that the default for queue_size is fixed based on which thread pool, but then below it says it defaults to -1. Am I just missing something here?
</description><key id="23023564">4221</key><summary>Documentation inconsistent on thread pool defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">pvulgaris</reporter><labels /><created>2013-11-20T20:59:06Z</created><updated>2014-04-25T19:42:06Z</updated><resolved>2014-04-25T19:42:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-21T08:56:12Z" id="28966724">If you change the configuration of a thread_pool to `fixed`, but dont specify a `queue_size`, then it is configured as -1 by default, see https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L327

Do you think the wording is bad? How could we improve it?
</comment><comment author="spinscale" created="2014-04-25T19:42:06Z" id="41431846">closing this one (very happy to reopen or get a PR or hints how to improve the documentation to make it more understandable). Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk should support shard timeout like the index api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4220</link><project id="" key="" /><description>It'd be nice if the bulk and delete api supported the shard timeout setting like the index api.  This would allow updates to fail quickly if the shard is offline rather than waiting for the client side timeout and leaving the operation hanging around on the server.
</description><key id="23004574">4220</key><summary>Bulk should support shard timeout like the index api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-20T16:54:06Z</created><updated>2014-01-10T19:58:09Z</updated><resolved>2013-11-25T10:48:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-20T16:58:54Z" id="28907063">- delete does support setting the timeout, no?
- bulk doesn't support setting the shard availability timeout, bit it does uses the default value (which is bounded).
</comment><comment author="nik9000" created="2013-11-20T17:34:33Z" id="28910488">&gt; delete does support setting the timeout, no?

Looks like that is just a documentation problem then.  http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-delete.html doesn't have the word timeout on it.

&gt; bulk doesn't support setting the shard availability timeout, bit it does uses the default value (which is bounded).

It'd be useful if I could set it on a per request basis.  Some of my requests I'd rather fail fast and others I'd rather let them wait.  It isn't a big deal, the bulk commands that I need to fail fast I can convert into repeated update/index/delete calls and get a similar effect without too much overhead.  They aren't huge bulk commands.
</comment><comment author="kimchy" created="2013-11-20T20:52:16Z" id="28929763">fixed the delete docs: a9880dcbf1130381c4643bda1bf7e9ca0c64494a
</comment><comment author="nik9000" created="2013-11-25T13:20:27Z" id="29200700">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[API] bulk request now supports timeout, see #4220</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file></files><comments><comment>Bulk should support shard timeout like the index api</comment><comment>closes #4220</comment></comments></commit></commits></item><item><title>NullPointerException on shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4219</link><project id="" key="" /><description>With `index.fielddata.cache` property set to `soft`, I receive the following NullPointerException on `close()` of an embedded elasticsearch node.

[INFO] [es] stopping ...
Nov 19, 2013 4:17:33 PM org.elasticsearch.common.cache.LocalCache processPendingNotifications
WARNING: Exception thrown by removal listener
java.lang.NullPointerException
        at org.elasticsearch.index.fielddata.IndexFieldDataCache$FieldBased.onRemoval(IndexFieldDataCache.java:89)
        at org.elasticsearch.common.cache.LocalCache.processPendingNotifications(LocalCache.java:1952)
        at org.elasticsearch.common.cache.LocalCache$Segment.runUnlockedCleanup(LocalCache.java:3455)
        at org.elasticsearch.common.cache.LocalCache$Segment.postWriteCleanup(LocalCache.java:3431)
        at org.elasticsearch.common.cache.LocalCache$Segment.remove(LocalCache.java:3102)
        at org.elasticsearch.common.cache.LocalCache.remove(LocalCache.java:4168)
        at org.elasticsearch.common.cache.LocalCache$LocalManualCache.invalidate(LocalCache.java:4747)
        at org.elasticsearch.index.fielddata.IndexFieldDataCache$FieldBased.onClose(IndexFieldDataCache.java:143)
        at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:319)
        at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:312)
        at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:113)
        at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:231)
        at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:350)
        at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:231)
        at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:111)
        at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
        at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:253)
        at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearcher.release(RobinEngine.java:1483)
        at org.elasticsearch.search.internal.SearchContext.release(SearchContext.java:226)
        at org.elasticsearch.search.SearchService.freeContext(SearchService.java:508)
        at org.elasticsearch.search.SearchService.doStop(SearchService.java:137)
        at org.elasticsearch.common.component.AbstractLifecycleComponent.stop(AbstractLifecycleComponent.java:105)
        at org.elasticsearch.node.internal.InternalNode.stop(InternalNode.java:261)
        at org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:276)
        at org.n3integration.ElasticRepository.close(ElasticRepository.java:122)
        at org.n3integration.AnalysisService.shutDown(AnalysisService.java:116)
        at com.google.common.util.concurrent.AbstractExecutionThreadService$1$1.run(AbstractExecutionThreadService.java:65)
        at java.lang.Thread.run(Thread.java:722)
</description><key id="22996508">4219</key><summary>NullPointerException on shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">n3integration</reporter><labels /><created>2013-11-20T15:03:45Z</created><updated>2013-11-21T15:47:51Z</updated><resolved>2013-11-21T15:47:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-20T15:13:57Z" id="28896857">which version are you using? also, prefer not to set `index.fielddata.cache` to `soft`, instead, set `indices.fielddata.cache.size` to bound it by size.
</comment><comment author="n3integration" created="2013-11-20T16:05:10Z" id="28901838">version number is helpful....we are currently running 0.90.5.
</comment><comment author="kimchy" created="2013-11-20T17:35:51Z" id="28910611">pushed a possible fix: d8dfbfd5de007c96a6ac18391a43640c3a0ebcb0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactored delete index api to make use of the new recently introduced generic ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4218</link><project id="" key="" /><description>Refactored delete index api to make use of the new recently introduced generic ack mechanism

Side note: the double notification from each node (one after deleting from metadata, one after deleting from file system) that was in place is not needed anymore, as each node returns the ack in the clusterStateProcessed, which gets called after all the listeners (one of which actually deletes from file system)

Closes #4217
</description><key id="22987263">4218</key><summary>Refactored delete index api to make use of the new recently introduced generic ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-20T12:02:56Z</created><updated>2014-06-15T00:20:55Z</updated><resolved>2013-12-11T21:09:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-21T12:20:39Z" id="28979409">some very minor comments LGTM +1 to push in the next round
</comment><comment author="javanna" created="2013-11-21T21:02:34Z" id="29024091">Talked to @kimchy and we decided to wait a bit with this change, as it would give us less flexibility, which we might need in the specific case of the delete index.

The delete index requires two actions: 
1) the cluster state update, that removes the index from the cluster state
2) the actual deletion of the index from file system

The proposed change would work fine with the current implementation, but relies on the fact that the deletion of the files (second action) happens in a synchronous manner through a cluster state listener, before each node gets back to the cluster state publish received from the master. If we are ever going to change this (e.g. make the files deletion async), a second notification (what currently happens) from the data nodes (note: not from all the nodes, but onlly from the ones that hold data) would be required again.
</comment><comment author="javanna" created="2013-12-11T21:09:09Z" id="30364178">Closing this PR as we decided not to merge it for the reason stated above. I'll see if I can manage to make use of the new ack mechanism only for the first part (cluster state update) and leave a separate notifcation for the files deletion, will do it eventualy on another PR.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move delete index api to new acknowledgement mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4217</link><project id="" key="" /><description>Move delete index api to new acknowledgement mechanism introduced in #3786 .
</description><key id="22987102">4217</key><summary>Move delete index api to new acknowledgement mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label></labels><created>2013-11-20T11:59:42Z</created><updated>2015-05-28T09:10:42Z</updated><resolved>2015-05-28T09:10:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to install fr.pilato.elasticsearch.river/fsriver/0.3.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4216</link><project id="" key="" /><description>D:\elasticsearch-0.90.6\bin&gt;plugin -install fr.pilato.elasticsearch.river/fsrive
r/0.3.0
-&gt; Installing fr.pilato.elasticsearch.river/fsriver/0.3.0...
Trying http://download.elasticsearch.org/fr.pilato.elasticsearch.river/fsriver/f
sriver-0.3.0.zip...
Trying http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/ri
ver/fsriver/0.3.0/fsriver-0.3.0.zip...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/fr/p
ilato/elasticsearch/river/fsriver/0.3.0/fsriver-0.3.0.zip...
Trying https://github.com/fr.pilato.elasticsearch.river/fsriver/archive/v0.3.0.z
ip...
Trying https://github.com/fr.pilato.elasticsearch.river/fsriver/archive/master.z
ip...
Failed to install fr.pilato.elasticsearch.river/fsriver/0.3.0, reason: failed to
 download out of all possible locations..., use -verbose to get detailed informa
tion

I am not able to install fsriver plugin. what could be the problem and solution?
</description><key id="22985789">4216</key><summary>Failed to install fr.pilato.elasticsearch.river/fsriver/0.3.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">abhiage</reporter><labels /><created>2013-11-20T11:29:14Z</created><updated>2013-11-21T06:03:31Z</updated><resolved>2013-11-20T12:13:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-20T12:13:13Z" id="28884641">You should first use the mailing list before raising an issue.
In that case, I suggest that you open the issue in the plugin project itself not in elasticsearch core project.

That said, it works fine on my end:

```
-&gt; Installing fr.pilato.elasticsearch.river/fsriver/0.3.0...
Trying http://download.elasticsearch.org/fr.pilato.elasticsearch.river/fsriver/fsriver-0.3.0.zip...
Trying http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/fsriver/0.3.0/fsriver-0.3.0.zip...
Downloading ..........................................................................................................................................................................DONE
Installed fr.pilato.elasticsearch.river/fsriver/0.3.0 into /Users/dpilato/Documents/Elasticsearch/Talks/workshop/run/elasticsearch-0.90.6/plugins/fsriver
```

You are probably behind a firewall or something so you need to define a JVM proxy or something.
Or you can download it manually from your browser using http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/fsriver/0.3.0/fsriver-0.3.0.zip and then install it as a file.

See [Plugin documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#installing)
</comment><comment author="abhiage" created="2013-11-21T06:03:31Z" id="28960191">Thank you . Actually its my first issue/post on github.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removing EOL client rubberband and adding official php client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4215</link><project id="" key="" /><description>- Removing EOL client rubberband ( see https://github.com/grantr/rubberband )
- Adding official php client

Update 1:
- Fix typo
- Add some more clients
</description><key id="22979308">4215</key><summary>Removing EOL client rubberband and adding official php client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels /><created>2013-11-20T09:14:03Z</created><updated>2014-06-20T03:30:45Z</updated><resolved>2014-04-07T12:22:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>bulkRequest.execute().actionGet() Does not return</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4214</link><project id="" key="" /><description>Hi

  I was trying some load tests using the Bulk API.  I created a situation where i am reading from a large file (with individual json objects) and forming separate Bulkrequests.  It so happens that the source file has a large number of json records - 2976800 to be precise.  I increased my heap settings to ensure that the program and also elasticsearch does not run out of memory.  

 On execution, the java program reaches the below line and does not proceed further
                BulkResponse bulkResponse = bulkRequest.execute().actionGet();

However when i queried the _count endpoint for that index, i already see that it has indexed 2967800 records.  Here's the output of _count endpoint

{
  "count" : 2976800,
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  }

Does the above mean that the documents have already been indexed?  If so, why does  bulkRequest.execute().actionGet() not return.  I looked into some older issues; looks like it might be related to Issue #1839

  I understand i can probably chunk my requests using BulkProcessor.  I am going to try that; just wanted to check if there is some issue with the base BulkRequest.execute()
</description><key id="22967236">4214</key><summary>bulkRequest.execute().actionGet() Does not return</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">madheshr</reporter><labels /><created>2013-11-20T02:30:49Z</created><updated>2014-08-08T18:36:29Z</updated><resolved>2014-08-08T18:36:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-20T09:31:54Z" id="28873800">Hi,

If the count returns the right number it means that the document were indexed on at least some of the shards but potentially not on all replicas. ES uses sync replication and an indexing request will return when the data is stored safely according to your replication settings.

While this obviously abuses the system (and yes BulkProcessor is the way to go and will probably be faster as well because of having to allocate so much memory and GC it), I'd like to figure you why things hang for you as they shouldn't.

Can you please send the code you are using (or a small recreation if the code is long). I would also like to know the version of ES you are using plus the index settings (number of shards &amp; replicas etc.)?
</comment><comment author="kimchy" created="2013-11-20T16:38:00Z" id="28905061">also, were there any exception that you maybe saw in your code? for example, and OOM?
</comment><comment author="madheshr" created="2013-11-20T18:37:26Z" id="28915793">Hi
Thanks for the quick response.

@bleskes:  While i would expect some kind of delay in replication, in my scenario i had set number of replicas to 0. I also set the refresh interval to -1 (for the initial bulk index).  My idea was to change it later.  Is this causing the problem?

@kimchy:  No, i did not see any OOM or other exception.  The code prints the Start timestamp as expected and it just hangs.

 Here's the piece of code.  My elasticsearch version is
Version: 0.90.5, Build: c8714e8/2013-09-17T12:50:20Z, JVM: 1.6.0_65.  Also the size of each document is very small  (only in the range of 400 - 500 Bytes)

```
public static void main(String[] args)
{

    try
    {

        // Initialization 
        prop.load(DataLoader.class.getClassLoader().getResourceAsStream(
            "config.properties"));
        clusterName = prop.getProperty("CLUSTER_NAME");
            hostName = prop.getProperty("HOST_NAME");
            portNo = Integer.parseInt(prop.getProperty("PORT_NO"));

        dateFormat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss.SSS");


        Settings settings = ImmutableSettings
            .settingsBuilder()
            .put("cluster.name",clusterName)
            .build();
         client = new TransportClient(settings)
           .addTransportAddress(new InetSocketTransportAddress(hostName,portNo));

        // Building the index with specific settings
        Builder settings = ImmutableSettings.settingsBuilder()
                .put("index.number_of_shards", 10)
                   .put("index.number_of_replicas", 0)
                   .put("refresh_interval",-1);

            CreateIndexRequestBuilder cirb = client.admin().indices().prepareCreate(prop.getProperty("INDEX_NAME"));
            cirb.setSettings(settings);
            CreateIndexResponse createIndexResponse = cirb.execute().actionGet();

            //if (!createIndexResponse.isAcknowledged()) throw new Exception("Could not create index");

        if(args.length&gt;0)
            fileName = args[0];
        else
            fileName = prop.getProperty("INPUT_FILE");
    } catch (IOException e)
    {
        // TODO Auto-generated catch block
        System.out.println("Error initializing..check property file / classpath");
    } catch (Exception e)
    {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
    try
    {       
            System.out.println("Trying to read From File    "+fileName);
            reader = new BufferedReader(new FileReader(fileName));
            String line = null;
            while ((line = reader.readLine()) != null) 
            {
                //System.out.println(line);
                // Forming the bulk request
                bulkRequest.add(client.prepareIndex(prop.getProperty("INDEX_NAME"),prop.getProperty("INDEX_TYPE"))   
                            .setSource(line));
            }

            System.out.println("Number of requests in Bulk batch = "+bulkRequest.numberOfActions());
            System.out.println("START TIME:  "+dateFormat.format(new Date()));

            BulkResponse bulkResponse = bulkRequest.execute().actionGet();
            if (bulkResponse.hasFailures()) {

                // process failures by iterating through each bulk response item
                System.out.println("RESPONSE ERRORS");
            }
            System.out.println("END   TIME:  "+dateFormat.format(new Date()));
            System.out.println("All done");

            //changeIndexSettings();

    } 
    catch(ActionRequestValidationException ex)
    {
        System.out.println("Validation Exception"+ex.getMessage());

    }
    catch (IOException ioe) 
    {
        System.err.println("oops " + ioe.getMessage());
    }
    catch(Exception e)
    {
        System.out.println("Exception occured.....");
        e.printStackTrace();

    }
    finally
    {
        try
        {

            reader.close();
            client.close();

        } catch (IOException e)
        {
            // TODO Auto-generated catch block
            System.out.println("File Error");
            e.printStackTrace();
        }
    }
}
```
</comment><comment author="madheshr" created="2013-11-20T18:45:24Z" id="28916492">Hi

 I have posted a detailed response on the Github issues page itself.  Hope
that works better.

Thanks
Madhesh

On Wed, Nov 20, 2013 at 8:38 AM, Shay Banon notifications@github.comwrote:

&gt; also, were there any exception that you maybe saw in your code? for
&gt; example, and OOM?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4214#issuecomment-28905061
&gt; .
</comment><comment author="bleskes" created="2013-11-22T12:35:02Z" id="29069253">Hi @madheshr 

Thanks for the code. I tried to reproduce both on 0.90.5 and current version but failed. I added a test to our testing infrastructure, so it will keep on trying for us.

Before diving deeper, can I ask to try it again with 0.90.7? We've done some work on shard allocation and error handling in that area and I would like to double check this isn't already fixed.

If it fails for you, can I ask you for the data file you are using? Maybe the content of the document is important. Also, how many nodes are you using?

Cheers,
Boaz
</comment><comment author="clintongormley" created="2014-08-08T18:36:29Z" id="51641087">No feedback since last year. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/document/BulkTests.java</file></files><comments><comment>Added a bulk indexing while initializing test.</comment></comments></commit></commits></item><item><title>Feature external version 'force' option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4213</link><project id="" key="" /><description>Many times it would have been useful to force the update of a document ignoring the version when the version type is external. This is useful when trying to keep the external version in sync with the db optimistic lock version.
</description><key id="22965031">4213</key><summary>Feature external version 'force' option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">frazerh</reporter><labels><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-11-20T01:27:30Z</created><updated>2014-03-21T10:06:14Z</updated><resolved>2014-03-10T20:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-20T09:22:11Z" id="28873287">Hi Frazerh,

We could build something like this but I want to understand the use case a bit more to make sure we do the proper thing. Currently the external versioning will update a document only if the incoming version is higher than the stored one. I assume that still holds if you use optimistic locking to update your db and ES is updated once the db update succeeds? 
</comment><comment author="frazerh" created="2013-11-21T00:52:37Z" id="28948643">Hey @bleskes, yes the above still holds true. On occasion we just need to be able to override this and force and index ignoring the external version.
</comment><comment author="bleskes" created="2013-11-21T08:26:25Z" id="28965398">K. Fair enough. I'll look at the implications of allowing you to force set the version number. Thanks for clarifying.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteRequest.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/index/TransportShardDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>src/main/java/org/elasticsearch/index/VersionType.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/test/java/org/elasticsearch/index/VersionTypeTests.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/versioning/SimpleVersioningTests.java</file></files><comments><comment>Introduced VersionType.FORCE &amp; VersionType.EXTERNAL_GTE</comment></comments></commit></commits></item><item><title>Prevent modification or deletion of repositories while snapshots are running</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4212</link><project id="" key="" /><description>It shouldn't be possible to delete or modify parameters of a repository that is currently in use by one of the running snapshots.
</description><key id="22963674">4212</key><summary>Prevent modification or deletion of repositories while snapshots are running</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-11-20T00:54:15Z</created><updated>2014-07-16T21:51:14Z</updated><resolved>2013-11-22T01:51:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-21T22:39:52Z" id="29031920">LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.90.5 ClassNotFoundException[org.elasticsearch.common.UUID] when starting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4211</link><project id="" key="" /><description>Trying to start dedicated master nodes with the ES service wrapper and Zookeeper discovery (not sure if that part's relevant).

```
STATUS | wrapper  | 2013/11/19 13:31:26 | --&gt; Wrapper Started as Daemon
STATUS | wrapper  | 2013/11/19 13:31:26 | Java Service Wrapper Community Edition 64-bit 3.5.14
STATUS | wrapper  | 2013/11/19 13:31:26 |   Copyright (C) 1999-2011 Tanuki Software, Ltd. All Rights Reserved.
STATUS | wrapper  | 2013/11/19 13:31:26 |     http://wrapper.tanukisoftware.com
STATUS | wrapper  | 2013/11/19 13:31:26 |
STATUS | wrapper  | 2013/11/19 13:31:26 | Launching a JVM...
INFO   | jvm 1    | 2013/11/19 13:31:27 | WrapperManager: Initializing...
INFO   | jvm 1    | 2013/11/19 13:31:31 | {0.90.5}: Startup Failed ...
INFO   | jvm 1    | 2013/11/19 13:31:31 | - NoClassDefFoundError[org/elasticsearch/common/UUID]
INFO   | jvm 1    | 2013/11/19 13:31:31 |       ClassNotFoundException[org.elasticsearch.common.UUID]
STATUS | wrapper  | 2013/11/19 13:31:33 | &lt;-- Wrapper Stopped
```
</description><key id="22947521">4211</key><summary>0.90.5 ClassNotFoundException[org.elasticsearch.common.UUID] when starting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devoncrouse</reporter><labels><label>non-issue</label></labels><created>2013-11-19T20:43:49Z</created><updated>2013-11-19T20:55:07Z</updated><resolved>2013-11-19T20:52:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-19T20:52:16Z" id="28834099">The UUID class has been removed in `0.90.4` I think the plugin you are using hence zookeeper plugin doesn't work with the new version then.
</comment><comment author="devoncrouse" created="2013-11-19T20:53:53Z" id="28834380">Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>has_child Query scores Adversely Affected by filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4210</link><project id="" key="" /><description>I am encountering some extremely confusing behavior when employing a has_child query and a filter together on ElasticSearch 0.90.5. Our data involves records of US court cases (type "case") with child documents representing entries in the case's docket (type "docket-entry"). I've run a number of experiments to try and wrap my head around what's going on here. The TLDR version is this:

Despite `filtered` claiming that filters do not affect the score, scoring of a query including a `has_child` clause is adversely affected by filters.

To demonstrate the issue, I created and tested a set of filters that match _both_ every case and every docket entry in our database. This was to confirm that the problem was not something simple and silly, like the filters clause applying to the child documents and filtering them in some undesirable fashion. I obtained total document counts using:

``` json
{
  "query": {
    "match_all": {}
  }
}
```

This produced: 150662 cases and 7290248 docket entries.

I then ran my "match everything" filter:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "or": [
          {
            "range": {
              "last_docket_id": {
                "gte": "1"
              }
            }
          },
          {
            "missing": {
              "field": "last_docket_id"
            }
          }
        ]
      }
    }
  }
}
```

Where `last_docket_id` is a field that every case should have (as a 1-or-higher) and no docket entry should have. This also produced 150662 cases and 7290248 docket entries.

Using this data, I ran some test queries. First, a query involving a `has_child` without any filter attached:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "apple samsung",
                "default_operator": "AND"
              }
            },
            {
              "has_child": {
                "type": "docket-entry",
                "query": {
                  "query_string": {
                    "default_operator": "AND",
                    "query": "apple samsung"
                  }
                },
                "score_type": "sum"
              }
            }
          ]
        }
      }
    }
  }
}
```

This produced the expected results: 505 total documents and a max score of 420.29727, with the first page of results being highly relevant. I then applied my "match everything" filter:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "apple samsung",
                "default_operator": "AND"
              }
            },
            {
              "has_child": {
                "type": "docket-entry",
                "query": {
                  "query_string": {
                    "default_operator": "AND",
                    "query": "apple samsung"
                  }
                },
                "score_type": "sum"
              }
            }
          ]
        }
      },
      "filter": {
        "or": [
          {
            "range": {
              "last_docket_id": {
                "gte": "1"
              }
            }
          },
          {
            "missing": {
              "field": "last_docket_id"
            }
          }
        ]
      }
    }
  }
}
```

The degradation in results was immediately apparent. Not only did the `max_score` drop to 99.71274 and the quality of the first page of results decline to uselessness, the total documents dropped to 223 and randomly fluctuated with repeated runs of the query.

To confirm, I isolated the two segments of the query and ran the same experiments in isolation:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "apple samsung",
                "default_operator": "AND"
              }
            }
          ]
        }
      }
    }
  }
}
```

Returned 60 results with a `max_score` of 1.5968025

``` json
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "apple samsung",
                "default_operator": "AND"
              }
            }
          ]
        }
      },
      "filter": {
        "or": [
          {
            "range": {
              "last_docket_id": {
                "gte": "1"
              }
            }
          },
          {
            "missing": {
              "field": "last_docket_id"
            }
          }
        ]
      }
    }
  }
}
```

Returned the same - 60 results, 1.5968025 `max_score`, which is what I would expect given the documentation. The `has_child` half, on the other hand, is disastrous:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "has_child": {
                "type": "docket-entry",
                "query": {
                  "query_string": {
                    "default_operator": "AND",
                    "query": "apple samsung"
                  }
                },
                "score_type": "sum"
              }
            }
          ]
        }
      }
    }
  }
}
```

Starts with 498 hits and a `max_score` of 419.9289, but adding the filter:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "has_child": {
                "type": "docket-entry",
                "query": {
                  "query_string": {
                    "default_operator": "AND",
                    "query": "apple samsung"
                  }
                },
                "score_type": "sum"
              }
            }
          ]
        }
      },
      "filter": {
        "or": [
          {
            "range": {
              "last_docket_id": {
                "gte": "1"
              }
            }
          },
          {
            "missing": {
              "field": "last_docket_id"
            }
          }
        ]
      }
    }
  }
}
```

Drops us to 171 hits with a `max_score` of 99.37268.

At this point, my only conclusion is that either there's something about the has_child query that I'm completely failing to understand or that there's a bug in ElasticSearch.
</description><key id="22947038">4210</key><summary>has_child Query scores Adversely Affected by filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">npilon</reporter><labels /><created>2013-11-19T20:37:15Z</created><updated>2013-12-19T00:53:38Z</updated><resolved>2013-12-18T23:27:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-20T12:44:36Z" id="28886153">@npilon Just focusing in your last two queries. If the first query (has_child wrapped in bool query) returned 498 hits and then if you add your "all" filter to it should produce the same amount of hits. Assuming that the "all" filter really returns all document then this is strange. Are you able to reproduce this issue on a small scale? If so can you share it here, so we can further investigate?
</comment><comment author="npilon" created="2013-11-20T17:26:35Z" id="28909702">&gt; Assuming that the "all" filter really returns all document then this is strange

Do you have reason to believe it doesn't, given my first two tests?

&gt; Are you able to reproduce this issue on a small scale? If so can you share it here, so we can further investigate?

Working on this now.
</comment><comment author="npilon" created="2013-11-21T00:33:21Z" id="28947667">So, I've got an index containing only the documents from this query running on a single-node cluster.

Adding the filter causes three returned results to drop off, but does not create the drastic swings in score or returned results observed in our production environment. (Where, I should add, we build a new index every six hours and swap)

Do you have suggestions for other things I should add or incorporate to investigate this issue further? IE, should I try adding more nodes (we have three in our production environment), or more (unrelated) documents? What else should I do?
</comment><comment author="npilon" created="2013-11-21T17:39:38Z" id="29005456">Some further information: we recently migrated from ElasticSearch 0.90.1 to 0.90.5. I don't believe this problem occurred in 0.90.1; if we decide to revert, I will provide information about the result.
</comment><comment author="npilon" created="2013-11-22T00:53:03Z" id="29040143">While further testing will have to wait until I'm able to switch our web application over to the new cluster, I can confirm that, at first glance, this bug does not exist in ElasticSearch 0.90.2.
</comment><comment author="martijnvg" created="2013-11-22T17:49:22Z" id="29093803">Can you try to run the has_child query out of the filtered query and move it a layer up? Something like this?:

``` json
{
    "query": {
        "bool": {
            "must": [
               {
                   "has_child": {
                        "type": "docket-entry",
                        "query": {
                          "query_string": {
                            "default_operator": "AND",
                            "query": "apple samsung"
                          }
                        },
                        "score_type": "sum"
                    }
               },
               {
                  "filtered": {
                     "filter": {
                         "or": [
                              {
                                "range": {
                                  "last_docket_id": {
                                    "gte": "1"
                                  }
                                }
                              },
                              {
                                "missing": {
                                  "field": "last_docket_id"
                                }
                              }
                            ]
                     }
                  }
               }
            ]
        }
    }
}
```

I'm wondering if the issue then still occurs.
</comment><comment author="npilon" created="2013-11-27T01:36:33Z" id="29353585">So, on a newly-rebuilt index:

Your query: 497 results, 421.73798 max_score

Filtered has_child query with noop filter: 149 results, 421.63016 max_score

Filtered has_child query with no filter: 497 results, 421.63016 max_score
</comment><comment author="npilon" created="2013-11-27T01:43:25Z" id="29353834">As I noted in https://github.com/elasticsearch/elasticsearch/issues/4210#issuecomment-28947667 we rebuild our index every six hours. This has made diagnosing this problem extremely frustrating, as the results of "has_child query with filter" searches are very variable. They always return too few results, but the overall quality of the first page of results varies hugely. Further, sometimes the number of returned results varies between identical searches on the same index, which was how we first noticed the problem. For example, sometimes the "filtered has_child query with noop filter" on 0.90.5 returns only 146 results. Both your query and the "filtered has_child query with no filter" return stable total results.
</comment><comment author="martijnvg" created="2013-11-27T20:56:21Z" id="29419484">@npilon This really smells like a bug. I'm unsure what is causing this bug. You can confirm that `filtered has_child query with noop filter` produces expected results in ES version `0.90.2` (beyond the first glance)?
</comment><comment author="npilon" created="2013-11-27T21:01:19Z" id="29419772">Yes, 0.90.2 produces expected results for all test cases in this issue. I haven't tested other versions beyond those two but can do so; are there any you'd be especially interested in seeing results for?
</comment><comment author="martijnvg" created="2013-11-27T21:26:32Z" id="29421465">Can you try with version `0.90.3`? I suspect that #3190 maybe causing this issue.
Also when you're importing data, do you also execute delete or update requests?
</comment><comment author="npilon" created="2013-11-27T21:31:45Z" id="29421845">Not on the index we're creating. Our process - which is, I'm aware, possibly pretty weird, is:
1. Create a new index and use _bulk calls to populate it.
2. Swap the live alias over and close the old index.
3. Delete all but the most recent closed index.
4. Execute live updates - again using _bulk - against the new live index.

I don't believe we ever use delete to drop documents from an index. We're mostly a data warehousing-style application, and delete from our source database so rarely that we never bother to try to "propagate" the deletes to our ES cluster. The 0.90.5 cluster I'm testing against in https://github.com/elasticsearch/elasticsearch/issues/4210#issuecomment-29353585 has never had steps 2 through 4 used on it.
</comment><comment author="npilon" created="2013-11-27T21:48:04Z" id="29422957">I'll test against 0.90.3.
</comment><comment author="npilon" created="2013-11-28T00:01:55Z" id="29430390">On 0.90.3, the filtered has_child query with noop filter returns either 206 or 211 results.

The filterless has_child query returns 499 and seems stable.
</comment><comment author="martijnvg" created="2013-11-28T13:43:37Z" id="29464285">Ok, that is good to know. Can you try setting `short_circuit_cutoff` to `0` on the `has_child` query, run the `filtered has_child query with noop filter` again and see if the issue still occurs on 0.90.5? Something like this:

```
"has_child": {
    "type": "docket-entry",
    "short_circuit_cutoff" : 0,
    "query": {
      "query_string": {
        "default_operator": "AND",
        "query": "apple samsung"
      }
    },
    "score_type": "sum"
}
```
</comment><comment author="martijnvg" created="2013-12-02T09:18:16Z" id="29603513">@npilon I've added a fix via #4210 that fixes inconsistent results if the has_child query is used in the filtered query as a query.
</comment><comment author="martijnvg" created="2013-12-02T16:43:19Z" id="29633900">@npilon The fix I mentioned in my previous comment has been included in the just released 1.0 beta2, if you can it would be great if you can verify if your issue has been resolved with the latest release.
</comment><comment author="npilon" created="2013-12-02T23:34:28Z" id="29669687">I'm afraid 1.0 beta2 doesn't resolve the problem; in fact, it seems to make it worse. All results below are on 1.0 beta2 with a freshly-rebuilt index.

The "filtered has_child query with no filter" returns 408 results; with short circuit cutoff, I get 498.

The "filtered has_child query with noop filter" returns 59 results; with short circuit cutoff I get 499.

After the cluster updates to include one replica, the results change as follows:

The "filtered has_child query with no filter" returns 418 or 422 results; with short circuit cutoff, I get 498.

The "filtered has_child query with noop filter" returns 59 results; with short circuit cutoff I get 499.
</comment><comment author="martijnvg" created="2013-12-03T09:27:35Z" id="29694601">Too bad that this didn't fixed the inconsistent results. Just to be sure, the correct results should be: 498? Also when you say "with short circuit cutoff", you have added `"short_circuit_cutoff" : 0` to the query?
</comment><comment author="npilon" created="2013-12-03T17:27:35Z" id="29731324">Yes.
</comment><comment author="npilon" created="2013-12-18T23:27:28Z" id="30891826">I can confirm this has been fixed by 0.90.8. Thank you very much! :+1: 
</comment><comment author="martijnvg" created="2013-12-19T00:53:38Z" id="30896594">@npilon Thanks for providing the pointers that lead to finding this sneaky bug!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a field data based TermsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4209</link><project id="" key="" /><description>Add a terms filter that compares terms out of
the fielddata cache. When filtering on a large
set of terms this filter can be considerably faster
than using a standard lucene terms filter.

Add the "fielddata" execution mode to the
terms filter parser to enable the use of
the new FieldDataTermsFilter.

see #4181 
</description><key id="22933828">4209</key><summary>Add a field data based TermsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-19T18:12:05Z</created><updated>2013-11-19T18:32:09Z</updated><resolved>2013-11-19T18:32:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/search/FieldDataTermsFilter.java</file><file>src/test/java/org/elasticsearch/index/search/FieldDataTermsFilterTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Add a field data based TermsFilter</comment></comments></commit></commits></item><item><title>XBooleanFilter changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4208</link><project id="" key="" /><description>- Reordered logic in XBooleanFilter to support notion of a strategy on how to best execute the clauses.
- Added the or and and filter as a strategy to XBooleanFilter.
- Added minimum should match support to XBooleanFilter.
- Made sure that all places where XBooleanFilter is used, that minimum_should_match=1 in order to maintain the bw comp. with the previous version of this filter (forced implicit minimum_should_match=1).
</description><key id="22933172">4208</key><summary>XBooleanFilter changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-19T18:02:08Z</created><updated>2015-05-18T23:33:39Z</updated><resolved>2014-04-07T10:51:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Doc values support for geo points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4207</link><project id="" key="" /><description>We should add doc values support for geo points, probably via BinaryDocValues.

Similarly to #3993, the challenge is in the mappers, since the mapper needs to know all field values in order to be able to create the field instance (because there can be a single BinaryDocValues instance per document per field).

Another open question is about the encoding: storing two doubles (16 bytes) per point is probably wasteful, should we use instead a different encoding that would be more compact while keeping precision good enough? Since the range of possible values is fixed, I'm thinking that a fixed-size encoding that would map [-180,180] into [0, 2^&lt;sup&gt;bits_per_value&lt;/sup&gt;] would be rather efficient. For example, if my math is correct, a `bits_per_value` of 24 (62.5% reduction) could give a precision of ~5m, a `bits_per_value` of 32 (50% reduction) would give a precision of ~20mm, and a `bits_per_value` of 40 (37.5% reduction) would give a precision &lt;&lt; 1mm.
</description><key id="22931121">4207</key><summary>Doc values support for geo points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v1.0.0.RC1</label></labels><created>2013-11-19T17:32:39Z</created><updated>2013-12-27T11:52:22Z</updated><resolved>2013-12-27T11:52:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-19T17:35:47Z" id="28813334">I think this is desperately needed. The general assumption of reduced precsion could also be used in the in-memory field data version! +1 to explore the possibilities here!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file></files><comments><comment>Doc values for geo points.</comment></comments></commit></commits></item><item><title>Cluster reroute api should have a way to assign all unassigned shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4206</link><project id="" key="" /><description>Sometimes, during a disaster, you end up with shards stuck unassigned and you just want them reassigned regardless of data loss.  It'd be great if the cluster reroute api supported an assign_all action that just assigns all the shards to free nodes.  It'd need to also support `allow_primary`.  Without this you need some nasty shell script like this:

``` bash
function reroute() {
    curl -XPOST 'localhost:9200/_cluster/reroute?pretty' -d '{
        "commands" : [ {
                "allocate" : {
                    "index" : "'$1'",
                    "shard" : '$2'
                    "allow_primary" : true,
                    "node" : "&lt;node&gt;"
                }
            }
        ]
    }' &gt; /dev/null
    sleep 1
}
curl -s localhost:9200/_cluster/state?pretty | awk '
    BEGIN {more=1}
    {if (/"UNASSIGNED"/) unassigned=1}
    {if (/"routing_nodes"/) more=0}
    {if (unassigned &amp;&amp; /"shard"/) shard=$3}
    {if (more &amp;&amp; unassigned &amp;&amp; /"index"/) {print "reroute",$3, shard; unassigned=false}}
' &gt; runit
source runit
```

I'm sure there are better ways to write that script, but great code doesn't typically come in the middle of disasters.
</description><key id="22925761">4206</key><summary>Cluster reroute api should have a way to assign all unassigned shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-11-19T16:24:36Z</created><updated>2014-10-10T10:43:28Z</updated><resolved>2014-10-10T10:43:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-11-27T17:27:34Z" id="29403140">Pull request is to assign all unassigned primary shards.  That would have worked for me in my disaster.
</comment><comment author="billzhuang" created="2014-04-14T06:30:38Z" id="40336529">it,s great, just want to know when ES can merge @nik9000 PR.
</comment><comment author="joestump" created="2014-04-18T15:58:00Z" id="40819610">This pull request just saved my butt! Thanks @nik9000. Looking forward to #4285 being pulled in.
</comment><comment author="damm" created="2014-05-23T01:24:11Z" id="43962645">@nik9000 I forgot to thank you so very much for this pull request.  I can't name how many times it's saved me time and effort.
</comment><comment author="gnowxilef" created="2014-08-01T08:21:46Z" id="50860602">+1 merge please
</comment><comment author="clintongormley" created="2014-10-10T10:43:28Z" id="58639659">We want to make progress on this, but the PR seems to provide all the required info.  I'll close this issue in favour of the PR: #4285 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[doc]add elasticsearch-extended-analyze plugin line to doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4205</link><project id="" key="" /><description>I release extended-analyze plugin.
Please merge plugins pages my plugin entry.
</description><key id="22925494">4205</key><summary>[doc]add elasticsearch-extended-analyze plugin line to doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">johtani</reporter><labels /><created>2013-11-19T16:20:57Z</created><updated>2014-07-03T12:53:49Z</updated><resolved>2013-11-21T08:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-21T08:50:43Z" id="28966476">Hey,

Thanks for your contribution. just pushed your change at https://github.com/elasticsearch/elasticsearch/commit/7bbe45327308df6e8e981bcd79b199bd90917485

please go directly against master next time, as we usually back port from there
</comment><comment author="johtani" created="2013-11-21T08:57:05Z" id="28966777">thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Lat/Long are Reversed In GeoDistanceFilterBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4204</link><project id="" key="" /><description>The latitude and longitude values are being reversed by the GeoDistanceFilterBuilder when attempted to construct a search. I have had to do the following in my client code:

``` java
searchClient.prepareSearch("stores")
        .setTypes("store")
        .setQuery(QueryBuilders.matchAllQuery())
        .setFilter(FilterBuilders.geoDistanceFilter("location")
        .lat(longitude)          // BUG: These are reversed in the ES API
        .lon(latitude)
```

The issue is with the following line of code:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java#L125

Fixing this issue would however break the backwards compatibility of the API, if anyone happened to be relying on this functionality being reversed to get their application working.
</description><key id="22917540">4204</key><summary>Lat/Long are Reversed In GeoDistanceFilterBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darylrobbins</reporter><labels /><created>2013-11-19T14:43:47Z</created><updated>2013-11-19T14:52:25Z</updated><resolved>2013-11-19T14:52:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-19T14:45:27Z" id="28795065">Its not reversed, right? When using the array format, we conform to the geojson spec, and do `[lon, lat]`.
</comment><comment author="darylrobbins" created="2013-11-19T14:52:25Z" id="28795658">Well, that explains it. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Running the node stats api while a shard is moving onto the node logs an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4203</link><project id="" key="" /><description>Running the node stats api while a shard is moving onto the node logs this exception:

```
[2013-11-18 01:32:10,422][DEBUG][action.admin.cluster.node.stats] [elastic1002] failed to execute on node [sRNrZmovQIWTMO1WlDDulg]
org.elasticsearch.index.engine.EngineClosedException: [elwiki_content_1384450509][2] CurrentState[CLOSED] 
        at org.elasticsearch.index.engine.robin.RobinEngine.ensureOpen(RobinEngine.java:969)
        at org.elasticsearch.index.engine.robin.RobinEngine.segmentsStats(RobinEngine.java:1181)
        at org.elasticsearch.index.shard.service.InternalIndexShard.segmentStats(InternalIndexShard.java:509)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:154)
        at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:212)
        at org.elasticsearch.node.service.NodeService.stats(NodeService.java:165)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:100)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:43)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:146)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
```

I see that logging.yml logs actions at debug for easier debugging, but this one floods my logs when nodes move due to my monitoring.  I assume that it also stops stats from being returned which is also unfortunate.  I see it on 0.90.7.

I don't have a gist recreation right now.  Let me know if one would be useful.
</description><key id="22916561">4203</key><summary>Running the node stats api while a shard is moving onto the node logs an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-19T14:29:00Z</created><updated>2013-11-19T19:58:39Z</updated><resolved>2013-11-19T19:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-11-19T15:41:50Z" id="28800203">I was able to see this failure by running /_cluster/nodes/_local/stats?all=true repeatedly against a node that was receiving a shard.

It spits out this:

``` js
{
  "cluster_name": "NAME",
  "nodes": {}
}
```
</comment><comment author="kimchy" created="2013-11-19T17:52:19Z" id="28814871">thanks for reporting!, we should simply ignore it (the same way to automatically ignore those on index stats APIs), will push a fix soonish
</comment><comment author="nik9000" created="2013-11-19T19:58:39Z" id="28827061">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Removed exception handling in InternalIndexShard.docStats() &amp; storeStats()</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Removed exception handling in InternalIndexShard.completionStats()</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file></files><comments><comment>Running the node stats api while a shard is moving onto the node logs an exception</comment><comment>fixes #4203</comment></comments></commit></commits></item><item><title>Problem with parent id containing commas when talking to an index alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4202</link><project id="" key="" /><description>Hi,

I ran into problems trying to index a child document whose parent id contains commas. This only occurs using an index alias, with the index name it works. This is the resulting exception in 0.90.7:

```
ElasticSearchIllegalArgumentException: index/alias [parentid-testalias] provided with routing value [abc,2] that resolved to several routing values, rejecting operation
    at org.elasticsearch.cluster.metadata.MetaData.resolveIndexRouting(MetaData.java:338)
    at org.elasticsearch.action.index.IndexRequest.process(IndexRequest.java:557)
    ...
```

Here is some code to reproduce the problem:

```
Client client = new TransportClient().addTransportAddress(new InetSocketTransportAddress("localhost", 9300));

String indexName = "parentid-test";
String indexAlias = "parentid-testalias";
if (client.admin().indices().exists(new IndicesExistsRequest(indexName)).actionGet().isExists()) {
    client.admin().indices().delete(new DeleteIndexRequest(indexName)).actionGet();
}
CreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName);
createIndexRequest.mapping("chld", "{\"chld\":{\"_parent\":{\"type\":\"prnt\"}}}");
client.admin().indices().create(createIndexRequest).actionGet();
client.admin().indices().prepareAliases().addAlias(indexName, indexAlias).execute().actionGet();

client.prepareIndex(indexName, "prnt", "abc,1").setSource("{\"text\":\"parent abc,1\"}").execute().actionGet();
client.prepareIndex(indexName, "chld", "def,1").setParent("abc,1").setSource("{\"text\":\"child def,1\"}").execute().actionGet();

client.prepareIndex(indexAlias, "prnt", "abc,2").setSource("{\"text\":\"parent abc,2\"}").execute().actionGet();
// this causes the exception - indexAlias and comma in parent id
client.prepareIndex(indexAlias, "chld", "def,2").setParent("abc,2").setSource("{\"text\":\"child def,2\"}").execute().actionGet();

client.close();
```

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html tells me that this is intended behavior: "...search routing may contain several values separated by comma. Index routing can contain only a single value."

So, if this is how it should be, then I would suggest to add some documentation on invalid characters in IDs, as I didn't find anything regarding this. Any suggestions?
</description><key id="22907530">4202</key><summary>Problem with parent id containing commas when talking to an index alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">hkorte</reporter><labels /><created>2013-11-19T12:08:07Z</created><updated>2014-07-04T13:24:05Z</updated><resolved>2014-07-04T13:24:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-19T13:59:23Z" id="28791527">@hkorte I think this check that throws an error is only supposed to happen if you have a routing specified in the alias it self, not when someone is specifying it via routing or parent id. I think we can fix this, by only running this check if routing is resolved via the alias.
</comment><comment author="martijnvg" created="2013-11-19T14:01:26Z" id="28791670">But having a comma in the routing is tricky if it used at search time, maybe this should just be not allow a comma in the routing.
</comment><comment author="clintongormley" created="2014-07-04T13:24:05Z" id="48043428">Closing in favour of #6736 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow tweaking of primary allocations (ex excluding nodes/tags)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4201</link><project id="" key="" /><description>We have come up with a cluster config that allows us to backup the cluster from a single node in an off-site location using forced awareness. Our issue now is that we don't want  that off-site node to have primary shards.

As I can see there is currently no such option, but I think it would make sense, especially in the backup scenario.

Thank you
</description><key id="22901297">4201</key><summary>Allow tweaking of primary allocations (ex excluding nodes/tags)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ctrochalakis</reporter><labels /><created>2013-11-19T10:33:21Z</created><updated>2014-08-08T18:35:25Z</updated><resolved>2014-08-08T18:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-19T17:08:30Z" id="28809587">We stayed away from controlling where primary shards can be allocated compared to replicas, because then how would failover happen? A replica shard can't really exists without a primary shard existing at the same time.

It feels like its the wrong way to try and solve your backup requirements. Won't snapshot/restore feature in 1.0 help?
</comment><comment author="ctrochalakis" created="2013-11-20T10:28:01Z" id="28876984">I wasn't aware of the snapshot/restore feature, looks promising, we are gonna give it a try.

Regarding your comment on failover, In our setup we have 3 tag values with forced awareness, not 2. Let's say `dc1,dc2,backup` and we want to have primary shards only in `dc1,dc2` so failover can happen between `dc1` and `dc2`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Theadpool defaults in code do not match docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4200</link><project id="" key="" /><description>The defaults don't match the reference in several categories.

https://github.com/elasticsearch/elasticsearch/blob/f9ce791578ff103ce9af89fca1394513bc0150f7/src/main/java/org/elasticsearch/threadpool/ThreadPool.java

vs.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html
</description><key id="22877172">4200</key><summary>Theadpool defaults in code do not match docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jmwilson</reporter><labels /><created>2013-11-18T23:43:29Z</created><updated>2014-02-22T16:12:01Z</updated><resolved>2014-02-22T16:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-19T07:32:15Z" id="28771002">The link to the ThreadPool class on github is an older revision from 5 months ago.

Cant spot any huge differences with documentation and the current version, but I am running low on coffee at the moment. Pointers/Pull requests highly appreciated. Having documentation in sync is crucial, so thanks a lot for any help there!
</comment><comment author="spinscale" created="2014-02-22T16:12:01Z" id="35806301">closing due to lack of activity, happy to reopen if you spot something
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Java client] - FilterBuilder and QueryBuilder should throw ElasticSearchIllegalArgumentException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4199</link><project id="" key="" /><description>QueryBuilderException has been dropped in favor for throwing ElasticSearchIllegalArgumentException. The #buildAsBytes() methods now have `ElasticSearchException` in their signature.

This change only applies to the Java client.
</description><key id="22861835">4199</key><summary>[Java client] - FilterBuilder and QueryBuilder should throw ElasticSearchIllegalArgumentException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-18T19:43:31Z</created><updated>2013-11-18T19:48:29Z</updated><resolved>2013-11-18T19:47:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-18T19:47:18Z" id="28731124">Added to master (9c15b86b892d16c8c1865806091f06b27eda6930) and 0.90 (73ce47ced2b1f8f449a8e06104aea4c75f17372e)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Alias and terms filter with lookup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4198</link><project id="" key="" /><description>The lookup mechanism of the `terms` filter is not alias aware.

I have an index with multiple shards, and an alias on it with a routing key.
The `terms` filter using the lookup mechanism sais it uses the current index by default.
I expect it to work on the alias from the path I ran the query against, but since I get random failures, I suspect the filter of using the id as routing key (the default way) for the term lookup instead of the alias routing key.

By chance in my setup I know what index to provide, but if the query is run against multiple indices, there is no way to get the default behavior.
</description><key id="22856147">4198</key><summary>Alias and terms filter with lookup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2013-11-18T18:14:14Z</created><updated>2015-09-21T19:34:56Z</updated><resolved>2015-09-21T19:34:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-20T13:59:45Z" id="28890765">Right, If no lookup index has been specified it uses the resolved index instead. So if an alias is specified, then the index it points to is used for the terms lookup. 

I think it make sense to not use the resolved index, but the actual index or alias that was specified in the search request, so the alias support also work without specifying a lookup index. 
</comment><comment author="clintongormley" created="2015-09-21T19:34:56Z" id="142086791">Not sure when this was fixed, but it works in 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Making SearchRequest in PutWarmerRequest mandatory &amp; validated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4197</link><project id="" key="" /><description>The search request inside of a put warmer request was nullable, but actually we have to have that request in the transport action.
Validation and appropriate test added.

Closes #4196
</description><key id="22836475">4197</key><summary>Making SearchRequest in PutWarmerRequest mandatory &amp; validated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-18T14:10:47Z</created><updated>2014-06-18T22:16:42Z</updated><resolved>2013-11-19T14:06:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-18T14:39:59Z" id="28702456">+1
</comment><comment author="jpountz" created="2013-11-18T14:40:55Z" id="28702545">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Warmers API breaking when empty Warmup query is created</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4196</link><project id="" key="" /><description>Ran into this by accident, but basically if you add a warmup query with empty body, ES will accept it(even though I believe it shouldn't). 
After accepting this empty bodied query, it's not possible anymore to access warmup queries by regular exp. that would also match this query. Follows how to reproduce(on 0.90.8-SNAPSHOT)

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/bar/1 -d '{"id":1,"content":"one"}'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{"id":2,"content":"two"}'
curl -XPUT http://localhost:9200/foo/_warmer/warmer_1 -d '{"query": {"match_all":{}}}'
curl -XGET http://localhost:9200/foo/_warmer/w*
curl -XPUT http://localhost:9200/foo/_warmer/warmer_2
curl -XGET http://localhost:9200/foo/_warmer/w*
```

which yields:
{"error":"NullPointerException[null]","status":500}
</description><key id="22833107">4196</key><summary>Warmers API breaking when empty Warmup query is created</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-18T13:05:18Z</created><updated>2013-11-19T14:43:52Z</updated><resolved>2013-11-19T14:06:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-11-18T13:53:07Z" id="28698959">@spinscale i have some free time now. should i look into it or are you already doing it?
</comment><comment author="spinscale" created="2013-11-18T13:55:19Z" id="28699102">@lmenezes on it already... seems we should stop making the search request optional, when doing the validation.

thanks for your help, as usual :-)
</comment><comment author="lmenezes" created="2013-11-18T13:57:34Z" id="28699248">@spinscale np then. yep, i guess just rejecting empty requests would already solve the problem :)
thanks for the quick response!
</comment><comment author="lmenezes" created="2013-11-19T14:43:52Z" id="28794932">:+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestTests.java</file></files><comments><comment>Making SearchRequest in PutWarmerRequest mandatory &amp; validated</comment></comments></commit></commits></item><item><title>Refactoring IndicesAnalysisService to use enums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4195</link><project id="" key="" /><description>Using enums where possible in order to clean up the code in IndicesAnalysisService, also preparing those to support versioning, if wanted.

Also introduced a simpler more generic caching mechanism, and tests.
</description><key id="22832598">4195</key><summary>Refactoring IndicesAnalysisService to use enums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-18T12:55:32Z</created><updated>2014-07-16T21:51:16Z</updated><resolved>2013-11-20T11:24:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-19T11:19:42Z" id="28782319">LGTM :)
</comment><comment author="spinscale" created="2013-11-19T12:03:43Z" id="28784576">FYI: I plan to go on with the refactoring and see if we can get rid of some of the `FactoryFactory` factories after this is merged in...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSearch: - On migrating  from one network to another network elasticsearch is not able to show the indexed data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4194</link><project id="" key="" /><description>if we disconnect our computer from the network it shows exception like this :- 

Nov 16, 2013 11:50:49 AM org.elasticsearch.common.logging.slf4j.Slf4jESLogger internalWarn():109
WARN: [Riot Grrl] failed to connect to master [[Clown][GqXVvUlkQRuh8M3cfLTXTA][inet[/192.168.169.2:9300]]], retrying...
org.elasticsearch.transport.ConnectTransportException: [Clown][inet[/192.168.169.2:9300]] connect_timeout[30s]
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:671) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:610) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:580) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:127) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.discovery.zen.ZenDiscovery.innterJoinCluster(ZenDiscovery.java:337) [elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.discovery.zen.ZenDiscovery.access$500(ZenDiscovery.java:76) [elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.discovery.zen.ZenDiscovery$1.run(ZenDiscovery.java:290) [elasticsearch-0.90.1.jar:na]
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) [na:1.6.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) [na:1.6.0_45]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_45]
Caused by: java.net.SocketException: Network is unreachable: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.6.0_45]
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599) ~[na:1.6.0_45]
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-0.90.1.jar:na]
        ... 3 common frames omitted

Secondly if we we are using elasticssearch as a windows services,same problem occur . On restarting the services we are unable to find the same data that was indexed earlier.

Any comments are more welcome on this...........................
</description><key id="22827015">4194</key><summary>ElasticSearch: - On migrating  from one network to another network elasticsearch is not able to show the indexed data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">desaxena</reporter><labels /><created>2013-11-18T10:52:09Z</created><updated>2016-03-12T02:58:24Z</updated><resolved>2014-08-08T18:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-18T13:11:47Z" id="28696492">hey, can you be a bit more specific what your problem?

What happens above is, that if you disconnect your host from the network, the elasticsearch instance is not able to reach the other parts of the cluster anymore - which results in the above exception.

Can you also be more specific on what you mean that you are not able to find data you indexed earlier? If you disconnect a host, which included the data you search for (and you do not have any replicas), than this might be problem - but far more information is needed in order to help here.

Regarding the windows service: Can you be more specific what action you took? Did you index data, than stop the service, restarted it, and the data was gone? A list of steps to reproduce your problem would be great.
</comment><comment author="desaxena" created="2013-11-20T19:12:38Z" id="28919759">IN our program we are indexing data and searching the data. Once I move to another network I am not able to search the same data as in earlier case. At the server end the exception that I mentioned is thrown.

Is there any config in the .yml file so that moving from one network to another network, will also result in the same data as we searched before moving to another network.
</comment><comment author="spinscale" created="2013-11-23T07:02:38Z" id="29127407">Hey,

sorry, but again I have to ask you to be more specific. What exactly do you mean with "moving  to another network" - are you simply changing ip addresses or are you copying data? Or doing something else? Please be more specific here.

From the error message above it seems, as if by somehow changing your environment, you are not able anymore to reach parts of your cluster. This merely looks like a network problem from this perspective, so we need to get more information, if this is an elasticsearch issue at all.
</comment><comment author="desaxena" created="2013-11-24T07:22:53Z" id="29150445">"moving to another network"  --&gt; changing the ip.  THe data is indexed into the elasticsearch server. But I same dsl query query but not able to get the data. I have to restart the elasticsearch server. what is the setting that can be done so that changing the IP does not show the above error stated. 
</comment><comment author="desaxena" created="2013-11-26T05:27:11Z" id="29268406">if I using the default elasticsearch.yml file  and not uncommenting them what will be the network configuration in that case???
</comment><comment author="imotov" created="2013-12-01T20:16:05Z" id="29581984">@desaxena are you dynamically changing IP address of the server while elasticsearch is running and after IP is changed you cannot reconnect to this server? 
</comment><comment author="desaxena" created="2013-12-03T06:19:45Z" id="29686275">This would be the biggest problem, If I am using elasticsearch at home and move to some other area, where I have to change the IP to connect to different IP to connect to internet, all my indexed data will be lost??
</comment><comment author="kimchy" created="2013-12-03T08:23:22Z" id="29691031">@desaxena of course not, it will not be lost.
</comment><comment author="imotov" created="2013-12-09T23:13:05Z" id="30184267">@desaxena the data should be there, but elasticsearch nodes can be confused about sudden change of addresses. Basically client might still try to connect to server using old published IP address. If you are using elasticsearch on your notebook and connect to it locally, you can use local interface to prevent this from happening. Just add

```
network.host: "_lo0:ipv4_"
```

to elasticsearch.yml file and use `127.0.0.1` as an address in your client. 

Does it make sense? Do you think we can close this ticket?
</comment><comment author="desaxena" created="2013-12-10T07:19:31Z" id="30204383">instead of 127.0.0.1 local will do the work, secondly this network.host seems to be  IPv4 address setting, if it is IPv6 then? what is the seamless settings in all the scenario???
</comment><comment author="imotov" created="2013-12-10T11:56:13Z" id="30219633">Just use `localhost` everywhere then.
</comment><comment author="desaxena" created="2013-12-12T07:35:25Z" id="30394502">k
</comment><comment author="desaxena" created="2013-12-12T12:19:09Z" id="30415504">org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];[SERVICE_UNAVAILABLE/2/no master];
        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:138) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.checkGlobalBlock(TransportIndicesStatusAction.java:103) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.checkGlobalBlock(TransportIndicesStatusAction.java:59) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.&lt;init&gt;(TransportBroadcastOperationAction.java:167) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:77) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:48) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:47) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:65) ~[elasticsearch-0.90.1.jar:na]
        at org.elasticsearch.client.support.AbstractIndicesAdminClient.status(AbstractIndicesAdminClient.java:397) ~[elasticsearch-0.90.1.jar:na]
        at com.bmc.ola.es.utils.ESIndicesListContainer.&lt;init&gt;(ESIndicesListContainer.java:35) ~[ola-es-client-1.0-SNAPSHOT.jar:na]

getting this exception
</comment><comment author="bachi76" created="2014-02-20T21:00:36Z" id="35668286">I just had a similar issue when switching networks and could boil it down to this:

ElasticSearch installed on my local dev notebook
Access to my index works fine both from Java and REST (via Kibana)
Switched notebook to stand-by, went home, woke it in my home network
The REST interface still works fine (via Kibana)
The java interface (both java app and ES restarted) does throw errors, in which it mentions the old office IP:

[2014-02-20 21:28:14,875][WARN ][discovery.zen            ] [Mister Jip] failed to connect to master [[Dakimh the Enchanter][UNheDdOXRM-Jrp34OKuI7g][zen][inet[/192.168.3.140:9300]]], retrying...
org.elasticsearch.transport.ConnectTransportException: [Dakimh the Enchanter][inet[/192.168.3.140:9300]] connect_timeout[30s]

org.elasticsearch.discovery.MasterNotDiscoveredException: waited for [1m]
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:180)

192.168.3.140 was the previous ip in the office

The server however started using the new IP:
[Gabe Jones] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/192.168.1.7:9201]}

After a full reboot, everything was back to normal.

Conclusion: It seems that the java client somehow/somewhere caches the previous ip (of course I restarted the java client app).

Hope this helps.
</comment><comment author="desaxena" created="2014-02-21T06:56:01Z" id="35703712">is there any help available on this???
</comment><comment author="arult" created="2015-01-30T17:48:42Z" id="72240805">We are facing the same error. any update?
</comment><comment author="imotov" created="2015-01-30T19:39:31Z" id="72257893">@arult Elasticsearch doesn't support dynamic change of published network addresses. If you are working with elasticsearch on your laptop that moves from one network to another, simply follow the work-around described in https://github.com/elasticsearch/elasticsearch/issues/4194#issuecomment-30184267 It will also provide an additional level of security by making sure that nobody on the network can connect to your instance of Elasticsearch. 
</comment><comment author="arult" created="2015-01-31T05:07:13Z" id="72305103">@imotov Iam not connecting it locally. We have an AWS instance with elastic seach running. I created an AMI and launched another instance with different IP and made IP changes to host files and elastic search yml file. when we run we get this error. locally I have configured it as localhost and i dont get any problem. Thanks. any help appreciated
</comment><comment author="imotov" created="2015-01-31T05:16:28Z" id="72305329">@arult it sounds like you need some help with configuring elasticsearch in AWS. We are using github issue for bug reports and feature requests. So, let's move this discussion to the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch). Please make sure to include your configuration file and log file with error messages that you are getting. Thanks!
</comment><comment author="arult" created="2015-01-31T05:34:06Z" id="72305695">@imotov Oh..Thanks. I will move the question there and check. 
</comment><comment author="wangyz2015" created="2016-03-12T02:20:27Z" id="195637718">hi,masters, when i am indexing,it show org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/2/no master],i use thransport mode,and my cluster seems well now,my cluster crash yesterday but revover late
</comment><comment author="imotov" created="2016-03-12T02:51:44Z" id="195645515">@wangyz2015 the discussion forum was moved to https://discuss.elastic.co/ since my last post more than a year ago. Please, ask this question there. You can also [search the forum](https://discuss.elastic.co/search?q=SERVICE_UNAVAILABLE) to see if your question was already answered.
</comment><comment author="wangyz2015" created="2016-03-12T02:58:24Z" id="195645906">oh ho!  thx~
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added test for Update API via native scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4193</link><project id="" key="" /><description>Motivation was to have a test ready as well as something other people could have a look at.
</description><key id="22824508">4193</key><summary>Added test for Update API via native scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-18T10:01:42Z</created><updated>2014-07-16T21:51:17Z</updated><resolved>2013-11-18T10:45:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-18T10:17:42Z" id="28687059">LGTM
</comment><comment author="martijnvg" created="2013-11-18T10:18:31Z" id="28687102">+1!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing filter with nested objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4192</link><project id="" key="" /><description>Hi,
"missing" filter does not work with nested objetcs. Using nested filter with not filter works as a workaround, as you can see in https://gist.github.com/Erni/7484095
However what my gut feeling says is "use the missing filter to find the docs without or empty attribute, as it does with normal objects".

Would it be possible to make "missing" filter work with nested objects as well?
</description><key id="22823707">4192</key><summary>Missing filter with nested objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Erni</reporter><labels /><created>2013-11-18T09:43:46Z</created><updated>2014-08-08T18:31:59Z</updated><resolved>2014-08-08T18:31:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-18T09:49:57Z" id="28685522">@Erni I think we can make `missing` filter work. We can somehow detect that this is a nested object field and do the workaround you described automatically in a more optimised manner.
</comment><comment author="Erni" created="2013-11-18T09:55:28Z" id="28685845">Thank you very much, that would be great.
</comment><comment author="darkleaf" created="2014-04-11T17:27:08Z" id="40228366">:+1: 
</comment><comment author="clintongormley" created="2014-08-08T18:31:59Z" id="51640553">Closing in favour of #3495 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>please add a 'precise' option to the 'terms_stats' api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4191</link><project id="" key="" /><description>Thanks for your great work!
The 'terms_stats' is great and works very well. But for now, it return statistical data in float with long decimals. Sometimes, I only need a integer result, so I have to process the data format in code logic, and that is not such awesome.

So, could you add a 'precise' option to control how many decimals it shoud return. Thanks.

Another guy encounter this problem either. http://elasticsearch-users.115913.n3.nabble.com/numeric-precision-in-terms-stats-facet-td4020390.html
</description><key id="22812135">4191</key><summary>please add a 'precise' option to the 'terms_stats' api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yinchuan</reporter><labels /><created>2013-11-18T02:34:46Z</created><updated>2014-09-05T10:56:10Z</updated><resolved>2014-09-05T10:56:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-05T10:56:07Z" id="54611133">I am afraid it would raise more issues than it would solve. For example, if your values require about 40 bits (eg. a timestamp) and you aggregate more than 2^24 of them, you would need more than 64 bits to store the sum. On the other hand, double values would still be able to store such large numbers and accuracy would degrade gracefully.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>please add a 'precise' option to the 'terms_stats' api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4190</link><project id="" key="" /><description>Thanks for your great work!
The 'terms_stats' is great and works very well. But for now, it return statistical data in float with long decimals. Sometimes, I only need a integer result, so I have to process the data format in code logic, and that is not such awesome.

So, could you add a 'precise' option to control how many decimals it shoud return. Thanks.

Another guy encounter this problem either. http://elasticsearch-users.115913.n3.nabble.com/numeric-precision-in-terms-stats-facet-td4020390.html
</description><key id="22812126">4190</key><summary>please add a 'precise' option to the 'terms_stats' api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yinchuan</reporter><labels /><created>2013-11-18T02:34:14Z</created><updated>2013-11-18T02:36:40Z</updated><resolved>2013-11-18T02:36:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yinchuan" created="2013-11-18T02:36:40Z" id="28671846">a duplicate issue, close this one. sorry for this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>please add a 'precise' option to the 'terms_stats' api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4189</link><project id="" key="" /><description>Thanks for you great work!
The 'terms_stats' api is greate. But, for now, it return statistical data in float with a long decimals. Sometimes, I only need a integer result, I have to process the data precise in code logic, that's not such awesome.

So, could you add a 'precise' option, and we can deal this better. Thanks.
Another guys encounter this problem either.http://elasticsearch-users.115913.n3.nabble.com/numeric-precision-in-terms-stats-facet-td4020390.html
</description><key id="22811626">4189</key><summary>please add a 'precise' option to the 'terms_stats' api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yinchuan</reporter><labels /><created>2013-11-18T02:12:18Z</created><updated>2013-11-18T02:38:14Z</updated><resolved>2013-11-18T02:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yinchuan" created="2013-11-18T02:38:14Z" id="28671889">a nother re-post issue, close this. sorry about this, again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Handle pretty=false and missing line feed for pretty=true which is the d...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4188</link><project id="" key="" /><description>...efault for main REST action.

Following the debate on #4182 this PR keeps the fact that the main REST action is prettyfied by default but handling the pretty=false parameter and the added line feed in the prettification process.
</description><key id="22805306">4188</key><summary>Handle pretty=false and missing line feed for pretty=true which is the d...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-11-17T21:16:04Z</created><updated>2014-07-16T21:51:18Z</updated><resolved>2013-11-18T14:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-11-18T14:37:07Z" id="28702256">I think this is a much better suggestion, do you agree @dadoonet? If so, I'll go ahead and merge this.
</comment><comment author="dadoonet" created="2013-11-18T14:45:07Z" id="28702881">@dakrone I agree. That means we need to merge both PR, right?
</comment><comment author="dakrone" created="2013-11-18T14:46:04Z" id="28702960">@dadoonet nope, just this one. I'm on it.
</comment><comment author="dakrone" created="2013-11-18T14:49:30Z" id="28703259">Merged via 00be285, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in PluginsService when starting elasticsearch with a wrong user (Fix for 4186)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4187</link><project id="" key="" /><description>When starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.

To reproduce:
- create a plugins directory with `rwx` rights for root user only
- launch elasticsearch from another account (elasticsearch for example)

Related discussion: https://groups.google.com/forum/#!topic/elasticsearch/_WRW4Qfpo7M

Closes #4186.
</description><key id="22794178">4187</key><summary>NPE in PluginsService when starting elasticsearch with a wrong user (Fix for 4186)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-11-17T09:54:33Z</created><updated>2014-06-14T08:57:33Z</updated><resolved>2013-11-20T09:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>NPE in PluginsService when starting elasticsearch with a wrong user</comment><comment>When starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.</comment></comments></commit></commits></item><item><title>NPE in PluginsService when starting elasticsearch with a wrong user</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4186</link><project id="" key="" /><description>When starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.

To reproduce:
- create a plugins directory with `rwx` rights for root user only
- launch elasticsearch from another account (elasticsearch for example)

Related discussion: https://groups.google.com/forum/#!topic/elasticsearch/_WRW4Qfpo7M
</description><key id="22794127">4186</key><summary>NPE in PluginsService when starting elasticsearch with a wrong user</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-17T09:49:48Z</created><updated>2014-02-20T13:21:13Z</updated><resolved>2013-11-20T09:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>NPE in PluginsService when starting elasticsearch with a wrong user</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>NPE in PluginsService when starting elasticsearch with a wrong user</comment><comment>When starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.</comment></comments></commit></commits></item><item><title>Increase the usage of ImmutableOpenMap for immutable data structures.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4185</link><project id="" key="" /><description>Cut over the immutable data structures to ImmutableOpenMap in:
- MetaData
- IndexMetaData
- ClusterState
</description><key id="22777039">4185</key><summary>Increase the usage of ImmutableOpenMap for immutable data structures.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-16T12:24:30Z</created><updated>2015-05-18T23:33:36Z</updated><resolved>2013-11-25T22:20:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-25T22:20:42Z" id="29248529">Pushed on master:
- https://github.com/elasticsearch/elasticsearch/commit/036febe110f0ea87d96bfe2dce71f97469d5f317
- https://github.com/elasticsearch/elasticsearch/commit/ac03fba9d31c9cca163739df3c05bc1984d230dc

And 0.90: https://github.com/elasticsearch/elasticsearch/commit/aa2f9f63d2efafe12930aecb3f5bb74e9ce1253b
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update FilterBuilder interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4184</link><project id="" key="" /><description>Make the FilterBuilder interface consistent with the QueryBuilder
interface.
</description><key id="22767854">4184</key><summary>Update FilterBuilder interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-11-16T00:18:16Z</created><updated>2014-06-19T13:47:21Z</updated><resolved>2013-11-18T19:47:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-11-16T00:18:54Z" id="28614354">Another part of breaking up #3278. 

/cc @martijnvg 
</comment><comment author="kimchy" created="2013-11-16T12:49:04Z" id="28625899">this looks good to me. I just don't think we need to FitlerBuilderException? Typically we will simply throw illegal argument and such, no? @martijnvg what do you think? 
</comment><comment author="mattweber" created="2013-11-16T15:56:04Z" id="28629466">`QueryBuilder` throws a `QueryBuilderException` so is this not needed as well?
</comment><comment author="martijnvg" created="2013-11-16T19:54:19Z" id="28634339">We have `QueryBuilderException` for `QueryBuilder`, so it makes sense for `FilterBuilderException` for `FilterBuilder` for consistency. Or maybe we can have a generic builder exception for both? Not sure about the name.
</comment><comment author="kimchy" created="2013-11-16T20:37:20Z" id="28635195">@martijnvg I would probably remove `QueryBuilderException` and throw the relevant ES illegal argument exception where appropriate..., seems cleaner.
</comment><comment author="mattweber" created="2013-11-16T20:58:48Z" id="28635654">Thanks, I will get rid of the `FilterBuilderException`.  Want me to nuke the `QueryBuilderException` in this PR as well?
</comment><comment author="kimchy" created="2013-11-16T20:59:28Z" id="28635668">great, lets remove `QueryBuilderException` in another one...
</comment><comment author="martijnvg" created="2013-11-16T21:01:02Z" id="28635695">+1 On removing both Exception and IllegalArgumentException instead for both query and filter builders.
</comment><comment author="mattweber" created="2013-11-17T16:44:29Z" id="28652081">Replaced throwing a `FilterBuilderException` with an `ElasticSearchException`. 
</comment><comment author="martijnvg" created="2013-11-18T09:06:37Z" id="28683271">@mattweber There are a number of places where inside the builder's doXContent() we throw a `QueryBuilderException`. Can you replace them by `ElasticSearchIllegalArgumentException`? For example `BoostingQueryBuilder`, `FuzzyLikeThisFieldQueryBuilder` and others.
</comment><comment author="mattweber" created="2013-11-18T16:58:27Z" id="28716042">@martijnvg How does this look?
</comment><comment author="martijnvg" created="2013-11-18T17:12:26Z" id="28717392">@mattweber Looks good! I'll pull this in.
</comment><comment author="martijnvg" created="2013-11-18T19:47:54Z" id="28731184">pulled and added related issue #4199
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update BloomFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4183</link><project id="" key="" /><description>Allow the user to configure the number of hash functions as well as add
support for serializing/deserializing the bloom filter from a stream.
Add a hashCode to the bloom filter.
</description><key id="22767240">4183</key><summary>Update BloomFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-11-16T00:01:17Z</created><updated>2014-06-18T07:23:42Z</updated><resolved>2013-11-18T09:02:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-11-16T00:01:41Z" id="28613662">Another part of breaking of #3278.

/cc @martijnvg 
</comment><comment author="mattweber" created="2013-11-17T16:36:47Z" id="28651906">@kimchy no more magic. 
</comment><comment author="kimchy" created="2013-11-17T16:45:37Z" id="28652112">LGTM, @martijnvg if its good with you, lets pull it in
</comment><comment author="martijnvg" created="2013-11-18T08:44:25Z" id="28682206">LGTM, I'll pull it in.
</comment><comment author="martijnvg" created="2013-11-18T09:02:24Z" id="28683076">pushed to master (55300f3a3fd4ce07aa2239466ecf8603f7beccb7) and 0.90 (80a8222725e3a23a6a928db3e975d8e54fbe6621)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disable auto pretty print for main REST action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4182</link><project id="" key="" /><description>Not a big deal but for the sake of consistency I find it strange that the main REST action is the only one to be auto pretty printed.

If it is on purpose maybe it would be better to at least add the call to the .lfAtEnd() that is called when setting the pretty param to true.
</description><key id="22766185">4182</key><summary>Disable auto pretty print for main REST action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2013-11-15T23:33:11Z</created><updated>2014-06-14T08:56:36Z</updated><resolved>2013-11-18T14:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-16T01:43:50Z" id="28616886">I would not merge it as is as I find it convinient to have it pretty printed by default.

That said, if we run http://localhost:9200/?pretty=false it stills display it pretty printed which is a bug to me :-)

What do you and others think?
</comment><comment author="wayne530" created="2013-11-16T02:32:24Z" id="28617893">+1 for disable pretty print by default to make it consistent with all other endpoints.
</comment><comment author="dakrone" created="2013-11-16T02:33:54Z" id="28617929">I'm curious what the reasoning is behind disabling it (besides the rest of the APIs don't format by default)? I like it; is it causing issues with anything in particular?
</comment><comment author="wayne530" created="2013-11-16T02:36:39Z" id="28617977">that's the only reason i see: to be consistent with everything else. i'm not sure why it's a big deal - you can add ?pretty=true just as easily.
</comment><comment author="dadoonet" created="2013-11-16T06:42:33Z" id="28621009">IMHO this is the first command a new user run. I'd prefer to welcome him nicely. 

My 0.05 cents.
</comment><comment author="Paikan" created="2013-11-17T21:19:30Z" id="28664245">@dakrone @dadoonet as I said not a big deal or issues here with the default prettification it was only for consitency.

I understand your point wanting to give a warmer welcoming to the first command of a new user. I have opened PR #4188 to at least support pretty=false and the line feed in the prettification.

Feel free to close this one when you see fit.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>FieldDataTermsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4181</link><project id="" key="" /><description>Add a terms filter that compares terms out of
the fielddata cache. When filtering on a large
set of terms this filter can be considerably faster
than using a standard lucene terms filter.

Add the "fielddata" execution mode to the
terms filter parser to enable the use of
the new FieldDataTermsFilter.

Add supporting tests and documentation.
</description><key id="22765906">4181</key><summary>FieldDataTermsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mattweber</reporter><labels /><created>2013-11-15T23:27:21Z</created><updated>2014-07-16T21:51:20Z</updated><resolved>2013-11-19T18:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-11-15T23:32:23Z" id="28612278">@martijnvg and I decided that #3278 was getting too complex and needed to be broken up.  This is the first part of that refactor by creating a terms filter that operates against the fielddata cache.  During testing of #3278 I found that this was considerably faster than using the standard lucene `TermsFilter` when filtering on a large set of terms.   
</comment><comment author="kimchy" created="2013-11-16T12:43:34Z" id="28625821">I think a better solution for this would be to have the ability to build a terms filter based on fielddata based on the `FieldMapper`. In a similar manner where `NumberFieldMapper#rangeFilter(IndexFieldDataService ...`. And then, the terms filter parser would simple delegate the call to the mapper.

Also, the filter caching behavior for this filter should be no caching if none is explicitly specified.
</comment><comment author="mattweber" created="2013-11-16T18:13:49Z" id="28632250">@kimchy thanks for the tip, the code is much cleaner using this approach.  What do you think of the latest version?
</comment><comment author="kimchy" created="2013-11-16T20:49:28Z" id="28635447">- added a comment regarding the parsing of values to numbers.
- we should only have an HPPC native set for the fielddata filter, nothing else. This "match" code is going to be called the most, its better not to have 3 'ifs' there, and have the most optimized data structure to support this call. I would only support the HPPC set across this class (remove the array and Collection variants), it will simplify and reduce the code.
- I don't really see a need for `DoublesFieldDataDocSet`? Its a base class that is begin extended by one other class. Lets just have the logic in a single class. Same applies to the other doc sets in the field data class.
</comment><comment author="mattweber" created="2013-11-16T21:01:22Z" id="28635706">Sounds good, I'll get these fixed up!  Thanks for the feedback!
</comment><comment author="mattweber" created="2013-11-17T22:29:04Z" id="28666078">@kimchy Updated with your suggestions.  Let me know how this version looks.  Thanks!
</comment><comment author="martijnvg" created="2013-11-18T10:18:11Z" id="28687086">This looks good. Can you squash the commits and open an issue for this PR, so that it will be taken into account in the next release notes?

Two things that we can do after this gets in:
- The `execution` option only works in a `terms` filter without lookup. I think it makes sense that all `execution` modes work in any `terms` filter (with or without lookup by id / query).
- In the case the `fielddata` mode is selected and lookup is used the hppc sets will get large, it makes sense to cache the set instances via the CacheRecycler like we do in p/c queries.  
</comment><comment author="mattweber" created="2013-11-18T17:52:09Z" id="28720988">@martijnvg Want me to look into these additional items before you merge it?

@s1monw Yes, I think it might be better to use a `BytesRefHash`.  We are basically only using it for lookups on data pulled from the field data so using the hash code from the field data is ideal.  Let me switch `ObjectOpenHashSet&lt;BytesRef&gt;` to a `BytesRefHash` and see how things look.  
</comment><comment author="mattweber" created="2013-11-18T20:16:33Z" id="28733736">@s1monw I started migrating to a `BytesRefHash` but ran into a few issues maybe you can help with.
1. It doesn't implement `hashCode` or `toString` and doesn't provide access to the internal arrays so I can calculate myself.
2. Performs rehash at 50% insertion.  We know the exact size so the only way to prevent rehashing is to init with `size + (size &gt;&gt; 1) + 1` which seems wasteful.
3. Need to have the `close` method called... not sure where I can call this inside the filter.

Since this is a final class, maybe we need to temporarily duplicate it as `XBytesRefHash`?
</comment><comment author="martijnvg" created="2013-11-18T20:21:43Z" id="28734197">@mattweber We can do these action points later.
</comment><comment author="s1monw" created="2013-11-19T11:17:16Z" id="28782185">we can do it later though. We can also take those points and feed them into lucene I guess. Especially the rehashing is important I guess.

Lemme still ask some questions
1. What do you need hashCode / toString for?
2. You don't have to close it - ignore that.
3. I think we should allow more control over the load factor. I will try adding it to lucene
</comment><comment author="martijnvg" created="2013-11-19T11:51:39Z" id="28783964">@mattweber @s1monw I think we should do it later, once the changes are in Lucene we migrate from `ObjectOpenHashSet&lt;BytesRef&gt;` to `BytesRefHash`.
</comment><comment author="mattweber" created="2013-11-19T16:16:06Z" id="28803463">@s1monw I would like to use them in the filters `equals`, `toString`, and `hashCode` methods which I believe are used during filter caching if no `cacheKey` is specified.  I'd like access to the backing arrays, if possible, so I can stream/serialize a `BytesRefHash` across the transport in #3278.   Even a way to get all the existing id's without requiring sorting would be nice.  

@martijnvg I have squashed the commits, let me know if there is anything else you would like done in this version of the filter.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implementation of blocking close method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4180</link><project id="" key="" /><description>Blocks until all bulk requests have completed.  This fixes #4158
</description><key id="22762338">4180</key><summary>Implementation of blocking close method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">matt-preston</reporter><labels /><created>2013-11-15T22:11:29Z</created><updated>2014-07-11T09:19:06Z</updated><resolved>2014-07-11T09:19:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-29T15:35:26Z" id="29522983">I think only `closeInternal` should be `synchronized` and not `close` or `awaitClose`, and the test if its closed should be there as well?
</comment><comment author="javanna" created="2014-06-13T12:20:56Z" id="46004527">Hi @matt-preston sorry for the terrible delay, I noticed you updated the PR according to the review feedback you got. Looks good! Can you sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can get this in please? 

Also, I noticed you used 2 spaces for indentation, while we use 4 as mentioned [here](https://github.com/elasticsearch/elasticsearch/blob/master/CONTRIBUTING.md#contributing-to-the-elasticsearch-codebase). Would you mind changing that?

Thanks!
</comment><comment author="matt-preston" created="2014-06-22T16:20:22Z" id="46785367">Hi, I've signed the CLA and updated the issue with a new commit that fixes the formatting.
Thanks.
</comment><comment author="javanna" created="2014-06-25T15:08:27Z" id="47113938">Hi @matt-preston, thanks a lot! I can't see the new commit though, maybe you didn't push it yet?
</comment><comment author="matt-preston" created="2014-06-25T15:53:08Z" id="47120748">Hi,

Sorry for the confusion, I pushed it as a new PR:
https://github.com/elasticsearch/elasticsearch/pull/6586

Matt

On 25 June 2014 16:09, Luca Cavanna notifications@github.com wrote:

&gt; Hi @matt-preston https://github.com/matt-preston, thanks a lot! I can't
&gt; see the new commit though, maybe you didn't push it yet?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/4180#issuecomment-47113938
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enhancement: Add cluster name to stats resource</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4179</link><project id="" key="" /><description>Version: 0.90.6

When querying the "_cluster/nodes/stats" resource the returned document contains the key "cluster_name"

When querying the "_stats" resource, cluster_name is not available while it would make sense for the application consuming this data to identify the dataset.

cluster_name should be added to the document returned when querying the "_stats" resource.
</description><key id="22753301">4179</key><summary>Enhancement: Add cluster name to stats resource</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smetj</reporter><labels /><created>2013-11-15T19:27:51Z</created><updated>2014-08-08T18:29:03Z</updated><resolved>2014-08-08T18:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:29:03Z" id="51640216">Done
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bug: Timestamp has wrong format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4178</link><project id="" key="" /><description>Version:  0.90.6

When requesting resource "_cluster/nodes/stats" the returned JSON document contains wrong data for the "timestamp" field.  I presume this is epoch but it seems the last 3 digits are sub second values and the  decimal point is missing.

For example:

"timestamp":1384542689096

should be

"timestamp":1384542689.096
</description><key id="22752859">4178</key><summary>Bug: Timestamp has wrong format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smetj</reporter><labels /><created>2013-11-15T19:19:58Z</created><updated>2013-11-15T19:59:19Z</updated><resolved>2013-11-15T19:59:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-15T19:29:39Z" id="28596617">@smetj not sure I understand what you mean by "wrong data". The `timestamp` is `long` value returned by `System#currentTimeMillis()`. It's basically the number of milliseconds since midnight, January 1, 1970 UTC.
</comment><comment author="smetj" created="2013-11-15T19:59:19Z" id="28598762">@imotov you're right.  I was blinded since I'm rather used to work with unixtime format instead.

Thanks, 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OutOfMemoryError[Java heap space] when executing a query with a high 'from' value.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4177</link><project id="" key="" /><description>Hi, 

I am executing a simple query (using the Chrome Postman App) on an index with around 80000 records, with a 'from' value which is way higher than the number of available pages.

{
  "from" : 200000000,
  "size" : 1,
  "query" : { 
    "query_string" : {
        "query" : "test"
    }
  }
}

This results in the following response:

{
  "error" : "ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: OutOfMemoryError[Java heap space]; ",
  "status" : 503
}

Interestingly, when I execute the same query from a Java client using the Java API, the same error occurs but the value can be much lower, around 2000000. Elasticsearch and Client running in separate JVMs,

Our web application fronting ElasticSearch restricts the page size but not the page number as I assumed this would always return 0 results if the client request exceeds the available number of pages. However, if the client provides a very high number, this OOM occurs.

Setup:
$ java -version
java version "1.7.0_17"
Java(TM) SE Runtime Environment (build 1.7.0_17-b02)
Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)
Windows 7
Elasticsearch 0.90.5 (standard setup, no settings changed after install)
# ES LOG after query direct from postman:

[2013-11-15 14:05:04,323][INFO ][node                     ] [Guthrie, Jebediah] version[0.90.5], pid[14304], build[c8714e8/2013-09-17T12:50:20Z]
[2013-11-15 14:05:04,324][INFO ][node                     ] [Guthrie, Jebediah] initializing ...
[2013-11-15 14:05:04,330][INFO ][plugins                  ] [Guthrie, Jebediah] loaded [], sites []
[2013-11-15 14:05:07,601][INFO ][node                     ] [Guthrie, Jebediah] initialized
[2013-11-15 14:05:07,602][INFO ][node                     ] [Guthrie, Jebediah] starting ...
[2013-11-15 14:05:07,790][INFO ][transport                ] [Guthrie, Jebediah] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.201.87.208:9300]}
[2013-11-15 14:05:11,006][INFO ][cluster.service          ] [Guthrie, Jebediah] new_master [Guthrie, Jebediah][rwzTTiUKQXG7mX0YuYc3SQ][inet[/10.201.87.208:9300]], reason: zen-disco-join (elected_as_master)
[2013-11-15 14:05:11,079][INFO ][discovery                ] [Guthrie, Jebediah] elasticsearch/rwzTTiUKQXG7mX0YuYc3SQ
[2013-11-15 14:05:11,228][INFO ][http                     ] [Guthrie, Jebediah] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.201.87.208:9200]}
[2013-11-15 14:05:11,229][INFO ][node                     ] [Guthrie, Jebediah] started
[2013-11-15 14:05:11,942][INFO ][gateway                  ] [Guthrie, Jebediah] recovered [2] indices into cluster_state
[2013-11-15 14:05:44,744][DEBUG][action.search.type       ] [Guthrie, Jebediah] [gtr][0]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2bc44fb7] while moving to second phase
java.lang.OutOfMemoryError: Java heap space
    at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:64)
    at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:37)
    at org.elasticsearch.search.controller.ScoreDocQueue.&lt;init&gt;(ScoreDocQueue.java:31)
    at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:248)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:85)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:409)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:241)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:219)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:216)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2013-11-15 14:05:46,165][INFO ][cluster.service          ] [Guthrie, Jebediah] added {[Man-Spider][4F01Q8RwQHOhDW_LB2u1YQ][inet[/10.201.87.208:9301]]{data=false, local=false, master=false},}, reason: zen-disco-receive(join from node[[Man-Spider][4F01Q8RwQHOhDW_LB2u1YQ][inet[/10.201.87.208:9301]]{data=false, local=false, master=false}])
# CLIENT Log using Java API:

org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [query], [reduce] 
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:180) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:154) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:148) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:251) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:242) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:153) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-0.90.5.jar:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_17]
        at java.lang.Thread.run(Thread.java:722) ~[na:1.7.0_17]
java.lang.OutOfMemoryError: Java heap space
        at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:64) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]
        at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:37) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]
        at org.elasticsearch.search.controller.ScoreDocQueue.&lt;init&gt;(ScoreDocQueue.java:31) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:248) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerExecuteFetchPhase(TransportSearchDfsQueryThenFetchAction.java:185) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:178) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:154) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:148) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:251) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:242) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:153) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-0.90.5.jar:na]
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-0.90.5.jar:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]
</description><key id="22734192">4177</key><summary>OutOfMemoryError[Java heap space] when executing a query with a high 'from' value.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">spaisey</reporter><labels><label>enhancement</label></labels><created>2013-11-15T14:13:23Z</created><updated>2015-09-21T19:28:21Z</updated><resolved>2015-09-21T19:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-15T17:03:36Z" id="28585264">this happens because of the very large `from` parameter, it ends up requiring a lot of resources (even google doesn't allow for endless pagination while scoring). We should do better job at rejecting those failures to begin with..., but for now, make sure you don't issue it.
</comment><comment author="spaisey" created="2013-11-15T17:34:11Z" id="28587734">Thanks for the swift response.
I'll add an upper bounds check to ensure the request is rejected before it gets to ES.
</comment><comment author="clintongormley" created="2015-09-21T19:28:21Z" id="142085360">Closed by #13188
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow scripts to set the value of a script field to primitive arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4176</link><project id="" key="" /><description>Script fields could not be set to int[] and float[] by native
scripts because StreamInput and StreamOutput could not handle
them.

closes #4175
</description><key id="22731441">4176</key><summary>Allow scripts to set the value of a script field to primitive arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-11-15T13:16:48Z</created><updated>2014-06-18T18:52:12Z</updated><resolved>2013-11-15T18:03:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-11-15T18:03:10Z" id="28589920">pushed to master (faf2380605) and 0.90 (fa164fdc7cf)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow native scripts to set the value of a script field to primitive arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4175</link><project id="" key="" /><description>Currently, primitive arrays such as int[] and float[] cannot be set as the value of a script field by a native script because StreamInput and StreamOutput cannot read and write these types. This results in an Exception once the result is streamed.
</description><key id="22731242">4175</key><summary>Allow native scripts to set the value of a script field to primitive arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-15T13:12:23Z</created><updated>2013-11-20T09:21:27Z</updated><resolved>2013-11-15T18:01:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptFieldTests.java</file></files><comments><comment>Allow native scripts to set the value of a script field to primitive arrays</comment></comments></commit></commits></item><item><title>how to config the port range in ealsticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4174</link><project id="" key="" /><description>there are some port like 57171,57172......used when i started elasticsearch. can i config the port range in ealsticsearch.yml?  and how to config?
</description><key id="22718157">4174</key><summary>how to config the port range in ealsticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhyj0121</reporter><labels /><created>2013-11-15T07:48:19Z</created><updated>2013-11-15T08:50:32Z</updated><resolved>2013-11-15T08:50:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-15T08:50:32Z" id="28554930">Not exactly sure what you need, but I suggest you look at the comments at the file for instructions. If things are not clear please ask this on the mailing list here : https://groups.google.com/forum/#!forum/elasticsearch . We try to keep the github issues are for feature requests and bug reports.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query multi-type with same field matched.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4173</link><project id="" key="" /><description>Hi all,

I have one index with 2 type such as : post and comment.

So i'd like to query into comment first then get all docs matched some condition such as specific comment_id, then after that I want to continue query to post type to found all docs in post type have comment_id I found before.

Is it possible to query like join in SQL like that ?

Regards
H.L
</description><key id="22711519">4173</key><summary>Query multi-type with same field matched.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">usebee</reporter><labels /><created>2013-11-15T04:01:10Z</created><updated>2013-11-15T09:57:54Z</updated><resolved>2013-11-15T09:30:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-15T09:30:54Z" id="28556828">Hi,

Can you please ask this type of questions on the elasticsearch google group: https://groups.google.com/forum/#!forum/elasticsearch   ?

We try to keep github issues for feature requests and bug reports.

Thx!
</comment><comment author="usebee" created="2013-11-15T09:57:54Z" id="28558241">I'll do that,

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 130</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4172</link><project id="" key="" /><description>Crashes don't seem to be related to any particular query and have been occurring for a number of weeks.

Two node cluster running on Debian 7.2

I can confirm both nodes are running the exact same Elasticsearch and java versions.

`Elasticsearch 0.90.5` 

`openjdk-7-jdk:amd64 7u25-2.3.10-1~deb7u1`

`Linux int-elastic-01 3.2.0-4-amd64 #1 SMP Debian 3.2.51-1 x86_64 GNU/Linux`

```
[2013-11-14 14:01:30,311][INFO ][cluster.metadata         ] [int-elastic-01] [test_iss3.dmadeley-django64.dev] update_mapping [suggestions] (dynamic)
[2013-11-14 14:01:31,489][DEBUG][action.admin.cluster.node.info] [int-elastic-01] failed to execute on node [lWRF61bgQ66ip1BmLKygFg]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:147)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 130
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
    at org.elasticsearch.common.io.stream.AdapterStreamInput.readByte(AdapterStreamInput.java:35)
    at org.elasticsearch.common.io.stream.StreamInput.readBoolean(StreamInput.java:267)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:231)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:145)
    ... 23 more
[2013-11-14 14:01:32,278][INFO ][cluster.metadata         ] [int-elastic-01] [test_iss3.dmadeley-django64.dev] deleting index
```
</description><key id="22707774">4172</key><summary>java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 130</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">sammcj</reporter><labels /><created>2013-11-15T01:47:40Z</created><updated>2014-03-04T11:16:05Z</updated><resolved>2014-03-04T09:41:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-15T09:08:21Z" id="28555724">Hi sammcj,

A couple of questions:
1) Do you see anything in the log of the other machine?
2) Are you using any Java clients? more specifically node clients. If so - are you sure they are of the same ES version?
</comment><comment author="prashanttct07" created="2014-03-03T05:52:17Z" id="36483416">Hi sammcj/bleskes,

I am also getting the same issue, can you tell me the solution if you got already?
</comment><comment author="bleskes" created="2014-03-03T07:49:06Z" id="36487795">Hi Prashy,

Can you give some more info about you're set up? Which ES version are you using? Are you using the java clients and if so, are they of the same version of the main ES cluster?
</comment><comment author="prashanttct07" created="2014-03-03T08:37:31Z" id="36490247">Hi Bleskes,

I am using ES 1.0.1 and on top of that I have installed Head,Carrot2 plugin with lingo3g configuration.

So while running query sometimes I am getting the error as  IndexOutOfBoundsException

And also the below exception is seen as well while executing the cluster query.
 {
error: ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: c[bytes can be at most 32766 in length; got 131074]; 
status: 503
}
</comment><comment author="bleskes" created="2014-03-03T08:50:33Z" id="36490989">The original issue was about a NodeInfo call, while yours is a search one. Do you have a complete stack trace in the logs? Also - are you using the java client?
</comment><comment author="prashanttct07" created="2014-03-03T08:59:52Z" id="36491586">Below is the call logs for the issue:
[2014-03-03 19:52:09,663][DEBUG][action.search.type       ] [Node164] failed to reduce search
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce]
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:182)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:156)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:150)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:407)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: lingo3g.ft$c: bytes can be at most 32766 in length; got 45087
        at lingo3g.ft.a(Unknown Source)
        at lingo3g.ft.a(Unknown Source)
        at lingo3g.cx.a(Unknown Source)
        at lingo3g.cx.a(Unknown Source)
        at lingo3g.df.b(Unknown Source)
        at lingo3g.df.a(Unknown Source)
        at lingo3g.v.a(Unknown Source)
        at com.carrotsearch.lingo3g.Lingo3GClusteringAlgorithm.a(Unknown Source)
        at com.carrotsearch.lingo3g.Lingo3GClusteringAlgorithm.a(Unknown Source)
        at com.carrotsearch.lingo3g.Lingo3GClusteringAlgorithm$1.process(Unknown Source)
        at org.carrot2.text.clustering.MultilingualClustering.clusterByLanguage(MultilingualClustering.java:237)
        at org.carrot2.text.clustering.MultilingualClustering.process(MultilingualClustering.java:119)
        at com.carrotsearch.lingo3g.Lingo3GClusteringAlgorithm.process(Unknown Source)
        at org.carrot2.core.ControllerUtils.performProcessing(ControllerUtils.java:106)
        at org.carrot2.core.Controller.process(Controller.java:356)
        at org.carrot2.core.Controller.process(Controller.java:269)
        at org.carrot2.elasticsearch.ClusteringAction$TransportClusteringAction$1.onResponse(ClusteringAction.java:664)
        at org.carrot2.elasticsearch.ClusteringAction$TransportClusteringAction$1.onResponse(ClusteringAction.java:623)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:198)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:180)
        ... 8 more

And yes, I am using some of java lib for clustering and lingo3g.
</comment><comment author="sammcj" created="2014-03-03T22:07:59Z" id="36566397">We resolved this issue by not using the ES transport with logstash and switching to the HTTP only method.
</comment><comment author="bleskes" created="2014-03-04T09:41:58Z" id="36607030">@sammcj yes, this confirm the suspicion this is caused by a java client which is of different version than the cluster.

@prashy I'm pretty sure you have the same problem. Can you verify that the version if the carrot2 plugin is compatible with the ES version you use? see https://github.com/carrot2/elasticsearch-carrot2. As the original issue is resolved, I'm closing this ticket for now.
</comment><comment author="prashanttct07" created="2014-03-04T11:06:08Z" id="36613730">Hi Bleskes,

I am using : 
ES version: 1.0.1
Lingo3g Version: 1.9.0
Clustering Plugin version: 1.3.1

Should I open a new case ?
</comment><comment author="bleskes" created="2014-03-04T11:16:05Z" id="36614379">@prashy upon closer inspection of your stack trace - the error comes from the clustering algorithm code of lingo3g and it isn't related to the issue described here. I suggest you open an issue on the carrot2 project site.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date fields in  "fields" :  returning wrong month</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4171</link><project id="" key="" /><description>## Defined index type 

{
    "logLine": {
        "properties": {
         "operation": {
                "type": "string",
                "store": "yes"
            },
            "duration": {
                "type": "double",
                "store": "yes"
            },
            "page": {
                "type": "string",
                "store": "yes"
            },
            "start_time": {
                "type": "date",
                "format": "YYYY-MM-DD HH:mm:ss zzz",
                "store": "yes"
            },
            "finish_time": {
                "type": "date",
                "format": "YYYY-MM-DD HH:mm:ss zzz",
                "store": "yes"
            },
            "status": {
                "type": "string",
                "store": "yes"
            }
        }
    }
}

store an entry 

---

{
               "operation": "CREATE BOOKING",
               "duration": "1293.0",
               "status": "200",
               "page": "/booking", 
               "finish_time": "2013-11-14 12:42:21 UTC",
               "domain": "CO",
               "start_time": "2013-11-14 12:42:20 UTC",

 }

GET Match_all returns proper dates as stored 

i.e  "finish_time" = "2013-11-14 12:42:21 UTC",

but when i add fields list the results have wrong month
## QUERY

{

```
    "fields" : ["finish_time", "duration"],
    "size" : "10",
    "query" : {       
            "term" : { "operation" : "CREATE BOOKING" }
        }
} 
```
## Results

{
           "duration": 1293,
               "finish_time": "2013-01-14 12:42:21 UTC"

}

in the result the finish_time MONTH returned as 01 instead of 11

appreciate your help

thanks
Soma G
</description><key id="22701894">4171</key><summary>Date fields in  "fields" :  returning wrong month</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">somag</reporter><labels /><created>2013-11-14T23:25:14Z</created><updated>2013-11-15T02:55:59Z</updated><resolved>2013-11-15T02:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-15T02:55:59Z" id="28543779">The date format should be `yyyy-MM-dd HH:mm:ss zzz`. Upper case `DD` is day of the year, not day of the month. This is way you are getting back dates in january. See [Joda DateTimeFormat](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html) for more information. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactored open/close index api to make use of the new recently introduced generic ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4170</link><project id="" key="" /><description>Refactored open/close index api to make use of the new recently introduced generic ack mechanism

Closes #4169
</description><key id="22698963">4170</key><summary>Refactored open/close index api to make use of the new recently introduced generic ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-14T22:38:04Z</created><updated>2014-06-13T00:15:31Z</updated><resolved>2013-11-20T09:41:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-19T10:44:00Z" id="28780423">this PR looks good to me but I still need to understand why `NodeIndicesStateUpdatedAction` is now gone entirely. 
This might not be ported to `0.90.x` since it removes transport classes right?
</comment><comment author="javanna" created="2013-11-19T11:04:36Z" id="28781551">Thanks for the review! The change is similar to what I did in  #4115 for aliases. We can backport it as well as explained there. Those custom notifications sent back to the master may be sent to a non existing endpoint during a rolling upgrade, and that wouldn't cause any trouble as exception sent back from the master are literally ignored in that case (see `EmptyTransportResponseHandler`).
</comment><comment author="s1monw" created="2013-11-20T09:28:34Z" id="28873610">The updates LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move open/close index api to new acknowledgement mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4169</link><project id="" key="" /><description>Move open/close index api to new acknowledgement mechanism introduced in #3786 .
</description><key id="22697855">4169</key><summary>Move open/close index api to new acknowledgement mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-14T22:19:45Z</created><updated>2013-11-20T09:41:22Z</updated><resolved>2013-11-20T09:41:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeIndicesStateUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/close/RestCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/open/RestOpenIndexAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/indices/state/OpenCloseIndexTests.java</file></files><comments><comment>Refactored open/close index api to make use of the new recently introduced generic ack mechanism</comment></comments></commit></commits></item><item><title>A way to retrieve a list of low/high frequency terms across a set of documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4168</link><project id="" key="" /><description>When using the common terms query, it is difficult to understand exactly why it behaves the way it does without knowing which terms are being considered as low frequency and which terms are being considered high frequency. So, it would be very useful to have a way of retrieving a list of low/high frequency terms across a type or index with a given cutoff frequency threshold. 
</description><key id="22692442">4168</key><summary>A way to retrieve a list of low/high frequency terms across a set of documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshfg</reporter><labels><label>discuss</label></labels><created>2013-11-14T21:01:33Z</created><updated>2014-09-05T11:01:57Z</updated><resolved>2014-09-05T11:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-05T11:01:57Z" id="54611675">You can retrieve the most common terms using a terms aggregation.  From there it is easy to calculate document frequencies, which you can use to set the `cutoff_frequency`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Option to calculate term frequencies by type with common terms query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4167</link><project id="" key="" /><description>If I post a common terms query to /myindex/mytype/_search with a cutoff frequency of 0.01, then a term must appear in less than 1% of documents to be considered low frequency. However, this percentage is calculated across all documents in myindex. In my case, I wanted the term frequencies to be calculated over only the mytype documents, and not documents of other types. It would be good if there was an option to do this!
</description><key id="22692189">4167</key><summary>Option to calculate term frequencies by type with common terms query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshfg</reporter><labels /><created>2013-11-14T20:57:27Z</created><updated>2014-08-08T18:27:32Z</updated><resolved>2014-08-08T18:27:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="joshfg" created="2014-02-11T17:02:12Z" id="34775524">bump
</comment><comment author="s1monw" created="2014-02-11T20:21:03Z" id="34801236">I don't think we should provide this really. If you want this I recommend to set the cutoff frequency as an absolute number ie. `10` and do the calculation yourself by running a count on the type. The reason why I don't want to do this is since we don't have this statistic at hand but we might be able to do that somehow in the future. what we can do and maybe should provide as an option is to calculate this not on the  number of docs in the index but on the `docCount` which is the number of docs that have a value in the field you run the query on. That way you can just have a type private field and get that for free.
</comment><comment author="joshfg" created="2014-02-12T09:22:14Z" id="34851499">yes that sounds like a good idea - it would solve the problems we have had! We are using an absolute number for now, but it would be nice to be able to use a percentage.
</comment><comment author="fresh83" created="2014-06-20T06:33:25Z" id="46648680">i second this and think its a bit ridiculous its not a option, but type would work fine as well ...even though it would make my indexes a bit messier 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices filters doesn't support _name parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4166</link><project id="" key="" /><description>Indices filters doesn't support `_name` parameter, useful to get back which filters a document matched using [named filters](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html).
</description><key id="22666795">4166</key><summary>Indices filters doesn't support _name parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-14T14:47:51Z</created><updated>2013-11-14T17:47:12Z</updated><resolved>2013-11-14T17:47:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/IndicesFilterParser.java</file><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Added support for _name parameter in indices filter</comment></comments></commit></commits></item><item><title>multi_match query supports fields as single string and array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4165</link><project id="" key="" /><description>The multi_match query accepted only an array in the fields parameter. This patch allows to use a single string as well.

Also added tests for parsing in both cases.

Closes #4164
</description><key id="22665090">4165</key><summary>multi_match query supports fields as single string and array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-11-14T14:18:50Z</created><updated>2014-07-16T21:51:22Z</updated><resolved>2013-11-14T14:55:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-14T14:32:15Z" id="28487784">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi_match fails if fields param is not an array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4164</link><project id="" key="" /><description>The `fields` param in `multi_match` should accept a string, not just an array of strings.  A single string could contain a wildcard, and thus refer to multiple fields.

```
POST /_validate/query?explain
{
   "multi_match": {
      "query": "foo bar",
      "fields": "foo"
   }
}
```

give you:

```
{
   "valid": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "explanations": [
      {
         "index": "test",
         "valid": false,
         "error": "org.elasticsearch.index.query.QueryParsingException: [test] [match] query does not support [fields]"
      }
   ]
}
```
</description><key id="22659570">4164</key><summary>Multi_match fails if fields param is not an array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-14T12:27:13Z</created><updated>2013-12-06T08:37:35Z</updated><resolved>2013-11-14T14:55:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>multi_match query supports fields as single string and array</comment></comments></commit></commits></item><item><title>WeightedFilterCache::FilterCacheValueWeigher does not use key size when calculating weight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4163</link><project id="" key="" /><description>You might consider this not to be a big deal, but this discrepancy may cause significant inaccuracies in cache size calculation, especially in the case of the value being empty (which equates to a size of just 1 byte in the current calculation). For an empty value, the real cost of that entry in the cache is far higher as the key may be hundreds of bytes in size.
</description><key id="22633847">4163</key><summary>WeightedFilterCache::FilterCacheValueWeigher does not use key size when calculating weight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dspangen</reporter><labels /><created>2013-11-14T00:25:08Z</created><updated>2014-09-05T10:50:57Z</updated><resolved>2014-09-05T10:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-14T12:20:55Z" id="28479600">That is correct. There isn't really a good way to get the size in bytes of the filter (which is used as the key...), it requires enabling in Lucene (which we have already internally discussed) to make it possible. You can always use the cache key option on filter to control the key the filter is stored under, btw.
</comment><comment author="dspangen" created="2013-11-14T21:26:09Z" id="28524331">Understood. Just noticed that our cache was using far more memory than 20%, and digging in noticed that the keys weren't part of the size calculation. In our case, our keys were a ~5% of our heap; our values were ~22% (and the cache itself took up a non-trivial amount of space as well).

I also wanted to report this in case someone else sees something similar with cache sizing that seems out of whack with their indices.cache.filter.size parameter.
</comment><comment author="kimchy" created="2013-11-15T09:44:41Z" id="28557553">Make sense..., it is annoying, and we should find a way to solve it one way or another. By the way, there is another more subtle complication, which is properly counting the filter (assuming we can size it) because of how segment based filter caching works. For example, a search on a shard with 8 segment comes in, the same filter instance will be used as the cache key for all 8 and should be counted once. If another segment gets created, and the same search request is executed again, then we need to count that filter again (or potentially do key based lookup and reuse the same instance..., which can prove to be expensive...)
</comment><comment author="markharwood" created="2014-09-05T10:50:47Z" id="54610633">It is hard for us to compute the memory cost of the keys involved as they are the complex, composite objects that are the results of the query parsing process. Rather than try and solve this complex memory reporting problem we generally advise that users provide a more efficient form of key in the first place using a custom _cache_key provided as part of the request.
This approach is documented here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filters.html

While we could have chosen to do this key-making internally and automatically by using a hashing function (we like to be helpful!) in this case it would introduce the possibility of hash collisions which is a risk we do not want to assume responsibility for. That is why we rely on clients providing us with the cache key to be associated with the request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TransportClient.connectedNodes should contain up to date node info when using SimpleNodeSampler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4162</link><project id="" key="" /><description>The transport client exposes a list of nodes it's connected to via the connectedNodes property. If sniffing is turned off (which is default), that list is not updated to contain actually node information, like ID, name, attributes etc.

Having that information is handy and can avoid confusing people which verify the properties of the nodes they are connected to.
</description><key id="22601934">4162</key><summary>TransportClient.connectedNodes should contain up to date node info when using SimpleNodeSampler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-13T16:11:07Z</created><updated>2014-05-22T13:33:00Z</updated><resolved>2013-11-13T16:37:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientTests.java</file></files><comments><comment>Make SimpleNodeSampler populate the list of connected nodes using the information returned from the cluster</comment></comments></commit></commits></item><item><title>make term statistics and term vectors accessible in scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4161</link><project id="" key="" /><description>Below is a minimal example which you can run to see how it works in principle. Documentation lists all options.

At this point I would first like to make sure that the general concept is OK. After that I will add complete javadoc etc.

closes #3772

Minimal example:

```
DELETE paytest

PUT paytest
{
    "mappings": {
        "test": {
            "properties": {
                "text": {
                    "index_analyzer": "fulltext_analyzer",
                    "type": "string"
                }
            }
        }
    },
    "settings": {
        "analysis": {
            "analyzer": {
                "fulltext_analyzer": {
                    "filter": [
                        "my_delimited_payload_filter"
                    ],
                    "tokenizer": "whitespace",
                    "type": "custom"
                }
            },
            "filter": {
                "my_delimited_payload_filter": {
                    "delimiter": "+",
                    "encoding": "float",
                    "type": "delimited_payload_filter"
                }
            }
        },
        "index": {
            "number_of_replicas": 0,
            "number_of_shards": 1
        }
    }
}


POST paytest/test/1
{
    "text": "the+1 quick+2 brown+3 fox+4 is quick+10"
}

POST paytest/test/2
{
    "text": "the+1 quick+2 red+3 fox+4"
}

POST paytest/_refresh

#get the total term frequency for "quick"

POST paytest/_search
{
    "script_fields": {
       "ttf": {
          "script": "_index[\"text\"][\"quick\"].ttf()"
       }
    }
}

# get the term frequencies
POST paytest/_search
{
    "script_fields": {
       "freq": {
          "script": "_index[\"text\"][\"quick\"].freq()"
       }
    }
}
POST paytest/test/2/_termvector
# get the payloads which are floats in this case
POST paytest/_search
{
    "script_fields": {
       "payloads": {
          "script": "term = _index[\"text\"].get(\"red\",_PAYLOADS);payloads = []; for(pos : term){payloads.add(pos.payloadAsFloat(-1));} return payloads;"
       }
    }
}

#compute very simple score: just use the term frequency
POST paytest/_search
{
   "script_fields": {
      "tv": {
         "script": "_index[\"text\"][\"quick\"].freq()"
      }
   },
   "query": {
      "function_score": {
         "functions": [
            {
               "script_score": {
                  "script": "_index[\"text\"][\"quick\"].freq()"
               }
            }
         ]
      }
   }
}

```

and so on. see documentation for all options.

Note that `_index` was called `_shard` before. See #4584
</description><key id="22590812">4161</key><summary>make term statistics and term vectors accessible in scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2013-11-13T13:27:29Z</created><updated>2014-06-13T04:54:58Z</updated><resolved>2014-01-02T11:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-11-14T12:25:20Z" id="28479826">Was talking to @s1monw and we now think we have to optimize more. For example use posting list info instead of term vectors unless term vectors are explicitly requested, do not initialize all positions, payloads, ... unless the user really wants to. Also, we should make available an iterator for the positions. Will be back with a new version soon. 
</comment><comment author="clintongormley" created="2013-12-09T12:51:18Z" id="30129375">Hi @brwe 

I've tidied up the docs a bit: https://gist.github.com/clintongormley/6af45dbfb60617264a94

I think the docs should be on their own page - there is a lot of content and it is for advanced use only, so definitely shouldn't be above the common `mvel` functions.

A few things:

`getDocCount()` is actually `docCount()` i've updated in the docs

This is currently only available inside `function_score`, eg you can't use this inside `script_fields`.  Should you be able to? It'd be nice for debugging, eg being able to dump the `POS_OBJECT` as a JSON string.

`_DO_NOT_RECORD` feels wrong -- perhaps it should be called `_CACHE` instead? (that would assume it is off by default. i'm guessing most people would iterate once).  If you think it should be on by default, try `_NO_CACHE`

`numDocs` should be called `maxDocs`, leaving `numDocs` to be `maxDocs - Deleted`

Does `docCount` include deleted docs or not? Can we calculate the count for live docs only?

Wondering if `sumttf` and `sumdf` should be `total_ttf` and `total_df` instead.

Why do we use `tf` in some places and `freq` in others?

Should we also expose `idf`?

I don't understand the note at the end about the Lucene `Field`
</comment><comment author="brwe" created="2013-12-10T16:07:57Z" id="30240593">@clint Thanks for cleaning up the docs! Here is what I made of your comments:
1. Move doc to different file: Can I just put that at a separate file that is called "advancedscripting" and list it right under "scripting" and also link it in the scripting guide?
2. Currently only available in `function_score`: No, it is actually available for native scripts that inherit from `AbstractSearchScript` and for `mvel` scripts regardless of the context. I use the scripts as scriptfield and in `function_score` in the tests. 
3. Rename `_DO_NOT_RECORD` -&gt; `_NO_CACHE` or `_CACHE` depending on default: Names sound much better indeed. Will change the default also, so that users must set `_CACHE` if they want to iterate twice
4. `numDocs`, `maxDoc`, `deletedDocs` : I added the `maxDoc` method. As far as I understand in lucene `maxDoc` is the number of documents without subtracting the deleted ones, `numDocs` is the `maxDoc`-`numDeleted` (my doc comment was wrong). I changed code and documentation accordingly.
5. Can `docCount` be computed for live docs only?: No, unfortunately not possible. I added a comment to the docs.
6. Rename `sumttf` -&gt; `total_tf` and `sumdf` -&gt; `total_df`: In lucene it is called that way which is why I thought for consistency it might make sense to call it so. Also, I think `sum` tells you a little more about the nature of the value than `total`. But I am not too passionate, if you insist I'll change it.
7. Inconsistent naming  `freq()` vs `...tf()`: Right, will rename `freq()` -&gt; `tf()`. Is then different to lucene but the naming makes more sense.
8. Expose `idf`:  inverted document frequency is a value computed from `docCount` and `df`. Do you mean we should expose the value that is computed in the lucene tfidf scoring? The only reason to do so that I could think of is saving time, that is: instead of computing the value once per doc, use the pre computed value stored already. However, if a user wishes more flexibility, I think it makes more sense to provide a mechanism to evaluate scripts once before search actually starts and then we would not have to expose pre-computed values. Does that make sense?
9. lucene `Field`: I added a hopefully better explanation:

[float]
=== Term vectors:

The implementation so far does not give you the term vector but rather statistics for single terms. If you want to gather information on all terms in a field, you must store the term vectors (set `term_vector` in the mapping as described in the &lt;&lt;mapping-core-types,mapping documentation&gt;&gt;). To access them, call
`_shard.getTermVectors()` to get a
https://lucene.apache.org/core/4_0_0/core/org/apache/lucene/index/Fields.html[Fields]
instance. This object can then be used as described in https://lucene.apache.org/core/4_0_0/core/org/apache/lucene/index/Fields.html[lucene doc] to iterate over fields and then for each field iterate over each term in the field.
The method will return null if the term vectors were not stored.

Is that better?
</comment><comment author="clintongormley" created="2013-12-10T16:29:29Z" id="30242762">&gt; Move doc to different file: Can I just put that at a separate file that is called "advancedscripting" and list it right under "scripting" and also link it in the scripting guide?

Yes, you can just add another include statement.

&gt; Currently only available in function_score: No, it is actually available for native scripts that inherit from AbstractSearchScript and for mvel scripts regardless of the context. I use the scripts as scriptfield and in function_score in the tests.

Ah that was probably me trying `getDocCount()` which resulted in an error.

&gt; Rename sumttf -&gt; total_tf and sumdf -&gt; total_df: In lucene it is called that way which is why I thought for consistency it might make sense to call it so. Also, I think sum tells you a little more about the nature of the value than total. But I am not too passionate, if you insist I'll change it.

I don't feel strongly about it, it was just a suggestion. I'm open to what others think here.

&gt; Expose idf: inverted document frequency is a value computed from docCount and df. Do you mean we should expose the value that is computed in the lucene tfidf scoring? The only reason to do so that I could think of is saving time, that is: instead of computing the value once per doc, use the pre computed value stored already. However, if a user wishes more flexibility, I think it makes more sense to provide a mechanism to evaluate scripts once before search actually starts and then we would not have to expose pre-computed values. Does that make sense?

Yes it makes sense. I don't know if idf is useful or not - was just a suggestion in case it had been overlooked

&gt; lucene Field: I added a hopefully better explanation:

yes, much more understandable :)
</comment><comment author="brwe" created="2013-12-10T16:54:46Z" id="30245425">Thank you all so much for the comments! I added another commit that implements the changes. The two things I did not do yet:
1. [`setDocId()`](https://github.com/elasticsearch/elasticsearch/pull/4161/#discussion_r8194098) and [`setNextReader()`](https://github.com/elasticsearch/elasticsearch/pull/4161/#discussion_r8194082) called twice needs to be looked into and potentially an own issue
2. Rename `sumttf, sumdf` -&gt; `total_ttf, total_df` is undecided yet. If I get no more opinions on that, I'll leave it as is.
</comment><comment author="brwe" created="2013-12-19T09:57:51Z" id="30917003">Implemented all changes as discussed
</comment><comment author="s1monw" created="2013-12-20T16:19:20Z" id="31020994">I left a couple of minor comments. I guess you can just fix them and push to master!
</comment><comment author="brwe" created="2014-01-02T11:29:27Z" id="31447710">Pushed to master (1ede9a5) and 0.90 (d1f753e)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make Stored Compression Configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4160</link><project id="" key="" /><description>Allow to make the stored compression configurable on the index level (`index.store.compression.level`), and use that when constructing the codec to use.

If possible (will be tricky), allow to set it on a live index, so for example, for time base indices, older indices can move to use higher compression mode and optimized (which should probably be done over a different issue once the setting is implemented).
</description><key id="22580640">4160</key><summary>Make Stored Compression Configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>stalled</label></labels><created>2013-11-13T09:51:19Z</created><updated>2014-12-11T06:14:47Z</updated><resolved>2014-12-11T06:14:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="avleen" created="2013-11-13T20:32:01Z" id="28431052">This would be a huge plus to us right now - we're getting ready to set up a very large cluster (~30Tb+ data), and the current compression (which is designed to be a somewhat compressive, but mostly _fast_) is painful.

Being able to set a high compression rate, and accept that it will make things slightly slower, is a really big win.
</comment><comment author="kimchy" created="2013-11-14T12:19:03Z" id="28479511">Indeed, note, this will require some changes in Lucene as well, which we are working on to make it happen. We obviously will keep this issue updated with progress.
</comment><comment author="s1monw" created="2014-03-12T20:04:28Z" id="37457178">pushing out to `1.2`
</comment><comment author="avleen" created="2014-07-05T17:13:10Z" id="48091480">Hi folks, we're into `1.2` now. Is this still on the map? I would still gladly give up some Elasticsearch CPU time for better compression :-)
</comment><comment author="kimchy" created="2014-07-05T17:15:22Z" id="48091541">@avleen this is tricky to implement, effectively, it means that we would somehow change Lucene configuration (codec) to have high LZ4 compression on the fly, which is not possible today. We are investigating how to properly expose it in Lucene safely, which proves to be a bit tricky. The hope is to address it soon, but no concrete timeframe. /cc @jpountz 
</comment><comment author="avleen" created="2014-07-07T05:24:11Z" id="48141956">Thanks Shay!

On Sat, Jul 5, 2014 at 1:15 PM, Shay Banon notifications@github.com wrote:

&gt; @avleen https://github.com/avleen this is tricky to implement,
&gt; effectively, it means that we would somehow change Lucene configuration
&gt; (codec) to have high LZ4 compression on the fly, which is not possible
&gt; today. We are investigating how to properly expose it in Lucene safely,
&gt; which proves to be a bit tricky. The hope is to address it soon, but no
&gt; concrete timeframe. /cc @jpountz https://github.com/jpountz
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/4160#issuecomment-48091541
&gt; .
</comment><comment author="jpountz" created="2014-09-03T09:45:11Z" id="54273777">FYI there is now an open issue on the Lucene side: https://issues.apache.org/jira/browse/LUCENE-5914
</comment><comment author="kimchy" created="2014-12-11T06:14:47Z" id="66577783">closed in favor of #8863
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for Lucene's new SimpleQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4159</link><project id="" key="" /><description>Lucene's new SimpleQueryParser is designed to be able to parse human-entered queries without throwing any exceptions, we should add this and expose it in Elasticsearch.

See https://issues.apache.org/jira/browse/LUCENE-5336
</description><key id="22535086">4159</key><summary>Add support for Lucene's new SimpleQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>feature</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-11-12T17:15:21Z</created><updated>2013-12-12T19:22:47Z</updated><resolved>2013-12-12T19:20:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-11-12T17:21:12Z" id="28313621">nice one! should probably be named `simple_query_string`
</comment><comment author="kimchy" created="2013-11-13T10:49:59Z" id="28385263">I think this potentially will end up being the more common "free" text option, so `simple_query_string` as a name is not good. We can either have a new (better) name, have an option on `match` query to use it, or have an option on `query_string` to use it. 

@clintongormley thoughts?
</comment><comment author="clintongormley" created="2013-11-13T12:02:17Z" id="28389189">I think the `match` query already does a lot of stuff and we shouldn't overload it with parsing query strings as well.  

I'd like to to see a new query dedicated to parsing query languages, not just this simple one.  It should support different "dialects", and each dialect may have different flags and options that can be configured. Perhaps the `parsed` query? Or `human`?  `search_box`?  

/me is short on inspiration here

The `query_string` query already has a lot of options and a whole lot of history, so i would prefer to avoid it too.  That said, a number of those parameters are things which we would need to add to this new multi-dialect query.
</comment><comment author="nik9000" created="2013-11-13T13:26:26Z" id="28393539">I could see a `parsed` or a `smart` query type with with a `dialect` option.  I can imagine the documentation page for it listing all the options and all the `dialect`s to which they apply.  It would help to make sure that all the options that do the same thing have the same name.
</comment><comment author="s1monw" created="2013-11-13T13:27:30Z" id="28393611">oh yeah `smart` comes right after `fast` :)
</comment><comment author="nik9000" created="2013-11-13T13:35:01Z" id="28394118">Can the `dialect` for `query_string` be `cranky`?

In all seriousness, `smart` is a very marketing-y name and I withdraw my pseudo-recommendation for it.  I don't like `parsed` though.  `interpreted` has baggage and isn't much better than `parsed` from a "what does this thing do?"

I mean, what word say "explodes user provided text into potentially complex query?"  `exploded`?  
</comment><comment author="clintongormley" created="2013-11-13T13:58:20Z" id="28395800">`dwim`? :)
</comment><comment author="dakrone" created="2013-11-14T17:01:34Z" id="28501683">I have this implemented (without tests yet) as the `simple` query type here: https://github.com/dakrone/elasticsearch/compare/add-simple-query-parser

Here's what this looks like so far:

In simplest form:

``` json
{
  "query": {
    "simple": {
      "description": "foo bar"
    }
  }
}
```

And with all the options:

``` json
{
  "query": {
    "simple": {
      "query": "foo bar",
      "fields": ["body^3", "name"],
      "default_operator": "and",
      "analyzer": "myanalyzer"
    }
  }
}
```

How does this look? I'd like to nail down a name/syntax before writing the tests/documentation/javadocs and submitting a pull request.
</comment><comment author="dakrone" created="2013-11-14T17:18:17Z" id="28503285">Here's my thoughts on why it should be a separate query:
- It doesn't fit in `query_string` for me since you can't prefix terms with a field like `name:foo`, and doesn't support the same query syntax (which could be really confusing).
- It doesn't fit in `match` for me since it includes "special" characters, ie `word + (thing | "other stuff") -bad`
</comment><comment author="clintongormley" created="2013-11-14T17:33:30Z" id="28504603">Hi @dakrone 

I just want to make sure that, whatever name we choose, we don't end up blocking the possibility of adding other dialects later on.  Some of those dialects MAY understand the field: prefix, or handle wildcards, fuzzy etc, so I don't want the form of this simple query parser to be the only thing we consider when choosing a name.

I do agree that this simple syntax probably does make sense as the default dialect in whatever this query is called.  

Regarding the short form of the query: 

```
"query": {
    "name_to_be_chosen": {
        "description": "foo bar"
    }
}
```

The `simple` dialect doesn't have the field: prefix, so this form looks fine.  However it is likely that other dialects will have field: .  We're just about to deprecate the `field` query in favour of just the `query_string` query for exactly this reason.  So I'm wondering whether we should support the short form or not.

Thoughts?
</comment><comment author="dakrone" created="2013-11-14T17:36:34Z" id="28504903">&gt; So I'm wondering whether we should support the short form or not.
&gt; Thoughts?

I'd be in favor of that, the only reason I actually added it was to "simplify" this query to try and get it down to something that looks like a `term`/`match` query. I understand the concern for future dialects, so I can go either way.
</comment><comment author="clintongormley" created="2013-11-15T11:01:59Z" id="28561620">So then all that's left is choosing the name :) 

`syntax` ?  `parsed`? `dsl`? `bazinga`?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/XSimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Add new `simple_query_string` query type</comment></comments></commit></commits></item><item><title>Add a blocking variant of close() method to BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4158</link><project id="" key="" /><description>Using BulkProcessor I often want to block when calling BulkProcessor.close() until all  BulkRequests have completed.

Currently this isn't possible. If you use concurrentRequests &gt; 0, calling close() will execute a new bulk request asynchronously then return immediately.  

It's possible to write a wrapper around BulkProcessor to implement this behaviour, but it's a bit tricky as you have to understand the Exception handling quirks of BulkProcessor and be aware that every call to listener.beforeBulk() will not guarantee a corresponding call to listener.afterBulk().  

It would be nice to have this behaviour implemented as part of the core library.  I suggest using this method name:

boolean awaitClose(long timeout, TimeUnit unit) throws InterruptedException;
</description><key id="22529402">4158</key><summary>Add a blocking variant of close() method to BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">matt-preston</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2013-11-12T16:01:09Z</created><updated>2014-07-17T14:33:08Z</updated><resolved>2014-07-17T14:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-13T11:01:33Z" id="28385940">Sounds good to me!, would you be interested at working on it?
</comment><comment author="matt-preston" created="2013-11-13T11:12:20Z" id="28386566">Absolutely, I have something working locally, so I'll finish it up and submit a pull request.
</comment><comment author="s1monw" created="2014-07-17T14:29:18Z" id="49314081">Looks good.. I will pull that in soon
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file></files><comments><comment>[Bulk] Add blocking close method to BulkProcessor</comment></comments></commit></commits></item><item><title>Size docIdsToLoad appropriately in SearchService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4157</link><project id="" key="" /><description>Closes #4156
</description><key id="22526064">4157</key><summary>Size docIdsToLoad appropriately in SearchService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">markelliot</reporter><labels /><created>2013-11-12T15:22:18Z</created><updated>2014-06-14T06:44:49Z</updated><resolved>2013-11-18T12:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-18T12:36:48Z" id="28694658">Pulled into master and 0.90. Thanks for contributing @markelliot! 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/scroll/SearchScrollTests.java</file></files><comments><comment>Added test for PR #4157 (deep pagination in a small index)</comment></comments></commit></commits></item><item><title>SearchContext maintains a `size` length `int[]` when there are fewer than `size` results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4156</link><project id="" key="" /><description>Reproduce with:

```
Node node = NodeBuilder.nodeBuilder().node();
Client client = node.client();

client.prepareIndex("twitter", "twitter")
    .setSource("{ \"user\" : \"kimchy\", \"post_date\" : \"2009-11-15T14:12:12\", \"message\" : \"trying out Elastic Search\" }")
    .execute()
    .get();

WildcardQueryBuilder query = QueryBuilders.wildcardQuery("message", "trying");
SearchRequestBuilder request = client.prepareSearch("twitter")
        .setSearchType(SearchType.DFS_QUERY_AND_FETCH)
        .setQuery(query)
        .setSize(100000)
        .setScroll("1m");
SearchResponse response = request.execute().get();
for (SearchHit hit : response.hits()) {
    System.out.println(hit.getId());
}

client.close();
node.close();
```

Observe by setting a breakpoint in `SearchContext#docIdstoLoad(int[], int, int)` and notice that despite creating a brand new index with only one result the `int[]` is sized at 100,000, to match the specified size.

This is leading to unwanted memory pressure, and appears to be fixable by editing `SearchService#shortcutDocIdsToLoad(SearchContext)` to size the `docIdsToLoad` array to the actual number of results.
</description><key id="22523638">4156</key><summary>SearchContext maintains a `size` length `int[]` when there are fewer than `size` results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">markelliot</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-12T14:53:20Z</created><updated>2013-11-18T12:34:37Z</updated><resolved>2013-11-18T12:34:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markelliot" created="2013-11-18T03:18:25Z" id="28673035">I ran into an issue where an application that makes extensive use of Elasticsearch was making high-rate scroll-type searches with a size set to 100,000, but for many of the searches we were seeing only 0 or 1 results. Regardless of the result size, Elasticsearch was creating int[100000], and eventually OOM'd as a result. We've been running the patch in afb2a5a for about a week now and despite the application continuing to make the same kind of requests, we're no longer experiencing OOMs.

It seems like this would help in terms of stability in memory use. I'm new to open source contribution and to ES, so I'm not sure what the standard process looks like, but I've filled out and submitted the forms described on the contributions page.

Any thoughts around if/when this PR would get incorporated, or something I should be doing in order to help the process move along?
</comment><comment author="martijnvg" created="2013-11-18T11:53:29Z" id="28692407">@markelliot Thanks for opening and fixing this issue! This looks good and I'll pull it in.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Size docIdsToLoad appropriately in SearchService</comment></comments></commit></commits></item><item><title>Release semaphore if client call throws and exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4155</link><project id="" key="" /><description>Closes #4153
</description><key id="22520338">4155</key><summary>Release semaphore if client call throws and exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-12T14:08:22Z</created><updated>2014-06-13T18:38:53Z</updated><resolved>2013-11-12T14:47:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-12T14:32:33Z" id="28297381">LGTM
</comment><comment author="javanna" created="2014-06-13T15:56:45Z" id="46028315">As a side note, `afterBulk` is now called symmetrically with `beforeBulk`, see #6495. I think that the feature proposal still makes a lot sense though, hopefully we can merge that in soon.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for ParseContext.externalValue() to all build-in field mappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4154</link><project id="" key="" /><description>`ParseContext.externalValue()` is not supported by some build-in field mappers (for example: GeoPointFieldMapper or BooleanFieldMapper)

i guess this feature was implemented as a 'hack', but it will be great if it could be somehow formalized and propagated to other field mappers

i need it for my external plugin (i want to be able to inject value for every build-in field mapper)
</description><key id="22518378">4154</key><summary>Add support for ParseContext.externalValue() to all build-in field mappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2013-11-12T13:39:50Z</created><updated>2014-03-14T14:55:50Z</updated><resolved>2014-03-14T14:55:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-12T15:41:58Z" id="28303683">I think this is related to #2573 as well.
</comment><comment author="dadoonet" created="2014-03-14T14:55:50Z" id="37655522">Closing in favor of #4986 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalIndexModule.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapperPlugin.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/RegisterExternalTypes.java</file></files><comments><comment>Support externalValue() in mappers</comment></comments></commit></commits></item><item><title>Deadlock in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4153</link><project id="" key="" /><description>I'm experiencing a deadlock in BulkProcessor during heavy bulk indexing to a single node cluster.

Occasionally, during heavy indexing, the node becomes unresponsive causing the client to throw NoNodeAvailableException, triggering the issue.

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java#L279-305

A semaphore is acquired before the call to client.bulk(request, listener) and only released in the callback.  If an Exception, such as NoNodeAvailableException, is thrown from client.bulk(request, listener), the semaphore is never released.

When the number of Exceptions thrown by client.bulk(request, listener) is greater than the number of concurrent requests supported by BulkProcessor a deadlock occurs and no more documents can be indexed.

BulkProcessor should be able to handle this by wrapping client.bulk(request, listener) inside a try/catch block and handling the Exception, calling the listener and releasing the semaphore.
</description><key id="22515345">4153</key><summary>Deadlock in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">matt-preston</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-12T12:41:16Z</created><updated>2014-09-23T16:32:54Z</updated><resolved>2013-11-12T14:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-12T13:08:38Z" id="28291652">good call! we are looking into this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file></files><comments><comment>Release semaphore if client call throws and exception</comment></comments></commit></commits></item><item><title>Define a dynamic not_analyzed field for a nested document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4152</link><project id="" key="" /><description>I have a document like below, the "tags" field is a nested document, and I want to make all child field for tags document to be index = not_analyzed. The problem is that field in tags will be dynamic. any tag could possible.
So how I can define dynamic mapping for this. 

```
{
 strong text'level': 'info',
 'tags': {
  'content': u'Nov  6 11:07:10 ja10 Keepalived_healthcheckers: Adding service [172.16.08.105:80] to VS [172.16.1.21:80]',
  'id': 1755360087,
  'kid': '2012121316',
  'mailto': 'yanping3,chunying,pengjie',
  'route': 15,
  'service': 'LVS',
  'subject': 'LVS_RS',
  'upgrade': 'no upgrade configuration for this alert'
 },
 'timestamp': 1383707282.500464
}
```
</description><key id="22495772">4152</key><summary>Define a dynamic not_analyzed field for a nested document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kerwin</reporter><labels /><created>2013-11-12T03:36:27Z</created><updated>2013-11-12T03:46:05Z</updated><resolved>2013-11-12T03:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-12T03:46:05Z" id="28265768">@kerwin we have very active [mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch), where you can ask questions like this one. I am sure somebody there will be able to assist you with this problem. The github issues should be used for feature requests and bug reporting. Thank you for your understanding. Please, see http://www.elasticsearch.org/help for more information. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove heap used from _cat/nodes and add RAM percentage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4151</link><project id="" key="" /><description>In this view you never care about the actual heap used bytes; you only want to know that your max is set to what you meant and what percentage you're currently using.
</description><key id="22479355">4151</key><summary>Remove heap used from _cat/nodes and add RAM percentage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.Beta2</label></labels><created>2013-11-11T21:49:47Z</created><updated>2013-11-15T02:53:52Z</updated><resolved>2013-11-15T02:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file></files><comments><comment>Drop the heap and RAM used from _cat/nodes and add the RAM percentage.</comment></comments></commit></commits></item><item><title>accessibility of includeInAll field (in field mappers)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4150</link><project id="" key="" /><description>all field mappers that implement AllFieldMapper.IncludeInAll interface also define includeInAll field, there is no public getter for that field. 

do you think it will be good idea to add it to it to  AllFieldMapper.IncludeInAll interface:

``` java
    public interface IncludeInAll extends Mapper {

        Boolean includeInAll(); // addidtional method to return includeInAll flag

        void includeInAll(Boolean includeInAll);

        void includeInAllIfNotSet(Boolean includeInAll);
    }
```

i need it for my external plugin
</description><key id="22467155">4150</key><summary>accessibility of includeInAll field (in field mappers)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2013-11-11T18:35:01Z</created><updated>2015-09-21T20:27:42Z</updated><resolved>2015-09-21T20:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-21T20:27:41Z" id="142099341">I don't think we should make `IncludeInAll` any more complicated than it already is. Instead, we should remove it altogether. There should be no inheritance, it should be a direct setting on fields.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>External plugin: stored fields taken from _source (even when _source is disabled)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4149</link><project id="" key="" /><description>if _source is still 'warm' in transaction log and fields query parameter is specified,  requested stored fields are extracted from _source (this happens even when _source is disabled). 

This behavior is causing issues for me, as im trying to create external plugin that will allow indexing of computed/scripted fields. Computed fields are not present in _source, so when marking them as stored, they still cant be immediately retrieved using fields query parameter. They are available eventually after transaction log gets flushed. 

It will be great if it will be possible to change this behavior or at least override it on index level.
</description><key id="22458840">4149</key><summary>External plugin: stored fields taken from _source (even when _source is disabled)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2013-11-11T16:26:11Z</created><updated>2014-08-08T18:25:07Z</updated><resolved>2014-08-08T18:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:25:07Z" id="51639699">The `_source` parameter has been added explicitly to solve this issue.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A problem when term searching with Chinese.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4148</link><project id="" key="" /><description>I am doing text searching via ElasticSearch, and There is a problem on querying with term type. What I am doing below is basically,
1. Add a document with Chinese string (你好).
2. Querying with text method, and it return the document.
3. Querying with term method, return nothing.

So, Why it's happen? and how to resolve it.

```
➜  curl -XPOST 'http://localhost:9200/test/test/' -d '{ "name" : "你好" }'
{"ok":true,"_index":"test","_type":"test","_id":"VdV8K26-QyiSCvDrUN00Nw","_version":1}%

➜  curl -XGET 'http://localhost:9200/test/test/_mapping?pretty=1'
{
  "test" : {
    "properties" : {
      "name" : {
        "type" : "string"
      }
    }
  }
}% 

➜  curl -XGET 'http://localhost:9200/test/test/_search?pretty=1'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "VdV8K26-QyiSCvDrUN00Nw",
      "_score" : 1.0, "_source" : { "name" : "你好" }
    } ]
  }
}%                                                                                                                                                                                

➜  curl -XGET 'http://localhost:9200/test/test/_search?pretty=1' -d '{"query" : {"text" : { "name" : "你好" }}}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.8838835,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "VdV8K26-QyiSCvDrUN00Nw",
      "_score" : 0.8838835, "_source" : { "name" : "你好" }
    } ]
  }
}%                                                                                                                                                                                

➜  curl -XGET 'http://localhost:9200/test/test/_search?pretty=1' -d '{"query" : {"term" : { "name" : "你好" }}}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}%
```
</description><key id="22439269">4148</key><summary>A problem when term searching with Chinese.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kerwin</reporter><labels /><created>2013-11-11T10:13:58Z</created><updated>2013-11-12T03:27:31Z</updated><resolved>2013-11-12T03:27:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-12T03:27:31Z" id="28265143">@kerwin it looks like you are using standard analyzer, which splits your text into 2 terms: "你" and "好". 

```
curl "localhost:9200/_analyze?pretty" -d '你好'
{
  "tokens" : [ {
    "token" : "你",
    "start_offset" : 0,
    "end_offset" : 1,
    "type" : "&lt;IDEOGRAPHIC&gt;",
    "position" : 1
  }, {
    "token" : "好",
    "start_offset" : 1,
    "end_offset" : 2,
    "type" : "&lt;IDEOGRAPHIC&gt;",
    "position" : 2
  } ]
}
```

The text query also analyzes the string, splits it into two tokens and as a result finds the record, the term query doesn't analyze the string and searches for the token "你好", which doesn't exist in your index (remember it was split into two tokens). 

Closing the issue. If you have any future questions, please don't hesitate to ask them on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndexShardGatewayService should not call post_recovery if shard is in STARTED state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4147</link><project id="" key="" /><description>At the end recovery, the IndexShardGatewayService will double check the gateway has moved the shard to POST_RECOVERY and if not, do it it self.
The shard state could have already move to started, causing the post_recovery call to throw an exception and the entire shard recovery to fail.

This can happened if, after the gateway moved the shard to POST_RECOVERY:
1) master sent a new cluster state indicating shard is initializing
2) IndicesClusterStateService#applyInitializingShard will send a shard started event
3) Master will mark shard as started and this will be processed quickly and move the shard to STARTED.
</description><key id="22432087">4147</key><summary>IndexShardGatewayService should not call post_recovery if shard is in STARTED state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-11T07:50:46Z</created><updated>2013-11-11T07:56:50Z</updated><resolved>2013-11-11T07:56:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file></files><comments><comment>IndexShardGatewayService should not call post_recovery if shard is in STARTED state.</comment></comments></commit></commits></item><item><title>IndexShardGatewayService should not call post_recovery if shard is in STARTED state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4146</link><project id="" key="" /><description>At the end recovery, the IndexShardGatewayService will double check the gateway has moved the shard to POST_RECOVERY and if not, do it it self.
The shard state could have already move to started, causing the post_recovery call to throw an exception and the entire shard recovery to fail.

This can happened if, after the gateway moved the shard to POST_RECOVERY:
1) master sent a new cluster state indicating shard is initializing
2) IndicesClusterStateService#applyInitializingShard will send a shard started event
3) Master will mark shard as started and this will be processed quickly and move the shard to STARTED.
</description><key id="22420092">4146</key><summary>IndexShardGatewayService should not call post_recovery if shard is in STARTED state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-11-10T23:23:05Z</created><updated>2014-07-16T21:51:24Z</updated><resolved>2013-11-11T07:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-10T23:32:33Z" id="28164318">looks good!
</comment><comment author="bleskes" created="2013-11-11T07:58:19Z" id="28179833">committed to master and 0.90 : f9af329443df67e2ad090a4ac08ed6e8eff07ab4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose maximum heap settings + heap usage in percent in JVM memory stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4145</link><project id="" key="" /><description>Currently the OS section of the node returns memory usage in bytes but also as percent of the total memory. The JVM stats only return heap usage information but not the configured max or the usage in percent.  The max heap settings is handy to have and the usage percent is a useful shortcut.
</description><key id="22415736">4145</key><summary>Expose maximum heap settings + heap usage in percent in JVM memory stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-10T19:53:20Z</created><updated>2013-11-10T20:12:31Z</updated><resolved>2013-11-10T20:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java</file></files><comments><comment>Added heap_max(_in_bytes) and heap_used_percent to JVM node stats.</comment></comments></commit></commits></item><item><title>Fixes bug in bool filter where it doesn't emit docs as hits while they are hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4144</link><project id="" key="" /><description>There is an optimization that executes bit based (slow) filters in the end. Matched docs could be unset if they didn't match with any of these filters. The bug was that also iterator based (fast) filters should be checked.
This change checks all should filters in the end part (if must or must_not clauses exists), so it can now correctly unset matched docs. The current bool filters requires that at least one should clause must match for docs to be match regardless of any other clauses.

Relates to #4130
</description><key id="22411379">4144</key><summary>Fixes bug in bool filter where it doesn't emit docs as hits while they are hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-10T16:34:37Z</created><updated>2015-05-18T23:33:40Z</updated><resolved>2013-11-13T11:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-12T17:34:14Z" id="28314774">@martijnvg I tried hard to understand this thing and I failed so I left some comments in there to make it simpler. I rather spend some time writing a test that compare this beast against boolean query and it bumped up some failures on the your PR as well as on master etc. If fails on master all the times but passes with the comments I made regarding `advance` calls in `iteratorMatch` so I think you are doing ok here but this test should help you to get this to a good state! here is the commit https://github.com/elasticsearch/elasticsearch/commit/904c0abb3e8ce6ed26890b37633ff798a36bd7de

I annotated it with awaitsfix so you can just pull it in and rebase.

simon
</comment><comment author="s1monw" created="2013-11-13T10:50:45Z" id="28385302">I think we can push this and open a new issue to refactor this beast! makes sense @martijnvg 
</comment><comment author="martijnvg" created="2013-11-13T11:02:36Z" id="28386003">Pushed to 0.90 and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make sure rivers get started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4143</link><project id="" key="" /><description>With #3782 we changed the execution order of dynamic mapping updates and index operations. We now first send the mapping update to the master node, and then we index the document. This makes sense but caused issues with rivers as they are started due to the cluster changed event that is triggered on the master node right after the mapping update has been applied, but in order for the river to be started its _meta document needs to be available, which is not the case anymore as the index operation most likely hasn't happened yet. As a result in most of the cases rivers don't get started.
What we want to do is retry a few times if the _meta document wasn't found, so that the river gets started anyway.

Closes #4089, #3840
</description><key id="22411353">4143</key><summary>Make sure rivers get started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-10T16:33:09Z</created><updated>2014-07-10T15:33:08Z</updated><resolved>2013-11-10T20:04:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-10T16:38:10Z" id="28154191">I think the retry logic should execute in a way that it also gets the latest cluster state, current one will continuously use the same cluster state change event that happened before.

If this becomes too complicated, we can simply block the rivers execution with a time based retry, if we can't get the meta document in 5 seconds (which is reasonable I think), then something is really wrong...
</comment><comment author="dadoonet" created="2013-11-10T16:53:12Z" id="28154504">I think we should add _start / _stop endpoints to rivers. That was an old feature request BTW.
Then, when we create the river, we don't autostart it anymore.

That said, it's perhaps too complicated to implement it now and does not worth it.
</comment><comment author="javanna" created="2013-11-10T18:55:00Z" id="28157482">Makes sense @kimchy ! I tend to prefer the scheduled retry over the blocking wait. Just updated my commit, I think the code complexity is still acceptable. Let me know what you think!
</comment><comment author="kimchy" created="2013-11-10T19:19:30Z" id="28158021">looks good to me
</comment><comment author="javanna" created="2013-11-10T20:04:35Z" id="28159100">Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bool filter should execute the same as the bool query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4142</link><project id="" key="" /><description>Right now at least one should clause in a bool filter should match with a document, otherwise this document isn't a match regardless if there are any other must or must_not clause. Even though the bool filter doesn't have a `minumum_should_match` open the bool filter behaves as if `minumum_should_match` is set to 1.

This behaviour is inconsistent with the bool query and I would like to see that the bool filter works the same as the bool query with regards to the should clauses, so in the case should clauses are defined when there are must_not or must clauses then the should clause are irrelevant. If there are only should clauses defined a document just needs to match with a single should clause.

I don't think we should support `minumum_should_match` option as is in bool query (makes bool filter execution expensive), but maybe have an option that enables &lt;= 0.90.x behaviour.
</description><key id="22411215">4142</key><summary>Bool filter should execute the same as the bool query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label></labels><created>2013-11-10T16:24:38Z</created><updated>2014-01-07T11:54:46Z</updated><resolved>2013-12-17T11:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-11T11:50:19Z" id="28192462">@martijnvg i'm not sure this makes sense for the bool filter.  `should` clauses in the query affect relevance, but the filter works with plain boolean logic.  if we were to set `minimum_should_match` to zero in the bool filter, then no `should` clauses would be required, and you may as well not execute any of them. 

In other words you lose the ability to write the following with a single `bool` query:

```
WHERE foo=1 AND (bar = 2 OR baz=2)
```
</comment><comment author="martijnvg" created="2013-11-11T15:30:27Z" id="28208978">@clintongormley I don't like the implicit `minimum_should_match=1`. I think it makes sense to ignore the `should` clauses from a boolean logic point of view if `must` or `must_not` is specified. If someone wants the `should` clauses also to matter (in addition to the specified `must` / `must_not`), then the option that enables the `minimum_should_match=1` behaviour should be enabled. We just need to add this `minimum_should_match=1` option.
</comment><comment author="clintongormley" created="2013-11-11T15:56:20Z" id="28211374">Why would you disable the should clauses? If you don't want to run the should clauses, then don't specify them.  The `bool` filter is a different beast from the `bool` query because the filter is pure boolean logic.  

So you need to support AND, OR and NOT.  With `must` and `must_not` you have AND and NOT, but not OR.  Why should I have to add the `minimum_should_match` parameter just to re-enable OR?
</comment><comment author="martijnvg" created="2013-11-13T21:40:07Z" id="28436541">I get your point about not specifying `should` clauses at all when these clauses shouldn't be taken into account.

Personally I think the bool filter should behave the same as if I would wrap the bool query into a filter, but that isn't the case right now, because of the implicit `minimum_should_match=1`. Functionally I see the `bool` query as an extension of the `bool` filter with scoring is an addition.
</comment><comment author="lmenezes" created="2013-11-19T15:59:18Z" id="28801834">hey, sorry about jumping in... 

I actually got here looking for something related to a bug I found(and it's actually already solved https://github.com/elasticsearch/elasticsearch/issues/2979. really need to update...).

But anyway, I do think that the implicit minimum_should_match=1 is good. At the end, it doesn't make sense making optional the should block as a whole(if it doesn't affect score and it's optional, you might as well throw it away).

On the other hand, it could be interesting being able to increase the value of minimum_should_match(even though that makes things more expensive).

currently, If I want 2 out of 3 clauses to match(using a filter), what would be the recommended approach? And would it be more performant than having this configurable minimum_should_match inside the boolfilter?
</comment><comment author="martijnvg" created="2013-11-21T09:38:56Z" id="28969680">&gt; But anyway, I do think that the implicit minimum_should_match=1 is good. At the end, it doesn't make sense making optional the should block as a whole(if it doesn't affect score and it's optional, you might as well throw it away).

That makes sense, however I'm still leaning towards `minimum_should_match=0` as default for consistency with the bool query.

&gt; currently, If I want 2 out of 3 clauses to match(using a filter), what would be the recommended approach? And would it be more performant than having this configurable minimum_should_match inside the boolfilter?

I would wrap the bool query in a query filter, this should give what you want. Btw I started PR #4208 that includes changes that allow the bool filter to support `minimum_should_match`.
</comment><comment author="lmenezes" created="2013-11-21T09:49:44Z" id="28970541">@martijnvg minimum_should_match=0 in case you have no must/must_not, right? as long as changing the minimum_should_match is possible, i'm cool with that :)

about the 2 out of 3 clauses, i didn't really mean how to achieve it, but rather as an argument on how other approaches would compare in performance to allowing changing minimum_should_match inside boolfilter. but seeing you already started a PR for that, i guess it answers my question as well. 

cool, looking forward to it.
</comment><comment author="martijnvg" created="2013-11-21T10:08:31Z" id="28971628">@lmenezes I think the default for `minimum_should_match`  should be `0` in the case if there're must or must_not clauses. If there are only `should` clauses then the default `minimum_should_match` should be `1`. I want the `minimum_should_match` to be changeable as well :)
</comment><comment author="lmenezes" created="2013-11-21T10:16:57Z" id="28972122">yep, messed up with the no must, it should be in case you have must/must_not clauses. :)
</comment><comment author="clintongormley" created="2013-12-17T11:11:34Z" id="30742839">We've agreed that defaulting `minimum_should_match` to 0 for the  `bool` filter is impractical.  Would be nice in the future to make `minimum_should_match` configurable at the cost of heavier execution, but closing this ticket for now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for human flag to all REST APIs that output settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4141</link><project id="" key="" /><description>Closes #4140
</description><key id="22397913">4141</key><summary>Add support for human flag to all REST APIs that output settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-11-10T02:02:06Z</created><updated>2014-07-01T21:41:59Z</updated><resolved>2014-01-13T17:10:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-10T09:36:37Z" id="28147199">This is a great one. I wonder if the `human` flag is the correct one or not to use for this? For example, I think that this is how we should always output the settings, regardless of the `human` flag (and potentially allow for a "flat_settings` flag option?). I can't see a case where flat would be better when asking for it through the API?
</comment><comment author="clintongormley" created="2013-11-10T11:30:25Z" id="28148648">Agreed - the only reason I can see for not making this the default would be bwc concerns.  but perhaps there aren't any...
</comment><comment author="awick" created="2013-11-10T12:11:04Z" id="28149233">Flat make is much easier to programaticly check if a setting is present or not, so it would be nice to retain some method to access, doesn't neeed to be default.  Although not impossible to rewrite code to walk the levels :)
</comment><comment author="synhershko" created="2013-11-10T16:20:40Z" id="28153829">+1 @awick. The flat format is much easier to type and look for, especially if (once) the Sense UI supports it.
</comment><comment author="kimchy" created="2013-12-17T09:16:48Z" id="30736112">I see, then maybe `flat_settings` set to `true` (defaults to `false`), read from Params? I would love to get this into 1.0.
</comment><comment author="imotov" created="2013-12-18T03:09:09Z" id="30812791">Sent an alternative PR with `flat_settings` - #4497. I don't have a strong opinion either way. I just want this functionally to be available somehow :-) .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Settings should be rendered in more readable way when human flag is set to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4140</link><project id="" key="" /><description>Settings in many REST APIs are hard to read because they are presented in a flattened format. REST APIs that output settings should observe the `human` flag and render settings in a structured format if flag `human` is `true`. 

Without human flag:

```
$ curl -XGET 'http://localhost:9200/test-idx/_settings?pretty'
{
  "test-idx" : {
    "settings" : {
      "index.analysis.analyzer.default.filter.1" : "lowercase",
      "index.analysis.analyzer.default.filter.2" : "stop",
      "index.analysis.analyzer.default.filter.0" : "standard",
      "index.analysis.analyzer.default.tokenizer" : "uax_url_email",
      "index.analysis.analyzer.default.type" : "custom",
      "index.number_of_shards" : "5",
      "index.number_of_replicas" : "1",
      "index.version.created" : "1000002",
      "index.uuid" : "_VSsOKiwTJ-uq8-3n74nog"
    }
  }
}
```

with human flag:

```
$ curl -XGET 'http://localhost:9200/test-idx/_settings?pretty&amp;human'
{
  "test-idx" : {
    "settings" : {
      "index" : {
        "uuid" : "_VSsOKiwTJ-uq8-3n74nog",
        "analysis" : {
          "analyzer" : {
            "default" : {
              "type" : "custom",
              "filter" : [ "standard", "lowercase", "stop" ],
              "tokenizer" : "uax_url_email"
            }
          }
        },
        "number_of_replicas" : "1",
        "number_of_shards" : "5",
        "version" : {
          "created" : "1000002"
        }
      }
    }
  }
}
```
</description><key id="22397892">4140</key><summary>Settings should be rendered in more readable way when human flag is set to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-11-10T02:00:41Z</created><updated>2014-01-08T15:40:07Z</updated><resolved>2014-01-08T15:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-10T07:51:00Z" id="28145998">+1 to the feature!
</comment><comment author="clintongormley" created="2013-11-10T11:27:36Z" id="28148595">Oh yes please!!!

I really wanted to ask for this, but couldn't figure out how to use it without breaking bwc.  The human flag is a great way of doing it!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>src/test/java/org/elasticsearch/common/settings/ImmutableSettingsTests.java</file></files><comments><comment>Add support for flat_settings flag to all REST APIs that output settings</comment></comments></commit></commits></item><item><title>Fix NPE in has_child filter and query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4139</link><project id="" key="" /><description>Fixed NPE if matched parent docs is higher than short_circuit_cutoff.
Closes #4135
</description><key id="22394068">4139</key><summary>Fix NPE in has_child filter and query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-09T21:20:47Z</created><updated>2015-05-18T23:33:40Z</updated><resolved>2013-11-11T13:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-09T22:43:26Z" id="28138918">looks good
</comment><comment author="s1monw" created="2013-11-10T07:49:40Z" id="28145971">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Javadoc] Fix copy/paste comment on YAML type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4138</link><project id="" key="" /><description>Copy/paste from SMILE type
</description><key id="22382782">4138</key><summary>[Javadoc] Fix copy/paste comment on YAML type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arey</reporter><labels /><created>2013-11-09T09:00:05Z</created><updated>2014-07-16T21:51:27Z</updated><resolved>2013-11-12T09:21:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-12T09:21:07Z" id="28278684">I just merged this in: 2afdb4c8e7974cd26b78c5f25d3068e3f63fc148

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fixed typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4137</link><project id="" key="" /><description>I'm going through and finally reading all of the Elasticsearch documentation, so I'll just accumulate any typo/style problems I find here. If it ends up being a lot of commits, I can rebase it into one so it can be applied more easily.
</description><key id="22371499">4137</key><summary>fixed typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">deoxxa</reporter><labels /><created>2013-11-08T23:34:32Z</created><updated>2014-07-16T21:51:28Z</updated><resolved>2013-12-02T09:10:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-02T09:10:02Z" id="29603082">Pushed into master and 0.90, see https://github.com/elasticsearch/elasticsearch/commit/87246af256563f227d4e0b6b2b2af7f37e2fba19

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change default store impl to mmapfs on 64bit Linux.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4136</link><project id="" key="" /><description>Fixes #4134
</description><key id="22362570">4136</key><summary>Change default store impl to mmapfs on 64bit Linux.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">aochsner</reporter><labels /><created>2013-11-08T20:39:47Z</created><updated>2014-06-16T18:28:38Z</updated><resolved>2013-11-09T18:41:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-11-09T18:41:56Z" id="28133781">Merged in 914fd29
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE for has_child query if number of results exceed certain limit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4135</link><project id="" key="" /><description>If the number of results exceed a certain limit, has_child query throws a NullPointerException.  

```
curl -x '' -s -XPOST "http://localhost:9200/myindex/vendor/_search?from=0&amp;size=20&amp;pretty=true&amp;routing=0" -d '{
   "query" : {
      "has_child" : {
         "query" : {
            "query_string" : { "query" : "signed_date:[now-120d TO now-90d]" }
         },
         "type" : "transaction"
      }
   }
}
'

[2013-11-08 15:01:41,825][DEBUG][action.search.type       ] [Phage] [myindex_20131108][1], node[fuBzjMWDQgmr_qm0cVLIog], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@23b13dc6]
org.elasticsearch.transport.RemoteTransportException: [Ch'od][inet[/192.168.175.128:9301]][search/phase/query+fetch]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [myindex_20131108][1]: query[filtered(child_filter[transaction/vendor](filtered(signed_date:[1373572895448 TO 1376164895448])-&gt;cache(_type:transaction)))-&gt;cache(_type:vendor)],from[0],size[20]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:306)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:686)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:1)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
        at org.elasticsearch.index.search.child.ChildrenConstantScoreQuery$ParentWeight.scorer(ChildrenConstantScoreQuery.java:178)
        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:533)
        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:133)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:119)
        ... 7 more
[2013-11-08 15:01:41,827][DEBUG][action.search.type       ] [Phage] All shards failed for phase: [query_fetch]
```

I debugged the code and found the issue happens due to class ChildrenConstantScoreQuery.java, line #115 is passing null shortCircuitFilter. This happens when remaining value &gt; 8192 so it's not initialized 

```
        Filter shortCircuitFilter = null;
        if (remaining == 1) {
            BytesRef id = collectedUids.v().iterator().next().value.toBytesRef();
            shortCircuitFilter = new TermFilter(new Term(UidFieldMapper.NAME, Uid.createUidAsBytes(parentType, id)));
        } else if (remaining &lt;= shortCircuitParentDocSet) {
            shortCircuitFilter = new ParentIdsFilter(parentType, collectedUids.v().keys, collectedUids.v().allocated);
        }

        ParentWeight parentWeight = new ParentWeight(parentFilter, shortCircuitFilter, searchContext, collectedUids);
```

This is bug introduced after 0.90.1 as the query was running fine in past. Thanks!
</description><key id="22361155">4135</key><summary>NPE for has_child query if number of results exceed certain limit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">ajhalani</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-08T20:15:00Z</created><updated>2013-11-11T13:42:29Z</updated><resolved>2013-11-11T13:42:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-08T22:44:14Z" id="28104322">@martijnvg can you look into this? 
</comment><comment author="martijnvg" created="2013-11-09T21:21:34Z" id="28137285">Thanks for reporting this issue @ajhalani! We will fix this soon.
</comment><comment author="ajhalani" created="2013-11-10T05:03:51Z" id="28144247">You got it, thanks for getting to it quickly! 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Fixed NPE if matched parent docs is higher than short_circuit_cutoff.</comment><comment>Closes #4135</comment></comments></commit></commits></item><item><title>Make mmapfs the default store for 64-bit Linux</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4134</link><project id="" key="" /><description>It appears that ElasticSearch wants to mimic Lucene's defaults in FSDirectory#open (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/store/IndexStoreModule.java#L51)

Lucene's defaults now include Linux in addition to Solaris and Windows when considering MMapDirectory (https://issues.apache.org/jira/browse/LUCENE-3198)

ElasticSearch should follow suit and, according to a lot of discussions on the mailing list, many are already doing this via configuration.
</description><key id="22360089">4134</key><summary>Make mmapfs the default store for 64-bit Linux</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">aochsner</reporter><labels><label>enhancement</label><label>v1.0.0.Beta2</label></labels><created>2013-11-08T19:58:17Z</created><updated>2013-11-09T18:42:40Z</updated><resolved>2013-11-09T18:35:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-11-09T18:37:10Z" id="28133661">Hi @aochsner, I've merged this only to master, since it's a pretty major change and we don't want to change the defaults for the 0.90.x releases. It will be part of the next 1.0 beta release.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/IndexStoreModule.java</file></files><comments><comment>Change default store impl to mmapfs on 64bit Linux.</comment><comment>Fixes #4134</comment></comments></commit></commits></item><item><title>Root field mappers duplicated in DocumentMapper and DocumentMapperParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4133</link><project id="" key="" /><description>Hi,

looks like `DocumentMapper` is creating its own list of root field mappers:

``` java
 ...
            // UID first so it will be the first stored field to load (so will benefit from "fields: []" early termination
            this.rootMappers.put(UidFieldMapper.class, new UidFieldMapper());
            this.rootMappers.put(IdFieldMapper.class, idFieldMapper);
            this.rootMappers.put(RoutingFieldMapper.class, new RoutingFieldMapper());
            // add default mappers, order is important (for example analyzer should come before the rest to set context.analyzer)
            this.rootMappers.put(SizeFieldMapper.class, new SizeFieldMapper());
            this.rootMappers.put(IndexFieldMapper.class, new IndexFieldMapper());
            this.rootMappers.put(SourceFieldMapper.class, new SourceFieldMapper());
            this.rootMappers.put(TypeFieldMapper.class, new TypeFieldMapper());
            this.rootMappers.put(AnalyzerMapper.class, new AnalyzerMapper());
            this.rootMappers.put(AllFieldMapper.class, new AllFieldMapper());
            this.rootMappers.put(BoostFieldMapper.class, new BoostFieldMapper());
            this.rootMappers.put(TimestampFieldMapper.class, new TimestampFieldMapper());
            this.rootMappers.put(TTLFieldMapper.class, new TTLFieldMapper());
            this.rootMappers.put(ParentFieldMapper.class, new ParentFieldMapper());
 ...
```

and `DocumentMapperParser` is creating its own:

``` java
...
        rootTypeParsers = new MapBuilder&lt;String, Mapper.TypeParser&gt;()
                .put(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser())
                .put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser())
                .put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser())
                .put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser())
                .put(AllFieldMapper.NAME, new AllFieldMapper.TypeParser())
                .put(AnalyzerMapper.NAME, new AnalyzerMapper.TypeParser())
                .put(BoostFieldMapper.NAME, new BoostFieldMapper.TypeParser())
                .put(ParentFieldMapper.NAME, new ParentFieldMapper.TypeParser())
                .put(RoutingFieldMapper.NAME, new RoutingFieldMapper.TypeParser())
                .put(TimestampFieldMapper.NAME, new TimestampFieldMapper.TypeParser())
                .put(TTLFieldMapper.NAME, new TTLFieldMapper.TypeParser())
                .put(UidFieldMapper.NAME, new UidFieldMapper.TypeParser())
                .put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser())
                .immutableMap();
...
```

From external plugin it is possible to modify only root type parsers from `DocumentMapperParser`, but it is the list from `DocumentMapper` that is used when creating `ParsedDocument` instance.

BTW,
im trying to create plugin that will add support for computed fields (based on script), i wanted to borrow some 'inspiration' from how '_all' field is implemented (as it is in a way computed field). Is implementing root field mapper the right way to do it ? I tried to use field mappers (not root), but looks like they are not used unless field value is present in indexed document (which is kinda problem for computed fields, as they are never present in source document)
</description><key id="22358581">4133</key><summary>Root field mappers duplicated in DocumentMapper and DocumentMapperParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2013-11-08T19:34:46Z</created><updated>2013-11-08T19:36:28Z</updated><resolved>2013-11-08T19:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karol-gwaj" created="2013-11-08T19:36:28Z" id="28091123">double clicked (dup: #4132)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Root field mappers duplicated in DocumentMapper and DocumentMapperParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4132</link><project id="" key="" /><description>Hi,

looks like `DocumentMapper` is creating its own list of root field mappers:

``` java
 ...
            // UID first so it will be the first stored field to load (so will benefit from "fields: []" early termination
            this.rootMappers.put(UidFieldMapper.class, new UidFieldMapper());
            this.rootMappers.put(IdFieldMapper.class, idFieldMapper);
            this.rootMappers.put(RoutingFieldMapper.class, new RoutingFieldMapper());
            // add default mappers, order is important (for example analyzer should come before the rest to set context.analyzer)
            this.rootMappers.put(SizeFieldMapper.class, new SizeFieldMapper());
            this.rootMappers.put(IndexFieldMapper.class, new IndexFieldMapper());
            this.rootMappers.put(SourceFieldMapper.class, new SourceFieldMapper());
            this.rootMappers.put(TypeFieldMapper.class, new TypeFieldMapper());
            this.rootMappers.put(AnalyzerMapper.class, new AnalyzerMapper());
            this.rootMappers.put(AllFieldMapper.class, new AllFieldMapper());
            this.rootMappers.put(BoostFieldMapper.class, new BoostFieldMapper());
            this.rootMappers.put(TimestampFieldMapper.class, new TimestampFieldMapper());
            this.rootMappers.put(TTLFieldMapper.class, new TTLFieldMapper());
            this.rootMappers.put(ParentFieldMapper.class, new ParentFieldMapper());
 ...
```

and `DocumentMapperParser` is creating its own:

``` java
...
        rootTypeParsers = new MapBuilder&lt;String, Mapper.TypeParser&gt;()
                .put(SizeFieldMapper.NAME, new SizeFieldMapper.TypeParser())
                .put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser())
                .put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser())
                .put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser())
                .put(AllFieldMapper.NAME, new AllFieldMapper.TypeParser())
                .put(AnalyzerMapper.NAME, new AnalyzerMapper.TypeParser())
                .put(BoostFieldMapper.NAME, new BoostFieldMapper.TypeParser())
                .put(ParentFieldMapper.NAME, new ParentFieldMapper.TypeParser())
                .put(RoutingFieldMapper.NAME, new RoutingFieldMapper.TypeParser())
                .put(TimestampFieldMapper.NAME, new TimestampFieldMapper.TypeParser())
                .put(TTLFieldMapper.NAME, new TTLFieldMapper.TypeParser())
                .put(UidFieldMapper.NAME, new UidFieldMapper.TypeParser())
                .put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser())
                .immutableMap();
...
```

From external plugin it is possible to modify only root type parsers from `DocumentMapperParser`, but it is the list from `DocumentMapper` that is used when creating `ParsedDocument` instance.

BTW,
im trying to create plugin that will add support for computed fields (based on script), i wanted to borrow some 'inspiration' from how '_all' field is implemented (as it is in a way computed field). Is implementing root field mapper the right way to do it ? I tried to use field mappers (not root), but looks like they are not used unless field value is present in indexed document (which is kinda problem for computed fields, as they are never present in source document)
</description><key id="22357522">4132</key><summary>Root field mappers duplicated in DocumentMapper and DocumentMapperParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2013-11-08T19:18:21Z</created><updated>2013-11-11T16:30:19Z</updated><resolved>2013-11-11T16:30:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karol-gwaj" created="2013-11-11T16:30:19Z" id="28214543">ok, i think i jumped the gun here, although root mappers are defined in two different places, it is not causing real issues with registering custom root type parsers. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Path, index_name, multi_fields and pooled fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4131</link><project id="" key="" /><description>This issue tries to summarise a number of other open issues:

The `{path: "just_name"|"full"}` and `index_name` parameters cause a lot of confusion, eg see #4123 and #3457

For instance, consider this mapping:

```
{
   "mappings": {
      "t": {
         "properties": {
            "one": {
               "properties": {
                  "foo": {
                     "type": "multi_field",
                     "path": "just_name",
                     "fields": {
                        "foo": {
                           "type": "string"
                        },
                        "shared": {
                           "type": "string",
                           "index_name": "shared"
                        }
                     }
                  }
               }
            },
            "two": {
               "properties": {
                  "foo": {
                     "type": "multi_field",
                     "path": "just_name",
                     "fields": {
                        "foo": {
                           "type": "string",
                           "index_name": "two.foo"
                        },
                        "shared": {
                           "type": "string",
                           "index_name": "shared"
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
```

The `just_name` setting can only be applied to all sub-fields and pushes all of this fields up to the top-level, so the above mapping ends up indexing fields `one.foo` and `two.foo` into the single field `foo`.  This can be worked around by setting the `index_name` for each of these fields to `one.foo` and `two.foo` respectively, but it is confusing and verbose. 

One possibility is to support `index_name` as above, and `index_path` to specify an absolute path name, which can include `.`'s.  I'd deprecate `path` altogether - it just adds confusion.
## What are the use cases for `index_name` and `path`?

While you may for some reason use `index_name` to provide a field alias, the most common use case by far is with multi-fields, to create a custom `all` field, like the example above. Also see #3093.

The current syntax implies that the mapping from each `shared` sub-field is applied to the values from `one.foo` and `two.foo` before being indexed into the `shared` field.  See #4108. This is not the case - one of the mappings get chosen and applied.

It would be great to support the application of each mapping at index time, and then make searching on `{ fields: ["one.foo.shared", "two.foo.shared"]}` do the right thing, but I'm not sure how feasible this is. (See #1169)

If it isn't feasible, then we should change the syntax to make that obvious.

One suggestion was made here: https://github.com/elasticsearch/elasticsearch/issues/4099#issuecomment-27981537
ie:

```
{
    "my_type": {
        "properties": {
            "name": {
                "properties": {
                    "first": {"type":"string", "copy_to":["name.full"]},
                    "last": {"type":"string", "index_name": "surname", "copy_to":["name.full"]},
                    "full": {"type": "string"}
                }
            }
        }
    }
}
```

This has its own issues: `copy_to` would end up indexing the values from `first` under both `first` and `full`, although you could use `index: no` on the `first` field to only index under `full`.  

Also, the mapping for `full` may not have been seen while parsing the mapping for `first`.  I don't know if this is an issue or not - no data will be added until index time, and if `full` hasn't been defined, then dynamic mapping should be able to auto-add that field?

I know @imotov has some ideas of alternate ways to specify the same thing. See #2035.
## Multi-fields are verbose

While we're talking about multi-fields, the syntax is verbose, eg it's a big change from:

```
{ title: { type: "string"}
```

to:

```
{
    title: {
        type: "multi_field",
        fields: {
           title: { type: "string"},
           raw: { type: "string", index: "not_analyzed"}
        }
    }
}
```

Could we not simplify this, by supporting `"fields"`  under all core field types, eg:

```
{
    title: {
        type: "string",
        fields: {
           raw: { index: "not_analyzed"}
        }
    }
}
```

The sub-fields could inherit the mappings of the main field, so you only need to specify the changes.
## Shared fields and index time boost

If we can support the application of different mappings on shared fields (ie values in `one.foo` get tokenised according to the mapping in `one.foo.shared` before being indexed into `shared`, and similar thing at search time)  then we should also look at the possibility of supporting field-level index-time boosting.

This is supported in the `_all` field, using payloads.  Each term stores the `boost` setting of the field where that value originated.  I realise that this makes queries slower, but it is a useful feature (and about the only use case for field-level boosting at index time).

Would it be possible to only enable payloads on shared fields where at least one of the fields specifies a `boost`? See #4108 
## Recommendations
1. Deprecate `index_name` and `path`
2. Support `index_path` if we really see the need to rename the field
3. Add simpler, more obvious syntax for indexing values into a shared field
4. Make multi_field mappings less verbose
5. Look at the possibility of supporting the application of independent mappings at index/search time, and possibly index time boost.
</description><key id="22353062">4131</key><summary>Path, index_name, multi_fields and pooled fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-11-08T17:59:00Z</created><updated>2014-01-13T11:36:13Z</updated><resolved>2013-12-19T16:46:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-11-08T18:07:09Z" id="28084086">+1, great write up!  As an FYI, the `copy_to` option was meant to have the value indexed in the original field and the copy field.
</comment><comment author="nik9000" created="2013-11-08T18:11:36Z" id="28084443">1.  +1
2.  I'm really not sure why you'd want to rename the field.  Maybe you mentioned it above and I was just reading too fast.
3.  +1
4.  The more I think about this the more it grows on me.  I think it might have clarity issues but those are rapidly evaporating the more I think about it....
</comment><comment author="clintongormley" created="2013-11-08T18:12:51Z" id="28084532">@nik9000 re 2 - no i didn't mention it above, i was just leaving the option open in case there was a use case i hadn't seen.
</comment><comment author="nik9000" created="2013-11-08T18:14:27Z" id="28084642">@clintongormley re 2 - sounds good.  I wonder if there is something really compelling I've just missed.  Thanks.
</comment><comment author="roytmana" created="2013-11-08T18:18:18Z" id="28084914">Great summary. Many thanks for following up on the submitted issues! It is very pleasing to work wit a very responsive dev. team
</comment><comment author="clintongormley" created="2013-12-19T16:46:37Z" id="30944567">After discussion with @imotov I've split this issue into two separate issues: #4520 and #4521.

Comments welcome!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: Wrong result on bool filter with 'must_not' and 'geo_distance'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4130</link><project id="" key="" /><description>The Bool Filter returns the wrong result when combining the 'must_not' (or 'must') part with a 'should' containing a 'geo_distance' filter.

Consider following example:

```
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test
curl -XPUT 'localhost:9200/test/test/_mapping' -d '{"test" : {"properties" : { "prop1" : {"type":"string"}, "location": {"type":"geo_point"}}}}'

curl -XPUT 'localhost:9200/test/test/1' -d '{"prop1": "5", "location":{"lon":"0", "lat":"0"}}'
curl -XPUT 'localhost:9200/test/test/2' -d '{"prop1": "5", "location":{"lon":"0", "lat":"0"}}'
curl -XPUT 'localhost:9200/test/test/3' -d '{"prop1": "4", "location":{"lon":"12", "lat":"12"}}'
```

Example query 1:

```
{
"filter" : {
    "bool": {
        "must_not": [
            {
                "terms": {
                    "_id": [
                        "1"
                    ]
                }
            }
        ],
        "should": [
            {
                "term": {
                    "_name": "filter_requirements",
                    "prop1": "5"
                }
            },
            {
                "geo_distance": {
                    "_name": "filter_location",
                    "distance": "6km",
                    "location": {
                        "lat": "12", 
                        "lon": "12"
                    }
                }
            }
        ]
    }
}}}
```

This returns only the document with the id 3. It should return both, 2 and 3 because 2 also matches the 'should' part. Document with id 1 is excluded as expected.

Example query 2:

```
{"filter" : {
    "bool": {
        "should": [
            {
                "term": {
                    "_name": "filter_requirements",
                    "prop1": "5"
                }
            },
            {
                "geo_distance": {
                    "_name": "filter_location",
                    "distance": "6km",
                    "location": {
                        "lat": "12",
                        "lon": "12"
                    }
                }
            }
        ]
    }
}}}
```

This returns all 3 documents as expected. Note that the 'must_not' part has been removed. Bool filter works as expected.

Example query 3:

```
{"filter" : {
    "bool": {
        "must_not": [
            {
                "terms": {
                    "_id": [                       
                        "1"     
                    ]
                }
            }
        ],                       
        "should": [                            
            {                         
                "term": {        
                    "_name": "filter_requirements",
                    "prop1": "5"   
                }    
            }    
        ]
    }
}}}
```

This will correctly return only the document with id 2. The example above does not show this.. and it's kinda late now. You can remove either the 'should not' or the 'geo_distance' and the query will execute correctly.
</description><key id="22345840">4130</key><summary>Query DSL: Wrong result on bool filter with 'must_not' and 'geo_distance'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Akii</reporter><labels><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-08T16:07:00Z</created><updated>2013-11-13T11:00:54Z</updated><resolved>2013-11-13T11:00:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-08T19:20:48Z" id="28089831">@Akii Thanks for reporting this! I can verify this issue and will fix it soon.
</comment><comment author="Akii" created="2013-11-08T19:43:37Z" id="28091736">Wow that was quick. Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/XBooleanFilterTests.java</file></files><comments><comment>Fixed bug in bool filter where it doesn't emit docs as hits while they are hits.</comment></comments></commit></commits></item><item><title>Lazily fill CharTermAttribute if needed in CompletionTokenStream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4129</link><project id="" key="" /><description>This adds a delegate to CharTermAttributeImpl to be compatible
with the Percolator that needs a CharTermAttribute. Yet compared
to CharTermAttributImpl we only fill the BytesRef with UTF-8 since
we already have it and only if we need to convert to UTF-32 we do it.

Closes #4028
</description><key id="22342139">4129</key><summary>Lazily fill CharTermAttribute if needed in CompletionTokenStream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-08T15:11:26Z</created><updated>2014-07-13T01:47:42Z</updated><resolved>2013-11-10T19:46:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>[doc] incorrect statsD plugin link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4128</link><project id="" key="" /><description>The statsD plugin link seems to be incorrect here [docs/reference/modules/plugins.asciidoc](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/modules/plugins.asciidoc)? (See "ElasticSearch Statsd Plugin" link).
May be it should point to https://github.com/swoop-inc/elasticsearch-statsd-plugin
</description><key id="22338614">4128</key><summary>[doc] incorrect statsD plugin link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-11-08T14:10:15Z</created><updated>2013-11-08T19:29:59Z</updated><resolved>2013-11-08T19:29:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Fix link to statsd plugin</comment></comments></commit></commits></item><item><title>using JAVA_OPTS might fail in service.bat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4127</link><project id="" key="" /><description>Relates to #4086 
`JAVA_OPTS` and `ES_JAVA_OPTS` can trip `service.bat` if the former contains multiple parameters or/and the former contains parenthesis.
</description><key id="22336672">4127</key><summary>using JAVA_OPTS might fail in service.bat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-08T13:29:14Z</created><updated>2013-11-08T13:35:43Z</updated><resolved>2013-11-08T13:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fix issues with JAVA_OPTS/ES_JAVA_OPTS in service.bat</comment></comments></commit></commits></item><item><title>Package test-framework as individual jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4126</link><project id="" key="" /><description>This commit causes all classes under 'org.elasticsearch.test.*'
to be included in a 'elasticsearch-${version}-test.jar' that can be
inclued by 3rd party projects or plugins via:

```
 &lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
    &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
    &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
    &lt;type&gt;test-jar&lt;/type&gt;
    &lt;scope&gt;test&lt;/scope&gt;
 &lt;/dependency&gt;
```
</description><key id="22327794">4126</key><summary>Package test-framework as individual jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-08T10:03:57Z</created><updated>2014-06-12T08:17:23Z</updated><resolved>2013-11-08T14:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Changed pom to allow import and running from eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4125</link><project id="" key="" /><description>Currently when importing projects into eclipse you need to run 'mvn
eclipse:eclipse' on the command line to generate the poject files. This
means that when the pom changes you need to re-run the command on the
command line to reflect those changes in the project in eclipse.  This
commit allows the developer to import the project as an existing maven
project (can be shared using git after import) and then allows the
application to be run inside eclipse using the .launch file in
/dev-tools enabling easy debugging of the application within eclipse
without requiring a maven build.  Changes are addition of a lauch file and update of the eclipse-lifecycle-plugin so do not affect the normal build or any other IDEs
</description><key id="22304755">4125</key><summary>Changed pom to allow import and running from eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2013-11-07T23:31:13Z</created><updated>2014-08-21T15:07:53Z</updated><resolved>2013-11-21T10:17:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-07T23:38:41Z" id="28018252">As far as I can recall my experience with eclipse, [M2E eclipse plugin](http://wiki.eclipse.org/Maven_Integration) manage all that in a very efficient manner.
Wondering if it still the case.
</comment><comment author="colings86" created="2013-11-07T23:44:19Z" id="28018596">It does, however, with the current settings in the lifecycle mapping plugin cause the copy-dependency=plugin to be skipped and will not build correctly if imported into eclipse using m2e (as existing maven project) because eclipse does not know how to deal with the forbidden-apis-plugin and exec-pliugin.  The update just fixes those minor issues and adds a launch configuration to enable a noe to be run within eclipse
</comment><comment author="dadoonet" created="2013-11-07T23:45:52Z" id="28018688">I see. Thanks for the explanation.
@jpountz As an eclipse user as well, what do you think? Did you hit the same issue?
</comment><comment author="s1monw" created="2013-11-08T08:18:30Z" id="28045374">_Disclaimer: I am an ex eclipse user and I have not much of a clue about maven._

I can't judge the pom changes etc. but from what @colings86 explains this makes lots of sense. I always ignored the problems with forbidden APIs etc. but this sounds useful? One thing about the code change, it seems the formatting is jagged?
</comment><comment author="nik9000" created="2013-11-08T13:38:57Z" id="28062891">I'm a current Eclipse user.  I'll have a look at this in a bit.
</comment><comment author="s1monw" created="2013-11-08T13:41:38Z" id="28063031">@nik9000 thanks!
</comment><comment author="nik9000" created="2013-11-08T14:50:54Z" id="28067687">I've been doing all my Elasticsearch development with Eclipse and m2e so I like this pr.

I suggest also adding something like 

Elasticsearch also works perfectly with Eclipse's [http://www.eclipse.org/m2e/](m2e).  Once you've installed m2e you can import Elasticsearch as an `Existing Maven Project`.

to  CONTRIBUTING.md.
</comment><comment author="colings86" created="2013-11-08T15:01:58Z" id="28068588">Thanks for the feedback everyone. I'll make those changes as suggested and submit a new pull request

Colin

On Fri, Nov 8, 2013 at 2:51 PM, Nik Everett notifications@github.com
wrote:

&gt; I've been doing all my Elasticsearch development with Eclipse and m2e so I like this pr.
&gt; I suggest also adding something like 
&gt; 
&gt; ``````
&gt; Elasticsearch also works perfectly with Eclipse's [http://www.eclipse.org/m2e/](m2e).  Once you've installed m2e you can import Elasticsearch as an `Existing Maven Project`.
&gt; ``` to  CONTRIBUTING.md.
&gt; ---
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/4125#issuecomment-28067687
&gt; ``````
</comment><comment author="nik9000" created="2013-11-08T15:04:40Z" id="28068788">I'd just modify this one.  I typically amend commits and then force push them into github's branch and that changes the pull request.
</comment><comment author="colings86" created="2013-11-08T19:47:40Z" id="28092113">Have updated the pull request with the suggested changes.  The launch file is now less complex so I believe it will work on most if not all versions of eclipse
</comment><comment author="nik9000" created="2013-11-08T19:53:07Z" id="28092564">Ahk!  You reformatted the whole pom file!  I get that sometime when I try to format a section and I haven't properly highlighted just that section.  Can you just format the new lines, please?  That'd make the diff a lot simpler to read.
</comment><comment author="colings86" created="2013-11-08T20:06:05Z" id="28093687">Oops sorry.  Have fixed it now I think and have changed the tabs to indent in block of 4 spaces.  Let me know if there are any more problems
</comment><comment author="nik9000" created="2013-11-08T21:18:21Z" id="28098778">The pom changes are working well for me.  The launch configuration is working, but not well.  Please see my comment a few minutes ago.  Thanks for working on this.  It'll make my life better.
</comment><comment author="colings86" created="2013-11-08T21:27:46Z" id="28099416">Ok I think i can actually remove that -Des.path.home parameter.  I added it to stop the data directory from accidentally being added to git but I actually see that data is already added to the gitignore file so es.path.home is safe to be at the root of the project.
</comment><comment author="nik9000" created="2013-11-09T19:29:05Z" id="28134832">Everything looks great to me.  It seems to merge clean to both 0.90 and master as well.
</comment><comment author="colings86" created="2013-11-18T23:34:04Z" id="28750925">Is there anything more I need to do on this for it to be committed?
</comment><comment author="nik9000" created="2013-11-19T01:04:55Z" id="28756649">Not really, so long as it merges clean with master. 
</comment><comment author="s1monw" created="2013-11-21T10:17:31Z" id="28972155">pushed to `master` and `0.90` via https://github.com/elasticsearch/elasticsearch/commit/76c5f53dfa1ae510e76ada99e521a1e6fc943d25 sorry due to rebase the commit hash got messed up and it's not auto closing. Thanks for the fix!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_default_ mapping not applied when using separate master/data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4124</link><project id="" key="" /><description>In 0.90.6 and 0.90.5, _default_ mappings will not be applied to newly created types if your cluster topology has separate master and data nodes.

Working case (all nodes are master/data eligible) with a two node cluster:

```
# create index 
curl -XPUT http://localhost:9200/test-index
# create _default_ mapping
curl -XPUT http://localhost:9200/test-index/_default_/_mapping -d'{"_default_": {"_timestamp": {"enabled": true}}}'
# verify contents
curl http://localhost:9200/test-index/_default_/_mapping
# &gt;&gt; {"_default_":{"_timestamp":{"enabled":true},"properties":{}}}

# add a new type
curl -XPUT http://localhost:9200/test-index/new-mapping/_mapping -d'{"properties": {"foo": "string"}}'
#verify contents have _default_ applied
curl http://localhost:9200/test-index/new-mapping/_mapping
# &gt;&gt; {"new-mapping":{"_timestamp":{"enabled":true},"properties":{}}}
```

Broken case (master/data nodes separate) with a two node cluster:

Set each node with appropriate settings:

```
# node1
node.master: true
node.data: false

# node2
node.master: false
node.data: true
```

The same set of commands yield a mapping with defaults applied:

```
curl -XPUT http://localhost:9200/test-index
curl -XPUT http://localhost:9200/test-index/_default_/_mapping -d'{"_default_": {"_timestamp": {"enabled": true}}}'
curl http://localhost:9200/test-index/_default_/_mapping
# &gt;&gt; {"_default_":{"_timestamp":{"enabled":true},"properties":{}}}

curl -XPUT http://localhost:9200/test-index/new-mapping/_mapping -d'{"properties": {"foo": "string"}}'
curl http://localhost:9200/test-index/new-mapping/_mapping
# &gt;&gt; {"new-mapping":{"properties":{}}}
```
</description><key id="22303638">4124</key><summary>_default_ mapping not applied when using separate master/data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">olimcc</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-07T23:08:43Z</created><updated>2013-11-08T09:55:10Z</updated><resolved>2013-11-08T09:55:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/test/java/org/elasticsearch/cluster/SpecificMasterNodesTests.java</file></files><comments><comment>_default_ mapping not applied when using separate master/data nodes</comment><comment>fixes #4124</comment></comments></commit></commits></item><item><title>Posting highlighter highlight fields with the same name across all document objects when path:just_name is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4123</link><project id="" key="" /><description>https://gist.github.com/roytmana/f34d71bb80f8bd8f9eea
</description><key id="22292785">4123</key><summary>Posting highlighter highlight fields with the same name across all document objects when path:just_name is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">roytmana</reporter><labels /><created>2013-11-07T20:14:46Z</created><updated>2014-07-09T13:09:58Z</updated><resolved>2014-07-09T13:09:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-07T20:43:40Z" id="28004497">The tricky bit here is that you have two fields, `category.name` and `type.name`, both configured as `"path":"just_name"`. As a result, they get collapsed and merged into the same lucene field, called `name`. 

This is problematic when making highlighting based on the additional information stored in the index, compared to using the plain highlighter that just re-analyzes the text on-the-fly. In fact the content of the field is loaded from the `_source` field, but the offsets relate to a bigger field, which leads to wrong snippets. Exactly the same happens with the fast vector highlighter.

A workaround would be to configure those fields as `"store":"yes"`. Then the content would be loaded from the same lucene field and the offsets would match, but you would get snippets coming from both the json fields.
</comment><comment author="javanna" created="2013-11-07T20:53:59Z" id="28005350">Even better, if you do need to collapse those fields into the same lucene field, I'd suggest to use a `multi_field` and keep a variation of the field with a unique name so that the `just_name` won't be a problem and you can highlight against it.

You could for instance have the following `category.name` multi_field with the `hl` variation that has a unique name (actually the full_path set through the `index_name`) :

```
{
    "category" : {
        "name" : {
            "type":"multi_field",
            "path":"just_name",
            "fields" : {
                "name" : {"type":"string"},
                "hl":{"type":"string","index_name":"category.name.hl"}
            }
        }
    }
}
```
</comment><comment author="roytmana" created="2013-11-07T21:21:33Z" id="28007567">I do not know lucene internals but from ES search API perspective name field is not collapsed into one field across type and category properties and can be searched independently from each other. 
I always assumed their lucene name is their full name (type.name and category.name) just like if i did not use just_name since they are the primary fields of  multifield mappings (their field name matches  property name)

the quick test shows that what you say is correct and primary field 'name' in my case acts no different than 'all' field in my mapping which i intended to COLLAPSE 

If ES does not expand primary field name  to full path, than the whole concept of multi_field is flawed! All ES examples virtually imply that it is not so

I am already using full path name for parts of multifield that I do not want to collapse but I thought the primary fields will be different. I will have to switch to using full path for all field of multifields  that i do not intend to collapse - what a pain
</comment><comment author="roytmana" created="2013-11-07T23:12:25Z" id="28016514">i switched to unique index_name for all primary fields in multifield mapping and highlighting seem to work now 
</comment><comment author="javanna" created="2013-11-07T23:14:23Z" id="28016627">Cool, thanks for your feedback @roytmana !
</comment><comment author="roytmana" created="2013-11-07T23:22:34Z" id="28017166">phew I spent way too much time on highlighting :-)

I still feel that the way multifield handles primary fields is totally counter-intuitive 
at least docs for the multifield should stress using of index_name to ensure its uniqueness

should I close the issue since it is consistent with current behavior of the primary multifield or you are going to research moret?
</comment><comment author="javanna" created="2013-11-08T10:12:42Z" id="28052779">The problem is not related to `multi_field` itself, you perfectly got how `multi_field` works. 

The issue happens when you use `"path":"just_name"` or the same `index_name` for different fields. The way `index_name` and `"path":"just_name"` work is described [here](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-object-type.html#_path_3), maybe we can improve the docs there, although there is an example that describes what happens under the hood.

As for this issue, I don't see a way to fix it unfortunately, but we can leave it open for now so that people know this is actually a problem. What we could do is try to at least not provide snippets (or return an error?) instead of returning wrong snippets, but that wouldn't fix the problem anyway.
</comment><comment author="clintongormley" created="2014-07-09T13:09:58Z" id="48467197">`path` has been removed, so this should no longer be an issue. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added support for external query in postings highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4122</link><project id="" key="" /><description>It is now possible to highlight an external query using the postings highlighter, relates to #3630

Closes #4121
</description><key id="22289500">4122</key><summary>Added support for external query in postings highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-07T19:26:44Z</created><updated>2014-06-30T12:58:54Z</updated><resolved>2013-11-08T10:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-08T09:47:34Z" id="28051454">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postings highlighter to support query other than the search query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4121</link><project id="" key="" /><description>Plain and fast vector highlighter can highlight based on a query which is not the search query, called `highlight_query`, introduced in #3630. Postings highlighter doesn't support the external query yet and always highlights based on the search query.
</description><key id="22286436">4121</key><summary>Postings highlighter to support query other than the search query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-07T18:38:33Z</created><updated>2013-11-08T10:55:04Z</updated><resolved>2013-11-08T10:55:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Added support for external query in postings highlighter</comment></comments></commit></commits></item><item><title>Remove query_and_fetch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4120</link><project id="" key="" /><description>The `query_and_fetch` and `dfs_query_and_fetch` search types are not useful (except for internal optimizations on single shard queries).

Remove them from the API
</description><key id="22277747">4120</key><summary>Remove query_and_fetch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-11-07T16:29:33Z</created><updated>2014-03-12T20:02:56Z</updated><resolved>2014-03-12T20:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-11-07T16:35:02Z" id="27981538">http://www.elasticsearch.org/blog/understanding-query-then-fetch-vs-dfs-query-then-fetch/ references it.  It might be worth writing a new version of it and linking to it from the old source.
</comment><comment author="clintongormley" created="2013-11-07T16:54:20Z" id="27983361">Sorry Nick, I meant query_and_fetch.  Issue updated
</comment><comment author="s1monw" created="2013-11-08T08:21:46Z" id="28045615">hmm I am not sure here to be honest. If I have been working on a bunch of projects that store nothing (not even the `_source` and only return doc Ids. In such a case this can actually safe a lot of roundtrips. I don't think it's useless?
</comment><comment author="lmenezes" created="2013-11-16T10:48:36Z" id="28624172">I agree with @s1monw. And I actually use query_and_fetch for some particular use cases.
</comment><comment author="uboness" created="2013-11-24T15:33:04Z" id="29157840">@s1monw for cases where `_source` is not stored with can do this optimization internally (same as we do if the search is on a single shard)
</comment><comment author="clintongormley" created="2014-01-15T19:25:20Z" id="32399584">@lmenezes would your use cases be served by the optimization that @uboness suggested?
</comment><comment author="lmenezes" created="2014-01-16T09:56:47Z" id="32455291">@clintongormley would it be possible to also do this optimization if all the request fetches are ids, independently of _source being stored or not? That would suit me much better, and I guess it also makes more sense.
</comment><comment author="imotov" created="2014-01-16T16:24:52Z" id="32484481">Several times I saw it being used to retrieve all records (with source and fields) in a single request when number of records in the result list is guaranteed to be small by the application. I think it's important use case.
</comment><comment author="s1monw" created="2014-03-12T20:02:56Z" id="37457017">I will close this for now we can still reopen...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename filter param to search api "post_filter"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4119</link><project id="" key="" /><description>Users tend to use the top-level `filter` param for general filtering, instead of using a `filtered` query.

It should be renamed to `post_filter` to dissuade users from using it unless they understand what it is for and `filter` should be deprecated.

The top level `filter` is still valid field for backwards compatibility, but it will removed in the future.
</description><key id="22277475">4119</key><summary>Rename filter param to search api "post_filter"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-11-07T16:27:19Z</created><updated>2014-09-16T15:41:55Z</updated><resolved>2013-12-16T16:10:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-11-07T16:31:15Z" id="27981193">I assume I'm using it wrong then:)
</comment><comment author="clintongormley" created="2013-11-07T16:33:55Z" id="27981424">:) It should only be used when you want to aggregate facets on the query results, but then filter out some query results before returning the hits, eg:
- query on brand: gucci
- facet on: color
- return results only for: brand:gucci, color: red

If you don't have the faceting requirement, then the color:red filter should be in a filtered query
</comment><comment author="nik9000" created="2013-11-07T16:38:39Z" id="27981894">I'm not sure where I got the idea that I should just shove whatever filters I'm doing there.  Maybe I just inferred it from the parameter name.  Maybe some rogue documentation.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java</file><file>src/main/java/org/elasticsearch/search/query/PostFilterParseElement.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/indices/cache/CacheTests.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java</file></files><comments><comment>Renamed top level `filter` to `post_filter`.</comment></comments></commit></commits></item><item><title>Deprecate partial_fields in the search API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4118</link><project id="" key="" /><description>Now that we have `_source`, `_source_include`, `_source_exclude`, we no longer need `partial_fields` http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fields.html#partial
</description><key id="22277218">4118</key><summary>Deprecate partial_fields in the search API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v1.0.0</label></labels><created>2013-11-07T16:24:54Z</created><updated>2014-05-07T14:16:20Z</updated><resolved>2014-01-21T09:36:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file></files><comments><comment>Deprecated use of partial fields in Java API, was already deprecated in the docs for the search API</comment></comments></commit></commits></item><item><title>Change ignore_malformed to malformed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4117</link><project id="" key="" /><description>When a value is indexed into a numeric field, it can either be:
- an allowed numeric value
- a string which can be cast to a number
- a float which can be cast to an integer, or rounded down to the correct numeric range
- an illegal value

Currently we have `ignore_malformed`.  I suggest changing this to `malformed` which can accept:
- `cast` - the default - casts when it can but throws an error when it can't
- `ignore` - casts when it can, but silently ignores errors
- `strict` - throws an error unless the value is a valid number which fits the field type
</description><key id="22275925">4117</key><summary>Change ignore_malformed to malformed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-11-07T16:06:43Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2014-01-13T18:05:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-07T16:19:40Z" id="27980041">Ping @karmi 
</comment><comment author="karmi" created="2013-11-11T10:47:44Z" id="28189366">This looks very good to me. I''m a bit cautious about the `cast` name, but couldn't come up with something which would feel significantly better. I was thinking about `convert`, `parse`, etc. Would maybe like `convert` a bit more, since it could be used in other places maybe.
</comment><comment author="bleskes" created="2013-11-12T09:10:30Z" id="28278081">I agree `cast` is a bit technical. The ideal situation is to have something that conveys it will try but still throw an error if possible. How about `lenient` (as opposed to strict) - trying to do the best job and fail if not possible.

Also `ignore` to me suggest it will ignore malformed values completely as opposed to try casting them and ignoring errors.  Perhaps we need another mode like `lenient_ignore_errors` (but shorter ... ).
</comment><comment author="karmi" created="2013-11-12T10:08:07Z" id="28281548">@bleskes So your suggestion is to use instead `lenient` instead of `ignore`?

Both do the job to me, with a preference for `lenient`, since that maps well to plain English, "when receiving _malformed_ input, _ignore_ it", assuming that's what we do (ie. do not even index/store the field).

If we do something else, e.g. we receive a malformed value (`"count": "123"`), we index/store it as it is, i.e. as a string instead of number.
</comment><comment author="bleskes" created="2013-11-12T11:04:35Z" id="28284874">@karmi I mean `lenient` instead of `cast`. Sorry if I wasn't clear.
</comment><comment author="clintongormley" created="2013-11-12T11:13:32Z" id="28285388">@karmi @bleskes the last two comments demonstrate the problem :) what does `lenient` mean? `cast` or `ignore`?  I agree that `cast` is a bit technical, but it is accurate.

Possibly we could use `silent`, `convert`, `strict`?

That said, I think I still prefer `ignore`, `cast`, `strict` - they're closer to what is actually happening.  `cast` is easily explained in the docs, and will be the default so most people will never need to change it or even look it up.

But I'm not particularly tied to `cast`, happy to use something else as long as it is short and easy to understand
</comment><comment author="bleskes" created="2013-11-12T11:55:31Z" id="28287570">I have a slight preference to `convert` over `cast`.  

Just to make sure my second point is not lost in the discussion - I think `ignore` is confusing because it still tries to convert things like strings to numbers. Especially because this is not a settings people often set or use, I would prefer something slightly longer but clearer - even going all the way to `convert_ignore_errors`. 
</comment><comment author="clintongormley" created="2013-11-12T11:57:55Z" id="28287707">How about `convert`, `convert_silent`, `strict`, `strict_silent`?
</comment><comment author="bleskes" created="2013-11-12T11:59:36Z" id="28287793">+1
</comment><comment author="GlenRSmith" created="2013-11-12T14:41:20Z" id="28298083">coerce? ;-)
</comment><comment author="karmi" created="2013-11-13T09:08:44Z" id="28378870">a) I'm not a fan of these underscored values,
b) I like `coerce` :)
</comment><comment author="clintongormley" created="2013-11-13T10:01:05Z" id="28382411">I like `coerce` too. 

@karmi you want to suggest alternatives for the underscored values?
</comment><comment author="merrellb" created="2013-12-03T08:59:03Z" id="29692921">It seems like we are trying to combine two related but different ideas.  What efforts do we attempt when a value is not as expected, and what do we do if these efforts fail.   It sounds like there aren't too many options in terms of coercing.  Do we convert strings to numerics and/or do we convert floats to integers (is rounding the only option)?  Regarding failures do we only fail or ignore?  Do we ever want to report the errors (ie not silently "ignore")?

It seems difficult to combine these two aspects in one attribute in a way that enumerates the options (and avoids underscores).  Here is one proposal that uses two attributes to provide additional (perhaps more than we need) flexibility.

`coerce`
- `loose`  (convert strings to numerics, convert floats to ints) default
- `string` (convert strings to numerics, fail if float value in an int field)
- `float` (convert floats to ints, fail if string)
- `exact` (must be the exact expected type) (perhaps `strict` or `none`?)

`malformed`
- `ignore` (accept the document, exclude the bad fields silently) (perhaps `silent`?)
- `report` (accept the document, exclude the bad fields and complain) (perhaps `loud`?)
- `fail`  (refuse the document, complain) default

I don't mean to muddy the waters but thought I would at least throw out some additional options.

-brian
</comment><comment author="imotov" created="2014-01-07T16:56:01Z" id="31755539">To summarize the discussion and make some progress on the issue, here is what I am proposing to implement:
- `strict` - doesn't try to perform any conversion, throws an error unless the value is a valid number which fits the field type
- `cast` - casts when it can but throws an error when it can't (**default**)
- `ignore` - doesn't try to cast and silently ignores errors
- `coerse` - casts when it can and silently ignores errors

In other words, `strict` and `ignore` don't try to convert, `cast` and `coerce` do. `strict` and `cast` complain, `ignore` and `coerce` don't.

Migration path: in 1.0, we will support both `ignore_malformed` and `malformed` as well as
- `true` which will be equivalent to `coerce`
- `false` which will be equivalent to `cast`

In 1.1 we will remove support for `ignore_malformed`, `true` and `false`.
</comment><comment author="markharwood" created="2014-01-08T11:00:47Z" id="31822365">Summarising the options then:

&lt;table&gt;
  &lt;tr&gt;
       &lt;th&gt;&lt;./th&gt;
       &lt;th&gt;Attempts conversion&lt;./th&gt;
       &lt;th&gt;Does not attempt conversion&lt;./th&gt;
 &lt;/tr&gt;
  &lt;tr&gt;
       &lt;th&gt;Throws errors&lt;./th&gt;
       &lt;td&gt;Cast&lt;./td&gt;
       &lt;td&gt;Strict&lt;./td&gt;
 &lt;/tr&gt;
  &lt;tr&gt;
       &lt;th&gt;Doesn't throw error&lt;./th&gt;
       &lt;td&gt;Coerce&lt;./td&gt;
       &lt;td&gt;Ignore&lt;./td&gt;
 &lt;/tr&gt;
&lt;/table&gt;

I feel slightly uneasy about "ignore_malformed" being truncated to just "malformed". It changes the meaning from clearly describing a policy to a word usually used to describe a condition. How about "conversion_policy"? 

In fact it may be simpler to understand all round and avoid any deprecation notices if we have 2 booleans representing the fundamental choices in the table - "ignore_malformed"  and a new "attempt_conversions" which defaults to true. (This is based on the same point merrellb had but with simplified options)
</comment><comment author="kimchy" created="2014-01-08T12:18:40Z" id="31826627">++, makes sense to me. Just a note, for backward support, we should support the existing flags, but serialize in the `toXContent` to the new options (I think thats what suggested, just verifying), so existing mappings will effectively be "upgraded" to the new format.
</comment><comment author="clintongormley" created="2014-01-08T13:27:57Z" id="31830491">Sounds good to me
</comment><comment author="markharwood" created="2014-01-08T15:53:13Z" id="31846501">Update after a chat with Kevin and Clinton:

We chose not to use the one flag with 4 options (strict/cast/ignore/coerce).

Instead, we add a new boolean "coerce". The settings that govern format transformations are therefore:

1) The existing "ignore_malformed" option will behave as usual where "true" will index a doc minus the malformed value and false will throw an error rejecting the document.
2) The new "coerce" flag controls if any attempts are made to convert formats. A value is only considered malformed if it is not possible to force a transformation via the standard Java functions. Any information loss such as rounding or truncation (e.g. when calling Integer.toShort() ) is not considered to be a malformed value and is considered an acceptable transformation when coercing data into the target format. 

If coercion is set to false then we need to check no information loss is occurring as part of the transformation from JSON to target format. Unfortunately the Jackson Json parser we use will silently drop the fraction part of a number when parsing as an int, short or long and so it is necessary to compare this output with the output when parsed as a double to ensure there is no data loss.
</comment><comment author="markharwood" created="2014-01-08T19:21:58Z" id="31868141">For back compatibility, the default setting for coerce should be true and if it is set to false we then don't allow any of the following JSON examples and throw them out as malformed:

```
"numberAsString":"1.2" // when mapped to any number
"numWithFraction":1.3 // fractions when mapped to short,int,or long
```

Regardless of the "coerce" flag setting, overly-large values such as these will always be considered malformed:

```
"badShort":1234567789  //Value out of range for short
```
</comment><comment author="markharwood" created="2014-01-10T12:01:51Z" id="32022110">In the Int/Short/Long...FieldMapper classes there is special code that allows for content to be passed from sources other than JSON as a Java object called "externalValue". Few classes appear to use this currently and an example is GeoPointFieldMapper which derives lat/lon fields etc from things like GeoHashes provided in JSON.

The Int/Short/Long...FieldMapper classes have logic that attempts to cast these externalValue objects to strings and then parse them into numerics.

@kimchy agreed that these special conversions should not be governed by the "coerce" policy that is applied to the more common case of handling plain values passed as part of the raw JSON content
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Explicit.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentParser.java</file><file>src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java</file></files><comments><comment>Adds a new coerce flag for numeric field mappings which is defaulted to true.</comment><comment>When set to false a new strict mode of parsing is employed which</comment><comment>a) does not permit numbers to be passed as JSON strings in quotes</comment><comment>b) rejects numbers with fractions that are passed to integer, short or long fields.</comment></comments></commit></commits></item><item><title>Postings highlighter doesn't return snippets when using "path":"just_name"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4116</link><project id="" key="" /><description>The postings highlighter doesn't return snippets when the field name in the index is different from the one specified in the search request. That happens either using `"path":"just_name"` or using a custom `index_name` in the mapping.
</description><key id="22275408">4116</key><summary>Postings highlighter doesn't return snippets when using "path":"just_name"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-07T15:59:34Z</created><updated>2013-11-07T20:05:04Z</updated><resolved>2013-11-07T16:21:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-11-07T17:29:30Z" id="27986740">Hi @javanna just checked it out and made a build. now more records are highlighted but not all
also when I search on some words  bunch of fields get highlighted  from start which has nothing to do with the word I am searching on (but the word in the legitimate field does occur at the start of the field) and the same with another word in the middle it highlight bunch of the fields in the same position.

the problem I think is because  there are very many fields named "name"  in various mappings (for example award.type.name) and they all are lumped together somehow! 

```
"award": {
....
"type": {
              "dynamic": "true",
              "properties": {
                "id": {
                  "type": "integer",
                  "include_in_all": false
                },
                "name": {
                  "type": "multi_field",
                  "path": "just_name",
                  "fields": {
                    "name": {
                      "type": "string",
                      "index_options": "offsets",
                      "boost": 0.3
                    },
                    "all": {
                      "type": "string",
                      "index_options": "offsets",
                      "boost": 0.01
                    },
                    "all_stem": {
                      "type": "string",
                      "index_options": "offsets",
                      "boost": 0.01
                    }
                  }
                }
              }
            }
```
</comment><comment author="javanna" created="2013-11-07T19:37:09Z" id="27998702">I see what you mean, I think it happens using `"path":"just_name"` for different fields with same name but different path. That leads to having different json fields merged into the same lucene field, but the content that is loaded from the `_source` is only part of it, thus the offsets don't match the field content.

I think the same would happen with the fast vector highlighter too, while with the plain highlighter it doesn't happen because the text is being re-analyzed on-the-fly.

Can you open another issue with a small recreation for it?
</comment><comment author="roytmana" created="2013-11-07T19:46:12Z" id="27999514">Yes I can but i thought that primary field in multifield mapping gets resolved to full name regardless of  the "just_name" option. If so why would there be any name clash? 

BTW I posted another issue #4108 which seems to be looking into. With all the traffic it is easy to miss but is rather critical for us, could you possible tell me if it is a fixable bug or a feasible enhancement or it can't be achieved?
</comment><comment author="javanna" created="2013-11-07T20:05:04Z" id="28001152">@roytmana the fact that a field gets resolved doesn't say much about how it is actually indexed. When you use "just_name" for different fields with different path but same name, they get merged into the same lucene field. E.g. `obj1.name` and `obj2.name` both get indexed into `name`. If you can open the issue and send a full recreation we'll take it from there.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Used the actual index_name when making highlighting using the postings highlighter</comment></comments></commit></commits></item><item><title>Refactored indices aliases api to make use of the new ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4115</link><project id="" key="" /><description>Refactored indices aliases api to make use of the new recently introduced generic ack mechanism

Closes #4114
</description><key id="22259337">4115</key><summary>Refactored indices aliases api to make use of the new ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-07T11:42:29Z</created><updated>2014-06-19T05:05:14Z</updated><resolved>2013-11-14T22:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-14T11:59:17Z" id="28478455">Looks good. Note that by removing the node aliases action, it means that if we back port it to 0.90, then failures will happen between different version nodes (on aliases API) until the cluster is on the same version.
</comment><comment author="javanna" created="2013-11-14T22:09:34Z" id="28527954">I had a deeper look at this and figured out that it shouldn't be a problem. 

If an updated master receives a `cluster/nodeAliasesUpdated` request (the old format of ack for aliases) there's no endpoint registered for it, then it will return an exception (`ActionNotFoundTransportException`). But the sender node won't do anything with it as it uses an `EmptyTransportResponseHandler`. As a result the only downside, which doesn't hurt, is a warning in the master log: 

```
Message not fully read (request) for [*] and action [cluster/nodeAliasesUpdated], resetting
```

The other way around, if an updated node doesn't send back the old format of ack request, an older version master will just timeout and return `acknowledged:false`.

I think it's fine in case of rolling upgrade, will merge this and backport it to 0.90 too.
</comment><comment author="javanna" created="2013-11-14T22:14:30Z" id="28528333">Merged via https://github.com/elasticsearch/elasticsearch/commit/b2ad34cf9993c554eec0c8a926c2319f547648a8 and backported to 0.90.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move indices aliases api to new acknowledgement mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4114</link><project id="" key="" /><description>Move indices aliases api to new acknowledgement mechanism introduced in #3786 .
</description><key id="22259164">4114</key><summary>Move indices aliases api to new acknowledgement mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-11-07T11:38:53Z</created><updated>2013-11-14T22:19:20Z</updated><resolved>2013-11-14T22:11:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeAliasesUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Refactored indices aliases api to make use of the new recently introduced generic ack mechanism</comment></comments></commit></commits></item><item><title>External method to set rootTypeParsers in DocumentMapperParser incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4113</link><project id="" key="" /><description>looks like there is small bug in putRootTypeParser method in DocumentMapperParser class:

method:

``` java
    public void putRootTypeParser(String type, Mapper.TypeParser typeParser) {
        synchronized (typeParsersMutex) {
            rootTypeParsers = new MapBuilder&lt;String, Mapper.TypeParser&gt;()
                    .putAll(typeParsers)
                    .put(type, typeParser)
                    .immutableMap();
        }
    }
```

should look more like this:

``` java
    public void putRootTypeParser(String type, Mapper.TypeParser typeParser) {
        synchronized (typeParsersMutex) {
            rootTypeParsers = new MapBuilder&lt;String, Mapper.TypeParser&gt;(rootTypeParsers)
                    .put(type, typeParser)
                    .immutableMap();
        }
    }
```

current version when called will override root type parsers with type parsers
</description><key id="22218298">4113</key><summary>External method to set rootTypeParsers in DocumentMapperParser incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-06T19:23:20Z</created><updated>2013-11-07T00:41:15Z</updated><resolved>2013-11-07T00:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-06T23:57:56Z" id="27926403">indeed, its a bug. this method is mainly aimed at external plugins using it, is that what you are trying to use it for?
</comment><comment author="karol-gwaj" created="2013-11-07T00:41:15Z" id="27928668">kinda, im planning to use putTypeParser method in my plugin
simply i accidentally eyeballed this method too (as they are close to each other in the class)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file></files><comments><comment>External method to set rootTypeParsers in DocumentMapperParser incorrect</comment><comment>fixes #4113</comment></comments></commit></commits></item><item><title>Unable to create a nested filtered alias on a dataless master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4112</link><project id="" key="" /><description>We set-up a two node cluster the first node is a master and contains no data.  The second node has data but is not master eligible.  Both a running 0.90.0. Also, retested on 0.90.6.

First I create an index with an nested document:

curl -X POST http://masternode:9200/index_a -d '{
    "mappings" : {
       "type_a" : {
          "properties" : {
            "table_a" : { "type" : "nested", "properties" : {
              "field_a" : { "type" : "string" },"field_b" : { "type" : "string" }
            }}
          }
       }
    }
}
'

Returns: {"ok":true,"acknowledged":true}

Then I try to create the filter alias:

curl -X POST http://datanode:9200/_aliases -d '
{
    "actions" : [
        { "add" : { "index" : "index_a", "alias" : "alias_a" , "filter": {
        "bool": {
          "must": [
            {
              "nested": {
                "path": "table_a",
                "query": {
                  "bool": {
                    "must": [
                      {
                        "term": {
                          "table_a.field_a": "y"
                        }
                      }
                    ]
                  }
                }
              }
            }
          ]
        }
      }} }
    ]
}'

Returns: {"error":"RemoteTransportException[[dlesmaster01][inet[/10.13.10.38:9300]][indices/aliases]]; nested: ElasticSearchIllegalArgumentException[failed to parse filter for [alias_a]]; nested: QueryParsingException[[index_a] [nested] failed to find nested object under path [table_a]]; ","status":400}

If I submit it to the master node like this:

curl -X POST http://masternode:9200/_aliases -d '
{
    "actions" : [
        { "add" : { "index" : "index_a", "alias" : "alias_a" , "filter": {
        "bool": {
          "must": [
            {
              "nested": {
                "path": "table_a",
                "query": {
                  "bool": {
                    "must": [
                      {
                        "term": {
                          "table_a.field_a": "y"
                        }
                      }
                    ]
                  }
                }
              }
            }
          ]
        }
      }} }
    ]
}'

Returns: {"error":"ElasticSearchIllegalArgumentException[failed to parse filter for [alias_a]]; nested: QueryParsingException[[index_a] [nested] failed to find nested object under path [table_a]]; ","status":400}

These all work when the elected master also contains the data.  
</description><key id="22209750">4112</key><summary>Unable to create a nested filtered alias on a dataless master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">irieksts</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-06T17:11:54Z</created><updated>2013-11-08T10:23:11Z</updated><resolved>2013-11-08T10:23:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>src/test/java/org/elasticsearch/cluster/SpecificMasterNodesTests.java</file></files><comments><comment>Unable to create a nested filtered alias on a dataless master</comment><comment>fixes #4112</comment></comments></commit></commits></item><item><title>Indices query/filter skip parsing altogether for irrelevant indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4111</link><project id="" key="" /><description>Fixes #2416
</description><key id="22198996">4111</key><summary>Indices query/filter skip parsing altogether for irrelevant indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ofavre</reporter><labels /><created>2013-11-06T14:54:45Z</created><updated>2014-06-27T03:56:24Z</updated><resolved>2013-11-14T17:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2013-11-06T15:41:42Z" id="27883753">With this patch, the queries won't be parsed iff the `indices` are provided before the `query` or `no_match_query`.
They will still be parsed if the `indices` are provided after.
The same is true for the filter variant.
</comment><comment author="ofavre" created="2013-11-07T15:30:48Z" id="27975100">Packaged as [a plugin](https://github.com/yakaz/elasticsearch-query-indices2416) for those you can't update.
The plugin builds against ElasticSearch 0.90.4+, but can easily be backported down to 0.19.0!
</comment><comment author="javanna" created="2013-11-13T11:00:57Z" id="28385907">Thanks a lot for the pull request @ofavre ! I left some comments, let me know if they make sense!
Also, could you update the docs page for both indices filter and query adding that the position where you put the indices makes a difference?
</comment><comment author="ofavre" created="2013-11-13T11:09:55Z" id="28386411">Thanks for all that detailed feedback. I'll take a detailed look at each and update the pull request. If I have any questions, I'll post under the appropriate comment.
</comment><comment author="ofavre" created="2013-11-13T17:25:23Z" id="28414705">I've updated the pull request with all your suggested changes and requests.
Let me know.
</comment><comment author="javanna" created="2013-11-13T17:58:08Z" id="28417624">Looks great, thanks! I'll have another look and push it soon!
</comment><comment author="javanna" created="2013-11-14T17:35:53Z" id="28504839">I just pushed this to master and backported to 0.90. 

I also added a commit with a few minor changes to code, docs and tests, I thought you might want to have a look at those: https://github.com/elasticsearch/elasticsearch/commit/0aaa39d00ac838cb436687e06333262e3495859c .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postings highlighter does not highlight majority of the results while plain highlighter works as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4110</link><project id="" key="" /><description>please refer to #4103 for some background

majority of my results now do not get highlighted with postings even with a simple match query.
Everything gets highlighted with plain highlighter

I was not able to recreate it but here are some materials:

recreation that does not show the issue with few mappings similar to my real ones https://gist.github.com/roytmana/7336502

my real mapping https://gist.github.com/roytmana/7330531

sample query https://gist.github.com/roytmana/7337278
but it is the same with query_string and match/OR
</description><key id="22198915">4110</key><summary>Postings highlighter does not highlight majority of the results while plain highlighter works as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">roytmana</reporter><labels /><created>2013-11-06T14:53:36Z</created><updated>2013-11-07T16:00:30Z</updated><resolved>2013-11-07T16:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-06T16:12:09Z" id="27886783">Hey @roytmana thanks again for reporting this. I'm afraid I need a recreation though in order to understand what happens in your case. I really cannot reproduce what you are describing.
</comment><comment author="roytmana" created="2013-11-06T16:18:28Z" id="27887397">can you provide any suggestions on what may be affecting highlighting to make it easier to recreate?
size of data, number of highlighted fields, query, analysis chain....
it is hard to reproduce trying blindly as I am not familiar with the algorithm and lucene internals

did anything in my mapping looked suspicious to you?
</comment><comment author="javanna" created="2013-11-06T16:33:38Z" id="27888873">I don't really know, the issue you are describing seems related to #4103 , which is now fixed. By the way the fix is included in 1.0.0.beta1 which is worth to check out.

If you still see this behaviour I wonder why you cannot reproduce it, that seems totally weird to me. I just tested some more comparing snippets obtained from plain and postings highlighter, and I didn't see anything strange.
</comment><comment author="roytmana" created="2013-11-06T16:41:16Z" id="27889692">ok, thanks i will keep digging. I will try to load my real index with just
a few records of fake data see if it'll do it
I may need to lay it off for few days - a real crunch at work is coming

I am building from 0.9 branch as I need to start finalizing the app in few
months. not sure if I can afford using 1.0 beta
is it worth while to test highlighting with 1.0beta or it should be
identical in both?
</comment><comment author="javanna" created="2013-11-06T17:00:28Z" id="27891675">0.90 and 1.0 should be identical for what concerns highlighting, the difference is that with 1.0.0.beta1 you'd work with a released version rather than a snapshot, as the fix for #4103 is not included in 0.90.6.
</comment><comment author="roytmana" created="2013-11-06T22:36:00Z" id="27920921">Luca here is gist with working recreation
i only loaded two documents and stripped 99% of the fields but it shows highlighting issue

https://gist.github.com/roytmana/5c12896420629fd3b090
</comment><comment author="javanna" created="2013-11-07T14:23:30Z" id="27968200">Hi @roytmana thanks for your gist. I'm having problems recreating the index locally and your mapping is quite big, thus it's hard to understand what is going wrong.

Anyways, you defined the field `award.type.name` in your mapping, but it has no value in your documents, as the field used in your documents is called `award.name`. Same for `activity` which in your mapping is child of `award` while in your documents is a top level field.

It seems to me that you are making highlighting using the postings highlighter against fields that are properly configured in the mapping but never have a value in your documents, which is why you don't get any error back forcing the highlighter type "postings", but at the same time you don't get any snippets back.

Does this make sense to you? 
If you still have highlighting issues can you try and trim down your mapping to make sure everything is fine there?
</comment><comment author="roytmana" created="2013-11-07T14:28:07Z" id="27968752">I hand-edited the data so I may have screwed it up. let me doublecheck
</comment><comment author="roytmana" created="2013-11-07T14:53:36Z" id="27971604">but why would the plain highlighter work if the field has no data?

i did screwed up activity while editing but not type. Here is formatted payload from the bulk:

```
{
   "award": {
      "id": 100,
      "type": {
         "id": 1000,
         "name": "BPA Call"
      }
   },
   "activity": {
      "code": "N02",
      "name": "Non Research and Development"
   }
}

{
   "award": {
      "id": 101,
      "type": {
         "id": 1000,
         "name": "BPA Call"
      }
   },
   "attr": {
      "fpds8a": {
         "id": "6640",
         "name": "LABORATORY EQUIPMENT AND SUPPLIES"
      },
      "fpds6m": "105027         SRV-PCR"
   }
}
```

I will fix Activity in the bulk to be under award but it did not matter as the query (and highlight) should have been against  award.

What problems did you have with creating the index? The mapping is big but the small recreation i posted before did not show the issue.

I can't exactly test curl itself because on windows it does not work with command line payloads but I copied mapping directly from the file which I execute with curl. Would you rather get mapping and data as files so I can test them before sending them to you? Can I attache them somewhere? 
</comment><comment author="javanna" created="2013-11-07T16:00:30Z" id="27978248">Right, I was finally able to trim down your mapping and get to a small recreation, I just opened another issue (#4116) that is more specific to the problem as this one seems confusing to me. The problem is related to using `just_name` in the mapping. A fix is on its way too. 

Thanks a lot for your help @roytmana !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>missing-PT_GNU_STACK-section libsigar-x86-linux.so</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4109</link><project id="" key="" /><description>The libsigar.so shipped with elasticsearch does not pass rpmlint:

Here's an explanation of the error.
http://wiki.rosalab.ru/en/index.php/Rpmlint_Errors#missing-PT_GNU_STACK-section
</description><key id="22193851">4109</key><summary>missing-PT_GNU_STACK-section libsigar-x86-linux.so</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gpaul</reporter><labels /><created>2013-11-06T13:26:12Z</created><updated>2014-08-08T18:24:27Z</updated><resolved>2014-08-08T18:24:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-06T14:43:29Z" id="27878426">you can open an issue on sigar if you want..., we can't fix it here I assume (unless we can tell rpm to ignore it)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index time boost in multi_field ignored?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4108</link><project id="" key="" /><description>I use multi_field to index content of multiple properties into a single _all-like multi_field. Each contributing property may define different boost when defining it's instance of the multi_field. these boosts are however ignored when searching against such multi_fied while honored while searching against _all

full recreation is here https://gist.github.com/roytmana/7330956

it compares the same query against my multi_field all and ES _all
</description><key id="22192360">4108</key><summary>Index time boost in multi_field ignored?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">roytmana</reporter><labels /><created>2013-11-06T12:55:43Z</created><updated>2014-01-09T15:23:49Z</updated><resolved>2014-01-09T15:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-11-07T17:34:00Z" id="27987516">Any comment? It's a real showstopper for us! 

I did a big (considering size of our mappings) rework getting away from using __all because I needed a combined field like _all but analyzed differently (stemmed, shingled, regular...) just to find that multifield is unusable for us because it does not take boost per contributing field into account like _all does

Please let me know if it is a bug and can be fixed or it is not possible i must go back to using _all :-( 
</comment><comment author="javanna" created="2013-11-07T23:06:30Z" id="28016111">This is again related to the use of `"path":"just_name"`. The `boost` is usually taken into account when using an ordinary `multi_field`, but if you use "just_name" for fields with same name, you merge their content into the same lucene field. That said, applying different boosts to the same lucene field doesn't make much sense to me, and that is why your `boost` gets ignored. If you want to give different weights to those fields, you need to keep them distinct.

I would keep a variation with unique name so that the boost will be taken into account, as they will actually be different lucene fields. Otherwise you could just drop index time boosting and use a multi_match query against multiple fields, giving a different weight to each field.
</comment><comment author="roytmana" created="2013-11-07T23:17:35Z" id="28016827">Thank you @javanna but in this case my intention is to combine data from multiple properties into a single field to act like _all. 

_all does support bust based on which field contributes to it:

"One of the nice features of the _all field is that it takes into account specific fields boost levels. Meaning that if a title field is boosted more than content, the title (part) in the _all field will mean more than the content (part) in the _all field."

The boost can be applied to individual tokens right? what I expected is that each individual property contributing to the shared (collapsed) multi_field would mark its content with defined boost
</comment><comment author="clintongormley" created="2013-11-07T23:24:55Z" id="28017336">I agree with @roytmana - the field boosts are retained when indexing into `_all`, and I assumed that the same thing would apply when indexing from multiple fields into a single index_name.

In fact, I'd say that this is the one place where field-level index time boosting has a purpose.
</comment><comment author="javanna" created="2013-11-07T23:28:43Z" id="28017591">Yeah I see your point guys, I agree, looking into it :)
</comment><comment author="roytmana" created="2013-11-07T23:55:47Z" id="28019247">more so, I expected analyzers and position offset gaps to be honored per contributing field so we have a fine-grained control over how such combined field get put together

for example I use phrase searches and I want to make sure searches across content from different contributing fields are not matched - I would use position_offset_gap for such fields

or I want some fields to contribute stemmed content and few other (say people names or some codes) not stemmed etc.

This is what makes it so powerful

And thanks for looking into it i was getting kind of desperate of this issue being "ignored" I banked lots of my design on multifield power

now when's it going to be fixed ? :-) ha ha
</comment><comment author="clintongormley" created="2013-11-07T23:57:19Z" id="28019324">&gt;  I expected analyzers and position offset gaps to be honored per contributing field so we have a fine-grained control over how such combined field get put together

are they not? i was pretty sure they were.

If not, could you provide a recreation?
</comment><comment author="roytmana" created="2013-11-08T00:06:02Z" id="28019807">maybe they are working I guess I was dramatizing it a bit :-) after struggle with multifield and related highlighting issues (i feel current multifield primary field to index_name naming  when using 'just_name' is very unintuitive right now see #4123 as it lumps all primary fields in one lucene field which is in 99% of cases is not what I would expect)

I will test it later tonight or tomorrow and report
</comment><comment author="javanna" created="2013-11-08T00:50:06Z" id="28022049">I checked how the `_all` field works and it has a special treatment, which is why we specifically mention that it keeps the boost from the original fields. 

The reason why I said in the first place that it doesn't make sense to have more than one boost for the same field is that index time boosting is per field, using field norms, thus only one value per field. To work around this the `_all` field contains a payload per term with the original boost, that is used at query time for scoring. This is something that we do only for the `_all` field and I'm not even sure if we should do it whenever using the same `index_name` for different fields.

But we should definitely take this into account in the discussion on https://github.com/elasticsearch/elasticsearch/issues/4099 regarding future improvements.
</comment><comment author="roytmana" created="2013-11-08T01:55:43Z" id="28025717">oh no! back to using _all field then :-( 
so much time wasted and no improvements I hoped for.
I should have tested multifield boosts before basing my design on them

oh well hope you will be able to pull a miracle out of the hat :-)
</comment><comment author="clintongormley" created="2013-12-03T14:54:51Z" id="29715711">@roytmana you can achieve what you are after (ie per-field index time boosts) by querying both your custom `_all` field and the individual field, eg lets say you wanted to search `first_name` and `last_name` using the single `full_name` field, your mapping would look like this:

```
curl -XPUT "http://localhost:9200/myindex" -d'
{
    "mappings": {
        "person": {
            "properties": {
                "first_name": {
                    "type": "multi_field",
                    "path": "just_name",
                    "fields": {
                        "first_name": { "type": "string"},
                        "full_name": { "type": "string"}
                    }
                },
                "last_name": {
                    "type": "multi_field",
                    "path": "just_name",
                    "fields": {
                        "last_name": { "type": "string"},
                        "full_name": { "type": "string"}
                    }
                }
            }
        }
    }
}'
```

Then if you wanted to give a slight boost to the `last_name` field, you could do:

```
curl -XPOST "http://localhost:9200/myindex/person/_search" -d'
{
   "query": {
      "bool": {
         "must": {
            "match": {
               "full_name": "john smith"
            }
         },
         "should": {
            "match": {
               "last_name": {
                  "query": "john smith",
                  "boost": 0.5
               }
            }
         }
      }
   }
}'
```

You could even use the `rescore` functionality to achieve something similar:

```
curl -XPOST "http://localhost:9200/myindex/person/_search" -d'
{
   "query": {
      "match": {
         "full_name": "john smith"
      }
   },
   "rescore": {
      "window_size": 50,
      "query": {
         "rescore_query_weight": 0.5,
         "rescore_query": {
            "match": {
               "last_name": {
                  "query": "john smith"
               }
            }
         }
      }
   }
}'
```

And this would probably be more efficient (and certainly more flexible) than using payloads to implement field-level index time boosts on custom `_all` fields.
</comment><comment author="roytmana" created="2013-12-03T15:14:36Z" id="29717536">Thank you @clintongormley I did not think of re-score but I did use the first approach. My issue is that I have a highly structured data - over 100 fields and it is just the beginning. Half of them are not very useful but i can't afford not to include them in my search but meed to massively de-emphasize them or they will drown the useful results. Another half provide good search corpus with some being more important than others. Out of this half there is a handful of highly relevant fields that gets special boost. 
I find that managing it all in queries will bloat queries to no end, require to make sure it is consistent across all the queries, require third parties who may develop against the index to have same knowledge, require the same analysis on the individual fields as  on the _all (ex. stemming)

I do not know I guess I could go this route with should clause listing all "important" fields (over 70 now) with various boosts and make sure they analyzed the same as _all (or my _all-like multifield) but I do not know if it will scale in terms of complexity as number of my fields triples in next version. Not sure if it can cause performance issues
</comment><comment author="javanna" created="2014-01-09T15:23:49Z" id="31941748">Closing this issue in favour of #4520, which will take care of custom `_all` fields.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed fetch subphase to not recreate the HitContext for each hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4107</link><project id="" key="" /><description>Fixed fetch subphase to not recreate the HitContext for each hit, so that the object cache stays the same

Fixed also bug in the fast vector highlighter which was raised by enabling the object cache, due to null FieldQuery (NPE) in case the objects are taken from the cache

Added tests to check if there are issues when highlighting multiple fields at the same time

Closes #4106
</description><key id="22191498">4107</key><summary>Fixed fetch subphase to not recreate the HitContext for each hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-06T12:37:27Z</created><updated>2014-07-11T09:32:03Z</updated><resolved>2013-11-07T14:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-06T14:05:49Z" id="27875370">LGTM
</comment><comment author="javanna" created="2013-11-07T14:48:40Z" id="27971139">Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlight hit object cache gets reset per hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4106</link><project id="" key="" /><description>`FetchSubPhase.HitContext` exposes a cache that allows to reuse objects which can be shared between different hits, when executing subfetch phases. 

This is meant to be used especially in combination with highlighting, as some of the objects are heavy to create and don't need to be recreated per hit.

This mechanism is not working properly, as `HitContext` gets recreated per hit, thus the cache is always empty and the objects are never reused.
</description><key id="22190697">4106</key><summary>Highlight hit object cache gets reset per hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-06T12:20:19Z</created><updated>2013-11-07T14:48:59Z</updated><resolved>2013-11-07T14:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fixed fetch subphase to not recreate the HitContext for each hit, so that the object cache stays the same</comment></comments></commit></commits></item><item><title>Custom rewrite methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4105</link><project id="" key="" /><description>It would be nice if ES accepts custom rewrite methods. Currently only a limited number of rewrite methods are supported.

My suggestion is to register a rewrite method with a name, so that you can use that method later on in, for instance, a match-query.

Background: I would like to have a dis_max rewrite method with a tie-breaker
</description><key id="22186450">4105</key><summary>Custom rewrite methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">pweerd</reporter><labels /><created>2013-11-06T10:48:37Z</created><updated>2014-08-08T18:20:07Z</updated><resolved>2014-08-08T18:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-06T13:09:48Z" id="27871727">Would it make sense to add this rewrite method rather than opening up the plugin mechanism for this. I think it's generally useful. ie others can benefit from it as well?
</comment><comment author="pweerd" created="2013-11-06T13:55:27Z" id="27874670">Hi Simon,Adding makes sense.Currently I'm playing with a way how to handle compound terms.I don't exactly know what I want. The direction is clear, but the exact implementation isn't.Playing with queryBoosts, doing and/or, etc.So for me it makes sense to have a way to plugin my own rewriter...Btw: for me to play, I created a custom query that behaves like a match query, but supports extra rewrites.Creating such a custom query is a pain, because I cannot leverage the ES base implementation.It would be nice if the parser called a protected method like:     protected boolean handleUnkownValue ( XContentParser parser, MatchQuery query) {         return false;     }If this method returns false, the normal QueryParsingException will be raised, otherwise, !
 parsing will go on.In this way, I can simply inherit the standard ES query parsers, do some extra stuff, and there I go...What is your opinion about this?Kind regards,Peter----- Original Message -----From: "Simon Willnauer" notifications@github.comTo: "elasticsearch/elasticsearch" elasticsearch@noreply.github.comSent: Wednesday, November 06 2013 02:10:45 PMSubject: Re: [elasticsearch] Custom rewrite methods (#4105)Would it make sense to add this rewrite method rather than opening up the plugin mechanism for this. I think it's generally useful. ie others can benefit from it as well?

???Reply to this email directly or view it on GitHub.
</comment><comment author="bleskes" created="2014-01-16T19:47:21Z" id="32522219">Hey Peter,

I started to look into this and I want to make sure we implement what you need. You refer to rewrite methods but those only get applied if you set fuzziness. On the other hand you mention dealing with compound words, which are typically implemented using a token filter and adding tokens with 0 position increment. 

I want to make sure we implement something that would help in your use case - can you please clarify?

Cheers,
Boaz
</comment><comment author="pweerd" created="2014-01-20T08:10:46Z" id="32740665">Ha Boaz,

Thanks for picking this up.

Indeed, I noticed that rewriting is not done on normal behavior.

My usecase is that I want do do special things with for instance bed&amp;breakfast or B&amp;B.

I created my own query (derived from multi_match) for now and 

1)      split the input text by spaces to separate individual terms

2)      for each term, split the term by using the defined filter. Say into &lt;A&gt; and &lt;B&gt;.

3)      Now, search for:

a.       each term (1.0)

b.      AB (0.95)

c.       A AND B (0.8)  (weights adjustable)

By the way, I also changed the way multi_match behaves:

Multimatch favores the individual terms to be in the same field, but in most situations it doesn’t matter in what field the hit is.

So I search for each terms in all fields (combining them with a dismax) and combine the term-results with an OR or an AND.

The scores are highly different, and it behaves more like _my_ intuition.

Is this clear to you? And does it make sense?

Thanks,

Peter 

Van: Boaz Leskes [mailto:notifications@github.com] 
Verzonden: donderdag 16 januari 2014 20:48
Aan: elasticsearch/elasticsearch
CC: pweerd
Onderwerp: Re: [elasticsearch] Custom rewrite methods (#4105)

Hey Peter,

I started to look into this and I want to make sure we implement what you need. You refer to rewrite methods but those only get applied if you set fuzziness. On the other hand you mention dealing with compound words, which are typically implemented using a token filter and adding tokens with 0 position increment. 

I want to make sure we implement something that would help in your use case - can you please clarify?

Cheers,
Boaz

—
Reply to this email directly or view it on GitHub https://github.com/elasticsearch/elasticsearch/issues/4105#issuecomment-32522219 .Afbeelding verwijderd door afzender.
</comment><comment author="bleskes" created="2014-01-30T16:13:22Z" id="33702839">Hi Peter,

I'm not sure I follow, but this is becoming a discussion about how to best approach a problem. Can you move it to the google group (I will respond there)? if we need concrete features developed we can open tickets from them here.

Thanks,
Boaz
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Apply fix for LUCENE-5330 pruning the IndexWriter queue to get rid of pending events.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4104</link><project id="" key="" /><description>This is a workaround for LUCENE-5330 to prune the event queue on getReader()

Closes #4093
</description><key id="22183392">4104</key><summary>Apply fix for LUCENE-5330 pruning the IndexWriter queue to get rid of pending events.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-06T09:51:21Z</created><updated>2014-07-10T03:36:15Z</updated><resolved>2013-11-06T13:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-06T10:03:43Z" id="27858664">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postings highlighter wrong highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4103</link><project id="" key="" /><description>As reported as a comment in #4042, the postings highlighter has a weird behaviour, it seems like it remembers the offsets of the previously highlighted documents. Well, what happens is that it actually highlights the right text against the wrong offsets, because of most likely the silliest mistake one can make working with lucene, that is using doc ids that are relative to the segment they belong as they were unique instead of the doc ids that contain the segment offset too.
Surprising that this wasn't raised by our tests though, as we have a pretty good coverage for the postings highlighter. Will improve that too.
</description><key id="22161981">4103</key><summary>Postings highlighter wrong highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-11-06T00:03:32Z</created><updated>2013-11-06T14:53:36Z</updated><resolved>2013-11-06T00:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-11-06T14:06:03Z" id="27875390">Hi Luca,

The prior test works with new commit however my original issue of not highlighting many fields at all remains.
I test it by switching from plain to postings highlighter and the difference is dramatic with my real mapping and data - very few fields are now highlighted with postings.

I am trying to reproduce it with a recreation but so far I could not.

to remind you of the progression of fixes to the highlighter
1. first version worked fine with simple tokens but not wildcards
2. with wildcard enhancement i had some results randomly not highlighted and the small recreation showed scrambled highlighting 
3. with the latest fix lot more results (majority) does not get highlighted but i can't recreate it on small mapping/data set

Do you have any suggestions for recreation? Does highlighting with postings depends on search query at all?

here is a recreation with few mappings similar to my real ones https://gist.github.com/roytmana/7336502
but it works

any suggestions towards recreation of the issue would be very helpful

Alex
</comment><comment author="clintongormley" created="2013-11-06T14:17:34Z" id="27876308">@roytmana i had an example of one not highlighting at all from your example yesterday.  when searching for `"photo*"` or `"photo"` or `"photography"`,  "&lt;photo&gt; equipment" was highlighted, but "photography" or "photography equipment" wasn't highlighting.

also got different responses if i used query_string vs match vs match_phrase_prefix

sorry can't provide more details atm
</comment><comment author="roytmana" created="2013-11-06T14:32:15Z" id="27877477">this test works now with the latest 0.90 snapshot but I still am having issue of not highlighting at all which I could not reproduce with a recreate so far :-(
</comment><comment author="javanna" created="2013-11-06T14:33:51Z" id="27877633">Hi @roytmana, thanks a lot for your feedback! 

What you called "scrambled highlighting" has been solved :)

I wonder if you are seeing a regression or a problem that's always been there. Sounds weird that you say the difference is dramatic but you can't reproduce it. It might depend either on your queries or your analysis chain, would be great if you can open another issue with some examples of what doesn't work.
</comment><comment author="roytmana" created="2013-11-06T14:40:27Z" id="27878185">well it worked (except for the wildcards) in the very first drop I tested
(remember when I posted some performance stats) and then it had scrambled
highlighting with some results not highlighted at all and now it
practically highlight nothing

I will open a new issue based on my last comment
I will attach the sample recreation with analysis chain I use and a sample
query but it does not show the issue
I can post my real mapping and few real queries if it is of any help
hopefully it may give you some ideas and you could suggest steps for
troubleshooting/recreating
is there anything else I can provide?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fixed doc_id used in combination with context.searcher(), needs to be topLevelId rather than just docId</comment></comments></commit></commits></item><item><title>Index Stats: Add support for segments stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4102</link><project id="" key="" /><description>closes #4101
</description><key id="22159170">4102</key><summary>Index Stats: Add support for segments stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2013-11-05T23:08:02Z</created><updated>2014-07-11T18:54:13Z</updated><resolved>2013-11-06T19:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-06T09:58:39Z" id="27858007">Good addition. Left some minor comments.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index Stats: Add support for segments stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4101</link><project id="" key="" /><description>Would be nice to have the number of segments as a stats info on a shard/index/node level (in `CommonStats`).
</description><key id="22154410">4101</key><summary>Index Stats: Add support for segments stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-05T21:55:32Z</created><updated>2013-11-07T00:23:03Z</updated><resolved>2013-11-06T19:54:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/SimpleIndexStatsTests.java</file></files><comments><comment>Index Stats: Add support for segments stats</comment><comment>closes #4101</comment></comments></commit></commits></item><item><title>NPE when closing XContentBuilder and using 'pretty' query parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4100</link><project id="" key="" /><description>This is happening in 0.90.6 and didn't happen in 0.90.5.

I created a gist to demonstrate the problem.  https://gist.github.com/seallison/7325931

If you don't close the XContentBuilder, the pretty query parameter works fine.  

If you _do_ close w/ pretty, the close() method attempts to write a newline character to the end of the closed generator.  This creates the NullPointerException as shown in the gist.  This line appears to be the culprit:

```
generator.writeRaw(LF);
```
</description><key id="22150605">4100</key><summary>NPE when closing XContentBuilder and using 'pretty' query parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seallison</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-11-05T20:58:30Z</created><updated>2013-11-06T00:11:04Z</updated><resolved>2013-11-06T00:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="salyh" created="2013-11-05T21:12:38Z" id="27812766">i guess you should not close the builder:

&lt;pre&gt;&lt;code&gt;
try {
            XContentBuilder builder = RestXContentBuilder.restContentBuilder(request).startObject();
            builder.field("foo", "bar");
            builder.endObject();
            channel.sendResponse(new XContentRestResponse(request, RestStatus.OK, builder));
        }
        catch (Exception e) {
            logger.error("error", e);
        }
&lt;/code&gt;&lt;/pre&gt;

this works for me
</comment><comment author="seallison" created="2013-11-05T21:17:17Z" id="27813161">Yes, not closing the builder solves the problem, as I noted in the comment in my gist.

The point is the builder isClosed() should be tested before blindly writing to it.
</comment><comment author="salyh" created="2013-11-05T21:19:35Z" id="27813342">If builder is closed explicitly then sendResponse() failed with a NPE on &lt;code&gt;UTF8JsonGenerator._writeBytes(byte[]) line: 1125&lt;/code&gt;  because &lt;code&gt;_outputBuffer&lt;/code&gt; in  &lt;code&gt;System.arraycopy(bytes, 0, _outputBuffer, _outputTail, len)&lt;/code&gt; is null because its already closed. 

Maybe a fix could look like this in &lt;code&gt;JsonXContentGenerator&lt;/code&gt;

&lt;pre&gt;&lt;code&gt;
 @Override
    public void close() throws IOException {
        if (writeLineFeedAtEnd &amp;&amp; !generator.isClosed()) {
            flush();
            generator.writeRaw(LF);
        }
        generator.close();
    }
&lt;/code&gt;&lt;/pre&gt;
</comment><comment author="seallison" created="2013-11-05T21:41:59Z" id="27815120">Sure, if you don't want the newline character to be added - which was the point of the feature.. maybe leveraging Jackson's pretty printing features would be a solution.

This really is a simple/minor problem, I'm just complaining because I closed all my objects and now need to remove that just to get the same queries that worked in 0.90.5 to work in 0.90.6.  Because this wasn't documented as a breaking change, it caught me by surprise.
</comment><comment author="kimchy" created="2013-11-06T00:01:32Z" id="27826322">yea, unexpected to be honest..., but we can fix it nonetheless..., will push a fix.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java</file></files><comments><comment>NPE when closing XContentBuilder and using 'pretty' query parameter</comment><comment>fixes #4100</comment></comments></commit></commits></item><item><title>Support multifield path per field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4099</link><project id="" key="" /><description>Currently multifield mapping support path parameter per property so if you need to have several fields mapped for a property they will either all have full path name or just_name 

it is rather inconvenient when you want to have the property with say two secondary fields one with full name (because it only makes sense as a variant of primary field say not analyzed) and one with just_name because you want to have an all-like field to which many of your properties contribute.

consider an example (a part of a bigger json)

"category": {"code":"CTZ", "description":"My Description"}

code was indexed as multifield resulting in names
category.code
category.code.untouched

later I want to have my_all field to where I want to index category.code as well as other fields

I will add path:"just_name" to my mapping and another field my_all

that will immediately break my application because untouched will become a just_name mapping as well and all untouched from all my data elements will be rolled into it

My current workaround is to provide full name for untouched field (category.code.untouched) so it retains its full name. I am not sure it is intentional behavior but it seems to work (need to test it more)

But much cleaner approach would be to allow path per field in multifield mapping
</description><key id="22147519">4099</key><summary>Support multifield path per field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-11-05T20:11:54Z</created><updated>2014-01-13T11:36:24Z</updated><resolved>2014-01-08T23:45:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-06T03:34:16Z" id="27839893">I attempted to solve this problem a few months ago in #2535. I even started implementing it, but then I stumbled upon an issue that I couldn't quite figure out. In order to be consistent if we support field-level path for multi_field, we also need to support it for objects. And with objects adding field-level path creates an interesting issue. Here is an example that demonstrates this issue. Assuming that we enable "path":"just_name" on the field level, how should we deal with the following mapping:

```
{
    "my_type": {
        "properties": {
            "abc": {
                "properties": {
                    "foo": {
                        "type": "object",
                        "path": "just_name",
                        "properties": {
                            "bar": { 
                                "type": "string"
                            }
                        }
                    }, 
                    "baz": {
                        "type": "string",
                        "path": "just_name"
                    }
                }
            }
        }
    }
}
```

Here we have  object `abc` with two properties `foo` and `baz`. With `baz` everything is clear, we have field-level `"path":"just_name"`, so `{"abc":{"baz":"123"}}` should be indexed `baz:123`. 

With `foo` it's not as clear. Does `"path":"just_name"` apply to `foo` as a property of `abc`, or does it apply to `foo` as an object (the way it works today). In the former case, `"path":"just_name"` is a field-level path and therefore `{"abc": {"foo": {"bar":123}}}` should be indexed as `foo.bar:123`. In the latter case, `"path":"just_name"` is an object-level path and it affects only properties of `foo` and therefore `{"abc": {"foo": {"bar":123}}}` should be indexed as `bar:123`. 
</comment><comment author="roytmana" created="2013-11-06T14:13:25Z" id="27875976">Thank you Igor. Do you see any danger in giving multi_fields "."-separated names to mimic full name?

like award.activity.code.na in the example below. I can work around the issue using this approach if use of "." would not pose any future compatibility risks. It works so far but I want to be sure it is not accidental

```
...
"activity": {
              "dynamic": "true",
              "properties": {
                "code": {
                  "type": "multi_field",
                  "path": "just_name",
                  "fields": {
                    "code": {
                      "type": "string",
                      "index_options": "offsets",
                      "boost": 0.3
                    },
                    "all": {
                      "type": "string",
                      "index_options": "offsets",
                      "boost": 0.3
                    },
                    "all_stem": {
                      "type": "string",
                      "index_options": "offsets",
                      "boost": 0.3
                    },
                    "award.activity.code.na": {
                      "type": "string",
                      "index": "not_analyzed",
                      "omit_norms": true,
                      "index_options": "docs",
                      "include_in_all": false
                    }
                  }
                },
...
```
</comment><comment author="imotov" created="2013-11-07T02:04:12Z" id="27932521">I think your workaround should work, but it's not pretty. I would love to come up with a better solution. Perhaps we can change the `path_only` behavior, or add some `field_path_only` attribute to differentiate it from object-level `path_only` or perhaps add a `full_index_name` attribute that would work like this. Fields in a document with the following mapping:

```
{
    "my_type": {
        "properties": {
            "obj": {
                "properties": {
                    "foo": {
                        "type": "string",
                    }, 
                    "bar": {
                        "type": "string",
                        "index_name": "alt_bar"
                    }, 
                    "baz": {
                        "type": "string",
                        "full_index_name": "alt_baz"
                    }
                }
            }
        }
    }
}
```

would be indexed as `obj.foo`, `obj.alt_bar` and `alt_baz`. 

In other words, I think that we've identified the problem, but solution still requires some brain-storming. 
</comment><comment author="clintongormley" created="2013-11-07T13:04:23Z" id="27960626">I find the `{path: just_name | full}` setting rather confusing, for the reasons listed above.

My suggestion is to remove the `path` setting and instead to support `index_name` and `index_path`. 

If you specify an `index_name` then it is appended to the full path.
If you specify an `index_path` then it sets the absolute field name.

This would allow mappings like:

```
{
    "my_type": {
        "properties": {
            "name": {
                "properties": {
                    "first": {
                        "type": "multi_field",
                        "fields": {
                            "first": { "type": "string" },
                            "full": { 
                                "type": "string",
                                "index_path": "name.full"
                            },
                        }
                    }, 
                    "last": {
                        "type": "multi_field",
                        "fields": {
                            "last": { "type": "string", "index_name": "surname" },
                            "full": { 
                                "type": "string",
                                "index_path": "name.full"
                            },
                        }
                    } 
                }
            }
        }
    }
}
```

In this example, you'd have:
-  `name.first` and `name.first.first`
- `name.last` and `name.last.surname`
- `name.full` (which would index values from `name.first` and `name.last`)

What do you think of this proposal?
</comment><comment author="imotov" created="2013-11-07T14:04:09Z" id="27966106">Yes! `index_path` - that's the name I was looking for. I love it! We need to think about the proper way of removing `path` though. We need to make sure that it's still possible to migrate current indices with `path` in them to the next version without reindexing. Maybe we can deprecate `path` and keep it for a while. It will continue to affect `index_name`, but not `index_path`.
</comment><comment author="roytmana" created="2013-11-07T14:31:00Z" id="27969079">Good news! Never liked just_name thing. Good riddance. What would prevent me from putting wrong index name for the primary field or is it always translated to index name via its mapping json key (matching property name)? 

What if we replace fields with array and get rid of redundant json keys for the fields.
where absence of index_name/path indicate the primary field to make sure it is always in sync with property name

```
"last": {
                        "type": "multi_field",
                        "fields": [
                            { "type": "string"},
                            { "type": "string", "index_name": "untouched" ...},
                            { "type": "string", "index_path": "name.full" },
                        ] 
                    }   
```
</comment><comment author="clintongormley" created="2013-11-07T14:40:23Z" id="27970178">@roytmana for 90% of use cases, specifying the field name and the mapping is more understandable than the array with index_name, so I'd keep that as it is.
</comment><comment author="roytmana" created="2013-11-07T14:49:00Z" id="27971165">@clintongormley Requirement to provide field name matching property name to identify the primary field is error prone and redundant better supply no name at all. Of course not supplying the name would not work with json object structure.

May be it was intuitive before you introduced concept of index_name and index_path but not any more. now these attributes are the important ones and field json keys are truly redundant  and serve no purpose just making mapping bigger and more confusing with competing naming strategies

just my 2c
</comment><comment author="mattweber" created="2013-11-07T16:35:02Z" id="27981537">I would like to see something we we can do a basic field "copy" without needing all the multi_field stuff unless you need different analyzer settings, etc.  Something like:

```
{
    "my_type": {
        "properties": {
            "name": {
                "properties": {
                    "first": {"type":"string", "copy_to":["name.full"]},
                    "last": {"type":"string", "index_name": "surname", "copy_to":["name.full"]},
                    "full": {"type": "string"}
                }
            }
        }
    }
}
```

This would give us:

name.first
name.surname
name.full (w/ both first and last)

This way we can avoid all the duplicated "full" mappings.
</comment><comment author="roytmana" created="2013-11-07T21:24:43Z" id="28007837">please also see #4123 particularly my last comment

---

I do not know lucene internals but from ES search API perspective name field is not collapsed into one field across type and category properties and can be searched independently from each other. 
I always assumed their lucene name is their full name (type.name and category.name) just like if i did not use just_name since they are the primary fields of multifield mappings (their field name matches property name)

the quick test shows that what you say is correct and primary field 'name' in my case acts no different than 'all' field in my mapping which i intended to COLLAPSE

If ES does not expand primary field name to full path, than the whole concept of multi_field is flawed! All ES examples virtually imply that it is not so

I am already using full path name for parts of multifield that I do not want to collapse but I thought the primary fields will be different. I will have to switch to using full path for all field of multifields that i do not intend to collapse - what a pain
</comment><comment author="imotov" created="2014-01-08T23:45:09Z" id="31888898">This issue was superseded by #4520 and #4521. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add /_cat endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4098</link><project id="" key="" /><description /><key id="22147139">4098</key><summary>Add /_cat endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-11-05T20:06:17Z</created><updated>2014-07-16T21:51:34Z</updated><resolved>2013-11-05T21:10:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-05T20:31:54Z" id="27809569">+1
</comment><comment author="imotov" created="2013-11-05T21:10:21Z" id="27812566">Pushed to master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ArrayIndexOutOfBoundsException when creating ImmutableSettings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4097</link><project id="" key="" /><description>This line appears to cause an ArrayIndexOutOfBoundsException if the settings parameter is nonempty.

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java#L542

I'm using the scalastic wrapper and following the steps in README.md. The following line errors out:
indexer.createIndex(indexName, settings = Map("number_of_shards" -&gt; "1"))
</description><key id="22144344">4097</key><summary>ArrayIndexOutOfBoundsException when creating ImmutableSettings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chelseaz</reporter><labels /><created>2013-11-05T19:28:48Z</created><updated>2013-11-05T20:05:38Z</updated><resolved>2013-11-05T19:42:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-05T19:33:50Z" id="27804590">seems like a problem in scalastic or scala? we have a put(Map) as well, which should be used when passing a Map, no?
</comment><comment author="chelseaz" created="2013-11-05T19:42:26Z" id="27805351">Ah yes. Scalastic is passing in a scala map and not explicitly converting it to a java map. Before this put method was added, scalastic was relying on an implicit conversion to a java map. I'll raise the issue with them. Thanks!
</comment><comment author="kimchy" created="2013-11-05T19:48:07Z" id="27805849">I think we can fix something on our end as well, lemme check....
</comment><comment author="kimchy" created="2013-11-05T20:05:38Z" id="27807473">I improved the handling for single value parameter in the put method for master/0.90 on our end.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file></files><comments><comment>better support for single value to settings builder where the type gets lost</comment><comment>relates to #4097</comment></comments></commit></commits></item><item><title>fixes issue #4028; CompletionTokenStream was abusing the TermToBytesRefA...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4096</link><project id="" key="" /><description>...ttribute interface by reseting the underlying bytesRef on each term.

Didn't add any tests as I didn't know where to start doing so.

Note that issue #4040 attempted to fix this but instead broke the CompletionTokenStream entirely.
</description><key id="22137391">4096</key><summary>fixes issue #4028; CompletionTokenStream was abusing the TermToBytesRefA...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dspangen</reporter><labels /><created>2013-11-05T17:42:43Z</created><updated>2014-06-18T18:53:23Z</updated><resolved>2013-11-08T17:18:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-08T17:18:42Z" id="28080267">@dspangen setting the bytesref multiple times is completely fine. I will close this PR for now. See https://github.com/elasticsearch/elasticsearch/pull/4129 where we handle this problem now. Thanks for opening this anyways!
</comment><comment author="s1monw" created="2013-11-10T19:44:58Z" id="28158597">@dspangen actually your patch would have fixed this issue. Nevertheless I changes some other things to support CharTermAttribute lazily in #4129 

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consistent support for fields/_source params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4095</link><project id="" key="" /><description>The `fields` parameter is supported by:
- `explain`
- `get`
- `mget`
- `search`
- `update`

The `_source`, `_source_includes`, `_source_excludes` parameters are supported by:
- `explain`
- `get`
- `get_source`
- `mget`
- `search`

To be consistent, the `fields` parameter should be added to:
- bulk update 
- `msearch`

and the `_source*` parameters should be added to:
- `update`
- bulk update
- `msearch`

@bleskes ^^
</description><key id="22134995">4095</key><summary>Consistent support for fields/_source params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2013-11-05T17:06:29Z</created><updated>2015-06-07T11:39:10Z</updated><resolved>2015-06-07T11:39:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-07T11:39:06Z" id="109741817">Closing in favour of #11527
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>indices.recovery.concurrent_small_file_streams is not dynamically settable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4094</link><project id="" key="" /><description>`indices.recovery.concurrent_small_file_streams` has all the code to be dynamically settable, but hasn't been added to the `ClusterDynamicSettingsModule`.
</description><key id="22134545">4094</key><summary>indices.recovery.concurrent_small_file_streams is not dynamically settable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>bug</label><label>v0.90.7</label></labels><created>2013-11-05T17:00:22Z</created><updated>2013-11-05T17:01:03Z</updated><resolved>2013-11-05T17:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file></files><comments><comment>Fix `indices.recovery.concurrent_small_file_streams` not being dynamically settable</comment></comments></commit></commits></item><item><title>0.90.6 memory issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4093</link><project id="" key="" /><description>0.90.6 needs a lot more direct memory compared to 0.90.5 when using an in memory store for tests.

Setting needed for 0.90.6

```
-XX:MaxDirectMemorySize=4608m
```

Setting needed for 0.90.5

```
-XX:MaxDirectMemorySize=512m
```
</description><key id="22134432">4093</key><summary>0.90.6 memory issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">brackxm</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-11-05T16:59:06Z</created><updated>2013-11-12T17:21:05Z</updated><resolved>2013-11-06T13:15:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-05T17:11:17Z" id="27792163">Hiya

Yes, this is a known issue, see #4078 

We will be releasing version 0.90.7 soon, which will fix this.  In the meantime, you can add this to your config:

```
index.warmer.enabled: false
```
</comment><comment author="clintongormley" created="2013-11-05T17:19:54Z" id="27792997">(at least I assume this is the same thing)

Could you try the setting above and let me know if it fixed the issue?

thanks
</comment><comment author="brackxm" created="2013-11-05T17:30:54Z" id="27794008">No, that setting does not fix the issue.
</comment><comment author="clintongormley" created="2013-11-05T17:33:41Z" id="27794281">Could you give us some idea of what your tests do and how you configure elasticsearch?
</comment><comment author="brackxm" created="2013-11-05T17:34:07Z" id="27794318">stacktrace

```
java.lang.OutOfMemoryError: Direct buffer memory
    at java.nio.Bits.reserveMemory(Bits.java:658)
    at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)
    at org.apache.lucene.store.bytebuffer.PlainByteBufferAllocator.allocate(PlainByteBufferAllocator.java:55)
    at org.apache.lucene.store.bytebuffer.CachingByteBufferAllocator.allocate(CachingByteBufferAllocator.java:52)
    at org.elasticsearch.cache.memory.ByteBufferCache.allocate(ByteBufferCache.java:101)
    at org.apache.lucene.store.bytebuffer.ByteBufferIndexOutput.switchCurrentBuffer(ByteBufferIndexOutput.java:106)
    at org.apache.lucene.store.bytebuffer.ByteBufferIndexOutput.writeBytes(ByteBufferIndexOutput.java:93)
    at org.elasticsearch.common.lucene.store.BufferedChecksumIndexOutput.flushBuffer(BufferedChecksumIndexOutput.java:69)
    at org.apache.lucene.store.BufferedIndexOutput.flushBuffer(BufferedIndexOutput.java:113)
    at org.apache.lucene.store.BufferedIndexOutput.flush(BufferedIndexOutput.java:102)
    at org.elasticsearch.common.lucene.store.BufferedChecksumIndexOutput.flush(BufferedChecksumIndexOutput.java:80)
    at org.apache.lucene.store.BufferedIndexOutput.close(BufferedIndexOutput.java:126)
    at org.elasticsearch.common.lucene.store.BufferedChecksumIndexOutput.close(BufferedChecksumIndexOutput.java:60)
    at org.elasticsearch.index.store.Store$StoreIndexOutput.close(Store.java:587)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:140)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter.close(Lucene41PostingsWriter.java:582)
    at org.apache.lucene.util.IOUtils.closeWhileHandlingException(IOUtils.java:81)
    at org.apache.lucene.codecs.BlockTreeTermsWriter.close(BlockTreeTermsWriter.java:1082)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsConsumer.close(BloomFilterPostingsFormat.java:408)
    at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat$1.close(ElasticSearch090PostingsFormat.java:63)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsConsumerAndSuffix.close(PerFieldPostingsFormat.java:86)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:163)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.close(PerFieldPostingsFormat.java:154)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:140)
    at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:102)
    at org.apache.lucene.index.TermsHash.flush(TermsHash.java:116)
    at org.apache.lucene.index.DocInverter.flush(DocInverter.java:53)
    at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:81)
    at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:466)
    at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:499)
    at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:609)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:367)
    at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:277)
    at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:252)
    at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:242)
    at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:170)
    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:118)
    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:155)
    at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:204)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:786)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:448)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:228)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```
</comment><comment author="brackxm" created="2013-11-05T17:36:28Z" id="27794491">configuration

```
        final Node node = NodeBuilder.nodeBuilder()
                .local(true)
                .clusterName(clusterName)
                .settings(ImmutableSettings.settingsBuilder()
                        .put("index.store.type", "memory")
                        .put("index.number_of_shards", "1")
                        .put("index.number_of_replicas", "0")
                        .put("gateway.type", "none")
                        .put("http.enabled", false)
                        .put("index.warmer.enabled", false)
                        .put("path.data", "target/es")
                        .put("path.logs", "target/es")
                        .put("path.work", "target/es")
                        .build())
                .build();
```
</comment><comment author="kimchy" created="2013-11-05T17:39:49Z" id="27794784">can you maybe write isolate it to a test case that shows that it passes in 0.90.5, while it fails in 0.90.6 (with the `index.warmer.enabled` set to `false`)? it would be a great help in chasing it down...
</comment><comment author="brackxm" created="2013-11-05T18:14:26Z" id="27797694">```
import org.elasticsearch.client.Client;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.node.Node;
import org.elasticsearch.node.NodeBuilder;

import java.util.UUID;

public class Issue {
    public static void main(final String[] args) {
        final Node node = NodeBuilder.nodeBuilder()
                .local(true)
                .clusterName("test")
                .settings(ImmutableSettings.settingsBuilder()
                        .put("index.store.type", "memory")
                        .put("index.number_of_shards", "1")
                        .put("index.number_of_replicas", "0")
                        .put("gateway.type", "none")
                        .put("http.enabled", false)
                        .put("index.warmer.enabled", false)
                        .build())
                .build();
        node.start();
        final Client client = node.client();
        final byte[] source = "{\"a\":\"a1\"}".getBytes();
        for (int i = 0; i &lt; 1000; i++) {
            final String id = UUID.randomUUID().toString();
            client.prepareIndex("index1", "type1", id)
                    .setSource(source)
                    .setRefresh(true)
                    .execute()
                    .actionGet();
        }
    }
}
```
</comment><comment author="kimchy" created="2013-11-05T22:24:33Z" id="27819572">@s1monw and myself chased it up, seems like a regression in Lucene, opened an issue: https://issues.apache.org/jira/browse/LUCENE-5330 and already have a patch for it: https://issues.apache.org/jira/secure/attachment/12612263/LUCENE-5330.
</comment><comment author="brackxm" created="2013-11-05T22:58:44Z" id="27822382">thanks for looking into this
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineIntegrationTest.java</file></files><comments><comment>Reduce number of docs in test for #4093 - 30 docs are enough to trigger the bug</comment></comments></commit><commit><files><file>src/main/java/org/apache/lucene/index/XIndexWriter.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineIntegrationTest.java</file></files><comments><comment>Apply fix for LUCENE-5330 pruning the IndexWriter queue to get rid of pending event</comment></comments></commit></commits></item><item><title>Change 'standard' analyzer to use empty stopword list by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4092</link><project id="" key="" /><description>The 'default' / 'standard' analyzer can be a trappy default sicne it filters
english stopwords by default. Yet a default should not be dedicated to a certain language
since elasticsearch is used in many different scenarios where a standard analysis chain
with specialization to english full-text might be rather counter productive.

This commit changes the 'standard' analyzer to use an empty stopword list for indices
that are created from 1.0.0.Beta1 version onwards but will maintain backwards compatibiliy
for older indices.

Closes #3775
</description><key id="22132324">4092</key><summary>Change 'standard' analyzer to use empty stopword list by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-05T16:30:34Z</created><updated>2014-06-23T23:49:53Z</updated><resolved>2013-11-05T20:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-05T17:17:54Z" id="27792790">This looks great to me!, should we go over all the built in analyzers we have today, and remove stop words from them as well? Like dedicated language analyzers.
</comment><comment author="s1monw" created="2013-11-05T19:44:18Z" id="27805499">@kimchy I think the lang analyzers should have the stopwords that's their purpose really. I don't think we need to go over the others (lang ones). I will still check the others but IMO should be done in a different issue.

@clintongormley I updated the docs if you can take another look I wanna merge it in soonish.
</comment><comment author="kimchy" created="2013-11-05T19:49:59Z" id="27806025">@s1monw sounds good on a different issue (regarding stop words for lang analyzers). I think it comes from how we look at it, is the default, regardless of anything, we don't recommend stopwords?
</comment><comment author="clintongormley" created="2013-11-05T20:08:11Z" id="27807679">@s1monw docs look good to me. 
@kimchy re "we don't recommend stopwords" is an interesting question.  i'd be tempted to say yes, esp with (a) modern hardware, (b) common terms query and (c) difficult to find stopword lists.  @s1monw the language analyzers  also do stemming so they're still useful without stopwords. what do you think would be the better default?  If with stopwords, why?
</comment><comment author="s1monw" created="2013-11-05T20:09:35Z" id="27807810">pushed
</comment><comment author="kimchy" created="2013-11-05T20:19:24Z" id="27808624">:+1: !, here is for never hearing "why is `a` not sortable" again :)
</comment><comment author="nik9000" created="2013-11-05T20:29:23Z" id="27809394">Not using stop words is in my list but we use the query_string query type which doesn't look like it supports building common terms queries.  I know query_string isn't exactly recommended either but our (power) users are used to syntax and rebuilding it isn't something I'd enjoy doing.
</comment><comment author="s1monw" created="2013-11-05T20:33:04Z" id="27809655">@nik9000 we clearly need a simpler query parser that allows for the basics. What are the essentials for you?
</comment><comment author="nik9000" created="2013-11-05T20:35:04Z" id="27809808">@s1monw I'm willing to write it up but would you like to move this to another issue?
</comment><comment author="clintongormley" created="2013-11-05T20:36:22Z" id="27809919">Please ping me when you do - I have things I want in there too :)
</comment><comment author="s1monw" created="2013-11-05T20:36:39Z" id="27809944">sure! feel free to open one unless it's like (all of the existing features :) )
</comment><comment author="s1monw" created="2013-11-05T20:37:14Z" id="27810002">@clintongormley but this will not support the full power of perl regex :)
</comment><comment author="clintongormley" created="2013-11-05T20:39:44Z" id="27810183">@s1monw all i want is the elegance of Perl ;)
</comment><comment author="uboness" created="2013-11-05T20:45:50Z" id="27810639">@clintongormley I guess it's time to revive our experiments :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix missing affectation in SimpleChildQuerySearchTests.testParentChildQu...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4091</link><project id="" key="" /><description>Fix missing affectation in `SimpleChildQuerySearchTests.testParentChildQueriesCanHandleNoRelevantTypesInIndex()`.
The test continued to assert against the result of the first search request.

Fortunately, this hid no failing assertions.
</description><key id="22132322">4091</key><summary>Fix missing affectation in SimpleChildQuerySearchTests.testParentChildQu...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ofavre</reporter><labels /><created>2013-11-05T16:30:31Z</created><updated>2014-07-16T21:51:35Z</updated><resolved>2013-11-07T15:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-07T15:00:03Z" id="27972212">Merged, thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dist. Percolation: Use .percolator instead of _percolator for type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4090</link><project id="" key="" /><description>Use `.percolator` as the internal (hidden) type name for percolators within the index. Seems nicer name to represent "hidden" types within an index.
</description><key id="22130112">4090</key><summary>Dist. Percolation: Use .percolator instead of _percolator for type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-11-05T16:03:30Z</created><updated>2013-11-05T19:50:35Z</updated><resolved>2013-11-05T19:50:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java</file><file>src/test/java/org/elasticsearch/percolator/ConcurrentPercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/MultiPercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorFacetsTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/RecoveryPercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file></files><comments><comment>Dist. Percolation: Use .percolator instead of _percolator for type name</comment><comment>Use .percolator as the internal (hidden) type name for percolators within the index. Seems nicer name to represent "hidden" types within an index.</comment><comment>closes #4090</comment></comments></commit></commits></item><item><title>River does not start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4089</link><project id="" key="" /><description>In 0.90.6, creating a river seems not to start the river.

Example debug log (for JDBC river)

https://gist.github.com/jprante/7321028

What am I doing wrong?
</description><key id="22129752">4089</key><summary>River does not start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jprante</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-11-05T15:58:54Z</created><updated>2014-04-08T13:56:45Z</updated><resolved>2013-11-10T20:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="richardwilly98" created="2013-11-05T16:03:09Z" id="27785351">More details here [1]

[1] - https://groups.google.com/forum/#!topic/elasticsearch/hkBqWisL4UI
</comment><comment author="imotov" created="2013-11-07T01:24:02Z" id="27930836">I briefly looked at this issue and it looks like the problem is caused by race condition in creation of river's `_meta` document and propagation of cluster state with the new river mapping. Basically, river creation and startup fails when [RiversRouter](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/river/routing/RiversRouter.java#L110) tries to load `_meta` record for the new river mapping and this `_meta` document doesn't exist yet. 

I can only reproduce it if the `_river` index doesn't exists or exists but doesn't have any types in it. After I create the first [dummy river](http://www.elasticsearch.org/guide/en/elasticsearch/rivers/current/how-it-works.html), all other rivers seem to work just fine.
</comment><comment author="richardwilly98" created="2013-11-07T11:47:29Z" id="27956385">Even with your suggestion in my "@BeforeSuite" method:

``` java
    private void registerDummyRiver() {
        if (!node.client().admin().indices().prepareExists("_river").get().isExists()) {
            node.client().admin().indices().prepareCreate("_river").get();
        }
        if (!node.client().prepareGet("_river", "my_dummy", "_meta").get().isExists()) {
            node.client().prepareIndex("_river", "my_dummy", "_meta").setSource("{ \"type\": \"dummy\" }").get();
        }
        refreshIndex("_river");
        Assert.assertTrue(node.client().prepareGet("_river", "my_dummy", "_meta").get().isExists());
    }

```

I still have tests failing randomly.
</comment><comment author="javanna" created="2013-11-10T20:04:17Z" id="28159096">I pushed a fix for this. Could you confirm it fixes the issue you are experiencing?
</comment><comment author="richardwilly98" created="2013-11-11T15:04:32Z" id="28206675">@javanna it definitely help in my integration test.  
All test are now passing after a small change when the river is created.

In 0.90.5 it looks like `node.client().prepareIndex("_river", "my_dummy", "_meta")` and wait for 1 second was enough to ensure that the river was registered and started.

So as workaround I am checking a flag attribute set by `river.start()` method.

Would it make sense to have such feature in ES api?
</comment><comment author="javanna" created="2013-11-11T18:38:13Z" id="28226266">I'm not sure I got where you put the 1 second wait with 0.90.5. 

Anyways, with 0.90.6 there is a different problem, as we send the mapping update to the master node (when you create the river type by indexing the `_meta` document) before actually indexing the documen. The mapping update on the master should trigger the river start, but it needs the `_meta` document to be available, which is not the case, thus in most cases rivers won't get started.

My fix addresses this for rivers by scheduling a retry (actually a few of them just in case) if the `_meta` document isn't found. I think this should be enough, could you confirm this?
</comment><comment author="richardwilly98" created="2013-11-11T20:27:18Z" id="28235569">As I said all test are now passing so your fix addresses the issue.

I am just trying to understand how I could make sure from my integration test when the river is really started. Right now (and it was already the case before I assume) there is no way to tell that from the api so it has to be built in the river.  

Is that correct?

Thanks a lot for the fix.
</comment><comment author="javanna" created="2013-11-11T21:08:57Z" id="28238919">Ok thanks @richardwilly98 , got it, I just wanted to make sure no other workarounds are needed.

I got what you are asking, you can have a wait and check if something happened (for instance check if an index is expected to be created is there). You are right, you need to handle this in the river itself. Would be even better to have different retries and a maximum wait time. We are going to package our test classes as a separate jar with the next release, so that those classes can easily be used to test plugins; the method that you'd need in this case is `ElasticsearchTestCase#awaitBusy`. 
</comment><comment author="jprante" created="2013-11-12T09:29:33Z" id="28279162">Tests with JDBC river now pass with ES 0.90.7-SNAPSHOT.

Thanks @javanna !
</comment><comment author="richardwilly98" created="2013-11-17T14:10:56Z" id="28649104">@javanna unfortunately I am still having issues with ES 0.90.7 where rivers are not started.

I have post a gist [1] with ES log level to DEBUG.  
Starting line 6982 you can see the sequence after `node.client().prepareIndex("_river", river, "_meta").setSource(settings).get();` I wait up to 30 seconds but the river does not start.

In my scenario rivers are started about 50 times and 14 of them fail.

[1] - https://gist.github.com/richardwilly98/7513822/raw/16e30de3f7c7e78a56b9e1bfa68c8fbbb48ad132/gistfile1.txt
</comment><comment author="javanna" created="2013-11-18T09:57:15Z" id="28685950">@richardwilly98 that's odd. We currently retry registering the river for max 5 times, one attempt per second, in case the `_meta` document is not found when the master node gets the mapping update caused by the index operation of the `_meta` doc itself. That would mean that in your case, the `_meta` document takes more than 5 seconds to show up after its mapping update, which sounds strange.

Did anything change since you previously said that the issue was fixed? I'm curious on how you create the river. Is the river already registered in the cluster state or do you recreate everything from scratch in your tests?
</comment><comment author="richardwilly98" created="2013-11-18T11:44:43Z" id="28691952">@javanna last time I have only executed the test on a dummy river which register only 1 river but did not test it on the real river.
- River is created using this code [1].
- River is delete using this code [2].

[1] - https://github.com/richardwilly98/elasticsearch-river-mongodb/blob/master/src/test/java/org/elasticsearch/river/mongodb/RiverMongoDBTestAbstract.java#L300
[2] - https://github.com/richardwilly98/elasticsearch-river-mongodb/blob/master/src/test/java/org/elasticsearch/river/mongodb/RiverMongoDBTestAbstract.java#L378
</comment><comment author="javanna" created="2013-11-18T13:35:48Z" id="28697849">@richardwilly98 I spent some time looking at your log file. I do see that 5 attempts are not enough in some cases, but it's hard to understand why and if it's a test problem or not. Do you have failures on the same tests or random ones? Can you make logging more specific to rivers, log when the before test method starts and ends, same for the after method and run it again? Maybe that would help understanding what's going on.

@jprante everything ok with your river or do you have the same problem?
</comment><comment author="richardwilly98" created="2013-11-18T13:45:58Z" id="28698473">@javanna 
The failures are random. The same tests work fine with ES 0.90.5.
What king of information should I log in before / after test method?
</comment><comment author="jprante" created="2013-11-18T14:32:16Z" id="28701832">@javanna With 0.90.7, I can create JDBC river instances flawlessly (a manual test succeeds, also junit tests). If required, I could spend some time on setting up random test creations of river instances.
</comment><comment author="javanna" created="2013-11-18T14:48:23Z" id="28703165">Thanks @jprante for your feedback! We do have a test for river creation now, which is part of our randomized tests that countinuously run. That test never failed since I added it, maybe it's just not nasty enough.

@richardwilly98 I just want to be able to read those logs properly. From the big log you attached it's hard to understand when a test method starts, what is part of the before test and what is part of the after test cleanup. We need to isolate the failures in order to understand what caused them.
</comment><comment author="richardwilly98" created="2013-11-18T14:59:36Z" id="28704162">@javanna I understand I will produce better logs and isolate a working test and a failed one.
</comment><comment author="richardwilly98" created="2013-11-18T16:19:31Z" id="28712059">@javanna 
I have posted a new gist [1]. It contains 2 file:
- RiverMongoInitialImportTest a successful test
- RiverMongoInitialTimestampTest test where river is not started

[1] - https://gist.github.com/richardwilly98/7530444/edit
</comment><comment author="richardwilly98" created="2013-11-19T11:03:29Z" id="28781503">@javanna 
Do not spend time on analyzing the logs. That might be due to the way river closing is handled on my side.
I will keep you updated.
</comment><comment author="richardwilly98" created="2013-11-19T13:48:46Z" id="28790804">@javanna 
I had an issue on my side where one thread was not correctly stopped on closing the river.
I am not sure the reason tests were still successful with ES 0.90.5...
</comment><comment author="javanna" created="2013-11-19T14:54:38Z" id="28795806">Ok, thanks @richardwilly98 . Does it mean that all your tests are consistently green now?
</comment><comment author="richardwilly98" created="2013-11-19T14:57:08Z" id="28796024">Yes
</comment><comment author="javanna" created="2013-11-19T15:03:45Z" id="28796601">Cool, glad to hear that!
</comment><comment author="ryan1234" created="2014-01-21T06:08:36Z" id="32823842">I'm having similar problems with 0.90.9 and the JDBC river plugin. I have a simple two node cluster and the JDBC river seems to not start roughly 4 out of 5 times. I have another custom river running and that always starts and runs. The DEBUG log for my cluster is obviously very verbose. What does a retry look like in the logs? 

From reading through the logs I can't seem to find any errors or warnings about starting the JDBC river.  When I restart the master node it just seems to ignore that river altogether.
</comment><comment author="dadoonet" created="2014-01-21T06:20:02Z" id="32824235">Are you using templates?
</comment><comment author="ryan1234" created="2014-01-21T06:22:24Z" id="32824322">Sorry for my ignorance, but what is a template?
</comment><comment author="dadoonet" created="2014-01-21T06:24:38Z" id="32824399">Forget my previous comment. There is an issue #4577 which I think could produce this when you have more than one river running.
</comment><comment author="ryan1234" created="2014-01-21T06:28:55Z" id="32824566">We don't have any templates defined. I have seen an example where more than one river runs successfully one our cluster. Is it a rare use case to have more than one river running?
</comment><comment author="ryan1234" created="2014-01-21T19:36:30Z" id="32954527">Some additional notes:
- I ended up scaling down from a cluster of two nodes to one node. The rivers start reliably on one node for whatever reason.  For the time being we'll scale that node up and hopefully in a month or two we'll work through this issue.
- In this function: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/river/routing/RiversRouter.java#L113 I see various logging lines as Elastic looks for the `_meta` documents.  I can't find any of the `_meta document found.` or the `failed to get/parse _meta for` lines in my log file even though it is set to DEBUG.  I would assume no matter what, when a master node is restarted for a cluster that one of those lines would be logged.
- I keep seeing lines like this in my log:

[2014-01-21 19:27:59,091][DEBUG][river.cluster            ] [Isaac] processing [reroute_rivers_node_changed]: execute
[2014-01-21 19:27:59,097][DEBUG][river.cluster            ] [Isaac] processing [reroute_rivers_node_changed]: no change in cluster_state
- The missing log lines plus the "no change in cluster_state" make me think the `updateRiverClusterState` function is never getting fired.  Am I way off on that?
</comment><comment author="jprante" created="2014-01-21T21:47:28Z" id="32967238">@ryan1234 is it possible you can try JDBC river 1.0.0.RC1.2 if the problem still persists? I just tested a simple MySQL river with two nodes in all combinations with 1.0.0.RC1.2 but I have no problems with starting rivers.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/cluster/RiverClusterService.java</file><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file></files><comments><comment>Schedule retry if the river type is available but the _meta document isn't</comment></comments></commit></commits></item><item><title>Add /_cat/help endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4088</link><project id="" key="" /><description>Would be great to have a unixy usage output, eg:

```
GET /_cat/help  # or GET /_cat
```

and, if useful:

```
GET /_cat/help/recovery
```
</description><key id="22124874">4088</key><summary>Add /_cat/help endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-11-05T14:56:04Z</created><updated>2013-12-12T16:16:44Z</updated><resolved>2013-12-12T16:16:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-11-05T16:05:50Z" id="27785601">+1, great idea
</comment><comment author="kimchy" created="2013-11-05T17:42:33Z" id="27795019">my thoughts talking to @drewr was to add a flag to each cat command, and each cat command will associate with the headers it registers, a description, so we can automate the whole "help" process on each one.
</comment><comment author="drewr" created="2013-12-12T16:16:44Z" id="30436400">Taken care of in 7a348cd2, 28eff2ba, aec2b954, 53be1fe9.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>.deb installation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4087</link><project id="" key="" /><description>Installation via apt would be so much easier, just like for Mongo
(see : http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/#install-mongodb)

Do you plan to offer this way to install Elasticsearch ?
</description><key id="22117269">4087</key><summary>.deb installation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lucasmichot</reporter><labels /><created>2013-11-05T12:45:09Z</created><updated>2013-11-05T12:52:48Z</updated><resolved>2013-11-05T12:52:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-05T12:52:48Z" id="27769879">We are looking into it, there are already a couple of issues around the same problem: #3286 and #2757 . Closing this one as duplicate.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed two problems with JAVA_OPTS handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4086</link><project id="" key="" /><description /><key id="22115770">4086</key><summary>Fixed two problems with JAVA_OPTS handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">salyh</reporter><labels /><created>2013-11-05T12:14:11Z</created><updated>2014-07-01T06:28:58Z</updated><resolved>2013-11-08T13:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-11-08T11:09:16Z" id="28055608">See http://commons.apache.org/proper/commons-daemon/procrun.html specifically ++JvmOptions:
"The options are separated using either # or ; characters."
What issues do you encounter?
</comment><comment author="costin" created="2013-11-08T11:20:34Z" id="28056120">By the way, can you explain what did you try to fix - I'm having a hard time following the patch and understanding what the issue was? The use of parenthesis again (line 173)?
</comment><comment author="salyh" created="2013-11-08T11:34:28Z" id="28056696">Sorry for the cumbersome patch, here are more detailed explanations:

First: `JAVA_OPTS=%JAVA_OPTS: =%` can lead to a situation that java opts like `-Dmyprop=123 -Dsecondprop=456 -XX:+UseParNewGC` results in  `-Dmyprop=123-Dsecondprop=456-XX:+UseParNewGC` which is not a valid command line (L130)

Second: I have really troubles with the parantheses (L177) in that way that

&lt;pre&gt;&lt;code&gt;
if not "%ES_JAVA_OPTS%" == "" (
 set JVM_ES_JAVA_OPTS=%ES_JAVA_OPTS: =#%
 set JVM_OPTS=%JVM_OPTS%;%JVM_ES_JAVA_OPTS%
 )
&lt;/code&gt;&lt;/pre&gt;

behaves like 

&lt;pre&gt;&lt;code&gt;
if not "%ES_JAVA_OPTS%" == "" (
 set JVM_ES_JAVA_OPTS=%ES_JAVA_OPTS: =#%
 rem the second statement is not executed, so JVM_OPTS=%JVM_OPTS%;%JVM_ES_JAVA_OPTS% will never happen, which means that ES_JAVA_OPTS values are ignored
 )
&lt;/code&gt;&lt;/pre&gt;

I can submit a testcase which reproduce the problems, if neccessary
</comment><comment author="costin" created="2013-11-08T13:36:50Z" id="28062782">Pushed a fix for both issues in 0.90 and master. Let me know if it fixes your issues.

Cheers!
</comment><comment author="salyh" created="2013-11-14T12:30:23Z" id="28480063">issue fixed, thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fix issues with JAVA_OPTS/ES_JAVA_OPTS in service.bat</comment></comments></commit></commits></item><item><title>Auto-close polygons in geoshapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4085</link><project id="" key="" /><description>Although polygons are required to have the same start and end points, apparently most of the geo implementations will auto-close polygons.

Should we do the same?  Perhaps it needs a setting to allow strict mode?
</description><key id="22106640">4085</key><summary>Auto-close polygons in geoshapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label></labels><created>2013-11-05T08:58:35Z</created><updated>2015-06-06T17:08:06Z</updated><resolved>2014-07-04T13:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-11-21T04:12:04Z" id="28956833">Hi @clintongormley, if you mean to add the last point (clone of the first) automatically if it's missed to set. I'd say we shouldn't do it without defining it through the mapping, since we extend the GeoJSON specification. But thinking of extending the mapping by something like an option `autoclose` this would be neat.

```
"area": {
    "type": "geo_shape",
    "autoclose": true
}
```

On the other hand we should go a little further and extend it by a `validation` option (default `true` to keep thinks like they are now), which allows to index arbitrary polygons like self-intersection ones.
</comment><comment author="s1monw" created="2013-11-21T08:50:28Z" id="28966459">I think autoclose should default to true no?
</comment><comment author="s1monw" created="2014-03-12T20:03:41Z" id="37457099">is there any progress on this?
</comment><comment author="kevinkluge" created="2014-05-05T22:44:49Z" id="42249721">I'm moving to 2.0 based on the comment on #5478 that this potentially breaks backwards compat.
</comment><comment author="colings86" created="2014-07-04T13:37:19Z" id="48044481">It looks like the polygons already get auto-closed as searching for a polygon which is entirely within an indexed "open" polygon returns a hit which indicates that the polygon was auto-closed on indexing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change GeoBBox to be consistent with other implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4084</link><project id="" key="" /><description>Apparently in the rest of the geo world, people expect to specify `bottom-left` and `top-right`, instead of `top-left` and `bottom-right`.

Should change to be consistent (while still supporting the old params for bwc)
</description><key id="22106570">4084</key><summary>Change GeoBBox to be consistent with other implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-11-05T08:56:19Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-01-11T13:02:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-12-19T06:45:48Z" id="30908565">I'm going to change the names to `northwest` and `southeast`. I guess we should also setup `northeast` and `southwest`. Do you think we should also keep the old naming?
</comment><comment author="chilling" created="2014-01-11T13:02:04Z" id="32095442">close by #4580 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/EnvelopeBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetParser.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Geo clean Up</comment><comment>============</comment><comment>The default unit for measuring distances is *MILES* in most cases. This commit moves ES</comment><comment>over to the *International System of Units* and make it work on a default which relates</comment><comment>to *METERS* . Also the current structures of the `GeoBoundingBox Filter` changed in</comment><comment>order to define the *Bounding* by setting abitrary corners.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/EnvelopeBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetParser.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Geo clean Up</comment><comment>============</comment><comment>The default unit for measuring distances is *MILES* in most cases. This commit moves ES</comment><comment>over to the *International System of Units* and make it work on a default which relates</comment><comment>to *METERS* . Also the current structures of the `GeoBoundingBox Filter` changed in</comment><comment>order to define the *Bounding* by setting abitrary corners.</comment></comments></commit></commits></item><item><title>Make 'length' parameters consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4083</link><project id="" key="" /><description>- `flt`, `flt_field`, `fuzzy`, and `match` queries use `prefix_length`
- `query_string` query uses `fuzzy_prefix_length`
- `completion` suggested uses `min_length` and `prefix_length`

But
- `mlt_field_query` and `mlt_query` use `min_word_len` and `max_word_len`
- `completion` suggester uses `max_input_len`
- `phrase` and `term` suggesters use `min_word_len` and `prefix_len`

Change all length parameters to ...`_length`
</description><key id="22075079">4083</key><summary>Make 'length' parameters consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-11-04T19:53:27Z</created><updated>2014-01-13T16:00:50Z</updated><resolved>2014-01-13T16:00:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/common/ParseField.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/rest/action/mlt/RestMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/search/suggest/SuggestUtils.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java</file><file>src/main/java/org/elasticsearch/search/suggest/term/TermSuggestionBuilder.java</file><file>src/test/java/org/elasticsearch/index/mapper/completion/CompletionFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Standardized use of “*_length” for parameter names rather than “*_len”.</comment><comment>Java Builder apis drop old “len” methods in favour of new “length”</comment><comment>Rest APIs support both old “len: and new “length” forms using new ParseField class to a) provide compiler-checked consistency between Builder and Parser classes and</comment><comment>b) a common means of handling deprecated syntax in the DSL.</comment><comment>Documentation and rest specs only document the new “*length” forms</comment><comment>Closes #4083</comment></comments></commit></commits></item><item><title>Rename fuzziness/min_similarity to edit_distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4082</link><project id="" key="" /><description>Currently we have:
- `flt` | `flt_field` have `min_similarity`
- `fuzzy` has `min_similarity`
- `query_string` has `fuzzy_min_sim`
- `match` has `fuzziness`
- the completion suggester uses `{ fuzzy: { edit_distance: 2}}`

Fuzziness in Elasticsearch refers to edit-distance, which can be set to 0,1 or 2.  

`min_similarity` accepts a float value between 0 and 1, but now gets converted to an edit distance based on word length. eg a word of two characters with an edit distance of 2 would match any other word of length 2.

 I would suggest renaming `fuzziness` and `min_similarity` to `edit_distance` everywhere. It should accept 0,1,2 and `auto`, which sets the edit_distance to 1 for words of 1..3 characters, and 2 for words of 4 characters or more.

The only fly in the ointment is the `fuzzy` query which also handles fuzzy numbers and dates, which have nothing to do with edit distance.  See proposed deprecation in #4076 
</description><key id="22073324">4082</key><summary>Rename fuzziness/min_similarity to edit_distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-11-04T19:27:12Z</created><updated>2014-01-28T21:17:05Z</updated><resolved>2014-01-09T14:24:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-02T16:12:41Z" id="31462544">I opened a pull request for this that is split into 2 commits. One commit adds the generalization in terms of naming but handles all the old naming gracefully. The other commit breaks BW compat and updates docs etc. I want to pull one of the commits into 0.90 for easier transition. I also tried to keep most of the defaults in this issue to not do N things at once. @clintongormley can you give it a review?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/elasticsearch/common/ParseField.java</file><file>src/main/java/org/elasticsearch/common/unit/Fuzziness.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java</file><file>src/test/java/org/elasticsearch/common/ParseFieldTests.java</file><file>src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Rename edit_distance/min_similarity to fuzziness</comment></comments></commit></commits></item><item><title>Field resolution should be unambiguous</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4081</link><project id="" key="" /><description>As far as I understand it, fields are resolved on a _first found_ basis. So given the following documents:

```
PUT /index/foo/1
{
    "count": 1,
    "foo": {
        "count": 1
    }
}

PUT /index/bar/2
{
    "count": 1,
    "foo": {
        "count": 1
    }
}
```

.... the field `foo.count` could resolve to `foo.count`, `foo.foo.count`, or `bar.foo.count`, depending on which is found first.

Field resolution should be unambiguous. Field names should be grouped by type and sorted in descending order by number of `.`. So the above mappings should result in:

```
bar:
    foo.count
    count

foo:
   foo.count
   count
```

Then if no type is specified (or multiple types are specified), go through the groups in alphabetical order.

This would result in the following resolutions:

```
foo.foo.count   =&gt; (foo) foo.count
foo.count       =&gt; (foo) count
count           =&gt; (bar) count
*.foo.count     =&gt; (bar) foo.count
*.count         =&gt; (bar) count
*.*.count       =&gt; (bar) foo.count
```
</description><key id="22070100">4081</key><summary>Field resolution should be unambiguous</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label></labels><created>2013-11-04T18:36:01Z</created><updated>2015-06-06T17:08:15Z</updated><resolved>2014-12-10T13:54:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-29T08:40:20Z" id="29502584">I'd even be tempted to change the format of fieldnames which include the type, to make the type unambiguous, eg:

```
foo:foo.count
foo:count
```

This particular syntax would however interfere with fieldname parsing in the query-string query.  To use a colon as a type/field name separator in the query string query it would have to be escaped:

```
foo\:foo.count:10
```

I doubt that type names are often used in the qs syntax
</comment><comment author="dadoonet" created="2013-11-29T10:00:25Z" id="29506260">I'm +1 for this proposal. It really helps to clarify type vs field names.
I'm wondering if solving this will also fix #3005 (side effect)?
</comment><comment author="benjismith" created="2014-02-13T16:27:55Z" id="34995970">I'd love to see this in the next ES release. We're making heavy use of ES at my company, and this is the only remaining issue causing us significant headaches.

We frequently have different fields in different types with the same names. In particular, we often have "id" fields modeled as integers or strings. And because those IDs are used throughout our architecture (relational databases, protobufs, etc), making changes to either field names or field types creates a lot of friction in our codebase.

Please consider storing fully-qualified field names for all top-level and nested types ("user.id" is a different field from "item.id" or "user.address.id"). The extra few bytes of storage a small price to pay for eliminating ambiguity in mapping and faceting.

For anyone depending on the current behavior (multiple logically-distinct fields consolidated into a single lucene field), it might be possible to provide an option (similar to the [copy_to](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#copy-to) param) that lets them continue consolidating field data even when ES default behavior is to use fully-qualified field names.

Or maybe the [Create Index API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-create-index.html) could provide an option for choosing between a new "unambiguous fully-qualified-field-name indexing" vs the current behavior.

Just a thought :)
</comment><comment author="benjismith" created="2014-02-13T17:06:29Z" id="35000538">@clintongormley Is there any reason you prefer "foo:bar.baz" instead of just "foo.bar.baz" for fully-qualified field names? I'd like to see the dot notation used throughout the mappings. I'd even go so far as to say "index.type.nested_type.field" would be perfect.
</comment><comment author="karmi" created="2014-04-09T19:32:33Z" id="40006616">@clintongormley I guess I would lean to the full dot-based notation as well, ie. `&lt;INDEX&gt;.&lt;TYPE&gt;.foo.bar.baz`, not the the special `:` operator...
</comment><comment author="clintongormley" created="2014-04-15T12:46:18Z" id="40476748">I'm not sure that we need the index name in there as well... querying multiple indices but then in a subclause requiring a match on a field in just a particular index is a seldom used edge case, which could be accomplished by just filtering on that index.

And re `type:foo.bar` vs `type.foo.bar`, no I have no particular reason to favour the colon version over just dots.

@benjismith note: there's no bwc issue with making field resolution deterministic.  Everything will work just as it does today. The only difference would be that fields can be referred to unambiguously, which isn't the case at the moment.
</comment><comment author="benjismith" created="2014-04-15T20:21:30Z" id="40529281">@clintongormley Regarding bwc, I'm not sure I understand what you mean when you say "everything will work just as it does today".

I was hoping this change would fix https://github.com/elasticsearch/elasticsearch/issues/2603. Is that not the case? In particular, I'm interested in this comment made by @kimchy ...

&gt; Same field name under different types should have the same mappings, otherwise
&gt; there will be problems. Internally, in order to conserve space, we map them to the
&gt; same field name. If you really feel you need to separate between them, use different
&gt; indices. We should give better failure message when we detect conflicts in mappings.

After this change, will it still be problematic to have two different fields with the same name and different mappings (either in different types or at different levels of nesting within the same type) in the same index?
</comment><comment author="clintongormley" created="2014-04-17T10:46:48Z" id="40702008">@benjismith Correct - this doesn't fix #2603.

&gt; After this change, will it still be problematic to have two different fields with the same name and different mappings (either in different types or at different levels of nesting within the same type) in the same index?

Yes, correct. But now that I understand what you were thinking, it's an interesting idea.  I've opened issue #5851 to discuss it.
</comment><comment author="clintongormley" created="2014-07-04T07:42:30Z" id="48016685">To make sure that path resolution is unambiguous, we should require full path names and never resolve just the short path name eg:

```
{ "name": { "first": "john"}}
```

would require searching on `name.first`. `first` would no longer work, hence bumping to 2.0
</comment><comment author="clintongormley" created="2014-07-08T19:39:17Z" id="48389710">Also see #5851
</comment><comment author="clintongormley" created="2014-07-18T08:59:53Z" id="49409701">Also, mapping two fields with the same name differently in two types should throw an error.
</comment><comment author="clintongormley" created="2014-07-23T14:09:27Z" id="49878430">Possibly we can also remove the need for the `path` argument in nested queries?
</comment><comment author="OlegYch" created="2014-08-20T13:52:10Z" id="52780523">what's the current recommended strategy of mitigating this?
</comment><comment author="OlegYch" created="2014-08-20T13:54:40Z" id="52780882">does this currently (as of 1.3.1/1.3.2) affect fields with same name or only fields with exactly the same path?
</comment><comment author="OlegYch" created="2014-08-21T11:47:01Z" id="52909259">@clintongormley any idea?
</comment><comment author="clintongormley" created="2014-08-22T13:50:07Z" id="53062319">@OlegYch 

&gt; what's the current recommended strategy of mitigating this?

Make sure that fields with the same path have the same mapping, and refer to fields by their full paths, not just by name, optionally prepending the `_type` name if necessary to further disambiguate.
</comment><comment author="OlegYch" created="2014-08-22T14:22:01Z" id="53066085">thanks, prepending _type name works nicely for me
</comment><comment author="OlegYch" created="2014-08-22T16:54:27Z" id="53088747">except it seems to break my percolators in some cases..
i.e. the same percolator works without prepending type while stops matching the document when adding it, the percolator query finds the document fine in either case...
</comment><comment author="OlegYch" created="2014-08-22T17:01:47Z" id="53089683">the percolators are broken because document is saved after the percolator, not sure why it worked with unprefixed fields
</comment><comment author="clintongormley" created="2014-08-22T17:27:33Z" id="53092897">@OlegYch yes - see https://github.com/elasticsearch/elasticsearch/issues/6664
</comment><comment author="coderplay" created="2014-12-04T20:15:28Z" id="65695719">@clintongormley 

Does it solve the problem when two different types have the same parent ?  I meant say if we have type C1, C1 with their parent name P, both C1 and C2 have internal `_parent` field. Thus caused a same field name in different types.
</comment><comment author="clintongormley" created="2014-12-09T12:09:31Z" id="66273366">@coderplay The parent field will still be configurable per type

&gt; Does it solve the problem when two different types have the same parent

What problem are you referring to?  It is perfectly acceptable for two types to have the same parent.
</comment><comment author="clintongormley" created="2014-12-10T13:54:02Z" id="66453948">Closed in favour of #8870
</comment><comment author="coderplay" created="2014-12-10T17:57:04Z" id="66493814">@clintongormley 

Because that all children types share the same "_parent" index name,  you will be needed to load all children types' "_parent" field data when you just try to fetch something from one of the types through the "_parent" field data. If a parent has hundreds to thousands children, that field data will be huge. right?
</comment><comment author="clintongormley" created="2014-12-11T13:34:50Z" id="66619139">@coderplay that's exactly how it works today
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes #4047 - Empty objects are not stored in _source when an include/ex...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4080</link><project id="" key="" /><description>...clude list is present

Closes #4047 

This was originally being tracked by pull request #4048.  I closed that pull request because it was sloppy.

I am uncertain about the behavior for handling includes/excludes with empty objects and wildcards, so I added tests to cover the following cases, but please let me know if you think the results are incorrect...

Given

```
{
  "object1": { },
  "object2": { }
}
```

And a mapping

```
{
  ...
  {
    "_source" : {
      "excludes": "object1.*,
      "includes": "object2.*"
    }
  }
}
```

Results in a filtered source of

```
{
  "object1": { },
}
```

However, if given

```
{
  "object1": {
    "key1": "value1"
   },
  "object2": {
    "key2": "value2"
  }
}
```

and the same mapping, the filtered source would be

```
{
  "object2": {
    "key2": "value2"
  }
}
```
</description><key id="22067819">4080</key><summary>Fixes #4047 - Empty objects are not stored in _source when an include/ex...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">RobCherry</reporter><labels /><created>2013-11-04T17:57:53Z</created><updated>2014-06-16T08:14:28Z</updated><resolved>2014-01-31T09:37:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-11T10:34:14Z" id="28188644">Hi,

Once again - thx for looking into this. I don't think we should make a distinction between having an `obj.*` and `obj.f` as excludes (or includes for that matter) where `f` is the only field in that object. Also, for excludes, we always make to remove an object that became empty after applying filters to it's content. With includes, we do not return objects if they have no matching properties.

 So with this object, I think this is what it should do:

```
{
   "object1": { "f1": 1},
   "object2": { "f2": 2},
   "object3": { }
}
```

Then `exclude=["object1.*"]`  should return  

```
{
   "object2": { "f2": 2},
   "object3": { }
}
```

Also, 
`include=["object3.*"]` should return  (because the filters found nothing):

```
{}
```

while `include=["object3"]` returns

```
{ "object3": {} }
```

and for your example, with the following object:

```
{
  "object1": { },
  "object2": { }
}
```

A mapping with  `{ "excludes": "object1.*,  "includes": "object2.*" }` should result in:

```
{}
```

Makes sense?
</comment><comment author="RobCherry" created="2013-11-21T19:25:50Z" id="29015123">@bleskes I believe this will take care of the use case you outlined.
</comment><comment author="bleskes" created="2013-11-22T09:36:38Z" id="29060108">Thanks @RobCherry , will look at it asap.
</comment><comment author="bleskes" created="2013-11-25T15:07:37Z" id="29208448">Hi,

Left some comments. Thanks again for looking into this, I know it can be very tricky to get it right. Also the extra tests are a great addition.

Cheers,
Boaz
</comment><comment author="bleskes" created="2013-12-02T15:42:40Z" id="29627811">Hi Rob,

Sorry for not looking at this. I was travelling the whole of last week. Back now and will pick it asap.
</comment><comment author="RobCherry" created="2013-12-02T19:01:53Z" id="29646719">Great.  Let me know what you think about https://github.com/elasticsearch/elasticsearch/pull/4080#discussion_r7893231
</comment><comment author="bleskes" created="2013-12-04T10:36:42Z" id="29794529">Hi Rob,

I thought about this some more and I do think you have a point. However, that will be a bigger change and it will break backwards compatibility. I think suggest we do this in small steps - first solve the bug you've found and then open another issue to discuss the general behavior and wether we want to change it (with the next major version which is 1.0). 

If you agree, here is my idea for solving the bug: let's change the internal filter function ( https://github.com/elasticsearch/elasticsearch/pull/4080/files#diff-85c60dcb441d2ac9b132d73b9b6d2bbbR143 ) to return a bool indicating whether it actually filtered something away. Then we can use the old code, but only skip empty objects if the recursive call actually filtered something.  Note that this can happen if some elements did not match an include rule or if one element matched an exclude rule.
</comment><comment author="RobCherry" created="2013-12-17T17:08:53Z" id="30769699">I have opened https://github.com/elasticsearch/elasticsearch/issues/4491.
</comment><comment author="bleskes" created="2013-12-18T08:34:45Z" id="30824648">Hi Rob,

Do you still want on fixing this issue, while we resolve the discussion about the more general approach?
</comment><comment author="RobCherry" created="2013-12-18T17:46:53Z" id="30863541">I don't have time to work on this right now, it may be a few weeks before I can circle back.  If it is still open when I get time to work on it I would be happy to make the recommended changes.
</comment><comment author="bleskes" created="2014-01-31T09:37:27Z" id="33772420">closing this one as well, as the issue is resolved: https://github.com/elasticsearch/elasticsearch/issues/4047
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Warmers: Dedicated Norms/Terms warm options in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4079</link><project id="" key="" /><description>Similar to the loading part in `fielddata`, where someone can configure if they are loaded eagerly or not, we should have the same option in mappings for `norms`, and for `terms`.

Idea regarding naming, within a field:

```
{
   "terms" : {"loading" : "lazy|eager"},
    "norms" : {"enabled" : false|true, "loading" : "lazy|eager"}
}
```

Note, the above will deprecate the `omit_norms` field mapping, and follows similar structure to the `fileddata` mapping config.
</description><key id="22066209">4079</key><summary>Warmers: Dedicated Norms/Terms warm options in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-11-04T17:31:25Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2014-01-06T08:54:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-13T11:25:08Z" id="28387269">moved over to `0.90.8`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/FieldDataType.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/indices/warmer/IndicesWarmer.java</file><file>src/main/java/org/elasticsearch/indices/warmer/InternalIndicesWarmer.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/CustomBoostMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/FieldLevelBoostTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Eager norms loading options.</comment></comments></commit></commits></item><item><title>Remove Index Reader warmer introduced in 0.90.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4078</link><project id="" key="" /><description>Remove the automatic index warmer introduced in 0.90.6 release, since it create the following challenges:
- it automatically load all the norms for all fields. This should be an opt in feature similar to the new `loading` feature in field data. Will open a separate issue for it.
- It automatically loads all doc values for all fields (if they have it), overriding effectively the `loading` option of field data when its backed by doc values.
</description><key id="22065319">4078</key><summary>Remove Index Reader warmer introduced in 0.90.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>regression</label><label>v0.90.7</label></labels><created>2013-11-04T17:17:32Z</created><updated>2013-11-13T05:38:07Z</updated><resolved>2013-11-04T17:32:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Remove Index Reader warmer introduced in 0.90.6</comment><comment>It create the following challenges:</comment><comment>- it automatically load all the norms for all fields. This should be an opt in feature similar to the new loading feature in field data. Will open a separate issue for it.</comment><comment>- It automatically loads all doc values for all fields (if they have it), overriding effectively the loading option of field data when its backed by doc values.</comment></comments></commit></commits></item><item><title>Added documentation for the keep word token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4077</link><project id="" key="" /><description /><key id="22063088">4077</key><summary>Added documentation for the keep word token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-11-04T16:43:26Z</created><updated>2014-07-16T21:51:36Z</updated><resolved>2013-11-04T17:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-04T17:40:05Z" id="27705550">committed: https://github.com/elasticsearch/elasticsearch/commit/a9fdcadf01cb6a3c74a3c94ca54fdac4e3ed2872
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate fuzzy numeric/dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4076</link><project id="" key="" /><description>The fuzzy query supports "fuzzy" queries on numeric and date fields, which just get translated into range queries.

The support is only there as a hack to allow using the fuzzy "tilde" syntax on numbers and dates in the query string query.

I don't know of anybody who uses this (if you DO use it, please comment below!), it's a hack and the same functionality can be achieve with ranges anyway.

I'm for removing this
</description><key id="22062356">4076</key><summary>Deprecate fuzzy numeric/dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-11-04T16:32:51Z</created><updated>2014-01-09T13:45:29Z</updated><resolved>2014-01-09T13:45:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-04T16:34:57Z" id="27699641">I actually like this aspect in the query string, and use it quite a lot when in "query string" mode. I think people use it less since we don't talk about it enough (similar to the improved range syntax we now support as well).
</comment><comment author="clintongormley" created="2013-11-04T17:22:20Z" id="27703922">I don't like it because it is obscure - what does ~0.5 mean?  you have to remember whether you've set `fuzzy_factor` on a field, and then do a calculation in your head to figure out whether it's the right fuzzy value for the range you want.

Personally I think it's easier to use explicit ranges, but maybe that's just me.
</comment><comment author="clintongormley" created="2013-11-04T19:27:55Z" id="27714709">It also interferes with renaming `min_similarity` to `edit_distance`. See #4082
</comment><comment author="kimchy" created="2013-11-04T19:32:26Z" id="27715140">@clintongormley fuzzy factor is no longer relevant, and actually has been removed!, fuzzy in query string now supports a "string", so you can do "now~1d" for example for days...
</comment><comment author="clintongormley" created="2013-11-04T19:34:08Z" id="27715301">ok cool - i'm fine with supporting it in the query string query, but i think it just muddies the water of the fuzzy query, which should be cleaned up.
</comment><comment author="clintongormley" created="2013-12-17T16:56:58Z" id="30768595">To clarify, it would be good to keep the "fuzzy syntax" for dates and numbers in the query_string query syntax, without exposing that functionality as part of the `fuzzy` query.  In other words the query_string code should decide what type of query should be run based on the field type, not leave it to the `fuzzy` query code.
</comment><comment author="s1monw" created="2014-01-08T11:25:59Z" id="31823794">I think we can actually support string values for `edit_distance` as well like we do in the `query_string` but I think if we do this we shouldn't name is `edit_distance`. Something like `fuzziness` might be better here since it can be an edit distance or a string like `1day`. What do you think @clintongormley 
</comment><comment author="clintongormley" created="2014-01-09T13:45:12Z" id="31931848">Decided against deprecating `fuzzy` on numeric and date fields. Just improved documentation and interface with @s1monw 's issue: #4082 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate the `more_like_this` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4075</link><project id="" key="" /><description>Change the `more_like_this` query to accept an `index`, `type` and `id` parameter and use that to retrieve the doc to be searched.  This way we can deprecate the `more_like_this` API.
</description><key id="22060531">4075</key><summary>Deprecate the `more_like_this` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:More Like This</label></labels><created>2013-11-04T16:05:55Z</created><updated>2015-07-06T14:55:38Z</updated><resolved>2015-07-06T14:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-11-04T16:13:01Z" id="27697579">+1
</comment><comment author="spinscale" created="2014-01-07T12:32:54Z" id="31733690">For clarification: Deprecation only, no removal done (will be done in the next major version)

Support for `index`, `type`, `id`, `routing` needs to be added in the MLT query
</comment><comment author="spinscale" created="2014-01-08T13:47:29Z" id="31831839">Decided to postpone this issue after 1.0 and leave the functionality as it is for now, as it needs further thoughts to stay fast.

If you move this functionality into the query parser, the `GetRequest` for the index/type/id lookup is executed on every shard involved and will most likely need to fetch data from a remote node resulting in a couple of roundtrips, which gets worse with every shard involved.
</comment><comment author="clintongormley" created="2014-07-03T19:58:31Z" id="47977431">Depends on #6719 
</comment><comment author="alexksikes" created="2015-07-06T14:55:11Z" id="118879819">Closed by https://github.com/elastic/elasticsearch/pull/11003 and related to https://github.com/elastic/elasticsearch/issues/10217
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/morelikethis/MoreLikeThisFetchService.java</file><file>src/main/java/org/elasticsearch/search/SearchModule.java</file><file>src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java</file><file>src/test/java/org/elasticsearch/index/query/ItemSerializationTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>More Like This Query: Added searching for multiple items.</comment></comments></commit></commits></item><item><title>Make search APIs consistently accept a query param</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4074</link><project id="" key="" /><description>The `search` and `explain` APIs both accept a `query` parameter in the body, while `count` and `validate_query` expect the body to be the query itself.

We should change the `count` and `validate_query` APIs to accept a `query` parameter.  Possibly even change the `count` request to use the same code as for `_search` (with `search_type=count`).
</description><key id="22060379">4074</key><summary>Make search APIs consistently accept a query param</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-11-04T16:03:43Z</created><updated>2014-01-02T09:06:29Z</updated><resolved>2014-01-02T09:06:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-04T18:48:33Z" id="27711307">I'd consider removing the current count api and make it a shortcut to a search request with `search_type=count`, makes sense!
</comment><comment author="martijnvg" created="2013-12-16T16:21:36Z" id="30674175">I think also the `delete_by_query` api needs to accept the query in a `query` parameter instead of directly in the request body.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file></files><comments><comment>Fix possible BWC break after upgrading from pre 1.0.0</comment></comments></commit><commit><files /><comments><comment>[DOCS] fixed count docs, it now requires a top-level query object, same as other apis</comment></comments></commit><commit><files /><comments><comment>[DOCS] fixed count and validate query docs, they now require a top-level query object, same as other apis</comment></comments></commit><commit><files /><comments><comment>[TEST] Split delete by query tests pre-1.0 and post-1.0</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/count/ShardCountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/IndexDeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/ShardDeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/explain/ExplainRequest.java</file><file>src/main/java/org/elasticsearch/action/explain/ExplainRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/support/QuerySourceBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/test/java/org/elasticsearch/ElasticSearchExceptionTests.java</file><file>src/test/java/org/elasticsearch/broadcast/BroadcastActionsTests.java</file><file>src/test/java/org/elasticsearch/count/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Made APIs consistently accept a query in the request body's `query` field.</comment></comments></commit></commits></item><item><title>Configuring a keep words token filter to load words from file results in error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4073</link><project id="" key="" /><description>If one sets the `keep_words_path` of that filter, you'd always get an error:

```
keep requires either `keep_words` or `keep_words_path` to be configured
```

even if `keep_words` is not specified. 
</description><key id="22059148">4073</key><summary>Configuring a keep words token filter to load words from file results in error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-11-04T15:46:20Z</created><updated>2013-11-04T15:50:53Z</updated><resolved>2013-11-04T15:50:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java</file><file>src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java</file></files><comments><comment>Keep word filter through an error if `keep_word_path` was specified.</comment></comments></commit></commits></item><item><title>Post filter phrase suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4072</link><project id="" key="" /><description>This implementation has a bunch of problems that'll need to be worked
before it is a valid candidate for merging.  I don't have time to rebase
it right now but would still love the feedback on problem.  The ones I
remember:
1.  It performs the filtering by blocking the suggesting thread.
2.  Because there is no "exists" query type it uses a limit.  I now know
   that isn't ass efficient as just using a count but it might be worth
   implementing an exists query type for it any way.
3.  It feels like there are a lot of plumbing changes required for this
   feature.  My guess is that is because I'm going about it wrong.  This
   correlates with #1 pretty well.
4.  I have to wrap the filter through the map nodes and parse it during
   the reduce step.  That feels silly.

Closes #3482
</description><key id="22051448">4072</key><summary>Post filter phrase suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-11-04T13:42:55Z</created><updated>2014-06-20T18:06:31Z</updated><resolved>2014-03-05T16:52:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brupm" created="2014-06-20T02:12:53Z" id="46638597">Hi nik9000, did you ever sold this problem?
</comment><comment author="nik9000" created="2014-06-20T18:06:31Z" id="46708635">I mostly solved it by splitting my index in half and only query which half I need to at query time.  So suggestions are only made on the half that you query.  You can still get suggestions for missing results but its much much much less likely.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make indices APIs consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4071</link><project id="" key="" /><description>An index accepts settings, mappings, warmers and aliases.  However, some use singular and some use plural, eg:

Mappings:

```
PUT /index/type/_mapping
{ type: { .... }}

GET /index/{type|*|blank}/_mapping
```

Settings:

```
PUT /index/_settings
GET /index/_settings
```

Warmers:

```
PUT /index/_warmer/name
{ ... }
GET /index/_warmer/{name|*|blank}
```

Aliases:

```
POST /_aliases
{ actions: [....] }

 PUT /index/_alias/alias
 { ... }

 GET /{index|*|_all}/_alias/{alias|*}
```

There are a couple of possibilities:

Distinguish between singular and plural when PUT/POSTing

Create/update a mapping, warmer or alias (not settings) with: 

```
PUT /index/_mapping|_alias|_warmer/{name}
{ ... }
```

This breaks bwc for mapping (`/index/type/_mapping` vs `/index/_mapping/type`) - possibly support both variants for all actions?

Create/update mappings, warmers, aliases, settings with:

```
POST /index/_mappings|_settings|_warmers|_aliases
{ name: {....}, name2: {....} ... }
```

Retrieve via singular or plural:

```
GET /index/_mapping|_alias|_warmer|_setting/{name|*|_all|blank}
GET /index/_mappings|_aliases|_warmers|_settings/{name|*|_all|blank}
```

Alternatively, make singular/plural synonyms, so either can be used in any of the above examples

One final thing, all indices can be specified as `_all` or `*` (depending on the API), but eg warmers and aliases only understand wildcards.  I think `_all` should be an acceptable alias for `*`for all of them
</description><key id="22051396">4071</key><summary>Make indices APIs consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-11-04T13:41:40Z</created><updated>2014-02-16T15:36:15Z</updated><resolved>2014-01-15T19:09:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-01-03T16:39:18Z" id="31534457">[...]

_I think _all should be an acceptable alias for \* for all of them_

I agree.

Just for completeness: we also currently have the `DELETE` for alias

```
  DELETE /{index}/_alias/name
```

and also for warmers:

```
 DELETE index/_warmer/name

 DELETE index/_warmer/prefix*

 DELETE index/_warmer/
```

For mappings, delete is:

```
DELETE index/type
```

For settings there is none and would probably not make sense to have that.
</comment><comment author="brwe" created="2014-01-03T16:39:31Z" id="31534483">So we have `PUT/POST`, `GET` and `DELETE` for all three we have several options. I list some here again. Please feel free to add your own option and vote.

I vote for 

`PUT/POST`: Option 1

`GET`: Option 2

`DELETE`: Option 1

## `PUT/POST`:

### Option 1:

Remove all singular `PUT`s, do not distinguish between `PUT` and `POST` and only allow putting multiple items for all 4 APIs like this:

```
   POST (or PUT) /index/_mappings|_settings|_warmers|_aliases
   { name: {....}, name2: {....} ... }
```

The `PUT` like

```
 PUT /index/_alias|_warmer/{name}
```

would be removed completely. This might cause some overhead when typing requests but at least you can remember the syntax even when you are braindead.

### Option 2:

Make the distinction singular/plural (`PUT/POST`) like @clintongormley wrote:

```
 PUT /index/_mapping|_alias|_warmer/{name}
 { ... }

 POST /index/_mappings|_settings|_warmers|_aliases
 { name: {....}, name2: {....} ... }
```

This would cause the syntax to be slightly more complex but is still sort of intuitive.

### Option 3:

Like option 2 but allow singlar and plural for `PUT` and `POST`. Would not break bwc.

## `GET`

### Option 1

allow plural and singular but only `GET /index/_api/{name|*|_all|blank}`

```
 GET /index/_mapping|_alias|_warmer|_setting/{name|*|_all|blank}
 GET /index/_mappings|_aliases|_warmers|_settings/{name|*|_all|blank}
```

### Option 2

allow only plural

```
GET /index/_mappings|_aliases|_warmers|_settings/{name|*|_all|blank}
```

### Option 3

allow plural and singular with `GET /index/_api/{name|*|_all|blank}` and `GET /index/{name|*|_all|blank}/_api`

```
 GET /index/_mapping|_alias|_warmer|_setting/{name|*|_all|blank}
 GET /index/_mappings|_aliases|_warmers|_settings/{name|*|_all|blank}

 GET /index/{name|*|_all|blank}/_mapping|_alias|_warmer|_setting
 GET /index/{name|*|_all|blank}/_mappings|_aliases|_warmers|_settings
```

Would not break bwc.

### Option 4

allow only plural

```
GET /index/_mappings|_aliases|_warmers|_settings/{name|*|_all|blank}
GET /index/{name|*|_all|blank}/_mappings|_aliases|_warmers|_settings
```

## `DELETE`

### Option 1

Make the `DELETE` available for 3 APIs like in `_warmer`:

```
 DELETE index/_mapping|_alias|_warmer/name

 DELETE index/_mapping|_alias|_warmer/prefix*

 DELETE index/_mapping|_alias|_warmer/
```

### Option 2:

Leave everything as is.
</comment><comment author="brwe" created="2014-01-07T11:29:35Z" id="31730378">After discussion here is what I will implement:

`PUT/POST`: Option 2. both plural and singular version should be supported for PUT and POST but only singular for `PUT` and plural for `POST` documented.

For `PUT` `_mapping` remove "type" from body, because it has to be defined in url anyway and is therefore redundant.

`GET`: Option 1 but also support two different orderings for `_mapping` in order to not break bwc. Do not document this. Make sure that for index `*` and `_all` is supported. Make sure that for type `*`, `_all` and blank is supported.
Return data structure should be the same for getting a single item or multiple ones. For example `_mapping`: Getting a single type and multiple types should be 

```
{
     "index": {
         "type" : {
                 .....
        }
    }
}
```

`DELETE`: Should be like `GET` but do not support blank as item name in order to remove all, only `_all` or `*` allowed.
</comment><comment author="electrical" created="2014-01-07T12:54:05Z" id="31734830">Just like to give my £0.02

few years ago i was designing REST API's and was following some webinars by a company who design api's ( http://www.youtube.com/apigee ) and following in a few nice discussions on the google groups ( https://groups.google.com/forum/#!forum/api-craft )

One of those items was the singular/plural discussion and at the end of it the 'pragmatic' approach would be to use only plural names.
Having singular and plural endpoints can become very confusing especially if its for same sort of endpoint ( GET /index/_mapping &amp; POST /index/_mappings for example )

An other thing is the ordering, by allowing different methods it can become confusing again.
Personally i like the GET /index/_mappings/name  because it makes it very clear what you expect.. ( GET me mapping 'name' from 'index' )

Cheers.
</comment><comment author="electrical" created="2014-01-07T13:02:10Z" id="31735240">Forgot to address the single or multiple items in an endpoint call.
Personally i wouldn't differentiate between a single or multiple items in a call but always name them like its done with multiple items right now.

```
POST /index/_mappings|_settings|_warmers|_aliases
{ name: {....}, name2: {....} ... }
```

this makes things consistent across the board and avoid having different endpoints for pretty much the same thing.
</comment><comment author="brwe" created="2014-01-07T16:03:29Z" id="31750150">In principle I agree to the plural only. However, since some APIs are already singular, it might be good have plural and singular as synonyms?

There is several issues we would like to fix:
1. plural and singular should be unified. 
2. index and item name definition should be treated the same way. That is, `*`, `_all` and `blank` should be synonyms, except for `DELETE` where we should not allow `blank`
3. Order of `index/_api/item_name` should be consistent.
4. `PUT` and `POST` should be equally supported by all four APIs.

Assuming that we agree on making plural and singular synonyms, this would require the following list of changes:

## `_mapping`

### `PUT`

```
PUT {index|_all|*|prefix*}/type/_mapping
{
    "type" : {
          "properties": {...}
    }
}
```

would become

```
PUT {index|_all|*|prefix*|blank}/_mapping/type
{
     "properties": {...}
}
```

and `_mappings` as synonym.

We would keep the old behavior but remove the documentation.

Note that currently there is also the option to write 

`PUT {index|_all|*|prefix*}/_mapping?type=typename`

although this is not documented.

### `POST`

Currently exactly the same as PUT.

We would add 

```
POST {index|_all|*|prefix*|blank}/_mappings
{
    "type1" :
         "properties": {...}
    },
    "type2" :
         "properties": {...}
    },
    ...
}
```

and `_mapping` as synonym.

### `GET`

`GET {index|_all|*|prefix*|blank}/{type|*|blank}/_mapping`

would become

`GET {index|_all|*|prefix*|blank}/_mappings/{type|_all|*|prefix*|blank}`

and `_mapping` as synonym.

We would keep the old behavior but remove the documentation.

### `DELETE`

`DELETE index/type`

would become:

`DELETE {index|_all|*|prefix*|blank}/_mapping/{type|_all|*|prefix*}`

We would add `_mappings` as synonym.

We would keep the old behavior but remove the documentation.

## `_settings`

### `PUT`

```
PUT {index|_all|*|prefix*|blank}/_settings {
    ...
}

```

would remain.

We would add `_setting` as synonym

### `POST`

Does not exist yet. 

We would add 

```
POST {index|_all|*|prefix*|blank}/_settings {
    ...
}

```

We would add `_setting` as synonym.

### `GET`

`GET {index|*|_all|pref*|blank}/_settings`

would remain as is.

We would add `_setting` as synonym.

### `DELETE`

Does not exist and we will not add it since it is unclear what it should do.

## `_warmer`

### `PUT`

```
PUT {index|_all|*|prefix*}/{type|_all|*|prefix*|blank}/_warmer/warmer_name {
    ...
}
```

would become:

```
PUT {index|_all|*|prefix*|blank}/{type|_all|*|prefix*|blank}/_warmer/warmer_name {
    ...
}
```

We would add `_warmers` as synonym.

### `POST`

does not exist yet. We would add

```
POST {index|_all|*|prefix*|blank}/{type|_all|*|prefix*|blank}/_warmers {
    "warmer1":{...},
    "warmer2":{…},
    ...
}
```

We would add `_warmer` as synonym.

### `GET`

`GET {index|*|_all|pref*}/_warmer/{warmer_name|*|pref*}`

would become

`GET {index|*|_all|pref*|blank}/{type|_all|*|prefix*|blank}/_warmers/{warmer_name|*|_all|pref*|blank}`

We would add `_warmer` as synonym.

### `DELETE`

`DELETE {index|_all|*|prefix*}/_warmer/{warmer|*|prefix*}`

would become

`DELETE {index|_all|*|prefix*}/{type|_all|*|prefix*}/_warmers/{warmer|*|_all|prefix*}`

We would add `_warmer` as synonym.

## `_alias`

### `PUT`

`PUT index/_alias/alias`

would become

`PUT {index|_all|*|prefix*|blank}/_alias/alias`

We would add `_aliases` as synonym.

### `POST`

Remains as is. The alias API is a special case because post triggers different actions.

### `GET`

`GET {index|_all|*|prefix*|blank}/_alias/{alias|*|prefix*}`

and

`GET {index|_all|*|prefix*|blank}/_aliases`

would become

`GET {index|_all|*|prefix*|blank}/_aliases/{alias|*|prefix*|_all|blank}`

We would add `_alias` as synonym.

### `DELETE`

`DELETE index/_alias/alias`

would become 

`DELETE {index|_all|*|prefix*|blank}/_alias/{alias|*|prefix*|_all}`

We would add `_aliases` as synonym.

Note that `DELETE index/_alias/*` currently returns ok but does not actually remove the aliases.

## Things to keep in mind

Keep an eye on https://github.com/elasticsearch/elasticsearch/issues/4549
</comment><comment author="clintongormley" created="2014-01-09T10:45:34Z" id="31919497">Here's a proposed roadmap of what should be done for RC1, and what can follow later:

## PUT (singular)

```
Request                                 Status
-----------------------------------------------------------------------------------
PUT /{index}/_mapping/{type}            Add URL
{ single defn }

PUT /{index}/_warmer/{name}             Already supported
{ single defn }

PUT /{index}/_alias/{name}              Already supported
{ single defn }

PUT /{index}/_setting/{name}            Will not support
{ single defn }
```

Also support:

```
 PUT /{index}/{type}/_mapping           Already supported
 PUT &amp; POST                             Add URLs
```

Where: 
- `{index}` = `blank | * | _all | prefix* | name,name`
- `{name}`  = `name`

## POST (plural)

```
Request                                 Status
-----------------------------------------------------------------------------------
POST /{index}/_mappings                 Post 1.0
{ multi defn }

POST /{index}/_warmers                  Post 1.0
{ multi defn }

POST /_aliases                          Already supported
{ actions }

POST /{index}/_settings                 Already supported
{ multi defn }
```

Also support:

```
 PUT &amp; POST                             Add URLs
```

Where: 
- `{index}` = `blank | * | _all | prefix* | name,name` (Or should we just allow `name,name`? `prefix*`?)

Should we also support this? 

```
POST /{index}/_aliases
{ alias: { defn},... }
```

Probably not as we can't delete aliases at the same time.

## GET (singular or plural)

```
Request                                 Status
-----------------------------------------------------------------------------------
GET /{index}/_mappings/{type}           Add URL - make response consistent
GET /{index}/_mapping/{type}            Add URL - make response consistent

GET /{index}/_warmers/{name}            Add URL
GET /{index}/_warmer/{name}             Already supported

GET /{index}/_aliases/{name}            Already supported
GET /{index}/_alias/{name}              Already supported

GET /{index}/_settings/{name}           Already supported where {name}=blank
                                        Add {name} support post 1?
GET /{index}/_setting/{name}            Add URL? Or should not support?
```

Also support:

```
GET /{index}/{type}/_mapping            Already supported - make response consistent
```

 Where: 
- `{index}` = `blank | * | _all | prefix* | name,name`
- `{name}`  = `blank | * | _all | prefix* | name,name`

Missing types/warmers/aliases/settings should return empty objects, not throw 404s

## DELETE (singular or plural)

```
Request                                 Status
-----------------------------------------------------------------------------------
DELETE /{index}/_mappings/{type}        Add URL
DELETE /{index}/_mapping/{type}         Add URL

DELETE /{index}/_warmers/{name}         Add URL
DELETE /{index}/_warmer/{name}          Already supported

DELETE /{index}/_aliases/{name}         Already supported
DELETE /{index}/_alias/{name}           Already supported

DELETE /{index}/_settings/{name}        Will not be supported
DELETE /{index}/_setting/{name}         Will not be supported
```

Also support:

```
DELETE /{index}/{type}/_mapping         Already supported
```

 Where: 
- `{index}` = `* | _all | prefix* | name,name` BLANK NOT ALLOWED
- `{name}`  = `* | _all | prefix* | name,name` BLANK NOT ALLOWED

If no matching types/warmers/aliases/settings found, throw 404.
</comment><comment author="clintongormley" created="2014-01-10T17:01:41Z" id="32045177">The output from GET requests should have the following format:

```
{
    "index_name": {
        "element_name":  ...element value ...,
        ...
    },
    ....
}
```

In other words:
- the first level should always be the index name or names
- the second level is the name of each element, eg the name of the type for `_mappings`, the name of the warmer for `_warmers`etc
- if no matching element is found, then we return an empty object `{ }`, we don't throw a 404
- the response object is only populated with the elements we find.  

For example:

```
GET /index_1,index_2,alias_3/_mappings/foo
```

If `index_1` and `alias_3` (which points to `index_3`) both contain type `foo`, but `index_2` doesn't, then the response would be:

```
{
    "index_1": {
        "foo": { .... }
    },
    "index_3": {
        "foo": { .... }
    }
}
```
</comment><comment author="clintongormley" created="2014-01-15T19:09:53Z" id="32398112">RC1 work completed.  Remaining work moved to #4743 
</comment><comment author="lfrancke" created="2014-02-16T15:36:15Z" id="35199544">Two questions about this.

The first is about a comment @brwe left:

&gt; For PUT _mapping remove "type" from body, because it has to be defined in url anyway and is therefore redundant.

Was this implemented in the end? When I'm posting a mapping containing a type to a URL with a non-matching I see a message acknowledging it but I don't think anything actually happened.

Second question is about deprecation. The PUT Mapping API doc mentions this:

&gt; Instead of `_mapping` you can also use the plural `_mappings`. The uri `PUT /{index}/{type}/_mapping` is still supported for backwards compatibility.  

Are the singular versions supposed to be removed at some point?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>update rest spec to be consistent with recent changes</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java</file></files><comments><comment>Consistent APIs: Get field mapping API includes 'mappings'</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/common/Strings.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestXContentBuilder.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file></files><comments><comment>Consistent REST API changes for GETting data</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/common/util/CollectionUtils.java</file><file>src/main/java/org/elasticsearch/indices/TypeMissingException.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/delete/AliasesMissingException.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/delete/RestIndexDeleteAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/put/RestIndexPutAliasAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/delete/RestDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/delete/RestDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/search/warmer/IndexWarmerMissingException.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleDeleteMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/LocalGatewayIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/mlt/MoreLikeThisActionTests.java</file><file>src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationTests.java</file></files><comments><comment>Make PUT and DELETE consistent for _mapping, _alias and _warmer</comment></comments></commit></commits></item><item><title>Added tool to build a complete Elasticsearch release.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4070</link><project id="" key="" /><description>This tool builds a release and runs several checks to make sure the
release is in a reasonable shape (smoke test). From a top level
perspective it runs the following steps:
- clean the build environment `mvn clean`
- check if a Java 6 JDK is available
- run the tests with network and local
- generates the checksums for the binary packages
- uploads the binary packages to Amazon S3
- runs a 'mvn deploy' to publish the maven artifacts

The script will create an intermediate branch from a given 'release
branch' updates all versions based on the version we are currently
releasing. Updates the 'pom.xml' file as well as the 'Version.java'
class. Once this is done it commits the changes and rebase with the
branch we want to release from, merges the changes from the intermediate
branch and pushes to the given remote repository including the release
tag.
</description><key id="22049577">4070</key><summary>Added tool to build a complete Elasticsearch release.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-11-04T13:00:31Z</created><updated>2014-07-16T21:51:38Z</updated><resolved>2013-11-04T13:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-04T16:07:21Z" id="27696997">Woot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add GET /index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4069</link><project id="" key="" /><description>While we have the `index exists` API:

```
HEAD /index
```

and we have:

```
GET /index/_aliases
GET /index/_mapping
GET /index/_settings
GET /index/_warmer
```

We don't have a `GET /index` API which would return all info about the index (ie aliases, settings, mappings, warmers) which could be used to recreate the index with a different name or on a different cluster.
</description><key id="22046453">4069</key><summary>Add GET /index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>feature</label><label>v2.0.0-beta1</label></labels><created>2013-11-04T11:43:21Z</created><updated>2014-09-11T10:33:10Z</updated><resolved>2014-09-11T10:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-11-24T09:29:31Z" id="29151842">Hey,

implementation for this is not too complex (see my commit), however, do you think it makes sense, if warmers and aliases could also directly be added when  creating an index, so one could pipe this into a create index call?

This is the current output

```
» curl 'localhost:9200/test?pretty'
{
  "settings" : {
    "index.tests.seed" : "-429205130377286056",
    "index.compound_format" : "0.9941336967222938",
    "index.merge.policy.type" : "org.elasticsearch.index.merge.policy.LogDocMergePolicyProvider",
    "index.number_of_replicas" : "5",
    "index.number_of_shards" : "10",
    "index.version.created" : "1000002",
    "index.uuid" : "d70IIOiGQLiFdau4ZBVifw",
    "index.refresh_interval" : "10s"
  },
  "mappings" : {
    "test" : {
      "properties" : {
        "message" : {
          "type" : "string",
          "store" : true
        }
      }
    }
  },
  "aliases" : {
    "thetest" : { }
  },
  "warmers" : {
    "warmer_1" : {
      "types" : [ ],
      "source" : {
        "query" : {
          "match_all" : { }
        }
      }
    }
  }
}
```
</comment><comment author="dakrone" created="2013-12-06T23:06:24Z" id="30037575">@spinscale +1, I personally vote for keeping the warmers and aliases in, just to get all the information.
</comment><comment author="clintongormley" created="2013-12-17T15:22:31Z" id="30759382">@spinscale Nice - I didn't see this commit.  Warmers can already be added in the create index API, but you're right, aliases can't be.  I think it would be nice to support this too.

Also see #4140 

Bumping this issue to 1.0.1
</comment><comment author="clintongormley" created="2014-04-28T11:18:00Z" id="41547002">Also see #4614
</comment><comment author="clintongormley" created="2014-04-28T11:18:49Z" id="41547056">Copied comment over from #4614 

With the removal of _aliases (#4539) we've lost an endpoint for efficiently listing available indices &amp; aliases, which Kibana relies on for computing timestamped indices.

It would be nice to have an _indices endpoint that could efficiently return a list of indices with a configurable amount of additional granularity, eg mappings, settings, aliases. 

Bonus points if we could support wildcard matching here for both index and alias names, eg

*\* Every index name **

```
/_indices
```

*\* Every index with its aliases and settings **

```
/_indices?aliases
```

*\* Every index named mydata-\* or with an alias matching mydata-_, along with its mapping *_

```
/mydata-*/_indices?mapping
```
</comment><comment author="spinscale" created="2014-07-29T08:13:18Z" id="50447481">@clintongormley your proposal currently clashes with the `RestGetMappingAction` due to a mapping for `/{index}/_mapping/{type}`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/common/path/PathTrie.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/get/RestGetIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexTests.java</file><file>src/test/java/org/elasticsearch/common/path/PathTrieTests.java</file><file>src/test/java/org/elasticsearch/transport/ActionNamesTests.java</file></files><comments><comment>Indices API: Added GET Index API</comment></comments></commit></commits></item><item><title>Make SimpleNodeSampler populate the list of connected nodes using the information returned from the cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4068</link><project id="" key="" /><description>This is to allow people to introspect things like data settings and attributes. Also makes it consistent with the sniff sampler.

This was the result of confusion on the mailing list where someone tried to verified he is not connected to data nodes by looking at the `TransportClient.connectedNodes` information.
</description><key id="22045687">4068</key><summary>Make SimpleNodeSampler populate the list of connected nodes using the information returned from the cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-11-04T11:22:47Z</created><updated>2014-07-16T21:51:38Z</updated><resolved>2013-11-22T10:38:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-11T11:31:58Z" id="28191589">Updated the PR. SimpleNodeSampler now uses the address of the listedNodes in it's connected nodes. Not sure about @kimchy's comment not to use `connectToNodeLight` when connecting to the listed nodes. I think it's good? 
</comment><comment author="kimchy" created="2013-11-11T11:35:32Z" id="28191773">looks good to me, now that we copy over the discovery node, I think it makes sense to keep the connect to node light, and try and keep that connection open to the listed node address.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Run elasticsearch http server under a configurable context-path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4067</link><project id="" key="" /><description>Hi,

Currently the context path of the HTTP/REST interfaces seems to be hard coded to / (root) and is not possible to configure. I would like to be able to use one single apache reverse proxy in front of many elastic-search nodes which multiplexes depending on the context path:
http://apache/es1/ should go to http://internal01:9200/es1/
http://apache/es2/ should go to http://internal01:9201/es2/
http://apache/es3/ should go to http://internal02:9200/es3/
...
What I need for this to work is a way to tell the elasticsearch node that it should use a context-path (e.g. es1): generated links should use this context-path and it should be ignored when testing paths.

Cheers,
Dimo
</description><key id="22045565">4067</key><summary>Run elasticsearch http server under a configurable context-path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dimovelev</reporter><labels><label>feedback_needed</label></labels><created>2013-11-04T11:19:38Z</created><updated>2014-09-06T19:06:14Z</updated><resolved>2014-09-06T19:06:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bradvido" created="2014-07-09T21:25:27Z" id="48536344">This would be useful for me as well!
</comment><comment author="clintongormley" created="2014-07-25T09:22:25Z" id="50126900">@dimovelev What generated links are you referring to?  I'm struggling to understand why you can't rewrite the paths in your application.  
</comment><comment author="clintongormley" created="2014-09-06T19:06:14Z" id="54724929">No further feedback. Closing. Feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rest API: Ensure 503 signals ==  retry on another node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4066</link><project id="" key="" /><description>The `503` status code is used by the clients to signal "I can't handle this for some reason, but you can retry on another node"

For instance, if a node can't see the minimum master nodes, then it returns a 503, in which case the clients should try another node in the list.  

At times, 503 has been used for other types of responses (eg no indices available to search on).  Worth going through the codebase to check that 503s are used consistently.
</description><key id="22045552">4066</key><summary>Rest API: Ensure 503 signals ==  retry on another node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2013-11-04T11:19:13Z</created><updated>2015-08-13T15:29:57Z</updated><resolved>2014-07-03T19:52:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-02T15:58:54Z" id="29629388">The only other place that 503 is used in ES is when rejecting requests due to thread poll constraints, and I think 503 is correct for it?
</comment><comment author="clintongormley" created="2013-12-02T15:59:59Z" id="29629497">Yes agreed - that is an appropriate use case.

If those are the only two places, then I'll close this issue.
</comment><comment author="kimchy" created="2013-12-02T16:05:54Z" id="29630085">but what will we do in the clients then? thread pool rejection is different compared to a node returning 503 because its no longer part of the cluster... . I do think we need to somehow tell the different from the clients perspective?
</comment><comment author="clintongormley" created="2013-12-02T16:08:06Z" id="29630296">OK - reopened.  503 for me means: retry on another node.  I guess if a request on another node gets rerouted back to the same node you could end up with the same 503 response?

What should the clients do for thread pool rejection then?
</comment><comment author="kimchy" created="2013-12-02T16:09:29Z" id="29630432">yea, exactly, a 503 because of thread pool rejection is different, and internally, the language client should not retry, but expose it as a proper failure to the user code, and then they can decide what to do (back off, ...)
</comment><comment author="uboness" created="2013-12-02T16:13:33Z" id="29630832">maybe we can use the `Retry-After` header, which will indicate high load (and try to come up with a default value for it :))
</comment><comment author="kimchy" created="2013-12-02T16:20:02Z" id="29631453">@uboness we can't really put a value there..., I think its more geared towards single "usage" type, we can't really sensibly give any value there
</comment><comment author="uboness" created="2013-12-02T16:36:15Z" id="29633075">yeah.. I know.. hence the :).. that's the closes "official" way I thought of to distinguish between the two scenarios 
</comment><comment author="xyu" created="2014-06-16T23:25:27Z" id="46250608">Maybe use a 502 for when the node is no longer part of the cluster? Not exact but if one considers the elected master the "upstream" server simply because it delivers cluster state needed in order to route queries it kinda works.
</comment><comment author="clintongormley" created="2014-07-03T19:52:45Z" id="47976718">Fixed by #6627
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changes to the cluster state API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4065</link><project id="" key="" /><description>The cluster-state API by default returns all info, and requires the user to specify info that they DON’T want to see. This is inconsistent and difficult to remember.

The following filters can be specified:
- `filter_blocks`
- `filter_index_templates`
- `filter_indices`
- `filter_metadata`
- `filter_nodes`
- `filter_routing_table`

Change the path to:

```
/_cluster/{metric}/state/{indices}
```

Where:
- `metric` is blank or a comma separated list of: `blocks`, `index_templates`, `metadata`, `nodes`, `routing_table`
- `indices` is blank or `_all` or  a comma-separated list of indices
</description><key id="22044856">4065</key><summary>Changes to the cluster state API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-11-04T11:01:18Z</created><updated>2014-01-08T08:44:57Z</updated><resolved>2014-01-08T08:44:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-04T11:20:34Z" id="27678491">I think it will better (and more consistent with other apis) to have `/_cluster/state/{metrics}/{indices}` where metrics can be `all` for people that want to get all info for specific indices.
</comment><comment author="spinscale" created="2014-01-02T12:19:04Z" id="31449577">Going with boaz suggestion sounds good, makes it more consistent.
</comment><comment author="spinscale" created="2014-01-02T13:46:11Z" id="31453190">I think we can remove the `index_templates` metric (is it is part of the `metadata` structure being returned), but add a  parameter `index_templates`, which contains the templates to be returned
</comment><comment author="spinscale" created="2014-01-02T14:32:57Z" id="31455725">After spending a bit more time with this, the following issue needs to be solved in a useful manner:

Right now, all the filtering capabilities for the indices are only applied for the metadata being returned, either getting certain index_templates or getting back data for specific indices only. However the latter one would be as important in the routing table as well (as the routing table can get pretty big when the data for all indices is being returned and there are lots of shards).

Possible solution would be to either apply the filtering logic everywhere in the cluster state response where index data is being returned, as I think it would be confusing for users to filter by index the see more data being returned in the routing table data.
</comment><comment author="spinscale" created="2014-01-02T15:43:34Z" id="31460426">Going to enhance the transport cluster state action logic to filter out indices everywhere it is needed... including the routing table
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestMasterAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java</file><file>src/test/java/org/elasticsearch/cluster/BlockClusterStatsTests.java</file><file>src/test/java/org/elasticsearch/cluster/SimpleClusterStateTests.java</file><file>src/test/java/org/elasticsearch/gateway/local/LocalGatewayIndexStateTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Cluster state API: Improved consistency</comment></comments></commit></commits></item><item><title>Make script reloading dynamically configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4064</link><project id="" key="" /><description>Re #4062 

Not sure if we need two separate settings for script reloading, ie `watcher.interval` and `script.auto_reload_enabled`.  I'd make it just one: `script.auto_reload.interval` which defaults to `-1`.

I'd also like it to be dynamically configurable, so that I can turn it on (with a short interval) while making changes, then turn it off again.

Perhaps we should keep `script.auto_reload_enabled` and default it to `true`, but make it non-dynamic, to avoid a potential exploit.
</description><key id="22041702">4064</key><summary>Make script reloading dynamically configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-11-04T09:46:43Z</created><updated>2015-09-21T19:24:48Z</updated><resolved>2015-09-21T19:24:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T19:24:48Z" id="142084633">Meh, changed my mind. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis Synonym: Shared on node level + auto reload from file system</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4063</link><project id="" key="" /><description>Two important changes to synonyms:
- Share the synonym FST on the node level, keyed by the `Settings`, so if there are multiple indices sharing the same settings, they would also share the same FST.
- Using the new file watcher service, we can automatically reload synonyms if configured on the file system and changed. Might make sense to have a "reloadble" token filter support for other cases where we are interested in it.

This is a great candidate to backport to 0.90 if implemented.
</description><key id="22040840">4063</key><summary>Analysis Synonym: Shared on node level + auto reload from file system</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Analysis</label><label>adoptme</label><label>feature</label></labels><created>2013-11-04T09:26:28Z</created><updated>2015-11-20T14:11:24Z</updated><resolved>2015-09-21T19:07:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T20:19:09Z" id="37458967">I'd love this to come sooon! Still pushing to `1.2`
</comment><comment author="clintongormley" created="2014-07-11T08:40:11Z" id="48707198">Also see #5124 for centralised synonym lists
</comment><comment author="synhershko" created="2014-07-25T11:44:51Z" id="50138717">It would be very helpful (especially now that #5124 was closed) if this would be done in a more generic way to support other types of analyzers, including custom, requiring wordlists or other types of auxiliary data or dictionaries.

I'll be happy to chip in with code or scenarios, like always.
</comment><comment author="clintongormley" created="2015-09-21T19:07:08Z" id="142080017">Closing in favour of #8961
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Automatic script reload</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4062</link><project id="" key="" /><description>Add ability to automatically reload scripts in the `config/scripts` directory as they changes.

The `config/scripts` directory will be scanned periodically for changes. New and changed scripts will be reloaded and deleted script will be removed from preloaded scripts cache. The reload frequency can be specified using `watcher.interval` setting, which defaults to `60s`. To disable script reloading completely set `script.auto_reload_enabled` to `false`.
</description><key id="22032987">4062</key><summary>Automatic script reload</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-11-04T02:54:21Z</created><updated>2013-11-04T09:46:43Z</updated><resolved>2013-11-04T03:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/FileChangesListener.java</file><file>src/main/java/org/elasticsearch/watcher/FileWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherModule.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>src/test/java/org/elasticsearch/watcher/FileWatcherTest.java</file></files><comments><comment>Initial implementation of ResourceWatcherService</comment></comments></commit></commits></item><item><title>Fix possible NPE in ClusterState.toString()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4061</link><project id="" key="" /><description>ClusterState.toString() can throw NPE if at least on index template exists in the cluster state.
</description><key id="22032270">4061</key><summary>Fix possible NPE in ClusterState.toString()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-11-04T02:14:14Z</created><updated>2014-06-17T21:02:26Z</updated><resolved>2013-11-04T11:52:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-04T08:51:28Z" id="27670222">LGTM +1
</comment><comment author="javanna" created="2013-11-04T09:04:57Z" id="27670848">+1!
</comment><comment author="imotov" created="2013-11-04T11:52:49Z" id="27679938">Pushed to master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add some documentation indicating when you would use _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4060</link><project id="" key="" /><description /><key id="22013881">4060</key><summary>Add some documentation indicating when you would use _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">benmccann</reporter><labels /><created>2013-11-03T03:10:06Z</created><updated>2014-06-17T21:27:28Z</updated><resolved>2013-11-04T10:08:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-04T10:08:49Z" id="27674719">I reworded things a bit and pushed it. Thx for doing this!

See: https://github.com/elasticsearch/elasticsearch/commit/46edfc484a513c0e567e9e55f38ca3ddce41292d
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow for storage and indexing of computed field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4059</link><project id="" key="" /><description>I'd like to be able to store a computed field. Right now the only option is to query against it is to compute it and explicitly store it in my source document. E.g. I'm syncing my docs from mongodb with a river and would like to boost using the length of an array in the document, but I have to update all my docs in mongodb with the array length in order to do that. I would rather be able to have elasticsearch compute and store the field so that I do not have to update my docs in mongodb.

I've seen others asking for this as well:
https://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/computed/elasticsearch/ro187eatzIo/lTrl4sFSJqoJ
</description><key id="22013741">4059</key><summary>Allow for storage and indexing of computed field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">benmccann</reporter><labels /><created>2013-11-03T02:56:58Z</created><updated>2014-01-14T22:00:59Z</updated><resolved>2013-11-04T08:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-11-04T08:19:00Z" id="27669014">If you need to update a document because it's not well formed on a source level (missing parts), I'd either:
- Add that process in a ETL layer whatever the technology you use to fetch your documents from the source.
- You mentioned mongodb river. Then have a look at [script support](https://github.com/richardwilly98/elasticsearch-river-mongodb/wiki#script-filters).
- Run at some points: [doc update API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-update.html) but that will cost you at the end of the day Lucene delete and index operations

It's not IMHO elasticsearch responsability to add hook scripts at index time on top of your documents. If you really need to do this inside elasticsearch instead of in another layer, then you can think of implementing a new mapper as the [attachment mapper plugin](https://github.com/elasticsearch/elasticsearch-mapper-attachments) but I guess you don't really need such a complexity.

Feel free to reopen if I misunderstood your feature request.
</comment><comment author="karol-gwaj" created="2013-11-13T00:02:39Z" id="28347525">have a look at https://github.com/SkillPages/elasticsearch-computed-fields
as of time of creating this comment it is still early beta, but it should work fine for simple use cases 
</comment><comment author="kdkeck" created="2014-01-09T20:08:23Z" id="31970587">Karol, thanks for your comment.  The computed fields plugin is _almost_ exactly what we (I work with benmccann) need, but arrays of objects in the source (JSON) document are not themselves indexable fields, and the plugin appears to only support reference to indexable fields in the calculation script.  Would it be difficult to modify it to support references to source fields?
</comment><comment author="karol-gwaj" created="2014-01-10T16:10:02Z" id="32040203">Hi Kevin,

i updated the plugin to support arrays and inner objects (SkillPages/elasticsearch-computed-fields#2)

Cheers,
</comment><comment author="kdkeck" created="2014-01-14T22:00:59Z" id="32312269">Awesome, it works great.  I'll most more feedback on the closed issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>URI routing parameter support with multi search API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4058</link><project id="" key="" /><description>Support URI level routing parameter for the multi search API, that will act as the default routing (unless explicitly set).
</description><key id="22012800">4058</key><summary>URI routing parameter support with multi search API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-11-03T01:19:41Z</created><updated>2013-11-03T01:30:21Z</updated><resolved>2013-11-03T01:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file></files><comments><comment>URI routing parameter support with multi search API</comment><comment>Support URI level routing parameter for the multi search API, that will act as the default routing (unless explicitly set).</comment><comment>closes #4058</comment></comments></commit></commits></item><item><title>Changes to the nodes stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4057</link><project id="" key="" /><description>Recommendations:

Remove the `all` and `clear` flags. Default to return `all` by default, unless specific metrics specified.

Change the path format to:

```
/_nodes/{nodes}/stats/{metric}
```

where:
- `{nodes}`: comma-delimited list of nodes or blank
- `{metric}` is blank or comma-delimited list of metrics

Metrics can be any of:
- `http`
- `jvm`
- `network`
- `os`
- `process`
- `thread_pool`
- `transport`
- `fs`
- `indices`

If `indices` is specified in the metrics list, then the path expands to accept:

```
/_nodes/{nodes}/{metrics}/stats/indices,fs,jvm/{index-metrics}
```

where:
- `{index}` is a comma-delimited list of indices, `_all`, or blank
- `{index-metrics}` is a comma-delimited list of the metrics specified in #4054 or blank for all

and the query-string options accepted by indices statistics in #4054 are also accepted here
</description><key id="22008746">4057</key><summary>Changes to the nodes stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-11-02T20:26:24Z</created><updated>2014-01-06T07:38:02Z</updated><resolved>2014-01-06T07:38:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file></files><comments><comment>Simplify nodes stats API</comment></comments></commit></commits></item><item><title>Add wabisabi to Scala clients.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4056</link><project id="" key="" /><description /><key id="22008579">4056</key><summary>Add wabisabi to Scala clients.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gphat</reporter><labels /><created>2013-11-02T20:15:41Z</created><updated>2014-07-16T21:51:39Z</updated><resolved>2013-11-08T09:44:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-08T09:44:32Z" id="28051311">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changes to the nodes info API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4055</link><project id="" key="" /><description>Recommendations:

Remove the `all` and `clear` flags. Default to return `all` by default, unless specific metrics specified.

Change the path format to:

```
/_nodes/{nodes}/{metric}
```

where:
- `{nodes}`: comma-delimited list of nodes or blank
- `{metric}` is blank or comma-delimited list of metrics

Metrics can be any of:
- http
- jvm
- network
- os
- plugin
- process
- settings
- thread_pool
- timeout
- transport

NOTE: Node names and metric names could clash, if you name your nodes eg `process`.  Should probably accept `_all` in the `{nodes}` parameter.
</description><key id="22008270">4055</key><summary>Changes to the nodes info API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-11-02T19:55:52Z</created><updated>2014-01-08T09:03:21Z</updated><resolved>2014-01-08T09:03:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-04T15:32:36Z" id="29814576">After reading #4057 - why not simply make the URL `/_nodes/{nodes}/infos/{metric}` and allow `nodes` and `metric` to be blank. Removes also the possible clash you mentioned in the last sentence...
</comment><comment author="clintongormley" created="2013-12-04T15:34:52Z" id="29814767">I'd be up for supporting `/_nodes/{nodes}/info/{metric}` (note `info` with the `s`), but i would make it optional, so that `GET /_nodes` would return the same thing as `GET /_nodes/info`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Updated nodes info endpoint in elasticsearch.yml</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoTests.java</file></files><comments><comment>Simplify usage of nodes info API</comment></comments></commit></commits></item><item><title>Changes to the indices stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4054</link><project id="" key="" /><description>Recommendations:

Remove the `all` and `clear` flags. Default to return `all` by default, unless specific metrics specified.

Change the path format to:

```
/{index}/_stats/{metric}
```

where:
- `{index}`: comma-delimited list of indices, `_all`, or blank
- `{metric}` is blank or comma-delimited list of metrics

Metrics can be any of:
- `completion`
- `docs`
- `fielddata`
- `filter_cache`
- `flush`
- `get`
- `id_cache`
- `indexing`
- `merge`
- `refresh`
- `search`
- `store`
- `warmer`

Four metrics take optional parameters, which can be specified in the query string.  These are:
- completion: `fields` or `completion_fields`
- fielddata: `fields` or `fielddata_fields`
- indexing: `types`
- search: `groups`

Change `level` to accept `cluster`, `indices` and `shards`, as per the cluster health API.  Default to `indices`.  If set to `cluster`, only return the `_all` stats.

Add the `translog` info from the index status API into the shard-level info of the indices stats API.
</description><key id="22007639">4054</key><summary>Changes to the indices stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-11-02T19:18:44Z</created><updated>2014-01-06T09:52:24Z</updated><resolved>2014-01-06T06:45:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-06T11:06:39Z" id="29980329">Started working on this.

It seems, that the `level` parameter is only mentioned in the documentation, but actually nowhere in the REST action. Maybe I am missing something?
</comment><comment author="spinscale" created="2013-12-06T13:10:15Z" id="29989196">Forget my previous comment, it is hidden in the stats response in the `toXContent` serialization, not in the request...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStats.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/test/java/org/elasticsearch/indices/stats/SimpleIndexStatsTests.java</file></files><comments><comment>Simplify indices stats API</comment></comments></commit></commits></item><item><title>URI routing parameter does not work with Bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4053</link><project id="" key="" /><description>When I specify the routing in BULK requests, it does not import all documents to the same shard I specify. It does work when I index 1 document at a time, or when I search. Is this a bug in the Bulk API?

curl -XPOST 'http://192.168.1.115:9200/_bulk?routing=a' -d '
{ "index" : { "_index" : "articles", "_type" : "article", "_id" : "1" } }
{ "title" : "value1" }
{ "delete" : { "_index" : "articles", "_type" : "article", "_id" : "2" } }
{ "create" : { "_index" : "articles", "_type" : "article", "_id" : "3" } }
{ "title" : "value3" }
{ "update" : {"_id" : "1", "_type" : "article", "_index" : "index1"} }
{ "doc" : {"field2" : "value2"} }'
</description><key id="22007087">4053</key><summary>URI routing parameter does not work with Bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">HenleyChiu</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-11-02T18:50:32Z</created><updated>2013-11-03T01:20:11Z</updated><resolved>2013-11-03T01:16:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-03T01:05:21Z" id="27636471">Thats because the routing parameter is not applied at all globally from the URI for bulk request. We can support that..
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file></files><comments><comment>URI routing parameter does not work with Bulk API</comment><comment>closes #4053</comment></comments></commit></commits></item><item><title>Added support for highlighting multi term queries using the postings highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4052</link><project id="" key="" /><description>Added rewrite of the query using the current top level reader instead of an empty one, rewritten all multi term queries using `TopTermsScoringBooleanQueryRewrite(1024)` rewrite mode.

As a result, postings highlighter supports now wildcard query, prefix query, fuzzy query, term range query and regexp query.

Closes #4042
</description><key id="22001272">4052</key><summary>Added support for highlighting multi term queries using the postings highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-02T12:53:33Z</created><updated>2014-06-12T14:11:26Z</updated><resolved>2013-11-04T11:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-04T11:37:09Z" id="27679246">+1 lgtm!
</comment><comment author="s1monw" created="2013-11-04T11:52:55Z" id="27679942">+1 looks great!
</comment><comment author="javanna" created="2013-11-04T11:57:59Z" id="27680172">Merged 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fixed multi term queries support in postings highlighter for non top-level queries</comment></comments></commit></commits></item><item><title>Finalize acknowledgements work, move over all the api, strenghten ack api and remove boiler plate code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4051</link><project id="" key="" /><description>All the apis that modify the cluster state and already supported acknowledgement but used the old mechanism with custom notifications have been migrated to the new generic ack mechanism introduced in #3786 : indices aliases, open/close index, delete index, put mapping, create index.

A MetaDataService that actually submits the cluster state update task has been introduced in cluster reroute, cluster update settings, put/delete warmer (similar to what other apis already do) rather than making changes directly in the `Transport*Action`. The `AcknowledgedRequest` gets converted to an internal `ClusterStateUpdateRequest` (or subclass) that is used for the cluster state update which returns a `ClusterStateUpdateResponse` (or subclass), that needs to be converted back to `AcknowledgedResponse`.

Introduced a `TransportClusterStateUpdateAction` base class that allows to share code and avoid repeating boiler plate code in each action. Made `AckedClusterStateUpdateTask` an abstract class that contains the implementation for the methods that are always the same all over the place (similar to #4001). Unified the rest layer too to parse the `timeout` and `master_timeout` parameters in a single place.

The `TransportDeleteIndexAction` is the only one that has not been moved over to `TransportClusterStateUpdateAction` as it needs some rework: it currently performs multiple cluster state updates, one per index, and there's a metadata lock involved per each index.

Also, I wonder if we should go ahead and add ack support in put/delete index template as they both already support all the needed parameters, which are currently pointless (timeout ignored, acknowledged always true).
</description><key id="21999276">4051</key><summary>Finalize acknowledgements work, move over all the api, strenghten ack api and remove boiler plate code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-11-02T09:48:49Z</created><updated>2014-06-19T19:22:06Z</updated><resolved>2013-11-07T11:25:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-07T11:25:00Z" id="27955215">Closing this one. Going to send smaller pull requests per api instead of this big one that's hard to review.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _cat/health</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4050</link><project id="" key="" /><description>Need health one-liner.
</description><key id="21986026">4050</key><summary>Add _cat/health</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-11-01T21:35:40Z</created><updated>2013-11-01T21:37:33Z</updated><resolved>2013-11-01T21:37:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestHealthAction.java</file></files><comments><comment>Add _cat/health.</comment></comments></commit></commits></item><item><title>Unable to add custom log4j layout (probably appender too)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4049</link><project id="" key="" /><description>I want to plug elasticsearch logs into logstash, but it doesn't work.
https://github.com/logstash/log4j-jsonevent-layout - here is custom layout I want to use.
I also have opened ticket https://github.com/logstash/log4j-jsonevent-layout/issues/28 

I'm not sure is it a bug or configuration issue.
1. I've installed elasticsearch 0.90.5
2. Put jsonevent-layout-1.4.jar into ES lib directory
3. I've changed logging.yml appender -&gt; file section like this:

```
  file:
    type: dailyRollingFile
    file: ${path.logs}/${cluster.name}.log
    datePattern: "'.'yyyy-MM-dd"   
    layout:
      type: net.logstash.log4j.JSONEventLayout
```
1. removed logs and Restarted elasticsearch.

Result: no logs, even no file ${path.logs}/${cluster.name}.log

Global goal: force elasticsearch to write logs using an ampq/rabbitmq appender with json_event layout (compatible with logstash events)
</description><key id="21981860">4049</key><summary>Unable to add custom log4j layout (probably appender too)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsirex</reporter><labels /><created>2013-11-01T20:21:50Z</created><updated>2013-11-04T08:53:51Z</updated><resolved>2013-11-04T08:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-02T23:36:37Z" id="27635206">Try adding jsonevent-layout-1.4.jar dependencies to ES lib directory as well: json-smart-1.1.1.jar, commons-lang-2.4.jar
</comment><comment author="jsirex" created="2013-11-04T08:53:51Z" id="27670324">Thanks a lot! Completely forget for dependencies.
Now it works :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed bug where empty objects are not stored in _source if an include/ex...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4048</link><project id="" key="" /><description>...clude list is present

This fixes an issue where an empty object would not be stored in the _source field.  Fields whose values become empty as the result of filtering will continue to be removed from _source.

Closes #4047
</description><key id="21974658">4048</key><summary>Fixed bug where empty objects are not stored in _source if an include/ex...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RobCherry</reporter><labels /><created>2013-11-01T18:33:57Z</created><updated>2014-06-16T06:53:39Z</updated><resolved>2013-11-04T17:37:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-04T10:34:42Z" id="27676213">Hi Rob, 

Thanks for doing this. I think there is a slight problem when you use include filters to refer to the content of the empty object.

For example

```
{
   "object": { },
   "object2": { 
       "key": "a"
  }
```

Filtered with `include=[ "object2.other_key"]` returns and empty object `{}`. To be consistent we want `include=[ "object.what_ever"]` to also return `{}` rather than `{ "object": {} }`.

I left a comment on the code with a possible solution. Can you update and also add a test for that?

Cheers,
Boaz
</comment><comment author="RobCherry" created="2013-11-04T16:33:45Z" id="27699538">@bleskes Feel free to ignore changes to XContentMapValues.java in 9102277cac125c0de6addfb57650592b05b89ffe, they are corrected in 459b3023eed75546fd5e3d801f7d7340058a13d9.

I am uncertain about the behavior for handling includes/excludes with empty objects and wildcards, so I added tests to cover the following cases, but please let me know if you think the results are is incorrect...

Given

```
{
  "object1": { },
  "object2": { }
}
```

And a mapping

```
{
  ...
  {
    "_source" : {
      "excludes": "object1.*,
      "includes": "object2.*"
    }
  }
}
```

I expect a filtered source of

```
{
  "object1": { },
}
```
</comment><comment author="RobCherry" created="2013-11-04T17:37:46Z" id="27705339">Ignore this, I am going to create a new PR w/ a cleaner patch and put it in a bug branch of my repo.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Empty objects are not stored in _source when an include/exclude list is present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4047</link><project id="" key="" /><description>```
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test/test/1 -d '{ "empty": {}, "not_empty": { "key": "value" } }'
curl localhost:9200/test/test/1/_source
```

Returns `{ "empty": {}, "not_empty": { "key": "value" } }` as expected.

```
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test
curl -XPUT localhost:9200/test/test/_mapping -d '{ "test": { "_source" : { "excludes": [ "ignored" ] } } }'
curl -XPUT localhost:9200/test/test/1 -d '{ "empty": {}, "not_empty": { "key": "value" } }'
curl localhost:9200/test/test/1/_source
```

Returns `{"not_empty":{"key":"value"}}` which is not expected.
</description><key id="21971833">4047</key><summary>Empty objects are not stored in _source when an include/exclude list is present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">RobCherry</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-11-01T17:48:34Z</created><updated>2014-01-14T16:07:22Z</updated><resolved>2014-01-14T16:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>excluding all fields of an object should not remove parent.</comment></comments></commit></commits></item><item><title>fixes #4045 - allow aliases on morelikethis on nodeclients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4046</link><project id="" key="" /><description>Even though this was addressed a long time ago, at the time the fix didn't take into account that on redirects, the alias also needed to be resolved. I added a new tests that covers this case(uses a nodeclient to execute the request,so request will be redirected to a node data that can actually resolve the request).
</description><key id="21969881">4046</key><summary>fixes #4045 - allow aliases on morelikethis on nodeclients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-11-01T17:18:43Z</created><updated>2014-07-16T21:51:41Z</updated><resolved>2013-11-05T03:44:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-11-05T03:44:44Z" id="27745644">Pushed to master and 0.90. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error when using aliases on a MoreLikeThisQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4045</link><project id="" key="" /><description>there was a commit that actually fixed that: https://github.com/imotov/elasticsearch/commit/963468cc5c751c49e8fb0494b1578d1382014656 , but  it missed doing that on redirect(that's why this only happens when using a NodeClient to execute the query).

anyway, doing the same here https://github.com/elasticsearch/elasticsearch/blob/4fa8f6f61f886446a09e3324a74e146ab73b1d0a/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java#L239 fixes the issue. or at least i couldn't reproduce it anymore.
</description><key id="21968652">4045</key><summary>Error when using aliases on a MoreLikeThisQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-11-01T16:57:02Z</created><updated>2013-11-05T03:42:43Z</updated><resolved>2013-11-05T03:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-11-04T13:01:48Z" id="27683070">@imotov maybe you could take a look at this, since you wrote the original patch? it would be great if this could be integrated into 0.90.6
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/test/java/org/elasticsearch/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>Resolve aliases on redirects inside TransportMoreLikeThisAction</comment></comments></commit></commits></item><item><title>ContextSuggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4044</link><project id="" key="" /><description>This commit extends the `CompletionSuggester` by context
informations. In example such a context informations can
be a simple string representing a category reducing the
suggestions in order to this category.

Three base implementations of these context informations
have been setup in this commit.
- a Category Context
- a Geo Context
- a Field Context

All the mapping for these context informations are
specified within a context field in the completion
field that should use this kind of information.
## Mapping Example

The following example shows the mapping for a GeoContext.

```
{
    "testType":{
        "properties":{
            "testField":{
                "type":"completion",
                "index_analyzer":"simple",
                "search_analyzer":"simple",
                "payloads":true,
                "preserve_separators":false,
                "preserve_position_increments":true,
                "context":{
                    "geo":{
                        "separator":"|",
                        "precision":8,
                        "neighbors":true
                    }
                }
            }
        }
    }
}
```
## Indexing

During indexing a document the subfield `context` of the
completion field contains the data to be used in order
to provide suggestions.

```
{
    "testField":{
        "input":[["pizza - berlin","pizza","food"]],
        "context":"u33dc1v0xupz"
    }
}
```
## Suggestion Example

The Suggestion request is extended by a context value. The
suggest request for a geolocation looks like

```
{
    "suggest":{
        "text":"pizza",
        "completion":{
            "field":"testField",
            "size":10,
            "context":{
                "geo":"u33dc0cpke4q"
            }
        }
    }
}
```

The context objects contains a field with the same name as
defined in the mapping. According to the type of the context
this field contains the data associated with the suggestion
request. In this example the geohash of a location.
## Category Context

The simplest way to use this feature is a category context. It
supports a arbitrary name of a category to use with the completion
suggest API.

To set the context support to the category type this option must be
set to `true`:

```
"testField":{
    "type":"completion",
    "context":{
        "category": true
    }
}
```

The name of the context category then needs to be set within the
suggestion context during indexing:

```
{
    "testField":{
        "input":[["pizza - berlin","pizza","food"]],
        "context":"delivery"
    }
}
```

and can be used by setting the category value:

```
{
    "suggest":{
        "text":"pizza",
        "completion":{
            "field":"testField",
            "size":10,
            "context":{
                "category":"delivery"
            }
        }
    }
}
```
## Field Context

The Field Context works like the category context but the value of this
will context will not explicitly be set. It refers to another field in
the document. In example a `category` field.

```
{
    "category":{
        "type": "string"
    },
    "testField":{
        "type":"completion",
        "context":{
            "field": "category"
        }
    }
}
```

for indexing the field context must be set to true:

```
{
    "category":"delivery",
    "testField":{
        "input":[["pizza - berlin","pizza","food"]],
        "context":true
    }
}
```

and suggestions use the `context.field` value

```
{
    "suggest":{
        "text":"pizza",
        "completion":{
            "field":"testField",
            "size":10,
            "context": {
                "field": "delivery"
            }
        }
    }
}
```
## Geo Context

The last context feature is the GeoContext. It take a location into account.
For example if one searches for delivery services it might be use full to find
results around the location the query was sent. This context internally works
on geohashes only but the REST API allows any form defined for `geo_points`

In the mapping this kind of context is configured by two parameters:
- precision
- neighbors

The `precision` option is used to configure the range of result. If the
`neighbors` option is enabled not only the given geohash cell will be used
but also all it's neighbors. 

```
"context":{
    "geo":{
        "separator":"|",
        "precision":8,
        "neighbors":true
    }
}
```

The context during indexing is set to the location of the input:

```
{
    "testField":{
        "input":[["pizza - berlin","pizza","food"]],
        "context": "u33dc1v0xupz"
    }
}
```

To get a ist of suggestions around a specific area the `context.geo` field
must contain the position of this area:

```
{
    "suggest":{
        "text":"pizza",
        "completion":{
            "field":"testField",
            "size":10,
            "context": {
                "geo": "u33dc0cpke4q"
            }
        }
    }
}
```

Closes #3959
</description><key id="21959876">4044</key><summary>ContextSuggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>:Suggesters</label><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2013-11-01T14:31:11Z</created><updated>2015-06-06T18:46:46Z</updated><resolved>2014-03-13T11:16:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-01T14:33:28Z" id="27569504">haven't reviewed the code, API wise it looks good to me, but can we also have as part of this pull request the relevant changes to our docs?
</comment><comment author="chilling" created="2013-11-01T14:36:32Z" id="27569748">@kimchy of course
</comment><comment author="s1monw" created="2013-11-07T12:21:04Z" id="27958123">A couple of comments based on the documetnation: 

For mapping:

```
 "geo":{
        "separator":"|",
        "precision":8,
        "neighbors":true
    }
```

I don't think we should expose the separator? What is the reason this is an implementation detail and should not be exposed to the user.

On the request side of things I don't think we should require folks to specify stuff like this:

```
"context": {
 "field": "delivery"
}
```

it should rather look like this:

```
"context":  "delivery"
```

Instead, since we know how to parse the values ie. we know from the mapping that it is a field or geo?!

For the mapping of `Category` and `Field` I think we should fold the two into one and make the `Field` the default. Something like this:

```
"testField":{
    "type":"completion",
    "context":{
        "category": {
           "default_field" : "cat_type",
           "default_value : "delivery"
         }
    }
}
```

Then you can decide if you want to specify it manually like this:

```
{
    "testField":{
        "input":[["pizza - berlin","pizza","food"]],
        "context" : "delivery"
    }
}
```

or via a field:

```
{
    "cat_type" : "delivery",
    "testField":{
        "input":[["pizza - berlin","pizza","food"]],
        "context":"delivery"
    }
}
```

and since we defined the default value in the mapping we can still have a context if the `cat_type` field is not there and we didn't specify anything as the context. Makes sense?
</comment><comment author="s1monw" created="2013-11-07T12:32:13Z" id="27958716">I left some comments on the code, looks great but I think we can simplify the API as I described above! If you push changes don't rebase your branch with master or so please just add the relevant changes add, commit and push the branch to your github repo so we can see them here.

thanks !!
</comment><comment author="clintongormley" created="2013-11-14T10:30:04Z" id="28473644">Hiya

I've got some comments about how to improve the API.

There is no mention of what happens if a `context` field contains multiple values. That said, geohashes already has multiple contexts, as edge ngrams are generated. I'd also like to be able to use a multi-value field as a context, eg: 

```
colors: [red,green]
```

I probably would also like to combine multiple fields to generate a context, eg:
-  country
-  region
-  city

Specifying this via the API could make things messy and overly complicated. The simple answer to this would be to concatenate these values into one field.  I'd also like it if we could accept scripts which can generate a context value (or array of values) based on the _source.

So we have:
-  value:  passed-in || default_value
-  field:  passed-in || field_value || default_value
-  script: passed-in || generated by the script || default_value
-  geo:    could be any of the above, but requires special handling

The mappings should look like the following:

## Mapping for value

No default value set:

```
"testField":{
    "type":"completion"
}
```

Default value set:

```
"testField":{
    "type":"completion",
    "context":{
        "default":  "foo"
    }
}
```

Should `default` actually be called `null_value`?

## Mapping for field

```
"testField":{
    "type":"completion",
    "context":{
        "field":    "tags",
        "default":  "foo"    # optional
    }
}
```

## Mapping for script

```
"testField":{
    "type":"completion",
    "context":{
        "script": {
            "lang": "mvel",
            "script": "ctx._source.foo"
        },
        "default":  "foo"    # optional
    }
}
```

## Mapping for geo

By specifying `geohash:{}` it triggers geohash handling of the `context` value. The geohash itself can come from anywhere.

(btw, `neighbours` should default to `true`, I think)

**Geo as value:**

```
"testField":{
    "type":"completion",
    "context":{
        "geohash": {                
            "precision": 8,         # optional
            "neighbours": true      # optional
        },
        "default":  "u33dc1v0xupz"  # optional
    }
}
```

**Geo as a field:**

```
"testField":{
    "type":"completion",
    "context":{
        "field":    "location",
        "default":  "u33dc1v0xupz"  # optional
        "geohash": {                
            "precision": 8,         # optional
            "neighbours": true      # optional
        },
    }
}
```

Although we could derive the geohash stuff from the referenced field, for consistency's sake, I think I prefer actually specifying it.  All it needs is `"geohash:{}"`, it doesn't need to set anything else.

**Geo as a script:**

```
"testField":{
    "type":"completion",
    "context":{
        "default":  "u33dc1v0xupz"  # optional
        "geohash": {                
            "precision": 8,         # optional
            "neighbours": true      # optional
        },
        "script": {
            "lang":   "mvel",
            "script": "foo.bar.value"
        }
    }
}
```

## Indexing a document

Specifying the `context` directly:

```
"testField": {
    "input":    ["pizza - berlin", "pizza","food"],
    "context":  "foo" 
}
```

`context` not specified

```
"testField": {
    "input":    ["pizza - berlin", "pizza","food"]
}
```

Derives value from :

```
 -&gt; script  (if specified)
 -&gt; field   (if specified)
 -&gt; default (if specified)
```

Can have multiple contexts, eg:

```
"testField": {
    "input":    ["pizza - berlin", "pizza","food"],
    "context":  ["italian","fastfood"] 
}
```

This should generate two suggestion paths

## Searching:

There is no need to specify geo etc, as we can figure that out from the mapping, so searching just looks like:

```
{
    "suggest":{
        "text":"pizza",
        "completion":{
            "field":"testField",
            "size":10,
            "context": "u33dc0cpke4q"
        }
    }
}
```
</comment><comment author="chilling" created="2013-11-15T04:15:43Z" id="28546240">Thanks a lot for your suggestions. I had already started implementing the new style of the API and the multivalued contexts.
</comment><comment author="chilling" created="2013-11-20T04:21:29Z" id="28862819">Hey @simon,

I think we should keep the `separator` option. For contexts with arbitrary text values we need to distinguish between different context/values to guarantee unique prefixes. Think of a context `a` and a suggestion value of `|b` in the first place and a context `a|` and a value of `b`. Both will create the same path (`a||b`) in the underlying FST.
</comment><comment author="chilling" created="2013-11-20T04:33:57Z" id="28863181">@clintongormley I like the idea of using multivalued fields as context. But I think we have two options here:
- combine the values `and-wise`
- combine the values `or-wise`

The first option requires an order to create an unique context. The second option will simply generate alternative contexts. I think the later option should be used. But I like to hear your idea of those contexts.
</comment><comment author="s1monw" created="2013-11-20T08:48:01Z" id="28871567">@chilling I think we should use a less promient character to separate to begin with like `\u001F` and if the context contains it we reject it. It's very unlikely that it contains it. 

Can you reply to all the code comments etc. as well please.
</comment><comment author="chilling" created="2013-11-20T08:54:16Z" id="28871851">@s1monw, you're right. I think this will be an acceptable solution.
</comment><comment author="chilling" created="2013-12-12T08:54:50Z" id="30399247">I just managed to push a clean update. So for now we support multiple values and multiple context in the context suggester. Currently I'm working on the documentation but feel free to have a first look at the current implementation.
</comment><comment author="s1monw" created="2013-12-12T10:20:35Z" id="30404247">hey florian, I took a loot at the PR and I think it looks good though. I wonder a bit how the API (REST) looks like at this point. Can you add documentation to the PullRequest so we can also see the API from a REST perspective. It's kind of hard to tell from the tests etc. :) 
</comment><comment author="chilling" created="2013-12-12T10:22:41Z" id="30404371">Thanks @s1monw. I already started working on docs. So documentation will be there tomorrow.
</comment><comment author="chilling" created="2013-12-13T08:10:59Z" id="30492875">@s1monw for now I worked on your code review and changed the most of the thinks. What keeps me busy right now, is the fuzzy logic. I think I'm not able to adjust the `fuzzyprefix`, because the `ContextSuggester` supports multiple values as alternatives. So there could be different paths with different length. So I'm trying to build two separate Automatons. One for the prefixes and the other from the actual Suggestion. But I haven't found the right place for this now. Today I tested also tested the REST API and I still have some minor issues on parsing. The API should look like this

## Mapping

```
        "properties" : {
            "name" : { "type" : "string" },
            "TypeOfService" : { "type" : "string", "index": "not_analyzed" },
            "suggest" : {
                "type" : "completion",
                "index_analyzer" : "simple",
                "search_analyzer" : "simple",
                "payloads" : true,
                "context": [
                    { "geo": { "precision": "50m", "neighbors": true } },
                    { "field": { "fieldname": "TypeOfService", "default": "unknown" } },
                    { "category": { "default": "none" } },
                ]
            }
```

## Index

```
{
    "name" : "Hotel Berlin, Tokyo",
    "TypeOfService" : "hotel",
    "suggest" : {
        "input": [ "Hotel", "Berlin", "Hotel Berlin"],
        "output": "Hotel Berlin, Tokyo",
        "context": [ {"lat": 35.689506, "lon": 139.6917}, on, ["hotel", "rooms"]
        "payload" : { "id" : 1 },
    }
}
```

## Query

```
{
    "suggest" : {
        "text" : "b",
        "completion" : {
            "field" : "suggest",
            "context": [{"lat": 35.689506, "lon": 139.6917}, "hotel", "rooms"]
        }
    }
}
```
</comment><comment author="s1monw" created="2013-12-13T09:48:29Z" id="30497412">&gt; &gt; I think I'm not able to adjust the fuzzyprefix, because the ContextSuggester supports multiple values as alternatives. So there could be different paths with different length. 

Wait, you are only using exactly one prefix to lookup the suggestion since our interface doesn't allow multiple. At index time is a different story. But at serach time you should be able to tell how long the prefix is?
</comment><comment author="chilling" created="2013-12-13T17:16:34Z" id="30526222">No. Consider the category context. It allows you to define multiple categories at the query which are used as alternatives. But maybe I just should remove this part.
</comment><comment author="s1monw" created="2013-12-13T19:26:47Z" id="30536200">I don't follow, the alternatives are applied at index time not at query time no? The prefix length is a query time parameter.
</comment><comment author="chilling" created="2014-01-10T12:02:05Z" id="32022123">@s1monw please have a look.Maybe you have an idea how to setup and merge a FST for the QueryPart.
</comment><comment author="chilling" created="2014-01-16T11:22:17Z" id="32460769">@s1monw the context part now seems to work together with the fuzzy logic. I need to setup some more tests, but maybe you can have a look at this version
</comment><comment author="s1monw" created="2014-01-16T11:45:05Z" id="32462170">I will look @chilling thanks!
</comment><comment author="s1monw" created="2014-01-17T20:48:57Z" id="32644965">@chilling I reviewed it and I think it looks very close! I want @clintongormley to look at it again API wise and @spinscale should review it as well since he needs to be on top of the suggesters as well! I left some comments on  the commit, can you please take a look?
</comment><comment author="spinscale" created="2014-01-30T14:38:16Z" id="33692884">Hey,

I have just finished the first iteration code review and will start playing around with it now, couple of comments. Functionality wise this definately screams for a blog post :-)
- As Simon mentioned: `mvn license:check`
- Even though are very small, but maybe unit test the prefixanalyzer?
- YAML tests would be awesome I guess

If you execute a suggestion, the context field often looks like `"context": [["hotel", "place"]]` - is it possible to support a single array notation here in that special? So it becomes `"context": ["hotel", "place"]` - Would save some work/questions by users I guess, also maybe support `"context": "hotel"`?

Another big thing in my opinion: The whole implementation is rather _unguicy_ and static. I think it would be nice for the user to write his/her own implemention of context mappers, so they need to be registered and proplerly integrated with guice (maybe in the next bigger refactoring).
</comment><comment author="spinscale" created="2014-01-31T12:53:56Z" id="33791571">Found an NPE, if you try to execute a suggestions on a non-existing/wrong mapped field, easy to reproduce

```
curl -X DELETE localhost:9200/pois
curl -X PUT localhost:9200/pois
curl -X POST 'localhost:9200/pois/_suggest?pretty' -d '{
  "suggest" : {
    "text" : "b",
    "completion" : {
      "field" : "suggest_field",
      "size": 10,
      "context": [["car"]]
    }
  }
}'
```

No appropriate check in the CompletionSuggestParser (seems to have been there before...)
</comment><comment author="s1monw" created="2014-02-19T17:40:03Z" id="35525706">I left a lot of comments but nothing critical I think we are really close here. I like how many tests we have and how it turned out with exposing this feature on the suggester level! Good stuff!
</comment><comment author="chilling" created="2014-02-26T13:01:21Z" id="36122157">hey guys,
next iteration. @s1monw maybe you can have a review? @spinscale I had a discussion with simon on the REST-API. So far I hope I got all the changes. @clintongormley I discussed with simon, that scripting should be left out for now, but I think so far the rest works. The docs also are complete now @kimchy
</comment><comment author="s1monw" created="2014-02-26T14:15:00Z" id="36128642">Florian, the commit LGTM - I think we can squash and rebase with master. I want @clintongormley to read the documentation one more time but I think we are ready to go here!
</comment><comment author="chilling" created="2014-02-26T14:16:18Z" id="36128758">@s1monw cool
</comment><comment author="clintongormley" created="2014-02-28T18:14:15Z" id="36378346">Hi @chilling 

I've finished reviewing the docs.  I think there are improvements we can make to the API:
- Change `missing` to `default`
- Merge `category` and `field`. So you can specify default categories, and optionally a `path` containing the name a field which contains the value.  At index time you can either specify a context manually, or retrieve it from the field (if specified) or use the `default`.
- Geo contexts should allow multiple precisions, in which case you would need to specify the required precision at search time. (If only one precision is specified then it would use that by default).
- Should `neighbours` default to `true`?
</comment><comment author="chilling" created="2014-03-05T08:41:09Z" id="36720571">hi @clintongormley,

for now I worked in most of your comments. Currently I actually working on the multiple precisions. I makes completely sense. My basic idea in the initial commit was to index the whole geohash path. Namely every prefix. Since this obviously caused trouble I removed this. But the idea of having at least a precision at the query, will solve the problem. Do you think it makes sense to index all prefixes of a geohash and the just query by a certain precision?
</comment><comment author="clintongormley" created="2014-03-05T10:10:27Z" id="36727374">Hi @chilling 

&gt; Do you think it makes sense to index all prefixes of a geohash and the just query by a certain precision?

That makes sense to me, but I have a feeling that it may generate very large FSTs given that you essentially add all the data 12 times.  If it were possible to compress the FST that would be awesome.  Failing that, I think the list of precisions is the best compromise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Two stage cluster restart to reduce recovery time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4043</link><project id="" key="" /><description>My idea may be rather naive and not workable for various reasons but looks like quite a few people have issues with very long recovery time even when use planned full cluster shutdown/restart.

I wonder if a two step process may help
1. Switch cluster to "service mode" - cluster stops accepting any requests and prepares for shutdown (flush etc). 
2. When all nodes entered service mode, shutdown command can be issued to shut down the cluster (or each node could be shut down manually)
3. Upon restart, all nodes start in service mode rejecting any requests until the cluster is fully recovered (or it is forced into operational mode)
4. cluster is switched into operational mode

Any variations could be added (i.e in service mode cluster could serve read-only requests or may be updates could be queued up) 
</description><key id="21958451">4043</key><summary>Two stage cluster restart to reduce recovery time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-11-01T14:01:51Z</created><updated>2014-08-08T18:17:56Z</updated><resolved>2014-08-08T18:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:17:56Z" id="51638752">Hi @roytmana 

The slow startup is due to the replicas being recovered from the primaries.  This can be improved by disabling allocation before restarting, but the main improvement will come by adding sequence IDs.

Closing in favour of #6069 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postings highlighter does not support wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4042</link><project id="" key="" /><description>Currently postings highlighter does not support wildcards and I suspect prefix queries as well - it would only highlight complete terms. 

Would it be possible to make it support at least trailing wildcards and prefix queries such as photo\* 
if not possible, the documentation should state that
</description><key id="21957550">4042</key><summary>Postings highlighter does not support wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">roytmana</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-11-01T13:43:03Z</created><updated>2013-11-06T00:03:32Z</updated><resolved>2013-11-04T11:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-01T13:57:50Z" id="27567012">Thanks for reporting this, we'll see what we can do about it. 
The documentation already states "Note that the postings highlighter is meant to perform simple query terms highlighting" but maybe that sentence needs to be made even clearer.
</comment><comment author="roytmana" created="2013-11-01T14:08:28Z" id="27567762">I read it as if it does not account for any boolean logic and phrases given the emphasis on that in the doc and missed wildcard aspect till i hit the issue.

Hopefully trailing wildcards/prefixes  will be easy :-)
</comment><comment author="roytmana" created="2013-11-04T16:47:07Z" id="27700822">Hi Luca,

Are trailing wildcard supposed to work? I built 0.9.7-SNAPSHOT as of an hour ago and it does not seem to work. Actually even regular terms seem to not highlight randomly I can't quite pin when it highlight and when not. Something seems to be broken?

Alex
</comment><comment author="javanna" created="2013-11-04T17:10:28Z" id="27702859">This is in the just released 0.90.6, no need to build from a snapshot. Just tested with 0.90.6 and saw trailing wildcard matches highlighted, as well as ordinary queries (e.g. match query).
I'd love to help if you saw issues but I'm going to need a curl recreation. Maybe you can open a new issue and describe what happens?
</comment><comment author="roytmana" created="2013-11-04T17:14:16Z" id="27703207">Luka,

Let me test 0.9.6 (it was not in maven couple oh hours ago yet) and if it
fails I will try to prepare the recreation late tonight.
Just wanted too check if it is supposed to work before trying to extract a
recreation

thanks,
Alex

On Mon, Nov 4, 2013 at 12:11 PM, Luca Cavanna notifications@github.comwrote:

&gt; This is in the just released 0.90.6, no need to build from a snapshot.
&gt; Just tested with 0.90.6 and saw trailing wildcard matches highlighted, as
&gt; well as ordinary queries (e.g. match query).
&gt; I'd love to help if you saw issues but I'm going to need a curl
&gt; recreation. Maybe you can open a new issue and describe what happens?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4042#issuecomment-27702859
&gt; .
</comment><comment author="roytmana" created="2013-11-05T18:27:03Z" id="27798731">Luka,

It does not work with 0.90.6 either. Just a very simple example (not exactly my problem of not highlighting at all but highlighting incorrectly):

```
curl  -XDELETE http://localhost:9200/test
curl  -XPOST http://localhost:9200/test -d'{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "ht": {
      "properties": {
        "name": {
          "type": "string",
          "index_options": "offsets"
        }
      }
    }
  }
}'

curl -XPOST "http://localhost:9200/test/_search" -d'
{
   "query": {
      "query_string": {
         "query": "name:photo*"
      }
   },
   "highlight": {
      "fields": {
         "name": {
            "type": "postings"
         }
      }
   }
}'

result:


{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "ht",
            "_id": "JxpSkfEwTJyJ5tiKDk951Q",
            "_score": 1,
            "_source": {
               "name": "photography"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;photography&lt;/em&gt;"
               ]
            }
         },
         {
            "_index": "test",
            "_type": "ht",
            "_id": "O-KeBwKpTxeXQdfvfc84mg",
            "_score": 1,
            "_source": {
               "name": "photo equipment"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;photo equip&lt;/em&gt;ment"
               ]
            }
         }
      ]
   }
}
```
</comment><comment author="clintongormley" created="2013-11-05T18:45:09Z" id="27800298">Yeah, I'm seeing weird stuff happening here too, eg:

```
POST /test/ht/_search
{
   "query": {
      "match": {
         "name": "equipment little"
      }
   },
   "highlight": {
      "fields": {
         "name": {
            "type": "postings"
         }
      }
   }
}


  "hits": [
     {
        "_index": "test",
        "_type": "ht",
        "_id": "2",
        "_score": 0.061554804,
        "_source": {
           "name": "photo equipment"
        },
        "highlight": {
           "name": [
              "photo &lt;em&gt;equipment&lt;/em&gt;"
           ]
        }
     },
     {
        "_index": "test",
        "_type": "ht",
        "_id": "1",
        "_score": 0.049243845,
        "_source": {
           "name": "littlexxxxxxx photograph equipment"
        },
        "highlight": {
           "name": [
              "little&lt;em&gt;xxxxxxx p&lt;/em&gt;hotograph equipment"
           ]
        }
     }
  ]
```

It's like the offsets are set on the first document, then not reset for subsequent documents.
</comment><comment author="javanna" created="2013-11-05T20:26:16Z" id="27809148">Indeed, this is odd. I'm looking into it and I'll open another issue later on as the problem is not only with wildcards.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Added support for highlighting multi term queries using the postings highlighter</comment></comments></commit></commits></item><item><title>On full cluster restart, node fails to detect master, then hangs for 10+ minutes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4041</link><project id="" key="" /><description>## Context

ES 0.90.2 on a cluster of 4 large Redhat machines (24 core, 96GB RAM). Multicast discovery disabled.
## Symptoms

When I do a full cluster restart, there is often one machine that fails to join the cluster properly, and hangs after the "started" log entry for 10 minutes or more, before retrying.

This is a _different_ machine (and a different master) each time, so I don't think it's a hardware fault.

Here's some log entries from a recent case. In this case, _lo1uspaldbs006.palomino.pearson.com_ was the master, and _lo1uspaldbs009.palomino.pearson.com_ was the confused slave.

The confused slave shows a sequence like this in its log:

```
[2013-11-01 11:00:08,718][DEBUG][discovery.zen.fd         ] [lo1uspaldbs009.palomino.pearson.com] [master] starting fault detection against master [[lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]]], reason [initial_join]
[2013-11-01 11:00:09,840][DEBUG][discovery.zen.fd         ] [lo1uspaldbs009.palomino.pearson.com] [master] pinging a master [lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]] but we do not exists on it, act as if its master failure
[2013-11-01 11:00:09,841][DEBUG][discovery.zen.fd         ] [lo1uspaldbs009.palomino.pearson.com] [master] stopping fault detection against master [[lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]]], reason [master failure, do not exists on master, act as master failure]
[2013-11-01 11:00:09,842][INFO ][discovery.zen            ] [lo1uspaldbs009.palomino.pearson.com] master_left [[lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]]], reason [do not exists on master, act as master failure]
```

However, the master shows no connection attempt from the slave during this time.

The slave then sits in the "started" stated, but responding only with MasterNotDiscoveredException, until some time later:

```
[2013-11-01 11:11:38,573][DEBUG][discovery.zen.fd         ] [lo1uspaldbs009.palomino.pearson.com] [master] restarting fault detection against master [[lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]]], reason [new cluster state received and we are monitoring the wrong master [null]]
[2013-11-01 11:11:38,599][INFO ][cluster.service          ] [lo1uspaldbs009.palomino.pearson.com] detected_master [lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]], added {[lo1uspaldbs007.palomino.pearson.com][ooTKn5ADRmG16KZk5BDS2w][inet[/10.162.45.36:9300]],[lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]],[lo1uspaldbs008.palomino.pearson.com][z3kkkvJOR3eeyoYeRf6Cpw][inet[/10.162.45.37:9300]],}, reason: zen-disco-receive(from master [[lo1uspaldbs006.palomino.pearson.com][eRv40ALrSrWDm7w6rdbmOA][inet[/10.162.45.35:9300]]])
```
## Clock sync issue?

I've noticed that the clocks are slightly out of sync in this cluster. Could this cause a problem? I know this is potentially problematic for some distributed databases.

(I accounted for this in trying to track down join attempts in the master log.)
## Relevant changes

I've checked the changelog since 0.90.2 and found only two potentially relevant-looking fixes, #3515 and #3361, of which the former seems like a red herring. The latter might be relevant; I'm hoping to upgrade our cluster and retest in mid/late November.
</description><key id="21952845">4041</key><summary>On full cluster restart, node fails to detect master, then hangs for 10+ minutes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels><label>feedback_needed</label></labels><created>2013-11-01T11:42:15Z</created><updated>2014-10-06T16:21:05Z</updated><resolved>2014-09-26T18:14:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-30T10:10:24Z" id="50596727">Hi @andrewclegg 

Sorry it has taken a while to get to this.  Are you still seeing these issues in recent releases?
</comment><comment author="clintongormley" created="2014-09-26T18:14:31Z" id="56999830">No more info received. Closing, please feel free to reopen if it is still a problem
</comment><comment author="andrewclegg" created="2014-10-06T16:21:05Z" id="58043279">Hey, sorry, I missed this when it came in. I actually moved to a different team, and the cluster in question was shut down, so I would have had no way of checking anyway. Sorry to waste your time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed bug that document with a suggest field can't be percolated.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4040</link><project id="" key="" /><description>The CompletionTokenStream doesn't properly forward the call to its attributes, so when the percolator needs to access terms of this stream, null was returned and this isn't expected in the MemoryIndex.

Relates to #4028
</description><key id="21949500">4040</key><summary>Fixed bug that document with a suggest field can't be percolated.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-11-01T10:06:32Z</created><updated>2015-05-18T23:33:41Z</updated><resolved>2013-11-08T17:14:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-01T10:16:35Z" id="27556944">good catch LGTM
</comment><comment author="javanna" created="2013-11-01T10:18:51Z" id="27557037">Maybe we can add a test for this using the recreation from #4028 ?
</comment><comment author="martijnvg" created="2013-11-01T10:20:22Z" id="27557111">@javanna Sure, I'll add one. I was just verifying if the fix was ok.
@s1monw Can the ByteTermAttributeImpl inner class in CompletionTokenStream be removed?
</comment><comment author="s1monw" created="2013-11-01T11:57:59Z" id="27561036">@martijnvg no you can't remove it otherwise there will be no impl for `ByteTermAttribute` this is how AttributeSource works though :)
</comment><comment author="martijnvg" created="2013-11-01T12:23:42Z" id="27562083">@s1monw Got it: because the impl ends with `Impl` suffix Lucene can find and instantiate it automatically.
</comment><comment author="s1monw" created="2013-11-05T19:36:29Z" id="27804818">@martijnvg any update on this? if you have a test I can look into it!
</comment><comment author="martijnvg" created="2013-11-07T14:25:05Z" id="27968396">For example: CompletionSuggestSearchTests#testSimple
I don't understand why my change result in no suggest options be returned.

On 5 November 2013 14:36, Simon Willnauer notifications@github.com wrote:

&gt; @martijnvg https://github.com/martijnvg any update on this? if you have
&gt; a test I can look into it!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/4040#issuecomment-27804818
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="s1monw" created="2013-11-07T20:44:25Z" id="28004572">@martijnvg I am rather looking for a test that shows that percolator fails so I check what is wrong there
</comment><comment author="s1monw" created="2013-11-08T17:14:31Z" id="28079901">superceded by https://github.com/elasticsearch/elasticsearch/pull/4129 (including the test)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PutMapping API now throws a MapperParsingException if the mapping's root node name is not equal to the type's name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4039</link><project id="" key="" /><description>Fixed tests which were using mapping with the wrong type name as root node.
Removed OverrideTypeMappingTests due to the removal of the functionality it tests.
Better naming for the default percolator mapping and change it's content use _default_ as root node.

Closes #4038
</description><key id="21948569">4039</key><summary>PutMapping API now throws a MapperParsingException if the mapping's root node name is not equal to the type's name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-11-01T09:36:55Z</created><updated>2014-06-17T23:38:57Z</updated><resolved>2013-11-07T22:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-11-04T13:03:50Z" id="27683175">+1 The changes look good to me!
</comment><comment author="javanna" created="2013-11-07T22:10:34Z" id="28011756">This one got merged right @bleskes ?
</comment><comment author="bleskes" created="2013-11-07T22:23:30Z" id="28012758">yes.. thx for catching this. Closing.

See https://github.com/elasticsearch/elasticsearch/commit/0ef2493b2c2ce7f5845736e1e5f501fd5ae1a182
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reject put mapping requests when the root node is not equal to `type`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4038</link><project id="" key="" /><description>Currently the put mapping api expects the root node of the body to be the `type` in question:

```
curl -XPUT "http://localhost:9200/index/craze_type/_mapping" -d'
{
    "crazy_type": {
        "properties": { ... }
    }

}'
```

Right now we verify the content has only one root node, but we do not check it's name. This means that the following get's accepted &amp;ignored while returning a `200 OK` status:

```
curl -XPUT "http://localhost:9200/index/craze_type/_mapping" -d'
{
    "properties": { ... }
}'
```

We should throw a `MapperParsingException` .
</description><key id="21948000">4038</key><summary>Reject put mapping requests when the root node is not equal to `type`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-11-01T09:19:04Z</created><updated>2013-11-05T11:37:25Z</updated><resolved>2013-11-05T11:37:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/test/java/org/elasticsearch/gateway/local/SimpleRecoveryLocalGatewayTests.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/overridetype/OverrideTypeMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetMappingsTests.java</file><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Throw an exception if a type's mapping root node is not equal to the type in question.</comment></comments></commit></commits></item><item><title>[DOC] Improve documentation on search stats groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4037</link><project id="" key="" /><description>Document the ability to return all search statistics groups and provide examples of returning search statistics for groups.
</description><key id="21939594">4037</key><summary>[DOC] Improve documentation on search stats groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">deverton</reporter><labels /><created>2013-11-01T02:33:22Z</created><updated>2014-07-16T21:51:43Z</updated><resolved>2013-11-01T12:56:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-01T12:56:38Z" id="27563584">I reworded it a bit and pushed: https://github.com/elasticsearch/elasticsearch/commit/6df60b7271ef96a9987a1178bf6f448e1276c7a2

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scan site for broken links</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4036</link><project id="" key="" /><description>There are tons of broken links to docs. E.g. if I click on "match_phrase_prefix" in http://www.elasticsearch.org/blog/starts-with-phrase-matching/ then it doesn't work (the correct link should be http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-match-query.html)
</description><key id="21937642">4036</key><summary>Scan site for broken links</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels /><created>2013-11-01T01:07:44Z</created><updated>2013-11-01T14:24:36Z</updated><resolved>2013-11-01T14:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-01T14:24:36Z" id="27568856">Thanks for reporting this, I just fixed the broken links on that specific article. There is some work being done around this and we already have an issue for it: #3964 .  Closing this one, you can report similar problems in #3964 .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add recovery stats to index stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4035</link><project id="" key="" /><description>Today, the index stats API is cleaner and can replace almost fully the index status API, what its missing is the recovery stats part. I suggest adding the recovery stats API to the index stats API as an option.
</description><key id="21929761">4035</key><summary>Add recovery stats to index stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Index APIs</label><label>feature</label></labels><created>2013-10-31T21:53:51Z</created><updated>2015-06-06T18:45:12Z</updated><resolved>2014-01-09T16:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2014-01-03T19:42:20Z" id="31547858">Taking a look per @s1monw 's request.
</comment><comment author="aleph-zero" created="2014-01-03T21:45:58Z" id="31556318">This is my understanding of what needs to be done.

Currently, the &lt;b&gt;'status'&lt;/b&gt; API (http://localhost:9200/_status or http://localhost:9200/{index}/_status) can report on the recovery status of shards if one passes the 'recovery=true' flag. This information is reported per-shard in a stanza such as:

```
  "shards" : {
    "1" : [ {
        ....
      "gateway_recovery" : {
        "stage" : "DONE",
        "start_time_in_millis" : 1388781755266,
        "time_in_millis" : 178,
        "index" : {
          "progress" : 100,
          "size_in_bytes" : 1238348243,
          "reused_size_in_bytes" : 1238348243,
          "expected_recovered_size_in_bytes" : 0,
          "recovered_size_in_bytes" : 0
        },
        "translog" : {
          "recovered" : 0
        }
      }
    } ]
  }
```

As I understand @kimchy 's request, we would like to include this information in the &lt;b&gt;'stats'&lt;/b&gt; API ((http://localhost:9200/_stats or http://localhost:9200/{index}/_stats).

To keep this API change simple and as consistent as possible with the existing functionality in the &lt;b&gt;'status'&lt;/b&gt; API, I propose we add &lt;b&gt;recovery&lt;/b&gt; as a parameter, but only make it active when 'level=shards' is set. 

An example request would be: curl -XGET 'localhost:9200/{index}/_stats/indexing?recovery=true&amp;level=shards'. The format of the information would be the same as the sample payload above.
</comment><comment author="aleph-zero" created="2014-01-03T23:23:50Z" id="31562541">Had a brief discussion with @kimchy. He'd like to know if we can bring in recovery information from the snapshot/restore feature. Will reach out to @imotov and @kimchy over voice. 
</comment><comment author="aleph-zero" created="2014-01-09T16:31:22Z" id="31949677">Closing issue in favor of https://github.com/elasticsearch/elasticsearch/issues/4637. This issue gives a more comprehensive treatment of cluster wide recovery status.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecate `numeric_range` filter, add a field data execution option to range filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4034</link><project id="" key="" /><description>`numeric_range` is very confusing, at the end, the question is if range filter should be done using the index itself, or using field data (in memory). I suggest deprecating `numeric_range`, and adding `execution` field to `range` filter, with possible values of `index` and `fielddata`. The `numeric_range` filter will be removed in the next major version after 1.0.

If its set to `fielddata`, we simply try and execute it using it (and if a mapper doesn't implement it, we simply fail the execution with unsupported exception).
</description><key id="21929068">4034</key><summary>Deprecate `numeric_range` filter, add a field data execution option to range filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-10-31T21:41:56Z</created><updated>2014-08-12T17:47:46Z</updated><resolved>2013-11-25T22:44:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-10-31T21:42:57Z" id="27531579">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/NumericRangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NumericRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Remove `numeric_range` filter</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/NumericRangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NumericRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/test/java/org/elasticsearch/count/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Added execution option to `range` filter, with the `index` and `fielddata` as values.</comment><comment>Deprecated `numeric_range` filter in favor for the `range` filter with `fielddata` as execution.</comment></comments></commit></commits></item><item><title>Remove text and field queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4033</link><project id="" key="" /><description>`text` query has been replaced by `match` query and been deprecate for some time.

`field` query is very confusing, its effectively the Lucene query string syntax on a specific field, but its not evident from the name (can be confuse with `match`). It would be much better to remove it, and ask people to use `query_string` with a properly set default field. This is much more explicit regarding what it does.
</description><key id="21928924">4033</key><summary>Remove text and field queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-10-31T21:39:24Z</created><updated>2014-05-20T13:33:09Z</updated><resolved>2013-12-16T16:09:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-02T18:32:58Z" id="27628106">++
</comment><comment author="clintongormley" created="2013-11-04T19:55:37Z" id="27717094">Also remove `mlt_field` in favour of just `mlt`, for the same reasons as `field`
</comment><comment author="jeacott" created="2014-03-14T08:33:31Z" id="37625962">I lament this decision, queryString is a very loose interface when trying to construct queries dynamically in Java. it seems we're now reduced to string concat ("field:value") instead of the much more robust fieldQuery(field, value).
what am I missing?
</comment><comment author="kimchy" created="2014-03-14T08:37:40Z" id="37626243">you can use match query on specific fields. If you still want to have the query string syntax that the field query supported, you can use query_string query, and set the default field on it (this is exactly what the field query was doing behind the scenes).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Remove the `field` and `text` queries.</comment></comments></commit></commits></item><item><title>Add suggest stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4032</link><project id="" key="" /><description>We have the suggest API, but we don't have specific statistics for the suggest API (how long it took to execute it on each shard, similar to query phase in search, or percolation).

Potentially, this can be safely backported to 0.90 as well.
</description><key id="21928717">4032</key><summary>Add suggest stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2013-10-31T21:35:53Z</created><updated>2014-03-28T10:14:35Z</updated><resolved>2014-03-28T10:14:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-31T21:36:28Z" id="27531134">+1
</comment><comment author="nik9000" created="2013-10-31T22:10:27Z" id="27533617">It'd be nice to be able to get the amount of time that is spent on suggestions executed during search as well. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/suggest/SuggestShardModule.java</file><file>src/main/java/org/elasticsearch/index/suggest/stats/ShardSuggestService.java</file><file>src/main/java/org/elasticsearch/index/suggest/stats/SuggestStats.java</file><file>src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>src/test/java/org/elasticsearch/index/suggest/stats/SuggestStatsTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/SimpleIndexStatsTests.java</file></files><comments><comment>Add suggest stats</comment></comments></commit></commits></item><item><title>Add tags to all "stats" related APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4031</link><project id="" key="" /><description>All APIs that end up contributing to stats, would be great to have a a common way to associate them with a group of tags. This is similar to the feature of "groups" we have in the search API, it just needs to be consistent across all the APIs. Then, one can get the stats for a specific tag (and it might be used in other places).

APIs that make sense to have it for: `index`, `delete`, `get`, `search`, `percolate`, `refresh` (and `suggest` if we add it).

Note, with refresh API, where it also executes in a scheduled manner, maybe it can execute "under" a tag called `_scheduled`.
</description><key id="21928007">4031</key><summary>Add tags to all "stats" related APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Stats</label><label>discuss</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-10-31T21:26:58Z</created><updated>2015-11-20T14:11:51Z</updated><resolved>2015-10-14T12:16:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T19:04:43Z" id="142079074">Nobody seems to have shown an interest in this issue. I wonder if we really need this? @kimchy ?
</comment><comment author="clintongormley" created="2015-10-14T12:16:57Z" id="148030819">Closing for now.  Can reopen if there is any interest
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cat/recovery API should also show relocations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4030</link><project id="" key="" /><description>It'd be super handy to also show the progress of relocation operations in the `_cat/recovery` output.
</description><key id="21920377">4030</key><summary>_cat/recovery API should also show relocations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label></labels><created>2013-10-31T19:32:48Z</created><updated>2015-06-07T16:24:57Z</updated><resolved>2014-05-09T21:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2014-05-09T21:13:35Z" id="42714817">Hey @dakrone - both the cat and json versions of the recovery api show all shard movements in the cluster, including relocations. Okay by you if I close this?
</comment><comment author="dakrone" created="2014-05-09T21:50:37Z" id="42717781">@aleph-zero sounds good, closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move systemd files from /etc to /usr/lib</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4029</link><project id="" key="" /><description>As documented in systemd's manual pages tmpfiles.d(5) and systemd.unit(5),
a package should install its default configuration in /usr/lib, which can
be overridden by system administrators in /etc.

New locations in the rpm:
/usr/lib/systemd/system/elasticsearch.service
/usr/lib/tmpfiles.d/elasticsearch.conf

The man pages are available online:
http://www.freedesktop.org/software/systemd/man/tmpfiles.d.html
http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Unit%20Load%20Path
</description><key id="21909452">4029</key><summary>Move systemd files from /etc to /usr/lib</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Dridi</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v0.90.13</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-10-31T16:48:30Z</created><updated>2015-06-07T16:25:10Z</updated><resolved>2014-03-17T13:09:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="obourgain" created="2014-01-02T14:17:16Z" id="31454781">+1
</comment><comment author="Dridi" created="2014-01-13T18:39:11Z" id="32197312">Hi,

In case you'd be wondering, I have signed the CLA before sending this PR and you don't need to be a systemd expert to merge it.

I've noticed a new systemd file in 81e13a8, and this time it lands in `/usr/lib/sysctl.d` as expected and can be overridden by sysadmins in `/etc/sysctl.d`. This is the same idea for this PR.

If you don't feel comfortable with systemd, I'll gladly answer your questions.
</comment><comment author="drewr" created="2014-03-07T16:35:59Z" id="37041204">:+1: 
</comment><comment author="spinscale" created="2014-03-07T17:53:56Z" id="37049377">I will get this in next week, just need to test with opensuse/redhat to make sure everything works  (yes I know, it should work flawless)
</comment><comment author="s1monw" created="2014-03-17T13:01:21Z" id="37812161">LGTM
</comment><comment author="s1monw" created="2014-03-17T13:01:43Z" id="37812193">as discussed I guess this should go to all branches?
</comment><comment author="spinscale" created="2014-03-17T13:09:10Z" id="37812867">merged via https://github.com/elasticsearch/elasticsearch/commit/9500dddad3931afae04df39dc02c960ac8ac5018

thanks for your patience!
</comment><comment author="Dridi" created="2014-03-17T13:58:45Z" id="37817708">Glad I could help, even with a trivial fix :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE when percolating a document that contains a completion field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4028</link><project id="" key="" /><description>Although it might not make much sense to percolate a document containing a completion field, that's what you end up doing if you percolate while indexing, and your mapping contains a completion field. When adding the completion field to the memory index a NullPointerException is thrown. 

This happens with the 0.90 branch. Happens also with master, at least percolating an existing document.

Here is the recreation:

```
curl -XPUT localhost:9200/hotels -d '
{
  "mappings": {
    "hotel" : {
      "properties" : {
        "name" : { "type" : "string" },
        "city" : { "type" : "string" },
        "name_suggest" : {
          "type" :     "completion"
        }
      }
    }
  }
}'

curl -XGET localhost:9200/hotels/hotel/_percolate -d '{
    "doc" : {
        "name" :         "Mercure Hotel Munich",
        "city" :         "Munich",
        "name_suggest" : "Mercure Hotel Munich"
    }
}
'
```

Here is the stacktrace:

```
java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:463)
    at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:370)
    at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:450)
    at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:422)
    at org.elasticsearch.index.percolator.PercolatorService.percolate(PercolatorService.java:111)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:93)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:41)
    at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.util.BytesRefHash.add(BytesRefHash.java:274)
    at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:437)
    ... 10 more
```
</description><key id="21909025">4028</key><summary>NPE when percolating a document that contains a completion field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-10-31T16:42:14Z</created><updated>2013-11-10T19:46:06Z</updated><resolved>2013-11-10T19:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionTokenStream.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionTokenStreamTest.java</file></files><comments><comment>Lazily fill CharTermAttribute if needed in CompletionTokenStream</comment></comments></commit></commits></item><item><title>Fixed ack behaviour when no ack is expected from any node or timeout is set to 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4027</link><project id="" key="" /><description>We now return acknowledged true when no wait is needed (mustAck always returns false). We do wait for the master node to complete its actions though. Previously it would try to timeout and hang due to a CountDown#fastForward call when the internal counter is set to 0

We now return acknowledged false without starting the timeout thread when the timeout is set 0, as starting the wait and immediately stopping the thread seems pointless.

Added coverage for ack in ClusterServiceTests
</description><key id="21900718">4027</key><summary>Fixed ack behaviour when no ack is expected from any node or timeout is set to 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-10-31T14:47:36Z</created><updated>2014-07-16T21:51:44Z</updated><resolved>2013-11-01T17:29:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-01T17:17:17Z" id="27582439">LGTM
</comment><comment author="javanna" created="2013-11-01T17:29:38Z" id="27583348">Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bound the number of search results returned by elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4026</link><project id="" key="" /><description>Hello,

When making a search request (post) like the following to elasticsearch using kibana I get a java heap space error and my elasticsearch node can't recover. 

{"query":{"filtered":{"query":{"bool":{"should":[{"query_string":{"query":"*"}}]}},"filter":{"bool":{"must":[{"match_all":{}},{"bool":{"must":[{"match_all":{}}]}}]}}}},"highlight":{"fields":{},"fragment_size":2147483647,"pre_tags":["@start-highlight@"],"post_tags":["@end-highlight@"]},"size":&lt;b&gt;1000000&lt;/b&gt;,"sort":[{"_id":{"order":"desc"}}]}

&lt;b&gt;Question&lt;/b&gt;: Is it possible to somehow restrict the maximum value of "size" that someone can use? In general, Is it possible to bound the number of results returned by elasticsearch to avoid out of memory errors?

I don't want to increase the heap size (currently 1gb) as this would not solve the problem.

Regards,
Nick
</description><key id="21899932">4026</key><summary>Bound the number of search results returned by elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicktgr15</reporter><labels><label>adoptme</label></labels><created>2013-10-31T14:36:54Z</created><updated>2015-01-30T11:39:22Z</updated><resolved>2015-01-30T11:39:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nicktgr15" created="2013-11-01T15:35:23Z" id="27574346">It looks like the reason why the nodes were unable to recover was the fact that the cluster was getting into a split brain state (multiple master nodes). 

In general I don't think that there is a way to limit the number of results returned by a query. 
</comment><comment author="javanna" created="2013-11-04T15:27:55Z" id="27693356">We plan to have something called "circuit breaker" that allows to prevent queries from bringing down a node if there is not enough memory. The related issue is #2929 .
In your case your size is way too high though, thus I would suggest to just lower it to a reasonable amout of documents.
</comment><comment author="clintongormley" created="2014-09-05T10:32:14Z" id="54609084">Rather than adding a setting specifically to limit the size of the priority queue, we should aim to limit the amount of memory used by a request, and how long a request can run.  This potentially allows admins to specify different policies for different users.

First step is to add the priority queue to the circuit breaker.
</comment><comment author="bobbyhubbard" created="2014-09-23T18:13:41Z" id="56564548">Just to be clear, #5466 addresses a bug related to specifying a size above 999999 that causes significant performance degradation. The size of the index and memory consumed seem to have absolutely nothing to do with the issue. i can reproduce this bug even with an index with 1 tiny document in it... so it can't be related to loading a huge result set in memory. For example in our production ES cluster (v1.1.1):

```
PUT sizebugtest/nada/1
{
  "key":"value"
}
PUT sizebugtest/nada/2
{
  "key":"value2"
}

#returns both documents in 2-3ms
GET sizebugtest/nada/_search?   
#returns both documents in 3-5ms
GET sizebugtest/nada/_search?size=999999
#returns both documents in 8-25ms
GET sizebugtest/nada/_search?size=9999999
#returns both documents in 50-100ms
GET sizebugtest/nada/_search?size=99999999
#returns both documents in 7000-30,000ms!! Somestimes times out. same 2 documents!
GET sizebugtest/nada/_search?size=999999999
#400 - Awesome...no longer an int...!
GET sizebugtest/nada/_search?size=9999999999
```

Why the significant difference in response time for same index simply by specifying a different size? That seems like a different issue from what #4026 addresses imo.
</comment><comment author="clintongormley" created="2014-09-23T18:15:44Z" id="56564851">@bobbyhubbard no they are related.  specifying a large size (or a high `from` offset) means creating a large priority queue.  By adding the size of the priority queue to the circuit breaker, we can abort the search if too much memory is required to service the request.  That's a good generic solution instead of having a separate setting for each little part of the request.
</comment><comment author="bobbyhubbard" created="2014-09-23T18:16:42Z" id="56564985">Ah ok. BTW - I just upgraded our dev environment to 1.3.2 to confirm if this was still an issue or not. In production running 1.1.1 I can reproduce it all day long using the test case above. However, I cannot reproduce it against 1.3.2.
</comment><comment author="clintongormley" created="2015-01-30T11:39:22Z" id="72189022">Closing in favour of #9311
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Randomize exceptions when file is deleted while still open.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4025</link><project id="" key="" /><description>This somehow emulates a behavior on windows but we should test the
other code paths as well.
</description><key id="21898316">4025</key><summary>Randomize exceptions when file is deleted while still open.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-31T14:12:48Z</created><updated>2014-07-16T21:51:44Z</updated><resolved>2013-11-01T10:50:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-01T10:50:19Z" id="27558341">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Guarantee sorted order for [Double|Long|Bytes]Values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4024</link><project id="" key="" /><description>Values returned by [Double|Long|Bytes]Values are sorted today which
is guaranteed by the underlying Lucene index. Several implementations can
make use of this property but the interfaces don't guarantee this behavior.
This commit adds the guarantees and makes use of them in several places.

Note: This change might require sorting for 3rd party implemenations of these
interaces.
</description><key id="21895402">4024</key><summary>Guarantee sorted order for [Double|Long|Bytes]Values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-31T13:22:16Z</created><updated>2014-07-16T21:51:44Z</updated><resolved>2013-11-07T09:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Field data can take up way more heap than the configured limit, in practice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4023</link><project id="" key="" /><description>I had a memory issue on a server, so took a heap dump with jmap and loaded it into MAT for analysis.

I found the culprit was IndicesFieldDataCache: http://i.imgur.com/su2jsMF.png

Even though it was set to a 50% limit, and was reporting its own size as 40GB / 50% of heap (see attributes in left side of screenshot), it was actually taking up 67GB / 80% of heap (see top of table in right side of screenshot).

I suspect something's wrong in the cache size calculations.

This is with 0.90.2, with no time-based expiry, or fancy field data options like filtering.
</description><key id="21893202">4023</key><summary>Field data can take up way more heap than the configured limit, in practice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels /><created>2013-10-31T12:37:10Z</created><updated>2014-08-09T19:27:52Z</updated><resolved>2014-08-08T18:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-10-31T12:53:59Z" id="27483112">Hi @andrewclegg 

Any chance you could see if you can replicate this in 0.90.5 or even the 0.90 branch?  There were several changes to fielddata since 0.90.2. I'm hoping that one of them fixed this

clint
</comment><comment author="andrewclegg" created="2013-10-31T13:22:04Z" id="27484810">Sure, I'll give it a go in a sandbox machine with a more recent release. I won't be able to easily test on our full staging environment yet, as this would require a big redeploy, plus rebuilding the application that runs on that cluster with the newer libraries.
</comment><comment author="clintongormley" created="2013-10-31T13:23:44Z" id="27484921">Thanks Andrew.  Would also be really useful to know if it happens with eg strings,  numbers or both, single or multi values etc.
</comment><comment author="andrewclegg" created="2013-10-31T15:01:05Z" id="27493044">First observation: the data seems to largely consist of PagedBytesAtomicFieldData containing SinglePackedOrdinals.
</comment><comment author="andrewclegg" created="2013-10-31T15:12:00Z" id="27493973">It's going to be a while before I get a chance to test this properly using 0.90.5 or greater, sadly. Our sandboxed dev machine just doesn't have enough data on it to recreate similar conditions to what we saw on the big cluster, but upgrading the big cluster to 0.90.5 is quite a big job for various reasons.

I'm travelling a lot over the next couple of weeks, but will hopefully get a chance to do an upgrade sometime in mid-November, and then re-test.
</comment><comment author="clintongormley" created="2013-11-05T16:22:17Z" id="27787241">I've tried this with single and multi-value integer and string fields, with fielddata cache set to an absolute value and a percentage. I've tried hanging on to old segments with a scroll while optimizing to new segments.

...it all works correctly.

How are you setting the cache size? with `indices.fielddata.cache.size`?  or with the pre-0.90 option: `index.cache.field.max_size`?
</comment><comment author="clintongormley" created="2013-11-05T16:40:29Z" id="27789138">Still doesn't explain why the reported field cache was different from the actual size, unless the actual size included data which hadn't been GC'ed yet.
</comment><comment author="bleskes" created="2013-11-06T09:08:49Z" id="27852935">It would also help if we knew the field types which are cached and their mapping.  Any chance you can share this? also, a break down by field size via `curl -XGET 'http://localhost:10000/_stats/fielddata/*'` might help
</comment><comment author="andrewclegg" created="2013-11-22T16:06:20Z" id="29084045">Sorry for the slow replies on this....

The cache was configured with:

indices.fielddata.cache.size: 50%

This would have been with a mixture of string fields (single-valued, unanalyzed) and datetimes i.e. longs. Because our big queries are all about doing date histograms, and counting uniques of various string based fields.

(We use our own plugin for this -- but we don't do anything especially unusual with field data.)

Actual size _shouldn't_ (in theory at least) include un-GC'ed data, as MAT is supposed to trace live paths from root points the same way the GC would.

Here's the fielddata stats on our live cluster currently, NB I've taken the cache size down from 50% to 30%.

https://gist.github.com/andrewclegg/6c771ef35b4c3fa91fea

And here's the mappings for one of the types. We have many different types, but all with quite similar mappings.

https://gist.github.com/andrewclegg/bedee4da10176676f2bc

I spoke to @kimchy recently and he said it might be to do with the FST data structures, which are held on to but not included in the size calculations? (I think. I was slightly drunk by then...)
</comment><comment author="clintongormley" created="2014-08-08T18:15:29Z" id="51638419">Hi @andrewclegg 

Is this still an issue? We haven't seen any reports about this recently.  Please could you reopen if it is.

thanks
</comment><comment author="andrewclegg" created="2014-08-09T19:27:52Z" id="51696119">Thanks chaps, I kind of lost track of this one as I moved to another team
which doesn't use ES as much, and no longer have access to the cluster that
we observed this on. But it was a fairly old version so I'm more than happy
to assume that recent changes will have fixed this.

On Friday, 8 August 2014, Clinton Gormley notifications@github.com wrote:

&gt; Closed #4023 https://github.com/elasticsearch/elasticsearch/issues/4023.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/4023#event-150872800
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Way to recieve '_node' for hits without explain option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4022</link><project id="" key="" /><description>Hi,
I need to have information which node hit received from.
Exactly the same as "explain" does. But without "explanation", same manner an "_index" field.

The way I found is to change  InternalSearchHit.java:

@Override
public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
        builder.startObject();
        if (explanation() != null) {
            builder.field("_shard", shard.shardId());
            builder.field("_node", shard.nodeIdText());
        }

just move  builder.field("_node", shard.nodeIdText()); out from braces (same as for index name).

But I'm looking for some method to do this without code modification.
 E.g.:
1. some access from script field,
2. or access by some plugin,
3. or the best from my opinion is to support _shard, _node, _index and other "shard" object properties in "fields" clause.
</description><key id="21892224">4022</key><summary>Way to recieve '_node' for hits without explain option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ashpynov</reporter><labels><label>feedback_needed</label></labels><created>2013-10-31T12:14:40Z</created><updated>2014-09-26T18:13:46Z</updated><resolved>2014-09-26T18:13:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:13:58Z" id="51638159">Hi @ashpynov 

What's your use case for this?
</comment><comment author="clintongormley" created="2014-09-26T18:13:46Z" id="56999713">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve new field mapping introduction performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4021</link><project id="" key="" /><description>Improve the introduction of new fields into the concrete parsed mappings by not relying on immutable maps and copying over entries, but instead using open maps (which will also use less memory), and using clone to perform the copy on write logic
</description><key id="21872740">4021</key><summary>Improve new field mapping introduction performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2013-10-31T02:11:46Z</created><updated>2014-07-16T21:51:45Z</updated><resolved>2013-10-31T10:09:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-31T07:59:20Z" id="27467957">LGTM.
I was looking at whether pre-allocating the maps before copying would help but cloning is quite fast and most likely offset the cost of 'growing' the map.
</comment><comment author="s1monw" created="2013-10-31T08:24:43Z" id="27469090">I think the idea is great but I really don't like the fact that we move from an immutable data-structure to a mutable one. I think it is worth encapsulating the entire String -&gt; FieldMapper mapping into a dedicated class that takes care of all the publishing etc. and optimizes internally. This would also make it unittestable!
</comment><comment author="kimchy" created="2013-10-31T08:45:23Z" id="27469938">@s1monw we already have unit tests for the mappers, so am not concerned about testing specifically. Also, the scope of the String-&gt;FieldMappers is encapsulated nicely within the DocumentFieldMappers class, and its pretty simple, and it makes sure not to change it, I don't see a reason to add another class just for it.
</comment><comment author="kimchy" created="2013-10-31T09:44:56Z" id="27472889">@s1monw ok, I think we can create a nice encapsulation across the name/index_name/full_name part, and it will also abstract the immutability part, I will update the pull request.
</comment><comment author="s1monw" created="2013-10-31T10:00:10Z" id="27473755">this second commit looks much better I think we can add further cleanups later. LGTM go ahead and push
</comment><comment author="kimchy" created="2013-10-31T10:09:17Z" id="27474255">pushed to master / 0.90.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add heap used percentage to _cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4020</link><project id="" key="" /><description>While we're printing mem information, include the heap ratio for quick trouble-spotting. Let's do this for now instead of a separate `heap` endpoint like with es2unix.
</description><key id="21848779">4020</key><summary>Add heap used percentage to _cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-10-30T18:41:21Z</created><updated>2013-10-30T18:50:48Z</updated><resolved>2013-10-30T18:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file></files><comments><comment>Add heap used percentage to _cat/nodes.</comment></comments></commit></commits></item><item><title>Fix bug in TransportShardReplicationOperationAction retry mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4019</link><project id="" key="" /><description>This issue was causing some index requests against shards in POST_RECOVERY state to hang.
</description><key id="21847575">4019</key><summary>Fix bug in TransportShardReplicationOperationAction retry mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-30T18:26:27Z</created><updated>2014-06-14T14:33:01Z</updated><resolved>2013-10-30T20:51:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-30T18:31:07Z" id="27423982">+1, looks great!
</comment><comment author="s1monw" created="2013-10-30T18:48:59Z" id="27426675">+1 LGTM @imotov I will add a test that really shuts down nodes etc. right next to this test class and fix the naming etc. in a different PR. I think both tests are important, what do you think?
</comment><comment author="imotov" created="2013-10-30T20:35:44Z" id="27436553">@s1monw yes, I agree. I think we should have both tests. 
</comment><comment author="imotov" created="2013-10-30T20:51:41Z" id="27437887">Pushed to 0.90 and master
</comment><comment author="bleskes" created="2013-10-31T09:19:03Z" id="27471563">A bit late to the party here, but there is a problem I saw this morning that causes the timeout not to be honored. Removing the cluster state listener and then allowing the performOnPrimary to add it again is problematic because the timeout handler tries one last time before failing. That one last attempt succeeds as far as the listener can tell (== scheduled a call to performOnPrimary), only to fail again on the performOnPrimary, which adds a new listener and the story goes again.

See:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java?source=c#L522
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ignore slow log configuration on shard creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4018</link><project id="" key="" /><description>In case of a misconfigured slow search/index configuration (unparseable
TimeValue) an exception is thrown.

This is not a problem when creating a shard of an index, as an exception
is returned and all is good. However, this is a huge problem, when
starting up a node, as the shard creation is repeated endlessly.

This patch changes the behaviour to go on as usual and just disable the
slowlog, as an improper configuration of logging should not affect the
allocation behaviour.

Closes #2730
</description><key id="21838086">4018</key><summary>Ignore slow log configuration on shard creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-30T16:27:18Z</created><updated>2014-07-16T21:51:46Z</updated><resolved>2013-11-19T14:55:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi field: accessing explicitly full path property by just its name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4017</link><project id="" key="" /><description>hey, while doing some tests i found that if I have a multi field type and a non default field name is actually unique in the index, it can be accessed by its name only.

example: 

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/bar/_mapping -d '{ "bar": { "properties": { "content": { "type": "multi_field", "path": "full", "fields": { "content": { "analyzer": "standard", "type": "string" }, "content_b": { "analyzer": "keyword", "type": "string" } } } } } }'
curl -XPUT http://localhost:9200/foo/bar/1 -d '{"id":1, "content":"foo bar"}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{"facets":{ "content_b": { "terms": { "field":"content_b"}}}}'
```

Note that according to the docs, i should have to use "content.content_b".

However, if at a later stage, i update the mappings with a conflicting name(in this case, content_b), then I get the expected behavior(having to access that as content.content_b).

It doesn't break anything for me, so not really important. But this could be confusing and lead to some weird errors(specially in the case of 2 multi fields having 2 properties with the same name. which one is accessed directly?). So maybe this could either be "enforced" or documented?
- i tested that with 0.90.5 and also with latest from 0.90 branch
</description><key id="21837904">4017</key><summary>Multi field: accessing explicitly full path property by just its name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-10-30T16:25:03Z</created><updated>2014-08-08T18:10:16Z</updated><resolved>2014-08-08T18:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:10:16Z" id="51637713">Multi-fields have been replaced, and we're planning on making all paths absolute #4081. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ip and node name to _cat/recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4016</link><project id="" key="" /><description>Need these as it greatly improves readability.
</description><key id="21836892">4016</key><summary>Add ip and node name to _cat/recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-10-30T16:14:12Z</created><updated>2013-10-30T17:38:38Z</updated><resolved>2013-10-30T17:38:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file></files><comments><comment>Add node name &amp; ip to _cat/recovery.</comment></comments></commit></commits></item><item><title>Simplify FieldData API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4015</link><project id="" key="" /><description>Removing #getValue and #hasValue to have a simple and consistent API
for multiple values.
</description><key id="21833933">4015</key><summary>Simplify FieldData API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-30T15:40:20Z</created><updated>2014-07-16T21:51:46Z</updated><resolved>2013-10-31T09:26:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-30T16:38:38Z" id="27409070">+1, looks great!
</comment><comment author="uboness" created="2013-10-30T17:47:46Z" id="27417946">@s1monw this is a blessing :) +1 !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create _cat/allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4014</link><project id="" key="" /><description>Add endpoint to visualize shard distribution across cluster.
</description><key id="21831539">4014</key><summary>Create _cat/allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-10-30T15:13:44Z</created><updated>2013-10-30T15:14:26Z</updated><resolved>2013-10-30T15:14:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-10-30T15:14:18Z" id="27397806">Added in f16eb7a243185a0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RegexpQueryBuilder now implements MultiTermQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4013</link><project id="" key="" /><description>This allows the RegexpQueryBuilder to be used in span queries

Added tests for all span multi term queries.
Also updated the documentation and removed mentioning of numeric range
queries for span queries (they have to be terms).

Closes #3392
</description><key id="21825295">4013</key><summary>RegexpQueryBuilder now implements MultiTermQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-30T13:43:07Z</created><updated>2014-06-26T23:11:09Z</updated><resolved>2013-10-31T08:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-30T16:29:06Z" id="27407631">LGTM - thanks!
</comment><comment author="uboness" created="2013-10-30T16:45:06Z" id="27409930">  ++1 awesome!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Should NodeBuilder be more specific about client, local or data?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4012</link><project id="" key="" /><description>Maybe this is just me but I'm not a big fan of setting boolean flag. For example, it's like this in NodeBuilder

``` java
/**
     * Is the node a local node. A local node is a node that uses a local (JVM level) discovery and
     * transport. Other (local) nodes started within the same JVM (actually, class-loader) will be
     * discovered and communicated with. Nodes outside of the JVM will not be discovered.
     *
     * @param local Should the node be local or not
     */
    public NodeBuilder local(boolean local) {
        settings.put("node.local", local);
        return this;
    }

```

Shouldn't it be more specific such as 

``` java
    public NodeBuilder local() {
        settings.put("node.local", true);
        return this;
    }
```

And then we have default value to be put in node() method as I'm confused every time what flag go against what function. Just an idea to discuss with the community and to learn as well. If it's ok I can do a pull request.

Thanks.
</description><key id="21820201">4012</key><summary>Should NodeBuilder be more specific about client, local or data?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">noppanit</reporter><labels /><created>2013-10-30T12:00:04Z</created><updated>2014-09-05T10:18:32Z</updated><resolved>2014-09-05T10:18:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-05T10:18:32Z" id="54607899">This would break our convention in builders etc. where a method named after a property with parameters is a setter and without any parameters is a getter. Having a setter with no parameters would be confusing given this convention.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Also resend shard started message if shard state is in POST_RECOVERY and master thinks it's initializing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4011</link><project id="" key="" /><description>Closes #4009
</description><key id="21815828">4011</key><summary>Also resend shard started message if shard state is in POST_RECOVERY and master thinks it's initializing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-10-30T10:28:59Z</created><updated>2014-06-18T05:56:39Z</updated><resolved>2013-10-30T10:45:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-30T10:37:29Z" id="27378840">looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add back hashed BytesValues optimization that got lost in a previous commit.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4010</link><project id="" key="" /><description>Some FieldData consumers require hash values per byte. We provide an optimization
that allows to cache the hashes internally if the consumer knows that they are needed
this optimization got lost in a previous commit. This commit adds them back and folds
the dedicated method into AtomicFieldData#getBytesValues(true|false)
</description><key id="21815812">4010</key><summary>Add back hashed BytesValues optimization that got lost in a previous commit.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-30T10:28:32Z</created><updated>2014-07-16T21:51:47Z</updated><resolved>2013-10-30T10:48:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-30T10:34:50Z" id="27378697">LGTM, +1
</comment><comment author="s1monw" created="2013-10-30T10:48:30Z" id="27379461">closed via https://github.com/elasticsearch/elasticsearch/commit/454dc53483c69a59090bf1c50bb2708e480c5858
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Not resending shard started messages when shard state is POST_RECOVERY and master died before processing the previous one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4009</link><project id="" key="" /><description>When a node completes recovering a shard, it changes the shard status to `POST_RECOVERY` and sends a shard started message to the master. The master processes it and send a new cluster state which cause the shard to be moved `STARTED`.

If the master dies before processing that message, the message needs to be resent to the new master. We already have a mechanism in place for that but it needs to be extended to cover the `POST_RECOVERY` state.
</description><key id="21815499">4009</key><summary>Not resending shard started messages when shard state is POST_RECOVERY and master died before processing the previous one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-30T10:22:05Z</created><updated>2013-10-30T10:44:40Z</updated><resolved>2013-10-30T10:44:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Also resend shard started message if shard state is in POST_RECOVERY and master thinks it's initializing.</comment></comments></commit></commits></item><item><title>FilterBuilders.termsFilter and minimum_should_match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4008</link><project id="" key="" /><description>Apparently it is not possible to set the `minimum_should_match` property when using a Terms Filter (it is possibile with a Terms Query).

Any plans to support `minimum_should_match` with Terms Filters?
</description><key id="21799576">4008</key><summary>FilterBuilders.termsFilter and minimum_should_match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thefosk</reporter><labels /><created>2013-10-30T01:10:27Z</created><updated>2013-11-08T19:41:59Z</updated><resolved>2013-11-08T19:41:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-08T19:41:58Z" id="28091594">Hi @thefosk 

No, no plans to support minimum_should_match with filters. Filters like the terms filter are implemented via bitsets, which is what makes them fast and memory efficient.  To implement minimum_should_match, we'd have to keep track of how many filters matched which will have an impact on performance.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Breaking compatibility changes for 1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4007</link><project id="" key="" /><description>Since 1.0 is the next large release, we have the opportunity to make breaking changes with regard to the API that we would not otherwise. This gives us a good chance to remove things that may have been deprecated for a long time, or rename confusing settings to names that make more sense.

This issue is for gathering suggestions from any of the community for settings that should be removed/renamed or otherwise changed in a breaking manor for the 1.0 release.

Here are some suggestions to get us started:
- Remove the `text` query (it was renamed `match` in 0.19.9)
- Rename `zen.minimum_master_nodes` to something like `zen.minimum_eligible_master_nodes`
- Unify edit distance naming (@s1monw can chime in on this one?)
- Rename `cluster.routing.allocation.disable_allocation` -&gt; `cluster.routing.allocation.allow_allocation` (double negatives are confusing)
- Changing fields element handling in search/get: `_source.X` returns X from _source, `_doc.X` returns X from field data, without auto detecting scripts, thats what `script_fields` are for...

Please chime in with suggestions and opinions about any other breaking changes!
</description><key id="21794244">4007</key><summary>Breaking compatibility changes for 1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-10-29T22:56:42Z</created><updated>2014-03-30T22:09:04Z</updated><resolved>2013-12-17T16:54:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="grantr" created="2013-10-30T01:32:25Z" id="27358802">Consistent naming for `/_cluster/state` and `/index/_status`. I never remember which is state and which is status.
</comment><comment author="kimchy" created="2013-10-30T01:36:38Z" id="27358943">Index Status API: remove the API, almost all can be gathered with index stats, all we need is to add dedicated recovery stats.
</comment><comment author="brwe" created="2013-10-31T08:02:58Z" id="27468118">Remove `custom_score`, `custom_filters_score` and `custom_boost`, is replaced by `function_score` now. But I am not sure about that because it was deprecated only recently.
</comment><comment author="uboness" created="2013-10-31T09:02:34Z" id="27470773">can we maybe also get rid of the `clear` query param for the stats api?
</comment><comment author="bleskes" created="2013-10-31T20:37:17Z" id="27525648">Put Mapping API forces you to specify a `type` in the url but also in the body of the request (as root key):

```
curl -XPUT "http://localhost:10000/index/type/_mapping" -d'
{
    "type": {
        ...
    }
}'
```

Remove the specification of `type` in the body and rely on url only? (potentially allow calling `_mapping` on an index with multiple types in the body)
</comment><comment author="jpountz" created="2013-10-31T20:44:24Z" id="27526844">The byte and short types are indexed and stored as integers. We used to have different field data impls that would use less memory for these types but the current field data impl computes the number of required bits per value dynamically. Maybe we could remove these types or find a way to make them only aliases of the integer type.
</comment><comment author="uboness" created="2013-10-31T23:01:18Z" id="27536377">implement a dedicated fielddata for boolean type and get rid of the `T` &amp; `F` values in favor of native `true` &amp; `false` returned by facets/aggs or any other field data related api
</comment><comment author="javanna" created="2013-11-04T18:41:26Z" id="27710720">Prevent people from deleting all indices without specifying any index.
The following should either do nothing or return an error:

```
curl -XDELETE locahost:9200
```

While the following actually deletes all indices:

```
curl -XDELETE localhost:9200/_all
```

Pretty much the same as we already did in open/close index api a while ago when the support for multiple indices was introduced.
</comment><comment author="nz" created="2013-11-19T15:40:16Z" id="28800063">Disambiguation of various settings keys. For example, index creation:
- `{"settings":{"index":{"number_of_shards":2}}}`
- `{"settings":{"index.number_of_shards":2}}}`
- `{"settings":{"number_of_shards":2}}}`

All of these are valid. While flexibility is a noble goal, it can also be unnecessarily ambiguous.

For this specific case, the docs mention in an aside:

&gt; Note you do not have to explicitly specify index section inside settings section.

In general, if something isn't required, and don't have a strong reason to exist, then it should just be eliminated.

Another suggestion along these lines, `number_of_shards` and `number_of_replicas` seems unnecessarily verbose. There may be some internal reason for those key names, but as a user I'd rather use, simply, `shards` and `replicas`.

```
curl localhost:9200/staging -d '{"settings":{"shards":2,"replicas":1}}'
```
</comment><comment author="andrewvc" created="2013-11-26T21:44:31Z" id="29337023">Remove `fuzzy` query. The match query is a superset of its functionality as near I can tell. The `fuzzy` behavior, which does not run an analyzer by default, is confusing.
</comment><comment author="Mpdreamz" created="2013-11-29T10:29:58Z" id="29507841">Referencing a document in query/filters does not quite follow the same syntax everywhere:

i.e the terms filter lookup:

```
    "id": "NEST",
    "type": "elasticsearchprojects",
    "index": "mydefaultindex",
    "path": "name"
```

vs the geo shape indexed filter:

```
    "id": "NEST",
    "type": "elasticsearchprojects",
    "index": "mydefaultindex",
    "shape_field_name": "name"
```
</comment><comment author="nz" created="2013-12-02T15:46:46Z" id="29628182">Lazy index creation should be opt-in.

Creating a simple index with default settings and dynamic mappings is trivial enough that it shouldn't hinder the early experience for new developers. On the other hand, lazy index creation can cause notable problems and confusion when later deploying to production.

A recent thread on the Elasticsearch mailing list: [A cautionary tale about index autocreation](https://groups.google.com/forum/#!topic/elasticsearch/L07E8A1_S-0)

At [bonsai.io](https://bonsai.io/), lazy index creation behavior is probably our largest source of support questions.
</comment><comment author="nik9000" created="2013-12-04T15:45:42Z" id="29816002">Just wanted to make sure that all these changes will be accessible in 0.90.X before upgrading to 1.0.  It'd be much more exciting if I had to time a release of my software with an upgrade of Elasticsearch.

Removing stuff is easy.  I'll stop using it before the release.  Just make sure there is a deprecation warning on all the 0.90.X documents.
Renaming parameters isn't too bad because I'll convince puppet to add both names to the file for the time period around the upgrade.
Breaking changes to APIs that aren't backported to 0.90.X would be dangerous.  I suppose some of the APIs you can expect not to be part of applications but have some code hitting them.
</comment><comment author="clintongormley" created="2013-12-17T16:54:21Z" id="30768360">I've opened up separate issues for all of the above, except for:
- `minimum_master_eligible_nodes` - this parameter needs explanation whether we use the short or long form, and the longer form is more susceptible to typos.
- disambiguation of `settings` keys: I agree that it is confusing. Internally it is stored with the `index` prefix and the `index.foo: ..` vs `index: { foo: ..}` style pervades all settings.  Changing these forms would break things for a lot of people. Setting just `number_of_shards` should be considered the shortcut.  I think the answer here is just to improve the documentation.
- `number_of_shards/replicas`to just `shards/replicas` - I kinda like that, but then I don't like just updating the settings to `replicas: 0`.  It somehow feels safer spelling out the option.  Open to other comments here.
- Remove `fuzzy` query - The `fuzzy` query is a low-level term query, just like the `term` and `prefix` queries. It can be useful for advanced users, so I don't think it should be removed.  However, I would like to see it simplified so that it is used only for `string` fields. (see #4076)
- Lazy index creation should be opt-in: As discussed in that thread, the "correct" default depends on what you are doing with Elasticsearch.  I'd prefer to stick with the convenience-over-configuration principle
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mget API - document the support for fields parameter in URL, add support for fields parameter in request body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4006</link><project id="" key="" /><description>The documentation for mget API only specifies that the fields parameter can be specified per document to get. But I tried and am using the un-documented feature of specifying the fields parameter in the URL. It would be good to make it a documented feature. It would also be good to support the fields parameter in the request body, as a top-level element. The motivation is that there is a limit on the URL size (4K as far as I remember) and a big field list may hit the limit. I had this problem in practice. It is also not best to repeat the same big field list for each document to get, and actually impossible at all if the ids element is used. Please see the reproduction below:

```
curl -XDELETE "http://localhost:9200/test/"; echo
curl -XPUT "http://localhost:9200/test/" -d'{
   "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 5
   }
}'; echo

curl -XPUT 'http://localhost:9200/test/order/1-1' -d '{
   "productName":"doc 1",
   "quantity":1
}'; echo
curl -XPUT 'http://localhost:9200/test/order/1-2' -d '{
   "productName":"doc 2",
   "quantity":20
}'; echo
curl -XPUT 'http://localhost:9200/test/order/1-3?refresh=true' -d '{
   "productName":"doc 3",
   "quantity":5
}'; echo

# This works, returns 3 docs with productName field only
curl -XPOST 'http://localhost:9200/test/order/_mget?pretty=true&amp;fields=productName' -d '{
   "ids" : [ "1-1", "1-2", "1-3" ]
}'; echo

# Fields parameter is ignored. This returns 3 docs with _source field
curl -XPOST 'http://localhost:9200/test/order/_mget?pretty=true' -d '{
   "fields" : [ "productName" ],
   "ids" : [ "1-1", "1-2", "1-3" ]
}'; echo
```
</description><key id="21791529">4006</key><summary>mget API - document the support for fields parameter in URL, add support for fields parameter in request body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">apidruchny</reporter><labels /><created>2013-10-29T22:03:59Z</created><updated>2014-11-08T12:56:01Z</updated><resolved>2014-11-08T12:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update multi-get.asciidoc</comment></comments></commit></commits></item><item><title>Retrieving field values from field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4005</link><project id="" key="" /><description>There are situations where it is advantageous to retrieve not the field as it was stored, but rather the value that was indexed. The example that spurred this was dates. 

Dates are stored in a variety of formats, but are always normalized into a long for indexing. However, when retrieving dates they come back as stringw that need to be parsed into the language's native date object, which requires knowing their format. Field data's normalized long could be parsed into a date object without needing to know the original source field's format.

The proposed feature would allow the user to specify that they want the field value to be retrieved from field data using the format: __fielddata.fieldname_, much like we support __source.fieldname_.

Eg:

```
{
    "query": { 
        "match_all": {
        }
    },
    "fields": ["_fielddata.user.id"]
}
```
</description><key id="21786475">4005</key><summary>Retrieving field values from field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels /><created>2013-10-29T20:43:51Z</created><updated>2014-02-13T21:47:17Z</updated><resolved>2014-02-13T19:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T19:54:25Z" id="35018561">Closing, this was implemented in #4492 and #4728 .
</comment><comment author="rashidkpc" created="2014-02-13T21:47:17Z" id="35030051">Awesome, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Path, dev and mount cannot be retrieved using the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4004</link><project id="" key="" /><description>The nodes stats api allows to get back information about the file system storage where the data is located.

Although the rest api returns `fs.path`, `fs.mount` and `fs.dev` (as the `FsStats#toXContent` method prints them out), those cannot be retrieved using the Java API as their getter methods are missing.
</description><key id="21757272">4004</key><summary>Path, dev and mount cannot be retrieved using the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-29T13:44:43Z</created><updated>2013-10-29T13:51:31Z</updated><resolved>2013-10-29T13:51:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/fs/FsStats.java</file></files><comments><comment>Added getters for fs.path, fs.mount and fs.dev in node stats api</comment></comments></commit></commits></item><item><title>[DOCS] Extending setup as a service documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4003</link><project id="" key="" /><description>- Tell people to use ES_JAVA_OPTS for es.node.name or similar parameters
- Showing a simple way to install Oracle JDK on ubuntu/debian

Closes #3999
</description><key id="21754087">4003</key><summary>[DOCS] Extending setup as a service documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-29T12:43:26Z</created><updated>2014-07-16T21:51:48Z</updated><resolved>2013-10-29T12:59:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>bulk response has errors indication + status per item</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4002</link><project id="" key="" /><description>When using the bulk API, in case of an error for one of the actions specified, the client needs to parse the entire response to figure out whether all of them succeeded or not.

Having a simple boolean field (`"errors" : true/false`) indicating whether all requests worked or at least one failed would help a lot as the response does not have to be parsed any more.
</description><key id="21749650">4002</key><summary>bulk response has errors indication + status per item</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">costin</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-10-29T11:06:39Z</created><updated>2013-12-11T15:04:48Z</updated><resolved>2013-12-11T15:04:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-29T11:07:52Z" id="27294154">while we do this, it would also be nice to have a `status` element on each failed item, with the status code for the failure.
</comment><comment author="roytmana" created="2013-10-30T12:58:43Z" id="27386512">Do we even need to return the complete list. Could we just return lost of failures with each failure carrying its original index so it can be matched to the bulk request element?
This way return will be much smaller without loss of info.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file></files><comments><comment>bulk response has errors indication + status per item</comment><comment>closes #4002</comment></comments></commit></commits></item><item><title>Abstract AckedClusterStateUpdateTask to remove boiler plate code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4001</link><project id="" key="" /><description>this seems like a common pattern and we should abstract it to remove unnecessary code and improve readability.
</description><key id="21745423">4001</key><summary>Abstract AckedClusterStateUpdateTask to remove boiler plate code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-29T09:36:07Z</created><updated>2014-07-16T21:51:48Z</updated><resolved>2013-10-29T18:41:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-29T18:41:47Z" id="27331129">I'm currently working on refactoring apis that already supported master acks to use the new ack mechanism. While I'm at it I'm planning to unify the way we submit cluster state updates and remove a lot of boiler plate code as suggested here, having a base Listener that does the common things in a single place. Thanks for the suggestion!
</comment><comment author="s1monw" created="2013-10-29T22:39:29Z" id="27350497">@javanna thanks for moving this further! once you have something can you reference it to this one so we have a clear history.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Resolved Issue #3975 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4000</link><project id="" key="" /><description>I changed the class MatchQuery for put the a block of "catch
(NumberFormatExpetion)"
</description><key id="21734816">4000</key><summary>Resolved Issue #3975 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eltu</reporter><labels /><created>2013-10-29T02:58:01Z</created><updated>2014-08-07T18:16:30Z</updated><resolved>2014-08-07T18:16:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow setting the node.name and other options in the Debian/RPM packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3999</link><project id="" key="" /><description>While you can set extra java options with the Debian/RPM packages, you can't pass other Elasticsearch options like node name, node.rack_id etc.

Would be useful to support these
</description><key id="21713693">3999</key><summary>Allow setting the node.name and other options in the Debian/RPM packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-10-28T19:16:19Z</created><updated>2013-10-29T12:59:16Z</updated><resolved>2013-10-29T12:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Extending setup as a service documentation</comment></comments></commit></commits></item><item><title>document multi term vector api </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3998</link><project id="" key="" /><description>multi term vector api (see #3536) was pushed to master but is not documented.
</description><key id="21707114">3998</key><summary>document multi term vector api </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>docs</label><label>v1.0.0.Beta2</label></labels><created>2013-10-28T17:44:17Z</created><updated>2013-11-26T16:03:50Z</updated><resolved>2013-11-26T16:03:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOC] add doc for multi term vector api</comment></comments></commit></commits></item><item><title>PR for mget API should support global routing parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3997</link><project id="" key="" /><description>mget API support `_routing` field but not `routing` parameter.

Reproduction here:

``` sh
curl -XDELETE "http://localhost:9200/test/"; echo
curl -XPUT "http://localhost:9200/test/" -d'{
   "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 5
   }
}'; echo

curl -XPUT 'http://localhost:9200/test/order/1-1?routing=key1' -d '{
   "productName":"doc 1"
}'; echo
curl -XPUT 'http://localhost:9200/test/order/1-2?routing=key1' -d '{
   "productName":"doc 2"
}'; echo
curl -XPUT 'http://localhost:9200/test/order/1-3?routing=key1&amp;refresh=true' -d '{
   "productName":"doc 3"
}'; echo

# This works and gives 3 documents
curl -XPOST 'http://localhost:9200/test/order/_mget?pretty' -d '{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "order",
            "_id" : "1-1",
            "_routing" : "key1"
        },
        {
            "_index" : "test",
            "_type" : "order",
            "_id" : "1-2",
            "_routing" : "key1"
        },
        {
            "_index" : "test",
            "_type" : "order",
            "_id" : "1-3",
            "_routing" : "key1"
        }
    ]
}'; echo

# This does not work and returns "exists" : false for each doc
curl -XPOST 'http://localhost:9200/test/order/_mget?pretty&amp;routing=key1' -d '{
    "ids": [
        "1-1",
        "1-2",
        "1-3"
    ]
}'; echo
```

Closes #3996.
</description><key id="21704217">3997</key><summary>PR for mget API should support global routing parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-10-28T17:04:35Z</created><updated>2014-06-29T14:25:21Z</updated><resolved>2013-10-28T20:09:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-28T17:47:23Z" id="27236552">+1 Looks good to me!
</comment><comment author="dadoonet" created="2013-10-28T20:09:18Z" id="27250917">Closed by https://github.com/elasticsearch/elasticsearch/commit/92780cbf172a7d2aa5bbe09a4b757dfcfb876d26 in 0.90 and https://github.com/elasticsearch/elasticsearch/commit/5d90abf7014fdf2e5e4813a0f6a60a3a4125bd31 in master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mget API should support global routing parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3996</link><project id="" key="" /><description>mget API support `_routing` field but not `routing` parameter.

Reproduction here:

``` sh
curl -XDELETE "http://localhost:9200/test/"; echo
curl -XPUT "http://localhost:9200/test/" -d'{
   "settings": {
      "number_of_replicas": 0,
      "number_of_shards": 5
   }
}'; echo

curl -XPUT 'http://localhost:9200/test/order/1-1?routing=key1' -d '{
   "productName":"doc 1"
}'; echo
curl -XPUT 'http://localhost:9200/test/order/1-2?routing=key1' -d '{
   "productName":"doc 2"
}'; echo
curl -XPUT 'http://localhost:9200/test/order/1-3?routing=key1&amp;refresh=true' -d '{
   "productName":"doc 3"
}'; echo

# This works and gives 3 documents
curl -XPOST 'http://localhost:9200/test/order/_mget?pretty' -d '{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "order",
            "_id" : "1-1",
            "_routing" : "key1"
        },
        {
            "_index" : "test",
            "_type" : "order",
            "_id" : "1-2",
            "_routing" : "key1"
        },
        {
            "_index" : "test",
            "_type" : "order",
            "_id" : "1-3",
            "_routing" : "key1"
        }
    ]
}'; echo

# This does not work and returns "exists" : false for each doc
curl -XPOST 'http://localhost:9200/test/order/_mget?pretty&amp;routing=key1' -d '{
    "ids": [
        "1-1",
        "1-2",
        "1-3"
    ]
}'; echo
```
</description><key id="21702399">3996</key><summary>mget API should support global routing parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-28T16:40:48Z</created><updated>2013-10-28T20:07:35Z</updated><resolved>2013-10-28T20:06:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file></files><comments><comment>mget API should support global routing parameter</comment></comments></commit></commits></item><item><title>Update cluster settings api to support acknowledgements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3995</link><project id="" key="" /><description>Add support for acknowledgements from the other nodes to the update cluster settings api. 
Add support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).
Add also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.
</description><key id="21702207">3995</key><summary>Update cluster settings api to support acknowledgements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-28T16:38:20Z</created><updated>2013-10-29T12:05:53Z</updated><resolved>2013-10-29T12:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/FilteringAllocationTests.java</file></files><comments><comment>Added support for acknowledgements to update cluster settings api</comment></comments></commit></commits></item><item><title>Setting threadpool.search.queue_size to -1 doesn't work as described</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3994</link><project id="" key="" /><description>The docs say this should give you an unbounded queue.

However, I still get EsRejectedExecutionException with this setting on the search queue, and can see the rejected count in the node stats API going up.

Steps to reproduce:

a. Set initial settings in elasticsearch.yml:

```
threadpool:
    search:
        type: fixed
        size: 72
        queue_size: -1
```

b. Run a large of queries in parallel, look out for EsRejectedExecutionException.

c. Raise the queue_size to a huge number:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
      "transient" : {
          "threadpool.search.queue_size" : 1000000
      }
  }'
```

d. Run a bunch more queries in parallel, no exceptions this time.

e. Change back to -1 via cluster settings, the exceptions restart.

Verified with 0.90.2.
</description><key id="21702014">3994</key><summary>Setting threadpool.search.queue_size to -1 doesn't work as described</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels /><created>2013-10-28T16:35:09Z</created><updated>2013-10-31T02:26:18Z</updated><resolved>2013-10-31T02:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-10-31T02:26:18Z" id="27456827">It looks like that issue was fixed in 0.90.3 by https://github.com/elasticsearch/elasticsearch/commit/9d0ce1b. Closing. Please feel free to reopen if you can reproduce it with the latest version of elasticsearch. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Numeric field mappers should encode doc values in binary doc values fields (?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3993</link><project id="" key="" /><description>Today we are using sorted-set doc values to encode doc values for numeric fields. However, numeric data differs a bit from string data in that values are usually more useful (and not much larger) than ordinals and sorted-set is quite slow at getting the values from a doc ID (because of the levels of indirection and that getting the values will require random disk I/O).

So I was thinking that it would probably a better trade-off to store numeric values in a binary doc values field. Since all per-doc values would be stored sequentially on disk, we wouldn't have the random I/O issue anymore.

The two main challenges if we go this way would be to allow a field mapper to buffer all field values before creating a single Lucene field instance, and to pick a space and CPU efficient encoding for multi-valued numeric fields.
</description><key id="21695779">3993</key><summary>Numeric field mappers should encode doc values in binary doc values fields (?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-10-28T15:13:53Z</created><updated>2016-11-03T06:24:56Z</updated><resolved>2013-12-26T09:04:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-29T13:54:48Z" id="27304150">I think if a field is MultiValues and numeric we need the values anyways so I think this batch optimization here makes lots of sense! +1
</comment><comment author="s1monw" created="2013-12-26T12:10:14Z" id="31219375">it's in, awesome! :)
</comment><comment author="24king" created="2016-11-03T06:24:04Z" id="258071604">elasticsearch 1.5.1 

```
// for mapping 
"customer_norm":{
  "type": "integer",
  "index":"no",
  "doc_values": true
}

// for similarity
context.reader().getNormValues('customer_norm'); 
```

it doesn't work very well for me. I found the **DocValuesType  for field customer_norm** in fieldInfo is SORTED_NUMERIC, why? Is there any method to change the **DocValuesType  for field customer_norm** to NUMERIC?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/ByteUtils.java</file><file>src/main/java/org/elasticsearch/common/util/CollectionUtils.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/fielddata/AbstractAtomicNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/DoubleValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/LongValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVNumericAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVNumericAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>src/main/java/org/elasticsearch/index/mapper/RootMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java</file><file>src/test/java/org/elasticsearch/common/util/ByteUtilsTests.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/FieldLevelBoostTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/nested/NestedMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Use BINARY doc values instead of SORTED_SET doc values to store numeric data.</comment></comments></commit></commits></item><item><title>Fixed concurrency issue in simple id cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3992</link><project id="" key="" /><description>The lget() of a map can only be used if the map isn't shared. This issue concurrency issue occurs in none of the released versions and only exists in the master and 0.90 branch. Also only the `top_children` query is affected.
</description><key id="21694717">3992</key><summary>Fixed concurrency issue in simple id cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-10-28T15:01:09Z</created><updated>2015-05-18T23:33:42Z</updated><resolved>2013-10-31T15:25:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-31T15:13:47Z" id="27494137">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>human flag should default to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3991</link><project id="" key="" /><description>Currently in master, the `human` flag (which outputs `1gb` instead of just the byte value) defaults to `false`, the logic being that these stats are usually consumed by computers instead of by humans.

I think we should default it to `true` instead because:

1) repeated calls from monitoring systems will use the same query over and over, so setting `human` to `false` is something you have to do once.

2) when running an ad-hoc query from the command line, you currently have to remember to set `human` to `true` every time you run a query, which will be irritating.
</description><key id="21692862">3991</key><summary>human flag should default to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-10-28T14:35:14Z</created><updated>2013-10-28T14:38:00Z</updated><resolved>2013-10-28T14:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-28T14:37:09Z" id="27216562">The problem with defaulting it to `true` is that most automatic systems that use the stats API will typically not bother looking into the `human` flag and how to use it (its the nature of how they end up being set up...), which adds an unneeded overhead (we saw it...) if polled frequently.
</comment><comment author="clintongormley" created="2013-10-28T14:38:00Z" id="27216642">fair enough
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix issue 3989</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3990</link><project id="" key="" /><description>Multi term vector request never returned if shards for all requested documents
were non existent.

closes #3989
</description><key id="21689421">3990</key><summary>Fix issue 3989</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-10-28T13:38:47Z</created><updated>2014-06-29T01:25:56Z</updated><resolved>2013-11-26T16:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-11-26T15:54:32Z" id="29303741">+1 to push
</comment><comment author="brwe" created="2013-11-26T16:06:20Z" id="29304994">pushed to master (1510cd6)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi term vector request never returns if the shards for all requested documents are non existent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3989</link><project id="" key="" /><description>To reproduce, run:

```
     curl -XPOST "http://localhost:9200/_mtermvectors" -d'
     {
        "docs": [
           {
              "_index": "indexX",
              "_type": "testX",
              "_id": "1"
           }]
     }'
```

(given you do not have an index called "indexX")
</description><key id="21688623">3989</key><summary>Multi term vector request never returns if the shards for all requested documents are non existent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>v1.0.0.Beta2</label></labels><created>2013-10-28T13:25:15Z</created><updated>2013-11-26T16:05:48Z</updated><resolved>2013-11-26T16:05:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvector/TransportMultiTermVectorsAction.java</file><file>src/test/java/org/elasticsearch/action/termvector/MultiTermVectorsTests.java</file></files><comments><comment>Fix issue 3989</comment></comments></commit></commits></item><item><title>Make ContextIndexSearcher delegate more method calls to Engine.searcher()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3988</link><project id="" key="" /><description>When running tests, Engine.searcher() is going to be an AssertingIndexSearcher
so we definitely don't want to discard it. This commit fixes it as well as the
bugs it found.

Closes #3987
</description><key id="21685813">3988</key><summary>Make ContextIndexSearcher delegate more method calls to Engine.searcher()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-10-28T12:29:26Z</created><updated>2014-07-16T21:51:50Z</updated><resolved>2013-10-28T13:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-28T13:11:47Z" id="27209485">+1 Looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ContextIndexSearcher discards Engine.searcher()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3987</link><project id="" key="" /><description>This is a problem for our tests since we mock RobinEngine so that it exposes an AssertingIndexSearcher in order to catch bugs. For instance we would have caught #3955 earlier if we had used this AssertingIndexSearcher.
</description><key id="21684933">3987</key><summary>ContextIndexSearcher discards Engine.searcher()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-28T12:09:33Z</created><updated>2013-10-28T13:43:29Z</updated><resolved>2013-10-28T13:13:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-28T12:10:08Z" id="27206160">+1 for keeping the searcher around!!!!!!
</comment><comment author="s1monw" created="2013-10-28T12:49:53Z" id="27208087">LGTM
</comment><comment author="jpountz" created="2013-10-28T13:14:28Z" id="27209705">Thanks @s1monw and @martijnvg for the reviews.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/XLuceneConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/EmptyScorer.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MoreLikeThisQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/search/facet/query/QueryFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/test/java/org/elasticsearch/index/cache/filter/FilterCacheTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Make ContextIndexSearcher delegate more method calls to Engine.searcher().</comment></comments></commit></commits></item><item><title>Added support for acknowledgements where missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3986</link><project id="" key="" /><description>- Update index settings api
- Delete mapping api
- Cluster reroute api
- Update cluster settings api
</description><key id="21643313">3986</key><summary>Added support for acknowledgements where missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-10-26T23:34:39Z</created><updated>2014-07-16T21:51:50Z</updated><resolved>2013-10-29T12:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-28T08:54:47Z" id="27196598">- UpdateSettingsClusterStateUpdateRequest needs to have a public constructor, otherwise it won't compile?
- If the cluster reroute API has dry run, then it will hang, since it will return the same cluster state, and no "ack" callback will be called. I think this should be fixed on a more infrastructure level, and if the same cluster state is returned, the `InternalClusterService` should probably call `onAllNodesAcked` (i.e. ack'ed in an empty set manner).
</comment><comment author="javanna" created="2013-10-28T08:57:26Z" id="27196724">Yes I think I forgot to update the PR with the latest commits I worked on yesterday (as the constructor is fixed on my local copy, but not here). Will update soon, thanks!
</comment><comment author="javanna" created="2013-10-29T12:07:20Z" id="27297283">Pushed to master and backported to 0.90.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster reroute api to support acknowledgements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3985</link><project id="" key="" /><description>Add support for acknowledgements from the other nodes to the cluster reroute api. 
Add support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).
Add also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.
</description><key id="21643200">3985</key><summary>Cluster reroute api to support acknowledgements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-26T23:25:32Z</created><updated>2013-10-29T12:05:53Z</updated><resolved>2013-10-29T12:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/rest/AcknowledgedRestResponseActionListener.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/reroute/RestClusterRerouteAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file></files><comments><comment>Added support for acknowledgements to cluster reroute api</comment></comments></commit></commits></item><item><title>Delete mapping api to support acknowledgements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3984</link><project id="" key="" /><description>Add support for acknowledgements from the other nodes to the delete mapping api. 
Add support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).
Add also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.
</description><key id="21643182">3984</key><summary>Delete mapping api to support acknowledgements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-26T23:24:23Z</created><updated>2013-10-29T12:05:53Z</updated><resolved>2013-10-29T12:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/delete/RestDeleteMappingAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file></files><comments><comment>Added support for node acknowledgements in delete mapping api</comment></comments></commit></commits></item><item><title>Update index settings api to support acknowledgements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3983</link><project id="" key="" /><description>Add support for acknowledgements from the other nodes to the update index settings api. 
Add support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).
Add also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.
</description><key id="21643156">3983</key><summary>Update index settings api to support acknowledgements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-26T23:22:25Z</created><updated>2013-10-29T12:05:53Z</updated><resolved>2013-10-29T12:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/settings/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerResponse.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedResponse.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/cluster/ack/ClusterStateUpdateListener.java</file><file>src/main/java/org/elasticsearch/cluster/ack/ClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/cluster/ack/ClusterStateUpdateResponse.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file></files><comments><comment>Added support for acknowledgement in update index settings api</comment></comments></commit></commits></item><item><title>Highlighting  working weirdly with synonym filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3982</link><project id="" key="" /><description>Steps to reproduce - 

Index creation script
- Download http://wordnetcode.princeton.edu/3.0/WNprolog-3.0.tar.gz  , extract  wn_s.pl and place it in config directory.
- Create an index using this script - https://gist.github.com/Vineeth-Mohan/7118283
  ./AboveScript localhost
  - Index a feed
    curl -XPOST 'http://localhost:9200/info/news' -d ' { "Event"  : "large forest in large" } '
- Execute this search - https://gist.github.com/Vineeth-Mohan/7169279
- Output obtained - https://gist.github.com/Vineeth-Mohan/7169285

```
Observation - Received "&lt;tag1&gt;large&lt;/tag1&gt; &lt;tag1&gt;forest&lt;/tag1&gt; in &lt;tag1&gt;large&lt;/tag1&gt;" rather than "&lt;tag1&gt;large&lt;/tag1&gt; forest in &lt;tag1&gt;large&lt;/tag1&gt;" for searching big ( big is present in the synonym file)

More observation 

Searching forest ,  i am getting the below highlight - 
large &lt;tag1&gt;forest&lt;/tag1&gt; in large

And for searching big or large 
&lt;tag1&gt;large&lt;/tag1&gt; &lt;tag1&gt;forest&lt;/tag1&gt; in &lt;tag1&gt;large&lt;/tag1&gt;

So conclusion is that on matching a term which don't have a synonym , things work fine
But while doing it on a term with synonym , every indexed term is getting highlighting.  

Few more example - 
Text - the lake was frozen and high , all my team was at their best. Lets do the best then

Search word - lake 
Result - the &lt;tag1&gt;lake&lt;/tag1&gt; was frozen and high , all my team was at their best. Lets do the best then

Search word - high
Result - the lake was frozen and &lt;tag1&gt;high&lt;/tag1&gt; , &lt;tag1&gt;all&lt;/tag1&gt; &lt;tag1&gt;my&lt;/tag1&gt; team was at their &lt;tag1&gt;best&lt;/tag1&gt;. Lets do the best then
Question - How did the term "all" , "my" and "best" got highlighted ?
Conclusion - Seems every indexed term is not getting highlighted. But there is no pattern in which terms are highlighted. May be all terms having synonym in the text are highlighted !!!

Search word - my
Result - the lake was frozen and high , all &lt;tag1&gt;my&lt;/tag1&gt; team was at their best. Lets do the best then
Conclusion - Above conclusion is wrong. If all terms with synonyms were highlighted on any synonym match , it should have happened for my also.

Also i don't find any issue with analyser or the wordnet. You can see the analyser output for the text "large forest" here - https://gist.github.com/Vineeth-Mohan/7165559

As far as i can see , all the tokens are correctly identified and placed. I feel this is a bug with highlighter.

Conclusion - I am not finding a pattern to this bug.

```
</description><key id="21633595">3982</key><summary>Highlighting  working weirdly with synonym filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>:Highlighting</label></labels><created>2013-10-26T13:09:38Z</created><updated>2016-11-06T10:48:15Z</updated><resolved>2016-11-06T10:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-10-26T17:34:45Z" id="27151194">Try indexing with term_vectors and use the FastVectorHighlighter. Using a recent version of ES (0.90.5) it should work flawlessly.
</comment><comment author="Vineeth-Mohan" created="2013-10-27T07:02:21Z" id="27164133">@synhershko  - I will do that but if its not giving the results in the default confoguration of highlighting even for ES (0.90.5) , should we see it as a bug.
</comment><comment author="clintongormley" created="2013-11-08T19:46:42Z" id="28092025">Hi @Vineeth-Mohan 

Any chance you could provide a simpler recreation? If recreations are long and complex, nobody wants to work on them :)

thanks
</comment><comment author="davidtme" created="2015-06-05T09:51:27Z" id="109234364">I think I'm seeing this issue and I’ve created a small test case:

Index: 

``` json
{
    "mappings": {
        "ev": {
            "properties": {
                "description": {
                    "type": "string",
                    "term_vector": "with_positions_offsets",
                    "analyzer": "synonym"
                },
                "id": {
                    "type": "integer"
                },
                "title": {
                    "type": "string"
                }
            }
        }
    },
    "settings": {
        "index": {
            "analysis": {
                "filter": {
                    "synonym": {
                        "type": "synonym",
                        "format": "wordnet",
                        "synonyms_path": "analysis/wn_s.pl"
                    }
                },
                "analyzer": {
                    "synonym": {
                        "filter": [
                            "synonym"
                        ],
                        "type": "custom",
                        "tokenizer": "whitespace"
                    }
                }
            }
        }
    }
}
```

Data:

``` json
{
    "id": 1,
    "title": "test 1",
    "description": "you'll help to make each visit"
}
```

Query:

``` json
{
    "query" : {
        "match": {
            "description": {
                "query": "help"
            }
        }
    },
    "highlight" : {
        "fields" : {
            "description" : {}
        }
    }
}
```

Result:

``` json
{
    "took": 3,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 0.3787904,
        "hits": [
            {
                "_index": "test",
                "_type": "ev",
                "_id": "AU3DG06KDdtspwrG66P0",
                "_score": 0.3787904,
                "_source": {
                    "id": 1,
                    "title": "test 1",
                    "description": "you'll help to make each visit"
                },
                "highlight": {
                    "description": [
                        "you'll &lt;em&gt;help&lt;/em&gt; &lt;em&gt;to&lt;/em&gt; make &lt;em&gt;each&lt;/em&gt; visit"
                    ]
                }
            }
        ]
    }
}
```

The problem I'm seeing is that "**to**" and "**each**" are being highlighted
</comment><comment author="clintongormley" created="2015-06-05T13:53:00Z" id="109299984">@davidtme the problem occurs because you are expanding synonyms both at index time and at search time, so it queries for multiple synonyms and matches all of them, then it highlights a selection of those (but admittedly in the wrong positions).

If you only expand synonyms at search or index time, then the highlighting works correctly.
</comment><comment author="clintongormley" created="2016-11-06T10:48:15Z" id="258673062">This appears to be fixed in 5.0, probably because of changes to the positions emitted by the synonyms token filter.  Multi-word synonyms will probably still be problematic.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Source filtering in mget doesn't understand 0/1 as false/true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3981</link><project id="" key="" /><description>In most places:

```
_source: true | false
```

can also be written as:

```
_source: 1 | 0
```

But this isn't supported in the `mget` api:

```
curl -XPUT 'http://localhost:9200/test_1/test/1?pretty=1' -d '
{
   "count" : "1",
   "include" : {
      "field1" : "v1",
      "field2" : "v2"
   }
}
'

curl -XPUT 'http://localhost:9200/test_1/test/2?pretty=1' -d '
{
   "count" : "1",
   "include" : {
      "field1" : "v1",
      "field2" : "v2"
   }
}
'

curl -XGET 'http://localhost:9200/_mget?pretty=1' -d '
{
   "docs" : [
      {
         "_source" : 0,
         "_index" : "test_1",
         "_id" : "1",
         "_type" : "test"
      },
      {
         "_source" : 1,
         "_index" : "test_1",
         "_id" : "2",
         "_type" : "test"
      }
   ]
}
'

# {
#    "docs" : [
#       {
#          "_source" : {
#             "count" : "1",
#             "include" : {
#                "field1" : "v1",
#                "field2" : "v2"
#             }
#          },
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "test",
#          "exists" : true,
#          "_version" : 1
#       },
#       {
#          "_source" : {
#             "count" : "1",
#             "include" : {
#                "field1" : "v1",
#                "field2" : "v2"
#             }
#          },
#          "_index" : "test_1",
#          "_id" : "2",
#          "_type" : "test",
#          "exists" : true,
#          "_version" : 1
#       }
#    ]
# }
```
</description><key id="21631754">3981</key><summary>Source filtering in mget doesn't understand 0/1 as false/true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2013-10-26T10:43:23Z</created><updated>2013-10-26T20:34:58Z</updated><resolved>2013-10-26T20:34:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>src/main/java/org/elasticsearch/search/fetch/source/FetchSourceParseElement.java</file></files><comments><comment>Use XContentParser.isBooleanValue to detect all boolean values for the `_source` parameter for both _mget and _search</comment></comments></commit></commits></item><item><title>The +index pattern without a wildcard in the index list is handled incon...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3980</link><project id="" key="" /><description>...sistently

Fixes #3979
</description><key id="21624876">3980</key><summary>The +index pattern without a wildcard in the index list is handled incon...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-10-26T01:10:33Z</created><updated>2014-07-01T17:05:41Z</updated><resolved>2013-10-30T20:52:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-28T08:02:29Z" id="27194394">+1
</comment><comment author="imotov" created="2013-10-30T20:52:51Z" id="27437978">Pushed to 0.90 and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The +index pattern without a wildcard in the index list is handled inconsistently </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3979</link><project id="" key="" /><description>If a cluster has three indices `foo`, `bar` and `baz`, the index list `b*,+foo` in search request is resolved into all three indices, while `+foo,b*` throws `IndexMissingException[[+foo] missing]`

Repro:

```
curl -XPUT "http://localhost:9200/foo/doc/1?pretty" -d '{"f": "v"}'
curl -XPUT "http://localhost:9200/bar/doc/1?pretty" -d '{"f": "v"}'
curl -XPUT "http://localhost:9200/baz/doc/1?pretty" -d '{"f": "v"}'
curl -XPOST "http://localhost:9200/_refresh?pretty"
echo "Searching indices b*,+foo - works"
curl "http://localhost:9200/b*,%2bfoo/_search?pretty"
echo "Searching indices +foo,b* - doesn't work"
curl "http://localhost:9200/%2bfoo,b*/_search?pretty"
```
</description><key id="21624736">3979</key><summary>The +index pattern without a wildcard in the index list is handled inconsistently </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-26T01:03:43Z</created><updated>2013-10-30T20:48:47Z</updated><resolved>2013-10-30T20:47:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java</file></files><comments><comment>The +index pattern without a wildcard in the index list is handled inconsistently</comment></comments></commit></commits></item><item><title>Documentation Correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3978</link><project id="" key="" /><description>Just happened to stumble across this

.contains -&gt; .containsKey

http://docs.oracle.com/javase/7/docs/api/java/util/HashMap.html
</description><key id="21618619">3978</key><summary>Documentation Correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">passion4code</reporter><labels /><created>2013-10-25T21:51:39Z</created><updated>2014-07-01T19:28:36Z</updated><resolved>2013-10-30T12:02:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T12:02:55Z" id="27383314">Hey,

thanks for your PR, but the `tags` field is a `List` and not a `HashMap` - so it indeed has a `contains()` method, but no `containsKey()` method... Using this leads to an exception

``` sh
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "ctx._source.tags.containsKey(tag) ? ctx.op = \"delete\" : ctx.op = \"none\"",
    "params" : {
        "tag" : "blue"
    }
}'
{"error":"ElasticSearchIllegalArgumentException[failed to execute script]; nested: PropertyAccessException[[Error: unable to resolve method: java.util.ArrayList.containsKey(java.lang.String) [arglength=1]]\n[Near : {... ctx._source.tags.containsKey(t ....}]\n             ^\n[Line: 1, Column: 1]]; ","status":400}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion Suggester: Reject non-integer weights on indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3977</link><project id="" key="" /><description>Right now the completion field mapper casts a float/double value to a long and stores it internally.

This may lead to confusion, because weight calculations then could possibly use the same weight, even though someone indexed a weight `4.2` and `4.5`.

Solution is to simply reject such an index requests with an appropriate error message to use an integer - which might result in rejected index requests (thus the breaking tag).
</description><key id="21594320">3977</key><summary>Completion Suggester: Reject non-integer weights on indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>breaking</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-25T14:38:20Z</created><updated>2014-10-15T10:41:28Z</updated><resolved>2013-10-25T14:48:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-10-15T10:33:43Z" id="59186751">Hi @spinscale,

I cannot reopen issues, but in fact the problem here is not completely solved. Should I open a new issue? My problem is that this check introduced here only works if the weight is a number. Unfortunately the parser has a bit strange logic: Your new code is not executed if the client (creating the JSON) is passing the weight as "string", e.g. { "weight" : "10" }

In fact the weight is then ignored completely and not even an error is given (this is an additional bug). This caused me headaches yesterday, because the weight was given as JSON string in the indexing document. For other fields this makes no difference while indexing.

The parser for completion fields should be improved to have the outer check on the JSON key first and later check the types, not vice versa.
</comment><comment author="clintongormley" created="2014-10-15T10:35:50Z" id="59186949">Hi @uschindler 

yes please open another issue for this
</comment><comment author="uschindler" created="2014-10-15T10:41:28Z" id="59187486">OK. done.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>CompletionFieldMapper: Return error if weight is no integer</comment></comments></commit></commits></item><item><title>Adding methods to HttpRequest for retrieving informations about the http client (remoteaddr ...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3976</link><project id="" key="" /><description>Added methods to `org.elasticsearch.http.HttpRequest` which can be used  within `RestFilters` for retrieving informations about the http client (like remote address and remote port). The local address, local port and the X-Opaque-Id header can also be retrieved. remote address and remote port are mainly for security purposes like implementing a `RestFilter` which grant different privileges to different client hosts. If you know that your `RestRequest` is a `HttpRequest` (or check this via `instanceof`) a cast to `HttpRequest` gives you access to the following new methods:

```
public String localAddr();
public long localPort();
public String remoteAddr();
public long remotePort();
public String opaqueId();
```

The modified `org.elasticsearch.http.HttpRequest` interface should not
break the https://github.com/sonian/elasticsearch-jetty plugin (and hopefully not any other code).
</description><key id="21591440">3976</key><summary>Adding methods to HttpRequest for retrieving informations about the http client (remoteaddr ...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">salyh</reporter><labels /><created>2013-10-25T13:50:39Z</created><updated>2014-07-16T21:51:52Z</updated><resolved>2014-01-08T23:18:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-08T23:18:17Z" id="31887059">Added the feature here with some modifications: bc0909b2325edb97ccb7254ce956469e3d678920 (and also in 0.90), thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>multi_match query crashes with NumberFormatException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3975</link><project id="" key="" /><description>When execute this simple example it crashes:

```
curl -XPUT http://localhost:9200/blog/post/1?pretty=1 -d '{"foo":123, "bar":"xyzzy"}'

curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query": {"multi_match": {"fields": ["foo", "bar"], "query": "xyzzy", "useDisMax": true}}}'
```

stack trace:
{
"error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[BIXGo8_wTEiO9jVc0KjZ_w][blog][1]: SearchParseException[[blog][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"foo\", \"bar\"], \"query\": \"xyzzy\", \"useDisMax\": true}}}]]]; nested: NumberFormatException[For input string: \"xyzzy\"]; }{[BIXGo8_wTEiO9jVc0KjZ_w][blog][0]: SearchParseException[[blog][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"foo\", \"bar\"], \"query\": \"xyzzy\", \"useDisMax\": true}}}]]]; nested: NumberFormatException[For input string: \"xyzzy\"]; }{[BIXGo8_wTEiO9jVc0KjZ_w][blog][4]: SearchParseException[[blog][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"foo\", \"bar\"], \"query\": \"xyzzy\", \"useDisMax\": true}}}]]]; nested: NumberFormatException[For input string: \"xyzzy\"]; }{[BIXGo8_wTEiO9jVc0KjZ_w][blog][2]: SearchParseException[[blog][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"foo\", \"bar\"], \"query\": \"xyzzy\", \"useDisMax\": true}}}]]]; nested: NumberFormatException[For input string: \"xyzzy\"]; }{[BIXGo8_wTEiO9jVc0KjZ_w][blog][3]: SearchParseException[[blog][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"foo\", \"bar\"], \"query\": \"xyzzy\", \"useDisMax\": true}}}]]]; nested: NumberFormatException[For input string: \"xyzzy\"]; }]",
"status" : 400
}
</description><key id="21590796">3975</key><summary>multi_match query crashes with NumberFormatException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eltu</reporter><labels><label>discuss</label></labels><created>2013-10-25T13:38:42Z</created><updated>2015-12-28T14:05:12Z</updated><resolved>2015-03-27T10:06:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T09:19:20Z" id="50126606">Doesn't make sense to include numeric fields in a multi-match query - closing
</comment><comment author="felipelrz" created="2015-01-07T11:56:10Z" id="69012564">@clintongormley, what if I do want to include numeric fields in a multi_match query (or any other sort of query)? What is the best approach, in this particular case?
</comment><comment author="clintongormley" created="2015-01-13T19:03:13Z" id="69799224">Hmm, the more I think about this, the more I think that we should handle this better.

A query string query like:

```
GET /t/_validate/query?explain
{
  "query": {
    "query_string": {
      "query": "bar 1",
      "fields": ["text","num"],
      "lenient": true
    }
  }
}
```

results in:

```
"(text:bar) (text:1 | num:[1 TO 1])"
```

while the equivalent multi-match just ignores the `num` field completely:

```
"((text:bar text:1))"
```

However, a multi-match query for just a number seems to work:

```
GET /t/_validate/query?explain
{
  "query": {
    "multi_match": {
      "query": "1",
      "fields": ["text","num"],
      "lenient": true
    }
  }
}

"(num:[1 TO 1] | text:1)"
```

Possibly, when using the `lenient` option on numeric fields, we should apply a whitespace analyzer to the string and discard anything which isn't a number.
</comment><comment author="bleskes" created="2015-03-27T10:06:04Z" id="86887796">The `lenient` flag current let you ignore values that don't fit specific fields (like when you try to search a numeric field with a string value that doesn't convert to a number). We discussed it and we feel this is good enough. It would be an overkill to try to parse the string looking for numbers in it. I'm closing this as I think @eltu 's issue is covered by this. @clintongormley if you feel differently, I think we should have a different issue for it and re-discuss.  
</comment><comment author="jreinke" created="2015-12-28T14:05:12Z" id="167577538">The `lenient` parameter seems to be ignored when using `cross_fields` type and multiple numeric fields:

```
curl -XPUT http://localhost:9200/blog/post/1?pretty=1 -d '{"foo":123, "bar":"xyzzy", "baz":456}'
```

**Search is OK with 1 numeric field**

```
curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query": {"multi_match": {"type": "cross_fields", "query": "xyzzy", "lenient": true, "fields": ["foo", "bar"]}}}'
```

```
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.30685282,
    "hits" : [ {
      "_index" : "blog",
      "_type" : "post",
      "_id" : "1",
      "_score" : 0.30685282,
      "_source":{"foo":123, "bar":"xyzzy", "baz":456}
    } ]
  }
}
```

**Search fails with 2 numeric fields**

```
curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query": {"multi_match": {"type": "cross_fields", "query": "xyzzy", "lenient": true, "fields": ["foo", "bar", "baz"]}}}'
```

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "number_format_exception",
      "reason" : "For input string: \"xyzzy\""
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "blog",
      "node" : "0TxGVVWsSu2qX63hZdOv2w",
      "reason" : {
        "type" : "number_format_exception",
        "reason" : "For input string: \"xyzzy\""
      }
    } ]
  },
  "status" : 400
}
```

Am I missing something?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unify REST responses for Acknowledged[Request|Response]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3974</link><project id="" key="" /><description>All Rest handlers that use AcknowledgedResponse look identical. Since
we gonna have more of them in the future we can abstract the logic away.
</description><key id="21586763">3974</key><summary>Unify REST responses for Acknowledged[Request|Response]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-25T12:25:49Z</created><updated>2014-06-14T18:14:53Z</updated><resolved>2013-10-25T12:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-25T12:35:26Z" id="27087134">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ES_STOP_TIMEOUT configuration variable to packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3973</link><project id="" key="" /><description>This variable allows to configure the waiting time after a TERM signal has
been sent. After that waiting time a KILL signal is sent to ensure the
service is stopped.

In case of a bigger installation the default values might be to slow, so it
now is configurable.

Original work done by @tmclaugh at the PR #3719

Closes #3719
Closes #3972
</description><key id="21580502">3973</key><summary>Add ES_STOP_TIMEOUT configuration variable to packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label></labels><created>2013-10-25T09:58:40Z</created><updated>2015-03-19T10:19:03Z</updated><resolved>2014-12-05T09:53:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-08-21T09:38:31Z" id="52898537">any reason this has not been resolved yet? @spinscale ?
</comment><comment author="spinscale" created="2014-08-22T07:29:54Z" id="53031233">@s1monw I think we need to fix the timeouts, as they are too low (20 seconds for TERM, 40 for KILL or something) and I would like to have someone with packaging knowledge review this one, if it is useful at all.

@drewr @electrical can you have a look maybe, if this is good &amp; useful?

Going to extend the timeouts asap
</comment><comment author="electrical" created="2014-08-22T12:57:10Z" id="53056827">Looks good to me.
I'm a bit scared by the kill -9 part though.
I believe that still should be a manual action instead of automated.
If it fails to stop normally there would be something wrong with the process.

Having the stop time configured defo makes sense.
</comment><comment author="spinscale" created="2014-08-27T16:16:03Z" id="53598875">@electrical The more I think about it the more I agree that the only one to execute `kill -9` should be the sysadmin and not an init script. If a process does not finish after `TERM`, manual intervention should be required.

any other/different opinions on that? @drew @s1monw ?
</comment><comment author="rjernst" created="2014-08-27T17:15:20Z" id="53607303">+1 to **not** do a `kill -9` from a script...
</comment><comment author="clintongormley" created="2014-09-06T12:54:45Z" id="54711512">+1, especially in light of #7563 which will perform a flush on shutdown
</comment><comment author="spinscale" created="2014-09-08T11:50:35Z" id="54807069">removed all the `KILL` relevant parts... note that by default, redhats `killproc` function still sends a `KILL` signal after the timeout passed and the process is still there...

should this explicitely be disabled (which is possible), to make sure an init mechanism never sends a `KILL` signal, even though it is part of the standard shutdown mechanism in init scripts?
</comment><comment author="spinscale" created="2014-09-12T10:04:16Z" id="55383366">@electrical can you have a look and comment on my last question, if it makes sense to change the `killproc` default behaviour in your opinion?
</comment><comment author="electrical" created="2014-09-25T12:53:43Z" id="56813991">On the one hand we should not deviate from standards handled by a service manager.
On the other hand with systems like ES I'm very afraid of SIGTERM's to it.

Imho when a stop fails there must be something wrong with it, for example shutdown takes longer or some other weird error.
It would be safer for ES to never execute a Kill / SIGTERM and let the sysadmin do that manually.
</comment><comment author="t-lo" created="2014-12-03T13:12:02Z" id="65404370">It is worth noting that the systemd service configuration _does_ use a kill mechanism (and a timeout of 20 seconds) already.
</comment><comment author="s1monw" created="2014-12-05T09:53:55Z" id="65768240">we decided to not allow for explicit configuration of this in our packages. If other 3rd party tool do that already that's a different story. If there is further evidence that we need to do that too we can still reopen the issue.
</comment><comment author="t-lo" created="2014-12-05T10:15:20Z" id="65770509">@s1monw this is partially right. As I wrote above - the elasticsearch systemd configuration uses an explicit kill timeout of 20 seconds (see https://github.com/elasticsearch/elasticsearch/blob/master/src/rpm/systemd/elasticsearch.service#L17), I suppose this is a bug, then?
</comment><comment author="s1monw" created="2014-12-05T10:29:44Z" id="65772026">&gt; suppose this is a bug, then?

I don't think so - I'd rather say it's maybe an inconsistency but we can fix it though. I don't have strong feelings about it to be honest I'd vote fore removing the kill there too?
</comment><comment author="t-lo" created="2014-12-05T11:00:14Z" id="65775183">+1 for being consistent - users would expect the same behaviour on different init systems.
</comment><comment author="t-lo" created="2014-12-05T11:09:17Z" id="65776113">Please note that there's a pull request for adding a systemd service file to the .deb package (since Debian will switch to systemd soon) which still has the kill timeout: https://github.com/elasticsearch/elasticsearch/pull/8765
@s1monw ping me if you want the timeout removed and I'll update that pull request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make shutdown wait time configurable for packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3972</link><project id="" key="" /><description>Enable a configurable delay for shutting down. This means, that firs a TERM signal for regular shutdown is sent, then the configured time is waited for, and the if the service is still up a KILL signal is sent.

Goal is to use the same parameter for the Redhat and Debian package
</description><key id="21580455">3972</key><summary>Make shutdown wait time configurable for packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>feature</label></labels><created>2013-10-25T09:57:51Z</created><updated>2014-12-24T16:31:46Z</updated><resolved>2014-12-24T16:31:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T16:31:46Z" id="68062317">Decided against doing this.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add reference for ember-data-elasticsearch-kit to integrations page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3971</link><project id="" key="" /><description /><key id="21572271">3971</key><summary>add reference for ember-data-elasticsearch-kit to integrations page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">OpakAlex</reporter><labels /><created>2013-10-25T06:21:27Z</created><updated>2014-07-16T21:51:52Z</updated><resolved>2013-10-31T10:45:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OpakAlex" created="2013-10-28T19:02:48Z" id="27244133">=)
</comment><comment author="OpakAlex" created="2013-10-31T09:59:58Z" id="27473740">Boys?
</comment><comment author="javanna" created="2013-10-31T10:45:25Z" id="27476345">Merged, thanks! It'll get published on the website in the next few hours.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the 'recovery' _cat API endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3970</link><project id="" key="" /><description>This adds the _cat/recovery/{index} API endpoint, which displays information about the status of recovering replica shards. An example of the output:

```
$ curl 'localhost:9200/_cat/recovery'
index shard node                   target    recovered %
test2 0     Fwo7c_6MSdWM0uM1Ho4t-g 147304414 19236101  13.1%
test  0     Fwo7c_6MSdWM0uM1Ho4t-g 145891423 119640535 82.0%
```

Feedback welcome for the output format.

Fixes #3969
</description><key id="21571792">3970</key><summary>Add the 'recovery' _cat API endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-10-25T06:02:30Z</created><updated>2014-12-12T16:27:44Z</updated><resolved>2013-10-28T19:42:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-10-28T16:02:32Z" id="27225087">Whoops, commented on the wrong place, @drewr I added a the "text-align:right;" and a check for a null primary size.
</comment><comment author="drewr" created="2013-10-28T16:33:36Z" id="27228358">Made one more comment.

Don't force-push the branch! You can always squash when you merge if you're concerned about the history.
</comment><comment author="drewr" created="2013-10-28T17:48:26Z" id="27236673">:+1:
</comment><comment author="dakrone" created="2013-10-28T19:42:23Z" id="27248200">Merged in e3db12bf500da8fd60beb7efad67ef99c3e384eb
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `_cat/recovery` API endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3969</link><project id="" key="" /><description>We should add the `_cat/recovery` API endpoint to display information at the command line about recovering replica shards.
</description><key id="21557914">3969</key><summary>Add `_cat/recovery` API endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-10-24T22:49:33Z</created><updated>2013-10-28T19:41:29Z</updated><resolved>2013-10-28T19:41:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-10-28T16:01:48Z" id="27225016">@drewr added "text-align:right;" and a check for the null primary size.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file></files><comments><comment>Add the 'recovery' _cat API endpoint</comment></comments></commit></commits></item><item><title>Resolved issue #3797</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3968</link><project id="" key="" /><description>I changed the class MultiMatchQuery for support the issue #3797
</description><key id="21540659">3968</key><summary>Resolved issue #3797</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">eltu</reporter><labels /><created>2013-10-24T18:10:05Z</created><updated>2014-07-05T22:56:57Z</updated><resolved>2013-10-24T22:52:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-24T22:52:42Z" id="27039704">Merged, thanks!!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 4.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3967</link><project id="" key="" /><description>Lucene 4.5.1 was just released - we should upgrade!
</description><key id="21535055">3967</key><summary>Upgrade to Lucene 4.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-24T16:39:34Z</created><updated>2014-06-14T18:14:55Z</updated><resolved>2013-10-24T17:22:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add version to prebuilt analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3966</link><project id="" key="" /><description>This patch takes the version of the created index into account when a
prebuilt analyzer is created.
So, if an index was created with 0.90.4, then the prebuilt analyzers
will be the same than on the 0.90.4 release.

One reason for this feature is the possibility to change pre built
analyzers like the standard one.

The patch tries to reuse analyzers as mutch as possible. So even if
version X.Y.Z and X.Y.A use the same lucene analyzers, the same instance
is reused in order to prevent overcreation of lucene analyzer instances.

Closes #3790
</description><key id="21527458">3966</key><summary>Add version to prebuilt analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-24T14:49:00Z</created><updated>2014-07-10T03:53:16Z</updated><resolved>2013-10-28T17:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-24T20:40:16Z" id="27027804">am I correct in the fact that we just cache the analyzers for the current version, and if the version is not CURRENT, then we create a new instance each time? If so:

First, this is not very evident in terms of design in the `PreBuiltAnalyzerProviderFactory` class (i.e. the CURRENT version is provided as a constructor parameter, which you don't know that its based on CURRENT, and then the check for CURRENT is done when asking to provide the analyzer.

The other part is that if someone upgrades, for example, from 0.90.5 to 0.90.6, then all the other indices in previous versions will now create all the analyzers for all the different indices on that version (0.90.5). I was thinking that `IndicesAnalysisService` would also cache the fact that an analyzer was create for version X, so it its asked again, it will return it. Otherwise, on many indices case deployment and a minor upgrade, we end up creating many analyzers. The caching can be a tuple of analyzer name and the Version, with its own built `PreBuiltAnalyzerProviderFactory`. Make sense?
</comment><comment author="kimchy" created="2013-10-24T21:43:42Z" id="27033871">Ahh, I missed the fact that we have the cache on the enum itself. 2 things that I think are still problematic, will comment over it.
</comment><comment author="spinscale" created="2013-10-25T06:48:25Z" id="27069073">Updated: Set the Scope to `INDICES` and made the `getAnalyzer` method synchronized. Thx for reviewing.
</comment><comment author="s1monw" created="2013-10-25T09:19:33Z" id="27076258">@spinscale maybe we should have a test that creates and closes indices randomly and makes sure we don't get the Scope wrong in the future. I think such a test would be very helpful!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException using "has_child" filter after upgrade to v0.90.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3965</link><project id="" key="" /><description>After upgrading from v0.90.1 -&gt; v0.90.5, we noticed that some of our has_child filter queries started to fail on some shards. Following is the request - 

```
curl -x '' -s -XPOST 'http://localhost:9201/data_index_20131011/vendor/_search?from=0&amp;size=2' -d '
{
  "filter" : {
    "has_child" : {
      "query" : {
        "match" : {
          "set_aside_descriptions" : {
            "query" : "No set aside used."
          }
        }
      },
      "type" : "transaction"
    }
  }
}
'
```

Gives failure in response 

```
  "_shards" : {
    "total" : 4,
    "successful" : 2,
    "failed" : 2,
    "failures" : [ {
      "index" : "data_index_20131011",
      "shard" : 0,
      "status" : 500,
      "reason" : "RemoteTransportException[[mach2.node][inet[/&lt;internal ip&gt;:9301]][search/phase/query]]; nested: QueryPhaseExecutionException[[data_index_20131011][0]: query[ConstantScore(cache(_type:vendor))],from[0],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "
    }, {
      "index" : "data_index_20131011",
      "shard" : 1,
      "status" : 500,
      "reason" : "QueryPhaseExecutionException[[data_index_20131011][1]: query[ConstantScore(cache(_type:vendor))],from[0],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "
    } ]
  },

```

And the error trace from logs - 

```
[2013-10-24 08:54:07,330][TRACE][search                   ] [mach2.node] Query phase failed
org.elasticsearch.search.query.QueryPhaseExecutionException: [data_index_20131011][0]: query[ConstantScore(cache(_type:vendor))],from[0],size[2]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:219)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:623)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:612)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.common.lucene.docset.MatchDocIdSet.shortCircuit(MatchDocIdSet.java:82)
        at org.elasticsearch.index.search.child.HasChildFilter$ParentDocSet.matchDoc(HasChildFilter.java:174)
        at org.elasticsearch.common.lucene.docset.MatchDocIdSet.get(MatchDocIdSet.java:69)
        at org.elasticsearch.common.lucene.search.FilteredCollector.collect(FilteredCollector.java:61)
        at org.apache.lucene.search.Scorer.score(Scorer.java:65)
        at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:245)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:162)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:488)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:444)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:134)
        ... 7 more
```

The same query works fine in v0.90.1. It's tough to provide a gist to replicate because the error is data dependent and index size is more than a GB, it only happens for some queries. For e.g. 
`"query" : "blah blah"` works fine but 
`"query" : "No set aside used."` fails. 
</description><key id="21523266">3965</key><summary>NullPointerException using "has_child" filter after upgrade to v0.90.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ajhalani</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-24T13:46:23Z</created><updated>2014-01-12T06:57:44Z</updated><resolved>2013-10-24T19:03:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-10-24T13:48:22Z" id="26993355">Could this be because of the stop word `no`?
</comment><comment author="ajhalani" created="2013-10-24T14:26:30Z" id="26996547">I am able to replicate with another query without `no`. 
</comment><comment author="clintongormley" created="2013-10-24T14:28:13Z" id="26996709">Do all of your child docs have the field `set_aside_descriptions`?

Do your child docs exist on all shards? or do you just have child docs at the moment?
</comment><comment author="ajhalani" created="2013-10-24T14:47:29Z" id="26998474">yes set_aside_descriptions field is populated for all child documents. 
Yes child docs exist on all shards. 

I notice that this issue is only happening for filter which result in a large number of child docs. For e.g. 
for field `set_aside_descriptions`, the 2 top match values(matching 1450731 and 249870 child docs)  are failing but not for the rest(46123 and below). 
Similarly for another field `socioeconomic_indicators_names` where the issue is happening, it fails for top 3 match values (matching 1373706, 876751 and 804976 child docs) but not for the rest (350865 and below)
</comment><comment author="s1monw" created="2013-10-24T15:47:07Z" id="27003897">I don't think 0.90.6 is vulnerable to this since we pull the iterator forcefully during weight creation

```
DocIdSet docIdSet = new ParentDocSet(context.reader(), parentsBits, collectedUids.v(), idReaderTypeCache);
 return ConstantScorer.create(docIdSet, this, queryWeight); // &lt;=== pulls the docIdSet.iterator()
```

I'd still be curious if we can reproduce this somehow. FYI we literally rewrote `ParentChild` internally to have proper queries etc.
</comment><comment author="clintongormley" created="2013-10-24T15:54:40Z" id="27004589">@ajhalani any chance you could try distilling this failure down to something we can replicate? That way we can make sure that it is fixed in 0.90.6, which is due out soon.
</comment><comment author="ajhalani" created="2013-10-24T15:58:34Z" id="27004957">it will be little tough to provide replicable steps since the issue seems to be data specific. I will try to write a script which populates millions of small child docs to and see if it still happens. If not, I can build the v0.90 master branch and test it against our data.
</comment><comment author="clintongormley" created="2013-10-24T15:59:38Z" id="27005046">even if it is long, it would be useful.

i'd prefer to have an actual test case that we can add to our test suite, rather than just making sure that it is fixed for now :)

thanks
</comment><comment author="ajhalani" created="2013-10-24T18:08:50Z" id="27015799">I understand replicable test will be very useful but I am unable to share the original index data. I tried replicating it in a new index with dummy data but cannot replicate the issue. 
</comment><comment author="ajhalani" created="2013-10-24T18:41:23Z" id="27018670">I updated one of the 3 nodes in cluster to v0.90 master and errors stopped happening. I don't understand why upgrading just one of the the 3 nodes stopped failures on other nodes as well. 

Reverting back to v0.90.5 they happen again :)
</comment><comment author="s1monw" created="2013-10-24T19:03:26Z" id="27020542">good stuff so I will close this for now! Thanks so much for verifying this is very much appreciated!
</comment><comment author="ajhalani" created="2013-10-24T19:12:14Z" id="27021226">thanks for all the help, looking forward to next release :)
</comment><comment author="joelabrahamsson" created="2013-11-01T12:13:39Z" id="27561662">We had the same issue with 0.90.5 and can confirm that when using a built version of the master branch instead the issue is resolved.
</comment><comment author="ajhalani" created="2013-11-01T16:30:59Z" id="27578811">Would it be possible to provide an ETA when the next ES version release is planned for? It will help to determine if we should put effort in a temporary workaround. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] many broken links due to new docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3964</link><project id="" key="" /><description>From various past blog posts etc. Building a re-routing table may be a pita but really helpful :)
</description><key id="21518135">3964</key><summary>[DOCS] many broken links due to new docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-10-24T12:06:16Z</created><updated>2013-11-07T12:02:21Z</updated><resolved>2013-11-07T12:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-10-24T12:25:23Z" id="26987564">Hiya

We implemented 400+ 301s precisely for this purpose.  If we've missed any, it would be helpful to know which ones we have missed so that we can fix them.

Clint
</comment><comment author="synhershko" created="2013-10-24T12:30:31Z" id="26987864">All the links in this URL, for instance: http://www.elasticsearch.org/blog/managing-relations-inside-elasticsearch/

Just run a crawler on your website to look for 404s, or use Google Analytics :)
</comment><comment author="clintongormley" created="2013-10-24T12:32:56Z" id="26988015">Bah, I see.  Those .html links were already redirects to page/index.html.  I'll look at getting those added.

thanks
</comment><comment author="clintongormley" created="2013-11-07T12:02:21Z" id="27957148">OK - All the broken links that we could find have been fixed.  There are probably some external blogs with really old links that 404, but short of scanning the interwebz, not much we can do there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use abstract classes as super typs for Acknowledge operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3963</link><project id="" key="" /><description>Currently we have a marker interface for Acknowledged[Request|Response],
this makes not much sense since we duplicate the code in each subclass
or class that implements the interface. We can simply use abstract
classes and have it implemented only once.
</description><key id="21518045">3963</key><summary>Use abstract classes as super typs for Acknowledge operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-24T12:04:27Z</created><updated>2014-06-12T08:26:40Z</updated><resolved>2013-10-24T12:15:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>introduce stop timeout and start type for windows service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3962</link><project id="" key="" /><description>Allow environment variable to customize the way the service installed is started (auto vs manual) and show it is being stopped (namely what's the timeout).
</description><key id="21514250">3962</key><summary>introduce stop timeout and start type for windows service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-24T10:39:01Z</created><updated>2013-10-24T12:29:10Z</updated><resolved>2013-10-24T12:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>add stop timeout and start mode for windows service.bat</comment></comments></commit></commits></item><item><title>throw exception if decay is requested for a field with multiple values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3961</link><project id="" key="" /><description>closes #3960
</description><key id="21512056">3961</key><summary>throw exception if decay is requested for a field with multiple values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-10-24T09:52:26Z</created><updated>2014-07-13T08:16:00Z</updated><resolved>2014-04-25T11:19:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:41:39Z" id="41176944">LGTM shoud be`1.x` and `master`
</comment><comment author="brwe" created="2014-04-25T11:19:34Z" id="41382209">closing in favor of #5940, see https://github.com/elasticsearch/elasticsearch/issues/3960#issuecomment-41279373
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>function_score should throw exception if decay function is used with multi valued field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3960</link><project id="" key="" /><description>The decay functions for function_score do not handle fields with multiple values. Decay functions silently compute the distance from the first value. This can lead to confusion, as seen in issue #3926. Instead of using the first value in the field, an exception should be thrown if a field has multiple values.
</description><key id="21511694">3960</key><summary>function_score should throw exception if decay function is used with multi valued field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2013-10-24T09:45:26Z</created><updated>2014-04-29T11:28:18Z</updated><resolved>2014-04-29T11:28:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2013-10-24T09:57:26Z" id="26979725">@brwe I'm wondering if "flattering" before applying `function_score` could work too. Like in #3926 each function could be applied to each value and `score_mode` could be used to calculate result from NxM values where N is functions count and M is values count.
</comment><comment author="s1monw" created="2013-10-24T10:06:23Z" id="26980239">we have a bunch of places where we do similar things like in Sorting we use `org.elasticsearch.index.fielddata.fieldcomparator.SortMode` where users can simply pick if they want the min/max/sum/average of a multivalued field.
</comment><comment author="brwe" created="2013-10-24T10:30:50Z" id="26981584">I see. Might make sense.
Do you think we should also take care of the following use case?

I have a multi valued field with N entries, but it actually represents an N-dim vector. Instead of computing avg, min or max distance, I actually want to give the origin as vector as well and then compute euclidean distance of origin and field vector.

In this case the number of values in the field should be the same as the number of values given in the `origin` parameter.
</comment><comment author="brwe" created="2014-04-24T13:23:03Z" id="41278894">Closed this issue. Feel free to open a new issue if you think multiple values should be handled as described above.
</comment><comment author="jpountz" created="2014-04-24T13:27:49Z" id="41279373">@brwe Unfortunately, the `isMultiValued` method might return true even if the field is single-valued. The reason is that our doc-values-based impls can't know whether they are single-valued. (This reminds me that we should randomly use doc values instead of uninverted field data in our tests /cc @javanna @s1monw)
</comment><comment author="brwe" created="2014-04-24T13:50:15Z" id="41281906">Thanks! I reverted the commits for now.
</comment><comment author="s1monw" created="2014-04-24T14:26:20Z" id="41286179">@brwe I pushed a commit on how we could solve this - just as an example to start the discussion
</comment><comment author="brwe" created="2014-04-24T14:36:17Z" id="41287365">@s1monw That would work I think. 
</comment><comment author="brwe" created="2014-04-25T09:33:32Z" id="41374848">Opened pull request #5940
</comment><comment author="brwe" created="2014-04-29T11:28:01Z" id="41665028">Closing, the issue is fixed by https://github.com/elasticsearch/elasticsearch/pull/5940
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreTests.java</file></files><comments><comment>Multi value handling in decay functions</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreTests.java</file></files><comments><comment>Throw exception if decay is requested for a field with multiple values</comment></comments></commit></commits></item><item><title>Context extension of the Suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3959</link><project id="" key="" /><description>The suggester API is a good choice to get simple suggestions. But these suggestions may take additional informations into account. Typing in something like `restaurant` will result in restaurants all over the world. But in this case the geolocation of the request can be used to get better results, namely those which are close to point of the request. Another usecase could be a simple `type` or `group` information which allows the suggester to return results fitting this context.
In general the suggester should be able to handle generic information and create result with this information taken into account.
</description><key id="21507965">3959</key><summary>Context extension of the Suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-10-24T08:24:38Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-03-13T11:16:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="julianhille" created="2013-10-24T16:51:50Z" id="27009547">Possible duplication ? https://github.com/elasticsearch/elasticsearch/issues/2842
</comment><comment author="chilling" created="2013-10-24T17:01:44Z" id="27010395">Hi @julianhille,
I don't think that this is a duplication. In my opinion filtering is some kind of post-processing. This issue instead describes a kind of pre-processing.
</comment><comment author="chilling" created="2014-03-13T11:16:59Z" id="37521838">closed by #4044
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature to filter by type for completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3958</link><project id="" key="" /><description>Completion suggester is great and we are already using it, however in few cases we would like to filter suggestions by one type only and currently even if we can store suggestion entries by type, we can't limit/filter query by one type only.

Thank you.
</description><key id="21503104">3958</key><summary>Feature to filter by type for completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darklow</reporter><labels /><created>2013-10-24T06:22:46Z</created><updated>2014-02-05T11:05:15Z</updated><resolved>2014-02-05T11:05:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="julianhille" created="2013-10-24T16:52:38Z" id="27009612">possible the same idea: https://github.com/elasticsearch/elasticsearch/issues/3944
</comment><comment author="clintongormley" created="2014-02-05T11:05:15Z" id="34156384">This will be possible using the new ContextSuggester when it is added. Closing this issue in favour of #4044 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix nonsensical sentence in standard analyzer documentation so that it is more understandable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3957</link><project id="" key="" /><description /><key id="21493324">3957</key><summary>Fix nonsensical sentence in standard analyzer documentation so that it is more understandable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels /><created>2013-10-24T00:03:13Z</created><updated>2014-07-16T21:51:55Z</updated><resolved>2013-10-24T22:20:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-24T22:20:48Z" id="27036829">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix for has_child can cause an infinite loop (100% CPU) when used in bool query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3956</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/issues/3955
</description><key id="21489393">3956</key><summary>Fix for has_child can cause an infinite loop (100% CPU) when used in bool query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshcanfield</reporter><labels /><created>2013-10-23T22:29:07Z</created><updated>2014-06-26T22:41:11Z</updated><resolved>2013-10-25T17:14:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file></files><comments><comment>Re-apply pull-request #3956 now that the CLA has been signed.</comment></comments></commit></commits></item><item><title>has_child can cause an infinite loop (100% CPU) when used in bool query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3955</link><project id="" key="" /><description>The defect is in 
ChildrenQuery.ParentScorer#nextDoc()

``` java
          @Override
            public int nextDoc() throws IOException {
                if (remaining == 0) {
                    return NO_MORE_DOCS;
                }
...
```

This fails to update `currentDocId`. This causes `ChildrenQuery.ParentScorer#docID()` to continue returning the last doc id forever.

The BooleanScorer calls

``` java
public boolean score(Collector collector, int max, int firstDocID)
```

passing the `firstDocID` it pulled from the `docID()`. When the ParentScorer finishes with a low id document but continues to get processed and added to the BucketTable it opens the possibility of a document from the other boolean sub queries overlapping and causing get an infinite loop.

The fix is to set 
`currentDocId = NO_MORE_DOCS;`
in both `int nextDoc()` and `int advance(int target)`
</description><key id="21487686">3955</key><summary>has_child can cause an infinite loop (100% CPU) when used in bool query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">joshcanfield</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-23T21:57:09Z</created><updated>2013-10-28T12:09:33Z</updated><resolved>2013-10-25T17:15:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-24T06:46:27Z" id="26969990">+1 to the fix! GOOD CATCH! If I recall correctly the test query utils check for this don't we pass those queries through it? @jpountz any idea?
</comment><comment author="jpountz" created="2013-10-24T07:21:33Z" id="26971292">@s1monw I had the same reaction, I'll try to understand why our tests didn't catch this.
</comment><comment author="jpountz" created="2013-10-25T17:15:27Z" id="27109758">Pushed. Thanks Josh!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file></files><comments><comment>Fix for has_child can cause an infinite loop (100% CPU) when used in bool query.</comment></comments></commit></commits></item><item><title>Transport: Have a separate channel for recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3954</link><project id="" key="" /><description>Have a separate channel for recovery, so it won't overflow the "low" channel which is also used for bulk indexing.

Also, rename the channel names to be more descriptive. Change `low` to `bulk` (for bulk based operations, currently just bulk indexing), `med` to `reg` (for "regular" operations), and `high` to `state` (for state based communication). The new channel for recovery will be named `recovery`, and the `ping` channel will remain the same.
</description><key id="21483944">3954</key><summary>Transport: Have a separate channel for recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-23T20:54:09Z</created><updated>2013-10-23T22:55:39Z</updated><resolved>2013-10-23T22:55:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkAction.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>src/main/java/org/elasticsearch/transport/TransportRequestOptions.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java</file></files><comments><comment>Transport: Have a separate channel for recovery</comment><comment>Have a separate channel for recovery, so it won't overflow the "low" channel which is also used for bulk indexing.</comment></comments></commit></commits></item><item><title>Initial implementation of Snapshot/Restore API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3953</link><project id="" key="" /><description>Closes #3826
</description><key id="21482123">3953</key><summary>Initial implementation of Snapshot/Restore API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-10-23T20:25:08Z</created><updated>2014-06-20T10:24:07Z</updated><resolved>2013-11-11T01:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-10-23T23:38:16Z" id="26955628">(still working through the code)

I think having restores explicitly using `_restore` as part of the URL seems like it would be helpful to me. As is, I'll constantly be checking the documentation because I'll forget whether PUT or POST does the backup or restore. I don't want to accidentally create a new snapshot, or accidentally restore an old one because I switched the two curl flags by mistake.
</comment><comment author="kimchy" created="2013-10-23T23:40:15Z" id="26955722">I tend to agree with @dakrone (which I missed on the issue), I think POST / PUT is tricky for this, would love to see it be more explicit both on the snapshot side and the restore side?
</comment><comment author="imotov" created="2013-10-23T23:44:36Z" id="26955952">So, currently two URLs are supported for create:

```
        controller.registerHandler(PUT, "/_snapshot/{repository}/{snapshot}", this);
        controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}/_create", this);
```

and two URLs are supported for restore:

```
        controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}", this);
        controller.registerHandler(POST, "/_snapshot/{repository}/{snapshot}/_restore", this);
```

I thought PUT/POST would be nice shortcuts to have. But if you disagree, it's easy to remove them.
</comment><comment author="dakrone" created="2013-10-24T07:54:59Z" id="26972923">I'm in favor of removing the shortcuts and making the action required, I think it will be less likely to cause accidental issues.
</comment><comment author="imotov" created="2013-10-24T13:39:55Z" id="26992717">@dakrone, @kimchy I was thinking more about and I think I still like the shortcuts. The general idea behind the shortcuts was to support the following operations on snapshot:

```
PUT /_snapshot/{repository}/{snapshot} - creates snapshot 
POST /_snapshot/{repository}/{snapshot} - restores snapshot 
DELETE /_snapshot/{repository}/{snapshot} - deletes snapshot  
```

I understand that PUT and POST might look similar to some. However, it would be difficult to accidentally call one instead of another. For PUT to work, indices should be open and snapshot shouldn't exist, for the POST to work indices should be closed or shouldn't exist while snapshot should exist. So, you would have to mess up at least 3 things for such accidental execution to have a negative effect. 
</comment><comment author="s1monw" created="2013-11-10T20:03:35Z" id="28159080">The latest commits LGTM - lets move it into master! +1 to push. Great stuff Igor!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Empty query_string generates SearchParseException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3952</link><project id="" key="" /><description>When executing a query with an empty query_string an org.elasticsearch.search.SearchParseException is raised.

How to reproduce:

```
curl -XPUT localhost:9200/test/test/1 -d '{}'

curl -XPOST http://localhost:9200/test/test/_search -d '{ 
    "query" : {"query_string":{ query: ""}},
}'
```

Tested with 0.90.5.

Response:
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[MkT5D2yYRgq6g73gx9141A][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\" : {\"query_string\":{\"query\": \"\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse '': Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\" : {\"query_string\":{\"query\": \"\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse '': Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\" : {\"query_string\":{\"query\": \"\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse '': Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\" : {\"query_string\":{\"query\": \"\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse '': Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\" : {\"query_string\":{\"query\": \"\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse '': Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \"&lt;EOF&gt;\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    &lt;BAREOPER&gt; ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    &lt;REGEXPTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }]","status":400}

It will generate the following exception on the server:
[2013-10-23 11:31:10,295][DEBUG][action.search.type       ] [Tomazooma] [test][3], node[MkT5D2yYRgq6g73gx9141A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@acda796]
org.elasticsearch.search.SearchParseException: [test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query" : {"query_string":{"query": ""}}}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:561)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:464)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:449)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:442)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:214)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
Caused by: org.elasticsearch.index.query.QueryParsingException: [test] Failed to parse query []
        at org.elasticsearch.index.query.QueryStringQueryParser.parse(QueryStringQueryParser.java:231)
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:207)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:255)
        at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:549)
        ... 12 more
Caused by: org.apache.lucene.queryparser.classic.ParseException: Cannot parse '': Encountered "&lt;EOF&gt;" at line 1, column 0.
Was expecting one of:
    &lt;NOT&gt; ...
    "+" ...
    "-" ...
    &lt;BAREOPER&gt; ...
    "(" ...
    "*" ...
    &lt;QUOTED&gt; ...
    &lt;TERM&gt; ...
    &lt;PREFIXTERM&gt; ...
</description><key id="21442671">3952</key><summary>Empty query_string generates SearchParseException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">sk-</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2013-10-23T09:34:57Z</created><updated>2014-01-20T15:09:08Z</updated><resolved>2014-01-20T15:09:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="piskvorky" created="2014-01-19T11:07:14Z" id="32705739">Not only empty queries; a query containing only whitespace will fail too.

It's pretty annoying from a logical POV -- empty strings are nothing special, really. Littering the application code with checks for query emptiness/whitespace-only-ness is very ugly and unnatural. 

Should be done on ES side (no query restriction = match all). "400 Bad Request" is not an appropriate response.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Return `MatchNoDocsQuery` if query string is emtpy</comment></comments></commit></commits></item><item><title>GeoDocs Update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3951</link><project id="" key="" /><description>Updated docs of `geohash_precision` and `geohash-cell-filter` to point out `geohash_precision` and `geohash_prefix` option of geohashes
</description><key id="21441619">3951</key><summary>GeoDocs Update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-10-23T09:12:40Z</created><updated>2014-07-16T21:51:56Z</updated><resolved>2013-11-08T12:53:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-11-07T14:29:12Z" id="27968869">@chilling I've added this commit to the above: https://github.com/clintongormley/elasticsearch/commit/b11767ae513660920f0e924cd380460262269011

but i don't want to push because i'm not sure whether you want those code changes to go live yet or not
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file></files><comments><comment>In ctor of GeoPointFieldMapper, geohash_prefix now implicitly enables geohash option</comment><comment>Also improved docs for geopoint type and geohash_cell filte</comment></comments></commit></commits></item><item><title>My </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3950</link><project id="" key="" /><description>I have setup logstash, elasticsearch and kibana. It all works great however TTL is not always being mapped to indices when they are created (only some of the time)? Initially I tried setting up mappings as per Attaching a TTL field with every log sent via logstash to Elasticsearch . But this just does not seem to work for me? I tried both locations /etc/elasticsearch/mappings/_default/default.json, /etc/elasticsearch/config/mappings/_default/default.json but neither seemed to load the mapping. I did find instructions to create a /etc/elasticsearch/default-mapping.json file which seems to work intermittently.

The json config file always contained the following (on each location attempt):

{
    _default_ : {
        "_ttl" : { "enabled" : true, "default" : "1d" }
    }
}

{
logstash-2013.10.18: {
logs: {
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.09: {
logs: {
_ttl: {
enabled: true,
default: 86400000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.19: {
logs: {
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.07: {
logs: {
_ttl: {
enabled: true,
default: 86400000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
tags: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.14: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.15: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.16: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.17: {
logs: {
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.22: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.13: {
logs: {
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.21: {
logs: {
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.12: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
kibana-int: {
temp: {
properties: {
dashboard: {
type: "string"
},
group: {
type: "string"
},
title: {
type: "string"
},
user: {
type: "string"
}
}
}
},
logstash-2013.10.11: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.10: {
logs: {
_ttl: {
enabled: true,
default: 86400000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
tags: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.23: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
},
logstash-2013.10.20: {
logs: {
_ttl: {
enabled: true,
default: 2592000000
},
properties: {
@timestamp: {
type: "date",
format: "dateOptionalTime"
},
@version: {
type: "string"
},
facility: {
type: "long"
},
facility_label: {
type: "string"
},
host: {
type: "string"
},
logsource: {
type: "string"
},
message: {
type: "string"
},
priority: {
type: "long"
},
program: {
type: "string"
},
severity: {
type: "long"
},
severity_label: {
type: "string"
},
timestamp: {
type: "string"
},
type: {
type: "string"
}
}
}
}
}
</description><key id="21435011">3950</key><summary>My </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">norfolkislander</reporter><labels /><created>2013-10-23T05:55:43Z</created><updated>2013-10-23T08:08:13Z</updated><resolved>2013-10-23T08:08:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-10-23T08:08:13Z" id="26887101">Hi @norfolkislander,

Currently ttl is only supported on the document level. There is no ttl support for indices as a whole. There is an issue open requested that feature - so you can watch that for updates: #2114 

If you have anymore questions - can you please post them on the mailing list? we try to keep issues on github for feature request or bug reports.

Thx!
Boaz
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java API: Setting track scores does not affect scan search type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3949</link><project id="" key="" /><description>When setting track scores, the scan search type will return the scores for each document. The Java API builder does not properly set this value (it only sets it if a sort in in place, which is not relevant for scan search type).
</description><key id="21427866">3949</key><summary>Java API: Setting track scores does not affect scan search type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-23T00:59:50Z</created><updated>2013-10-23T01:50:54Z</updated><resolved>2013-10-23T01:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/test/java/org/elasticsearch/search/scan/SearchScanScrollingTests.java</file></files><comments><comment>Java API: Setting track scores does not affect scan search type</comment><comment>When setting track scores, the scan search type will return the scores for each document. The Java API builder does not properly set this value (it only sets it if a sort in in place, which is not relevant for scan search type).</comment></comments></commit></commits></item><item><title>Weighted shard allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3948</link><project id="" key="" /><description>So let me describe my problem before trying to come up with a solution because I'm not sure I have a good solution:
1.  I have machines that vary significantly:
   1a.  Some have SSDs and have some spinning disks.
   1b.  Some have 96GB of ram and some have 32GB.
   1c.  Some have six core, some have twelve.  Maybe we'll get more later with more.
2.  I have indexes that vary significantly:
   2a.  Some are small and see very little search or update traffic.
   2b.  Some are large and see very little search or update traffic.
   2c.  Some are small and see tons of searches and updates.
   2d.  Some are large and see tons of searches and updates.

It looks now like Elasticsearch treats all cluster members and all hosts equally.  Would it make sense to let the user set weights on factors on indexes and hosts?  Like:

Host A: traffic weight = 2, size weight = 1
Host B: traffic weight = 1, size weight = 2
Host C: traffic weight = 2, size weight = 2
etc

Index A: traffic weight = 3, size weight = 2
Index B: traffic weight = .1, size weight = 1
etc

I'm not sure exactly how the balancing would work from there.  Maybe just a single weight factor would be good enough.  It has to be user defined, though.
</description><key id="21394077">3948</key><summary>Weighted shard allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-10-22T16:24:25Z</created><updated>2014-08-08T18:07:44Z</updated><resolved>2014-08-08T18:07:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-22T18:13:29Z" id="26827696">Hey Nik,

maybe it is just late and I misunderstood your requirement (if so, just ignore me), but what part of that you cannot solve using http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#shard-allocation-filtering

You could use includes/excludes based on SSD/RAM/Cores (configured per node) and apply those per index depending on size/queries/updates.

Of course, you could put all this into a shard allocation decider similar to the `DiskThresholdDecider`... but I think maintaining all of the logic is at the end very similar to maintain a couple of tags which describe the hardware the elasticsearch instance runs on plus the requirement per index.

Do you see any means of automation here which I dont?
</comment><comment author="nik9000" created="2013-10-22T18:34:50Z" id="26829437">I _think_ includes/excludes works to keep the hot shards on hot hardware but it doesn't help prevent hot shards from being allocated together and it doesn't give me a good way of saying "hosting a shard in this index is ten time more work then hosting one in this index".  ES could use that information keep the high traffic shards apart and keep the medium traffic shards away from the high traffic shards.

My situation is a bit unique because I have physical hardware that is a bit of a jumble and some of my indexes get .1 searches per second and some get a hundred or so.
</comment><comment author="clintongormley" created="2014-08-08T18:07:44Z" id="51637396">Closed in favour of #4435 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CompletionFieldMapper: Return error if weight is no integer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3947</link><project id="" key="" /><description>In order to make sure that people do not get confused, if they
index a float as weight, it makes more sense to reject it instead of
silently parsing it to an integer and using it.

The CompletionFieldMapper now checks for the type of the number which
is being read and throws and exception if the number is something else
than int or long.
</description><key id="21390165">3947</key><summary>CompletionFieldMapper: Return error if weight is no integer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-22T15:29:21Z</created><updated>2014-07-16T21:51:57Z</updated><resolved>2013-10-25T14:48:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Search_mode : "always" not working when give the exact keyword</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3946</link><project id="" key="" /><description>```
 {
  "my-suggest-1" : {
    "text" : "Francis",
    "term" : {
      "field" : "director",
      "suggest_mode" : "always"
    }

  }
}
```

result ::

```
{
   "_shards": {
      "total": 90,
      "successful": 88,
      "failed": 0
   },
   "my-suggest-1": [
      {
         "text": "francis",
         "offset": 0,
         "length": 7,
         "options": []
      }
   ]
}
```

here when the query is made on "Francis":

```
{
   "_shards": {
      "total": 90,
      "successful": 88,
      "failed": 0
   },
   "my-suggest-1": [
      {
         "text": "francis",
         "offset": 0,
         "length": 7,
         "options": []
      }
   ]
}
```

and the result is 

```
{
   "_shards": {
      "total": 90,
      "successful": 88,
      "failed": 0
   },
   "my-suggest-1": [
      {
         "text": "franci",
         "offset": 0,
         "length": 6,
         "options": [
            {
               "text": "francis",
               "score": 0.8333333,
               "freq": 1
            }
         ]
      }
   ]
}
```
</description><key id="21384229">3946</key><summary>Search_mode : "always" not working when give the exact keyword</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">KanwalSingh</reporter><labels><label>:Suggesters</label><label>bug</label></labels><created>2013-10-22T14:09:07Z</created><updated>2016-11-06T10:37:05Z</updated><resolved>2016-11-06T10:37:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-06T10:37:05Z" id="258672520">Closing in favour of #15410
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow string fields to store term counts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3945</link><project id="" key="" /><description>To use this one you send a string to a field of type 'token_count'.  This
makes the most sense with a multi-field.
</description><key id="21377851">3945</key><summary>Allow string fields to store term counts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2013-10-22T12:15:22Z</created><updated>2014-06-30T11:04:58Z</updated><resolved>2013-12-03T10:05:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-10-22T16:10:22Z" id="26817290">I was having another idea, implementation-wise. If we don't care too much about analyzing the field twice, we could create a new field mapper, which would analyze a string and index its number of tokens as a long. Then one could index the number of tokens by having a `multi_field` for a string field where one of the sub-fields would use this new field mapper.
</comment><comment author="nik9000" created="2013-10-22T16:30:13Z" id="26818902">A `multi_field` makes sense for this.  A new mapper makes sense so long as it preserves most of the old mapper options.  I imagine some folks will want to store this while others will be happy just querying it.  Oddly, if unstored this would be the only thing you couldn't get back.  Also, do you think people might want to store this as a byte, for example?  I have some fields that can't be more than 255 bytes long, much less 255 terms.
</comment><comment author="jpountz" created="2013-10-22T16:45:32Z" id="26820157">&gt; A new mapper makes sense so long as it preserves most of the old mapper options.

I guess you are thinking about the analyzer?

&gt; Also, do you think people might want to store this as a byte, for example?

Thinking more about it, I think it would be more useful to return the sum of position increments instead of the token count? This way, the result would not depend on whether stop words or synonyms have been applied, which I guess is more useful? And since this number is guaranteed to be an integer, I would just force the underlying type to be `integer`: in practice, the byte and short types are indexed as integers, and the field data implementations try to guess the number of bits required to represent the values in memory, so in the end, there wouldn't be wasted space if we forced this number to be an integer.
</comment><comment author="nik9000" created="2013-10-22T17:05:06Z" id="26821838">&gt; I guess you are thinking about the analyzer?

I'm pretty much just thinking of `store` and `precision_step` and maybe `boost`.

&gt;  sum of position increments instead of the token count

Probably.  The particular field I'd be counting already doesn't use stop words but it wouldn't hurt.  Would it still make sense to call this `term_count`?

&gt;  I would just force the underlying type to be integer

Makes sense to me.
</comment><comment author="jpountz" created="2013-10-22T17:10:05Z" id="26822262">&gt; Would it still make sense to call this term_count?

I think so, or maybe `position_count` so that users don't expect that it computes a unique count.
</comment><comment author="nik9000" created="2013-10-22T18:22:49Z" id="26828528">`position_count` is very Lucene-y.  I'm not sure it'd be generally understood.  `term_count` is pretty good because the Lucene definition of `Term` is very like the English definition.  The Lucene definition of `position` would require more explanation.

So, if we are going to analyze the terms a second time, can't we just ask users not to include the stop word filter?  Or, rather, warn them what happens if they do?

About the configuration: are you thinking the `multi_field` would look like this:

``` bash
curl -XPUT http://localhost:9200/test/test/_mapping?pretty -d'{
  "test" : {
    "properties": {
      "foo" : {
        "type": "multi_field",
        "fields": {
          "foo": {
            "type": "string",
            "analyzer": "standard",
            "store": "yes"
          },
          "term_count": {
            "type": "term_count",
            "analyzer": "simple",
            "store": "yes"
          }
        }
      }
    }
  }
}
```
</comment><comment author="bleskes" created="2013-10-22T20:45:16Z" id="26845964">Just suggestion that occurred to me while reading along - would the name `token_count` be better? the word token is more intuitive for me in the context of analysis (and it is the wording `_analyze` API uses). I think it's also technically more accurate.

I agree counting tokens is a notion everyone quickly understands and that positions introduces complexities. It has some upsides (like correctly counting synonyms) but also down sides (if someone increases position at the end of a sentence to break phrase searches). Depends on the use case.  We can do tokens now and introduce a setting later to indicate what should be counted if someone needs something else?
</comment><comment author="jpountz" created="2013-10-23T08:08:53Z" id="26887134">&gt; So, if we are going to analyze the terms a second time, can't we just ask users not to include the stop word filter? &gt; Or, rather, warn them what happens if they do?

Actually, if we actually count the number of positions (even if we expose it as a token or term count), only the tokenizer matters since token filters are not allowed to remove or insert positions. So we could only allow for passing a tokenizer, and this would make this field faster to index since we wouldn't apply the useless token filters.
</comment><comment author="nik9000" created="2013-11-02T01:07:57Z" id="27611752">I've just uploaded a rather copy-and-paste-y implementation that I'll refine soon.  I'll also add one of the customary unit tests for FieldMappers.  I'm sure there is something about FieldMappers that I've forgotten with this implementation.  Please correct me when you get the chance.
</comment><comment author="nik9000" created="2013-11-04T20:04:46Z" id="27717881">Uploaded a less copy-and-pasty version with a proper unit test for merging the new field mapper.  I'm not sure what else is required/expected with regards to testing mappers but I'm reasonably happy with the implementation.
</comment><comment author="nik9000" created="2013-11-04T21:23:38Z" id="27723857">Added documentation.
</comment><comment author="nik9000" created="2013-11-18T14:03:03Z" id="28699594">I'm not going to have time to work on this this week but it'd be cool if someone could review it this week so I have something waiting for me when my schedule clears up :)  In other words, _poke_.
</comment><comment author="jpountz" created="2013-11-19T14:55:55Z" id="28795915">I left a few comments on the commit, but overall it looks good to me!
</comment><comment author="nik9000" created="2013-11-25T18:41:13Z" id="29229071">Finished with @jpountz changes and running a full regression test now.

@jpountz did you comment on the commit of the files changed?  I'm told if you comment on the files changed the comments will survive me pushing an amended commit which is pleasant.
</comment><comment author="jpountz" created="2013-11-26T09:14:58Z" id="29277224">Hmm, I thought I commented on the file changes but it looks like they are lost, or do I miss something?
</comment><comment author="jpountz" created="2013-11-27T22:32:05Z" id="29425724">Thanks Nik. I left comments, but once these minor issues are fixed, I think this pull request is ready!
</comment><comment author="nik9000" created="2013-11-29T20:22:24Z" id="29536030">Uploaded rebased, amended version with changes from comments.
</comment><comment author="jpountz" created="2013-12-03T10:05:20Z" id="29697050">Pushed, thanks Nik!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add filter option to completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3944</link><project id="" key="" /><description>Add optional filter option to completion suggester, that only allows suggest options that match with the filter.
</description><key id="21377362">3944</key><summary>Add filter option to completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">julianhille</reporter><labels /><created>2013-10-22T12:04:22Z</created><updated>2014-07-11T10:11:23Z</updated><resolved>2014-07-11T10:11:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T10:11:23Z" id="48714521">Hi @julianhille 

Adding a filter to the completion suggester would slow it right down, which defeats the point of using it.  Instead we've added "contexts", which allows you to segment your suggestions at index time.
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/suggester-context.html#suggester-context
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor FieldData iterations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3943</link><project id="" key="" /><description>This commit primarily folds [Double|Bytes|Long|GeoPoint]Values.Iter
into [Double|Bytes|Long|GeoPoint]Values. Iterations now don't require
a auxillary class (Iter) but instead driven by native for loops. All
[Double|Bytes|Long|GeoPoint]Values are stateful and provide `setDocId`
and `nextValue` methods to iterate over all values in a document.
This has several advantage:
- The amout of specialized classes is reduced
- Iteration is clearly stateful ie. Iters can't be confused to be local.
- All iterations are size bounded which prevents runtime checks and
  allows JIT optimizations / loop un-rolling and most iterations are
  branch free.
- Due to the bounded iteration the need for a `hasNext` method call
  is removed.
- Value iterations feels more native.

This commit also adds consistent documentation and unifies the calcualtion
if SortMode is involved.

This commit also changes the runtime behavior of BytesValues#getValue() such that it
will never return `null` anymore. If a document has no value in a field
this method still returns a `BytesRef` with a `length` of 0. To identify
documents with no values #hasValue() or #setDocument(int) should be used.
The latter should be preferred if the value will be consumed in the case
the document has a value.
</description><key id="21372728">3943</key><summary>Refactor FieldData iterations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-22T10:16:53Z</created><updated>2014-07-16T21:51:57Z</updated><resolved>2013-10-24T08:50:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-10-23T21:26:40Z" id="26947110">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a GetFieldMapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3942</link><project id="" key="" /><description>This new API allows to get the mapping for a specific set of fields rather than get the whole index mapping and traverse it.
The fields to be retrieved can be specified by their full path, index name and field name and will be resolved in this order.
In case multiple field match, the first one will be returned.

Since we are now generating the output (rather then fall back to the stored mapping), you can specify `include_defaults`=true on the request to have default values returned.

Closes #3941
</description><key id="21319031">3942</key><summary>Add a GetFieldMapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-10-21T14:33:02Z</created><updated>2014-07-16T21:51:58Z</updated><resolved>2013-10-30T15:18:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-23T11:28:11Z" id="26897376">I am in the same boat as luca with putting `if (includeInDefaults)` in front of everything inside of `toXContent()` in the mappers is adding up to complexity. I am in favor of having a `toXContentEverything()` and the old `toXContent()` methods or at least having sth like `if (includeInDefaults) { } else { }` in `toXContent()` - even at the price of duplicating code I think this makes it way more readable.

Apart from that: from functionality point of view this is a huge +1
</comment><comment author="bleskes" created="2013-10-23T14:49:09Z" id="26911130">@javanna @spinscale thx for the review. Going back to the point you made about potentially having a `doXContentBodyWithDefaults()` method that will remove the need for `includeDefaults` tests. I see your point but to me it's a choice between evils. The down side of such a method is that when we add something we need to remember to add it both places and the code doesn't remind us to (if you don't notice there is another method, you'd miss it). Same goes for bug fixes in existing code. Unless someone feels strongly about it, my choice goes to leave it as is.
</comment><comment author="kimchy" created="2013-10-23T14:51:15Z" id="26911322">on the fence on this as well, leaning towards the current solution, where the serialization of a single mapping attribute is done in a single place (compared to over several places).
</comment><comment author="javanna" created="2013-10-23T14:57:50Z" id="26911950">I was more leaning towards a common method that does the `if`, as mentioned [above](https://github.com/elasticsearch/elasticsearch/pull/3942#discussion_r7157471), but there certainly are downsides to it as well (too many variations needed is the main one). I think I prefer those downsides anyway over the repeated `if`, but it's hard to tell without writing the code I guess... :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GetFieldMapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3941</link><project id="" key="" /><description>This new API allows retrieving the mappings for specific fields. This is offered as an alternative to traversing the results of the standard Get Mapping API, which returns the entire mapping for indices and types.

The new API is available under the following REST endpoints, allowing to resolve fields for multiple indices and types in one `GET` request:

```
/_mapping/field/{fields}
/{index}/_mapping/field/{fields}
/{index}/{type}/_mapping/field/{fields}
```

For example, the following call will return the mapping information for the `text` and `user.name` of an index of tweets:

```
curl -XGET "http://localhost:9200/twitter/tweet/_mapping/field/text,user.name"
```

Returns:

```
{
   "twitter": {
      "tweet": {
         "text": {
            "full_name": "text",
            "mapping": {
               "text": { "type": "string" }
            }
         },
         "user.name": {
            "full_name": "user.name",
            "mapping": { 
               "name": { "type": "string" }
            }
         }
      }
   }
}
```

Similar to other APIs, fields names can be one of the following (in resolution order):
- Full names
- Index names
- Field name (excluding complete path)

The response will use the same naming specified in the url:

```
curl -XGET "http://localhost:9200/_mapping/field/name"
```

returns

```
{
   "twitter": {
      "tweet": {
         "name": {
            "full_name": "user.name",
            "mapping": {
               "name": { "type": "string" }
            }
         }
      }
   }
}
```

Last, you request outputting default values (suppressed by default):

```
curl -XGET "http://localhost:9200/_mapping/field/name?include_defaults=true" 
```

```
{
   "twitter": {
      "tweet": {
         "name": {
            "full_name": "user.name",
            "mapping": {
               "name": {
                  "type": "string",
                  "index_name": "name",
                  "boost": 1,
                  "index": "analyzed",
                  "store": false,
                  "term_vector": "no",
                  "omit_norms": false,
                  "index_options": "positions",
                  "analyzer": "default",
                  "postings_format": "default",
                  "doc_values_format": "default",
                  "similariry": "default",
                  "fielddata": {},
                  "null_value": null,
                  "include_in_all": false,
                  "position_offset_gap": 0,
                  "search_quote_analyzer": "default",
                  "ignore_above": -1
               }
            }
         }
      }
   }
}
```
</description><key id="21317919">3941</key><summary>GetFieldMapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>feature</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-21T14:15:02Z</created><updated>2013-12-30T12:49:54Z</updated><resolved>2013-10-30T15:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="awick" created="2013-10-21T18:37:31Z" id="26743894">Will this support _meta?

Issue #2857
</comment><comment author="bleskes" created="2013-10-22T08:26:47Z" id="26785194">@awick sadly no. This does nothing more (or less) than exposing the current mapping infrastructure on a per field basis.
</comment><comment author="jugaadi" created="2013-10-30T12:15:39Z" id="27383991">+1 for _meta
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/common/collect/MapBuilder.java</file><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormatService.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingsFormatService.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/similarity/Similarities.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java</file><file>src/main/java/org/elasticsearch/index/similarity/SimilarityService.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file><file>src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsTests.java</file></files><comments><comment>Add a GetFieldMapping API</comment></comments></commit></commits></item><item><title>Clean up wording to reduce confusion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3940</link><project id="" key="" /><description>The description of the timeout parameter was worded misleadingly; it implied that the API would wait until the cluster reached the desired level and then stayed at that level for the timeout. I've tweaked the sentence to remove the risk of confusion.
</description><key id="21309692">3940</key><summary>Clean up wording to reduce confusion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels /><created>2013-10-21T11:38:17Z</created><updated>2014-06-18T04:20:17Z</updated><resolved>2013-10-22T11:09:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-10-22T11:09:35Z" id="26794219">Thanks for the change! I've merged it in.

Cheers,
Boaz
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #3940 from rboulton/patch-1</comment></comments></commit></commits></item><item><title>Guide not very readable smartphone portrait</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3939</link><project id="" key="" /><description>The elasticsearch guides with the menu to the right don't look very good in portrait mode:

![Portrait mode example](http://i.imgur.com/zWKUgla.png) .

In general, space is used very generous on the small screen.

I tried to poke around myself but had to realize https://github.com/elasticsearch/docs/blob/master/resources/styles.css does not contain the media queries. If someone could point me to the right version of main.css being used ... thanks.
</description><key id="21297381">3939</key><summary>Guide not very readable smartphone portrait</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfn</reporter><labels /><created>2013-10-21T06:23:01Z</created><updated>2015-03-22T15:09:59Z</updated><resolved>2015-03-22T15:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T18:03:42Z" id="51636885">@jeff4 you may want to add this to the list of things that need sorting on the website :)
</comment><comment author="javanna" created="2015-03-22T15:09:59Z" id="84628005">I think this is solved with the new [website](http://www.elastic.co/guide/en/elasticsearch/reference/current/search.html), looks much better now, closing!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce ES_STARTUP_MODE variable to define procrun --Startup mode, give possibility to override</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3938</link><project id="" key="" /><description>This makes it possible to override the startup mode
when installing the windows service (e.g. if auto startup mode is
required). default is still manual
</description><key id="21260972">3938</key><summary>Introduce ES_STARTUP_MODE variable to define procrun --Startup mode, give possibility to override</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">salyh</reporter><labels /><created>2013-10-19T11:49:18Z</created><updated>2014-06-13T16:56:01Z</updated><resolved>2013-10-24T10:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-23T17:41:37Z" id="26926404">A good suggestion. We're looking into aligning the *nix scripts as well (so for the most the same env variables are used in both cases).
Will keep you posted.
</comment><comment author="costin" created="2013-10-24T10:41:41Z" id="26982143">Hi
I have raised a proper issue for this ( #3962) and committed a fix for it on the 0.90 branch.
 I have changed the env variable names to be somewhat shorter, namely:

`ES_START_TYPE` and `ES_STOP_TIMEOUT`

Feedback welcome!
</comment><comment author="salyh" created="2013-10-24T10:55:36Z" id="26982858">f1d6ccc looks very good, thanks
</comment><comment author="costin" created="2013-10-24T11:05:37Z" id="26983412">Thanks for the feedback. Cheers!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>add stop timeout and start mode for windows service.bat</comment></comments></commit></commits></item><item><title>Fix small typo in terms lookup tests mapping.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3937</link><project id="" key="" /><description>terms -&gt; term and terms -&gt; arr.term as used in the actual tests.  The tests had a mapping defined but were actually using dynamic mapping since docs were indexing with a field name other than what was defined in the mapping.
</description><key id="21226835">3937</key><summary>Fix small typo in terms lookup tests mapping.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattweber</reporter><labels /><created>2013-10-18T15:40:25Z</created><updated>2014-07-16T21:51:59Z</updated><resolved>2013-10-18T15:57:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-10-18T15:40:38Z" id="26605932">/cc @javanna 
</comment><comment author="javanna" created="2013-10-18T15:57:23Z" id="26607314">Merged, thanks @mattweber !!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>doc update: phrase_len is not called phrase_length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3936</link><project id="" key="" /><description /><key id="21219076">3936</key><summary>doc update: phrase_len is not called phrase_length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-10-18T13:30:38Z</created><updated>2014-07-16T21:52:00Z</updated><resolved>2013-10-18T13:35:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T13:35:53Z" id="26595767">Thanks @nik9000 !!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow awareness attributes to be reset via the API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3935</link><project id="" key="" /><description>Currently we don't allow resetting the awareness
attribute via the API since it requires at least one
non-empty string to update the setting. This commit
allows resetting this using an empty string.

Closes #3931
</description><key id="21216985">3935</key><summary>Allow awareness attributes to be reset via the API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-18T12:48:48Z</created><updated>2014-07-16T21:52:00Z</updated><resolved>2013-10-18T14:27:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-10-18T14:22:44Z" id="26599403">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changed the minScore comparator from &gt; to &gt;=</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3934</link><project id="" key="" /><description>The parameter `min_score` implies that any document that has at least that score is included, but the current logic requires that a document has a higher score than the minimum specified.

This doesn't make sense to me, so I've changed the comparator from &gt; to &gt;=
</description><key id="21215864">3934</key><summary>Changed the minScore comparator from &gt; to &gt;=</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-10-18T12:22:04Z</created><updated>2014-07-02T13:31:40Z</updated><resolved>2013-11-29T20:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-11-29T15:08:11Z" id="29521491">+1, @javanna can you push it?
</comment><comment author="javanna" created="2013-11-29T15:09:30Z" id="29521570">@kimchy sure I'll take care of it
</comment><comment author="javanna" created="2013-11-29T20:44:53Z" id="29536788">Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Stats : last index and last delete timestamps added</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3933</link><project id="" key="" /><description>Needed this during some debugging of "stuck" current index counts. 
I thought it might be useful to others. 

It is also nice to have the last indexed info available if you want to use it as a check for an external caches potential staleness.

Example node stats output

```
{
  "cluster_name" : "yourcluster",
  "nodes" : {
    "P7R8Cdi_RRqdEFaOgiqdNg" : {
      "timestamp" : 1381956529533,
      "name" : "nodename",
      "transport_address" : "inet[/0.0.0.0:9300]",
      "hostname" : "hostname",
      "attributes" : {
        "master" : "true"
      },
      "indices" : {
        "docs" : {
          "count" : 24954926,
          "deleted" : 174148
        },
        "store" : {
          "size" : "22.6gb",
          "size_in_bytes" : 24365314778,
          "throttle_time" : "5.9m",
          "throttle_time_in_millis" : 357469
        },
        "indexing" : {
          "index_total" : 2642838,
          "index_time" : "16.6m",
          "index_time_in_millis" : 1000743,
          "index_current" : 0,
          "last_index_timestamp" : 1381956529533,
          "delete_total" : 0,
          "delete_time" : "0s",
          "delete_time_in_millis" : 0,
          "delete_current" : 0,
          "last_delete_timestamp" : 1381956527890
        },
        "flush" : {
          "total" : 115,
          "total_time" : "2.9m",
          "total_time_in_millis" : 178234
        }
      }
    },
```
</description><key id="21198515">3933</key><summary>Node Stats : last index and last delete timestamps added</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meconlin</reporter><labels><label>discuss</label></labels><created>2013-10-18T03:15:48Z</created><updated>2014-08-08T08:38:56Z</updated><resolved>2014-08-08T08:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T08:38:56Z" id="51576663">Hi @meconlin 

Thanks for the PR.  For caching purposes, I think the new query cache will be a nicer solution than external caching, #7161 (even though it only does aggs so far, docs will follow).  Also, we are planning on adding sequence IDs which will be a better indicator of change, so I'm going to close this.

thanks anyway
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>AND, OR, NOT queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3932</link><project id="" key="" /><description>Today ES only have support for AND, OR and NOT filters, but this doesn't allow us to use scoring with it.

Follows bellow a use case for it.

I currently have parents indexed in elastic search (documents) and child (comments) related to these documents. My first objective was to search for a document with more than N comments, based on a child query. Here is how I did it:

```
documents/document/_search
{
    "min_score": 0,
    "query": {
       "has_child" : {
            "type" : "comment",
            "score_type" : "sum",
            "boost": 1,
            "query" : {
               "range": {
                    "date": {
                        "lte": 20130204,
                        "gte": 20130201,
                        "boost": 1
                    }
                }
            }
        }
    }
}
```

I used score to calculate the amount of comments a document has and then I filtered the documents by this amount, using "min_score". Now, my objective is to search not just comments, but several other child documents related to the document, always based on frequency. Something like the query bellow:

```
documents/document/_search
{
    "query": {
        "match_all": {
        }
    },
    "filter" : {
        "and" : [{
            "query": {
                "has_child" : {
                    "type" : "comment",
                    "query" : {
                      "range": {
                        "date": {
                            "lte": 20130204,
                            "gte": 20130201
                        }
                      }
                    }
                }   
            }   
        },
        {
        "or" : [
            {"query": {
                "has_child" : {
                    "type" : "comment",
                        "query" : {
                          "match": {
                            "text": "Finally"
                        }
                    }
                }
                }
            },
            { "query": {
                "has_child" : {
                    "type" : "comment",
                        "query" : {
                          "match": {
                            "text": "several"
                        }
                    }
                }
                }  
            }
        ]
        }
    ]
    }
}
```

The query above works fine, but it doesn't filter based on frequency as the first one does. As filters are computed before scores are calculated, I cannot use min_score to filter each child query.

With AND, OR and NOT queries, I would be able to use min_score to filter results and still have powerful custom queries.
</description><key id="21197694">3932</key><summary>AND, OR, NOT queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mvallebr</reporter><labels /><created>2013-10-18T02:40:37Z</created><updated>2013-10-18T08:32:07Z</updated><resolved>2013-10-18T08:31:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T08:31:53Z" id="26579525">Answered on the mailing list: https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/qLXupHz0PKo .
You can use a bool query.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Awareness attributes can't be reset once they are set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3931</link><project id="" key="" /><description>once you set `"cluster.routing.allocation.awareness.attributes"` via the API you can't reset it to an empty list which would disable the awareness entirely. This happens since the `Settings.getAsArray` returns the default if the list is null or empty which essentially prevents an update to an empty list.
</description><key id="21182386">3931</key><summary>Awareness attributes can't be reset once they are set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-17T20:22:31Z</created><updated>2013-10-18T14:26:54Z</updated><resolved>2013-10-18T14:26:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationTests.java</file></files><comments><comment>Allow awareness attributes to be reset via the API</comment></comments></commit></commits></item><item><title>add reference for ember-data elasticsearch kit client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3930</link><project id="" key="" /><description>Add reference for client https://github.com/roundscope/ember-data-elasticsearch-kit  - an ember-data kit for both pushing and querying objects to Elasticsearch cluster
</description><key id="21162210">3930</key><summary>add reference for ember-data elasticsearch kit client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">OpakAlex</reporter><labels /><created>2013-10-17T15:39:03Z</created><updated>2014-06-30T22:48:58Z</updated><resolved>2013-10-25T06:15:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OpakAlex" created="2013-10-24T13:33:26Z" id="26992211">I think, this is client for Ember-data.js, because Ember.js is full stack technology.  
</comment><comment author="karmi" created="2013-10-25T06:06:06Z" id="27067402">@OpakAlex Sorry for not getting to you sooner. I think @javanna is right, and http://www.elasticsearch.org/guide/en/elasticsearch/client/community/current/integrations.html is the correct place for Ember.js integration.
</comment><comment author="OpakAlex" created="2013-10-25T06:15:07Z" id="27067776">ok, thanks! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Settings queue_size on index/bulk TP can cause rejection failures when executed over network</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3929</link><project id="" key="" /><description>The #3526 fix was not complete, it handled cases of on node execution, but didn't properly handle cases where it was executed over the network, and forcing the execution of the replica operation when done over the wire.

This relates to #3854
</description><key id="21151685">3929</key><summary>Settings queue_size on index/bulk TP can cause rejection failures when executed over network</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-17T13:15:27Z</created><updated>2013-10-17T14:06:47Z</updated><resolved>2013-10-17T14:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalAllocateDangledIndices.java</file><file>src/main/java/org/elasticsearch/transport/BaseTransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/TransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file></files><comments><comment>Settings queue_size on index/bulk TP can cause rejection failures when executed over network</comment><comment>The #3526 fix was not complete, it handled cases of on node execution, but didn't properly handle cases where it was executed over the network, and forcing the execution of the replica operation when done over the wire.</comment></comments></commit></commits></item><item><title>service.bat incorrectly assumes JRE will have a 'client' directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3928</link><project id="" key="" /><description>``` bat
rem Check JVM server dll first
set JVM_DLL=%JAVA_HOME%\jre\bin\server\jvm.dll

if exist "%JVM_DLL%" goto foundJVM

set JVM_DLL=%JAVA_HOME%\bin\client\jvm.dll

if exist "%JVM_DLL%" (
echo Warning: JAVA_HOME points to a JRE and not JDK installation; a client (not a server^) JVM will be used...
) else (
echo JAVA_HOME points to an invalid Java installation (no jvm.dll found in "%JAVA_HOME%"^). Existing...
goto:eof
)
```

I have a JRE installation on a Windows Server system. It has no `client` directory, but it does have a `server` directory: `C:\Program Files\Java\jre7\bin\server`
</description><key id="21146785">3928</key><summary>service.bat incorrectly assumes JRE will have a 'client' directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">stijnherreman</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-17T11:28:33Z</created><updated>2013-10-24T14:11:29Z</updated><resolved>2013-10-24T12:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-24T11:00:24Z" id="26983147">What's your `JAVA_HOME` variable? Did you install a JDK or JRE (client or server)? x64 or 32 bit?
From the looks of it, it seems you have installed the JDK (on 64 bit) but the JAVA_HOME is incorrectly defined.
</comment><comment author="stijnherreman" created="2013-10-24T11:15:41Z" id="26983874">I've installed an x64 JRE. 

Confirmed by the directory structure:

```
 Directory of C:\Program Files\Java

25/07/2013  12:31    &lt;DIR&gt;          .
25/07/2013  12:31    &lt;DIR&gt;          ..
25/07/2013  12:31    &lt;DIR&gt;          jre7

 Directory of C:\Program Files\Java\jre7

25/07/2013  12:31    &lt;DIR&gt;          .
25/07/2013  12:31    &lt;DIR&gt;          ..
25/07/2013  12:31    &lt;DIR&gt;          bin
25/07/2013  12:31             3.409 COPYRIGHT
25/07/2013  12:31    &lt;DIR&gt;          lib
25/07/2013  12:31                41 LICENSE
25/07/2013  12:31                47 README.txt
25/07/2013  12:31               451 release
25/07/2013  12:31           125.105 THIRDPARTYLICENSEREADME-JAVAFX.txt
25/07/2013  12:31           175.640 THIRDPARTYLICENSEREADME.txt
25/07/2013  12:31               983 Welcome.html
```

Confirmed by the registry key (`Java Development Kit` does not exist):

```
HKEY_LOCAL_MACHINE\SOFTWARE\JavaSoft\Java Runtime Environment
```

Confirmed by command line

```
&gt;java -version
java version "1.7.0_25"
Java(TM) SE Runtime Environment (build 1.7.0_25-b17)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)

&gt;javac -version
'javac' is not recognized as an internal or external command,
operable program or batch file.
```

My `JAVA_HOME` is set to `C:\Program Files\Java\jre7`
</comment><comment author="costin" created="2013-10-24T12:10:41Z" id="26986755">I've managed to reproduce this on Windows Server - it looks like on that particular OS the JRE installs a server jvm instead of the typical client one.

I've pushed a fix to 0.90 branch (master will follow) - let me know if it fixes your problem.

Cheers,
</comment><comment author="stijnherreman" created="2013-10-24T13:46:28Z" id="26993204">I've confirmed that this fixes the problem, thanks!
</comment><comment author="costin" created="2013-10-24T14:11:29Z" id="26995294">Great - thanks for the feedback!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>add detection of JRE server (JRE on Windows Server)</comment></comments></commit></commits></item><item><title>Fix markup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3927</link><project id="" key="" /><description>Just a tiny tiny typo
</description><key id="21146053">3927</key><summary>Fix markup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mfn</reporter><labels /><created>2013-10-17T11:10:50Z</created><updated>2014-07-16T21:52:01Z</updated><resolved>2013-10-21T14:12:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-21T14:12:13Z" id="26720113">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_geo_distance sort: Support for many-to-many geo distance sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3926</link><project id="" key="" /><description>What we'd like to see:

Ability to specify many geo points in geo distance sort, like this:

``` json
{
    "sort": [
        {
            "_geo_distance": {
                "geo_points.point": [
                    {
                        "lat": 59.959946,
                        "lon": 30.313819
                    },
                    {
                        "lat": 59.979788,
                        "lon": 30.304513
                    }
                ]
            }
        }
    ]
}
```

Use-case: each user has several points of interest and wants to find other users who are close to these points. For example, I have work, home, and favourite breakfast cafe. I'd like to find people who hang out near any of those places. Right now I should fire as many queries, as many points of interest I have, and then I need to merge results outside of elasticsearch. Having ability to use single query for this.

Looks like it could be quite trivial change, an extra loop to iterate requested points. I might be wrong about this.

I understand, that it could be very cpu intensive for querying with many points, but the point here is to use small amount of points.
</description><key id="21141142">3926</key><summary>_geo_distance sort: Support for many-to-many geo distance sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2013-10-17T09:21:36Z</created><updated>2014-08-04T09:43:18Z</updated><resolved>2014-07-31T15:31:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2013-10-22T13:41:57Z" id="26803422">Any feedback about this?
</comment><comment author="brwe" created="2013-10-22T14:11:30Z" id="26805996">Have you considered using the [function_score query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/query-dsl-function-score-query.html)? 
You can configure it so that people returned by a query are sorted by their closest distance to any number of  defined points. Is this what you want? 

Here is how the query would roughly look like:

```
"query": {
      "function_score": {
         "query": {
            put here which kinds of people you look for and also maybe put distance filters to get rid of people that are too far anyway
         },
         "functions": [
            {
               "gauss": {
                   "name of geo location field for person": {
                        "origin": "geo point of favorite cafe 1",
                        "scale": ".."
                  }
               }
            },
            {
               "gauss": {
                  "name of geo location field for person": {
                      "origin": "geo point of favorite cafe 2",
                      "scale": "..."
                   }
               }
            },
            ...put even more places...
         ],
         .. use the score of nearest place...
         "score_mode": "max"
      }
   }
```
</comment><comment author="bobrik" created="2013-10-23T16:14:30Z" id="26919007">@brwe I think It could work too, but `_geo_distance` would be easier to debug, because it tells you the distance. I also think `_geo_distance` should work faster in my case.

I tried `function_score` with query like this:

``` json
{
  "fields": [
    "geo_points"
  ],
  "size": 10000,
  "filter": {
    "and": [
      {
        "or": [
          {
            "geo_distance": {
              "geo_points.point": {
                "lat": 59.953478,
                "lon": 30.315557
              },
              "distance": "1.2km"
            }
          },
          {
            "geo_distance": {
              "geo_points.point": {
                "lat": 60.002,
                "lon": 30.298391
              },
              "distance": "1.2km"
            }
          }
        ]
      },
      {
        "terms": {
          "user_id": [
            51933602,
            63087823,
            45214178
          ]
        }
      }
    ]
  },
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "score_mode": "max",
      "functions": [
        {
          "gauss": {
            "geo_points.point": {
              "origin": {
                "lat": 59.953478,
                "lon": 30.315557
              },
              "scale": "1.2km"
            }
          }
        },
        {
          "gauss": {
            "geo_points.point": {
              "origin": {
                "lat": 60.002,
                "lon": 30.298391
              },
              "scale": "1.2km"
            }
          }
        }
      ]
    }
  }
}
```

Notice that I filtered only 3 users, because their score for some reason is 0. Response looks like this:

``` json
{
  "total": 3,
  "max_score": 0,
  "hits": [
    {
      "_index": "female",
      "_type": "users",
      "_id": "45214178",
      "_score": 0,
      "fields": {
        "geo_points": [
          {
            "point": "59.957,30.303"
          },
          {
            "point": "0.004,0.004"
          },
          {
            "point": "59.948,30.276"
          },
          {
            "point": "59.944,30.272"
          }
        ]
      }
    },
    {
      "_index": "female",
      "_type": "users",
      "_id": "63087823",
      "_score": 0,
      "fields": {
        "geo_points": [
          {
            "point": "59.956,30.318"
          },
          {
            "point": "0,0"
          }
        ]
      }
    },
    {
      "_index": "female",
      "_type": "users",
      "_id": "51933602",
      "_score": 0,
      "fields": {
        "geo_points": [
          {
            "point": "59.956,30.314"
          },
          {
            "point": "0,-0.004"
          }
        ]
      }
    }
  ]
}
```

There are points far away from requested, but there are closer points in there same objects too. I thought that best match should be used. Am I missing something?
</comment><comment author="brwe" created="2013-10-23T17:03:42Z" id="26923197">No, you did not miss something, I did. I completely misunderstood your request. `function_score` unfortunately does not work for fields with multiple values. Hope you did not waste too much time with it.
</comment><comment author="imotov" created="2013-10-24T00:19:53Z" id="26957577">Perhaps you can try implementing it as [script-based sort](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_script_based_sorting) that will calculated distances to all points of interest and then return the shortest one. 
</comment><comment author="bobrik" created="2013-10-24T06:19:28Z" id="26969068">@brwe maybe `function_sort` should fire an exception if it called against field with multiple values?

@imotov yep, that should work, but scripting is much slower than native support. But I'll try.
</comment><comment author="bobrik" created="2013-10-24T09:49:36Z" id="26979296">@imotov Looks like I cannot iterate array of objects in elasticsearch. I've found this thread: https://groups.google.com/forum/#!topic/elasticsearch/8Z2KwuPlyas

My mapping looks like this:

``` json
{
  "users" : {
    "properties" : {
      "geo_points" : {
        "properties" : {
          "point" : {
            "type" : "geo_point"
          }
        }
      }
    }
  }
}
```

and script looks like this:

``` mvel
current_to_any_geo_distance = 100;
if (!doc["geo_points"].empty &amp;&amp; !current_geo.empty) {
    foreach (point : doc["geo_points"].values) {
        distance = point["point"].arcDistanceInKm(current_geo.lat, current_geo.lon);
        if (distance &lt; current_to_any_geo_distance) {
            current_to_any_geo_distance = distance;
        }
    }
}
```

and elasticsearch complains like this:

``` json
{
  "took": 44,
  "timed_out": false,
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 1,
    "failures": [
      {
        "index": "female",
        "shard": 3,
        "status": 500,
        "reason": "RemoteTransportException[[search04][inet[/192.168.1.91:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[female][3]: query[ConstantScore(cache(_type:users))],from[0],size[20],sort[&lt;custom:\"_script\": org.elasticsearch.index.fielddata.fieldcomparator.DoubleScriptDataComparator$InnerSource@1dd5b162&gt;!]: Query Failed [Failed to execute main query]]; nested: CompileException[[Error: No field found for [geo_points] in mapping with types [users]]\n[Near : {... order = OLDER_ORDER; ....}]\n             ^\n[Line: 1, Column: 1]]; nested: ElasticSearchIllegalArgumentException[No field found for [geo_points] in mapping with types [users]]; "
      }
    ]
  },
  "hits": {
    "total": 0,
    "max_score": null,
    "hits": []
  }
}
```

Is that kind of iteration really supported in elasticsearch? It looks like bug to me, but I couldn't find an existing issue.

cc @dadoonet
</comment><comment author="imotov" created="2013-10-25T00:31:38Z" id="27046302">@bobrik - such iteration is going to work only on `source`. A `doc` field can only be obtain by the actual field name - `geo_points.point` in your case. `doc["geo_points"]` simply doesn't exist in the index. Here is an example that demonstrates the idea - https://github.com/imotov/elasticsearch-test-scripts/blob/master/multi_geo_search.sh.
</comment><comment author="bobrik" created="2013-10-25T06:42:56Z" id="27068890">@imotov looks like I forgot to submit my solution yesterday :)

I looked at source code and found that I can actually iterate `doc["geo_points.point"].values`, but it will be an array of `object`s with `lat` and `lon` properties, not `geo_point`s, therefore I cannot compute distance.

Looks like I can compute distance in pretty hardcore way:

```
import org.elasticsearch.common.geo.GeoDistance;
import org.elasticsearch.common.unit.DistanceUnit;

foreach (point : doc["geo_points.point"].values) {
    distance = GeoDistance.ARC.calculate(point.lat, point.lon, current_geo.lat, current_geo.lon, DistanceUnit.KILOMETERS);
}
```

I'm not sure if this is friendly. It's not in the docs for sure :)
</comment><comment author="bobrik" created="2014-07-31T15:39:29Z" id="50776518">Nice, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SortParserTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>_geo_distance sort: allow many to many geo point distance</comment></comments></commit></commits></item><item><title>NewRelic intigration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3925</link><project id="" key="" /><description>I'm using @NewRelic to monitor most of my stack (PHP) and since I use ElasticSearch quite a lot in my stack, it would be nice to have ES monitored too

I've added `-javaagent:/usr/share/elasticsearch/lib/newrelic.jar` to my JAVA_OPTS and it does show up in NewRelic - but only as a JVM, there is no data shown for http traffic - even though NewRelic should support Netty 

I've followed [this guide](https://docs.newrelic.com/docs/java/java-agent-manual-installation) - but my practical experience with Java is quite limited :)
</description><key id="21122997">3925</key><summary>NewRelic intigration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jippi</reporter><labels /><created>2013-10-17T00:24:58Z</created><updated>2014-07-18T10:31:17Z</updated><resolved>2014-07-18T10:31:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-20T16:01:34Z" id="26675791">Hey,

have you tried the community written new relic plugin (also listed at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html)?

It is available at https://github.com/viniciusccarvalho/elasticsearch-newrelic

I have no experience with it, but would be interested to hear any feedback.
</comment><comment author="jippi" created="2013-10-20T16:45:07Z" id="26676776">I have not, it's listed as supporting only up to 0.90.2 - and I run the latest stable :)
</comment><comment author="jippi" created="2013-10-20T16:45:54Z" id="26676792">What I also want is netty statistics and the internal stuff exposed :)
</comment><comment author="spinscale" created="2013-10-21T09:30:09Z" id="26703229">Hey,

you could just try the plugin, if it is available for 0.90.3 it might work on 0.90.5 as well.

What kind of netty statistics do you need to be exposed? Can you be more exact? Maybe they are already exposed in the elasticsearch statistics as already...
</comment><comment author="spinscale" created="2014-07-18T10:31:17Z" id="49417148">Closing this for new, as NewRelic support will not be added in the core but rather want this available as a plugin (maybe someone can jump in and update the existing plugin?). Also, maybe https://github.com/secondimpression/newrelic_elasticsearch_agent helps?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature Request: pre-select terms in TermVector request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3924</link><project id="" key="" /><description>Dear All

  I have a feature request regarding the TermVector API. I was really happy to see this commit, which I had fledgingly written as a plugin before. Thanks @brwe ! Though, would it be possible to submit a list of terms and only have the TermVector returned for those? My hope is that it's considerably faster than the full request. I have a use case, where I know the terms for which I need the information before making the request.

   Some pointers of how to do it myself are appreciated, too, though I am afraid my solution won't be as efficient.

Best and many, many thanks for the great work.
Max.
</description><key id="21111462">3924</key><summary>Feature Request: pre-select terms in TermVector request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">mhoffman</reporter><labels><label>feature</label></labels><created>2013-10-16T21:19:58Z</created><updated>2014-08-08T18:02:34Z</updated><resolved>2014-08-08T18:02:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-10-17T11:30:01Z" id="26496727">Thanks! I am glad to hear that the term vector api is useful. I want to add that @bleskes deserves at least half the credit!

There is another issue related to gathering statistics on terms(#3920). Is this similar to what you need?
Could you describe your use case? 
</comment><comment author="mhoffman" created="2013-10-17T15:42:37Z" id="26518514">Thanks for the quick response. So big shoutout to @bleskes, too: Thank you! No in my case I am actually after the precise offset, but typically only of a few terms (say 2-5). To describe this in general terms: I am doing a distance based scoring for things in the text. Can I send you a prototype privately?
</comment><comment author="brwe" created="2013-10-19T13:26:43Z" id="26649649">If you by _privately_ mean that you want to push a branch to your repo without pull request and then discuss it here - sure! Just link the commit or branch here. 
</comment><comment author="mhoffman" created="2013-10-22T05:04:02Z" id="26777429">Thanks, for taking the time. I have tried to do that in a low-brow way and the result is at
User/Project@SHA: mhoffman/elasticsearch@5ed795c9306ced7da937c55fef5f8c90683266c2 .

Now, I have two problems so far:
- There is some speed-up compared to a full termvector, but it is less than I thought
- If the selected term(s) is not in the document, elasticsearch returns a 500, EOFException. Which is probably not desirable.

Any suggestions?
</comment><comment author="brwe" created="2013-10-22T17:02:31Z" id="26821620">Thanks for the commit! I assumed you wanted to discuss e7c1e9e980da986, since 5ed795c9306ced7da does some percolator things, right? Let me know if I am mistaken.

About the speed-up: I could imagine the speedup is not as great as expected because all term info is loaded from disc no matter how many of terms you actually return. Disc IO probably influences the performance most. I still think the change is useful for large documents with many terms when only few are requested.
What kind of documents did you use for measuring the speedup?

About the EOF: We should decide if a requested term should be returned with frequency 0 if it is not in the doc, or not return missing terms at all. I prepared a commit fixing the EOF for the former and added a comment for the latter, just so that you know where the changes have to be made here: 27bc8fd813ae8d342527c4f7d4d36e5b4bcaaddf

What do you think: Return the term with frequency 0 or not return it at all if it is not the document?
</comment><comment author="mhoffman" created="2013-10-22T17:45:21Z" id="26824998">  Thanks for the corrections! Sorry, about the confusion with the commit link. I must have gotten something wrong with linking into user-specific commits. I am also a bit confused how this 5ed795c you link to comes turn out to be there? I definitely didn't intend to touch any percolator stuff. Is it coincidence that the first 7 characters match?

  Anyhow, you seem to have gotten the right commit and in my use-case both solutions work equally well. Though one could argue that given that Disk IO will dominate the runtime of this request anyways, returning a 0 would simply make the result a bit more self-explaining.

Thanks again.
</comment><comment author="brwe" created="2013-10-23T12:47:47Z" id="26901527">Would you like to make a pull request for your changes? 
</comment><comment author="mhoffman" created="2013-10-25T07:16:55Z" id="27070079">Sorry for the delay. Why don't you go ahead since you have the last version
in your repo already.

2013/10/23 Britta Weber notifications@github.com

&gt; Would you like to make a pull request for your changes?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3924#issuecomment-26901527
&gt; .

## 

Max J. Hoffmann
Tel: +4989 289 13807 (office)
Room CH62115
TU München
Lichtenbergstr. 4
D-85747 Garching
</comment><comment author="brwe" created="2013-10-25T13:56:39Z" id="27094416">ok, I'll do that.
</comment><comment author="brwe" created="2013-11-13T17:48:36Z" id="28416805">About the speedup: With the current implementation, we load the whole term vector for a document. This makes sense if you need all the terms or do not know in advance which term is requested but also makes it slow.
For pre-selected terms the current implementation (see commits above) we also load the term vector and only keep the terms that we are interested in. I am now wondering if this makes sense at all for pre-selected terms. We do not need the full term vector and could also get all the information from the DocsEnum of the terms - this might be quicker than what we do right now. 
Summoning @s1monw here because I am unsure if it really is quicker in this case.

Also, please take a look at pr #4161. It implements access to term vectors in a script. I implemented it wrong there as well (load term vectors and get the information from there instead from the standard DocsEnum) but I can fix that. 
If the script term vector access gives you all you need, than maybe you do not even need the pre-selected term vectors in the _termvector api anymore?

Also, sorry for the delay.
</comment><comment author="brwe" created="2014-01-02T10:32:42Z" id="31445583">I just pushed the script api for term statistics (pr #4161) Could you check if this allows you to do all you need?
</comment><comment author="mhoffman" created="2014-02-09T21:37:02Z" id="34587653">Hi

Sorry for the slow reply. The function_score query seems to work as the
docs promise (at least using 'mvel' and 'native'). Though my actual
problem, that made me look for term vectors etc., is still too complex for
this query type: I would need to access the term vector of the _parent_ of
the object that I am scoring with the script. I guess this notion was
expressed before in #1071 and #761 but apparently easier said than done.

I think this could still be a nice feature and since parent/child object
follow the same routing shouldn't all too complicated. Any comments?

2014-01-02 11:33 GMT+01:00 Britta Weber notifications@github.com:

&gt; I just pushed the script api for term statistics (pr #4161https://github.com/elasticsearch/elasticsearch/pull/4161)
&gt; Could you check if this allows you to do all you need?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3924#issuecomment-31445583
&gt; .

## 

Max J. Hoffmann
Tel: +4989 289 13807 (office)
Room CH62115
TU München
Lichtenbergstr. 4
D-85747 Garching
</comment><comment author="brwe" created="2014-02-12T10:00:45Z" id="34854215">That would call for a different issue.

Just to be sure: Do you need both parent and child statistics in the same script? Can you elaborate a little on what exactly you need maybe with a small example?
</comment><comment author="mhoffman" created="2014-02-15T17:40:11Z" id="35162161">Just played with the script_score feature and thinks this allows a really
nice developing workflow. Sweet!

 My parent/child use case is the following (I hope this makes sense). In my
application I store my data with parents and children and each parent has
many children (~100). The children are very small in size (4 fields, short
strings each) but the parent has one large field (~1e6 characters). So from
an index size point of view it makes sense to keep the big field with the
parent once instead of with each of the many children

Then the 'functions' part of the 'function_score' request could look like
below, and it is run on the children doc_type but the '_parent' refers to
the respective parent. I think this could make sense since (if I understand
correctly) parent and child are always routed to the same node. I would
like to have a look into this, but it might take me a couple of days.

 'functions' : [{
                        'script_score': {
                            "params" : {
                                'pos': 520,
                                'terms': ['this','system'],
                            },
                            'script': """
                                score = 0;
                                for (term: terms) {
                                    offsets = _parent['body'].get(term,
_OFFSETS | _CACHE);
                                    for (offset: offsets){
                                        score = score + exp(-(pos -
offset.startOffset)**2 \* (-.0001));
                                    }
                                }
                                score

```
                        """,
                                    }
                }],
```

2014-02-12 11:01 GMT+01:00 Britta Weber notifications@github.com:

&gt; That would call for a different issue.
&gt; 
&gt; Just to be sure: Do you need both parent and child statistics in the same
&gt; script? Can you elaborate a little on what exactly you need maybe with a
&gt; small example?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3924#issuecomment-34854215
&gt; .

## 

Max J. Hoffmann
Tel: +4989 289 13807 (office)
Room CH62115
TU München
Lichtenbergstr. 4
D-85747 Garching
</comment><comment author="brwe" created="2014-05-06T06:13:01Z" id="42270734">Hi,

sorry for the late reply. Can you check if the `has_parent` query together with the function score would solve the problem?

Using several documents within a script for computing the score is not supported yet and would need a new issue. Let me know if I can close this one. Thanks!
</comment><comment author="mhoffman" created="2014-05-27T14:52:09Z" id="44286669">Looks good to me.
Am 06.05.2014 08:13 schrieb "Britta Weber" notifications@github.com:

&gt; Hi,
&gt; 
&gt; sorry for the late reply. Can you check if the has_parent query together
&gt; with the function score would solve the problem?
&gt; 
&gt; Using several documents within a script for computing the score is not
&gt; supported yet and would need a new issue. Let me know if I can close this
&gt; one. Thanks!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3924#issuecomment-42270734
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs on availability highlight_query missleading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3923</link><project id="" key="" /><description>The docs on `highlight_query` from http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html at the top breadcrumb say:

"Reference [0.90] » Search APIs » Request Body Search » Highlighting"

Unfortunately it's not clear that this does not mean any specific 0.90 release but actually talk about the 0.90 branch. The support for `highlight_query` was added somewhen after the 0.90.5 release in September.

It would be really nice if that docs what mention that this is an unreleased feature and, once released, mentions from which release on.
</description><key id="21105534">3923</key><summary>Docs on availability highlight_query missleading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfn</reporter><labels /><created>2013-10-16T19:57:53Z</created><updated>2013-10-18T19:44:46Z</updated><resolved>2013-10-18T15:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-10-16T20:08:25Z" id="26452876">I'd add a little [0.90.6] thing to the doc like the one on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-phrase.html but when I try to build the docs locally I don't get the pretty underlining and stuff.  Is that some other program I have to install?  Can it be made to say [coming in 0.90.6]?
</comment><comment author="mfn" created="2013-10-17T06:05:54Z" id="26481196">I'd argue that just having 0.90.6 is still not clear enough. I understand that both 0.90 and master refer to documentation of not-yet-released software.

IMHO 0.90 should properly reflect that fact and say something like 0.90-dev .

Ideally the documentation would be provided for every past release (to a certain point back), current 0.90-dev and master ... or something like that.
</comment><comment author="nik9000" created="2013-10-17T12:33:37Z" id="26500072">As a user more than a contributor I'd appreciate a blob that said "[coming in 0.90.6]" or something like that so I know that the feature exists but is unreleased.  It shouldn't just say "[0.90.6]" because it isn't clear that that isn't released yet.  I don't like "[0.90-dev]" much either because it is slightly less clear that the feature is unreleased (still pretty clear though) and it doesn't say for which release the feature is targeted.

As a sometimes contributor it'd be really convenient if I could add the macro to the docs that makes the "[0.90.4]" thing and it'd make a "[coming in 0.90.6]" if 0.90.6 is not yet released.

It looks like @clintongormley is working on Elasticsearch docs.  It'd be great to get his opinion on this.  Also, while I'm using github's magic mentioning summoning, it'd be great if there were actual instructions for how to build the docs including stuff like the [0.90.4] macros.  Without that I'm just adding documentation blind.
</comment><comment author="clintongormley" created="2013-10-17T13:48:55Z" id="26505753">Consider me duly summoned :)

The reason we don't have separate docs for 0.90.4, 0.90.5, 0.90.6 etc is that those releases are tags on the 0.90 branch.  If we built the docs based on those tags, then we wouldn't be able to push doc corrections until the next release (or we'd end up with so many branches that it would be overwhelming).

Regarding the added/deprecated macros, there are two forms:
- inline, eg see `enable_position_increments` on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html
- block, eg see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#query-dsl-function-score-query

I've purposely kept the inline form brief, either just a version number or a crossed out version number, with more info if you hover over it. 

So the `coming[0.90.6]` version would need a `will_be_deprecated[0.90.6]` analog (open to suggestions on the naming :) ). But rendering the inline versions of these as `[coming in 0.90.6]` or `[will be deprecated in 0.90.6]` seems to defeat the point of keeping it short.

The other thing is we'd need to change all the `coming` macros to `added` macros when doing a new release, but that'd be easy enough to do with a little script (Go Perl Go!).

The above are my musings - in general I think we should do something like this, just looking for consensus on exactly what to do.

@nik9000 Your wish is granted :)  https://github.com/elasticsearch/docs
</comment><comment author="nik9000" created="2013-10-17T20:39:36Z" id="26550639">&gt; The other thing is we'd need to change all the coming macros to added macros when doing a new release, but that'd be easy enough to do with a little script (Go Perl Go!).

Or you could keep a list of unreleased versions and make the `added` macro spit out the coming rendering if the version it is referring to is unreleased.

I don't think [coming in 0.90.6] is _too_ long for the inline form and it should be pretty rare because it'll change to [0.90.6] as soon as it is released.  I think the readability outweighs the brevity in this case.
</comment><comment author="nik9000" created="2013-10-17T21:23:06Z" id="26554257">&gt; Or you could keep a list of unreleased versions and make the added macro spit out the coming rendering if the version it is referring to is unreleased.

I had a peak at this and it would require a little bit of painful xsl hackery.  It'd probably be worth just using a new macro and the find/replace script instead.
</comment><comment author="clintongormley" created="2013-10-18T09:19:42Z" id="26582128">So what do we use for the will_be_removed_in[0.90.6] macro and its rendering?
</comment><comment author="nik9000" created="2013-10-18T12:36:58Z" id="26592125">I was thinking about that - maybe it isn't really important to have a different rendering for will be removed in.  I mean, the worse that could happen is someone stops using a feature before it is removed.  That isn't as bad as trying to use an unreleased feature.  I think.
</comment><comment author="clintongormley" created="2013-10-18T15:55:46Z" id="26607183">OK - this has been added now. See https://github.com/elasticsearch/docs/commit/18071dc1885a23aa33c0f0de58e581c6276e85d5

and for example:
- http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules.html
- http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_accuracy_control
</comment><comment author="clintongormley" created="2013-10-18T16:19:59Z" id="26609113">I've also added a coming[] flag to the highlighting docs: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#_highlight_query
</comment><comment author="mfn" created="2013-10-18T19:44:46Z" id="26624191">@clintongormley Absolutely fantastic and much appreciated! It's not often that the documentation is ahead of the product ;-) , kudos to everyone.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't honor lowercase expanded terms in regex query generated from query string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3922</link><project id="" key="" /><description>On the mailing list a user was using a query string query against an unanalyzed field that contained values such as:

5.2
5.3-SNAPSHOT

He wanted to return all documents that were not a snapshot version so they used a query string like `-version:/.*SNAPSHOT/`.  This did not work.  Turns out that `lowercase_expanded_terms` was turning the regex into `-version:/.*snapshot/` which doesn't match the unanalyzed field due to case.

He was using Kibana which does not currently expose the lowercaseExpandedTerms option, so the user really had no way to fix this without changing the mapping and reindexing or patching kibana.

Should the lowercase_expanded_terms option even be honored in a regex query?  I would think no.

I did not see a way to set a case insensitive flag on the lucene regex query and the `(?i)` flag from java regex did not work.
</description><key id="21100254">3922</key><summary>Don't honor lowercase expanded terms in regex query generated from query string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>adoptme</label></labels><created>2013-10-16T18:36:21Z</created><updated>2015-08-24T11:19:10Z</updated><resolved>2015-08-24T11:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-10-16T18:51:04Z" id="26446407">Or at least we shouldn't lowercase expanded terms when we're querying a `not_analyzed` field
</comment><comment author="amitelad7" created="2014-03-26T09:33:38Z" id="38663992">+1 running into this issue as well
</comment><comment author="clintongormley" created="2014-09-05T10:14:16Z" id="54607508">We should add a (default) third setting for the `lowercase_expanded_terms` of  `auto`.  That will only lowercase if the field is analyzed, but not if it is not analyzed.
</comment><comment author="NickMeves" created="2015-06-09T18:45:24Z" id="110461559">+1

Was about to open a new issue then I stumbled across this.  Running into this alot lately with both Kibana and Graylog.

Issue is with not_analyzed field using regex for us as well.
</comment><comment author="khadrin" created="2015-07-20T19:58:39Z" id="123010380">:+1: 
</comment><comment author="pickypg" created="2015-08-19T23:20:20Z" id="132823737">I wonder why we even need this setting. The analysis process should take care of lowercasing, if it's desirable for the fields in question, which is not a query-global decision, and it should otherwise not touch the entry.
</comment><comment author="pickypg" created="2015-08-19T23:21:20Z" id="132823909">Highly related to #9978
</comment><comment author="clintongormley" created="2015-08-24T11:19:10Z" id="134156611">Closing in favour of #9978
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java API Documentation (0.90+) needs update for accessors in Facets docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3921</link><project id="" key="" /><description>Just noticed that the doc for the Facets (e.g. http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/java-facets.html#_use_facet_response_7) contains lots of .facets() calls when it's been changed to getFacets()

I noticed this issue before with the MultiSearchResponse example. Either a lot of people don't use the latest API, or the docs with Java ;-)
</description><key id="21087063">3921</key><summary>Java API Documentation (0.90+) needs update for accessors in Facets docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">derryos</reporter><labels><label>docs</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-16T15:15:21Z</created><updated>2013-10-17T07:50:43Z</updated><resolved>2013-10-17T07:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-16T15:36:10Z" id="26429173">Ha thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Java API Documentation (0.90+) needs update for accessors in Facets docs</comment><comment>Closes #3921.</comment><comment>(cherry picked from commit a753c48)</comment></comments></commit></commits></item><item><title>Term Count on Search Results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3920</link><project id="" key="" /><description>Would anyone else be interesting in getting elasticsearch to return a count of the terms in a field in the search results?  If you (like me) need to return a word count of a field then this could be useful to you.  I also could get a count of distinct terms but I'm not super sure who'd use it.  I was thinking the api could be something like this:

``` bash
curl -XPOST "http://localhost:9200/test/test/_search?pretty" -d '{
  "fields": [ "foo._term_count" ],
  "query": {
    "query_string": {
      "query": "findme"
    }
  }
}'
```

And it'd return `"foo._term_count" : 6,` in the results.

It'd require `term_vector`s to be stored but not offsets or positions.  Since it'd count the terms on each search result it'd be similar to highlighting using the FVH but faster because it does essentially no work other than the term vector scanning.

I don't imagine you'd be able to sort by them.
</description><key id="21081817">3920</key><summary>Term Count on Search Results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>feature</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-10-16T13:59:58Z</created><updated>2013-12-03T10:05:54Z</updated><resolved>2013-12-03T10:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-10-17T11:29:29Z" id="26496700">Might this requirement be similar to #3924 ? Also I am curious: What is your use case? 
</comment><comment author="nik9000" created="2013-10-17T12:47:02Z" id="26500890">Sorry I wasn't clear.  On my search results page I have to return a word count of one of the fields for every search result.  It happens to be my longest field.  And it has to support scriptio continua languages so I can't do something simple like count the number of spaces in my app and save that number to ES to retrieve with the search results. Anyway, Elasticsearch has a word count already in the form of the per field per document term vectors that I already store to use the FVH.  Also luckily I process that field with an analyzer that doesn't add synonyms or funky word breaks.  If I can ask Elasticsearch to count the terms in that field that'll give me my word count.

Anyway, it doesn't what I need is pretty simple in comparison to the term vector api.  I won't be listing terms and I only want term information for a single document.  I also want it bundled in the search results so I don't have to make any additional requests.

I'll send a pull request that implements this today or tomorrow which should make it crystal clear
</comment><comment author="s1monw" created="2013-10-17T12:54:11Z" id="26501379">just for kicks, can you build a customer analyzer that consumes all tokens and returns the number of tokens in the field as a token and then sort by it. You would need to parse the string but it would work no?
</comment><comment author="synhershko" created="2013-10-17T13:03:17Z" id="26502040">+1. Use cases can include faceting, scripted scoring, record linkage and whatnot.

@s1monw all that is required is a custom TokenFilter really, but that token doesn't have access to the IW / Document object so you will need to go through the analysis chain twice
</comment><comment author="nik9000" created="2013-10-17T18:09:28Z" id="26533135">&gt; record linkage

Sorry, what do you mean?

&gt; custom TokenFilter 

I like this idea.  In that case it'd make sense to build the field in the mapping, maybe like this:

``` bash
curl -XPUT http://localhost:9200/test/test/_mapping?pretty -d'{
  "test" : {
    "properties": {
      "foo" : {
        "type": "string",
        "store": "yes",
        "write_term_count" : "foo_term_count"
      },
      "foo_term_count" : {
        "type": "integer",
        "store": "yes"
      }
    }
  }
}
```

It'd be a pain to have to use the custom analyzer and analyze everything twice but it'd be worth it if it enables lots of fun features.  I'll have a look later today I think.
</comment><comment author="synhershko" created="2013-10-17T20:58:26Z" id="26552317">Record linkage is whenever you want to find similar documents, and word count can be a good hint for that.
</comment><comment author="javanna" created="2013-10-18T15:13:16Z" id="26603687">This other issue looks similar to what was asked here, although it proposes a separate api for it: #640 .
</comment><comment author="synhershko" created="2013-10-19T16:37:45Z" id="26653493">I think someone is confusing Word-Count in a field of a specific document with Term Count of all documents in a field. Not sure who that is, though :)
</comment><comment author="javanna" created="2013-10-19T17:27:54Z" id="26654646">Indeed, that other issue is a completely different story, I should have read more carefully. Thanks for clarifying that @synhershko 
</comment><comment author="nik9000" created="2013-10-21T21:54:31Z" id="26760445">I got this working today.  I'll send a pull request for it as soon as it passes all of its tests.  Github has helpfully created a link to my implementation above for anyone curious.  The unit test covers returning the count in the search results, searching for it via a range query, and faceting.  It covers counting both single and multi-valued fields both on the root and inside of an object.  For multi-valued fields it writes multiple term counts - it doesn't add them.
</comment><comment author="nik9000" created="2013-10-21T22:09:02Z" id="26761390">Also, while I think about it I'm pretty sure I did a few things wrong and would love some tips on the right way:
1.  Create a new package for the implementation.  I'm sure there is some place simple where it belongs.
2.  I'm a bit hacky in the way that I override the Long field implementation and in the way I use that field regardless of what field the term counts are actually mapped to. Now that I think about it I didn't do a lot of testing around mapping the term count to things other than long.  I mean, it won't work at all if you map it to something non-number, but short and int and the like should work as expected.
3.  Everything else I haven't thought of:)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Errors with carrot2 clustering plugin - lingo3g module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3919</link><project id="" key="" /><description>Hello everybody, here are some log excerpt from ES:

[2013-10-16 10:36:43,867][INFO ][lingo3g.cc               ] Lexical resources reloaded.
[2013-10-16 10:38:27,315][DEBUG][action.search.type       ] [Blevins, Sally] [108] Failed to execute fetch phase
java.lang.RuntimeException: cannot invoke method: substring
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessor.getValue(MapAccessor.java:39)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
    at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:192)
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:75)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:197)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:414)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: cannot invoke method: min
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.StaticReferenceAccessor.getValue(StaticReferenceAccessor.java:31)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.executeAll(MethodAccessor.java:140)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)
    ... 17 more
Caused by: java.lang.RuntimeException: cannot invoke method: length
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessor.getValue(MapAccessor.java:39)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.executeAll(MethodAccessor.java:140)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)
    ... 23 more
Caused by: java.lang.NullPointerException
    at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)
    ... 30 more

which appears a couple of times, and is then followed by:

[2013-10-16 10:38:37,725][INFO ][lingo3g.cc               ] Lexical resources reloaded.
[2013-10-16 10:38:39,028][DEBUG][action.search.type       ] [Blevins, Sally] [161] Failed to execute fetch phase
java.lang.NullPointerException
    at ASMAccessorImpl_10516961681381912712750.getValue(Unknown Source)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)
    at ASMAccessorImpl_18419208811381912707400.getValue(Unknown Source)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)
    at ASMAccessorImpl_13890246471381912707370.getValue(Unknown Source)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
    at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:192)
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:75)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:197)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:414)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)

and there are loads of pages with this previous message
</description><key id="21071433">3919</key><summary>Errors with carrot2 clustering plugin - lingo3g module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">cless91</reporter><labels /><created>2013-10-16T10:19:50Z</created><updated>2013-10-21T13:42:16Z</updated><resolved>2013-10-21T13:42:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-20T16:18:38Z" id="26676195">Hey,

first I have not used the carrot2 plugin, but it looks as if those two messages are independent from each other from my 10000 feet perspective at the moment. The first one is informational about the successful loading from a lingo feature, where as the second occurs because of a search being executed.

Several blind questions following, where I need some help from your side:
- Maybe the carrot query executes a search on startup to load some information (dont have a lot of experience with carrot). Do you execute a search operation manually or does it happen when starting up?

Can you provide the log file, which reveals the original source from the NullPointerException? I would like to know/find out if this is a problem with the carrot2 module or with elasticsearch itself.

Also which elasticsearch version are you using? Did you try with a version which is supported by the module?
</comment><comment author="cless91" created="2013-10-21T13:20:52Z" id="26715962">Hello, i solved the problem, it was a field in the request i send which was
null and caused ES or the plugin to fail.
It was :
 "script_fields" : {
        "content" : {
//          "script" : "_source.content.substring(0,Math.min(200,
_source.content.length()))"

Now it is:
"script_fields" : {
        "content" : {
            "script" : "if (_source.content==null) return
null;_source.content.substring(0,Math.min(200, _source.content.length()))"
and it works just fine.

Sorry i should have answer earlier but i got caught in some other tasks and
just forgot

2013/10/20 Alexander Reelsen notifications@github.com

&gt; Hey,
&gt; 
&gt; first I have not used the carrot2 plugin, but it looks as if those two
&gt; messages are independent from each other from my 10000 feet perspective at
&gt; the moment. The first one is informational about the successful loading
&gt; from a lingo feature, where as the second occurs because of a search being
&gt; executed.
&gt; 
&gt; Several blind questions following, where I need some help from your side:
&gt; - Maybe the carrot query executes a search on startup to load some
&gt;   information (dont have a lot of experience with carrot). Do you execute a
&gt;   search operation manually or does it happen when starting up?
&gt; 
&gt; Can you provide the log file, which reveals the original source from the
&gt; NullPointerException? I would like to know/find out if this is a problem
&gt; with the carrot2 module or with elasticsearch itself.
&gt; 
&gt; Also which elasticsearch version are you using? Did you try with a version
&gt; which is supported by the module?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3919#issuecomment-26676195
&gt; .
</comment><comment author="spinscale" created="2013-10-21T13:42:16Z" id="26717532">happy you solved it. Thanks for the feedback!

Closing the ticket then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parse Exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3918</link><project id="" key="" /><description>Hi,
If I execute the follow query its shows a exception,
{
  "from" : 0,
  "size" : 20,
  "query" : {
    "multi_match" : {
      "fields" : [ "Description", "jobStatus","rxCustomerID" ],
"query" : "no"
    }
  },
  "sort" : [ {
    "_uid" : {
      "order" : "desc"
    }
  } ],
  "highlight" : {
    "pre_tags" : [ "&lt;b&gt;" ],
    "post_tags" : [ "&lt;/b&gt;" ],
    "fields" : {
      "*" : { }
    }
  }
}

Note: The field rxCustomerID is long

Exception: NumberFormatException[For input string: "no"]; }]

Can anyone explain how to resolve that?

I need the query for multi_match only
</description><key id="21061530">3918</key><summary>Parse Exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ashwin-raj</reporter><labels /><created>2013-10-16T05:50:39Z</created><updated>2013-10-16T08:26:18Z</updated><resolved>2013-10-16T08:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-10-16T08:26:18Z" id="26400507">You can add a `lenient: true` option to the `multi_match` query to make it ignore parsing problems (like when searching with a non-numeric string on a numeric field):

```
curl -XPOST "http://localhost:9200/index2/_search" -d'
{
   "from": 0,
   "size": 20,
   "query": {
      "multi_match": {
         "fields": [
            "Description",
            "jobStatus",
            "rxCustomerID"
         ],
         "query": "no",
         "lenient": true
      }
   }
}'
```

I will add this to the documentation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reject index requests that contain metadata fields in the request body.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3917</link><project id="" key="" /><description>Metadata fields (_routing, _parent, _ttl) can also be specified in the request body and in certain scenarios they don't get indexed (just stored in the source). 

Example (ttl is not configured):

``` json
{
"_ttl" : 1
}
```

A term query on `_ttl` field would fail in the case, this can be confusing, especially when the ttl feature isn't used at all.

In the case that metadata fields are in the request body and these fields don't get indexed then this index request should fail.
</description><key id="21044568">3917</key><summary>Reject index requests that contain metadata fields in the request body.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label></labels><created>2013-10-15T21:06:22Z</created><updated>2014-08-08T11:29:40Z</updated><resolved>2014-08-08T11:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T20:20:28Z" id="37459108">@martijnvg are you gonna work on this?
</comment><comment author="clintongormley" created="2014-08-08T11:29:40Z" id="51590234">Close in favour of #6736 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`ignore_indices=missing` option should be supported on closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3916</link><project id="" key="" /><description>When running a search on an alias which redirect queries to a closed index, search fails:

```
{"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}
```

Adding `ignore_indices=missing` has no effect here.

``` sh
curl -XDELETE 'http://localhost:9200/toto1';echo
curl -XDELETE 'http://localhost:9200/toto2';echo
curl -XPOST 'http://localhost:9200/toto1/test/1?refresh' -d '{ "user" : "kimchy" }';echo
curl -XPOST 'http://localhost:9200/toto2/test/1?refresh' -d '{ "user" : "kimchy" }';echo
curl -XPOST 'http://localhost:9200/_aliases' -d '{ "actions" : [ 
    { "add" : { "index" : "toto1", "alias" : "toto" } }, 
    { "add" : { "index" : "toto2", "alias" : "toto" } } 
] }';echo
curl -XPOST 'http://localhost:9200/toto2/_close'; echo

# This one should not fail
curl -XPOST 'http://localhost:9200/toto/_search?ignore_indices=missing' -d '{ "query" : { "match_all" : {} } }';echo

# This one should fail
curl -XPOST 'http://localhost:9200/toto/_search' -d '{ "query" : { "match_all" : {} } }';echo
```

We should support that option and ignore all closed indices when `ignore_indices=missing` is set.
</description><key id="21028529">3916</key><summary>`ignore_indices=missing` option should be supported on closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-10-15T16:39:57Z</created><updated>2014-02-10T19:58:42Z</updated><resolved>2014-02-10T19:58:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2013-10-15T16:57:42Z" id="26352425">I think it would be better to support the `ignore_indices=missing,closed` expression. This would allow for future variations for other cases and suggest correct user expectations?
</comment><comment author="dadoonet" created="2014-02-03T07:43:28Z" id="33929883">@martijnvg I think we can close this one, right?
</comment><comment author="martijnvg" created="2014-02-10T19:52:24Z" id="34674544">@dadoonet Yes, we can close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>rename and document "index.mapping.date.parse_upper_inclusive" setting f...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3915</link><project id="" key="" /><description>...or date fields

The setting causes the upper bound for a range query/filter to be rounded up,
therefore the name `round_ceil` seems to make more sense.

Also this commit removes the redundant fourth parameter to DateMathParser.parse(..)
which was never used.
was:    parse(String text, long now, boolean roundUp, boolean upperInclusive)
is now: parse(String text, long now, boolean roundCeil)

closes #3914
</description><key id="21025686">3915</key><summary>rename and document "index.mapping.date.parse_upper_inclusive" setting f...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-10-15T15:57:03Z</created><updated>2014-07-16T21:52:01Z</updated><resolved>2013-10-28T15:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-10-28T14:02:01Z" id="27213561">@s1monw I updated according to your comments (which I cannot see anymore after force push, but they are still here: https://github.com/brwe/elasticsearch/commit/566b693fffee6e37994602691d6385381199c0b4#commitcomment-4441061)
</comment><comment author="brwe" created="2013-10-28T15:11:48Z" id="27219952">pushed to master and 0.90 (c9dab69/e061423)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>rename and document "index.mapping.date.parse_upper_inclusive" setting for date fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3914</link><project id="" key="" /><description>The "upper_inclusive" suffix is misleading. The setting really triggers rounding up of range boundaries for the range filter and query. Also, this setting is undocumented.
</description><key id="21023882">3914</key><summary>rename and document "index.mapping.date.parse_upper_inclusive" setting for date fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-15T15:30:19Z</created><updated>2013-10-29T07:33:27Z</updated><resolved>2013-10-28T14:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>src/test/java/org/elasticsearch/search/simple/SimpleSearchTests.java</file></files><comments><comment>rename and document "index.mapping.date.parse_upper_inclusive" setting for date fields</comment></comments></commit></commits></item><item><title>Add support for Lucene SuggestStopFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3913</link><project id="" key="" /><description>The suggest stop filter is an improved version of the stop filter, which
takes stopwords only into account if the last char of a query is a
whitespace. This allows you to keep stopwords, but to allow suggesting for
"a".

Example: Index document content "a word". You are now able to suggest for
"a" and get back results in the completion suggester, if the suggest stop
filter is used on the query side, but will not get back any results for
"a " as this is identified as a stopword.

The implementation allows to set the `remove_trailing` parameter for a
custom stop filter and thus use the suggest stop filter instead of the
standard stop filter.
</description><key id="21018885">3913</key><summary>Add support for Lucene SuggestStopFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-15T14:15:06Z</created><updated>2013-10-16T06:49:34Z</updated><resolved>2013-10-15T14:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-15T14:15:46Z" id="26337657">Closed by https://github.com/elasticsearch/elasticsearch/commit/4d19239ec4966c82b2cc9793902686f0cbab0fcf and  https://github.com/elasticsearch/elasticsearch/commit/723cbaf4918d4578af17f1e45bb1c86766055a8f
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove term_index_interval/divisor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3912</link><project id="" key="" /><description>The `term_index_interval` and `term_index_divisor` settings are no longer relevant and should be removed
</description><key id="21010225">3912</key><summary>Remove term_index_interval/divisor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-10-15T11:22:18Z</created><updated>2013-12-11T15:31:14Z</updated><resolved>2013-12-10T15:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-15T11:25:06Z" id="26326406">+1
</comment><comment author="s1monw" created="2013-12-10T14:18:58Z" id="30230231">@clintongormley should we warn on the docs regarding this?
</comment><comment author="clintongormley" created="2013-12-10T14:19:55Z" id="30230295">Hasn't worked for several versions - I think we can just delete
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file></files><comments><comment>Remove 'term_index_interval' and 'term_index_divisor'</comment></comments></commit></commits></item><item><title>YAML as first class citizen for more operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3911</link><project id="" key="" /><description>Currently YAML can be only used for index configuration. But wouldn't it be helpful to allow also to index YAML documents and to allow us to write our mappings and queries also in YAML?

JSON is great but if your mapping is longer than the usual examples you get lost in { and }.

So, why not making YAML a first class citzien of ElasticSearch? 
</description><key id="21005110">3911</key><summary>YAML as first class citizen for more operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">obfischer</reporter><labels><label>discuss</label></labels><created>2013-10-15T09:23:00Z</created><updated>2014-09-17T06:33:11Z</updated><resolved>2014-09-05T09:59:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-15T12:52:41Z" id="26331192">We already have YAML as a first class citizen, you can get any response in ES in YAML by specifying the content type as a header, or adding `format` parameter.

Regarding ingesting YAML documents, if they start with `----`, which is the formal header for YAML, we will parse it as such.
</comment><comment author="obfischer" created="2013-10-15T13:55:22Z" id="26335851">Great. Thank you for the quick response. Is this somethere to find in the documentation?
</comment><comment author="obfischer" created="2013-10-17T13:19:09Z" id="26503186">YAML seems not to be supported by the bulk API

Create an index 

```
curl -X PUT http://localhost:9200/demoindex
```

Create an YAML example document `doc.yaml`with two YAML documents separated by `---`

```
---
surname: Maria
lastname: Schweitzer
age: 22
address:
   town: Berlin
   street: Gartenstraße 34
   code: 10437
  country: DE
---
surname: Peter
lastname: Schweitzer
age: 25
address:
  town: Berlin
  street: Gartenstraße 34
  code: 10437
  country: DE
```

Send the document to the newly created index:

```
curl -X POST http://localhost:9200/indexdemo/ad/_bulk --data-binary @doc.yaml
---
error: "yaml does not support stream parsing..."
```

Wouldn't it be possible to split the incomming stream at `---` ?
</comment><comment author="clintongormley" created="2014-08-08T18:00:11Z" id="51636418">YAML is such an error prone beast...  I don't think we want to extend support for it any more than we already have.  Besides which, most requests will be made by clients, so the user doesn't actually care about what serialization is used.  I'm going to close this.
</comment><comment author="obfischer" created="2014-08-09T07:19:47Z" id="51679670">Hi Clinton, YAML is from an DevOps perspective easier to read and to maintain than a JSON.  So please reopen it. This isssue is not about client request, it is about devs and ops... 

So please, help to make my daily work life easier...
</comment><comment author="clintongormley" created="2014-08-09T09:18:39Z" id="51681834">OK - I'll reopen and mark this for issue discussion.
</comment><comment author="clintongormley" created="2014-08-09T09:30:46Z" id="51682077">The problem with using YAML in the bulk API is the `---` is not a reserved character sequence.  It can occur anywhere in YAML.  So in order to understand that `---` represents a new YAML stream, you have to actually parse all of the YAML.

With the JSON bulk API, the only newline characters that appear are between action and document lines.  This allows us to only parse the metadata on the coordinating node, and leave the document parsing to the destination node.

ie supporting YAML in the bulk API will create much more garbage that has to be collected, and have a performance impact
</comment><comment author="clintongormley" created="2014-09-05T09:59:00Z" id="54606105">After discussion with the team, we've decided that we're not going to support YAML in the bulk API for the reasons mentioned above. Closing
</comment><comment author="obfischer" created="2014-09-05T10:12:19Z" id="54607329">But with support for all operational operations as mappings and queries?
</comment><comment author="clintongormley" created="2014-09-06T17:49:00Z" id="54722371">@obfischer that's already supported, try:

```
curl -XPUT localhost:9200/foo -d '---
  mappings:
    bar:
        properties:
            foo:
                type: string
'
```

or for search:

```
curl -XGET localhost:9200/_search -d '---
  query:
    match_all: {}

'
```
</comment><comment author="obfischer" created="2014-09-17T06:33:11Z" id="55854308">@clintongormley I use YAML already for a lot of configuration task. My intention was/is only to ensure that YAML will be supported also in the future as well as JSON (expect for bulk operations....)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add generic count down mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3910</link><project id="" key="" /><description>In our code we often need to wait for multiple events to happen up to a timeout (mainly in the node acknowledgement logic), and we then notify some listener. We usually do it using an `AtomicInteger` to count down and an `AtomicBoolean` to check whether the notification has already been sent.

It would be nice to have a single way to achieve this, having a class (e.g. CountDown) that exposes this behaviour, which can be thoroughly tested and used whenever we need to.
</description><key id="21003712">3910</key><summary>Add generic count down mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-15T08:52:03Z</created><updated>2013-10-15T15:48:57Z</updated><resolved>2013-10-15T15:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/CountDown.java</file><file>src/test/java/org/elasticsearch/common/util/concurrent/CountDownTest.java</file><file>src/test/java/org/elasticsearch/search/scan/SearchScanScrollingTests.java</file></files><comments><comment>Added simple count down class that allows to be fast forwarded</comment></comments></commit></commits></item><item><title>Cannot index some geo_shape geometries (before and after Geo Refactoring changes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3909</link><project id="" key="" /><description>Probably FAO @chilling .

I have a mapping that includes some geo_shape fields. My test data contains GeoJSON fields that specify points, but do so in the form of polygons with the same lon/lat repeated 4 (or sometimes 3) times. This causes a validation exception to be thrown up from within Spatial4J when it generates the polygon.

I am not sure if this is technically invalid GeoJSON or not - however, this is a form that the Twitter API generates frequently (around 1 in 1500 tweets in my dataset), and other libraries I have used can parse it OK. It would be good if ES can attempt to do what it can with data like this, rather than failing (i.e. treat it as another appropriate type like a point, or relax the verification).

This is tested against the branch that includes the Geo-Refactoring improvements at chilling/elasticsearch@0369983afdcc7c6e8db7703b20889f69ac14221a (it does also happen on master, if you can get that far!).

The stack trace:

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [place]
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:232)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:515)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:457)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:515)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:457)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:515)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:457)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:508)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:452)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:341)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:533)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:418)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    ... 1 more
Caused by: com.spatial4j.core.exception.InvalidShapeException: Too few distinct points in geometry component at or near point (-81.872495, 36.163117, NaN)
    at com.spatial4j.core.shape.jts.JtsGeometry.(JtsGeometry.java:90)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:153)
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:219)
    ... 15 more
```

To reproduce:

```
curl -XPUT http://server:9200/dummyindex -d '
{
    "mappings": {
        "dummytype": {
            "properties": {
                "place": {
                    "type": "geo_shape",
                    "tree": "quadtree",
                    "precision": "10m"
                }
            }
        }
    }
}'

curl -XPOST http://server:9200/dummyindex/dummytype/1 -d '
{
    "place": {
        "coordinates": [[[-81.872495, 36.163117], [-81.872495, 36.163117], [-81.872495, 36.163117], [-81.872495, 36.163117]]],
        "type": "Polygon"
    }
}'
```

I wrote a nasty, hacky workaround to fix this particular case - see below. This is obviously not an acceptable general purpose solution, as it doesn't address the issue for any other shapes, or other cases like only having 2 or 3 distinct points. I am not really familiar enough with the libraries, but I guess a real fix might involve simplifying or normalizing each geometry somehow before it gets passed to Spatial4J?

Alternatively it could be deemed that this is not ES's job, and the GeoJSON needs to be more well-formed - but I think this is likely to be a common problem due to the source of this data, and because of the different expectations of different GeoJSON libraries.

ShapeBuilder.java:

```
-        protected static PolygonBuilder parsePolygon(CoordinateNode coordinates) {
+        protected static ShapeBuilder parsePolygon(CoordinateNode coordinates) {
             LineStringBuilder shell = parseLineString(coordinates.children.get(0));
+            
+            if (new HashSet&lt;Coordinate&gt;(shell.points).size() == 1)
+                return newPoint(shell.points.get(0));
+            
             PolygonBuilder polygon = new PolygonBuilder(shell.points);
             for (int i = 1; i &lt; coordinates.children.size(); i++) {
                 polygon.hole(parseLineString(coordinates.children.get(i)));

...

         protected static MultiPolygonBuilder parseMultiPolygon(CoordinateNode coordinates) {
             MultiPolygonBuilder polygons = newMultiPolygon();
             for (CoordinateNode node : coordinates.children) {
-                polygons.polygon(parsePolygon(node));
+              polygons.polygon((PolygonBuilder)parsePolygon(node));
             }
             return polygons;
         }
```
</description><key id="21000756">3909</key><summary>Cannot index some geo_shape geometries (before and after Geo Refactoring changes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">tommcintyre</reporter><labels><label>:Geo</label><label>high hanging fruit</label></labels><created>2013-10-15T07:36:05Z</created><updated>2015-05-14T03:28:40Z</updated><resolved>2015-05-14T03:28:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tommcintyre" created="2013-10-15T16:22:20Z" id="26349529">Some chat from IRC with opinions about whether this belongs in ES or not... :)

```
&lt;Tim&gt; GeoJSON spec is a little vague. A polygon must have a LinearRing, defined as:
&lt;Tim&gt; A LinearRing is closed LineString with 4 or more positions. The first and last positions are equivalent (they represent equivalent points). Though a LinearRing is not explicitly represented as a GeoJSON geometry type, it is referred to in the Polygon geometry type definition.
&lt;Tim&gt; not really clear that they must be distinct or non-self-intersecting
&lt;Tom&gt; yeah - i think it's just not well defined
&lt;Tom&gt; the new geo-refactored ES code does seem to attempt to make the best possible use of "borderline" valid geometries
&lt;Tom&gt; for example, twitter emits open linestrings quite often, as well these polygons with repeating points
&lt;Tom&gt; and the new geo-refactored ES code does now accept open linestrings
&lt;Tom&gt; (the old code didn't)
```
</comment><comment author="chilling" created="2013-10-16T16:04:51Z" id="26431832">Hi @mvjars,

I think the GeoJson is technically valid. The problem is forced by the underlying libraries that we use to handle geo data inside ES. These libraries make some assumptions to the data which are checked before the data gets indexed. In my opinion this assumptions must not necessarily be checked if the GeoJson is indexed. But currently we use this libraries to index geo data also. I'm working on separating the GeoJson format specification from the logic layer and move this kind of exceptions to the logic layer.
</comment><comment author="lababidi" created="2015-04-24T22:49:06Z" id="96088896">Is there any update on this issue?
</comment><comment author="clintongormley" created="2015-04-26T17:33:48Z" id="96411793">@nknize any ideas here?
</comment><comment author="nknize" created="2015-04-26T20:33:30Z" id="96431637">@lababidi Out of curiosity why make this a polygon type? Why not change it to a multipoint?
</comment><comment author="lababidi" created="2015-04-27T02:20:16Z" id="96470721">@nknize Thanks for checking in. I'm not sure I understand what your question is. Let me see if I can reframe the issue, succinctly.

Twitter provides the following object. It is rejected by elasticsearch for only having 4 elements and not 5. This strict requirement is not consistent with GeoJSON. Please allow elasticsearch to accept this Object, out of the box, because technically it is legal GeoJSON. Currently I must write a middle-man to add a 5th-element (cue Bruce Willis jokes here) to this array of coordinates. This is a bit kludgy in terms of workflow:

```
"bounding_box": {
        "type": "Polygon",
        "coordinates": [
        [
        [
        -9.0915413,
        38.6713816
        ],
        [
        -9.0915413,
        38.9313732
        ],
        [
        -8.8102479,
        38.9313732
        ],
        [
        -8.8102479,
        38.6713816
        ]
        ]
        ]
        },
```
</comment><comment author="nknize" created="2015-04-28T13:21:50Z" id="97062011">@lababidi Thanks for the update. I mistook the problem you're describing for the original issue (repeated points in a polygon).  I agree we should accept Polygons like this and close them for you.
</comment><comment author="lababidi" created="2015-04-28T18:40:04Z" id="97165861">This is excellent news. I'd be happy to contribute any code if your plate is full. Just point me to where the preferred location that this logic belongs in and I will pull request.
</comment><comment author="lababidi" created="2015-05-12T19:53:00Z" id="101400066">@nknize If you prefer to close this issue, you may. I realized my statement was a separate issue from this issue and create it at #11131
</comment><comment author="nknize" created="2015-05-14T03:28:38Z" id="101898129">@lababidi Excellent!  If you'd like to get involved we'd love to have you contribute. You'll need to sign the [CLA](https://www.elastic.co/contributor-agreement) if you haven't already.  I went ahead and opened a PR to address #11131, have a look and try it out.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>removed a duplicate paragraph in config docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3908</link><project id="" key="" /><description /><key id="20986214">3908</key><summary>removed a duplicate paragraph in config docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smayzak</reporter><labels /><created>2013-10-14T22:40:00Z</created><updated>2014-07-16T21:52:02Z</updated><resolved>2013-10-14T23:01:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-14T22:42:50Z" id="26295097">LGTM, +1.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes problems with whitespaces and other nasty chars in service.bat (Issue #3906)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3907</link><project id="" key="" /><description>Fix for https://github.com/elasticsearch/elasticsearch/issues/3906
Now you can install the service even into "C:\Program Files (x86)"
</description><key id="20963549">3907</key><summary>Fixes problems with whitespaces and other nasty chars in service.bat (Issue #3906)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">salyh</reporter><labels /><created>2013-10-14T15:31:47Z</created><updated>2014-07-09T10:50:26Z</updated><resolved>2013-10-14T15:53:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-14T15:53:27Z" id="26265742">Hi,

as mentioned in #3906, not sure whether you've checked master or 0.90 branch (upcoming 0.90.6) but this has been fixed some time ago through #3725. 
Can you please confirm if it works for you or not?
I have tried it with multiple nested folders, with spaces, both on the ES and JDK/JRE side and it worked fine for me.

Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>service.bat fails if ES_HOME contains whitespaces and parentheses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3906</link><project id="" key="" /><description>Executing service.bat on windows 7 x64 in a directory like 
C:\Program Files (x86) 
fails due to "syntaktisch an dieser Stelle nicht verarbeitbar" (in englisch somewhat like: bad syntax or syntactically not processible). Seems to be a problem with double quote handling/escaping.
</description><key id="20963430">3906</key><summary>service.bat fails if ES_HOME contains whitespaces and parentheses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">salyh</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-14T15:29:47Z</created><updated>2013-10-15T21:44:11Z</updated><resolved>2013-10-14T15:50:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-14T15:50:45Z" id="26265535">Hi,

Not sure whether you've checked master or 0.90 branch but this has been fixed some time ago through #3725. Can you please confirm?
</comment><comment author="salyh" created="2013-10-14T19:03:47Z" id="26280104">i have checked master (https://github.com/elasticsearch/elasticsearch/blob/master/bin/service.bat) and
sorry, but i cannot confirm, its not working for me :-(

i always get a error like:
C:\test fol.der (86)\another nasty na..me (55)\elasticsearch-master\elasticsearch-master\bin&gt;service.bat
\elasticsearch-master\elasticsearch-master\bin\elasticsearch-service-x86.exe was
 unexpected at this time.

during my tests i have found a few other issues, please see https://github.com/salyh/elasticsearch/commit/591b8df0cca1ced71c7210301b26157aad0f80b7

That is working for me.

My test setup is win7, with jdk7 64 bit and jre7 32 bit
JAVA_HOME is C:\Program Files (x86)\Java\jre7
ES_HOME is C:\test fol.der (86)\another nasty na..me (55)\elasticsearch-master\elasticsearch-master
</comment><comment author="costin" created="2013-10-14T21:14:10Z" id="26289380">Hi @salyh 
Can you please try out the latest master/0.90? I've been able to replicate and fix the problem you had which was caused by the ")" which tripped the internal IFs.
I've 'refactored' the batch file so the IFs arenot more lenient and the escaping of user vars is not needed any more. This results in consistent behaviour between implicit and defined values (as the user does not use `"`).
Long story short, it should work now with the nested path above - please let me know if that's not the case.

Thanks!
</comment><comment author="salyh" created="2013-10-15T21:44:11Z" id="26375125">i can confirm that master is now working ...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>improve escaping of user paths</comment></comments></commit></commits></item><item><title>Prohibit indexing a document with parent for a non child type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3905</link><project id="" key="" /><description>Prohibit indexing a document with parent for a type that doesn't have a `_parent` field configured.

Relates to #3848
</description><key id="20956639">3905</key><summary>Prohibit indexing a document with parent for a non child type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-10-14T13:33:40Z</created><updated>2015-05-18T23:33:52Z</updated><resolved>2013-10-15T16:36:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Segments API: Support merge id on segments (groups segments being merged)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3904</link><project id="" key="" /><description>Return a `merge_id` element in each segment of the segments API, allowing to group segments that are being merged as part of a single merge and indicate which ones are being merged now.
</description><key id="20945944">3904</key><summary>Segments API: Support merge id on segments (groups segments being merged)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-14T09:00:59Z</created><updated>2013-10-30T11:51:28Z</updated><resolved>2013-10-14T09:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/index/TrackingConcurrentMergeScheduler.java</file><file>src/main/java/org/apache/lucene/index/TrackingSerialMergeScheduler.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>src/main/java/org/elasticsearch/index/engine/Segment.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/merge/OnGoingMerge.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/ConcurrentMergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/MergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/SerialMergeSchedulerProvider.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineTests.java</file></files><comments><comment>Segments API: Support merge id on segments (groups segments being merged)</comment><comment>Return a merge_id element in each segment of the segments API, allowing to group segments that are being merged as part of a single merge and indicate which ones are being merged now.</comment><comment>closes #3904</comment></comments></commit></commits></item><item><title>search_analyzer does not seem to kick in as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3903</link><project id="" key="" /><description>I realized this issue while working on recreation script for #3881

It seems to me that the `search_analyzer` does not kick in in some cases, the following two recreation scripts are provided.

The [first scripts](https://gist.github.com/lukas-vlcek/6972058) shows a simple use case where the `search_analyzer` kicks in as expected. Everything looks ok to me here.

The [second script](https://gist.github.com/lukas-vlcek/6972064) (which is based on the original recreation scripts from #3881) shows that the `search_analyzer` does NOT kick in and yield correct results ONLY if stated explicitly. Not sure if I understand what it is fundamental difference between the use case in first script and use case in the second script. Both scripts have very similar structure and unless I made some typo there I do not understand why the search analyzer haven't been used automatically.

Both scripts were tested against the latest master:

```
Elasticsearch version and build:
{
  "ok" : true,
  "status" : 200,
  "name" : "Anelle",
  "version" : {
    "number" : "1.0.0.Beta1",
    "build_hash" : "9a062e465ce86aa9a72dd6dd5337be6447a83506",
    "build_timestamp" : "2013-10-13T15:08:48Z",
    "build_snapshot" : true,
    "lucene_version" : "4.5"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="20943758">3903</key><summary>search_analyzer does not seem to kick in as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>non-issue</label></labels><created>2013-10-14T07:58:48Z</created><updated>2013-10-14T19:47:40Z</updated><resolved>2013-10-14T15:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-14T14:26:54Z" id="26258563">I checked your second script and found an error in the index creation. In fact if you try and get the mapping for your type you'll see it's empty. The `mappings` object in your script is within the `settings` one, while it should be at its same level.

Once you fixed that everything should work as expected, the search analyzer is used and so on. Can you confirm?
</comment><comment author="javanna" created="2013-10-14T14:27:22Z" id="26258598">Sorry, didn't mean to close before your reply, reopening...
</comment><comment author="lukas-vlcek" created="2013-10-14T15:27:41Z" id="26263531">You are correct, the nesting level of `mappings` section was wrong.
Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added missing builder.endObject calls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3902</link><project id="" key="" /><description>After closing #3878 it came to me, that we might have other code snippets of missing or too much `builder.endObject()` / `builder.startObject()` methods (same for arrays), so I wrote this kind of hacky shell script

``` sh
#/bin/bash
for x in Object Array ; do

    start="\.start$x"
    end="\.end$x"

    for i in $(ag -l "$start") ; do
        startingCount=$(grep -c "$start" $i)
        endingCount=$(grep -c "$end" $i)

        if [ $startingCount -ne $endingCount ] ; then
            echo "$i $x: start $startingCount end $endingCount"
        fi
    done

done
```

only found two other pieces of code and a couple of false positives.

I am thinking if we could automate this by doing something similar as the `forbidden-api` maven module does and parse the source itself (to prevent false positives).

Anyone having experience with that?
</description><key id="20925662">3902</key><summary>Added missing builder.endObject calls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-13T12:07:12Z</created><updated>2014-07-04T08:42:32Z</updated><resolved>2013-10-14T08:27:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-13T12:49:34Z" id="26217216">Nice script! I like it and the forbidden-api like thing.
</comment><comment author="s1monw" created="2013-10-13T15:57:10Z" id="26220512">+1 to commit. Yet, I opened #3889 to move all this serialisation to the corresponding classes - once we are on that we should also add tests for this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query String: Support multiple fields regexp queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3901</link><project id="" key="" /><description>The query string query should apply regexp queries to execute on multiple fields as all the other queries.
</description><key id="20919085">3901</key><summary>Query String: Support multiple fields regexp queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-12T23:27:15Z</created><updated>2013-10-12T23:27:45Z</updated><resolved>2013-10-12T23:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file></files><comments><comment>Query String: Support multiple fields regexp queries</comment><comment>The query string query should apply regexp queries to execute on multiple fields as all the other queries.</comment><comment>closes #3901</comment></comments></commit></commits></item><item><title>Join option on nested facet filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3900</link><project id="" key="" /><description>According to the documentation for the nested filter join option, it is enabled by default and would execute a block join between the parent and nested documents. When disabled, it returns the nested documents as stated. It is the default enabled state that I find confusing. I cannot come up with a test case where the block join would return any documents. The integration tests in SimpleNestedTests disable the join option.

I have not yet profiled the code, but I suspect that the code in NestedQueryParser is constructing a ToParentBlockJoinQuery with an incorrect parentFilter, probably the default NonNestedDocsFilter.INSTANCE.

If this is not a bug, can the tests be updated to include an example where the default join is enabled and returns documents? There are a few assertions that are commented out in SimpleNestedTests related to join queries.
</description><key id="20906739">3900</key><summary>Join option on nested facet filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">brusic</reporter><labels /><created>2013-10-12T06:53:29Z</created><updated>2013-10-13T18:35:59Z</updated><resolved>2013-10-13T18:35:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-12T15:41:25Z" id="26199727">The `join` option on the nested filter allows the block join to be enabled or disabled. If disabled just the inner query or filter is executed and the hits are never linked back to the parent/root document. By default the nested filter performs the join and as far as I know it should only be set to false when a nested filter is used as a facet filter in a facet with nested enabled. The non join mode isn't heavily tested, but it used in SimpleNestedTests line 459.  

What do you think is the bug?
</comment><comment author="brusic" created="2013-10-13T18:35:11Z" id="26223718">Thanks Martijn. Your response helped explain my confusion. I have only used the nested filter as a facet filter, so the join option was both unnecessary and verbose (compared to the more concise direct filter). The default (enabled) join option only works when the filter is used as a query filter, not a facet filter. In fact, when used as a facet filter, the default option returns no values, which was my issue.

Since my issue is technically is not a bug, I will go ahead and close the issue. However, perhaps some guidance would be appreciated.  What am trying to achieve is a distinct count of parents whose nested documents pass some filter. My hope was that returning just the parent documents with the join enabled, would lead to correct counts.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replaced hardcoded boolean value with correct parameter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3899</link><project id="" key="" /><description>Closes #3898

I've tracked the problem down to the MapperQueryParser class. As you can see from the commit the "true" value was hard coded for one of the code branches instead of using the 'quoted' parameter sent to the method.
</description><key id="20870297">3899</key><summary>Replaced hardcoded boolean value with correct parameter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bogdan-dumitrescu</reporter><labels /><created>2013-10-11T14:02:31Z</created><updated>2014-06-30T13:27:35Z</updated><resolved>2013-10-14T12:55:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-14T12:55:03Z" id="26253163">Merged 899189694f32edc808fd0e0ce3491587e9ae479e and backported to 0.90.

Thanks @bogdan-dumitrescu !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>No results are found with specific use case when using a custom word_delimiter filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3898</link><project id="" key="" /><description>The problem only manifests when all of the following conditions are met:
- a custom word delimiter is used when analyzing a string field
- the query string query is used
- use_dis_max parameter is set to false
- more than one field is used for the query
- query string contains a special character (like "." or "/")

This is how to reproduce the issue:

``` sh
# create the index
curl -XPOST http://localhost:9200/test -d '{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    },
    "analysis": {
      "analyzer": {
        "text_ascii": {
          "type": "custom",
          "tokenizer": "whitespace",
          "filter": ["asciifolding",
          "lowercase",
          "custom_word_delimiter"]
        }
      },
      "filter": {
        "custom_word_delimiter": {
          "type": "word_delimiter",
          "generate_word_parts": true,
          "generate_number_parts": false,
          "catenate_numbers": true,
          "catenate_words": false,
          "split_on_case_change": false,
          "split_on_numerics": false,
          "stem_english_possessive": false
        }
      }
    }
  },
  "mappings": {
    "person": {
      "properties": {
        "name": {
          "type": "string",
          "analyzer": "text_ascii"
        },
        "address": {
          "type": "string",
          "analyzer": "text_ascii"
        }
      }
    }
  }
}
'

# index the document
curl -XPUT 'http://localhost:9200/test/person/1' -d '{
    "name" : "bogdan mihai dumitrescu",
    "address" : "amsterdam"
}'

# search for the document with a dot in the query 
# expect one result to be returned, but this returns 0 hits
curl -XPOST 'http://localhost:9200/test/_search' -d '{
  "query": {
    "query_string": {
      "query": "bogdan.dumitrescu",
      "fields": ["name", "address"],
      "use_dis_max": false,
      "default_operator": "and",
      "analyzer": "text_ascii"
    }
  }
}'
```
</description><key id="20870158">3898</key><summary>No results are found with specific use case when using a custom word_delimiter filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bogdan-dumitrescu</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T14:00:02Z</created><updated>2013-10-14T12:53:34Z</updated><resolved>2013-10-14T12:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Replaced hardcoded boolean value with correct parameter</comment></comments></commit></commits></item><item><title>Add match query support for stacked tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3897</link><project id="" key="" /><description>SynonymFilters produces token streams with stacked tokens such that
conjunction queries need to be parsed in a special way such that the
stacked tokens are added as an innner disjuncition.

Closes #3881
</description><key id="20869565">3897</key><summary>Add match query support for stacked tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-11T13:49:54Z</created><updated>2014-08-15T01:34:40Z</updated><resolved>2013-10-14T12:10:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-10-11T18:27:58Z" id="26159924">Hi Simon,

Will it also work for stemming when both stemmed and original words are emitted? 

If so it would work great as it will make it possible to use AND logic in such cases knowing that original and stemmed tokens will be combined with OR inside of AND-ed statement

Thank you
Alex
</comment><comment author="s1monw" created="2013-10-11T21:04:10Z" id="26173894">@roytmana it should work as you describe - I will try to write a testcase for this soon
</comment><comment author="roytmana" created="2013-10-11T22:40:52Z" id="26180087">great thanks, will be happy to test it 
</comment><comment author="s1monw" created="2013-10-13T20:29:32Z" id="26226276">@roytmana I updated the commit with a test using stacked stems, works like expected.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added facet support to the percolate api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3896</link><project id="" key="" /><description>Relates #3851
</description><key id="20867112">3896</key><summary>Added facet support to the percolate api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-10-11T13:00:27Z</created><updated>2015-05-18T23:33:45Z</updated><resolved>2014-01-08T08:16:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-14T08:16:00Z" id="26241297">Added `StreamOutput.writeOptionalStreamable()` and used it for serializing facets.
</comment><comment author="kimchy" created="2013-10-15T21:58:12Z" id="26376097">I like!, LGTM.
</comment><comment author="martijnvg" created="2013-10-16T15:11:12Z" id="26426733">@s1monw I added the readOptionalStreamable method via the last commit.
</comment><comment author="s1monw" created="2013-10-17T17:47:29Z" id="26531264">I left some comments which are all minor. Lets iterate one more time and then push it ;)
</comment><comment author="martijnvg" created="2013-10-20T16:56:37Z" id="26677018">@s1monw I fixed the review feedback points.
</comment><comment author="s1monw" created="2013-10-20T17:32:37Z" id="26677827">LGTM +1 to push
</comment><comment author="benneic" created="2014-01-08T04:06:19Z" id="31804346">Any reason this hasn't been merged? Looking to start using facets in the percolator...
</comment><comment author="martijnvg" created="2014-01-08T08:16:30Z" id="31812601">@beichhor This has been merged in, the PR just hasn't been closed. Btw you can also use `aggregations` in the percolate api now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support date math for `origin` decay function parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3895</link><project id="" key="" /><description>The parser used the method that was supposed to be used for parsing on
the indexing side that never supported date math.

Closes #3892
</description><key id="20866307">3895</key><summary>Support date math for `origin` decay function parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-11T12:43:48Z</created><updated>2014-07-16T21:52:04Z</updated><resolved>2013-10-11T15:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add integration test for PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3894</link><project id="" key="" /><description>We want to make sure that Plugin Manager still downloading plugins from internet.
New tests requires internet access (`@Network` annotation has been added).

By default, tests annotated with `@Network` are not launched.

If you need to run these tests, use `-Dtests.network=true` option.
</description><key id="20865940">3894</key><summary>Add integration test for PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T12:35:59Z</created><updated>2013-10-11T13:30:28Z</updated><resolved>2013-10-11T13:17:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-11T12:59:54Z" id="26134918">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/junit/annotations/Network.java</file><file>src/test/java/org/elasticsearch/plugin/PluginManagerTests.java</file></files><comments><comment>Add integration test for PluginManager</comment></comments></commit></commits></item><item><title>Fix payloads tv</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3893</link><project id="" key="" /><description /><key id="20864611">3893</key><summary>Fix payloads tv</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-10-11T12:00:17Z</created><updated>2014-07-04T09:51:37Z</updated><resolved>2013-10-14T13:25:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-11T15:00:47Z" id="26144003">I am not sure what this PR fixes, its mixed with a new added token filter, maybe it makes sense to split it?
</comment><comment author="brwe" created="2013-10-13T14:44:34Z" id="26219199">Well, there was no way to see that, since I forgot to add the issue ids to the commit messages...

The payload delimiter filter (#3859) was added for testing things related to payloads. While testing using the term vector api I discovered the bug described in issue #3873. Because these two somehow depend on each other I made one pull request for both.

Let me know if you still think they should be split. 
</comment><comment author="brwe" created="2013-10-14T08:30:39Z" id="26241937">I updated the pull request. It now includes documentation for the delimited payload token filter and a more descriptive commit message for the fix in the term vector api. I changed this fix also: instead of setting payloads to `null` it just sets the length of the payload in the `TermVectorFields.payloads` array to 0 if no payload exists for a token at the position.

If you still think the pull requests should be split, let me know.
</comment><comment author="s1monw" created="2013-10-14T09:13:15Z" id="26243990">LGTM thanks for clarifying and adding the docs
</comment><comment author="brwe" created="2013-10-14T13:25:55Z" id="26254704">see ce0ab79 to c3ab79a
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>function_score (FunctionScoreQuery) decay functions do not allow date math</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3892</link><project id="" key="" /><description>For "linear", "gauss", "exp" ... functions, the "origin" function should allow parameters and date math.  "now" as origin fails to parse.  Just as "now-7d" would as well.  Obviously offset can replace part of the date math, the origin still would be useful to be set in that way.
</description><key id="20864574">3892</key><summary>function_score (FunctionScoreQuery) decay functions do not allow date math</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">apatrida</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T11:59:21Z</created><updated>2013-10-11T15:28:32Z</updated><resolved>2013-10-11T15:27:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-11T12:43:18Z" id="26134042">good point, thanks for opening this!
</comment><comment author="s1monw" created="2013-10-11T15:28:20Z" id="26146281">thanks @jaysonminard for bringing this up!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreTests.java</file></files><comments><comment>Support date math for `origin` decay function parsing</comment></comments></commit></commits></item><item><title>fix naming in function_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3891</link><project id="" key="" /><description>```
- "boost" should be "boost_factor"
- "mult" should be "multiply"
```

closes #3872 for master
</description><key id="20864437">3891</key><summary>fix naming in function_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-10-11T11:55:28Z</created><updated>2014-07-06T02:59:28Z</updated><resolved>2013-10-14T13:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-11T11:56:37Z" id="26131680">+1 LGTM
</comment><comment author="brwe" created="2013-10-14T13:23:57Z" id="26254610">pushed 34441f3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Explore Littles Law for automatic / dynamic queue sizes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3890</link><project id="" key="" /><description>we currently use static defaults for queues on threadpools. It might be worth looking into things like http://en.wikipedia.org/wiki/Little's_law for the queue sizes...
</description><key id="20864409">3890</key><summary>Explore Littles Law for automatic / dynamic queue sizes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>discuss</label><label>enhancement</label></labels><created>2013-10-11T11:54:32Z</created><updated>2017-05-16T17:16:51Z</updated><resolved>2017-05-16T17:16:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-11T11:55:15Z" id="26131605">this relates to #3888 
</comment><comment author="clintongormley" created="2015-09-21T18:57:09Z" id="142076392">@s1monw is this still of interest to you?
</comment><comment author="s1monw" created="2015-09-21T19:00:42Z" id="142077547">we can close and reopen if we pick it up
</comment><comment author="dakrone" created="2015-09-21T20:00:44Z" id="142093284">@clintongormley I actually had a feature branch for this from last year, though it's likely succumbed to bitrot by now.
</comment><comment author="clintongormley" created="2017-03-31T09:27:37Z" id="290664674">We're reviving this effort</comment><comment author="aanchal9" created="2017-04-19T12:58:00Z" id="295260529">Is it still an issue?</comment><comment author="javanna" created="2017-04-19T13:02:01Z" id="295261855">@aanchal9 yes it is, there is a PR open that will close it once merged (#23884). Why are you asking?</comment><comment author="aanchal9" created="2017-04-19T13:20:28Z" id="295267552">It was assigned to me on CodeTriage so I asked.</comment><comment author="jasontedor" created="2017-04-19T13:39:18Z" id="295273594">If you're looking for an issue, instead of using CodeTriage, I would recommend looking at our [contributing guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md), and finding issues that have at least the [adoptme](https://github.com/elastic/elasticsearch/labels/adoptme) label, and the [low hanging fruit](https://github.com/elastic/elasticsearch/labels/low%20hanging%20fruit) label if you're just getting started.</comment><comment author="aanchal9" created="2017-04-19T14:58:44Z" id="295299825">Ok thanks. That is of great help. Will look into contributing guidelines.

On 19 Apr 2017 7:09 p.m., "Jason Tedor" &lt;notifications@github.com&gt; wrote:

&gt; If you're looking for an issue, instead of using CodeTriage, I would
&gt; recommend looking at our contributing guidelines, and finding issues that
&gt; have at least the adoptme label, and the low-hanging-fruit label if you're
&gt; just getting started.
&gt;
&gt; —
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/3890#issuecomment-295273594&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AMn5nRP3uFAHLTIJ0vAfszi8rWrRapZrks5rxg6ogaJpZM4BFpBs&gt;
&gt; .
&gt;
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/QueueResizingEsThreadPoolExecutor.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/ResizableBlockingQueue.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/SizeBlockingQueue.java</file><file>core/src/main/java/org/elasticsearch/common/util/concurrent/TimedRunnable.java</file><file>core/src/main/java/org/elasticsearch/threadpool/AutoQueueAdjustingExecutorBuilder.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ExecutorBuilder.java</file><file>core/src/main/java/org/elasticsearch/threadpool/FixedExecutorBuilder.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/QueueResizingEsThreadPoolExecutorTests.java</file><file>core/src/test/java/org/elasticsearch/common/util/concurrent/ResizableBlockingQueueTests.java</file><file>core/src/test/java/org/elasticsearch/threadpool/AutoQueueAdjustingExecutorBuilderTests.java</file></files><comments><comment>Add ability to automatically adjust search threadpool queue_size</comment></comments></commit></commits></item><item><title>ActionResponse should implement ToXContentObject</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3889</link><project id="" key="" /><description>He should across the board implement `ToXContent` and all responses should implement is without surrounding `start/endObject`. I started doing one of them in this PR: https://github.com/elasticsearch/elasticsearch/pull/3871 but as @martijnvg pointed out we should have all of them being consistent about it. 
</description><key id="20864348">3889</key><summary>ActionResponse should implement ToXContentObject</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>enhancement</label></labels><created>2013-10-11T11:52:52Z</created><updated>2017-01-07T11:27:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-06T10:22:56Z" id="258671954">@s1monw @javanna @cbuescher Have we made progress on this? Is there still more work to be done?
</comment><comment author="javanna" created="2016-11-07T09:10:25Z" id="258782172">We didn't make any progress, I think it's still a nice to have, not super important though. In theory it should be about moving `ToXContent` response creation code from `RestAction`s to `ActionResponse`s.
</comment><comment author="javanna" created="2016-11-07T09:11:41Z" id="258782418">Also some related bits [here](https://github.com/elastic/elasticsearch/issues/16347#issuecomment-183332285) 
</comment><comment author="javanna" created="2016-12-16T10:14:19Z" id="267561256">This is related to the parsing effort we have been making for the High Level REST Client. We can definitely fix it as we go. @tlrx what do you think?</comment><comment author="tlrx" created="2016-12-16T10:25:05Z" id="267563328">I agree, would be nice to fix that.</comment><comment author="javanna" created="2017-01-07T11:27:49Z" id="271078023">I updated the title of this issue as at this point ActionResponse should implement `ToXCOntentObject` and not `ToXContent`</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set queue sizes by default on bulk/index thread pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3888</link><project id="" key="" /><description>Now that we properly fixed the ability to set the queue size on the index / bulk thread pool, we should actually set them to a somehow reasonable value to protect from users potentially overflowing our system.

I suggest defaults to be 50 for bulk, and 200 for indexing.

Also, set the thread pool for get, which we should set (in a similar value to a "read" queue size we have today).
</description><key id="20863125">3888</key><summary>Set queue sizes by default on bulk/index thread pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T11:19:40Z</created><updated>2013-10-12T19:51:48Z</updated><resolved>2013-10-12T19:51:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>Set queue sizes by default on bulk/index thread pools</comment><comment>Now that we properly fixed the ability to set the queue size on the index / bulk thread pool, we should actually set them to a somehow reasonable value to protect from users potentially overflowing our system.</comment></comments></commit></commits></item><item><title>Fix toXContent of GeoShapeQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3887</link><project id="" key="" /><description>A missing endObject() resulted in serialization errors.

Closes #3878
</description><key id="20861747">3887</key><summary>Fix toXContent of GeoShapeQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-11T10:45:17Z</created><updated>2014-06-25T16:31:40Z</updated><resolved>2013-10-11T15:53:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-11T11:17:34Z" id="26129903">+1
</comment><comment author="s1monw" created="2013-10-11T11:56:11Z" id="26131651">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make source filtering parameters of the Get Source API consistent with the rest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3886</link><project id="" key="" /><description>With #3301, we added _source_\* query parameters to many APIs allowing to filter the `_source` those APIs return. The get source api currently uses other naming for those parameters, not using the _source prefix. We should make it consistent with the ones, like this:

``````
curl -XGET "http://localhost:9200/index/type/1/_source/?_source_include=field1,field2" ```
``````
</description><key id="20860600">3886</key><summary>Make source filtering parameters of the Get Source API consistent with the rest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T10:14:59Z</created><updated>2013-10-11T10:27:09Z</updated><resolved>2013-10-11T10:27:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java</file></files><comments><comment>Make the get source REST API source fetching query parameters consistent with other API</comment></comments></commit></commits></item><item><title>Add `num_queries` and `memory_size` stats to percolate stats.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3885</link><project id="" key="" /><description>Related to #3883
</description><key id="20859176">3885</key><summary>Add `num_queries` and `memory_size` stats to percolate stats.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-10-11T09:42:56Z</created><updated>2015-05-18T23:33:55Z</updated><resolved>2013-10-15T08:31:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-10-12T23:24:00Z" id="26208170">Do we end up computing the size on each call to stats? That can be very expensive? Maybe we should explore computing the memory size on changes to the registered queries in men?
</comment><comment author="martijnvg" created="2013-10-14T07:57:36Z" id="26240569">Makes sense, I will change that.
</comment><comment author="martijnvg" created="2013-10-14T08:38:13Z" id="26242319">The size in memory of the queries is now computed when queries are added or removed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elasticsearch-service-x64 unable to reflect ES_HEAP_SIZE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3884</link><project id="" key="" /><description>Hi, 
I am running elasticsearch  0.90.5 on windows 2008 R2. In service.bat I updated ES_HEAP_SIZE as 2g I checked script with Echo ON and it showed my 2g .
However when I run the service and check heap committed it comes as 111 mb . 

I check running the es with elasticsearch.bat with ES_HEAP_SIZE=2g and it shows heap committed as 1.9 g. 
By the way I am using server jre 1.7 u40 . 
However I earlier tried client jre 1.7 u 25 but the result is same . 

Thanks
</description><key id="20858935">3884</key><summary>elasticsearch-service-x64 unable to reflect ES_HEAP_SIZE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">aamirmajeedkhan</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T09:38:22Z</created><updated>2013-11-07T06:33:10Z</updated><resolved>2013-10-13T09:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-13T09:55:46Z" id="26214680">Hi,

This is a duplicate of #3785 which has been fixed some time ago. Can you please try out the master/0.90 branch and let us know whether it works for you or not?

Thanks.
</comment><comment author="aamirmajeedkhan" created="2013-10-13T10:59:58Z" id="26215617">Hi,
It doesn't work as this time I recieved following message 

call:convertxm 2g JVM_XMX
The system cannot find the batch label specified - convertxm

Thanks
</comment><comment author="costin" created="2013-10-13T21:08:10Z" id="26227169">Unfortunately I am unable to reproduce the problem. I am testing the latest 0.90 branch.
Here are the commands that I am running from the console from a Windows 7 x64 SP1:

```
&gt; set ES_HEAP_SIZE=2g
&gt; service install

Installing service      :  "elasticsearch-service-x64"
Using JAVA_HOME (64-bit):  "c:\jvm\jdk1.7"
The service 'elasticsearch-service-x64' has been installed.

&gt; service manager
```

which displays the manager windows:
![service-manager](https://f.cloud.github.com/assets/76245/1322801/51839a86-344b-11e3-8ef2-c040d392d285.png)

Notice the Java tab which indicates the initial pool has been set to 2gb. I'm not sure what version you are using since by default, 1gb is used (which again shows up in the Java tab).
</comment><comment author="aamirmajeedkhan" created="2013-10-13T21:32:00Z" id="26227652">Hi,
Thanks for give it a try I am using Windows 2008 R2 . In my case it's not even writing anything in memory pool please have a look to attached snapshot 
![image](https://f.cloud.github.com/assets/3833241/1322820/166ad98e-344e-11e3-94e0-ad2256109fb3.png)

Interestingly service installed but because of missing memory value not able to start.
I believe that's happened because of failure of following statement in batch file 
call convertxm %ES_MIN_MEM% JVM_XMS  

I earlier tried 0.90.5 with NSSM - the Non-Sucking Service Manager and that seems to be working fine .
Since this is for live environment I am more inclined towards using 0.90.5 rather than latest build from github. What's your take on it .

Thanks
</comment><comment author="costin" created="2013-10-14T07:05:49Z" id="26238723">It looks like you are using the raw `service.bat` file from the trunk without having it build the project.
Can you please first build elasticsearch and then use the resulting file (from `target/releases` or `target/bin`)? Since otherwise the file is incomplete and results in an invalid build.

Try running `mvn clean package -DskipTests` and then looking at the relevant `service.bat` mentioned in the locations above. You can double check this by making sure no placeholders (${}) are used in your bat / settings.
</comment><comment author="costin" created="2013-10-14T07:06:30Z" id="26238744">One more thing - after updating the file, if it fails, can you run the bat as an administrator? Thanks.
</comment><comment author="costin" created="2013-10-14T15:46:52Z" id="26265222">Just tried the same ES configuration above (namely the master and trunk) on a fresh new install of Windows Server 2008 R2 x64 bits.
The `service.bat` installation works as expected - I've specified multiple combinations such as no environment variables, `ES_MAX_SIZE` and `ES_HEAP_SIZE`. For each I have changed the param, installed the service, removed the service.
P.S. I was running this from an administrator account (the default one)
</comment><comment author="aamirmajeedkhan" created="2013-11-07T06:33:10Z" id="27940751"> I can confirm it fixed .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add more percolate statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3883</link><project id="" key="" /><description>Add the following statistics to indices and node stats apis:
- `num_queries` - The number of loaded percolator queries.
- `memory_size` - The amount of memory the percolator queries take in memory.
</description><key id="20858296">3883</key><summary>Add more percolate statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T09:23:01Z</created><updated>2013-10-15T08:31:20Z</updated><resolved>2013-10-15T08:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java</file><file>src/main/java/org/elasticsearch/index/percolator/stats/ShardPercolateService.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Added `num_queries` and `memory_size` stats to percolate stats.</comment></comments></commit></commits></item><item><title>"bin/plugin --install" reports success for aborted installations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3882</link><project id="" key="" /><description>It looks like it happened successfully, but it actually aborted:

```
$ sudo /usr/share/elasticsearch/bin/plugin --install richardwilly98/elasticsearch-river-mongodb/1.7.1
-&gt; Installing richardwilly98/elasticsearch-river-mongodb/1.7.1...
Trying http://download.elasticsearch.org/richardwilly98/elasticsearch-river-mongodb/elasticsearch-river-mongodb-1.7.1.zip...
Trying http://search.maven.org/remotecontent?filepath=richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip...
Trying https://github.com/richardwilly98/elasticsearch-river-mongodb/archive/v1.7.1.zip...
Trying https://github.com/richardwilly98/elasticsearch-river-mongodb/archive/master.zip...
Downloading .................DONE
Installed richardwilly98/elasticsearch-river-mongodb/1.7.1 into /usr/share/elasticsearch/plugins/river-mongodb

$ sudo ls /usr/share/elasticsearch/plugins/river-mongodb
ls: cannot access /usr/share/elasticsearch/plugins/river-mongodb: No such file or directory
```
</description><key id="20853520">3882</key><summary>"bin/plugin --install" reports success for aborted installations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-11T07:14:50Z</created><updated>2013-10-14T14:00:51Z</updated><resolved>2013-10-14T14:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-11T13:29:32Z" id="26136779">Are you using an old version of elasticsearch?

When I test it on 0.90.6-SNAPSHOT, I get:

```
-&gt; Installing richardwilly98/elasticsearch-river-mongodb/1.7.1...
Trying http://download.elasticsearch.org/richardwilly98/elasticsearch-river-mongodb/elasticsearch-river-mongodb-1.7.1.zip...
Failed: IOException[Can't get http://download.elasticsearch.org/richardwilly98/elasticsearch-river-mongodb/elasticsearch-river-mongodb-1.7.1.zip to /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/river-mongodb.zip]; nested: FileNotFoundException[http://download.elasticsearch.org/richardwilly98/elasticsearch-river-mongodb/elasticsearch-river-mongodb-1.7.1.zip]; nested: FileNotFoundException[http://download.elasticsearch.org/richardwilly98/elasticsearch-river-mongodb/elasticsearch-river-mongodb-1.7.1.zip]; 
Trying http://search.maven.org/remotecontent?filepath=richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip...
Failed: IOException[Can't get http://search.maven.org/remotecontent?filepath=richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip to /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/river-mongodb.zip]; nested: FileNotFoundException[http://search.maven.org/remotecontent?filepath=richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip]; nested: FileNotFoundException[http://search.maven.org/remotecontent?filepath=richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip]; 
Trying https://oss.sonatype.org/service/local/repositories/releases/content/richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip...
Failed: IOException[Can't get https://oss.sonatype.org/service/local/repositories/releases/content/richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip to /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/river-mongodb.zip]; nested: FileNotFoundException[https://oss.sonatype.org/service/local/repositories/releases/content/richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip]; nested: FileNotFoundException[https://oss.sonatype.org/service/local/repositories/releases/content/richardwilly98/elasticsearch-river-mongodb/1.7.1/elasticsearch-river-mongodb-1.7.1.zip]; 
Trying https://github.com/richardwilly98/elasticsearch-river-mongodb/archive/v1.7.1.zip...
Failed: IOException[Can't get https://github.com/richardwilly98/elasticsearch-river-mongodb/archive/v1.7.1.zip to /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/river-mongodb.zip]; nested: FileNotFoundException[https://codeload.github.com/richardwilly98/elasticsearch-river-mongodb/zip/v1.7.1]; nested: FileNotFoundException[https://codeload.github.com/richardwilly98/elasticsearch-river-mongodb/zip/v1.7.1]; 
Trying https://github.com/richardwilly98/elasticsearch-river-mongodb/archive/master.zip...
Downloading ..................................................DONE
Installed richardwilly98/elasticsearch-river-mongodb/1.7.1 into /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/river-mongodb
Plugin installation assumed to be site plugin, but contains source code, aborting installation...
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
    -i, --install [plugin name]       : Downloads and installs listed plugins [*]
    -r, --remove  [plugin name]       : Removes listed plugins
    -l, --list                        : List installed plugins
    -v, --verbose                     : Prints verbose messages
    -s, --silent                      : Run in silent mode
    -h, --help                        : Prints this help message

 [*] Plugin name could be:
     elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)
     groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
     username/repository          for site plugins (download from github master)

Message:
   Error while installing plugin, reason: IllegalArgumentException: Plugin installation assumed to be site plugin, but contains source code, aborting installation.

Process finished with exit code 70
```

Which is good to me. I mean it does not fail silently.
</comment><comment author="benmccann" created="2013-10-11T14:44:03Z" id="26142544">I'm using 0.90.5. Maybe there's been some change that will fix it for the next release?
</comment><comment author="javanna" created="2013-10-14T14:00:32Z" id="26256769">Indeed, this is the pull request https://github.com/elasticsearch/elasticsearch/pull/3774 but there is no related issue, which makes it harder to trace. Going to be fixed with 0.90.6.

Thanks for opening this though!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multiple tokens at the same position not working correctly with match query if AND operator is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3881</link><project id="" key="" /><description>If multiple tokens are output at the same position then `match` queries are not working correctly if `AND` operator is used.

First I noticed this issue when using Hunspell token filter (something similar has been reported in [LUCENE-5057](https://issues.apache.org/jira/browse/LUCENE-5057) but it is not really a Lucene issue). With Hunspell it is possible to get multiple output tokens from a single input token, all at the same position. However, client query usually contains only one of those tokens or token that can output different set of tokens. When using `match` query and `AND` operator the document is not matching (while it should be).

_I also think that this can impact other linguistics packages (like Basis`s RBL?)_

Similar situation can be simulated using synonym filter. Imagine that we are using query time synonyms.

Let's say we index simple document:

```
{ text : "Quick brown fox" }
```

and we define query time synonym "quick, fast". Now let's see what we can do with this in the following [recreation script](https://gist.github.com/lukas-vlcek/6923179) (using ES 0.90.5), output commented below:

```
#!/bin/sh

echo "Elasticsearch version"
curl localhost:9200; echo; echo;

echo "Delete index"; curl -X DELETE 'localhost:9200/i'; echo; echo;

echo "Create index with analysis and mappings"; curl -X PUT 'localhost:9200/i' -d '{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "index" : {
          "type" : "custom",
          "tokenizer" : "standard",
          "filter" : ["lowercase"]
        },
        "search" : {
          "type" : "custom",
          "tokenizer" : "standard",
          "filter" : ["lowercase","synonym"]
        }
      },
      "filter" : {
        "synonym" : {
          "type" : "synonym",
          "synonyms" : [
            "fast, quick"
          ]
  }}},
  "mappings" : {
    "t" : {
      "properties" : {
        "text" : {
          "type" : "string",
          "index_analyzer" : "index",
          "search_analyzer" : "search"
}}}}}}'; echo; echo;

# Wait for all the index shards to be allocated
curl -s -X GET 'http://localhost:9200/_cluster/health?wait_for_status=yellow&amp;timeout=5s' &gt; /dev/null

echo "Test synonyms for 'fast': should output two tokens"; curl -X POST 'localhost:9200/i/_analyze?analyzer=search&amp;format=text&amp;text=fast'; echo; echo;

echo "Index data: 'Quick brown fox'"; curl -X POST 'localhost:9200/i/t' -d '{
  "text" : "Quick brown fox"
}'; echo; echo;

echo "Refresh Lucene reader"; curl -X POST 'localhost:9200/i/_refresh'; echo; echo;

echo "Testing search";
echo ===========================
echo "1) query_string: quick";
curl -X GET 'localhost:9200/_search' -d '{"query":{"query_string":{"query":"quick","default_field":"text"}}}'; echo; echo;

echo "2) query_string: fast - is search_analyzer used?";
curl -X GET 'localhost:9200/_search' -d '{"query":{"query_string":{"query":"fast","default_field":"text"}}}'; echo; echo;

echo "2.5) query_string: fast - forcing search_analyzer";
curl -X GET 'localhost:9200/_search' -d '{"query":{"query_string":{"query":"fast","default_field":"text","analyzer":"search"}}}'; echo; echo;

echo "3) query_string: fast - forcing search_analyzer, forcing AND operator";
curl -X GET 'localhost:9200/_search' -d '{"query":{"query_string":{"query":"fast","default_field":"text","analyzer":"search","default_operator":"AND"}}}'; echo; echo;

echo "4) match query: quick";
curl -X GET 'localhost:9200/_search' -d '{"query":{"match":{"text":{"query":"quick","analyzer":"search"}}}}'; echo; echo;

echo "5) match query: fast";
curl -X GET 'localhost:9200/_search' -d '{"query":{"match":{"text":{"query":"fast","analyzer":"search"}}}}'; echo; echo;

echo "6) match query: fast - forcing AND operator";
curl -X GET 'localhost:9200/_search' -d '{"query":{"match":{"text":{"query":"fast","analyzer":"search","operator":"AND"}}}}'; echo; echo;
```

Output of queries:

```
1) query_string: quick
{"took":4,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.15342641,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.15342641, "_source" : {
  "text" : "Quick brown fox"
}}]}}

2) query_string: fast - is search_analyzer used?
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

2.5) query_string: fast - forcing search_analyzer
{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {
  "text" : "Quick brown fox"
}}]}}

3) query_string: fast - forcing search_analyzer, forcing AND operator
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {
  "text" : "Quick brown fox"
}}]}}

4) match query: quick
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {
  "text" : "Quick brown fox"
}}]}}

5) match query: fast
{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {
  "text" : "Quick brown fox"
}}]}}

6) match query: fast - forcing AND operator
{"took":4,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

My comments on results:

_(note that comment no.2 may contain question regarding other non related issue)_

1) `query_string` for query "quick" works as expected.

2) `query_string` for query "fast" does not seem to work. According to the [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#string) I was expecting that `search_analyzer` defined in `string` type mapping would be used. But anyway, this should not be the topic of this issue... :smile:

2.5) `query_string` for query "fast" works (if I explicitly force `search` analyzer) so we can say query time synonym works fine.

3) The same situation as in 2.5) except we are forcing `AND` operator. It should work and it is working.

4) Now, let's use `match` query and query for "quick". It works fine.

5) Again, `match` query but query for "fast". It works, so far so good.

6) The same as in 5) except we are forcing `AND` operator. It should work (I hope) but it is not.

If I could speculate about why this is happening:

a) MatchQueryParser does something like:

```
... if ("and".equalsIgnoreCase(op)) {
    matchQuery.setOccur(BooleanClause.Occur.MUST);
} ...
```

b) and MatchQuery does not take account on the position of tokens. It simply stacks all incoming tokens into BooleanQuery. It contains patterns similar to the following excerpt:

```
BooleanQuery q = new BooleanQuery(positionCount == 1);
for (int i = 0; i &lt; numTokens; i++) {
    boolean hasNext = buffer.incrementToken();
    assert hasNext == true;
    final Query currentQuery = newTermQuery(mapper, new Term(field, termToByteRef(termAtt)));
    q.add(currentQuery, occur);
}
```

The position of tokens is not taken into account which would explain why this is not working as expected in combination with `AND` operator in situations described above.
I think if incoming tokens share the same position it should generate Boolean subquery with `OR` operator (?).
</description><key id="20830854">3881</key><summary>Multiple tokens at the same position not working correctly with match query if AND operator is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-10T19:51:49Z</created><updated>2013-10-15T08:16:55Z</updated><resolved>2013-10-14T12:10:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-11T13:00:36Z" id="26134964">&gt; I think if incoming tokens share the same position it should generate Boolean subquery with OR operator (?).

I agree!
</comment><comment author="lukas-vlcek" created="2013-10-11T13:43:13Z" id="26137777">btw @s1monw I do not want to hijack this issue but what do you think about my comment no.2 (to me it seems that the search analyzer is not used while it should be, no?) Is it worth opening a new issue or I am misunderstanding something here?
</comment><comment author="s1monw" created="2013-10-11T15:02:58Z" id="26144198">I updated the PR with a test for your issue no. 2 but I can't reproduce it though. Works just fine and uses the right filter or do I miss something?
</comment><comment author="lukas-vlcek" created="2013-10-11T15:19:13Z" id="26145513">If my recreation script returns one hit for the second query to you then this means it has been probably fixed already (or hard to say ... ). Just ignore it...
Thanks!
</comment><comment author="s1monw" created="2013-10-11T15:24:43Z" id="26146004">I will try to recreate it via REST maybe there is some problem there. I don't think I will get to it today so I will update it later!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/test/java/org/elasticsearch/count/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Add match query support for stacked tokens</comment></comments></commit></commits></item><item><title>Use Locale.ROOT by default to parse dates instead of the system locale.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3880</link><project id="" key="" /><description>SimpleDateMappingTests fails on my computer because the default locale is french and DateFieldMapper uses the system locale by default for date parsing.

This commit makes DateFieldMapper use Locale.ROOT instead by default.
</description><key id="20828271">3880</key><summary>Use Locale.ROOT by default to parse dates instead of the system locale.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-10-10T19:06:40Z</created><updated>2014-06-18T07:52:41Z</updated><resolved>2013-10-15T09:16:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-10T19:15:02Z" id="26082493">+1 I have no idea why I added this comment in there... odd
</comment><comment author="s1monw" created="2013-10-15T09:19:28Z" id="26320069">The tests should have caught this btw. but that test did not subclass `ElasticsearchTestCase` so it didn't get a random locale. I fixed all the tests to subclass the randomized base classes here edbfb04af69e240b2fafad8c5af12829042ac050
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document strict dynamic type mapping.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3879</link><project id="" key="" /><description>Document how to disable dynamic field mapping in the dynamic mapping section.  For #3877
</description><key id="20825457">3879</key><summary>Document strict dynamic type mapping.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattweber</reporter><labels /><created>2013-10-10T18:20:25Z</created><updated>2014-07-16T21:52:07Z</updated><resolved>2013-10-18T15:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-10-18T15:31:51Z" id="26605211">@javanna I have updated the PR to only include the doc updates and fixed the wording.  Thanks!
</comment><comment author="javanna" created="2013-10-18T15:46:09Z" id="26606358">Thanks @mattweber !!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search using BooleanQueryBuilder and GeoShapeQueryBuilder results in "Current context not an ARRAY but OBJECT"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3878</link><project id="" key="" /><description>First of all, thanks for building such an amazing service. I am loving my experience with ElasticSearch so far.

What I've run into is that trying to use the BooleanQueryBuilder in conjunction with the GeoShapeQueryBuilder is resulting in the following exception.

```
org.elasticsearch.search.builder.SearchSourceBuilderException: Failed to build search source
    at org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:579)
    at org.elasticsearch.action.search.SearchRequest.source(SearchRequest.java:258)
    at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:839)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
    at com.tomtom.lbs.vectordb.data.services.ElasticSearchService.search(ElasticSearchService.java:119)
    ... 38 more
Caused by: org.elasticsearch.common.jackson.core.JsonGenerationException: Current context not an ARRAY but OBJECT
    at org.elasticsearch.common.jackson.core.base.GeneratorBase._reportError(GeneratorBase.java:444)
    at org.elasticsearch.common.jackson.dataformat.smile.SmileGenerator.writeEndArray(SmileGenerator.java:553)
    at org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeEndArray(JsonXContentGenerator.java:59)
    at org.elasticsearch.common.xcontent.XContentBuilder.endArray(XContentBuilder.java:227)
    at org.elasticsearch.index.query.BoolQueryBuilder.doXArrayContent(BoolQueryBuilder.java:182)
    at org.elasticsearch.index.query.BoolQueryBuilder.doXContent(BoolQueryBuilder.java:149)
    at org.elasticsearch.index.query.BaseQueryBuilder.toXContent(BaseQueryBuilder.java:65)
    at org.elasticsearch.search.builder.SearchSourceBuilder.toXContent(SearchSourceBuilder.java:601)
    at org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:576)
    ... 43 more
```

Here is my code to build and execute the query:

``` java
GeoShapeQueryBuilder geoQuery = QueryBuilders.geoShapeQuery("searchGeometry", new RectangleImpl(y1, y2, x1, x2, SpatialContext.GEO));
BoolQueryBuilder query = QueryBuilders.boolQuery();

if (featureIds.length &gt; 0)
        query.must(QueryBuilders.termQuery("featureType", featureIds[0]));

query.must(geoQuery);

SearchRequestBuilder request = esClient.prepareSearch("index_name")
        .setSearchType(SearchType.QUERY_THEN_FETCH)
        .setQuery(query)
        .setFrom(0)
        .setSize(maxResults)
        .setExplain(false);

SearchResponse response = request.execute().actionGet();
```

Am I doing something wrong?  I have a feeling I am but it looks right to me.

This isn't a big deal for me, because I can just build a JSON query manually and that works perfectly fine.  However it'd be nice to be able to use the QueryBuilders as make the code much easier to read.

Thanks!
</description><key id="20825432">3878</key><summary>Search using BooleanQueryBuilder and GeoShapeQueryBuilder results in "Current context not an ARRAY but OBJECT"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">sowelie</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-10T18:20:05Z</created><updated>2016-04-04T15:47:58Z</updated><resolved>2013-10-11T15:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-11T08:41:56Z" id="26122547">Hey,

I can reproduce this and will take a further look at it... the combination of geoquery and term query leads to that (you get sort of a different exception when you switch term and geoquery in the must query).

Thanks for notifying!
</comment><comment author="sowelie" created="2013-10-11T14:03:03Z" id="26139193">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java</file></files><comments><comment>Fix toXContent of GeoShapeQueryBuilder</comment></comments></commit></commits></item><item><title>default to strict dynamic types when dynamic mapping is disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3877</link><project id="" key="" /><description>When dynamic mapping is disabled (`index.mapper.dynamic: false`), I think it would be good to default all types to `dynamic: strict` so unknown fields won't cause a dynamic mapping update.

I was just mixed up by this because I thought disabling dynamic mapping would prevent new fields from being created, not just new types.  The combination of disabled dynamic mapping and a strict dynamic setting is what I thought the default was.

At the very least, we should document strict dynamic on the dynamic mapping page so one could see that is also required to prevent dynamic field mapping.  Currently the strict dynamic documentation is buried in the object type docs.
</description><key id="20823523">3877</key><summary>default to strict dynamic types when dynamic mapping is disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-10-10T17:48:45Z</created><updated>2016-11-06T10:21:48Z</updated><resolved>2016-11-06T10:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-10-10T20:21:13Z" id="26087732">I added a PR to update docs and a small fix to the tests (see my comment on a5bf882).  This test triggering dynamic mapping updates is what ultimately led me to investigate the strict dynamic setting.
</comment><comment author="jpountz" created="2014-09-05T09:47:43Z" id="54605088">I agree it is confusing. I think there are valid use-cases for setting independantly dynamic types and dynamic fields so maybe the fix would be to rename `index.mapper.dynamic` to `index.mapper.dynamic_types` to make clear that it only applies to types.
</comment><comment author="clintongormley" created="2016-11-06T10:21:48Z" id="258671892">Given that this hasn't happened yet, the docs have been improved, and we're planning on removing types (https://github.com/elastic/elasticsearch/issues/15613), i'm going to close this issue
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow users to configure how filters should be cached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3876</link><project id="" key="" /><description>Lucene 4.5 changed the way filters are cached by default from bit sets to compressed bit sets: WAH8DocIdSet, which is based on word-aligned hybrid encoding but there is also an interesting in-memory doc id set implementation based on pfor-delta encoding.

We should consider allowing our users to use such filters, and maybe even change the defaults.
</description><key id="20819093">3876</key><summary>Allow users to configure how filters should be cached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-10-10T16:32:07Z</created><updated>2014-12-24T16:27:10Z</updated><resolved>2014-12-24T16:27:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="miccon" created="2014-07-08T15:32:41Z" id="48355026">Are there already any plans when this feature will be developed?
I can't wait to try it out ;-)
</comment><comment author="clintongormley" created="2014-12-24T16:27:10Z" id="68062068">No longer relevant. We're better placed to make such decisions than the user, and this is already implemented in master.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>(plugin) feature request: Add support for ComplexPhraseQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3875</link><project id="" key="" /><description>Lucene has a 'ComplexPhraseQueryParser' that allows wildcards and disjunctions within phrases. This allows for queries such as:

"john do*"
"(john OR jane) doe"~5

There are some earlier requests to add this to elasticsearch, in which the requester was told by kimchy to write a plugin or open an issue about it. As far as I can find, no plugins were published and no issue was made for this request, so I thought it would be best to open an issue to track the status of this request. 

I would be willing to write such a plugin but I don't know anything about the elastic search source code so  I'm afraid I would need a bit of hand holding. 

Is there already such a plugin? Are there any plans to add such a plugin? Can anyone help me write this plugin?

Some Links: 
Earlier request:
https://groups.google.com/forum/#!topic/elasticsearch/g5iK5TqigL0
Lucene: http://lucene.apache.org/core/4_1_0/queryparser/org/apache/lucene/queryparser/complexPhrase/ComplexPhraseQueryParser.html
Solr plugin: https://issues.apache.org/jira/browse/SOLR-1604
</description><key id="20813883">3875</key><summary>(plugin) feature request: Add support for ComplexPhraseQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vanatteveldt</reporter><labels /><created>2013-10-10T15:16:08Z</created><updated>2014-11-28T09:36:04Z</updated><resolved>2014-11-28T09:35:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vanatteveldt" created="2013-10-23T15:29:24Z" id="26914901">I looked at the parsing code but had a difficult time understanding it, and especially understanding how to adapt the lucene complex phrase query to work with elastic.

So... I decided to go another route and wrote a query parser in python for the subset of the search query syntax that we need, and now generate my DSL client side:

https://github.com/amcat/amcat/blob/83e52e3c8f4a11336fa4a39adda3503652504ac0/amcat/tools/queryparser.py

Anyway, I'll close the issue as I don't think anyone will work on it in the near future. 
</comment><comment author="mishakogan" created="2014-11-20T22:12:12Z" id="63889655">I am very much interested in exposing ComplexPhraseQueryParser  and having a simple query dsl like above: 

"john do*"
"(john OR jane) doe"~5

in particular there is a lot of interest in proximity searching with wildcards in one query so something like this: "(jo\* doe)"~5

Can we please reopen this issue? There is a lot of interest in it
</comment><comment author="clintongormley" created="2014-11-28T09:35:59Z" id="64873565">While there use cases for this parser, it is going to be slow and heavy, and we don't want to promote the use of tools like that by providing them in core. I suggest that, if somebody really needs this, they should go ahead and implement it as a plugin.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support year in date math - #3828</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3874</link><project id="" key="" /><description>Tests and fix for #3828
</description><key id="20810873">3874</key><summary>Support year in date math - #3828</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">subhash</reporter><labels /><created>2013-10-10T14:31:41Z</created><updated>2014-07-16T21:52:07Z</updated><resolved>2013-10-11T08:16:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-11T08:18:14Z" id="26121321">Pushed with updated documentation as well. Thanks! 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file></files><comments><comment>Support year units in date math expressions</comment><comment>According to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html, the date math expressions support M (month), w (week), h (hour), m (minute), and s (second) units. Why years are not supported? Please add support for year units.</comment></comments></commit></commits></item><item><title>Get term vector api broken for missing payloads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3873</link><project id="" key="" /><description>If not all tokens in a field have a payload, the java api for term vectors will return the payload of the previous token at that position if there was one.

For example, suppose a field only contains two tokens each occurring once, the first having a payload and the second not, then for the second token, the payload of the first would be returned.
</description><key id="20805522">3873</key><summary>Get term vector api broken for missing payloads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>v1.0.0.Beta1</label></labels><created>2013-10-10T13:04:29Z</created><updated>2013-10-15T08:11:02Z</updated><resolved>2013-10-14T11:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvector/TermVectorFields.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorWriter.java</file></files><comments><comment>fix bug in term vector api, payloads were not handled correctly when some where missing</comment></comments></commit></commits></item><item><title>inconsistent behavior and documentation(?) errors with function_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3872</link><project id="" key="" /><description>Trying to use the new `function_score` query in 0.90.5 I stumbled across some errors. In a couple of cases the code does not work as documented.
## boost_mode `multiply`

Using `"boost_mode": "multiply"` fails:

```
{
    "query": {
        "function_score": {
            "boost_mode": "multiply",
            "query": {
                "match": {
                    "title": "elasticsearch"
                }
            },
            "functions": [
                {
                    "gauss": {
                        "date": {
                            "scale": "4w"
                        }
                    }
                }
            ]
        }
    }
}
```

result:

```
QueryParsingException[[psb-1.1] function_score illegal boost_mode [multiply]]
```

`"boost_mode": "mult"` works. However, `score_mode` just behaves the other way round:

```
QueryParsingException[[psb-1.1] function_score illegal score_mode [mult]];
```
## `boost` as decay_function

Both the blog post introducing the feature and the documentation propose a query along the lines of

```
"query": {
  "function_score": {
    "query": {
      "match": { "title": "elasticsearch"}
    },
    "functions": [
      { "boost":  1 },
      {
        "gauss": {
          "timestamp": {
            "scale": "4w"
          }
        }
      }
    ],
    "score_mode": "sum"
  }
}
```

respectively

```
"function_score": {
    "functions": [
        {
            "boost": "3",
            "filter": {...}
        },
        {
            "filter": {...},
            "script_score": {
                "params": {
                    "param1": 2,
                    "param2": 3.1
                },
                "script": "_score * doc['my_numeric_field'].value / pow(param1, param2)"
            }
        }
    ],
    "query": {...},
    "score_mode": "first"
}
```

The second sample is intended to provide a replacement for the `custom_filters_score_query`. However, the first example fails against 0.90.5 with the following trace:

```
Parse Failure [Failed to parse source [{ "query": { "function_score": { "query": { "match": { "title": "elasticsearch" } }, "functions": [ { "boost": 1 }, { "gauss": { "timestamp": { "scale": "4w" } } } ], "score_mode": "sum" } } }]]]; nested: QueryParsingException[[psb-1.1] No function with the name [boost] is registered.]; }
```

I failed to figure out what's the right incantation to invoke `boost`.
</description><key id="20800489">3872</key><summary>inconsistent behavior and documentation(?) errors with function_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">Xylakant</reporter><labels><label>bug</label><label>docs</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-10T11:14:48Z</created><updated>2013-10-31T17:55:19Z</updated><resolved>2013-10-14T13:13:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-10-10T11:31:18Z" id="26045960">"boost_mode": "mult" is indeed wrong, it was supposed to be working with "boost_mode": "multiply". I will fix this.

As for the boost function, the documentation is wrong. The keyword is "boost_factor" instead of "boost". I will update the documentation accordingly.

Thanks a lot for reporting this! 
</comment><comment author="Xylakant" created="2013-10-10T15:45:11Z" id="26065189">If `multiply` is the right keyword to use with boost_mode, the first two samples in the doc are false as well since they still mention `mult`:

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "FUNCTION": {},
    "boost_mode":"(mult|replace|...)"
}
```

and

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "functions": [
        {
            "filter": {},
            "FUNCTION": {}
        },
        {
            "FUNCTION": {}
        }
    ],
    "max_boost": number,
    "score_mode": "(mult|max|...)",
    "boost_mode": "(mult|replace|...)"
}
```

Thanks for the quick response.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file></files><comments><comment>fix naming in function_score</comment></comments></commit></commits></item><item><title>Add toString and toXContent to IndexResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3871</link><project id="" key="" /><description>Currently `IndexResponse` does not like other responses implement `ToXContent` and neither a `toString` method. We should try to be consistent here.
</description><key id="20796829">3871</key><summary>Add toString and toXContent to IndexResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-10-10T09:47:53Z</created><updated>2015-03-24T10:05:18Z</updated><resolved>2015-03-24T10:05:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-10T09:53:08Z" id="26041097">+1!
</comment><comment author="martijnvg" created="2013-10-11T09:36:43Z" id="26125307">Looks good, but should we do this consistently for all response classes? (moving ToXContent from rest code to response classes and implement toString).  Like: UpdateResponse, DeleteResponse, SearchResponse, ExplainResponse, DeleteByQueryResponse etc?
</comment><comment author="s1monw" created="2013-10-11T09:48:37Z" id="26125923">I completely agree - I will look into this soon hopefully
</comment><comment author="s1monw" created="2015-03-24T10:05:08Z" id="85431350">closing for now this didn't go anywhere
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix small doc mistakes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3870</link><project id="" key="" /><description>It's not much :)
</description><key id="20793311">3870</key><summary>Fix small doc mistakes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jchampion</reporter><labels /><created>2013-10-10T08:29:44Z</created><updated>2014-07-16T21:52:07Z</updated><resolved>2013-10-10T09:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-10T09:21:56Z" id="26039438">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix for "has_child ignores type" - #3818</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3869</link><project id="" key="" /><description>`HasChildFilter` ignores the `childType` provided. The fix is to include this in a filter while searching for the children to fix the context.

Test cases verify that we do not qualify children whose types conflict with the provided type in `has_child`.

The test scenario is simpler than the description in #3818. Simply, create a parent with two children of different types and issue a `has_child` query with fields from one child and the type of another. This returns the parent, whereas with the fix, it won't
</description><key id="20786228">3869</key><summary>Fix for "has_child ignores type" - #3818</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">subhash</reporter><labels /><created>2013-10-10T04:04:47Z</created><updated>2014-07-03T03:53:56Z</updated><resolved>2013-10-14T10:48:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-14T10:48:08Z" id="26247627">@subhash Thanks for creating PR for this! The code has been updated in the meantime and it has been fixed in master and 0.90. Also it has been fixed in a slightly different manner, the child filter is being adding the has_child query parser, the benefit of that is that this filter that is being created in the parser is cached.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Please add a stats api to return the indices stats on a node individually</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3868</link><project id="" key="" /><description>The [node stats api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html) return the summary of all shards on a node. The [Indices stats api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-stats.html) return Index's stat on the cluster.
I also care about the individually shards stats info  on a node.
Thanks!
</description><key id="20785661">3868</key><summary>Please add a stats api to return the indices stats on a node individually</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yinchuan</reporter><labels /><created>2013-10-10T03:38:46Z</created><updated>2013-10-11T00:46:31Z</updated><resolved>2013-10-10T19:37:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-10-10T08:36:15Z" id="26037013">It's already there. You just need to add `level=shards` to your request. Try this:

```
$ curl "localhost:9200/_stats?pretty&amp;level=shards"
```
</comment><comment author="yinchuan" created="2013-10-10T09:26:25Z" id="26039676">I see it in the doc of Indices' api. Thanks for your reply very much.
Please close this issuse.
How can I get shards info on a node from the Node stats API, is there a
'level=indices' or something else?

On Thu, Oct 10, 2013 at 4:36 PM, Igor Motov notifications@github.comwrote:

&gt; It's already there. You just need to add level=shards to your request.
&gt; Try this:
&gt; 
&gt; $ curl "localhost:9200/_stats?pretty&amp;level=shards"
&gt; 
&gt; ¡ª
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3868#issuecomment-26037013
&gt; .

## 

Òü´¨ ¼Ü¹¹×éÊµÏ°Éú
QQ: 254113859
Google Plus: yinchuan.blue@gmail.com
</comment><comment author="imotov" created="2013-10-10T19:37:57Z" id="26084225">@yinchuan You can filter shards from non-relevant nodes on your side.
</comment><comment author="yinchuan" created="2013-10-11T00:46:31Z" id="26104602">I got it, Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error while updating index settings when adding url parameters to the URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3867</link><project id="" key="" /><description>to reproduce(0.90.3):

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/_settings -d '{"index.number_of_replicas":"0"}'
curl -XPUT http://localhost:9200/foo/_settings?foo=bar -d '{"index.number_of_replicas":"0"}'
```

so, the first update settings work ok as expected, but for the second one(with param foo=bar)
I get:

{"error":"ElasticSearchIllegalArgumentException[Can't update non dynamic settings[[index.foo]] for open indices[[foo]]]","status":400}

not sure if that is the way its supposed to be, since its not mentioned on the docs
(http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-update-settings.html)

anyway, if that is the way its supposed to be, is there anyway to go around this issue? this is kind of a problem for me when executing cross domain requests using jquery(it adds a callback parameter to the url).
i couldnt spot this in other places(for the cluster update settings api, if a setting is not known its just ignored).
</description><key id="20770528">3867</key><summary>Error while updating index settings when adding url parameters to the URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-10-09T21:02:42Z</created><updated>2013-10-13T09:34:20Z</updated><resolved>2013-10-13T09:34:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-10-10T16:24:00Z" id="26068507">@javanna any insight on this? i mean, just if this is really the expected behavior or not.
</comment><comment author="lmenezes" created="2013-10-10T20:30:23Z" id="26088690">@javanna maybe if it could have something like https://github.com/elasticsearch/elasticsearch/blob/285f165100eeae14ec540fcb8326a936c7cda4f1/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java for the non dynamic settings, it would be possible to check here https://github.com/elasticsearch/elasticsearch/blob/95855c515b7a5f84689f2480aa558d5c8e25f0a6/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java if the setting is actually a valid setting name, and if not, just drop it. sounds reasonable?
</comment><comment author="javanna" created="2013-10-11T08:36:10Z" id="26122213">Hi @lmenezes, it's the expected behaviour, athough undocumented.
You could eventually do `curl -XPUT http://localhost:9200/foo/_settings?number_of_replicas=1` without needing to add the whole json body. You are getting back the error since the parameter that you added is not a valid index settings that can be changed.

May I ask what is your usecase here and why this is a problem?
</comment><comment author="lmenezes" created="2013-10-11T08:49:18Z" id="26122915">the use case is basically this: https://github.com/lmenezes/elasticsearch-kopf/issues/10
meaning, that in order to allow cross domain requests i need to use jsonp, and doing that with jquery, it automatically adds a callback parameter to the url... so, i finally get a index.callback is not dynamic error. 
i just wonder now, if its possible to not throw an exception when the parameter is not actually a existing parameter(like what happens to cluster/settings, where unknown parameters are just ignored. could it be possible? 
</comment><comment author="lmenezes" created="2013-10-11T08:50:28Z" id="26122973">if it seems ok, let me know and i will be glad to do something like the IndexDynamicSettingsModule but for non-dynamic properties. with that, it would be easy to check if the property is actually an existing property before throwing an error.
</comment><comment author="javanna" created="2013-10-11T08:57:08Z" id="26123304">Ok got it, thanks for the explanation, I'll look into this.
</comment><comment author="lmenezes" created="2013-10-11T09:00:47Z" id="26123503">cool, thanks :+1: 
</comment><comment author="lmenezes" created="2013-10-13T09:34:20Z" id="26214387">@javanna hey, i'm closing this. doesn't make sense at all for my case(since cross domain PUTs with jsonp shouldn't work anyway).
if you think might be useful for someone else cool, but i will close the issue. thanks and sorry for the waste of time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PHP client quick start instructions are wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3866</link><project id="" key="" /><description>On the page: 
  http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_quickstart.html 
it states the require part of composer.json should be
  "elasticsearch/elasticsearch-php": "~0.4"
This doesn't work. Further investigation led me to the client pages on GitHub, where on
https://github.com/elasticsearch/elasticsearch-php
it says:
  "elasticsearch/elasticsearch": "~0.4"
which does work. I then noticed this is also what it says on 
  http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_installation_2.html
so the quick start page is wrong with the extra '-php' bit.

Also, incidentally, the link "Composer can be found at their website." at the bottom of 
  http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_installation_2.html
doesn't work: it has an extraneous colon (the one at the top of the page is fine)
</description><key id="20757159">3866</key><summary>PHP client quick start instructions are wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">davidearl</reporter><labels /><created>2013-10-09T17:23:49Z</created><updated>2013-10-10T12:52:29Z</updated><resolved>2013-10-10T12:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2013-10-10T12:34:48Z" id="26049507">Gah, silly mistake, thanks for catching that!  That would cause considerable confusion during installation.

If you see any other problems (with the docs or the client itself), I'll see issues faster if you open them under the [elasticsearch-php repo](https://github.com/elasticsearch/elasticsearch-php).

Thanks again!
</comment><comment author="davidearl" created="2013-10-10T12:52:29Z" id="26050621">&gt; That would cause considerable confusion during installation.

Yes, it did :-)

&gt; open them under the elasticsearch-php repo.

OK, noted.

Having successfully installed the client now, it all appears to be working as intended. Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changing node settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3865</link><project id="" key="" /><description>Currently it is possible to change various settings at the cluster level. Would it be  possible to change the same settings on specific nodes?

I looked at implementing the feature myself. A ClusterUpdateSettingsRequest is a subclass of MasterNodeOperationRequest and not NodesOperationRequest. The latter takes in a list of nodeIds that the operation should be executed on. I considered making a new UpdateSettingsRequest based on NodesOperationRequest, but from what I can tell, the settings are saved in the cluster state. If I inject the ClusterService, can I execute commands only on the local node (I assume that means working only with the DiscoveryNode)? Not sure if requests are propagated if not using a MasterNodeOperationRequest . Persistent updates that span restarts are not a concern for me since the settings are merely for A/B testing and final changes will be made permanent via the config file. The settings are tied to the cluster state and I do not think I can break the dependency (from the little of the code that I have read).
</description><key id="20755775">3865</key><summary>Changing node settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels><label>high hanging fruit</label></labels><created>2013-10-09T16:59:26Z</created><updated>2014-07-25T09:09:22Z</updated><resolved>2014-07-25T09:09:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T09:09:22Z" id="50125611">This would be a very large change for a small benefit.  We've discussed it and decided against supporting this, at least for now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Eager loading of field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3864</link><project id="" key="" /><description>Field data takes a long time to load and can make the first queries following a refresh very slow. The current way to solve this issue is to register warmers based on queries, but we could also have a configuration option for this in the mappings.
</description><key id="20751106">3864</key><summary>Eager loading of field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-09T15:48:05Z</created><updated>2013-10-23T10:07:09Z</updated><resolved>2013-10-09T15:54:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-10-09T15:54:06Z" id="25982856">Closed via ff06bd629b07570a65396f4a06408a628fe8ac5f

This can be configured through the `loading` parameter of the `fielddata` section in the mappings, for example:

``` javascript
"properties": {
    "price": {
        "type": "float",
        "fielddata": {
            "loading": "eager"
        }
    }
}
```

The default remains lazy loading, meaning that field data will be loaded per segment on the first query that requires it and will later be kept around for use in subsequent requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for Lucene SuggestStopFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3863</link><project id="" key="" /><description>The suggest stop filter is an improved version of the stop filter, which
takes stopwords only into account if the last char of a query is a
whitespace. This allows you to keep stopwords, but to allow suggesting for
"a".

Example: Index document content "a word". You are now able to suggest for
"a" and get back results in the completion suggester, if the suggest stop
filter is used on the query side, but will not get back any results for
"a " as this is identified as a stopword.
</description><key id="20750293">3863</key><summary>Add support for Lucene SuggestStopFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-09T15:36:53Z</created><updated>2014-07-16T21:52:08Z</updated><resolved>2013-10-15T14:13:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-09T22:38:44Z" id="26015262">Can we call this `completion_stop` rather than `suggest_stop` otherwise this might be confused to be useful for spellchecking? I also think we should include the documentation for this new filter in the commit?
</comment><comment author="spinscale" created="2013-10-10T06:40:13Z" id="26032158">Was thinking if it might be usable in `match_phrase_prefix` queries as well?
</comment><comment author="s1monw" created="2013-10-10T09:51:31Z" id="26041010">hmm maybe we do it totally different and make it an option on `stop` instead of adding a new token filter? like `remove_trailing : true|false`
</comment><comment author="spinscale" created="2013-10-15T09:01:43Z" id="26319042">Updated to work as an additional option for the existing stopfilter, instead of being an own one.
Also updated documentation.
</comment><comment author="s1monw" created="2013-10-15T10:06:09Z" id="26322605">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consider using SloppyMathUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3862</link><project id="" key="" /><description>SloppyMathUtils functions trade (little) precision for speed, this would probably be a better trade-off for our geo stuff?
</description><key id="20747877">3862</key><summary>Consider using SloppyMathUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-10-09T15:02:24Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-12-11T09:22:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2013-10-09T20:06:08Z" id="26003804">From a glance it seems elasticsearch uses the dot-product formula. So you might get a reasonable performance increase here (see Ted's microbenchmark results of various methods here: https://microbenchmarks.appspot.com/runs/b79313c0-bbc0-45bf-9f96-8ae5ff0bac77#r:scenario.benchmarkSpec.methodName)

ping me if you need anything
</comment><comment author="s1monw" created="2013-10-09T22:12:21Z" id="26013564">+1
</comment><comment author="chilling" created="2013-10-10T08:19:06Z" id="26036149">hi @jpountz, this is a very good idea. I'll set it up.
</comment><comment author="s1monw" created="2013-11-12T11:17:44Z" id="28285616">what is the status on this one @chilling 
</comment><comment author="jpountz" created="2013-12-03T09:43:05Z" id="29695619">A new issue that might be worth following: https://issues.apache.org/jira/browse/LUCENE-5271
</comment><comment author="kimchy" created="2013-12-11T15:58:50Z" id="30332506">should we back port this to 0.90?
</comment><comment author="s1monw" created="2013-12-11T15:59:14Z" id="30332570">absolutely!!
</comment><comment author="jpountz" created="2013-12-11T16:01:34Z" id="30332909">+1
</comment><comment author="chilling" created="2013-12-11T16:10:44Z" id="30333924">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/test/java/org/apache/lucene/util/SloppyMathTests.java</file></files><comments><comment>SloppyMath</comment></comments></commit></commits></item><item><title>Get document API can specify an alias, but will return documents that are not part of that alias (as defined by the filter for that alias)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3861</link><project id="" key="" /><description>Create an index and populate it with two documents. Create an aliases with a filter, such that the alias contains one document.

A search using the alias will return one result.
A get document request using the alias can retrieve both documents.

This may confuse users: "why does this document not turn up in my search results", and makes it hard to implement a security model using aliases.

The UidField.loadDocIdAndVersion() method already uses a filter to check whether the document has been deleted. It would be possible to pass in the alias filter, if defined.
</description><key id="20743479">3861</key><summary>Get document API can specify an alias, but will return documents that are not part of that alias (as defined by the filter for that alias)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ccw-morris</reporter><labels><label>:Aliases</label><label>adoptme</label><label>enhancement</label></labels><created>2013-10-09T13:56:33Z</created><updated>2017-01-06T04:47:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T11:32:23Z" id="26588771">The tricky bit here is that the GET api is realtime, as we can either get document by id from lucene or from the transaction log, before the next refresh happens and the newly indexed documents are made searchable.
Filters are part of the search capabilities of elasticsearch, using the search API, which are not applicable to the GET api. In fact we can only get docs by id from the transaction log, we cannot execute queries (or filters) on top of it.
That's why there are cases where the filter associated with an alias might be ignored. We should at least better document those cases.
</comment><comment author="javanna" created="2015-03-22T15:16:08Z" id="84628806">I would like to know what people think about the following options:
1) optionally index the document taken from the transaction log in an in memory index so that we can execute the filter against it... seems very slow, default would be not do it but could be enabled via setting
2) just document this, explain clearly why it happens, and make it clear that the get api against filtered aliases is not something you want to build a security model around. Move to search api instead, giving up on the real-time aspect, but relying on the guarantee that only documents that match the filter get returned.

I am personally afraid of the complexity and slowness that would be introduced with option 1. Do comment if you have better ideas around this.
</comment><comment author="javanna" created="2015-03-28T08:13:13Z" id="87189608">We discussed this, we'd rather prefer to reject get requests performed against a filtered alias, given that we cannot provide users the correct answer. If the get request has the `realtime` flag set to `false` though, meaning that the document will only be retrieved from the lucene index, we should not ignore the filter within the alias, instead apply it and do return the result. Marking as adoptme.
</comment><comment author="javanna" created="2015-06-05T10:44:22Z" id="109256450">Remarking for discussion... I attempted to solve this as explained above, but it's more complicated than it initially seemed, see [comment here](https://github.com/elastic/elasticsearch/pull/11494#issuecomment-109255972). We have to discuss how we want to move forward here, we have again a few options:
1) go all the way with rejecting a get against a filtered alias, wherever it can happen (also index api) and make filtered aliases consistent in every case, where the filter cannot be run we reject.
2) leave things as they are and better document limitations of filtered aliases
3) replace any internal get against a filtered alias with a search api (near real-time though)

other opinions are welcome.
</comment><comment author="clintongormley" created="2015-06-05T13:55:47Z" id="109301258">My preference is number 2.  I don't think we can make filtered aliases behave exactly like indices, We would just end up creating complexity, poorer performance, and still have edge cases.  Rather just explain how filtered aliases work and leave it at that.  If users need the functionality of indices, then they should use a real index instead.
</comment><comment author="clintongormley" created="2015-09-21T18:53:05Z" id="142074646">Closing this issue and leaving things as they are today
</comment><comment author="javanna" created="2016-10-24T11:45:32Z" id="255718267">Shall we reconsider this now that #20102 is in? Seems like we can actually run the filter now whenever a filtered alias is accessed? Maybe we should only accept operations against a filtered alias when realtime is `true` (default). Also reject write operations against a filtered alias, which is confusing. And we would have to adapt every api that accesses an alias to actually run the filter, which gets ignored at the moment. Thoughts?
</comment><comment author="javanna" created="2016-11-02T11:29:53Z" id="257839433">I am reopening for discussion.
</comment><comment author="clintongormley" created="2016-11-06T10:19:30Z" id="258671763">My feeling is that this has not been a big problem for users.  Plus there is a workaround: they can retrieve docs with a search-by-id against the alias instead of using GET (ie opt in to doing what you're suggesting themselves).  As we know from security in x-pack, there's a lot more to security than just making filtered aliases work for GET.  I think we should leave things as they are.
</comment><comment author="clintongormley" created="2016-12-23T10:03:15Z" id="268966702">Discussed in FixItFriday - we're good to implement this.</comment><comment author="idubinskiy" created="2017-01-06T04:47:53Z" id="270830594">Was this limitation ever documented (at least in the 2.3 docs)? Led to a lot of confusion for me while trying to handle an edge case.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for combining fields to the FVH</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3860</link><project id="" key="" /><description>Closes #3750
</description><key id="20740125">3860</key><summary>Add support for combining fields to the FVH</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2013-10-09T12:56:48Z</created><updated>2014-06-29T01:19:52Z</updated><resolved>2013-12-03T10:24:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-10-09T12:58:47Z" id="25968682">This still has a bunch of work to do around error handling, documentation, pushing the change to Lucene, and whatever structure Elasticsearch has around its "Lucene Monitor" classes but it seems promising to me.
</comment><comment author="jpountz" created="2013-10-10T12:37:58Z" id="26049718">@nik9000 +1 for trying to incorporate this feature into Lucene. Would you like to open an issue in Lucene's JIRA and attach a patch? Then once it's in Lucene we can have it pushed to Elasticsearch as well until the next Lucene release makes it unnecessary.
</comment><comment author="nik9000" created="2013-10-10T13:53:36Z" id="26055256">&gt; Would you like to open an issue in Lucene's JIRA and attach a patch?

https://issues.apache.org/jira/browse/LUCENE-5274

I'll rework my Elasticsearch stuff into a Lucene patch.
</comment><comment author="jpountz" created="2013-10-10T14:02:45Z" id="26055976">Thanks @nik9000 I just assigned the issue to myself.
</comment><comment author="nik9000" created="2013-10-22T18:13:42Z" id="26827712">Thanks so much for helping me get this into Lucene.  Now that it is merged, what is the procedure like to get Elasticsearch using it?
</comment><comment author="nik9000" created="2013-10-30T19:54:20Z" id="27432904">Just had a look and this feature is not in 4.5.1 that Elasticsearch is using.  It'll be in 4.6.  Anyway, pulling the code into Elasticsearch would require 4 or 5 of those XLuceneClassName classes which seems like a lot for this feature.  I suppose it'd be best just to wait until Lucene 4.6 is released.
</comment><comment author="kimchy" created="2013-10-30T19:59:06Z" id="27433293">++ on waiting for Lucene 4.6 if its possible timing wise for you.
</comment><comment author="nik9000" created="2013-10-30T20:09:53Z" id="27434224">I've just got a few bugs staring at me every time I open up my backlog that need this.  I imagine I can comfortably wait another month or so.  As time goes on more and more people are getting exposed to the subpar hack that I've got in place to emulate this which makes me uneasy.
</comment><comment author="kimchy" created="2013-10-30T20:11:22Z" id="27434330">@nik9000 understood, tell us if things become pressing, no problem with backporting to X classes for 0.90.7 if Lucene 4.6 doesn't happen by then...
</comment><comment author="nik9000" created="2013-11-25T21:18:02Z" id="29243076">Now that 4.6 is merged I can finish this!  Yay!  I'm double checking it and running the whole test suite again.  Also, another plug for #3757.  That one has a patch waiting on the Lucene side and would let me get full use out of this feature.
</comment><comment author="nik9000" created="2013-11-25T21:36:15Z" id="29244621">New update pushed.  Rebased and working without X classes against Lucene 4.6.
</comment><comment author="jpountz" created="2013-11-27T23:03:24Z" id="29427567">Hey Nik, this patch looks good to me. I think it just misses some documentation (as mentionned in the commit message) before we can push it.
</comment><comment author="nik9000" created="2013-11-27T23:42:34Z" id="29429490">Docs!  I've forgotten docs!  Will do it soon. 
</comment><comment author="nik9000" created="2013-11-30T16:10:50Z" id="29555048">Added docs.
</comment><comment author="jpountz" created="2013-12-03T10:24:17Z" id="29698225">Pushed. Thanks, Nik!
</comment><comment author="nik9000" created="2013-12-03T16:15:28Z" id="29723551">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable delimited payload token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3859</link><project id="" key="" /><description>The [DelimitedPayloadTokenFilter](http://lucene.apache.org/core/4_1_0/analyzers-common/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.html) would be useful to have in elasticsearch for testing functionality using payloads, for example the term vector api or scoring based on payloads as requested in issue #3772.
</description><key id="20739762">3859</key><summary>Enable delimited payload token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-10-09T12:50:09Z</created><updated>2013-10-15T08:11:27Z</updated><resolved>2013-10-14T11:55:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>src/main/java/org/elasticsearch/index/analysis/DelimitedPayloadTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/termvectors/GetTermVectorTests.java</file></files><comments><comment>enable delimited payload token filter</comment></comments></commit></commits></item><item><title>Migrate from Trove to Hppc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3858</link><project id="" key="" /><description /><key id="20727860">3858</key><summary>Migrate from Trove to Hppc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-09T08:08:14Z</created><updated>2013-10-10T11:56:49Z</updated><resolved>2013-10-09T08:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-09T08:08:25Z" id="25953350">Fixed via https://github.com/elasticsearch/elasticsearch/commit/088e05b3
</comment><comment author="karussell" created="2013-10-10T09:52:13Z" id="26041056">@martijnvg why was the move done? license? performance? ease of use? Or because of stability issues?

http://elasticsearch-users.115913.n3.nabble.com/Anyone-have-G1-GC-working-What-environment-configs-td4039878.html
</comment><comment author="martijnvg" created="2013-10-10T11:37:48Z" id="26046329">@karussell The main reason why we moved to hppc is because Hppc has a cleaner licence. Hppc is Apache licensed and Trove is LGPL licensed. Also we noticed that hppc performs a bit faster than trove does, so that was a second but less minor reason to move to Hppc. I actually didn't know about this stability issue with the G1 garbage collector, but that seems like a positive side effect.
</comment><comment author="karussell" created="2013-10-10T11:42:56Z" id="26046603">Thanks for the explanation! But JTS is still lgpl right?
</comment><comment author="martijnvg" created="2013-10-10T11:56:49Z" id="26047305">Yes, but that is an optional library.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Closes #3843, fixes river routing for river names containing the river type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3857</link><project id="" key="" /><description>this closes #3843
the fix splits the the node.river configuration at the comma separator (and leading and following spaces) and checks every part for equality with the current riverName.type() or riverName.name().

so a node with an attribute node.river: dummyRiverOnNodeA will not match a river with name dummyRiverOnNodeB which is of type "dummy".
</description><key id="20725710">3857</key><summary>Closes #3843, fixes river routing for river names containing the river type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgaedicke</reporter><labels /><created>2013-10-09T07:09:16Z</created><updated>2014-07-16T21:52:09Z</updated><resolved>2014-04-16T07:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Many percolators kills server with java.lang.OutOfMemoryError: Java heap space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3856</link><project id="" key="" /><description>ES version 0.90.5
Java version: 1.7.0_40
Linux CentOS 64bit (2.6.32-358.14.1.el6.x86_64)
Default config with some changes:

```
index.number_of_shards: 1
index.number_of_replicas: 0
bootstrap.mlockall: true
```

```
# testing environment set memory to 512MB
ES_HEAP_SIZE=512m
```

I wanted to test how many percolator documents can handle one ElasticSearch (ES) instance and how fast it works. First idea is to create 15000 percolator documents.
Tried 1GB HEAP size, then 2GB heap size but did not succeeded (on Windows). So moved to linux box. There is no much memory. enabled mlockall (BTW if it's enabled or disabled - same result), defined to create only one shard without replicas.

Created script that generates percolator and puts one by one to ES. Query is simple:

```
{
  "query":{
    "bool":{
      "should":[
        {"match":{"headline":"3BPV i3ZBGQgK mc72 5EhW"}},
        {"match":{"bodytext":"NGgfMl XOIb nEgbLoXQ eL1g ItP DvE OE7eMj OJawh3"}},
        {"wildcard":{"bodytext":"0vRa* Cf* 47b* hz735* cQJXyo* qMyL*"}}
      ]
    }
  }
}
```

every time text is random strings with different length but number of words is same.
I create index

```
curl -XPOST http://192.168.1.144:9200/demo
```

and then iterate 15000 times generating percolators and add one by one to ES. 

```
curl -XPUT http://192.168.1.144:9200/_percolator/demo/[name] -d '{
  "query":{
    "bool":{
      "should":[
        {"match":{"headline":"3BPV i3ZBGQgK mc72 5EhW"}},
        {"match":{"bodytext":"NGgfMl XOIb nEgbLoXQ eL1g ItP DvE OE7eMj OJawh3"}},
        {"wildcard":{"bodytext":"0vRa* Cf* 47b* hz735* cQJXyo* qMyL*"}}
      ]
    }
  }
}'
```

After ~2000 documents HEAP memory becomes full and ES becomes nonfunctional.

If i do not create index and add percolator documents then HEAP memory usage is low. I can create 15000 percolator documents easy and fast. After creating all percolator documents I create index - then HEAP memory fills up quickly and bye bye ES - nonfunctional again.
</description><key id="20724849">3856</key><summary>Many percolators kills server with java.lang.OutOfMemoryError: Java heap space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">xawiers</reporter><labels /><created>2013-10-09T06:39:44Z</created><updated>2014-08-08T17:53:30Z</updated><resolved>2014-08-08T17:53:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-09T10:20:38Z" id="25960501">@xawiers Can you share that script via a gist or something like that?

The `query` object in the percolate documents are parsed to a Lucene query and loaded into memory, so the number of number of percolate queries is bounded by the amount of memory you have per node. I don't expect that you would OOM when just indexing 2000 percolate queries.
</comment><comment author="xawiers" created="2013-10-09T10:52:22Z" id="25962103">Plan is to have ~15000 percolate queries (let's assume these will be alerts)
and every time I plan to check one by one document to see what alerts matches.

BTW changed wildcard search to one / two words with wildcard at the end and it worked. With 6-8 words it does not work. It's possible to kill system :) if someone gains access to creating such amount of percolate queries.

PHP code I used https://gist.github.com/xawiers/6899411
</comment><comment author="martijnvg" created="2013-10-10T16:21:12Z" id="26068273">@xawiers I see the same problem here, the wildcard queries take way more memory than just match queries.

This boils down on the Lucene level in the fact that the wildcard string in compiled into an automaton, which takes a lot of memory.
</comment><comment author="martijnvg" created="2013-10-11T13:31:26Z" id="26136956">@xawiers In your example I think you should use prefix query instead of the wildcard query. The `prefix` query does the same as the suffix wildcard query, but doesn't compile into a memory heavy automaton like the wildcard query does.

On top of this I think you should just use the `query_string` query, because that would do the expected behaviour in your case when you generate multiple words with wildcard. (each wildcard word, should be interpreted on its own instead of evaluating the complete string as wildcard expression).
</comment><comment author="martijnvg" created="2013-10-14T10:35:49Z" id="26247229">@xawiers Did you try to replace the `wildcard` query with `query_string`? This should significantly reduce the memory usage in your script.
</comment><comment author="clintongormley" created="2014-08-08T17:53:30Z" id="51635546">No feedback since last year. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document score needed for scan search_type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3855</link><project id="" key="" /><description>When using scan, all documents have a zero score. I need to retrieve a full result set with scores. My result set may contain 100K documents. How hard would it be to add scoring to the scan search_type?
</description><key id="20720361">3855</key><summary>Document score needed for scan search_type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jonusko</reporter><labels /><created>2013-10-09T03:12:26Z</created><updated>2013-10-23T01:50:54Z</updated><resolved>2013-10-23T01:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pmanvi" created="2013-10-10T10:44:53Z" id="26043655">+1
</comment><comment author="imotov" created="2013-10-23T01:50:54Z" id="26875132">Fixed in #3949. As a temporary workaround until 0.90.6 is released, you can set [track_scores](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_track_scores) flag in the body of the request instead of specifying it on URL. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>deleteByQuery shard failure when upgrading to 0.90.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3854</link><project id="" key="" /><description>I tried upgrading our cluster from 0.90.3 to 0.90.5. Started a rolling restart (one node at a time) of our 20 node cluster. Real time indexing/deleting/updating was still ongoing while the restart was occurring. About half way through a rolling restart of the cluster our deleteByQuery operations started taking a really long time to complete. Delete jobs that usually take half a second to complete would take 3-4 seconds. Also got a number of exceptions like the following:

&gt; Sep 25 14:35:19 es4.global.search.sat.wordpress.com [2013-09-25 14:35:19,033][WARN ][cluster.action.shard ] [es4.global.search.sat.wordpress.com] sending failed shard for [global-0-13m-14m][2], node[MxryNkAUTWSvMYDlvMtJpg], [R], s[STARTED], reason [Failed to perform [deleteByQuery/shard] on replica, message [RemoteTransportException[[es5.global.search.sat.wordpress.com][inet[/76.74.248.159:9300]][deleteByQuery/shard/replica]]; nested: EsRejectedExecutionException[rejected execution of [org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler]]; ]]

These failures only seemed to occur when going to nodes that had been updated to 0.90.5. Nodes still at 0.90.3 were not getting any errors.

Once I rolled back to 0.90.3 the problem went away. Looked through the 0.90.4 changes and #3526 looks like it could be related.

I have logs for that time period saved if that helps.

Potentially related. I am running the langdetect plugin and getting some exceptions when the text "has no features":

&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com [2013-09-25 14:33:04,259][DEBUG][action.langdetect        ] [es5.global.search.sat.wordpress.com] failed to execute [org.elasticsearch.action.langdetect.LangdetectRequest@1415bbce]
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com org.elasticsearch.ElasticSearchException: no features in text
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com     at org.elasticsearch.action.langdetect.TransportLangdetectAction.shardOperation(TransportLangdetectAction.java:94)
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com     at org.elasticsearch.action.langdetect.TransportLangdetectAction.shardOperation(TransportLangdetectAction.java:38)
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com     at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$1.run(TransportSingleCustomOperationAction.java:142)
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
&gt; Sep 25 14:33:04 es5.global.search.sat.wordpress.com     at java.lang.Thread.run(Thread.java:724)

I only mention it because it also seems to be related to the thread pool, and did somewhat occur at the same time.
</description><key id="20720163">3854</key><summary>deleteByQuery shard failure when upgrading to 0.90.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels /><created>2013-10-09T03:02:52Z</created><updated>2013-11-27T02:16:44Z</updated><resolved>2013-10-17T14:07:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-09T08:06:47Z" id="25953267">hey there,

does this error happen on the upgraded nodes (0.90.5) or the old nodes while you update? I'd also be curious if you have configured a queue size for you threadpools?
</comment><comment author="gibrown" created="2013-10-09T20:17:43Z" id="26004760">Hi

The error occurs in the logs of all nodes, but the replica where the failure occurred was always one of the nodes running 0.90.5.

My Threadpool settings:

threadpool:
    index:
        type: fixed
        size: 30
        queue_size: 1000
        reject_policy: caller
    bulk:
        type: fixed
        size: 30
        queue_size: 1000
        reject_policy: caller
    search:
        type: fixed
        size: 100
        queue_size: 300
        reject_policy: abort
    get:
        type: fixed
        size: 100
        queue_size: 300
        reject_policy: abort

Prior to the upgrade my search and get settings had been (I changed them due to the blocking type being deprecated):
    search:
        type: blocking
        min: 100
        size: 100
        queue_size: 500
        wait_time: 2s
    get:
        type: blocking
        min: 100
        size: 100
        queue_size: 500
        wait_time: 2s

So there were actually two changes being done when I saw the errors: upgrading to 0.90.5 and changing the threadpools. I assume that deleteByQuery uses the search threadpools?

However, I just tested changing these threadpools in 0.90.3 using the update cluster settings api, and after letting it run for a few hours I did not see any of the deleteByQuery errors nor any slowdowns in our deleteByQuery jobs. Also, watching in bigdesk, I am not seeing the thread queue for searches get above 20 in the current load (which is quite a bit higher than the load we were running during the original rolling restart).

Happy to run more tests.
</comment><comment author="s1monw" created="2013-10-17T13:03:51Z" id="26502079">&gt; &gt; However, I just tested changing these threadpools in 0.90.3 using the update cluster settings api, and after letting it run for a few hours I did not see any of the deleteByQuery errors nor any slowdowns in our deleteByQuery jobs. Also, watching in bigdesk, I am not seeing the thread queue for searches get above 20 in the current load (which is quite a bit higher than the load we were running during the original rolling restart).

does this mean you rolled back to 0.90.3 and used the same settings as you did on 0.90.5 but you see different behaviour?

the Delete By Query is executed on the `index` threadpool so your change could certainly cause this problem. In the upcoming release we have more infos in the error messages why a operation gets rejected. I will keep you posted if we find anything else.
</comment><comment author="kimchy" created="2013-10-17T13:15:51Z" id="26502943">@gibrown heya, I think I found the issue, see more here: #3929.
</comment><comment author="kimchy" created="2013-10-17T14:07:54Z" id="26508433">Fixed in #3929.
</comment><comment author="gibrown" created="2013-10-17T15:23:47Z" id="26516802">Awesome @kimchy looking forward to 0.90.6. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalAllocateDangledIndices.java</file><file>src/main/java/org/elasticsearch/transport/BaseTransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/TransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file></files><comments><comment>Settings queue_size on index/bulk TP can cause rejection failures when executed over network</comment><comment>The #3526 fix was not complete, it handled cases of on node execution, but didn't properly handle cases where it was executed over the network, and forcing the execution of the replica operation when done over the wire.</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 4.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3853</link><project id="" key="" /><description>Lucene 4.5 has been released and should now be used by Elasticsearch.
</description><key id="20712877">3853</key><summary>Upgrade to Lucene 4.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-08T22:57:53Z</created><updated>2013-10-08T22:58:48Z</updated><resolved>2013-10-08T22:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-10-08T22:58:27Z" id="25934828">Closed via 6b026119718a616fc76578aff3b20ceda023d403
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SimpleDateMappingTests.testLocale assumes that the default locale is english</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3852</link><project id="" key="" /><description>I got a failure running SimpleDateMappingTests.testLocale on a computer with a french locale because it couldn't figure out how to parse the date which is written in english.
</description><key id="20711358">3852</key><summary>SimpleDateMappingTests.testLocale assumes that the default locale is english</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-10-08T22:23:10Z</created><updated>2013-10-11T07:11:48Z</updated><resolved>2013-10-11T07:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file></files><comments><comment>Use Locale.ROOT by default to parse dates instead of the system locale.</comment></comments></commit></commits></item><item><title>Add facet support to percolate api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3851</link><project id="" key="" /><description>A percolate query is just a document with a query embedded into it and besides the query it can also contain arbitrary metadata. The facet support will allow that metadata of matched queries will be facetable. 

(note: this is different then just executing a search request on an index with _percolator type)
</description><key id="20681346">3851</key><summary>Add facet support to percolate api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-10-08T14:14:18Z</created><updated>2013-10-21T12:45:28Z</updated><resolved>2013-10-21T12:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/search/facet/InternalFacets.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorFacetsTests.java</file></files><comments><comment>Added facet support to the percolate api.</comment><comment>Closes #3851</comment></comments></commit></commits></item><item><title>Root analyzer is ignored with positions offset gaps.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3850</link><project id="" key="" /><description>Root analyzer (analyzer on a type) isn't picked for fields with `position_offset_gap` set.
</description><key id="20680831">3850</key><summary>Root analyzer is ignored with positions offset gaps.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label></labels><created>2013-10-08T14:05:39Z</created><updated>2014-12-24T16:26:00Z</updated><resolved>2014-12-24T16:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T16:26:00Z" id="68061996">The `analyzer` setting on type level is going away #8874.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prohibit adding a parent mapping at runtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3849</link><project id="" key="" /><description>Prohibit adding a parent mapping to a type at runtime by raising a MergeMappingException.

The breaking part here, is that if a document uses a top level `_parent` field, that it won't be indexed. This is because `_parent` is actually reserved metadata field and this also consistent with other metadata fields (like ttl, routing)
</description><key id="20674592">3849</key><summary>Prohibit adding a parent mapping at runtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>breaking</label><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-08T12:10:09Z</created><updated>2014-01-14T02:27:29Z</updated><resolved>2013-10-15T16:37:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MappingMetaDataParserTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Prohibit indexing a document with parent for a type that doesn't have a `_parent` field configured and prohibit adding a _parent field to an existing mapping.</comment></comments></commit></commits></item><item><title>Reject indexing requests which specify a parent, if no parent type is defined</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3848</link><project id="" key="" /><description>If you submit an indexing request and specify a parent but the mapping doesn't define a parent type, we currently silently ignore the parent value: 

```
curl -XPUT "http://localhost:9200/test/type/1?parent=1" -d'
{
   "field": "value" 
}'
```

We should reject the request.

The breaking part: Index requests with the parent flag will be rejected if there is no `_parent` metadata field in the mapping.
</description><key id="20674188">3848</key><summary>Reject indexing requests which specify a parent, if no parent type is defined</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>breaking</label><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-08T11:59:43Z</created><updated>2013-10-15T16:38:42Z</updated><resolved>2013-10-15T16:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MappingMetaDataParserTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Prohibit indexing a document with parent for a type that doesn't have a `_parent` field configured and prohibit adding a _parent field to an existing mapping.</comment></comments></commit></commits></item><item><title>plugin -remove deletes bin directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3847</link><project id="" key="" /><description>I tried to install a plugin directly from disk, but used the wrong arguments. Totally my fault for not RTFM properly.

However, by following the instructions printed out by `bin/plugin`, I ended up deleting my $ES/bin directory.

```
$ pwd
/tmp/elasticsearch-0.90.5
$ ls
LICENSE.txt    NOTICE.txt     README.textile bin            config         lib
$ bin/plugin --install file:///tmp/foo.zip
-&gt; Installing file:///tmp/foo.zip...
Failed to install file:///tmp/foo.zip, reason: plugin directory /tmp/elasticsearch-0.90.5/plugins already exists. To update the plugin, uninstall it first using -remove file:///tmp/foo.zip command
$ bin/plugin -remove file:///tmp/foo.zip
-&gt; Removing file:///tmp/foo.zip
Removed file:///tmp/foo.zip
$ ls
LICENSE.txt    NOTICE.txt     README.textile config         lib
```

I reproduced the problem in 0.90.5 and the latest master.
</description><key id="20672345">3847</key><summary>plugin -remove deletes bin directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dhepper</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-08T11:14:44Z</created><updated>2013-10-08T14:00:01Z</updated><resolved>2013-10-08T13:59:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-08T12:57:33Z" id="25887003">Thanks for raising that issue. Will fix it very soon!
</comment><comment author="dadoonet" created="2013-10-08T13:59:49Z" id="25891977">Thanks. Fix pushed in 0.90 and master.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/plugin/PluginManagerTests.java</file></files><comments><comment>plugin -remove deletes bin directory</comment><comment>Adding test for #3847.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>plugin -remove deletes bin directory</comment><comment>I tried to install a plugin directly from disk, but used the wrong arguments. Totally my fault for not RTFM properly.</comment></comments></commit></commits></item><item><title>Root document filtering in nested statistical facets.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3846</link><project id="" key="" /><description>I have to implement a statistical facet on a nested price field in order to get the min and max value for a slider. The problem is that facet_filter on nested facets seems to behave weirdly.

``` sh
#With this example mapping : 
curl -XPOST localhost:9200/test_nested_statistical -d '{
    "settings": {"number_of_shards": 1},
    "mappings": {
        "object": {
            "properties": {
                "title": {"type": "string"},
                "tags": {
                    "type": "string",
                    "index": "not_analyzed"
                },
                "prices": {"type": "nested",
                    "properties": {
                        "price": {"type": "float"},
                        "type": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                }
            }
        }
    }
}'

#Adding a few objects ...
curl -XPUT http://localhost:9200/test_nested_statistical/object/1 -d '{
    "title" : "Test One",
    "tags" : ["one", "two", "three"],
    "prices" : [{"price" : 20,"type": "TYPEONE"},{"price" : 30,"type": "TYPETWO"}]
}'

curl -XPUT 'http://localhost:9200/test_nested_statistical/object/2' -d '{
    "title" : "Test Two",
    "tags" : ["one", "four"],
    "prices" : [{"price" : 10,"type": "TYPEONE"},{"price" : 15,"type": "TYPETWO"}]
}'

#When I just filter the nested doc with a nested query set with join false, everything goes as intented : 
curl -XPOST "http://localhost:9200/test_nested_statistical/object/_search" -d'{
    "query" : {
                "terms" : {"tags":["one"]}
        },
        "facets" : {
                "priceone" : {
                        "statistical" : {
                                "field" : "prices.price"
                        },
                        "global" : true,
                        "facet_filter" : {
                                "nested" : {
                                        "path" : "prices",
                                        "query" : {
                                                "terms" : {
                                                        "prices.type" : ["TYPEONE"]
                                                }
                                        },
                                        "join" : false
                                }
                        },
                        "nested" : "prices"
                }
        }
}'
```

This query returns min : 10, max : 20, the right values for only the TYPEONE prices.

The issue comes up when I also try to filter the root documents used in for the facet. (Just in case : My use case prevents me from using global:false because I need the facet to have a wider context than my results but still be filtered some way, this example does not illustrate this difference in context, but you get the point)

``` sh
curl -XPOST "http://localhost:9200/test_nested_statistical/object/_search" -d'{
    "query" : {
                "terms" : {"tags":["one"]}
        },
        "facets" : {
                "priceone" : {
                        "statistical" : {
                                "field" : "prices.price"
                        },
                        "global" : true,
                        "facet_filter" : {
                                "and" : [
                                    {
                                        "nested" : {
                                            "path" : "prices",
                                            "query" : {
                                                    "terms" : {
                                                            "prices.type" : ["TYPEONE"]
                                                    }
                                            },
                                            "join" : false
                                        }
                                    },{
                                        "terms" : {
                                                "tags" : ["one"]
                                        }
                                    }
                                ]

                        },
                        "nested" : "prices"
                }
        }
}'
```

This query return Infinity and -Infinity as min/max values. So, the filter seems to not really look in the root document.

Maybe the boolean AND filter is not the right choice but with an Ids filter in the boolean, it seems to work : 

``` sh
curl -XPOST "http://localhost:9200/test_nested_statistical/object/_search" -d'{
    "query" : {
                "terms" : {"tags":["one"]}
        },
        "facets" : {
                "priceone" : {
                        "statistical" : {
                                "field" : "prices.price"
                        },
                        "global" : true,
                        "facet_filter" : {
                                "and" : [
                                    {
                                        "nested" : {
                                            "path" : "prices",
                                            "query" : {
                                                    "terms" : {
                                                            "prices.type" : ["TYPEONE"]
                                                    }
                                            },
                                            "join" : false
                                        }
                                    },{
                                        "ids" : {
                                                "type" : "object",
                                                "values" : ["1"]
                                        }
                                    }
                                ]

                        },
                        "nested" : "prices"
                }
        }
}'
```

Here, with the id filtering, min and max are set to 20, the value for the only root document matched by the Ids filter.

Can someone confirm whether this is really a bug or if I'm missing something here?

Thanks for your time.
</description><key id="20670652">3846</key><summary>Root document filtering in nested statistical facets.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JimminiKin</reporter><labels /><created>2013-10-08T10:31:48Z</created><updated>2014-03-13T16:54:32Z</updated><resolved>2014-03-13T16:54:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JimminiKin" created="2013-11-20T09:19:59Z" id="28873161">Follow up to this bug : 

in the example I first gave and for versions 0.90.1 to 0.90.5, setting statistical.field to "price" was working, as of 0.90.6 (also true for 0.90.7), setting statistical.field to the full field path ("prices.price") is necessary.

The initial issue is still there though.
</comment><comment author="Kamapcuc" created="2014-01-27T13:45:18Z" id="33368216">@JimminiKin We encountered with that bug too. We have expensive query, so it's very undesirable to make one more request just for calculating facet values. :( 
+1 vote for fixing that
</comment><comment author="cmenning" created="2014-03-13T15:50:58Z" id="37549441">I'm also running into this. I've got a nested terms facet, and am trying to filter it on the parent document. 
- Without a filter, the nested terms facet returns all nested terms across all parent documents.
- With the filter, the nested terms facet returns no results, even though there are matching documents. 

My filter works properly on non-nested facets.
</comment><comment author="clintongormley" created="2014-03-13T16:54:32Z" id="37557148">With the facets framework, there is no way of fixing this particular issue. It is a question of exposing either the root doc's context or the nested doc's context.

However, with the new aggregations, you can now do what you need. It's a bit verbose but it works:

```
POST /test/object/_search?size=0
{
  "query": {
    "terms": {
      "tags": [
        "one"
      ]
    }
  },
  "aggs": {
    "all_docs": {
      "global": {},
      "aggs": {
        "tags_one": {
          "filter": {
            "term": {
              "tags": "one"
            }
          },
          "aggs": {
            "prices": {
              "nested": {
                "path": "prices"
              },
              "aggs": {
                "type_one": {
                  "filter": {
                    "term": {
                      "prices.type": "TYPEONE"
                    }
                  },
                  "aggs": {
                    "min_max": {
                      "stats": {
                        "field": "prices.price"
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add FieldMappers before adding Mappers (used for parsing) and while holding the relevant's ObjectMapper mutex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3845</link><project id="" key="" /><description>Adding the mappers first could cause the wrong FieldMappers to be used during object parsing

Also:
Removed MergeContext.newFieldMappers, MergeContext.newObjectMappers and relatives as they are not used anymore.
Similarly removed MergeContext.addFieldDataChange &amp; fieldMapperChanges as they were not used.

Closes #3844
</description><key id="20670005">3845</key><summary>Add FieldMappers before adding Mappers (used for parsing) and while holding the relevant's ObjectMapper mutex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-10-08T10:15:57Z</created><updated>2014-07-16T21:52:10Z</updated><resolved>2013-10-08T13:00:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Rare race condition when indexing and merging mapping updates from master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3844</link><project id="" key="" /><description>When concurrently updating the mapping there is a rare racing condition that could cause indexing of fields with the wrong analyzer.
</description><key id="20662539">3844</key><summary>Rare race condition when indexing and merging mapping updates from master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-08T07:19:06Z</created><updated>2013-10-08T16:01:45Z</updated><resolved>2013-10-08T13:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MergeContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateTests.java</file></files><comments><comment>Add FieldMappers before adding Mappers (used for parsing) and while holding the relevant's ObjectMapper mutex</comment></comments></commit></commits></item><item><title>River routing with the attribute node.river does not work for river names containing the river type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3843</link><project id="" key="" /><description>1. configure a node to allow rivers of name or type "dummyOnNodeA"
   
   node.river : dummyOnNodeA
2. create a dummy river named "dummyOnNodeB" of type "dummy"
   
   curl -XPUT localhost:9200/_river/dummyOnNodeB/_meta -d '{
       "type" : "dummy"
   }'
3. restart the node to activate the river changes ( Issue #3840 )
4. see the logs of the node. The river with the name dummyOnNodeB is started. This is not what I would expect.

```
[2013-10-07 19:27:10,944][DEBUG][river.cluster            ] [nodeA] processing [reroute_rivers_node_changed]: execute
[2013-10-07 19:27:10,953][DEBUG][river.cluster            ] [nodeA] cluster state updated, version [1], source [reroute_rivers_node_changed]
[2013-10-07 19:27:10,954][DEBUG][river.cluster            ] [nodeA] processing [reroute_rivers_node_changed]: done applying updated cluster_state
[2013-10-07 19:27:10,955][DEBUG][river                    ] [nodeA] creating river [dummy][dummyOnNodeB]
[2013-10-07 19:27:10,958][INFO ][river.dummy              ] [nodeA] [dummy][dummyOnNodeB] create
[2013-10-07 19:27:10,959][INFO ][river.dummy              ] [nodeA] [dummy][dummyOnNodeB] start
```
1. this could be fixed in the RiverNodeHelper class
</description><key id="20636394">3843</key><summary>River routing with the attribute node.river does not work for river names containing the river type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgaedicke</reporter><labels /><created>2013-10-07T20:20:52Z</created><updated>2014-08-08T17:52:44Z</updated><resolved>2014-08-08T17:52:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:52:44Z" id="51635461">Hi @jgaedicke 

We will be removing rivers in the future, so we're not planning any further work on them. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES 0.90.5 Master losing track of client nodes (Client: "failed to send join request to master" Master: "received a join request for an existing node")</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3842</link><project id="" key="" /><description>Ever since moving to 0.90.x we've been having intermittent issues where clients will suddenly start sending join requests to the master node and the master, thinking the client already exists in the cluster, will not respond. Even more bizarre, during this time the client will usually continue to mostly function properly (responds to query and index requests), but eventually enough client nodes get into this state that the cluster must be restarted. Restarting the individual client node has no effect, only a full cluster restart will solve the problem completely. But only temporarily.

Our setup is 3 data nodes and 1 master node in each of 3 datacenters, for a total of 9 data nodes and 3 master nodes. Besides the upgrade to 0.90 from 0.20, we are also doing the following, possibly related, things:

1) We've been using dynamic mapping to track click counts for search results for certain queries. Basically, we have an object property with the queries as the keys and the click counts as the values (for boosting). This leads to massive mapping updates every few seconds when a new query is added to the mapping. Now that the rescore API is available, we can do the same thing without dynamic mapping. Should be completely moved away from dynamic after today. Will also try to setup a reproducible test case to see if this is the cause of the issue.

2) One particularly massive index (90gb+) with only 3 shards.

```
java -version
Java version "1.7.0_10"
Java(TM) SE Runtime Environment (build 1.7.0_10-b18)
Java HotSpot(TM) 64-Bit Server VM (build 23.6-b04, mixed mode)
```

Logs from master:

```
7037:[2013-10-07 09:57:46,216][INFO ][cluster.service          ] [elasticsearch020.prn1] removed {[elasticsearch003.lla1][MMZlC4qyQ129gA9NnTSr2A][inet[/10.88.27.43:9300]]{dc=lla1, master=false},}, reason: zen-disco-node_left([elasticsearch003.lla1][MMZlC4qyQ129gA9NnTSr2A][inet[/10.88.27.43:9300]]{dc=lla1, master=false})
7284:[2013-10-07 09:58:20,477][INFO ][cluster.service          ] [elasticsearch020.prn1] added {[elasticsearch003.lla1][ynx12931RFOoWNnU7QAEXw][inet[/10.88.27.43:9300]]{dc=lla1, master=false},}, reason: zen-disco-receive(join from node[[elasticsearch003.lla1][ynx12931RFOoWNnU7QAEXw][inet[/10.88.27.43:9300]]{dc=lla1, master=false}])
7285:[2013-10-07 09:58:27,070][WARN ][discovery.zen            ] [elasticsearch020.prn1] received a join request for an existing node [[elasticsearch003.lla1][ynx12931RFOoWNnU7QAEXw][inet[/10.88.27.43:9300]]{dc=lla1, master=false}]
7314:[2013-10-07 09:58:32,916][WARN ][discovery.zen            ] [elasticsearch020.prn1] received a join request for an existing node [[elasticsearch003.lla1][ynx12931RFOoWNnU7QAEXw][inet[/10.88.27.43:9300]]{dc=lla1, master=false}]
```

Logs from client:

```
1:[2013-10-07 09:55:27,844][INFO ][node                     ] [elasticsearch001.lla1] version[0.90.5], pid[21551], build[c8714e8/2013-09-17T12:50:20Z]
2:[2013-10-07 09:55:27,845][INFO ][node                     ] [elasticsearch001.lla1] initializing ...
3:[2013-10-07 09:55:27,896][INFO ][plugins                  ] [elasticsearch001.lla1] loaded [transport-thrift, mapper-attachments], sites [elasticsearch-paramedic]
4:[2013-10-07 09:55:30,122][DEBUG][gateway.local            ] [elasticsearch001.lla1] using initial_shards [quorum], list_timeout [30s]
5:[2013-10-07 09:55:30,288][DEBUG][gateway.local.state.meta ] [elasticsearch001.lla1] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
6:[2013-10-07 09:55:30,288][TRACE][gateway.local.state.shards] [elasticsearch001.lla1] [find_latest_state]: processing [global-32014]
7:[2013-10-07 09:55:30,309][DEBUG][gateway.local.state.shards] [elasticsearch001.lla1] took 21ms to load started shards state
8:[2013-10-07 09:55:30,332][INFO ][node                     ] [elasticsearch001.lla1] initialized
9:[2013-10-07 09:55:30,332][INFO ][node                     ] [elasticsearch001.lla1] starting ...
10:[2013-10-07 09:55:30,352][INFO ][thrift                   ] [elasticsearch001.lla1] bound on port [9500]
11:[2013-10-07 09:55:30,551][INFO ][transport                ] [elasticsearch001.lla1] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.88.17.25:9300]}
12:[2013-10-07 09:55:36,970][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
13:[2013-10-07 09:55:43,179][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
14:[2013-10-07 09:55:49,380][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
15:[2013-10-07 09:55:55,585][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
16:[2013-10-07 09:56:00,556][WARN ][discovery                ] [elasticsearch001.lla1] waited for 30s and no initial state was set by the discovery
17:[2013-10-07 09:56:00,557][INFO ][discovery                ] [elasticsearch001.lla1] xsearch2/P7aHjB88TGS4-ZP4_w9-7A
18:[2013-10-07 09:56:00,586][INFO ][http                     ] [elasticsearch001.lla1] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.88.17.25:9200]}
19:[2013-10-07 09:56:00,586][INFO ][node                     ] [elasticsearch001.lla1] started
20:[2013-10-07 09:56:01,785][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
21:[2013-10-07 09:56:07,983][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
22:[2013-10-07 09:56:14,178][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
23:[2013-10-07 09:56:20,376][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
24:[2013-10-07 09:56:26,574][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
25:[2013-10-07 09:56:32,771][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
26:[2013-10-07 09:56:38,969][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
27:[2013-10-07 09:56:45,169][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
28:[2013-10-07 09:56:51,364][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
29:[2013-10-07 09:56:57,558][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
30:[2013-10-07 09:57:03,753][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
31:[2013-10-07 09:57:09,950][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
32:[2013-10-07 09:57:16,141][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
33:[2013-10-07 09:57:22,336][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
34:[2013-10-07 09:57:28,530][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
35:[2013-10-07 09:57:34,727][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
36:[2013-10-07 09:57:40,923][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
37:[2013-10-07 09:57:47,179][INFO ][discovery.zen            ] [elasticsearch001.lla1] failed to send join request to master [[elasticsearch020.prn1][BHwePHiKRbi8p_mmF0w5og][inet[/10.79.29.71:9300]]{dc=prn1, data=false, master=true}], reason [org.elasticsearch.ElasticSearchTimeoutException: Timeout waiting for task.]
```
</description><key id="20625796">3842</key><summary>ES 0.90.5 Master losing track of client nodes (Client: "failed to send join request to master" Master: "received a join request for an existing node")</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pvulgaris</reporter><labels /><created>2013-10-07T17:22:50Z</created><updated>2014-08-08T17:51:33Z</updated><resolved>2014-08-08T17:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pvulgaris" created="2013-10-11T16:48:53Z" id="26152483">Seems similar to https://github.com/elasticsearch/elasticsearch/issues/2481
</comment><comment author="pvulgaris" created="2013-10-11T16:53:02Z" id="26152791">We've removed the massive index and have stopped abusing dynamic mapping, but the "failed to send join request to master" is still appearing in our client logs, even in newly spun-up nodes. In the past, we've been able to temporarily get rid of the issue by doing a full cluster restart. Before I do that, is there any way for me to dump some global cluster state to help debug this issue?
</comment><comment author="avleen" created="2013-11-13T20:40:50Z" id="28431703">I too have this with Logstash clients. If I restart them fast enough, I end up with this.
</comment><comment author="spinscale" created="2013-11-14T08:23:27Z" id="28466683">Hey,

@pvulgaris 
I just took a look at your setup and I have a couple of questions:
- Do you have a cluster that spans over several data centers? This is not really recommended, unless they are connected to each other very fast and without going over the non-reliability of the internet
- We changed a bit of code in the cluster state event handling with the 0.90.6 release - so it would be great if you could update to 0.90.7, which was released yesterday
- The node you are starting up/shutting down, is that a pure client node?
- Before you spin up the nodes, which fail, can you check the cluster state and see if those nodes are still mentioned in there? Simply check the output of `http://localhost:9200/_cluster/state`

@avleen 
what is your use case for reproducing? You stop/start your logstash clients? What are those? Client nodes as well? Do you initiate a clean stop? What is your setup? Every information helps :-)

Also, is this clearly reproducible for you? So you simply restart a couple of nodes, they try to join the cluster and fail (this is important for me, because maybe we can create a reproducible test case out of all of these information)
</comment><comment author="pvulgaris" created="2013-11-14T18:24:20Z" id="28508903">- We do have a cluster that spans datacenter's on the east coast, west coast and Europe. Our connection is as fast as possible, but we are seeing the majority of the issues coming from the European servers.
- We are still on 0.90.5 so I'll give 0.90.7 a shot today.
- Yes, we now have only pure data and master nodes.
- I'll check the state of a few nodes before I update them today and see if things look consistent.
</comment><comment author="pvulgaris" created="2014-01-03T23:47:08Z" id="31563724">Some followup. Tried having all the master nodes closer geographically (1 west coast, 2 east coast), but had similar issues. The only thing that seems to work is having all the master nodes in the same physical location. We have a backup setup of master nodes in another dc, but this seems less than optimal. I'll try to see if I can come up with a test case that makes this reproducible.
</comment><comment author="clintongormley" created="2014-08-08T17:51:33Z" id="51635312">Latency seems to have been the issue.  If you are still seeing similar problems, please could you open a new ticket.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add century interval in Date Histogram facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3841</link><project id="" key="" /><description>Heya

I would like to have a century interval option in the Date Histogram Facet.

``` javascript
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "histo" : {
            "date_histogram" : {
                "field" : "field_name",
                "interval" : "century"
            }
        }
    }
}
```

Closes #2473.

Cc @uboness: what do you think? Can we add it in 0.90 and master as well? If so, could you add this option as well for the aggregation feature?

BTW JODA does not seem to support decades. That's why I removed this option from the original feature request.
</description><key id="20623968">3841</key><summary>Add century interval in Date Histogram facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-10-07T16:50:39Z</created><updated>2014-07-16T21:52:10Z</updated><resolved>2014-02-03T07:38:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-03T07:38:51Z" id="33929682">Closing. I think we won't support `century` interval in date histogram facet. May be in aggregations?(cc @uboness)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>river instance not created after successful creation of _meta document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3840</link><project id="" key="" /><description>river instance is not created after successfully issuing PUT _meta request 

this problem happens when creating river on completely fresh cluster 
and only for first river (following updates to _meta document for this river still dont start it)
creating second river works fine (but first one is still down)

im using latest elasticsearch sources from master branch
and running it from eclipse (one node cluster, no replication)
to replicate delete all cluster data files, then start single node cluster and create river as first request to the cluster

overall it looks like racing condition, after debugging for while i see that when checking if river cluster state changed (RiverClusterStateUpdateTask), event.state().metaData().index(riverIndexName) is not returning newly created river (probably slowed down by initial index creation)

debug logs:

[2013-10-07 15:35:38,937][DEBUG][cluster.service          ] [Peepers] processing [create-index [_river], cause [auto(index api)]]: execute
[2013-10-07 15:35:38,950][DEBUG][indices                  ] [Peepers] creating Index [_river], shards [1]/[0]
[2013-10-07 15:35:43,713][DEBUG][index.mapper             ] [Peepers] [_river] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[file:/mnt/data/projects/git/elasticsearch/elasticsearch/target/classes/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]
[2013-10-07 15:35:43,722][DEBUG][index.cache.query.parser.resident] [Peepers] [_river] using [resident] query cache with max_size [100], expire [null]
[2013-10-07 15:35:43,965][DEBUG][index.store.fs           ] [Peepers] [_river] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]
[2013-10-07 15:35:45,458][INFO ][cluster.metadata         ] [Peepers] [_river] creating index, cause [auto(index api)], shards [1]/[0], mappings []
[2013-10-07 15:35:45,560][DEBUG][index.cache.filter.weighted] [Peepers] [_river] full cache clear, reason [close]
[2013-10-07 15:35:45,570][DEBUG][cluster.service          ] [Peepers] cluster state updated, version [3], source [create-index [_river], cause [auto(index api)]]
[2013-10-07 15:35:45,572][DEBUG][cluster.service          ] [Peepers] publishing cluster state version 3
[2013-10-07 15:35:45,574][DEBUG][cluster.service          ] [Peepers] set local cluster state to version 3
[2013-10-07 15:35:45,579][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: execute
[2013-10-07 15:35:45,584][DEBUG][indices.cluster          ] [Peepers] [_river] creating index
[2013-10-07 15:35:45,587][DEBUG][indices                  ] [Peepers] creating Index [_river], shards [1]/[0]
[2013-10-07 15:35:45,615][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-10-07 15:35:47,464][DEBUG][index.mapper             ] [Peepers] [_river] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[file:/mnt/data/projects/git/elasticsearch/elasticsearch/target/classes/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]
[2013-10-07 15:35:47,472][DEBUG][index.cache.query.parser.resident] [Peepers] [_river] using [resident] query cache with max_size [100], expire [null]
[2013-10-07 15:35:47,519][DEBUG][index.store.fs           ] [Peepers] [_river] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]
[2013-10-07 15:35:47,593][DEBUG][indices.cluster          ] [Peepers] [_river][0] creating shard
[2013-10-07 15:35:47,595][DEBUG][index.service            ] [Peepers] [_river] creating shard_id [0]
[2013-10-07 15:35:49,258][DEBUG][index.deletionpolicy     ] [Peepers] [_river][0] Using [keep_only_last] deletion policy
[2013-10-07 15:35:49,287][DEBUG][index.merge.policy       ] [Peepers] [_river][0] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]
[2013-10-07 15:35:49,293][DEBUG][index.merge.scheduler    ] [Peepers] [_river][0] using [concurrent] merge scheduler with max_thread_count[1]
[2013-10-07 15:35:49,377][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [CREATED]
[2013-10-07 15:35:49,389][DEBUG][index.translog           ] [Peepers] [_river][0] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]
[2013-10-07 15:35:49,467][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [CREATED]-&gt;[RECOVERING], reason [from gateway]
[2013-10-07 15:35:49,474][DEBUG][index.gateway            ] [Peepers] [_river][0] starting recovery from local ...
[2013-10-07 15:35:49,620][DEBUG][index.engine.robin       ] [Peepers] [_river][0] starting engine
[2013-10-07 15:35:49,708][DEBUG][cluster.service          ] [Peepers] processing [create-index [_river], cause [auto(index api)]]: done applying updated cluster_state (version: 3)
[2013-10-07 15:35:50,875][DEBUG][index.shard.service      ] [Peepers] [_river][0] scheduling refresher every 1s
[2013-10-07 15:35:50,882][DEBUG][index.shard.service      ] [Peepers] [_river][0] scheduling optimizer / merger every 1s
[2013-10-07 15:35:50,884][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [RECOVERING]-&gt;[POST_RECOVERY], reason [post recovery from gateway, no translog]
[2013-10-07 15:35:50,888][DEBUG][index.gateway            ] [Peepers] [_river][0] recovery completed from [local], took [1.4s]
[2013-10-07 15:35:50,890][DEBUG][cluster.action.shard     ] [Peepers] sending shard started for [_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING], indexUUID [hhVydPb3QvuD110I9V4XyA], reason [after recovery from gateway]
[2013-10-07 15:35:50,892][DEBUG][cluster.action.shard     ] [Peepers] received shard started for [_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING], indexUUID [hhVydPb3QvuD110I9V4XyA], reason [after recovery from gateway]
[2013-10-07 15:35:50,900][DEBUG][cluster.service          ] [Peepers] processing [shard-started ([_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute
[2013-10-07 15:35:50,905][DEBUG][cluster.action.shard     ] [Peepers] [_river][0] will apply shard started [_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING], indexUUID [hhVydPb3QvuD110I9V4XyA], reason [after recovery from gateway]
[2013-10-07 15:35:50,915][DEBUG][cluster.service          ] [Peepers] cluster state updated, version [4], source [shard-started ([_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING]), reason [after recovery from gateway]]
[2013-10-07 15:35:50,917][DEBUG][cluster.service          ] [Peepers] publishing cluster state version 4
[2013-10-07 15:35:50,920][DEBUG][cluster.service          ] [Peepers] set local cluster state to version 4
[2013-10-07 15:35:50,924][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: execute
[2013-10-07 15:35:50,936][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-10-07 15:35:50,937][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [POST_RECOVERY]-&gt;[STARTED], reason [global state moved to started]
[2013-10-07 15:35:51,155][DEBUG][cluster.service          ] [Peepers] processing [shard-started ([_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING]), reason [after recovery from gateway]]: done applying updated cluster_state (version: 4)
[2013-10-07 15:35:51,762][DEBUG][cluster.service          ] [Peepers] processing [update-mapping [_river][test]]: execute
[2013-10-07 15:35:51,962][DEBUG][cluster.metadata         ] [Peepers] [_river] update_mapping [test](dynamic) with source [{"test":{"index_analyzer":"default_index","search_analyzer":"default_search","properties":{"type":{"type":"string"}}}}]
[2013-10-07 15:35:52,022][DEBUG][cluster.service          ] [Peepers] cluster state updated, version [5], source [update-mapping [_river][test]]
[2013-10-07 15:35:52,032][DEBUG][cluster.service          ] [Peepers] publishing cluster state version 5
[2013-10-07 15:35:52,042][DEBUG][cluster.service          ] [Peepers] set local cluster state to version 5
[2013-10-07 15:35:52,055][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: execute
[2013-10-07 15:35:52,233][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-10-07 15:35:52,382][DEBUG][cluster.service          ] [Peepers] processing [update-mapping [_river][test]]: done applying updated cluster_state (version: 5)
</description><key id="20615956">3840</key><summary>river instance not created after successful creation of _meta document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta2</label></labels><created>2013-10-07T14:46:57Z</created><updated>2013-11-10T20:05:48Z</updated><resolved>2013-11-10T20:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-10T20:05:28Z" id="28159129">I pushed a fix for this, as I said in #4089 . Could you confirm it solves the issue?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/cluster/RiverClusterService.java</file><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file></files><comments><comment>Schedule retry if the river type is available but the _meta document isn't</comment></comments></commit></commits></item><item><title>http://example.com/ba*,-bar,-baz/_search will search over index foo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3839</link><project id="" key="" /><description>Create three indexes: foo, bar, and baz.
Submit a search request using the URL: http://example.com/ba*,-bar,-baz/_search. This performs a search over all indices.

The logic of the MetaData.convertFromWildcards() method is as follows:
First token is ba\* - expand that into bar and baz and add them to the result set.
Second token is a negation of bar so it removes that.
Third token is a negation of baz so it removes that to leave an empty set.

The problem is that an empty index set is also generated by http://example.com/_search.

There is later logic in PlainOperationRouting.computeTargetedShards() that treats the empty set as a request to search every index, including the index foo and the two indices that have been explicitly removed.

The proposed solution is:
1. RestSearchAction.parseSearchRequest() to detect when request.param("index") is null and to explicitly populate the indices array with "_all". Both /_all/_search and /_search then follow the exact same logical path - this may avoid future bugs.
2. PlainOperationRouting.computeTargetedShards() should treat an empty concreteIndices array as a null search.
</description><key id="20613347">3839</key><summary>http://example.com/ba*,-bar,-baz/_search will search over index foo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ccw-morris</reporter><labels><label>:Index APIs</label><label>bug</label><label>low hanging fruit</label></labels><created>2013-10-07T14:10:27Z</created><updated>2016-12-07T19:50:14Z</updated><resolved>2016-09-12T12:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ccw-morris" created="2013-10-09T14:20:15Z" id="25974696">There's a related issue.

I'm looking at implementing an authorization model by appending a list of indexes or aliases that the user shouldn't see to the user request. So, if the user requests an index that they shouldn't then the logic strips it out again. Alternate solutions are welcome.

Within that context, a request for http://example.com/foo,-foo/_search makes sense. Unfortunately, this doesn't work.

When MetaData.convertFromWildcards() encounters a -foo, it will either:
a) remove it from the list of fields iff a previous term has caused an expansion
b) add all other fields iff it is the first term
c) add it as is, including the minus sign

So, foo,-foo returns an exception saying that it can't find an index called -foo.

Instead, I would propose that a non-first minus term will populate the results set with all previous entries encountered. This logic is also used when processing a wildcard expression to resolve the expression "foo,ba*".
</comment><comment author="clintongormley" created="2014-08-08T17:49:41Z" id="51635091">The first issue has been fixed, the issue from the second comment is still valid:

```
GET  /foo,-foo/_search
```
</comment><comment author="clintongormley" created="2015-09-21T18:52:05Z" id="142074256">And, this searches all indices including bar and baz:

```
GET *,-bar,-baz/_search
```

While this search all indices except bar and baz:

```
GET -bar,-baz/_search
```
</comment><comment author="qwerty4030" created="2016-08-26T22:43:25Z" id="242869321">I was able to reproduce this on latest master with the following:

```
PUT foo/foo-type/foo-doc
{}
PUT bar/bar-type/bar-doc
{}
PUT baz/baz-type/baz-doc
{}
GET  foo,-foo/_search

response:
{
  "error": {
    "root_cause": [
      {
        "type": "index_not_found_exception",
        "reason": "no such index",
        "resource.type": "index_expression",
        "resource.id": "-foo",
        "index_uuid": "_na_",
        "index": "-foo"
      }
    ],
    "type": "index_not_found_exception",
    "reason": "no such index",
    "resource.type": "index_expression",
    "resource.id": "-foo",
    "index_uuid": "_na_",
    "index": "-foo"
  },
  "status": 404
}
```

Took a quick look at the code... l'll make a PR shortly with the fix.
Unable to reproduce bug with `GET *,-bar,-baz/_search` and `GET -bar,-baz/_search`.

@clintongormley While I was investigating this I noticed the following request resulted in a similar error:

```
GET  foo,+foo/_search

response:
{
  "error": {
    "root_cause": [
      {
        "type": "index_not_found_exception",
        "reason": "no such index",
        "resource.type": "index_or_alias",
        "resource.id": " foo",
        "index_uuid": "_na_",
        "index": " foo"
      }
    ],
    "type": "index_not_found_exception",
    "reason": "no such index",
    "resource.type": "index_or_alias",
    "resource.id": " foo",
    "index_uuid": "_na_",
    "index": " foo"
  },
  "status": 404
}
```

However this error is slightly different than with `GET  foo,-foo/_search`: it has `"resource.type": "index_or_alias"` instead of `"resource.type": "index_expression"`. Also notice `"index": " foo"` has a space before `foo`. I guessed this might be something to do with URL encoding and tried the following:

```
GET  foo,%2Bfoo/_search

response:
{
  "error": {
    "root_cause": [
      {
        "type": "index_not_found_exception",
        "reason": "no such index",
        "resource.type": "index_expression",
        "resource.id": "+foo",
        "index_uuid": "_na_",
        "index": "+foo"
      }
    ],
    "type": "index_not_found_exception",
    "reason": "no such index",
    "resource.type": "index_expression",
    "resource.id": "+foo",
    "index_uuid": "_na_",
    "index": "+foo"
  },
  "status": 404
}
```

This is the same error as with `GET  foo,-foo/_search` so that confirmed my suspicion. Is this a bug with ES or kibana/sense? Got the same behavior when I tried this with curl and an ES 2.3 cluster.
</comment><comment author="qwerty4030" created="2016-08-26T22:51:39Z" id="242870656">Would the indices be resolved differently if the `+` was removed? If not what is the purpose of it?
`GET  foo,+foo/_search` vs `GET  foo,foo/_search`
`GET  foo,+ba*/_search` vs `GET  foo,ba*/_search`
`GET  +foo/_search` vs `GET  foo/_search`
`GET  ba*,+f*,-foo/_search` vs `GET  ba*,f*,-foo/_search`

I tried removing all the `+`s from `WildcardExpressionResolverTests` and the tests still pass so I guess its not needed?
</comment><comment author="gerits" created="2016-12-07T19:13:39Z" id="265542999">There still seems to be a problem here. When I run the following query it won't work. But if I switch the position of the indices it works. 
non working: `GET /-foo,*/_search`
working: `GET /*,-foo/_search`

It seems the - sign is not interpreted correctly as in following example.
Not working: `GET /-foo,+bar/_search`
working: `GET /-*foo,+bar/_search`

This is for version 5.0.2</comment><comment author="javanna" created="2016-12-07T19:50:14Z" id="265553697">Hi @gerits yes that is correct. It's a feature, not a bug, see #20898. The negation has an effect only if there is a preceding wildcard expression. The implicit negation that we had before led to unexpected results, especially when e.g. deleting indices.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java</file><file>core/src/test/java/org/elasticsearch/cluster/metadata/WildcardExpressionResolverTests.java</file></files><comments><comment>Fix IndexNotFoundException if an multi index search request had a concrete index followed by an add/remove concrete index.</comment><comment>The code now properly adds/removes the index instead of throwing an exception.</comment></comments></commit></commits></item><item><title>Better logic ignoring filters when parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3838</link><project id="" key="" /><description>When parsing a filter, we effectively have a case where we need to ignore a filter. For example, when an "and" filter has no elements. Ignoring a filter is different compared to matching on all or matching on none, because an and filter with no elements should simply be ignored in the context of a bool filter for example, regardless if its used within a `must` or a `must_not` filter.

We should be consistent in our codebase when handling these use case. See also #3809 
</description><key id="20599864">3838</key><summary>Better logic ignoring filters when parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-07T09:07:37Z</created><updated>2013-10-22T22:38:56Z</updated><resolved>2013-10-07T09:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedFilter.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/terms/IndicesTermsFilterCache.java</file><file>src/main/java/org/elasticsearch/search/facet/filter/FilterFacetParser.java</file><file>src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java</file><file>src/main/java/org/elasticsearch/search/query/FilterParseElement.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Better logic ignoring filters when parsing</comment><comment>When parsing a filter, we effectively have a case where we need to ignore a filter. For example, when an "and" filter has no elements. Ignoring a filter is different compared to matching on all or matching on none, because an and filter with no elements should simply be ignored in the context of a bool filter for example, regardless if its used within a must or a must_not filter.</comment></comments></commit></commits></item><item><title>Remove @Required annotation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3837</link><project id="" key="" /><description>The @Required annotation is used for documentation purposes only anyway, so removing it might make sense, as it is used for methods only. If you dont require a method parameter it might jsut make sense to add another method anyway.
</description><key id="20598680">3837</key><summary>Remove @Required annotation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-10-07T08:38:40Z</created><updated>2014-07-16T21:52:10Z</updated><resolved>2013-10-07T15:27:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-07T09:59:13Z" id="25796779">+1

On 7 October 2013 10:38, Alexander Reelsen notifications@github.com wrote:

&gt; The @Required https://github.com/Required annotation is used for
&gt; documentation purposes only anyway, so removing it might make sense, as it
&gt; is used for methods only. If you dont require a method parameter it might
&gt; 
&gt; ## jsut make sense to add another method anyway.
&gt; 
&gt; You can merge this Pull Request by running
&gt; 
&gt;   git pull https://github.com/spinscale/elasticsearch remove-required-annotation
&gt; 
&gt; Or view, comment on, or merge it at:
&gt; 
&gt;   https://github.com/elasticsearch/elasticsearch/pull/3837
&gt; Commit Summary
&gt; - Remove @Required annotation
&gt; 
&gt; File Changes
&gt; - _M_
&gt;   src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-0(5)
&gt; - _M_
&gt;   src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-1(2)
&gt; - _M_
&gt;   src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-2(8)
&gt; - _M_ src/main/java/org/elasticsearch/action/count/CountRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-3(8)
&gt; - _M_ src/main/java/org/elasticsearch/action/delete/DeleteRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-4(3)
&gt; - _M_
&gt;   src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-5(7)
&gt; - _M_ src/main/java/org/elasticsearch/action/get/GetRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-6(2)
&gt; - _M_ src/main/java/org/elasticsearch/action/index/IndexRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-7(13)
&gt; - _M_
&gt;   src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-8(3)
&gt; - _D_ src/main/java/org/elasticsearch/common/Required.javahttps://github.com/elasticsearch/elasticsearch/pull/3837/files#diff-9(35)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elasticsearch/elasticsearch/pull/3837.patch
&gt; - https://github.com/elasticsearch/elasticsearch/pull/3837.diff

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="s1monw" created="2013-10-07T10:53:32Z" id="25799536">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in facet filter parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3836</link><project id="" key="" /><description>ES version 0.90.5, class `FacetParseElement`, method `parse`, line #86:

```
filter = context.queryParserService().parseInnerFilter(parser).filter();
```

Method `IndexQueryParserService.parseInnerFilter` is annotated as @Nullable and in fact it does return `null` in certain cases and it results in NPE in the aforementioned `FacetParseElement.parse` method.

Examples of query that throws NPE:

1) Empty 'facet_filter':

```
{
  "facets": {
    "foo": {
      "filter": {
        "match_all": {}
      },
      "facet_filter": {}
    }
  }
}
```

2) Empty 'filters' array in 'and' filter:

```
{
  "facets": {
    "foo": {
      "filter": {
        "match_all": {}
      },
      "facet_filter": {
        "and": {
          "filters": []
        }
      }
    }
  }
}
```

While syntactically or semantically those queries might not be correct ElasticSearch should return a meaningful error rather than NPE.
</description><key id="20579035">3836</key><summary>NPE in facet filter parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">wajda</reporter><labels /><created>2013-10-06T12:12:34Z</created><updated>2013-12-02T09:02:54Z</updated><resolved>2013-12-02T09:02:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-07T10:13:45Z" id="25797514">Hey,

I think this is already fixed in the master and 0.90 branch. Can you maybe retest and report if it works for you?
I tested your above examples, and they are returning without an exception...
</comment><comment author="spinscale" created="2013-12-02T09:02:54Z" id="29602726">Closing this. Looks fixed in master (and current 0.90 release). Please reopen if you spot this again.

Thanks a lot for reporting!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java Client TransportSerializationException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3835</link><project id="" key="" /><description>Greetings, I've been having trouble with the latest releases of elasticsearch and its java client.
I'm getting this exception for various things I'm trying to do:

&lt;code&gt;
[elasticsearch[Shrunken Bones][transport_client_worker][T#4]{New I/O worker #4}] WARN  org.elasticsearch.transport.netty - [Shrunken Bones] Message not fully read (response) for [1] handler org.elasticsearch.action.TransportActionNodeProxy$1@15eda628, error [true], resetting
Exception in thread "main" org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.StreamCorruptedException: unexpected end of block data
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1369)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:498)
    at java.net.InetSocketAddress.readObject(InetSocketAddress.java:187)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1866)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)
    ... 23 more
&lt;/code&gt;

Here are some notes:
1. I'm using the same jvm for the client and elasticsearch. Tested with java7_40, java7_26 and java6_something
2. I'm using matched versions of elasticsearch and its java client
3. I'm getting this exception for all versions of elasticsearch after 0.90.0.RC1 (yep, tested them all)
4. I'm using out-of-the-box configuration

Here's some example code that I'm running when I get the errors:
&lt;code&gt;
getClient().admin().indices().prepareDeleteMapping(INDEX_TAGS).setType(TYPE_TAG).execute().actionGet();
&lt;/code&gt;

Do you think it's an issue or am I messing up something?
</description><key id="20569828">3835</key><summary>Java Client TransportSerializationException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">phrone</reporter><labels /><created>2013-10-05T19:47:21Z</created><updated>2013-10-06T21:29:30Z</updated><resolved>2013-10-06T21:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kirubar" created="2013-10-06T08:31:10Z" id="25764363">How many nodes there in the cluster? check all nodes has same java version. it mostly happened when the java version differ  in nodes or client. 
check this link : http://jontai.me/blog/2013/06/elasticsearch-remotetransportexception-failed-to-deserialize-exception-response-from-stream/
</comment><comment author="phrone" created="2013-10-06T10:09:50Z" id="25765544">Hi, thank you for replying! 
It's just one node and I'm connecting to it with a TransportClient. The server is out-of-the-box with nothing changed in the configuration. 
I forgot to mention that I'm on Windows (but I tested in ubuntu as well), but that's probably irrelevant, since it's jvm.
It seems that I'm getting this only when some exception gets thrown and it fails to serialize/deserialize it. Other than that, the client is functioning, but still, it's a nasty problem to have.

Any ideas?
</comment><comment author="spinscale" created="2013-10-06T19:35:23Z" id="25774918">Hey,

can you ensure/check that the JVM versions of elasticsearch and the application creating the TransportClient are the same as well in their minor versions? Might be that one is using the JVM shipped/differently configured with your IDE, while the other one is using the one configured in JAVA_HOME. You can see the elasticsearch version in the node infos, by accessing `http://localhost:9200/_nodes?jvm=true`

You can easily ensure the above, by creating an embedded elasticsearch instance in the java application you create the `TransportClient` in like this

```
Node node = NodeBuilder.nodeBuilder().node().start()
```

Apart from that, you seem to be pretty sure, that you use the same, so I guess your code sample is pretty small and you can put it up somewhere, and we can try to reproduce it? Seems really strange to me, if all versions are the same (JVM the same, elasticsearch the same, etc).

Does this happen with an empty elasticsearch (as in no data) cluster as well?
</comment><comment author="phrone" created="2013-10-06T21:29:30Z" id="25777236">Oh, god, this is so embarrassing.. 
I got mislead by having the desired version of jdk checked in Eclipse's installed jre's and thinking this forces the IDE to use it when launching apps. Alas, it seems that this is only for specifying what jre to use when creating new projects in eclipse..
Anyway, I deleted all the other jres, tried again and I no longer get the TransportSerializationException. 
I would never have found out what the problem was without your suggestions, spinscale, thanks!

All right, this can be closed, it's not a bug. Sorry about the false alarm!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't start up elasticsearch on install as recommended by fedora.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3834</link><project id="" key="" /><description>This commit fixes #3722

There may be some issues with the systemctl section, let me know if you require more changes
</description><key id="20563905">3834</key><summary>Don't start up elasticsearch on install as recommended by fedora.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dipthegeezer</reporter><labels /><created>2013-10-05T12:21:26Z</created><updated>2014-07-16T21:52:11Z</updated><resolved>2014-04-17T14:47:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete warmer api to support acknowledgements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3833</link><project id="" key="" /><description>The delete warmer api always returns `"acknowledged":true`, as it doesn't support any acknowledgement mechanism when it comes to updating the cluster state. It currently returns after the master has applied the change locally, which doesn't necessarily mean that all the other nodes already know about the new cluster state version.

We then need to add support for acknowledgements from the other nodes and the timeout parameter, which refers to the maximum wait for acknowledgements (default 10s as in the other apis) in a backward compatible manner.
</description><key id="20553732">3833</key><summary>Delete warmer api to support acknowledgements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-04T22:37:15Z</created><updated>2013-10-15T15:48:57Z</updated><resolved>2013-10-15T15:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/delete/RestDeleteWarmerAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/LocalGatewayIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file></files><comments><comment>Delete warmer api to support acknowledgements</comment></comments></commit></commits></item><item><title>Deleting documents while scrolling deletes everything</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3832</link><project id="" key="" /><description>I have found rather a nasty bug (?) that I am not sure if it is due to my fault or ES. The code is intertwined with a lot of my own code so I am going to paste snippets. 
### What's happening?

When deleting **only** some documents while scrolling through a search query, then instead **all** the documents are deleted. 

This is how I delete:

``` java
client().prepareDeleteByQuery().setQuery(QueryBuilders.fieldQuery("_id", id))
```

Note: My routing is `uri` field so I have to delete by query. 
### More Details

This is how I do my search.

``` java
SearchResponse searchResponse = client()
                .prepareSearch(indicesForInterval(...))
                .setSize(2000)
                .setSearchType(SearchType.SCAN)
                .setScroll(new TimeValue(5, TimeUnit.MINUTES))
                .setFilter(...)
                .execute().actionGet();
```

**However**, if I use `termQuery` instead of `fieldQuery` then the correct behavior happens. 

I can provide more detail if needed. But I have over 300 lines of code that has all this stuff baked in to my own classes. 

Is this a bug? Is it supposed to delete all documents when I am doing a scroll?
</description><key id="20553132">3832</key><summary>Deleting documents while scrolling deletes everything</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">amir20</reporter><labels><label>non-issue</label></labels><created>2013-10-04T22:20:50Z</created><updated>2013-10-09T22:40:15Z</updated><resolved>2013-10-09T22:28:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-05T15:38:14Z" id="25750736">hey, can you provide a gist that shows you behaviour? I can't really reproduce anything like that
</comment><comment author="amir20" created="2013-10-05T17:41:17Z" id="25753122">Yes I can. I'll have to create by isolating my own code. Will get it to you soon.
</comment><comment author="s1monw" created="2013-10-06T07:49:17Z" id="25763913">@amir20 can you as a starter provide some insight about the `id` you deleting. Can I see the actual ID value?
</comment><comment author="amir20" created="2013-10-06T19:20:40Z" id="25774592">@s1monw The id is the auto generated id that elasticsearch provides. It looks something like `DuyfmWfHTUWSDdPK-udsHw`. I am working on the sample file right now. 
</comment><comment author="amir20" created="2013-10-06T20:04:34Z" id="25775497">@s1monw I was able to successfully reproduce this. I stripped all my code and left only Elasticsearch. Here is the gist:

https://gist.github.com/amir20/6858428

I would love to actually debug myself and find the bug to fix. But I don't have the slightest clue where to start. Let me know what else I can I do to help. 

/cc @kimchy 
</comment><comment author="kimchy" created="2013-10-07T07:49:22Z" id="25790155">Just as  a side note, you can delete (by id) while using a custom routing (which will be much more optimized), just set the routing on the delete request.
</comment><comment author="amir20" created="2013-10-07T17:08:25Z" id="25826547">@kimchy Yes, the problem was I didn't have the routing field at the time of deletion so I did not have a better choice. 
</comment><comment author="amir20" created="2013-10-09T21:32:05Z" id="26010782">@s1monw Any update on this? Were you able to reproduce it? It's not really blocking me but I just wanted to know that it is at least reproducible. 
</comment><comment author="s1monw" created="2013-10-09T22:28:25Z" id="26014639">yes I can reproduce it and I know what is going on. The reason is that the ids are auto generated can contain chars that are treated as operators if you use `FieldQuery` for instance `-T5xWVST0incNM6xqs1kQ` that will be parsed as a `NOT` query due to the leading `-` causing everything to be deleted that doesn't contain the reminder of the ID (essentially all documents). It's just very dangerous to use the FieldQuery here unless you are escaping the syntax like I did here: https://gist.github.com/s1monw/6909562 

I hope that makes sense, I will close the issues since IMO everything works as expected.
</comment><comment author="amir20" created="2013-10-09T22:36:25Z" id="26015131">Ahhh that makes sense. So I did the right thing by using `termQuery`. That's good to know. Thanks for looking in to it nonetheless. 
</comment><comment author="s1monw" created="2013-10-09T22:40:02Z" id="26015341">thanks for brining it up! it's important! And yes, `TermQuery` is the right thing to do here.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Put warmer api to support acknowledgements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3831</link><project id="" key="" /><description>The put warmer api always returns `"acknowledged":true`, as it doesn't support any acknowledgement mechanism when it comes to updating the cluster state. It currently returns after the master has applied the change locally, which doesn't necessarily mean that all the other nodes already know about the new cluster state version.

We then need to add support for acknowledgements from the other nodes and the `timeout` parameter, which refers to the maximum wait for acknowledgements (default 10s as in the other apis) in a backward compatible manner. 
</description><key id="20552370">3831</key><summary>Put warmer api to support acknowledgements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-04T22:01:34Z</created><updated>2013-10-15T15:48:57Z</updated><resolved>2013-10-15T15:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/LocalGatewayIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file></files><comments><comment>Put warmer api to support acknowledgements</comment></comments></commit></commits></item><item><title>Improved warm-up of new segments.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3830</link><project id="" key="" /><description>- Merged segments are now warmed-up at the end of the merge operation instead
  of _refresh, so that _refresh doesn't pay the price for the warm-up of merged
  segments, which is often higher than flushed segments because of their size.
- Even when no _warmer is registered, some basic warm-up of the segments is
  performed: norms, doc values (_version). This should help a bit people who
  forget to register warmers.
- Eager loading support for field data: when one can't predict what terms will
  be present in the index, it is tempting to use a match_all query in a warmer,
  but in that case, query execution might not be much faster than field data
  loading so having a warmer that only loads field data without running a query
  can be useful.
</description><key id="20549622">3830</key><summary>Improved warm-up of new segments.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-10-04T21:01:23Z</created><updated>2014-07-16T21:52:11Z</updated><resolved>2013-10-23T10:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-04T21:41:02Z" id="25731969">+1 Looks good!
</comment><comment author="jpountz" created="2013-10-05T12:17:47Z" id="25747434">@martijnvg I added a new commit to address your concerns. In particular the id cache is now loaded too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Doc values integration.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3829</link><project id="" key="" /><description>This commit allows for using Lucene doc values as a backend for field data,
moving the cost of building field data from the refresh operation to indexing.
In addition, Lucene doc values can be stored on disk (partially, or even
entirely), so that memory management is done at the operating system level
(file-system cache) instead of the JVM, avoiding long pauses during major
collections due to large heaps.

So far doc values are supported on numeric types and non-analyzed strings
(index:no or index:not_analyzed). Under the hood, it uses SORTED_SET doc values
which is the only type to support multi-valued fields. Since the field data API
set is a bit wider than the doc values API set, some operations are not
supported:
- field data filtering: this will fail if doc values are enabled,
- field data cache clearing, even for memory-based doc values formats,
- getting the memory usage for a specific field,
- knowing whether a field is actually multi-valued.

This commit also allows for configuring doc-values formats on a per-field basis
similarly to postings formats. In particular the doc values format of the
_version field can be configured through its own field mapper (it used to be
handled in UidFieldMapper previously).

Closes #3806
</description><key id="20531340">3829</key><summary>Doc values integration.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-10-04T15:11:19Z</created><updated>2014-06-12T14:38:07Z</updated><resolved>2013-12-20T09:01:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Support year units in date math expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3828</link><project id="" key="" /><description>According to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html, the date math expressions support M (month), w (week), h (hour), m (minute), and s (second) units. Why years are not supported? Please add support for year units.
</description><key id="20471360">3828</key><summary>Support year units in date math expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">apidruchny</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-03T17:42:41Z</created><updated>2013-10-30T20:44:32Z</updated><resolved>2013-10-11T08:16:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-03T18:04:50Z" id="25643676">Agree. See also #2473.
</comment><comment author="dadoonet" created="2013-10-07T16:08:41Z" id="25821687">The more I think about it the more I can't find a right way to support it. What duration in milliseconds should correspond to a year?

`365 * 24 * 60 * 60 * 1000` ?

What about leap years?

As there is no way to have an exact value, I don't think we should support it.

That said, you can define a year using `365d` or `366d`.
</comment><comment author="apidruchny" created="2013-10-07T17:31:27Z" id="25828249">David, I am looking at the commit that added the date math support originally: f997315f54ec7bf8b158e051a68a5580b5a023d8
There is no need to know the duration of the year in milliseconds, because org.joda.time.MutableDateTime methods are used for other units, for example, addMonths(). This class also support years, it has methods like year() and addYears(). Am I missing something?
</comment><comment author="dadoonet" created="2013-10-07T21:44:14Z" id="25847998">I think I was wrong here. I was mixing date histogram facet concern and your original concern.
</comment><comment author="apidruchny" created="2013-10-29T20:57:28Z" id="27342694">Thanks all. I am looking forward to release 0.90.6 to be able to use this feature.
</comment><comment author="apidruchny" created="2013-10-30T20:44:32Z" id="27437312">By the way, the day unit is supported. The documentation should be updated. The sentence:
`The units supported are y (year), M (month), w (week), h (hour), m (minute), and s (second).` should be `The units supported are y (year), M (month), w (week), d (day), h (hour), m (minute), and s (second).`
The next paragraph in the documentation even shows samples of rounding by day.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file></files><comments><comment>Support year units in date math expressions</comment><comment>According to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html, the date math expressions support M (month), w (week), h (hour), m (minute), and s (second) units. Why years are not supported? Please add support for year units.</comment></comments></commit></commits></item><item><title>Broken link on website</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3827</link><project id="" key="" /><description>Where it says "New to Elasticsearch? Setting up Elasticsearch" on http://www.elasticsearch.org/resources/ that link is broken
</description><key id="20438300">3827</key><summary>Broken link on website</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels /><created>2013-10-03T05:52:22Z</created><updated>2013-10-09T12:09:30Z</updated><resolved>2013-10-09T12:09:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-09T12:09:30Z" id="25965862">Thanks for pointing that out! Should be ok now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot/Restore API - Phase I</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3826</link><project id="" key="" /><description>## Snapshot And Restore

The snapshot and restore module will allow to create snapshots of individual indices or an entire cluster into a remote repository and restore these indices back to the same or a different cluster afterwards. The phase I will only support shared file system repository and S3 repository.
### Repositories

Before any snapshot or restore operation can be performed a snapshot repository should be registered in Elasticsearch. The following command registers a shared file system repository with the name `my_backup` that will use location `/mount/backups/my_backup` to store snapshots.

```
$ curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{
    "type": "fs",
    "settings": {
        "location": "/mount/backups/my_backup",
        "compress": true
    }
}'
```

Once repository is registered, its information can be obtained using the following command:

```
$ curl -XGET 'http://localhost:9200/_snapshot/my_backup?pretty'
```

```
{
  "my_backup" : {
    "type" : "fs",
    "settings" : {
      "compress" : "false",
      "location" : "/mount/backups/my_backup"
    }
  }
}
```

If a repository name is not specified, or `_all` is used as repository name Elasticsearch will return information about all repositories currently registered in the cluster:

```
$ curl -XGET 'http://localhost:9200/_snapshot'
```

or

```
$ curl -XGET 'http://localhost:9200/_snapshot/all'
```
##### Shared File System Repository

The shared file system repository (`"type": "fs"`) is using shared file system to store snapshot. The path specified in the `location` parameter should point to the same location in the shared filesystem and be accessible on all data and master nodes. The following settings are supported:

`location` - Location of the snapshots. Mandatory
`compress` - Turns on compression of the snapshot files. Defaults to `true
`concurrent_streams`- Throttles the number of streams (per node) preforming snapshot operation. Defaults to 5
`chunk_size` - Big files can be broken down into chunks during snapshotting if needed. Defaults to unlimited.
### Snapshot

A repository can contain multiple snapshots of the same cluster. Snapshot are identified by unique names within the cluster. A snapshot with the name `snapshot_1` in the repository `my_backup` can be created by executing the following command:

```
$ curl -XPUT "localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true"
```

The `wait_for_completion` parameter specifies whether or not the request should return immediately or wait for snapshot completion. By default snapshot of all open and started indices in the cluster is created. This behavior can be changed by specifying the list of indices in the body of the snapshot request.

```
$ curl -XPUT "localhost:9200/_snapshot/my_backup/snapshot_1" -d '{
    "indices": "index_1,index_2",
    "ignore_indices": "missing"
}'
```

The list of indices that should be included into the snapshot can be specified using the `indices` parameter that supports multi index syntax. The snapshot request also supports the `ignore_indices` option. Setting it to `missing` will cause indices that do not exists to be ignored during snapshot creation. By default, when `ignore_indices` option is not set and an index is missing the snapshot request will fail.

The index snapshot process is incremental. In the process of making the index snapshot Elasticsearch analyses the list of the index files that are already stored in the repository and copies only files that were created or changed since the last snapshot. That allows multiple snapshots to be preserved in the repository in a compact form. Snapshotting process is executed in non-blocking fashion. All indexing and searching operation can continue to be executed against the index that is being snapshotted. However, a snapshot represents the point-in-time view of the index at the moment when snapshot was created, so no records that were added to the index after snapshot process had started will be present in the snapshot.

Besides creating a copy of each index the snapshot process can also store global cluster metadata, which includes persistent cluster settings and templates. The transient settings and registered snapshot repositories are not stored as part of the snapshot.

Only one snapshot process can be executed in the cluster at any time. While snapshot of a particular shard is being created this shard cannot be moved to another node, which can interfere with rebalancing process and allocation filtering. Once snapshot of the shard is finished Elasticsearch will be able to move shard to another node according to the current allocation filtering settings and rebalancing algorithm.

Once a snapshot is created information about this snapshot can be obtained using the following command:

```
$ curl -XGET "localhost:9200/_snapshot/my_backup/snapshot_1"
```

All snapshots currently stored in the repository can be listed using the following command:

```
$ curl -XGET "localhost:9200/_snapshot/my_backup/_all"
```

A snapshot can be deleted from the repository using the following command:

```
$ curl -XDELETE "localhost:9200/_snapshot/my_backup/snapshot_1"
```

When a snapshot is deleted from a repository, Elasticsearch deletes all files that are associated with the deleted snapshot and not used by any other snapshots. If the deleted snapshot operation is executed while the snapshot is being created the snapshotting process will be aborted and all files created as part of the snapshotting process will be cleaned. Therefore, the delete snapshot operation can be used to cancel long running snapshot operations that were started by mistake.
### Restore

A snapshot can be restored using this following command:

```
$ curl -XPOST "localhost:9200/_snapshot/my_backup/snapshot_1/_restore"
```

By default, all indices in the snapshot as well as cluster state are restored. It's possible to select indices that should be restored as well as prevent global cluster state from being restored by using `indices` and `restore_global_state` options in the restore request body. The list of indices supports multi index syntax. The `rename_pattern` and `rename_replacement` options can be also used to rename index on restore using regular expression that supports referencing the original text as explained [here](http://docs.oracle.com/javase/6/docs/api/java/util/regex/Matcher.html#appendReplacement%28java.lang.StringBuffer,%20java.lang.String%29).

```
$ curl -XPOST "localhost:9200/_snapshot/my_backup/snapshot_1/_restore" -d '{
    "indices": "index_1,index_2",
    "ignore_indices": "missing",
    "restore_global_state": false,
    "rename_pattern": "index_(.)+",
    "rename_replacement": "restored_index_$1"
}'
```

The restore operation can be performed on a functioning cluster. However, an existing index can be only restored if it's closed. The restore operation automatically opens restored indices if they were closed and creates new indices if they didn't exist in the cluster. If cluster state is restored, the restored templates that don't currently exist in the cluster are added and existing templates with the same name are replaced by the restored templates. The restored persistent settings are added to the existing persistent settings.
</description><key id="20430411">3826</key><summary>Snapshot/Restore API - Phase I</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>feature</label><label>v1.0.0.Beta2</label></labels><created>2013-10-03T00:04:48Z</created><updated>2014-09-30T14:13:32Z</updated><resolved>2013-11-11T00:59:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-10-03T13:21:46Z" id="25619567">&gt; The list of indices that should be included into the snapshot can be specified using the indices parameter that supports multi index syntax.

Assuming this supports alias names you might want to mention that explicitly.  Also assuming aliases are supported you may want to mention if restoring an index with an alias automatically gets the alias.  If it does you may want to add an option to restore to prevent it from doing so.  I could see this being a nasty surprise when people start complaining about results getting doubled.

&gt; The index snapshot process is incremental. In the process of making the index snapshot Elasticsearch analyses the list of the index files that are already stored in the repository and copies only files that were created or changed since the last snapshot.

Might want to mention that this doesn't mean that the snapshot will be just a list of diffs.  It sounds like this will work similarly to file based replication so merges will cause big files.

&gt; The restore operation can be performed on a functioning cluster.

Can I use this to restore my cluster to another cluster?  This seems like a good opportunity for such a useful feature.
</comment><comment author="mattweber" created="2013-10-03T15:27:42Z" id="25630350">Is there a way to perform a full snapshot or merge all the incrementals into a single snapshot?  I am thinking of replicating an existing system that has maybe 10+ snapshots already.  Instead of restoring the 10+ incremental snapshots, doing a single restore would be much easier.  Maybe allow specifying multiple snapshot names in a single call and they will be restored in order?

```
curl -XPOST "localhost:9200/_snapshot/my_backup/snap1,snap2,snap3,snap4"
```
</comment><comment author="imotov" created="2013-10-03T16:34:32Z" id="25636165">@nik9000 Thanks! Good points, I will clarify the docs. And yes, you can restore into a different cluster.

@mattweber I wasn't quite clear in the description and I will fix it in the next iteration. By incremental, I meant that each snapshot only copies files that were changed since the last snapshot. Each snapshot always points to a complete view of snapshotted indices. However, if two snapshots share the same subset of files they will point to the same physical files in the repository and these files will be copied only once. 

In order to restore cluster to a particular state, you simply restore corresponding snapshot. In other words in order to restore cluster to the state that it was during 4th snapshot, you simple execute

```
curl -XPOST "localhost:9200/_snapshot/my_backup/snap4"
```

It doesn't matter how many snapshots you created before it. Moreover, you can delete intermediate snapshots (snap1, snap2, snap3), which will leave only files referenced by snap4. 
</comment><comment author="mattweber" created="2013-10-03T16:48:58Z" id="25637468">@imotov ok great, that make sense.  Can you clarify how we would replicate an existing cluster using this functionality?  I imagine is it something like:
1. Start up new cluster
2. Create your "my_backup" repository on the new cluster.
3. Copy/Rsync contents of the "my_backup" repository from existing cluster to the new cluster's "my_backup" repository location
4. Execute the restore of a snapshot.

Thanks!
</comment><comment author="imotov" created="2013-10-04T00:56:34Z" id="25670272">@mattweber yes that's the idea. Except in 3 is optional. You can restore from the same location where you back. There are no locks at the moment, so you need to make sure that you don't backup to the same location from two clusters at the same time, but you should be able to backup from one and restore from another without any issues.
</comment><comment author="kimchy" created="2013-10-05T15:56:50Z" id="25751069">a note on compression, I would default it to false since by default, we compress the index. 
</comment><comment author="jprante" created="2013-10-06T10:34:42Z" id="25765871">Thanks for starting the backup/restore effort, this is great news. In phase I, shared file system (NFS?) and S3 will be supported (the old deprecated gateways). What is planned for phase II? Maybe auto backup? Cross data center synchronization? If there is a plan for phase II, will it also be scheduled for 1.0?
</comment><comment author="imotov" created="2013-10-06T15:38:30Z" id="25770221">@jprante Not sure yet. Most likely, these features will appear after 1.0.
</comment><comment author="Mpdreamz" created="2013-10-10T12:27:54Z" id="26049112">It would be nice if we could specify a retention policy posting to `localhost:9200/_snapshot/_retention`

```
{
   max_age: "30d"
   min_number_of_backups: 20
}
```

The above would delete backups older then 30days but keep a minimum of 20 (possibly older than 30 days).
</comment><comment author="imotov" created="2013-10-10T21:16:37Z" id="26092519">@Mpdreamz for now you will need to delete snapshots manually, but in the future releases we might add retention policy and automatic scheduled snapshots.
</comment><comment author="dspangen" created="2013-11-06T17:36:48Z" id="27895058">I find the interface for initiating a snapshot vs initiating a restore a little troubling: the only difference is the HTTP verb. It seems very problematic that PUT will initiate the snapshot and POST will initiate the restore. This is waiting for someone to screw them up (and potentially cause a catastrophe that might be difficult to recover from).

A better approach might be to use an explicit path parameter to initiate a restore since it is the destructive operation:

`$ curl -XPOST "localhost:9200/_snapshot/my_backup/snapshot_1/_restore"`
</comment><comment author="thienchi" created="2013-12-13T04:12:59Z" id="30485228">nice API, thanks
</comment><comment author="sebaes" created="2014-01-06T14:38:36Z" id="31652513">Hi,

Just to report a small typo, in the docs it says the keyword to use to list all repositories is "_all", but the example curl line:

```
curl -XGET 'http://localhost:9200/_snapshot/all'
```

uses "all", which fails with:
{"error":"RepositoryMissingException[[all] missing]","status":404}

This works fine:

```
curl -XGET 'http://localhost:9200/_snapshot/_all'
```

Cheers,
Sebastian.
</comment><comment author="sebaes" created="2014-01-06T14:46:03Z" id="31653051">Another comment, I think the attributes "duration" and "duration_in_millis" contain the values of each other and should be interchanged, look at the values below:

curl -XPUT "localhost:9200/_snapshot/my_backup/snapshot_2?wait_for_completion=true&amp;pretty"
{
  "snapshot" : {
    "snapshot" : "snapshot_2",
    "indices" : [ "......MY INDICES....." ],
    "state" : "SUCCESS",
    "start_time" : "2014-01-06T14:42:54.957Z",
    "start_time_in_millis" : 1389019374957,
    "end_time" : "2014-01-06T14:43:04.128Z",
    "end_time_in_millis" : 1389019384128,
    "duration" : 9171,
    "duration_in_millis" : "9.1s",
    "failures" : [ ],
    "shards" : {
      "total" : 65,
      "failed" : 0,
      "successful" : 65
    }
  }
}
</comment><comment author="s1monw" created="2014-01-06T17:01:11Z" id="31665104">@sebaes do you mind opening issues for each of your findings - both of them are valid!
</comment><comment author="imotov" created="2014-01-07T19:48:02Z" id="31772180">@sebaes thanks for reporting. Fixed by 5d98341 and 2b49ec1.
</comment><comment author="sebaes" created="2014-01-09T16:22:26Z" id="31948656">Hi @s1monw and @imotov,
Sorry for not opening the issues sooner, I have just seen your comments with the fix already implemented.
Thanks!
</comment><comment author="keziacp" created="2014-09-30T13:40:18Z" id="57314693">Hi I am trying to make a snap shot of my disk in solaris and send to another server, but it is giving me a error, could you help me please?

root@srvdth03 # zfs receive -v rpool/vdisk/vdisk-hdd0@snap &lt; /rpool/snaps/vdisk-hdd0.snap2
cannot receive: can not specify snapshot name for multi-snapshot stream
</comment><comment author="jprante" created="2014-09-30T14:13:32Z" id="57319683">@keziacp you are performing a ZFS snapshot, not ES snapshot.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/TransportGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/package-info.java</file><file>src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RepositoryMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RestoreMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/SnapshotId.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/routing/ImmutableShardRouting.java</file><file>src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>src/main/java/org/elasticsearch/cluster/routing/MutableShardRouting.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RestoreSource.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/common/Classes.java</file><file>src/main/java/org/elasticsearch/common/blobstore/url/AbstractURLBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/url/URLBlobStore.java</file><file>src/main/java/org/elasticsearch/common/blobstore/url/URLImmutableBlobContainer.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRestoreException.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRestoreFailedException.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotAndRestoreService.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotException.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotFailedException.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotModule.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotStatus.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoriesModule.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoriesService.java</file><file>src/main/java/org/elasticsearch/repositories/Repository.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryException.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryMissingException.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryModule.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryName.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryNameModule.java</file><file>src/main/java/org/elasticsearch/repositories/RepositorySettings.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoryTypesRegistry.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreSnapshot.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepositoryModule.java</file><file>src/main/java/org/elasticsearch/repositories/uri/URLRepository.java</file><file>src/main/java/org/elasticsearch/repositories/uri/URLRepositoryModule.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/delete/RestDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/get/RestGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/repositories/put/RestPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/create/RestCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/delete/RestDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/get/RestGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/restore/RestRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/gateway/snapshot/RestGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/snapshots/ConcurrentSnapshotExecutionException.java</file><file>src/main/java/org/elasticsearch/snapshots/InvalidSnapshotNameException.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreInfo.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/main/java/org/elasticsearch/snapshots/Snapshot.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotCreationException.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotException.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotMissingException.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotRestoreException.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotShardFailure.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotState.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotUtils.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/snapshots/AbstractSnapshotTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SnapshotUtilsTests.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/BlobStoreWrapper.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/ImmutableBlobContainerWrapper.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepositoryModule.java</file></files><comments><comment>Initial implementation of Snapshot/Restore API</comment></comments></commit></commits></item><item><title>Threadpool defaults assume single node per machine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3825</link><project id="" key="" /><description>The [threadpool defaults](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L103) are too aggressive if you run more than one ES node on a single machine.  Would like to add a `threadpool.nodes_per_machine` setting (default `1`) which is used to calculate the defaults. Could be that influences the value of `availableProcessors` that is then used in the settings.
</description><key id="20424235">3825</key><summary>Threadpool defaults assume single node per machine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels /><created>2013-10-02T21:45:31Z</created><updated>2014-08-08T17:44:56Z</updated><resolved>2014-08-08T17:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-10-02T21:55:48Z" id="25580608">+1, I was going to ask about this on the webinar earlier today!
</comment><comment author="mattweber" created="2013-10-02T22:13:47Z" id="25581681">I think using 2767c081cd and setting the value to processors/nodes_on_machine would do the same thing correct?
</comment><comment author="kimchy" created="2013-10-02T22:19:12Z" id="25582037">yea, we already have support for that in the `processors` setting, which we will base all computations based on that.
</comment><comment author="drewr" created="2013-10-03T02:39:23Z" id="25593543">I had actually seen that but I didn't realize it was intended to serve this purpose. Makes total sense now though. Perhaps we could also support a `nodes_per_machine` to save the hassle of having to manually set it. I can envision having a heterogeneous environment where you have differing numbers of cores on machines but want to provision two nodes across the board. This could prevent people from having to do the work in chef or puppet to handle all cases. Too clever?
</comment><comment author="kimchy" created="2013-11-04T14:03:06Z" id="27686488">@drewr good Q..., feels like this should be left to the user..., but would love to get feedback from others
</comment><comment author="nik9000" created="2013-11-04T14:23:27Z" id="27687990">We won't run more than one node per machine without better support from the deb packages for any use cases I can cook up are just academic (for us) but even if we did we'd probably end up wanting more fine grained control then "divide the detected cores by n".  I'm thinking of use cases like:
1.  Run a master node in a separate vm from a data node on the same machine.  `processors` would be set to `1` and `-1` respectively.
2.  Run two data nodes on a machine with a ton of ram or two disks or something.  `processors` would both be set to `50%`.
   And a use cases _we'd_ never attempt but I imagine others might:
3.  Run Elasticsearch on a node also containing a db/intensive app server as some kind of demonstration node.  `processors` as is would work for this, maybe with the additional support of parsing percentages.  For most demonstration use cases this doesn't matter but I can imagine some where is might.
</comment><comment author="clintongormley" created="2014-08-08T17:44:56Z" id="51634555">No further interest since last year. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Checking for jvm.dll fails when %JAVA_HOME% contains spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3824</link><project id="" key="" /><description /><key id="20421206">3824</key><summary>Checking for jvm.dll fails when %JAVA_HOME% contains spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">ovidiupruteanu</reporter><labels /><created>2013-10-02T20:55:39Z</created><updated>2014-06-19T16:09:38Z</updated><resolved>2013-10-24T10:43:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-24T10:43:08Z" id="26982214">Hi,

This (along other bugs) has been fixed in 0.90 and master some time ago - see #3725. Can you please try it out and report back if it doesn't work for you?

Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Support the `has_child`, `has_parent`, `top_children` queries (and filters) in other apis than just search api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3823</link><project id="" key="" /><description>Relates to #3822
</description><key id="20418698">3823</key><summary> Support the `has_child`, `has_parent`, `top_children` queries (and filters) in other apis than just search api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-10-02T20:13:12Z</created><updated>2015-05-18T23:33:56Z</updated><resolved>2013-10-09T08:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-10-04T21:18:47Z" id="25730869">+1!
</comment><comment author="jondavidford" created="2014-08-26T21:50:20Z" id="53498188">I was confused when this PR came up in my search but I was still getting a message that has_parent was not supported by delete_by_query api. This is actually no longer implemented due to https://github.com/elasticsearch/elasticsearch/pull/5916 and https://github.com/elasticsearch/elasticsearch/issues/5828
</comment><comment author="martijnvg" created="2014-08-27T06:46:39Z" id="53532516">@jondavidford Part of this PR has been reverted. The only api the p/c queries and filters don't work is delete by query. In the count and explain apis the parent / child queries and filters don't fail anymore. 

The p/c queries don't support score explaining, so there not useful on their own in the explain api, but as part of a bigger query it still may be useful.
</comment><comment author="rmruano" created="2014-09-24T15:08:01Z" id="56684332">I was also super confused by the fact has_parent it's not supported anymore, thougth I was doing something wrong until I finally found https://github.com/elasticsearch/elasticsearch/pull/5916. Perhaps the delete by query documentation should be updated to inform about this limitation.
</comment><comment author="martijnvg" created="2014-09-25T07:45:36Z" id="56785073">@rmruano I made the suggested doc change to the delete by query api docs: https://github.com/elasticsearch/elasticsearch/commit/70303be50c47d277850b28c604e63723c2a2d752
</comment><comment author="rmruano" created="2014-09-25T08:25:12Z" id="56788407">Thanks @martijnvg !, I'm closing the PR I did regarding the doc change: https://github.com/elasticsearch/elasticsearch/pull/7858
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parent / child queries should also work apis other than search api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3822</link><project id="" key="" /><description>The `has_child`, `has_parent` and `top_children` now _only_ work in the search api due to how this api executed these queries. These queries should also work in other apis, like count, explain and delete by query apis.
</description><key id="20417989">3822</key><summary>Parent / child queries should also work apis other than search api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-02T20:02:14Z</created><updated>2014-01-31T22:19:02Z</updated><resolved>2013-10-09T08:20:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-02T20:02:39Z" id="25571542">huge +1  ;)
</comment><comment author="mattweber" created="2013-10-02T22:03:48Z" id="25581056">+1,  also good for #2705 and #1566?
</comment><comment author="martijnvg" created="2013-10-04T19:10:52Z" id="25723212">@mattweber Good that you pointed this out. With the new design this is possible, I made some minor changes and seems to work out, so yes :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/MatchDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java</file><file>src/main/java/org/elasticsearch/common/recycler/RecyclerUtils.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ConstantScorer.java</file><file>src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/DeleteByQueryWrappingFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/HasChildFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/HasParentFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/nested/IncludeNestedDocsQuery.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Support the `has_child`, `has_parent`, `top_children` queries (and filters) in other apis than just search api. The queries are also now supported in count, explain (return dummy explination) and delete by query apis.</comment><comment>Migrate SearchContext.Rewrite to Releasable. All the parent child queries are now implemented as Lucene queries and the state is kept in the Weight.</comment><comment>Completely disable caching for `has_child` and `has_parent` filters, this has never worked and it also can also never work. The matching docIds are cached per segment while the collection of parent ids is top level.</comment></comments></commit></commits></item><item><title>Add support for `shard_size` for terms &amp; terms_stats facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3821</link><project id="" key="" /><description>`shard_size` will enable to increase the accuracy of the returned term entries.

The `size` parameter defines how many top terms should be returned out
of the overall terms list. By default, the node coordinating the
search process will ask each shard to provide its own top `size` terms
and once all shards respond, it will reduces the results to the final list
that will then be sent back to the client. This means that if the number
of unique terms is greater than `size`, the returned list is slightly off
and not accurate (it could be that the term counts are slightly off and it
could even be that a term that should have been in the top `size` entries
was not returned).

The higher the requested `size` is, the more accurate the results will be,
but also, the more expensive it will be to compute the final results (both
due to bigger priority queues that are managed on a shard level and due to
bigger data transfers between the nodes and the client). In an attempt to
minimize the extra work that comes with bigger requested `size` we a
`shard_size` parameter was introduced. The once defined, it will determine
how many terms the coordinating node is requesting from each shard. Once
all the shards responded, the coordinating node will then reduce them
to a final result which will be based on the `size` parameter - this way,
once can increase the accuracy of the returned terms and avoid the overhead
of streaming a big list of terms back to the client.

Note that `shard_size` cannot be smaller than `size`... if that's the case
elasticsearch will override it and reset it to be equal to `size`.
</description><key id="20417559">3821</key><summary>Add support for `shard_size` for terms &amp; terms_stats facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-02T19:48:58Z</created><updated>2013-11-24T23:59:07Z</updated><resolved>2013-10-02T20:03:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/facet/terms/TermsFacetBuilder.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/TermsFacetParser.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/doubles/InternalDoubleTermsFacet.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/index/IndexNameFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/longs/InternalLongTermsFacet.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/strings/FieldsTermsStringFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/strings/HashedAggregator.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/strings/InternalStringTermsFacet.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/strings/ScriptTermsStringFieldFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringOrdinalsFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/TermsStatsFacetBuilder.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/TermsStatsFacetParser.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/doubles/InternalTermsStatsDoubleFacet.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/doubles/TermsStatsDoubleFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/longs/InternalTermsStatsLongFacet.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/longs/TermsStatsLongFacetExecutor.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/strings/InternalTermsStatsStringFacet.java</file><file>src/main/java/org/elasticsearch/search/facet/termsstats/strings/TermsStatsStringFacetExecutor.java</file><file>src/test/java/org/elasticsearch/search/facet/terms/ShardSizeTermsFacetTests.java</file><file>src/test/java/org/elasticsearch/search/facet/termsstats/ShardSizeTermsStatsFacetTests.java</file></files><comments><comment>introduced support for "shard_size" for terms &amp; terms_stats facets. The "shard_size" is the number of term entries each shard will send back to the coordinating node. "shard_size" &gt; "size" will increase the accuracy (both in terms of the counts associated with each term and the terms that will actually be returned the user) - of course, the higher "shard_size" is, the more expensive the processing becomes as bigger queues are maintained on a shard level and larger lists are streamed back from the shards.</comment></comments></commit></commits></item><item><title>Debian Package sets /etc/elasticsearch/* to 0644</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3820</link><project id="" key="" /><description>DEBIAN/postinst:37
    chmod 644 /etc/elasticsearch/*

this is only a good idea as long as there are no subdirectories in /etc/elasticsearch/ because after updating the elasticsearch package files in /etc/elasticsearch/synonyms (for example) can't be read anymore.
</description><key id="20404300">3820</key><summary>Debian Package sets /etc/elasticsearch/* to 0644</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ferbar</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-10-02T16:07:24Z</created><updated>2014-02-18T15:02:40Z</updated><resolved>2014-02-18T15:00:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Set permission in debian postinst script correctly</comment></comments></commit></commits></item><item><title>Better warm-up of merged segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3819</link><project id="" key="" /><description>Query-based warm-up is convenient as it doesn't require to know how things work under the hood to warm-up a segment: people just have to put there queries that are similar to their users' queries.

However, warm-up could be made more efficient through `IndexWriterConfig.setMergedSegmentWarmer`/`IndexWriter.IndexReaderWarmer` which doesn't require running a query at all and is run during the merging phase, before the new segment goes live. I think we should expose a way to register such warmers and maybe use something like `SimpleMergedSegmentWarmer` by default.
</description><key id="20389934">3819</key><summary>Better warm-up of merged segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-02T12:07:57Z</created><updated>2013-10-08T22:59:40Z</updated><resolved>2013-10-08T21:25:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-02T12:19:44Z" id="25533846">+1
</comment><comment author="dadoonet" created="2013-10-02T13:14:41Z" id="25537036">Ha! Nice!
</comment><comment author="nik9000" created="2013-10-02T13:58:08Z" id="25540395">I'd probably us it for my indexes that get a decent amount of traffic.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/fielddata/FieldDataType.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/indices/warmer/IndicesWarmer.java</file><file>src/main/java/org/elasticsearch/indices/warmer/InternalIndicesWarmer.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/facet/ExtendedFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Improved warm-up of new segments.</comment></comments></commit></commits></item><item><title>has_child ignores type field for Grandparent/Parent/Child query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3818</link><project id="" key="" /><description>I've got an issue where the type field seems to be ignored for the child document, ES (0.90.2) seems to return matches as long as any of it's child documents (regardless of type) has a match in a field with the same name. Please see the following gist: https://gist.github.com/erlingwl/6779401

This was confirmed by Martijn as well, see this thread: https://groups.google.com/forum/#!topic/elasticsearch/V-bKL9QHftc
</description><key id="20386212">3818</key><summary>has_child ignores type field for Grandparent/Parent/Child query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">erlingwl</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-02T10:32:18Z</created><updated>2013-10-28T09:16:19Z</updated><resolved>2013-10-14T10:43:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-10-02T12:28:54Z" id="25534311">Thanks for opening this issue @erlingwl I'll get to the bottom of this issue. 
</comment><comment author="martijnvg" created="2013-10-14T10:50:04Z" id="26247729">This bug only occurred for the `has_child` query. This bug didn't affect the `has_child` filter and the `has_parent` query and filter. 
</comment><comment author="erlingwl" created="2013-10-14T11:27:20Z" id="26249297">Thanks @martijnvg! 
</comment><comment author="Kamapcuc" created="2013-10-28T09:16:19Z" id="27197493">so, when new version with bugfix will be avalible?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>The `has_child` query's inner query now is wrapped in a filtered query with the child type as filter, this prevents other children from being returned as hit.</comment><comment>Extended the specialized simplified mapping source method to support metadata mapping fields. These fields can just specified as normal fields, but will automatically be placed as top level mapping field.</comment></comments></commit></commits></item><item><title>Getting missing templates returns an empty body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3817</link><project id="" key="" /><description>If no index templates exist, then:

```
GET /_template
```

currently returns an empty body. I think it should return `{}` instead

Also, trying to get a template that doesn't exist should throw a 404:

```
GET /_template/nonexistent
```

Currently this also returns an empty body
</description><key id="20386073">3817</key><summary>Getting missing templates returns an empty body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-02T10:28:39Z</created><updated>2013-10-02T11:22:22Z</updated><resolved>2013-10-02T11:22:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-10-02T11:22:22Z" id="25531201">Already fixed in https://github.com/elasticsearch/elasticsearch/commit/b5ae1fb51bce1290b1c6b30167cc4778e54d257d (not yet released)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms Facets: Allow to specify a set of terms to 'include' in the request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3816</link><project id="" key="" /><description>This is logically opposite to 'exclude' functionality in terms facet. Take linkedin as an example. We facet on country field and display the top-5 countries by frequency count. If I want to add some additional countries to the facet result, there is no easy way to do so. 

This is a feature request to add support for 'include' parameter that will allow to specify a set of terms that should be included in the terms facet request result. 

``` javascript
curl -XPOST localhost:9200/testindex -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "testtype" : {
            "properties" : {
                "countries" : { "type" : "string", "index" : "not_analyzed" }
            }
        }
    }
}'
```

``` javascript
curl -s -XPOST localhost:9200/_bulk -d '
{ "index" : { "_index" : "testindex", "_type" : "testtype", "_id" : "1" } }
{ "countries" : [ "Djibouti", "Saudi Arabia" ] }
{ "index" : { "_index" : "testindex", "_type" : "testtype", "_id" : "2" } }
{ "countries" : [ "United States" ] }
{ "index" : { "_index" : "testindex", "_type" : "testtype", "_id" : "3" } }
{ "countries" : [ "United States", "France" ] }
{ "index" : { "_index" : "testindex", "_type" : "testtype", "_id" : "4" } }
{ "countries" : [ "France" ] }
'
```

Desired behavior after change - 

``` javascript
curl -s -XGET 'localhost:9200/testindex/_search?pretty&amp;size=0' -d '
{
  "facets" : {
    "countries" : {
      "terms" : { 
        "field" : "countries", "size" : 2, "include" : [ "Djibouti" ]
      }
    }
  }
}
'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "facets" : {
    "countries" : {
      "_type" : "terms",
      "missing" : 0,
      "total" : 7,
      "other" : 1,
      "terms" : [ {
        "term" : "United States",
        "count" : 2
      }, {
        "term" : "France",
        "count" : 2
      }, {
        "term" : "Djibouti",
        "count" : 1
      } ]
    }
  }
}
```

Note: Due to nature of this feature, the size of facet terms in result can be more than the requested size, upto requested size + size of include terms. 
</description><key id="20360798">3816</key><summary>Terms Facets: Allow to specify a set of terms to 'include' in the request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajhalani</reporter><labels /><created>2013-10-01T21:39:13Z</created><updated>2014-08-08T17:43:48Z</updated><resolved>2014-08-08T17:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajhalani" created="2013-10-01T21:48:50Z" id="25494503">I will be glad to develop and submit this feature. 
On a high level changes will need to be done in TermsStringOrdinalFacetExecutor to save TermEntry for included terms. These included TermEntries will be member of InternalStringTermFacet like the ordered entries. When SearchPhaseController merges facet results from different shards, it will add the included entries to the ordered entries. Similar changes would be done for double/long terms facet. 
Thank you!
</comment><comment author="javanna" created="2013-10-03T14:03:19Z" id="25622795">I'm not quite sure if it makes sense to return more facet entries than `size`, I find it misleading. Can't you work around this using scripting? With a script you could control which entries are going to be part of the facet, depending on whether the script returns true or false. Have a look here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html .
</comment><comment author="ajhalani" created="2013-10-03T14:55:23Z" id="25627318">It seems to me that script can only get the terms satisfying the script, it will not include the top-n terms. If I misunderstood, could you please provide an example with the above data how it would work?
With respect to getting more count as misleading, I would disagree since the user is manually including terms so he expects extra results. As supporting examples, Amazon and Linkedin facets also work in similar fashion. 

Example amazon.com, language facet on a book search shows **15** results - 
![amazon_15_results](https://f.cloud.github.com/assets/1163503/1262151/0f41ad9c-2c3b-11e3-86c8-58d5bb95cf8b.PNG)

When I manually included another term, it now shows **16** shows results - 
![amazon_16_results](https://f.cloud.github.com/assets/1163503/1262160/23f3a452-2c3b-11e3-9660-9aac9e49b64d.PNG)
</comment><comment author="nariman-haghighi" created="2014-04-11T20:05:29Z" id="40247782">+1 would find this feature very useful.
</comment><comment author="clintongormley" created="2014-08-08T17:43:48Z" id="51634431">Facets are now deprecated, and aggregations has more extensive support for the above.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>River routing failed for river names containing the river type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3815</link><project id="" key="" /><description>Two rivers of type "myRiver" with name "myRiverOnNodeA" and "myRiverOnNodeB" can't be bound to node A or B by setting the configuration node.river to myRiverOnNodeA or myRiverOnNodeB. Both node.river strings contain the river type, and so every node could run the river. this is contrary to the description on http://www.elasticsearch.org/guide/reference/river/
</description><key id="20353785">3815</key><summary>River routing failed for river names containing the river type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgaedicke</reporter><labels /><created>2013-10-01T19:41:33Z</created><updated>2014-07-16T21:52:12Z</updated><resolved>2013-10-07T19:42:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Accept dynamic templates with only a match type criteria</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3814</link><project id="" key="" /><description>If some wants all strings in an index to default to `not_analyzed`, you can do that via the dynamic template settings:

```
curl -XPUT "http://localhost:9200/index" -d'
{
   "mappings": {
      "_default_": {
         "dynamic_templates": [
            {
               "test": {
                  "match_mapping_type": "string",
                  "match": "*", // &lt;- not needed when we have 'match_mapping_type'
                  "mapping": {
                      "index": "not_analyzed"
                  }
               }
            }
         ]
      }
   }
}'
```

However, if you remove the `"match":"*"`, you get the following error:

```
{
   "error": "MapperParsingException[mapping [_default_]]; nested: MapperParsingException[template must have match or path_match set]; ",
   "status": 400
}
```

A minor issue but if one specifies `match_mapping_type`, we can default `match` to `*`
</description><key id="20334744">3814</key><summary>Accept dynamic templates with only a match type criteria</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-01T14:25:48Z</created><updated>2013-10-01T15:05:37Z</updated><resolved>2013-10-01T15:05:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/object/DynamicTemplate.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file></files><comments><comment>Allow dynamic templates with match_mapping_type but no path_match or match.</comment></comments></commit></commits></item><item><title>Added official clients to the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3813</link><project id="" key="" /><description /><key id="20330489">3813</key><summary>Added official clients to the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2013-10-01T13:22:15Z</created><updated>2014-06-27T20:08:59Z</updated><resolved>2013-11-20T13:45:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HonzaKral" created="2013-11-20T13:45:50Z" id="28889859">This is now obsolete.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get index templates API returns and error if no template is supplied</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3812</link><project id="" key="" /><description>Call this:

```
curl -XGET "http://localhost:9200/_template"
```

And get a status code of 500.
</description><key id="20327122">3812</key><summary>Get index templates API returns and error if no template is supplied</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-10-01T12:14:44Z</created><updated>2013-10-01T12:20:22Z</updated><resolved>2013-10-01T12:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/GetIndexTemplatesRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/template/get/RestGetIndexTemplateAction.java</file></files><comments><comment>Calling `GET _template` gave a 500 error back.</comment></comments></commit></commits></item><item><title>support histogram interval of type double</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3811</link><project id="" key="" /><description>See Issue #3810.
</description><key id="20297412">3811</key><summary>support histogram interval of type double</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lepfhty</reporter><labels><label>discuss</label></labels><created>2013-09-30T22:34:47Z</created><updated>2014-08-11T08:50:42Z</updated><resolved>2014-08-08T08:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lepfhty" created="2013-10-18T01:38:36Z" id="26566795">The bucketing of doubles is not always accurate, since we are using floating-point arithmetic. I think (not completely sure) that if the interval can be expressed mathematically as `a / 2^n`, where a and n are integers, then the bucketing will be accurate. Some good intervals are: 0.125, 0.25, 0.5, 0.75, 3.125, etc...
</comment><comment author="ejain" created="2013-10-28T06:51:11Z" id="27192146">See also #3799
</comment><comment author="jpountz" created="2014-08-08T08:33:10Z" id="51576188">Facets are now deprecated, so I am closing in favor of https://github.com/elasticsearch/elasticsearch/issues/4847
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>histograms should support intervals of decimal type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3810</link><project id="" key="" /><description>Certain values, such as blood pH and body temperature in Celcius, require precision to the tenths or hundredths place. They also have very little variance. Blood pH ranges between 7.3 and 7.5. Body temperatures range between 35 and 40. 

In 0.90.5, the smallest supported interval currently is the smallest positive integer 1. If an interval of `0.1` is submitted, a parse error is returned, claiming that `[interval] is required to be set for histogram facet`.

To support histograms of fields with precision smaller than 1, elasticsearch should support intervals of decimal type. No new functionality is proposed, just changing the type of the interval from integer to double/float.

The following cURL setup and search:

```
curl -XPUT localhost:9200/test/test/1 -d '{ "value": 1.1 }'

curl -XPUT localhost:9200/test/test/2 -d '{ "value": 1.2 }'

curl -XPOST "http://localhost:9200/test/test/_search" -d'
{
    "query": {
        "match_all": {}
    },
    "size": 0,
    "facets": {
        "hist": {
            "histogram": {
               "field": "value",
               "interval": 0.1
            }
        }
    }
}'
```

Should produce the following histogram:

```
"entries": [
            {
               "key": 1.1,
               "count": 1
            },
            {
               "key": 1.2,
               "count": 1
            }
         ]
```
</description><key id="20297343">3810</key><summary>histograms should support intervals of decimal type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lepfhty</reporter><labels /><created>2013-09-30T22:33:14Z</created><updated>2017-01-17T20:33:36Z</updated><resolved>2014-08-08T17:43:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="loris" created="2013-10-15T09:34:15Z" id="26320905">+1
</comment><comment author="ejain" created="2013-10-28T06:51:05Z" id="27192143">See also #3799
</comment><comment author="clintongormley" created="2014-08-08T17:43:09Z" id="51634348">Closing in favour of #4847
</comment><comment author="Kamagawa" created="2017-01-17T20:33:36Z" id="273291124">+1</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Empty filters returns no result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3809</link><project id="" key="" /><description>to reproduce:

```
curl -XPOST http://localhost:9200/foo -d '{"settings":{"number_of_shards":1,"number_of_replicas":0}}'
curl -XPUT http://localhost:9200/foo/bar/1 -d '{"id":1,"content":"foobar"}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{"filter":{"and":{"filters":[]}}}'
```

this returns results for 0.90.0 until 0.90.2, and starting with 0.90.3 it returns no results.
i looked around a little bit and this seems to be due to https://github.com/elasticsearch/elasticsearch/commit/74a7c46b0e0e4c921e30574214da6b77ef354b18
just want to confirm this is and will be the correct behavior for this.
thanks
</description><key id="20280528">3809</key><summary>Empty filters returns no result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-09-30T18:11:29Z</created><updated>2013-10-07T10:56:22Z</updated><resolved>2013-10-07T10:43:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-30T18:22:23Z" id="25388759">thinking about it, I think the fix for #3356 was a mistake, and we should not use `match_no` filter when parsing a filter return null, we should just ignore it (as we do in `filtered` query when there is no filter, and we just use the `query`).

I think the correct fix, is that the `missing` filter should return a no match filter for the case described in #3356.
</comment><comment author="lmenezes" created="2013-09-30T18:30:24Z" id="25390112">sounds more intuitive. so i should expect 0.90.0 behavior back on 0.90.6?
just asking so i can prepare things on my end for an update. 
</comment><comment author="kimchy" created="2013-09-30T18:51:02Z" id="25392167">I am looking into it, will this ticket updated....
</comment><comment author="kimchy" created="2013-10-07T10:43:31Z" id="25798958">This is fixed in #3838 
</comment><comment author="lmenezes" created="2013-10-07T10:56:22Z" id="25799687">great! many thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedFilter.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/terms/IndicesTermsFilterCache.java</file><file>src/main/java/org/elasticsearch/search/facet/filter/FilterFacetParser.java</file><file>src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java</file><file>src/main/java/org/elasticsearch/search/query/FilterParseElement.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Better logic ignoring filters when parsing</comment><comment>When parsing a filter, we effectively have a case where we need to ignore a filter. For example, when an "and" filter has no elements. Ignoring a filter is different compared to matching on all or matching on none, because an and filter with no elements should simply be ignored in the context of a bool filter for example, regardless if its used within a must or a must_not filter.</comment></comments></commit></commits></item><item><title>pattern_capture token filter does not throw error with patterns missing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3808</link><project id="" key="" /><description>Just a simple note. When creating a custom_filter, the pattern_capture token filter does not throw an error with patterns missing. Instead, it must default to some behavior. I had mistakenly used "pattern" with a single string rather than "patterns" with an array.

https://gist.github.com/jtreher/6766911

{
    "settings": {
         "_default_" : {},
        "analysis": {
            "filter": {
                "sample":{
                    "type":"pattern_capture",
                    "pattern":"(([a-z]+)(\d*))"

```
            }
        }
    }
}
```

}
</description><key id="20276422">3808</key><summary>pattern_capture token filter does not throw error with patterns missing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jtreher</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-30T17:05:05Z</created><updated>2013-09-30T18:33:53Z</updated><resolved>2013-09-30T18:33:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-30T18:31:31Z" id="25390216">good call - I will push a fix and throw an ESIllegalArgumentException :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/PatternCaptureGroupTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java</file></files><comments><comment>Throw ESIA if required field 'patterns' is not present in filter config.</comment></comments></commit></commits></item><item><title>Low-disk space awareness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3807</link><project id="" key="" /><description>We are seeing some large indices being optimized and occasionally servers running very low on disk space because of that.

Maybe if an index doesn't have enough room to optimize a rebalancing should kick in?

At least be smart enough not to get to a point of running out of hard-disk space while optimizing, and then crashing. Which is something that we've seen happen.
</description><key id="20263822">3807</key><summary>Low-disk space awareness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-09-30T13:56:29Z</created><updated>2013-12-19T19:28:51Z</updated><resolved>2013-12-19T19:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-09-30T19:08:43Z" id="25393970">@synhershko - I think this is doable with a custom setting for the high watermark where ES automatically determines what the high watermark is based on the amount of space needed for a merge of the largest segment. Does that sound like it would work for your use case?

I'll work on getting this in.
</comment><comment author="synhershko" created="2013-10-01T11:10:52Z" id="25440946">This could work, yes. Will it first relocate before actually doing the merge in this scenario?
</comment><comment author="dakrone" created="2013-10-07T10:35:44Z" id="25798600">I'm not sure yet how it will work, I'll have to do some testing too see the best way to do this.
</comment><comment author="synhershko" created="2013-12-19T08:36:43Z" id="30912751">I'm thinking this can be fixed by #3637 - enabling the decider and having good settings for low watermark?
</comment><comment author="dakrone" created="2013-12-19T19:28:51Z" id="30958134">@synhershko I think so too, going to close this for now, feel free to reopen if it turns out not to be the case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disk-based field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3806</link><project id="" key="" /><description>Lucene 4.0 introduced _doc values_, which are very similar to field data except that they are computed and persisted to disk (per segment) at index time. At search time, these data-structures are either loaded into memory or directly read from disk  depending on the doc values format. Starting with Lucene 4.5, the default is to load the small data-structures that matter for performance into memory (ordinals) and to keep the large data-structures on disk (values). It would be interesting to have a new field data implementation that would be backed by doc values.

Integration into Elasticsearch would allow for having disk-based field data and for configuring smaller heaps, which would be less subject to garbage collection issues. On the other hand, this will require additional disk space and since doc values are disk-based by default, they will probably be slower for field-data-intensive workloads.
</description><key id="20260053">3806</key><summary>Disk-based field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-09-30T12:56:06Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2013-10-09T15:02:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/common/lucene/uid/Versions.java</file><file>src/main/java/org/elasticsearch/index/codec/CodecModule.java</file><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/AbstractDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DefaultDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DiskDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormatService.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/MemoryDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/PreBuiltDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingsFormatService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/AbstractAtomicNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/AtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/AtomicNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/FieldDataType.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/ByteValuesComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/ByteValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefFieldComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/FloatValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/IntValuesComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/IntValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/ShortValuesComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/ShortValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVBytesAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVNumericAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperBuilders.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/IndexUpgraderMergePolicy.java</file><file>src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/test/java/org/elasticsearch/benchmark/fielddata/LongFieldDataBenchmark.java</file><file>src/test/java/org/elasticsearch/codecs/CodecTests.java</file><file>src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java</file><file>src/test/java/org/elasticsearch/index/codec/CodecTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/FilterFieldDataTest.java</file><file>src/test/java/org/elasticsearch/index/fielddata/LongFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/LatLonMappingGeoPointTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/ByteNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/IntegerNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/index/search/nested/ShortNestedSortingTests.java</file><file>src/test/java/org/elasticsearch/search/facet/ExtendedFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticSearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/IndexFieldDataServiceTests.java</file></files><comments><comment>Doc values integration.</comment></comments></commit></commits></item><item><title>Translate Documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3805</link><project id="" key="" /><description>Hello,

Would you want to translate documentation on another language?

If yes, Can I help you with brazilian portuguese?

Thank you
</description><key id="20258230">3805</key><summary>Translate Documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eltu</reporter><labels /><created>2013-09-30T12:15:17Z</created><updated>2014-08-08T17:41:51Z</updated><resolved>2014-08-08T17:41:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:41:50Z" id="51634194">Hi @eltu 

If you are still interested, please get in touch with @lhawthorn, our community manager

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Feature Request] Add node version to NodeStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3804</link><project id="" key="" /><description>I know I can already get the version through a different endpoint, but it would be great if this could also be available through "_cluster/nodes/stats".
Sounds reasonable?
</description><key id="20237587">3804</key><summary>[Feature Request] Add node version to NodeStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-09-29T22:46:28Z</created><updated>2013-10-13T09:47:22Z</updated><resolved>2013-10-13T09:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-30T08:13:29Z" id="25343772">we have a clear separation today between data that changes (nodes stats) to data that does not change (nodes info). Adding version to node stats on top of it existing in nodes info (its already there) will break this notion..., not sure if we want to do it...
</comment><comment author="clintongormley" created="2013-09-30T09:16:38Z" id="25346925">@lmenezes why do you want it added to nodes stats? `/_nodes/stats` or did you mean nodes info? `/_nodes`
</comment><comment author="martijnvg" created="2013-09-30T09:42:46Z" id="25348231">Btw. the node info api already returns the ES version per node.
</comment><comment author="lmenezes" created="2013-09-30T11:57:48Z" id="25354468">@kimchy yes, i kind of got that idea reading the code("Node statistics (dynamic, changes depending on when created)") and it makes sense.
Anyway, this would just make a little easier for monitoring purposes(I like to ensure that version across cluster is the same), but doing 2 requests and crossing the results is also fine.
Just thought that since version is already available on the StreamInput, making it available would be really easy, and I see as useful as node name/hostname(which are also non dynamic and could be gathered from crossing data with another request by node id).

@clintongormley i meant stats, guess I'm just really lazy ;)
</comment><comment author="lmenezes" created="2013-10-13T09:47:22Z" id="26214570">i will just go around this issue since I guess it wont make it :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Facet on nested documents field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3803</link><project id="" key="" /><description>Hello,
I have to perform facets on a huge quantity of documents, but the query must be something like:

I am searching for documents having field name TheField containing XXXXXX and their children, then perform a facets on all of them on the field TheStatField.

I am unable to do it using nested documents.
</description><key id="20230896">3803</key><summary>Facet on nested documents field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">lucabelluccini</reporter><labels /><created>2013-09-29T15:42:47Z</created><updated>2014-01-22T10:52:35Z</updated><resolved>2014-01-22T10:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-30T21:37:32Z" id="25408557">@lucabelluccini Can give more information? Like how you are executing the search request and how you defined your mapping? And what is ES returning? zero result or a error message?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs missing while using bulkProcessor migrating index data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3802</link><project id="" key="" /><description>Hi Guys, 
I'm using bulkProcessor to migrate index data from one cluster to another. To enhance the speed, i used bulkProcessor instead of using single BulkRequestBuilder. And I found the new index doc size is always less than the source cluster one. 

My test code is somewhat like below: 
&lt;code&gt;
int total = 0;
        int pageSize = 50;
        String oldIndexName = "testold";
        String newIndexName = "testnew";
        String indexDocType = "test";
        Client sourceclient = ClientUtil.getSourceTransportClient();
        Client targetclient = ClientUtil.getTargetTransportClient();
        SearchResponse searchResponse = sourceclient.prepareSearch(oldIndexName).setSearchType(SearchType.SCAN)
                .setQuery(matchAllQuery()).setSize(pageSize).setScroll(TimeValue.timeValueSeconds(20)).execute()
                .actionGet();
        ImmutableMap&lt;String, IndexMetaData&gt; map = targetclient.admin().cluster().prepareState().execute().actionGet()
                .getState().getMetaData().getIndices();
        if (!map.containsKey(newIndexName))
            targetclient
                    .admin()
                    .indices()
                    .prepareCreate(newIndexName)
                    .setSettings(
                            settingsBuilder().put("index.number_of_replicas", 0).put("index.refresh_interval", "-1"))
                    .execute().actionGet();
        Thread.sleep(200);
        BulkProcessor bulkProcessor = BulkProcessor.builder(targetclient, new BulkProcessor.Listener()
        {

```
        @Override
        public void beforeBulk(long executionId, BulkRequest request)
        {

        }

        @Override
        public void afterBulk(long executionId, BulkRequest request, BulkResponse response)
        {
            if (response.hasFailures())
            {
                throw new RuntimeException("BulkResponse show failures: " + response.buildFailureMessage());
            }
        }

        @Override
        public void afterBulk(long executionId, BulkRequest request, Throwable failure)
        {
            throw new RuntimeException("Caught exception in bulk: " + request + ", failure: " + failure, failure);
        }
    }).setConcurrentRequests(10).build();

    while (true)
    {
        searchResponse = sourceclient.prepareSearchScroll(searchResponse.getScrollId())
                .setScroll(TimeValue.timeValueSeconds(20)).execute().actionGet();
        for (SearchHit hit : searchResponse.getHits())
        {

            IndexRequestBuilder indexRequestBuilder = targetclient.prepareIndex(newIndexName, indexDocType, hit
                    .getSource().get("_id").toString());
            indexRequestBuilder.setSource(hit.getSource());
            indexRequestBuilder.setOpType(IndexRequest.OpType.CREATE);
            bulkProcessor.add(indexRequestBuilder.request());
            total++;
        }
        System.out.println("Already migrated : " + total + " records!");
        if (searchResponse.getHits().hits().length == 0)
        {
            break;
        }
    }
          //close bulk processor at the end.
          bulkProcessor.close();
```

&lt;/code&gt;
I got no exception thrown out, and the ids of my docs are absolute unique. You guys can easily reproduce the issue using the above code. I don't know whether it's my error using of bulkProcessor or am I missing something? Or it's really a bug of bulkProcessor .

Thanks, 
Spancer
</description><key id="20222612">3802</key><summary>Docs missing while using bulkProcessor migrating index data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spancer</reporter><labels /><created>2013-09-29T02:15:47Z</created><updated>2013-12-16T09:03:10Z</updated><resolved>2013-09-29T08:31:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-29T04:56:14Z" id="25314501">Don't forget to close bulk processor at the end and perhaps wait for some seconds.
Otherwise the pending bulk actions won't be performed.

HTH

## 

David ;-)
Twitter : @dadoonet / @elasticsearchfr / @scrutmydocs

Le 29 sept. 2013 à 05:15, spancer notifications@github.com a écrit :

&gt; Hi Guys, 
&gt; I'm using bulkProcessor to migrate index data from one cluster to another. To enhance the speed, i used bulkProcessor instead of using single BulkRequestBuilder. And I found the new index doc size is always less than the source cluster one.
&gt; 
&gt; My test code is somewhat like below: 
&gt; 
&gt; int total = 0;
&gt; int pageSize = 50;
&gt; String oldIndexName = "testold";
&gt; String newIndexName = "testnew";
&gt; String indexDocType = "test";
&gt; Client sourceclient = ClientUtil.getSourceTransportClient();
&gt; Client targetclient = ClientUtil.getTargetTransportClient();
&gt; SearchResponse searchResponse = sourceclient.prepareSearch(oldIndexName).setSearchType(SearchType.SCAN)
&gt; .setQuery(matchAllQuery()).setSize(pageSize).setScroll(TimeValue.timeValueSeconds(20)).execute()
&gt; .actionGet();
&gt; ImmutableMap map = targetclient.admin().cluster().prepareState().execute().actionGet()
&gt; .getState().getMetaData().getIndices();
&gt; if (!map.containsKey(newIndexName))
&gt; targetclient
&gt; .admin()
&gt; .indices()
&gt; .prepareCreate(newIndexName)
&gt; .setSettings(
&gt; settingsBuilder().put("index.number_of_replicas", 0).put("index.refresh_interval", "-1"))
&gt; .execute().actionGet();
&gt; Thread.sleep(200);
&gt; BulkProcessor bulkProcessor = BulkProcessor.builder(targetclient, new BulkProcessor.Listener()
&gt; {
&gt; 
&gt; ```
&gt;     @Override
&gt;     public void beforeBulk(long executionId, BulkRequest request)
&gt;     {
&gt; 
&gt;     }
&gt; 
&gt;     @Override
&gt;     public void afterBulk(long executionId, BulkRequest request, BulkResponse response)
&gt;     {
&gt;         if (response.hasFailures())
&gt;         {
&gt;             throw new RuntimeException("BulkResponse show failures: " + response.buildFailureMessage());
&gt;         }
&gt;     }
&gt; 
&gt;     @Override
&gt;     public void afterBulk(long executionId, BulkRequest request, Throwable failure)
&gt;     {
&gt;         throw new RuntimeException("Caught exception in bulk: " + request + ", failure: " + failure, failure);
&gt;     }
&gt; }).setConcurrentRequests(10).build();
&gt; 
&gt; while (true)
&gt; {
&gt;     searchResponse = sourceclient.prepareSearchScroll(searchResponse.getScrollId())
&gt;             .setScroll(TimeValue.timeValueSeconds(20)).execute().actionGet();
&gt;     for (SearchHit hit : searchResponse.getHits())
&gt;     {
&gt; 
&gt;         IndexRequestBuilder indexRequestBuilder = targetclient.prepareIndex(newIndexName, indexDocType, hit
&gt;                 .getSource().get("_id").toString());
&gt;         indexRequestBuilder.setSource(hit.getSource());
&gt;         indexRequestBuilder.setOpType(IndexRequest.OpType.CREATE);
&gt;         bulkProcessor.add(indexRequestBuilder.request());
&gt;         total++;
&gt;     }
&gt;     System.out.println("Already migrated : " + total + " records!");
&gt;     if (searchResponse.getHits().hits().length == 0)
&gt;     {
&gt;         break;
&gt;     }
&gt; }
&gt; ```
&gt; 
&gt; I got no exception thrown out, and the ids of my docs are absolute unique. You guys can easily reproduce the issue using the above code. I don't know whether it's my error using of bulkProcessor or am I missing something? Or it's really a bug of bulkProcessor .
&gt; 
&gt; Thanks, 
&gt; Spancer
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="spancer" created="2013-09-29T06:55:13Z" id="25315479">Yep. Thanks dodoonet, I'll make some test across it. 
</comment><comment author="spancer" created="2013-09-29T08:31:48Z" id="25316513">Before calling bulk processor, wait some time, it does help. 
</comment><comment author="kimchy" created="2013-09-30T11:43:01Z" id="25353794">@spancer you don't have to wait, you can issue a refresh API call at the end of the bulk indexing process, to make sure you search on the data you already indexed.
</comment><comment author="dadoonet" created="2013-09-30T13:29:15Z" id="25361115">Ah thanks @kimchy!
Stupid me. 
</comment><comment author="spancer" created="2013-12-16T09:03:09Z" id="30645011">@kimchy  Thanks, shay.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ClassCastException when terms facet produces high counts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3801</link><project id="" key="" /><description>Looks like every terms facet that has to produce high term counts fails with this ClassCastException. This is 100% reproducible for me with this field.

If i narrow the query to a smaller result set it works because it seems like it does not have to cast.

ElasticSearch v0.90.5 on OSX 10.8.4.

```
POST http://localhost:9200/graylog2_*/_search

{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "min",
                "size" : 1    
            }    
        }
    }
}
```

The query returns:

```
{
    error: ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: ClassCastException[org.elasticsearch.search.facet.terms.longs.InternalLongTermsFacet$LongEntry cannot be cast to org.elasticsearch.search.facet.terms.doubles.InternalDoubleTermsFacet$DoubleEntry]; 
    status: 503
}    
```

From the ElasticSearch log:

```
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] 
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:182)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:156)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:407)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ClassCastException: org.elasticsearch.search.facet.terms.longs.InternalLongTermsFacet$LongEntry cannot be cast to org.elasticsearch.search.facet.terms.doubles.InternalDoubleTermsFacet$DoubleEntry
    at org.elasticsearch.search.facet.terms.doubles.InternalDoubleTermsFacet.reduce(InternalDoubleTermsFacet.java:182)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:340)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:193)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:180)
    ... 8 more
```
</description><key id="20220608">3801</key><summary>ClassCastException when terms facet produces high counts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lennartkoopmann</reporter><labels /><created>2013-09-28T22:43:57Z</created><updated>2014-08-08T17:41:01Z</updated><resolved>2014-08-08T17:41:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="the100rabh" created="2014-04-09T06:31:07Z" id="39932525">I too am facing similar issues with latest version of ES 1.1 
Weirdest part is the results depend on the ES node that I am trying to get the result from. This is a tip of a bigger issue I feel and should be looked into in priority
</comment><comment author="clintongormley" created="2014-04-15T11:58:38Z" id="40473178">cc @jpountz 
</comment><comment author="jpountz" created="2014-04-15T12:03:10Z" id="40473520">@the100rabh @lennartkoopmann Can you give more information about your mappings? I'm curious if the field that you are using for the terms facet is mapped as a long/int on some types and as a float/double on other types?
</comment><comment author="lennartkoopmann" created="2014-04-15T14:15:44Z" id="40486161">That is too long ago and I unfortunately cannot provide the mapping anymore. Sorry. :neutral_face:
</comment><comment author="the100rabh" created="2014-04-16T06:54:40Z" id="40568603">Complete Mapping 

```
{
    "2014-04-08" : {
        "mappings" : {
            "bidder" : {
                "properties" : {
                    "bidrequest_app_bundle" : {
                        "type" : "string"
                    },
                    "bidrequest_app_cat_0" : {
                        "type" : "string"
                    },
                    "bidrequest_app_cat_1" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_mraid_0_functions_0" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_mraid_0_functions_1" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_mraid_0_functions_2" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_mraid_0_functions_3" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_mraid_0_version" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_nativebrowserclick" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_banner_ext_video_linearity" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_banner_ext_video_maxduration" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_banner_ext_video_minduration" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_banner_ext_video_type_0" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_ext_video_type_1" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_banner_h" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_banner_pos" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_banner_w" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_bidfloor" : {
                        "type" : "double"
                    },
                    "bidrequest_imp_0_displaymanager" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_displaymanagerver" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_id" : {
                        "type" : "string"
                    },
                    "bidrequest_imp_0_instl" : {
                        "type" : "long"
                    },
                    "bidrequest_imp_0_tagid" : {
                        "type" : "string"
                    },
                    "bidrequest_site_cat_0" : {
                        "type" : "string"
                    },
                    "bidrequest_site_cat_1" : {
                        "type" : "string"
                    },
                    "bidrequest_site_cat_2" : {
                        "type" : "string"
                    },
                    "bidrequest_site_cat_3" : {
                        "type" : "string"
                    },
                    "bidrequest_site_content_keywords_0" : {
                        "type" : "string"
                    },
                    "bidrequest_site_content_keywords_1" : {
                        "type" : "string"
                    },
                    "bidrequest_site_content_keywords_2" : {
                        "type" : "string"
                    },
                    "bidrequest_site_domain" : {
                        "type" : "string"
                    },
                    "bidrequest_site_id" : {
                        "type" : "string"
                    },
                    "bidrequest_site_name" : {
                        "type" : "string"
                    },
                    "bidrequest_site_page" : {
                        "type" : "string"
                    },
                    "bidrequest_site_privacypolicy" : {
                        "type" : "long"
                    },
                    "bidrequest_site_publisher_id" : {
                        "type" : "string"
                    },
                    "bidrequest_site_publisher_name" : {
                        "type" : "string"
                    },
                    "bidrequest_site_ref" : {
                        "type" : "string"
                    },
                    "bidrequest_testing" : {
                        "type" : "boolean"
                    },
                    "bidrequest_tmax" : {
                        "type" : "long"
                    },
                    "bidrequest_user_buyeruid" : {
                        "type" : "string"
                    },
                    "bidrequest_user_gender" : {
                        "type" : "string"
                    },
                    "bidrequest_user_id" : {
                        "type" : "string"
                    },
                    "bidrequest_user_keywords" : {
                        "type" : "string"
                    },
                    "bidrequest_user_yob" : {
                        "type" : "long"
                    },
                    "bidrequest_wseat_0" : {
                        "type" : "string"
                    },
                    "bidresponse_bidid" : {
                        "type" : "string"
                    },
                    "bidresponse_cur" : {
                        "type" : "string"
                    },
                    "bidresponse_id" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_adid" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_adm" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_adomain" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_cid" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_crid" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_id" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_impid" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_iurl" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_nurl" : {
                        "type" : "string"
                    },
                    "bidresponse_seatbid_0_bid_0_price" : {
                        "type" : "double"
                    },
                    "bidresponse_seatbid_0_group" : {
                        "type" : "long"
                    },
                    "bidresponse_seatbid_0_seat" : {
                        "type" : "string"
                    }
                }
            }
        }
    }
}

```

The query(as generated by Kibana)

```
curl -XGET 'http://localhost:9200/2014-04-08/_search?pretty' -d '{
  "facets": {
    "terms": {
      "terms": {
        "field": "bidresponse_seatbid_0_bid_0_price",
        "size": 40,
        "order": "count",
        "exclude": []
      },
      "facet_filter": {
        "fquery": {
          "query": {
            "filtered": {
              "query": {
                "bool": {
                  "should": [
                    {
                      "query_string": {
                        "query": "*"
                      }
                    }
                  ]
                }
              },
              "filter": {
                "bool": {
                  "must": [
                    {
                      "match_all": {}
                    }
                  ]
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}'
```
</comment><comment author="hjz" created="2014-04-24T09:12:03Z" id="41259048">@jpountz seeing this issue as well. We have a field mapped as a Long on 1 type and String as another type, likely a requirement of the issue.
</comment><comment author="clintongormley" created="2014-08-08T17:41:01Z" id="51634067">Fields with the same name in different types in the same index should be mapped in the same way.  In the future we will be enforcing this.  Closing in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added generic cluster state update ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3800</link><project id="" key="" /><description>Added new AckedClusterStateUpdateTask interface that can be used to submit cluster state update tasks and allows actions to be notified back when a set of (configurable) nodes have acknowledged the cluster state update. Supports a configurable timeout, so that we wait for acknowledgement for a limited amount of time (will be provided in the request as it curently happens, default 10s).

Internally, a low level AckListener is created (InternalClusterService) and passed to the publish method, so that it can be notified whenever each node responds to the publish request. Once all the expected nodes have responded or the timeoeout has expired, the AckListener notifies the action which will return adding the proper acknowledged flag to the response.

Ideally, this new mechanism will gradually replace the existing ones based on custom endpoints and notifications (per api).

Once we get this in I'll start adding this new mechanism to the apis that don't currently support acknowledgements when changing the cluster state (e.g. put and delete warmer etc.).

Closes #3786
</description><key id="20218847">3800</key><summary>Added generic cluster state update ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-09-28T20:43:02Z</created><updated>2014-07-16T21:52:13Z</updated><resolved>2013-10-15T15:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-15T15:51:09Z" id="26346736">Merged and backported to 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>HistogramFacet with floating point intervals and offset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3799</link><project id="" key="" /><description>Currently only integer value (long) intervals are supported.

Useful when storing measurements in SI units and allowing users to get a histogram using custom units and intervals (e.g. 1lb=0.45kg).
</description><key id="20215776">3799</key><summary>HistogramFacet with floating point intervals and offset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels /><created>2013-09-28T17:33:25Z</created><updated>2014-08-08T08:32:28Z</updated><resolved>2014-08-08T08:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ejain" created="2013-10-28T06:49:46Z" id="27192104">Implemented in https://github.com/zenobase/decimal-histogram-facet
</comment><comment author="jpountz" created="2014-08-08T08:32:28Z" id="51576140">Closing in favor of https://github.com/elasticsearch/elasticsearch/issues/4847
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Resolved issue #3797</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3798</link><project id="" key="" /><description /><key id="20205197">3798</key><summary>Resolved issue #3797</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eltu</reporter><labels /><created>2013-09-28T03:07:24Z</created><updated>2014-07-16T21:52:14Z</updated><resolved>2013-09-29T14:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>multi_match lenient query with boosted field crashes with NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3797</link><project id="" key="" /><description>A lenient multi_match query with a boosted field with type mismatch crashes. Simple example:

`curl -XPUT http://localhost:9200/blog/post/1?pretty=1 -d '{"foo":123, "bar":"xyzzy"}'`
`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d '{"multi_match": {"fields": ["foo^2", "bar"], "lenient": true, "query": "xyzzy"}}' # crashes with NullPointerException`

Interestingly, it works for internal _id field:

`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d '{"multi_match": {"fields": ["_id^2", "bar"], "lenient": true, "query": "xyzzy"}}' # works`

And it doesn't crash when there's no type mismatch:

`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d '{"multi_match": {"fields": ["foo^2", "bar"], "lenient": true, "query": "123"}}' # works`

Other queries for reference:

`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d '{"multi_match": {"fields": ["foo", "bar"], "lenient": true, "query": "xyzzy"}}' # works`
`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d '{"multi_match": {"fields": ["foo", "bar"], "query": "xyzzy"}}' # crashes with NumberFormatException - expected`
</description><key id="20192391">3797</key><summary>multi_match lenient query with boosted field crashes with NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">IlyaSemenov</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-27T20:15:16Z</created><updated>2013-10-24T22:49:54Z</updated><resolved>2013-10-24T22:49:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eltu" created="2013-09-27T23:21:51Z" id="25284021">More information about error:

Happen the error in class LongFieldMapper.java in the method parsevalue(). The cause of error is a NumberFormatExpcetion, becuase the value(xyzzy) arrive in the method is String.

java.lang.NumberFormatException: For input string: "xyzzy"
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Fixed NPE in multi_match query when using lenient and field weight</comment></comments></commit></commits></item><item><title>Fix comment grammar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3796</link><project id="" key="" /><description /><key id="20188799">3796</key><summary>Fix comment grammar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">benmccann</reporter><labels /><created>2013-09-27T19:04:09Z</created><updated>2014-07-16T21:52:14Z</updated><resolved>2013-10-09T08:08:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-09T08:08:15Z" id="25953345">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>geo_shape filter yields 0 results for exact point match.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3795</link><project id="" key="" /><description>Given a document with a point geometry attribute, performing a geo_shape filter with that exact point as the query coordinate yields no results. I've included a reproducible case below.

I've also verified that the same query successfully yields results if the indexed shapes are polygons, and that providing a polygon as the query coordinates will correctly find the point document.

ES Version: 0.90.5

```
# create a test index
curl -XPUT  http://localhost:9200/test

# specify mapping with geo_shape type
curl -XPUT http://localhost:9200/test/testtype/_mapping -d'{
    "testtype": {
    "properties": {
        "geometry": {
            "type": "geo_shape",
            "tree": "quadtree",
            "tree_levels": "14"
        }
    }
    }
}'

# create a point document 
curl -XPUT http://localhost:9200/test/testtype/doc1 -d'
{
    "id": "doc1",
    "geometry": {
        "type": "point",
        "coordinates": [-88.31, 40.05]
    }
}'

# perform a point search, exact match for the inserted document coordinate
# yields 0 results
curl -XPOST "http://localhost:9200/test/testtype/_search" -d'
{
    "query": {
        "filtered": {
            "query": {
                "match_all": {}
            },
            "filter": {
                "geo_shape": {
                    "geometry": {
                        "shape": {
                            "type": "point",
                            "coordinates": [-88.31, 40.05]
                        }
                    }
                }
            }
        }
    }
}'

# envelope search around the area yields the doc 
curl -XPOST "http://localhost:9200/test/testtype/_search/" -d'
{
    "query": {
        "filtered": {
            "query": {
                "match_all": {}
            },
            "filter": {
                "geo_shape": {
                    "geometry": {
                        "shape": {
                            "type": "envelope",
                            "coordinates": [
                                [-88.32, 40.06],
                                [-88.31, 40.05]
                            ]
                        }
                    }
                }
            }
        }
    }
}'
```
</description><key id="20187566">3795</key><summary>geo_shape filter yields 0 results for exact point match.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">olimcc</reporter><labels /><created>2013-09-27T18:42:08Z</created><updated>2014-08-08T17:39:05Z</updated><resolved>2014-08-08T17:39:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="olimcc" created="2013-10-04T01:05:01Z" id="25670537">I spent some time debugging this and it looks like the problem is at the Lucene level. Here's the related bug:

https://issues.apache.org/jira/browse/LUCENE-4978
</comment><comment author="dsmiley" created="2013-10-04T20:21:37Z" id="25727413">Yup, that's it @olimcc
</comment><comment author="clintongormley" created="2014-08-08T17:39:05Z" id="51633834">This appears to have been fixed by the Lucene upgrade. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can the init script wait for green?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3794</link><project id="" key="" /><description>Would it be OK to add a configuration flag that forces the init script to wait for green until some timeout?  It'd be nice to be able to just restart elasticsearch and wait for the init script to return successfuly to know that it worked.  Not everyone is going to want it, and no one is going to want it all the time but it'd still be nice.
</description><key id="20178739">3794</key><summary>Can the init script wait for green?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-27T15:52:53Z</created><updated>2013-09-27T18:19:23Z</updated><resolved>2013-09-27T18:19:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-27T16:43:40Z" id="25259165">Maybe something like

``` bash
/etc/init.d/elasticsearch waitforgreen
```
</comment><comment author="kimchy" created="2013-09-27T17:14:50Z" id="25261317">Unsure about this one, feels like it's going beyond what the init is supposed to do? 
</comment><comment author="s1monw" created="2013-09-27T18:15:19Z" id="25265486">I mean we can add some of these utilities of some sort to the packages but I don't think init.d should contain that?
</comment><comment author="nik9000" created="2013-09-27T18:19:10Z" id="25265785">Yeah, you are probably right.  It might make sense to link some of the utilities to the status function of the init script, but just having the utilities would be simpler.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Order of parameters in suggest should not be important</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3793</link><project id="" key="" /><description>The following works:

```
curl -XPUT 'http://localhost:9200/test/test/1?pretty=1' -d '
{
   "color" : "green"
}
'


curl -XGET 'http://localhost:9200/_search?pretty=1&amp;size=0' -d '
{
   "suggest" : {
      "my_suggest" : {
         "text" : "green",
         "term" : {
            "field" : "color"
         }
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "suggest" : {
#       "my_suggest" : [
#          {
#             "length" : 5,
#             "options" : [],
#             "text" : "green",
#             "offset" : 0
#          }
#       ]
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```

If you switch the `term` and `text` order, then you get this instead:

```
curl -XGET 'http://localhost:9200/_search?pretty=1&amp;size=0' -d '
{
   "suggest" : {
      "my_suggest" : {
         "term" : {
            "field" : "color"
         },
         "text" : "green"
      }
   }
}
'

SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[u9ErnmcARw2y3uLywGfydQ][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"suggest":{"my_suggest":{"term":{"field":"color"},"text":"green"}}}]]]; nested: ElasticSearchIllegalArgumentException[The required text option is missing]; }{[u9ErnmcARw2y3uLywGfydQ][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"suggest":{"my_suggest":{"term":{"field":"color"},"text":"green"}}}]]]; nested: ElasticSearchIllegalArgumentException[The required text option is missing]; }{[u9ErnmcARw2y3uLywGfydQ][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"suggest":{"my_suggest":{"term":{"field":"color"},"text":"green"}}}]]]; nested: ElasticSearchIllegalArgumentException[The required text option is missing]; }]
```
</description><key id="20172576">3793</key><summary>Order of parameters in suggest should not be important</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2013-09-27T14:10:40Z</created><updated>2013-09-29T12:50:37Z</updated><resolved>2013-09-29T12:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-27T17:14:34Z" id="25261301">@clintongormley I tried to run your curl commands on master, 0.90 branch and 0.90.5 release and in all cases the last curl command works without an error.
</comment><comment author="s1monw" created="2013-09-27T18:04:33Z" id="25264739">@martijnvg I think it should not work on 0.90 but on master though the reason here might be that I didn't backport #3199 - still checking
</comment><comment author="s1monw" created="2013-09-27T18:10:22Z" id="25265134">@martijnvg I think you are right this seems to be fixed long ago... I will still port #3199 to 0.90
</comment><comment author="clintongormley" created="2013-09-29T12:38:30Z" id="25319543">Apologies - misread the version number.  Seems to have been fixed in 0.90.2
</comment><comment author="clintongormley" created="2013-09-29T12:50:37Z" id="25319694">Make that 0.90.3...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow starting multiple ES instance via init scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3792</link><project id="" key="" /><description>if you have really powerful machines, it is advised to start up several elasticsearch nodes inside of that machine.

Right now neither the deb nor the RPM init script support this. An idea is to configure a `NUM_OF_INSTANCES` parameter, which would allow to fire up and shut down more than one elasticsearch instance.
</description><key id="20169570">3792</key><summary>Allow starting multiple ES instance via init scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels /><created>2013-09-27T13:13:05Z</created><updated>2013-10-25T09:54:25Z</updated><resolved>2013-10-25T09:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-25T09:54:25Z" id="27078110">Closing this one again. We should not support this in init scripts, as they are intended to work per process. This adds up unwanted complexity. The correct workaround for this would be simply copy the init scripts and create the appropriate links in the runlevels.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion suggest: support setting different weights per input</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3791</link><project id="" key="" /><description>The completion suggest field mapper today only supports setting a global weight which will be the same for all inputs. It could be useful to support setting different weights per input, depending on how well they match the suggestion.
</description><key id="20158772">3791</key><summary>Completion suggest: support setting different weights per input</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-09-27T08:46:33Z</created><updated>2015-09-21T19:22:57Z</updated><resolved>2015-09-21T19:22:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fl00r" created="2013-10-28T10:58:26Z" id="27202667">Sounds good. In the case of long inputs I prefer to split it in a number of inputs: short, medium and full. Now they have got the same weigth, so they `autocomplete` phrase together, while they should appear one after another.

`How to prepare chocolate at home without milk` will be presented as three tokens:
`How to prepare chocolate`
`How to prepare chocolate at home`
`How to prepare chocolate at home without milk`

When someone prints `How to` he should see all small tokens like 
`How to make pasta`
`How to swim`
`How to prepare chocolate`

so now he choose last and see next suggestions:
`How to prepare chocolate at home`
`How to prepare chocolate for youngest brother`
`How to prepare chocolate without sugar`
etc

So if I could add weights to each input, I'll make it as a function of words count as well, so they will autocomplete one after another.
</comment><comment author="FabianKoestring" created="2014-03-18T09:08:38Z" id="37911609">:+1: 
</comment><comment author="dubadub" created="2014-03-25T12:11:38Z" id="38556624">+1
</comment><comment author="FabianKoestring" created="2014-07-24T14:59:32Z" id="50029729">Is there anyone working on this feature?
</comment><comment author="aykut" created="2015-04-30T13:08:04Z" id="97768854">+1
</comment><comment author="clintongormley" created="2015-09-21T19:22:57Z" id="142084300">The new completion suggester coming in 2.1.0 allows you to index an array of suggestions, each suggestion with its own weight:

```
POST {INDEX_NAME}/{TYPE_NAME}
{
"suggest_name": [
  {
    "input": "name1",
    "contexts" : [...],
    "weight": 7
  },
  {
    "input": "name2",
    "contexts" : [...].
    "weight": 14
  }
  ]
  ...
}
```

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add version to Analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3790</link><project id="" key="" /><description>This relates somewhat to #3775 since we want to change the default analyzer in 1.0 we somehow need to decide which version of an analyzer we need to load for a name in a mapping. For instance if an index was created with `0.90.x` or earlier the _old_ `default` analyzer must be used in order to maintain index backwards compatibility for searches etc.

We don't necessarily need this in `0.90` but for now I flagged it as `0.90` unless it's breaking anything
</description><key id="20156618">3790</key><summary>Add version to Analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-27T07:47:23Z</created><updated>2014-02-06T09:23:30Z</updated><resolved>2013-10-28T17:23:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-10-24T20:18:43Z" id="27026196">+1 to push!
</comment><comment author="nik9000" created="2013-10-25T13:14:10Z" id="27090891">It'd be nice to let the user request a specific version of the prebuilt analyzer.  Something like "standard circa 0.90.5".  That way when I update elasticsearch and build a new index using the prebuilt analyzer I still get the one I expect.
</comment><comment author="s1monw" created="2013-10-25T13:15:35Z" id="27090992">we can do this already, you can specify the lucene version. We will not change `standart` we will only change `default` really.
</comment><comment author="nik9000" created="2013-10-25T13:16:23Z" id="27091036">Sounds good to me.  Thanks!
</comment><comment author="s1monw" created="2013-10-25T13:17:03Z" id="27091085">cool!
</comment><comment author="s1monw" created="2013-10-25T18:59:42Z" id="27117279">looks great! thanks for the additional test. Please push it!
</comment><comment author="spinscale" created="2013-10-26T09:07:08Z" id="27142515">The integration test is actually useless, because I did not test if the corresponding analyzers were really loaded. By doing this I found out, that they were not - because we overwrite the `index.version.created` property always with `Version.CURRENT` when the index is created.

Fixed this, please check again in a second
</comment><comment author="spinscale" created="2013-10-26T11:18:13Z" id="27144239">Updated. Crucial change (allows to configure the index version) in https://github.com/spinscale/elasticsearch/commit/853f64a3ae16083d573eacb0a6ef40943ec5ba7c#diff-beee1cd752cd81f3779332628d5b296eL255 

Not sure if we want to leave this in?
</comment><comment author="s1monw" created="2013-10-28T10:00:19Z" id="27199599">I think that is ok though - I don't think we guarantee anything here so that is fine?

simon
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactory.java</file><file>src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java</file></files><comments><comment>Add version to prebuilt analyzers</comment></comments></commit></commits></item><item><title>The Book, The Documentation and related Community Contributions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3789</link><project id="" key="" /><description>Let me try to follow up on important points from #3775. The purpose of this issue is to formulate constructive questions and restate known facts about the Elasticsearch documentation, the upcoming book and community contribution process (not talking about source code contributions). And possibly clarify confusions and misunderstading. Any comments and further questions are warmly welcome.
#### The Book
1. Elasticsearch team works on a book. Elasticsearch team has been in negotiation with major publisher for some time now and until the deal is closed it is not possible to share any content that has been written so far. Thus it is also not possible to accept contributions or any public feedback now.
2. If it is that hard to close the deal and it takes a lot of time to finalise it, is it then worth the effort at all? Wouldn't it be simply more useful to consider a different means of publishing? (Still, looking from the point of the community)
3. Is there any plan to allow community contributions to the book now or in the future?
#### The Documentation
1. What is the relation of the book and the documentation? If the book is published, does it mean it will replace current documentation or some parts of it? Meaning current documentation content will be thrashed? (Available only in git repo history)
2. If it is publicly unknown what the book license, scope and relation to the documentation will be isn't there a risk that this uncertainty will discourage ppl from contributing to existing documentation? (Because their contribution effort could be virtually wasted anytime once the book deal is closed)
#### Community Contributions
1. As of now, contributing to the documentation/guide is possible via GitHub PR. It is up to committers (effectively Elasticsearch.com employees) to judge if it can be accepted. However, if it is not accepted a clear and honest explanation should be provided in comments. That is IMO the only way how to help the contributor do better next time and help cultivate contribution culture going forward.
2. If accepted contribution is found outdated or buggy later a fix should be preferred over silent content drop. At least it should be possible for the original/main contributor to find out what was wrong and what was the reason of content removal.
3. There used to be a Tutorials section in the past (seems now partly moved to a .org/blog). Are people welcome to contribute here too? The point here is that some contributions may simply not fit existing guide format (of example the content is too long or it covers more topics). If people want to contribute such content, what is the process?
4. The more detailed and practically oriented the contributed content is the better. One should not really worry about the fact that his/her contribution will not be accepted because it might be too overlapping or competitive to commercial training materials. (I am not saying training materials should be freely available, but what if someone creates good material and wants to share it with others via official channel?)
</description><key id="20154793">3789</key><summary>The Book, The Documentation and related Community Contributions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-09-27T06:44:45Z</created><updated>2014-03-26T09:06:24Z</updated><resolved>2014-03-26T09:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-27T08:04:32Z" id="25229095">Here are my thoughts:
- Book: We are working on it, all issues seem to be resolved though it did take time. Once we can, we will publish the first content of the book. The book content will be available publicly, and we will accept pull requests for it, though at the initial beta phase of the book, I suspect we will move fast and things will change quickly. The aim of the book will be to provide context based documentation.
- The guide: The aim for the guide is to remain a reference guide, with the book covering the more of a context based documentation. We happily accept pull request, but they need to be good, and something that we can feel as a community that we can maintain if the contributor of them goes away. The guide needs a lot more work, for example, having good examples on each analyzer. We have spent a lot of time moving all the current guide to the docs folder under the repo, to make sure it will be properly versioned, and a pull request will now also contain changes to the doc including the code. The guide will remain there and heavily maintained even when the book comes out.
- The thought around tutorials is to start, under docs (on elasticsearch repo) a cookbook like section. The problems with tutorials is that its another high maintenance section of docs. We want to make sure they won't go out of date. Thats why, btw, we moved to the docs folder in the main repo (and build a whole infrastructure around it for publishing), so it will be versioned, and hopefully more easily maintained by "moving with the code". The decision on which cookbook to add will require a lot of thought, to make sure its of high quality and well structured, compared to just one offs here and there that are forgotten.
  
  The main reason I am cautious here is the bad experience I had with the old tutorials section, where content got contributed, went out of date, and it fell on me (at the time) to make sure its up to date, because the contributor was not around, was busy, or did not want to contribute any more.
- There is _no_ thought around overlapping or competing content compared to training. If there is good content, we will add it (one would say that the book and the guide compete with training materials, or blog posts, thats pure nonsense, we went through great lengths to make sure the book will be available online). 

I want to stress the point that when something goes into the docs, its going to have to be well maintained so users won't get out of date information. Its important not to confuse documentation for a project with blog posts...
</comment><comment author="lukas-vlcek" created="2013-09-27T12:28:29Z" id="25241505">Thanks Shay,
I would like apology to you, Elasticsearch folks and rest of the community if I acted offensively. It was very stupid of me.
</comment><comment author="kimchy" created="2013-09-27T12:49:21Z" id="25242588">I do think that opening the discussion here is great, and if people are not aware of it, then its good that its in the open. My point was only that the discussion should not come from the aspect of there is some sort of malice, and thats why it doesn't happen. It might be a language barrier, but this is how it came out to me in some of your points. I encourage everybody to chip in with ways how to improve elasticsearch, but without the baggage of assumptions, just how things can be done better, and what can and sometimes can't be done around it (like the book).

The book is a good example, we can try and be public about everything around it, but in practice we can't until things are properly signed and agreed upon with the publisher. The publisher has been amazing, btw, its just how those things work, and they do take their time.

I think that this discussion brought out a good point, regarding how to view tutorials, and my thought around creating cookbooks under the docs filter in our repo. I will reopen it and see if other people would like to chip in.
</comment><comment author="benmccann" created="2013-09-27T19:10:48Z" id="25269474">I think it's much better to document and have it go out-of-date than it is to be cautious about this and have less documentation. If there is an out-of-date example then at least I have somewhere to start. It's usually much easier to figure out the new way to do something based off the old way than it is to figure out the new way based off of nothing.
</comment><comment author="benmccann" created="2013-09-27T19:12:04Z" id="25269553">Speaking of documentation, JavaDocs on the major user interfaces would be great too ;-)  https://github.com/elasticsearch/elasticsearch/issues/3787
</comment><comment author="lukas-vlcek" created="2013-09-28T07:48:05Z" id="25293735">@benmccann Thanks for bringing JavaDoc topic out. I did not originally wanted to mix it into this ticket because IMO JavaDoc is a big chapter. I hope what I am going to say will be understood as a constructive criticism.

When I created first prototype of our application based on ES we decided we want to invest more into this work and involve more people on our side into development. When people in our team saw what ES is capable of doing we were excited and as many developers they wanted to get their hands on it (at least I hope :smile:). We decided to use Rivers at some point because they seemed to be a good fit for want we wanted and this meant we needed to dive into this thing... and missing JavaDoc turned out to be a pain because we needed a lot more than simple River `start()` and `close()`. We needed to get much deeper. I saw people in our team were frustrated and the morale struggled and I felt personal responsibility for this situation. Show me a Java developer who likes missing JavaDoc. It is missed opportunity.
</comment><comment author="brusic" created="2013-09-28T17:45:17Z" id="25304343">I opened up an issue for JavaDocs two years ago. :) https://github.com/elasticsearch/elasticsearch/issues/1203

I went slightly off-topic with Simon's issue regarding default analyzers, so let me get off-topic again! I think part of Lukas' frustration is that he remembers when the elasticsearch project was a lot more open. Upcoming features were known and feedback was rapid. With the incorporation of elasticsearch the company, that world is now farther away. I personally have not heard about the book and I read almost every e-mail on the list, as well as most of the issues and commits on GitHub. The elasticsearch project is much bigger right now and there is no returning to 2010. The large volume of commits lately demonstrates the need for many new features and goes beyond what one person and open-source can do (for now).

Getting back on topic, the more documentation, the better. :) Shay, you mentioned that both the book and the guide both will be context based. I think that is one of the current issues with the current documentation is that it is too reference driven. For example, many mailing lists can be answered if there was a section on analysis. Deployment options? Another tutorial. Currently the documentation is viewed from the bottom-up, with documentation focused at the module level, when I think it should be from the top-down. Cookbooks are great for power users such as myself.

I would also add that (hosted) versioned documentation is important as well and deals slightly with having out-of-date information. If a user is asking for help about version 0.19, having correct-for-that-version documentation greatly helps.

BTW, Compass had great documentation.
</comment><comment author="kimchy" created="2013-10-02T22:12:01Z" id="25581565">@brusic: Some thoughts:
- Regarding being more open on features, we always moved quite fast when it came to features. We are also open on the large features planned to 1.0, like aggregations, snapshot/restore, and distributed percolation. Other features are basically coming as they are, based mostly on user feedback (issues, mailing list, ...), and we open issues to reflect them and get feedback. I do take your input, and we will check where we can be more open, sometimes, we do brainstorm on something like Aggregations before opening an issue for it, cause we have no idea how its going to look like, and the issue should include at least first thoughts around it, otherwise don't really have an issue to discuss here... 
- Regarding being open about the book, we simply can't share the plans on how it progress while things are not properly settled. We were open for a few months now that a book is in the works (we mentioned it on webinars, twitter, answers we gave regarding questions on documentation), and we want to have it available for free online, and it will be the basis for our context based documentation. I guess we could have done better with potentially mailing a note to the mailing list as well...
- The reference guide will remain a reference guide, not context based documentation. I think our projects needs both forms of documentation. For example, understand what analysis is, and then have reference docs with all the features of a specific analyzer. I do think we need to have a full review / fresh look of our current reference docs, and improve on them, including more examples. 

@benmccann I disagree that its better to have bad / not working tutorials compared to no tutorials. It creates a lot of frustration for our users (as I experienced). The problem of the need for tutorials is obviously there, and the way I see it, we solve it partially with the book (i.e. what is analysis), and with a curated list of cookbooks in the docs that are well maintained and are part of the code repo to make sure they are properly changed with relevant changes.
</comment><comment author="brusic" created="2013-10-04T18:12:39Z" id="25719246">The official elasticsearch blogs (there are two) never get discussed on the mailing list (I am a news junkie, long live RSS!), but they just posted something about the documentation: http://www.elasticsearch.org/blog/docs-docs-docs/

Not a fan of O'Reilly despite my bookshelf containing nothing but animal covers at one point, but still excited for the book. I hope there are some deep dives. O'Reilly does have quite a few open-sourced books like the Think Stats series.

@kimchy My point was that elasticsearch was more open earlier on, not that it is not open anymore. It is not a complaint, just an observation. The project is great and it seems like every new hire has been top notch. Sorry for going off topic again!

Glad that docs are finally versioned. I have been holding out on some doc fixes because of the transition, but I noticed some have already been fixed.
</comment><comment author="lukas-vlcek" created="2014-03-26T09:06:24Z" id="38662031">I have just learned about public release of "Elasticsearch: The Definitive Guide" from http://www.elasticsearch.org/blog/elasticsearch-definitive-guide/. Congrats on this to @clintongormley &amp; @polyfractal !
Closing this ticket. Feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>scripting preloaded scripts error?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3788</link><project id="" key="" /><description>script stored path &lt;code&gt;/usr/share/elasticsearch/config/scripts/demo/indexSort.js&lt;/code&gt;? 

``` bash
curl -X GET 'http://localhost:9200/activities/_search?pretty' -d '{
   "query": {
     "custom_score": {
       "query": {
         "query_string":{
            "default_field" : "activity_type",
            "query": "auction"
         }
       },
       "script": "demo_indexSort",
       "lang": "js"
     }
   },
   "sort": [{
     "_score": {            
       "order": "desc"
     }
   }],
   "size": 40
 }'
```

error log file

``` bash
[2013-09-27 14:30:31,673][DEBUG][action.search.type       ] [Styx and Stone] [activities][4], node[kMWrjokMT_qGBDnuHbu7Eg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2ed021d9]
org.elasticsearch.search.query.QueryPhaseExecutionException: [activities][4]: query[custom score (activity_type:auction,function=script[demo_indexSort], params [null])],from[0],size[40]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:206)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:193)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: org.mozilla.javascript.EcmaError: ReferenceError: "demo_indexSort" is not defined. (Script1.js#1)
```
</description><key id="20154481">3788</key><summary>scripting preloaded scripts error?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">huxinghai1988</reporter><labels /><created>2013-09-27T06:32:15Z</created><updated>2013-10-02T03:53:51Z</updated><resolved>2013-10-02T03:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-10-01T02:05:17Z" id="25421188">Did you restart elasticsearch after creating this script? Does user that elasticsearch is running under have read permissions for this file and demo directory?
</comment><comment author="huxinghai1988" created="2013-10-02T03:53:51Z" id="25511833">script file directory should is &lt;code&gt;/etc/elasticsearch/scripts/demo/indexSort.js&lt;/code&gt;
thank
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document River start/close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3787</link><project id="" key="" /><description>I'm helping to maintain the MongoDB ElasticSearch river. One thing I'm very unsure about is the conditions under which River.start and River.close will be called. Are these user or system initiated? Can you call start again after close has been called?

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/river/River.java
</description><key id="20154364">3787</key><summary>Document River start/close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels /><created>2013-09-27T06:27:57Z</created><updated>2013-11-12T07:39:33Z</updated><resolved>2013-11-10T20:01:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="benmccann" created="2013-11-12T00:20:33Z" id="28257213">thanks!
</comment><comment author="lukas-vlcek" created="2013-11-12T07:39:33Z" id="28274040">Thanks @javanna
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/River.java</file></files><comments><comment>Added docs for River start and close methods</comment></comments></commit></commits></item><item><title>Generic cluster state update ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3786</link><project id="" key="" /><description>Most of the apis that allow to make changes to the cluster state (e.g. delete index, put mapping, open/close index) currently support an acknowledgement mechanism.

When it comes to updating the cluster state, the update request (e.g. open/close index, put mapping etc.) is processed on the master, then the updated cluster state is pushed to all the other nodes. The json response contains a boolean `acknowledged` flag that tells whether the cluster state change has already been applied by all nodes. The master node waits (maximum 10 seconds, configurable per request) for an ack message from each node. Those ack notifications are api specific, although similar given their same purpose.

The goal of this issue is to add support for a generic ack mechanism that can be reused (and added where missing) in all apis  that update the cluster state. Ideally, the new mechanism should work at a lower level and consists of a listener that is called whenever we get a response directly to the publish request, instead of having an additional endpoint per api and waiting for custom notifications asynchronously.

This relates to the work done in #3736, as the master publish request gets now a response from each node when the new cluster state has already been processed (not only when it was received and the update was locally submitted, but not necessarily processed yet). 

Note that the ack mechanism needs to be completely detached from the 5 seconds wait introduced with #3736, whose goal was to try and wait for replies before processing another cluster state update on the master. Those same replies would become our generic ack messages, which determine the value of the `acknowledged` flag in the response: `true` if all nodes acked the cluster state update, `false` if the configurable timeout expired.
</description><key id="20132383">3786</key><summary>Generic cluster state update ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-26T19:49:45Z</created><updated>2013-12-11T21:15:43Z</updated><resolved>2013-10-15T15:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/delete/RestDeleteWarmerAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/LocalGatewayIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file></files><comments><comment>Delete warmer api to support acknowledgements</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/LocalGatewayIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file></files><comments><comment>Put warmer api to support acknowledgements</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/AckedClusterStateUpdateTask.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/discovery/AckClusterStatePublishResponseHandler.java</file><file>src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java</file><file>src/main/java/org/elasticsearch/discovery/ClusterStatePublishResponseHandler.java</file><file>src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoveryService.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file></files><comments><comment>Added generic cluster state update ack mechanism</comment></comments></commit></commits></item><item><title>service.bat doesn't properly set the memory limits for installed services</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3785</link><project id="" key="" /><description>Currently the memory options are passed on as java options but these are ignored by the jvm.dll. They need to be extracted and passed through different arguments so they can be used before starting the java process.
Unfortunately this also means doing some conversion (from GB to MB and MB to KB).
</description><key id="20125918">3785</key><summary>service.bat doesn't properly set the memory limits for installed services</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-26T18:04:10Z</created><updated>2013-10-13T09:55:46Z</updated><resolved>2013-09-26T20:43:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>service.bat properly configures mem settings</comment></comments></commit></commits></item><item><title>Add indexUUID to mapping-updated and mapping-refresh events and make sure they are applied to an index with same UUID.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3784</link><project id="" key="" /><description>This can go wrong if indices with the same name are repeatably created and deleted.

Also - some minor clean up in ShardStateAction where shard started events could be added twice to the to-be-applied list where the second instance will be ignored.

Closes #3783
</description><key id="20116659">3784</key><summary>Add indexUUID to mapping-updated and mapping-refresh events and make sure they are applied to an index with same UUID.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-09-26T15:25:32Z</created><updated>2014-06-23T14:20:23Z</updated><resolved>2013-09-27T12:43:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-09-27T12:43:48Z" id="25242304">merged.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use the new index UUID to ensure mapping update events from older indices are not applied to new indices with the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3783</link><project id="" key="" /><description>This can currently happen if an index is repeatedly created by an indexing operation and deleted quickly afterwards. 
</description><key id="20113236">3783</key><summary>Use the new index UUID to ensure mapping update events from older indices are not applied to new indices with the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-26T14:34:55Z</created><updated>2013-09-27T12:42:27Z</updated><resolved>2013-09-27T12:42:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/index/cache/id/SimpleIdCacheTests.java</file></files><comments><comment>Add indexUUID to mapping-updated and mapping-refresh events and make sure they are applied to an index with same UUID.</comment></comments></commit></commits></item><item><title>Update-mapping event is not sent if the subsequent indexing operation run into trouble</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3782</link><project id="" key="" /><description>If an index request updated the mapping but failed when actually indexing into a shard, the mapping update was not sent to master.
</description><key id="20106392">3782</key><summary>Update-mapping event is not sent if the subsequent indexing operation run into trouble</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-26T12:26:56Z</created><updated>2014-01-08T17:29:32Z</updated><resolved>2013-09-26T12:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/cluster/RiverClusterService.java</file><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file></files><comments><comment>Schedule retry if the river type is available but the _meta document isn't</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleDeleteMappingTests.java</file></files><comments><comment>Send the update-mapping events before actually indexing into the shard, because the latter may generate exceptions (like when the shard is not yet ready to accept new docs).</comment></comments></commit></commits></item><item><title>Random startup / connection failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3781</link><project id="" key="" /><description>Hello,

Elasticsearch has been part of [Semaphore](https://semaphoreapp.com)'s CI build platform since January 2013. Over time and with all versions (we're currently [running](http://docs.semaphoreapp.com/version-information) 0.90.2 inside Ubuntu 12.04 LXC containers) our users randomly get connection errors such as:

```
Skipping index creation, cannot connect to Elasticsearch
(The original exception was: #&lt;Errno::ECONNREFUSED: Connection refused - connect(2)&gt;)
```

User who's log I'm getting this from is running Tire 0.5.8.

This is how we set up Elasticsearch:

```
cd /tmp
wget http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.2.tar.gz -O elasticsearch.tar.gz
tar -xf elasticsearch.tar.gz
rm elasticsearch.tar.gz
mv elasticsearch-* elasticsearch
mv elasticsearch /usr/local/share
curl -L https://github.com/elasticsearch/elasticsearch-servicewrapper/tarball/master | tar -xz
mv *servicewrapper*/service /usr/local/share/elasticsearch/bin/
rm -Rf *servicewrapper*
/usr/local/share/elasticsearch/bin/service/elasticsearch install
ln -s `readlink -f /usr/local/share/elasticsearch/bin/service/elasticsearch` /usr/local/bin/rcelasticsearch
service elasticsearch start
```

In some cases we've even told users to run another `sudo service elasticsearch stop/start` themselves before the build, and even though the output of that looks good, the problem may still happen.

I'd really love to get some clue into why this is happenning. I would appreciate any help.
</description><key id="20054880">3781</key><summary>Random startup / connection failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">markoa</reporter><labels /><created>2013-09-25T16:49:17Z</created><updated>2014-08-08T17:36:49Z</updated><resolved>2014-08-08T17:36:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T11:54:38Z" id="27382876">Hey,

sorry for getting back late to you. Lets try to debug your issue a bit

First, the error message is pretty clear. Meaning no service is listening on that port.
From what I see in your logs, noone is taking into account, that starting up elasticsearch can take a couple of seconds (just guessing, maybe you are waiting some time)? Could this be a problem , or do the connection refused messages happen sometime inbetween?

What is the output of `service elasticsearch start` in your case?
</comment><comment author="spinscale" created="2013-12-02T09:03:29Z" id="29602760">Hey,

did you have any possibility to debug this issue further?
</comment><comment author="markoa" created="2013-12-02T16:52:14Z" id="29634915">Hi,

Now I'm sorry for a late response, but from now on I'll be able to provide timely input.

The way we set up ElasticSearch is now based on the official deb package: https://gist.github.com/markoa/1b73e8367aeade7bf50e

This is a pre-packaged VM template so the service should already be running when it boots. This is the output of `service elasticsearch start` during the build:

```
* Starting ElasticSearch Server
* Already running.       [ OK ]
```
</comment><comment author="spinscale" created="2014-07-18T10:23:05Z" id="49416532">Do you still have this issue (especially with newer debian packages)? From here it looks as if Elasticsearch was already running back then. Maybe you can evaluate? Is it shown as running but does not?
</comment><comment author="markoa" created="2014-07-18T14:57:20Z" id="49440242">It's been a really long time... we haven't had similar reports lately.

Yes the main point is, we always start ElasticSearch and there is corresponding output but somehow sometimes there would be a connection error. I know it's a long shot.

What do you mean by "evaluate" btw?
</comment><comment author="clintongormley" created="2014-08-08T17:36:49Z" id="51633541">It sounds like this is no longer an issue ,so I'm going to close.  Feel free to reopen if you see this again
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add more anchor links to documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3780</link><project id="" key="" /><description>Related to #3679
</description><key id="20053385">3780</key><summary>Add more anchor links to documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-09-25T16:26:15Z</created><updated>2014-12-12T16:27:40Z</updated><resolved>2013-09-30T19:12:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-09-26T03:32:45Z" id="25141646">@dakrone YTMND! :)
</comment><comment author="javanna" created="2013-09-26T22:36:13Z" id="25209750">+1
</comment><comment author="dakrone" created="2013-09-30T19:12:13Z" id="25394274">Merged in 0442b737be0f0bf11958ebf7facacfff674523cd
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>date_histogram facet with numeric interval is incorrect for negative timestamps (pre-1970)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3779</link><project id="" key="" /><description>Here are 4 musicians born in different decades:

```
curl -XPUT http://localhost:9200/musicians/will/jones -d '{"name": "Will Jones", "born": "1928-05-14"}'
curl -XPUT http://localhost:9200/musicians/will/smith -d '{"name": "Will Smith", "born": "1968-09-25"}'
curl -XPUT http://localhost:9200/musicians/will/iam -d '{"name": "will.i.am", "born": "1975-03-15"}'
curl -XPUT http://localhost:9200/musicians/will/young -d '{"name": "Will Young", "born": "1982-01-20"}'
```

To facet these by decade I am using `interval: 315576000000` although I realise this will result in boundaries which are out by a few days per decade (due to leap years/seconds).

```
curl http://localhost:9200/musicians/will/_search -d '{ "query": { "match_all": {} }, "facets": { "year": { "date_histogram": { "field": "born", "interval": 315576000000 } } } }' | python -mjson.tool

[…],
"facets": {
    "year": {
        "_type": "date_histogram",
        "entries": [
            {
                "count": 1,
                "time": -1262304000000
            },
            {
                "count": 2,
                "time": 0
            },
            {
                "count": 1,
                "time": 315576000000
            }
        ]
    }
},
[…],
```

Note that the facet has merged two results into the facet with time=0 (1970s). Also, Will Jones has been put into the 1930s bucket.

```
ruby -e 'puts Time.at(-1262304000000 / 1000)'
1930-01-01 00:00:00 +0000
```

It seems like timestamps have been grouped by rounding negative numbers towards zero rather than rounding down?

For reference:

```
curl -XGET http://localhost:9200/
{
  "ok" : true,
  "status" : 200,
  "name" : "Obliterator",
  "version" : {
    "number" : "0.90.5",
    "build_hash" : "c8714e8e0620b62638f660f6144831792b9dedee",
    "build_timestamp" : "2013-09-17T12:50:20Z",
    "build_snapshot" : false,
    "lucene_version" : "4.4"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="20050953">3779</key><summary>date_histogram facet with numeric interval is incorrect for negative timestamps (pre-1970)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gareth</reporter><labels /><created>2013-09-25T15:49:55Z</created><updated>2014-01-22T11:51:19Z</updated><resolved>2014-01-22T11:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-22T11:51:19Z" id="33014904">This issue has been fixed in [date histogram aggregations](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-datehistogram-aggregation.html).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Quickly deleting and creating an index with the same name could cause errors due to lingering shard state change events</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3778</link><project id="" key="" /><description>In some rare occasions shard started/failed events from the previous index that still needed to be processed will be applied to the new index routing table.
</description><key id="20047226">3778</key><summary>Quickly deleting and creating an index with the same name could cause errors due to lingering shard state change events</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-25T15:02:58Z</created><updated>2013-09-26T14:31:12Z</updated><resolved>2013-09-25T18:33:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/index/service/IndexService.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/index/cache/id/SimpleIdCacheTests.java</file></files><comments><comment>Introduced an index UUID which is added to the index's settings upon creation. Used that UUID to verify old and delayed shard started/failed events are not applied to newer indexes with the same name.</comment></comments></commit></commits></item><item><title>http://www.elasticsearch.org/contributing-to-elasticsearch/ tells people to contribute to elasticsearch.github.com</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3777</link><project id="" key="" /><description>http://www.elasticsearch.org/contributing-to-elasticsearch/ tells people to contribute to elasticsearch.github.com but the documentation is in the docs directory of the source code.  It'd be nice to have some instructions on how to convert asciidoc as well.
</description><key id="20045100">3777</key><summary>http://www.elasticsearch.org/contributing-to-elasticsearch/ tells people to contribute to elasticsearch.github.com</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-25T14:32:32Z</created><updated>2013-10-05T15:22:35Z</updated><resolved>2013-10-05T15:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-05T15:22:35Z" id="25750466">Thanks for the heads up, the page has been updated!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting could be made to produce larger fragments if there aren't as many fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3776</link><project id="" key="" /><description>Our current search system supports returning either one big fragment or two half sized ones.  It'll return one big fragment only if all the highlighting fits into fragment.  Elasticsearch could do that too, except letting the user pick maximum number of fragments.  Something like this:

``` javascript
"highlight": {
  "fields": {
    "foo": {
      "number_of_fragments": 2,
      "total_fragment_size": 150
    }
  }
}
```

I don't really know how important this is and I haven't thought through exactly how you'd do it.  I don't have any plans to implement this soon but if anyone is interested I could make time.
</description><key id="20040941">3776</key><summary>Highlighting could be made to produce larger fragments if there aren't as many fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>discuss</label><label>enhancement</label></labels><created>2013-09-25T13:22:03Z</created><updated>2016-12-16T10:58:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Change default Analyzer in next major version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3775</link><project id="" key="" /><description>The default analyzer (StandardAnalyzer in Lucene terms) is not a really good default since it really aims to be applied on English full-text. I think it would be wise to only use `StandardTokenizer` which is based on `Unicode Standard Annex #29` and a `LowercaseFilter`. I could think of using `ASCIIFoldingFilter` as well since most of the users will expect folding to work out of the box. 

This is really a basis for discussion but I think we should really get rid of stopwords in the default analyzer since it's really trappy.
</description><key id="20040710">3775</key><summary>Change default Analyzer in next major version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-09-25T13:17:38Z</created><updated>2013-11-05T20:09:15Z</updated><resolved>2013-11-05T20:09:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-25T13:33:23Z" id="25085853">I like this idea but you should be careful about upgrades.  Maybe make a new analyzer and default new fields to that.  Everything that was created as standard (defaulted or not) should stay standard or upgrading is going to be rough.

I'm torn on `ASCIIFoldingFilter`.  English users expect it but it is trappy for a bunch of other languages.

+1 on removing stop words.
</comment><comment author="s1monw" created="2013-09-25T13:38:19Z" id="25086227">@nik9000 we maintain compatibility if you do upgrades. if you create a new index you will get the new behaviour though. IMO we should keep the name `standard` for the lucene standard analyzer and call this one `es_default` and make the default analyzer point to `es_default` if a new index is created. I am on the fence for ascii folding as well. IMO we should just use tokenization and lowercase and drop stopwords
</comment><comment author="nik9000" created="2013-09-25T13:44:54Z" id="25086721">&gt; @nik9000 we maintain compatibility if you do upgrades. if you create a new index you will get the new behaviour though. IMO we should keep the name standard for the lucene standard analyzer and call this one es_default and make the default analyzer point to es_default if a new index is created. I am on the fence for ascii folding as well. IMO we should just use tokenization and lowercase and drop stopwords

Perfect.
</comment><comment author="drewr" created="2013-09-25T13:55:18Z" id="25087553">+1 :+1: 
</comment><comment author="brusic" created="2013-09-25T19:12:26Z" id="25115907">Another perspective:

Better documentation can alleviate some of the issues faced by beginners to elasticsearch. Currently, the only references to how analysis works is buried in the documentation of the index module. Users are confused by the standard analyzer because they do not understand the analysis pipeline. Analysis needs to be documented at the top-most level.If a new user understands analysis from the start, then the "pitfalls" of the standard analyzer can be anticipated.

Many of us come from a Lucene background, so the concepts are already familiar, but we might forget that not everyone has the some prior knowledge.
</comment><comment author="clintongormley" created="2013-09-25T19:16:25Z" id="25116234">I wouldn't add in the ASCII folding filter. Rather stick with a good generic standard.  
</comment><comment author="synhershko" created="2013-09-26T17:48:12Z" id="25188293">+1, and I'm pro ACIIFoldingFilter.

Maybe the right route here is to create multiple out of the box named analyzers, each being a good default for something else, and let the documentation do its thing. The actual default could be something not too strict (e.g. including the ASCII filter), and then switching it to something easier. Something like "Default English analyzer", "Non-ASCII folding English Analyzer", "French analyzer" and so on.

Another idea is to actually have that filter as a toggle on all known analyzers. I've seen many use cases where you want to use an analyzer, but it doesn't do ASCII folding, but you do want it to. Instead of sub-classing, this could be a great way out.

Might worth noting the actual tokenizer is source for troubles as well - it will now not preserve acronyms neither emails, and some users might have grown to expect that
</comment><comment author="clintongormley" created="2013-09-26T18:14:46Z" id="25190381">The danger that I see by adding too much stuff into the default analyzer is that you always have your own use case in mind.  So it ends up being very useful for that use case, and problematic for others.

The goal of the standard tokenizer is very simple and clean: break words on word boundaries.  Nothing more nothing less.  That plus lowercasing makes for a good general purpose analyzer.

We also have the language analyzers available by default.  I'd consider perhaps making the asciifolding an option there.  but then you have to ask yourself: do you want to strip diacritics, or do you want to index both versions: with and without diacritics.  Suddenly the number of choices start exploding.

We have a very flexible system for creating your own custom analyzers which do exactly what you want, so I'm not in favour of adding loads of prebuilt analyzers to try to cover every circumstance. Too many options will just make it more difficult for the user to understand.

and as @brusic said, better documentation will help.  /me is working on that
</comment><comment author="lukas-vlcek" created="2013-09-26T19:26:31Z" id="25196020">+1 @brusic 

@clintongormley If you are interested in contributions to analyzer documentation then I can offer updated materials that I have written about "Setting up Czech analysis in Elasticsearch" for dev site Zdrojak.cz [1,2]. Especially the second part focuses on Hunspell token filter and to date I am not aware of any other resource that goes into such important details (patting myself on the back). These concepts apply generally to a lot of other languages and I am sure it would be beneficial to wired audience if it were translated to English. Feel free to let me know if there is any public repo where ppl can contribute.

_.oO( Do I mind being asshole? Should I say the next paragraph? ... sigh!)_

Don't get me wrong, sometimes the _new ES documentation_ phenomena feels like an Apple TV to me. We all know nothing about it but still, we somehow hope it must be cool and worth waiting for, though it is little helping users in the meantime. Seriously, is there any reason why the documentation thing can not happen in more open and transparent way? Especially with the analysis part the input from more ppl having experience with different languages can be extremely useful. ( //cc @lhawthorn )

[1] http://www.zdrojak.cz/clanky/elasticsearch-vyhledavame-cesky/
[2] http://www.zdrojak.cz/clanky/elasticsearch-vyhledavame-hezky-cesky-ii-a-taky-slovensky/
</comment><comment author="brusic" created="2013-09-26T19:31:36Z" id="25196418">Sorry to derail the original topic with more about documentation, but what is the copyright of Lucene's documentation? A simple start would be to copy the existing documentation at http://lucene.apache.org/core/4_4_0/core/org/apache/lucene/analysis/package-summary.html
</comment><comment author="s1monw" created="2013-09-26T19:36:01Z" id="25196718">@brusic @lukas-vlcek can you take this discussion offline? I don't like if issues like this get hijacked. I appreciate you openness and I share the lack of documentation. Can we open a sep issue for this, I am happy to participate. 
</comment><comment author="nik9000" created="2013-09-26T19:39:37Z" id="25196992">On Thu, Sep 26, 2013 at 3:26 PM, Lukáš Vlček notifications@github.comwrote:

&gt; +1 @brusic https://github.com/brusic
&gt; 
&gt; @clintongormley https://github.com/clintongormley If you are interested
&gt; in contributions to analyzer documentation then I can offer updated
&gt; materials that I have written about "Setting up Czech analysis in
&gt; Elasticsearch" for dev site Zdrojak.cz [1,2]. Especially the second part
&gt; focuses on Hunspell token filter and to date I am not aware of any other
&gt; resource that goes into such important details (patting myself on the
&gt; back). These concepts apply generally to a lot of other languages and I am
&gt; sure it would be beneficial to wired audience if it were translated to
&gt; English. Feel free to let me know if there is any public repo where ppl can
&gt; contribute.

I'd love to see better language documentation.

I'd especially like English documentation on setting up some of the
plugins.  Unfortunately I can't read all the languages I have to support.

Nik
</comment><comment author="lukas-vlcek" created="2013-09-26T19:41:21Z" id="25197117">@s1monw I am sorry. Did not meant to hijack. /me shut-up!
</comment><comment author="s1monw" created="2013-09-26T19:42:02Z" id="25197163">@lukas-vlcek no speak up - this is open source. Just open another issue I am happy to join there!
</comment><comment author="kimchy" created="2013-09-26T21:16:05Z" id="25204604">@lukas-vlcek I will just answer your question regarding the docs, cause I want to answer it here so people will see (and we can continue the discussion on another thread).

We can't publish anything content related to the book we are working on because the contact with the publisher was not finalized yet. It took some time since we were very adamant on the fact that the book will be free online, which created complexities signing the contract. We have been open about that, btw, and I specifically answered that question raised by you several times already.

The work on moving the current guide to asciidoc is all in the open (and has required extensive work), btw, and we plan to obviously continue and improve it, and that is already all in the open. You can already submit pull request if you want to improve the guide docs (as you could before), though as is typical with us, we don't expect you to and will make sure it happens anyhow. For example, your hunspell docs can easily be added to our guide if appropriate.

I assume you didn't mean it that way, even though you stated it in the .oO, but that question could have been asked in a much nicer tone. This is the type of tone that I tried, and we try, to uphold in the elasticsearch community since inception. You should assume no malice, not the other way around.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java</file><file>src/test/java/org/elasticsearch/count/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/termvectors/AbstractTermVectorTests.java</file></files><comments><comment>Change 'standart' analyzer to use emtpy stopword list by default.</comment></comments></commit></commits></item><item><title>do not silently fail on plugin install with source files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3774</link><project id="" key="" /><description>When trying to install a plugin from source files only a debug error is issued, making the plugin cli exit without an error code. This is creates problems for automated installs because the error will not be detected.

Current behavior:

``` bash
ubuntu@es5:~$ sudo plugin -i elasticsearch/elasticsearch-river-wikipedia
-&gt; Installing elasticsearch/elasticsearch-river-wikipedia...
Trying https://github.com/elasticsearch/elasticsearch-river-wikipedia/archive/master.zip...
Downloading ...........DONE
Installed elasticsearch/elasticsearch-river-wikipedia into /usr/local/elasticsearch-0.90.5/plugins/river-wikipedia
ubuntu@es5:~$ echo $?
0
ubuntu@es5:~$ plugin -l
Installed plugins:
    - HQ
    - head
    - bigdesk
    - paramedic
    - browser
```

New behavior:

``` bash
bkw@Aeronaut ★ ~/tmp/es/elasticsearch-1.0.0.Beta1-SNAPSHOT
&gt; ./bin/plugin -i elasticsearch/elasticsearch-river-wikipedia
-&gt; Installing elasticsearch/elasticsearch-river-wikipedia...
Trying https://github.com/elasticsearch/elasticsearch-river-wikipedia/archive/master.zip...
Downloading ......DONE
Installed elasticsearch/elasticsearch-river-wikipedia into /Users/bkw/tmp/es/elasticsearch-1.0.0.Beta1-SNAPSHOT/plugins/river-wikipedia
Failed to install elasticsearch/elasticsearch-river-wikipedia, reason: Plugin installation assumed to be site plugin, but contains source code, aborting installation.
bkw@Aeronaut ☄ 1 ~/tmp/es/elasticsearch-1.0.0.Beta1-SNAPSHOT
&gt; echo $?
74
```

PR includes a dummy plugin zipfile with only one java file in it for testing.

CLA: XYVCQP8743E5W4R
</description><key id="20036012">3774</key><summary>do not silently fail on plugin install with source files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bkw</reporter><labels /><created>2013-09-25T11:29:35Z</created><updated>2014-06-13T16:54:14Z</updated><resolved>2013-09-26T10:36:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-25T12:24:10Z" id="25081568">Looks good to me - I will pull this soon!
</comment><comment author="bkw" created="2013-09-25T12:26:22Z" id="25081693">thanks. I wasn't sure about the class of exception to throw, but since other methods also use IOException I went with that.
</comment><comment author="s1monw" created="2013-09-25T12:33:22Z" id="25082080">hmm I think `IllegalArgumentException` or `IllegalStateException` would be a better fit. Might also play well with the return code. What do you think?
</comment><comment author="bkw" created="2013-09-25T12:54:27Z" id="25083350">Both are certainly better that IOException. I'd vote for IllegalArgumentException. Should I use ElasticSearchIllegalArgumentException or are these somehow reserved for "real" es stuff?
</comment><comment author="bkw" created="2013-09-25T13:24:30Z" id="25085196">Rebased version of this pr with ElasticSearchIllegalArgumentException and a simplified test: https://github.com/bkw/elasticsearch/compare/throwOnPluginSourceInstall2?expand=1

What do you think? Should I recreate a new pr from this?
</comment><comment author="s1monw" created="2013-09-25T13:34:10Z" id="25085901">I think you can just use `IAE` that should be fine. can you just rebase the branch you created this PR from that should be fine. no need for another PR
</comment><comment author="bkw" created="2013-09-25T13:49:30Z" id="25087082">ok, rebased into single commit and force-pushed. Hope this is what you meant.
</comment><comment author="s1monw" created="2013-09-25T14:06:28Z" id="25088446">Yes +1
</comment><comment author="s1monw" created="2013-09-26T10:36:43Z" id="25157839">pushed thanks!
</comment><comment author="bkw" created="2013-09-26T10:38:46Z" id="25157943">Thanks for merging! See you at the next es berlin meetup!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Issue #3768 Changing log level and not reporting IndexOutOfBoundExceptio...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3773</link><project id="" key="" /><description>Log level changed to warn and the exception is not reported..As it is not relevant
</description><key id="19994266">3773</key><summary>Issue #3768 Changing log level and not reporting IndexOutOfBoundExceptio...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">debarshri</reporter><labels /><created>2013-09-24T17:49:53Z</created><updated>2014-07-29T16:26:10Z</updated><resolved>2014-07-28T10:30:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T10:30:43Z" id="50322875">Hi @debarshri 

Thanks for the PR. We've discussed it and feel that the exception is important, so should remain.  Also, this is a message that can be repeated quite often, which is why we have it at info level rather than warn.   thanks anyway
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for using payloads to boost terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3772</link><project id="" key="" /><description>It would be great to be able so have a mapping field which stores payloads with terms and be able to use the payloads to boost the score of the document.

In my particular use case, I have documents which are tagged by users and after running through filters and algorithms we can determine which tags are most likely useful and which are likely spam. We'd like to pass that information on to the search index so that we can boost the documents we think are most appropriate to the search terms. In this case the boost is known at indexing time and applies to the terms themselves and not to the field or the documents.

This is something that's been possible with Lucene for quite awhile and which Solr had partial support for, but never fully implemented out of the box. (See for example http://wiki.apache.org/solr/Payloads, http://searchhub.org/2009/08/05/getting-started-with-payloads/, http://hnagtech.wordpress.com/2013/04/19/using-payloads-with-solr-4-x/).

Ideally, it would be best to pass the payload in as a separate JSON field value in the document. The Solr tokenizer for payloads (DelimitedPayloadTokenFilterFactory) uses a delimiter, but I've found this to be problematic when dealing with user generated terms. In addition, it would be best to have the payload value somehow available in scripting so the payloads can be indexed once and then the scoring algorithms tweaked as necessary to get the right scores.
</description><key id="19986599">3772</key><summary>Add support for using payloads to boost terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">bdurand</reporter><labels><label>feature</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-09-24T15:42:27Z</created><updated>2016-06-20T10:21:01Z</updated><resolved>2014-01-02T10:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-09-30T12:48:52Z" id="25357220">I think there are (at least) three issues here:
Taking the payloads into account for scoring could indeed be useful. I will try to come up with something. However, I would like to know how you believe the payload should affect the score. Since the same token can have different payloads, would you have an average of these numbers, the max, the min,...?

As for how to get the payloads in, I believe this is a different issue. It would be easy to expose the DelimitedPayloadTokenFilter in elasticsearch but passing the payloads in while indexing the document might be more tricky. If you desperately need that, could you open a new issue for that?

I do not fully understand how scipting support for payloads should work. Can you elaborate on this a bit or come up with an example?
</comment><comment author="bdurand" created="2013-10-09T17:22:04Z" id="25990387">In my mind the scripting and scoring are tied together simply because I believe this is the kind of issue where you'd need to play with the data after indexing it to get the right scoring. Since the payload would need to be added at indexing time, it would be much easier if the "payload score" could be exposed to a scoring script used for ordering.

My particular use case in detail would be:
1. While indexing documents, count the number of times each distinct tag has been applied by users to the document. This value would be included with each tag indexed for the document to indicate the weight for that particular tag.
2. When searching, we would apply the previously defined weights for the tags (terms) in a custom scoring script.

We haven't worked out the actual algorithms yet and it is definitely something we'd need to play around with to get the right values. I would imagine the payload, though, would likely be a number between 0.0 and 1.0 indicating the confidence that the term was an accurate one.
</comment><comment author="brwe" created="2013-10-24T14:30:27Z" id="26996906">Sorry for the late reply:
I agree that having the payloads available in a script would indeed be helpful to evaluate different scoring functions. But be warned: It will be very slow and only be good for prototyping.

I think the easiest way to do this is to simply make all term information for a document available in scripts. It would be similar to the term vector api. You would then have the freedom to choose any kind of document features for scoring. 
What do you think?
</comment><comment author="brwe" created="2013-11-13T13:55:35Z" id="28395595">I made the pull request (#4161) that allows to access payloads amongst other term information in a script. If you are still interested, take a look and see if this is useful for you!
</comment><comment author="bdurand" created="2013-11-13T17:08:08Z" id="28413106">Looks great! Thank you.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvector/TermVectorFields.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/util/MinimalMap.java</file><file>src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java</file><file>src/main/java/org/elasticsearch/search/lookup/PositionIterator.java</file><file>src/main/java/org/elasticsearch/search/lookup/ScriptTerm.java</file><file>src/main/java/org/elasticsearch/search/lookup/ScriptTerms.java</file><file>src/main/java/org/elasticsearch/search/lookup/SearchLookup.java</file><file>src/main/java/org/elasticsearch/search/lookup/ShardTermsLookup.java</file><file>src/main/java/org/elasticsearch/search/lookup/TermPosition.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java</file><file>src/test/java/org/elasticsearch/script/ShardLookupInScriptTests.java</file></files><comments><comment>make term statistics accessible in scripts</comment></comments></commit></commits></item><item><title>Introduce internal post recovery state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3771</link><project id="" key="" /><description>Introduce a new internal, index shard level, post recovery state, where the shard moves to when its done with recovery. The shard will now move to started only once the cluster state with its respective cluster state level state is started.

This change allow to have more fine grained control over when to allow reads on a shard, resolving potential refresh temporal visibility aspects while indexing and issuign a refresh. By only allowing reads on started shards, and making sure we refresh right before we move to started
</description><key id="19975906">3771</key><summary>Introduce internal post recovery state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2013-09-24T13:08:13Z</created><updated>2014-07-08T09:24:28Z</updated><resolved>2013-09-24T13:39:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-24T13:37:19Z" id="25003618">talked to @s1monw, will make the switch statement change in a different pull request, will push
</comment><comment author="kimchy" created="2013-09-24T13:39:11Z" id="25003789">merge to master / 0.90.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Acquire Read-Lock before reading SegmentInfos in RobinEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3770</link><project id="" key="" /><description>Before reading segmentinfos we should aquire the read lock to prevent
reading the segment infos on an already closed robin engine.
</description><key id="19972589">3770</key><summary>Acquire Read-Lock before reading SegmentInfos in RobinEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-24T12:00:51Z</created><updated>2014-07-16T21:52:16Z</updated><resolved>2013-09-24T14:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-26T13:04:21Z" id="25165542">this has been pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update Operation might hang (rarely) when retrying on invalid shard state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3769</link><project id="" key="" /><description>The retry logic when failing does not reset the operation started flag...
</description><key id="19970062">3769</key><summary>Update Operation might hang (rarely) when retrying on invalid shard state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-24T11:00:37Z</created><updated>2013-09-24T11:01:52Z</updated><resolved>2013-09-24T11:01:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file></files><comments><comment>Update Operation might hang (rarely) when retrying on invalid shard state</comment><comment>The retry logic when failing does not reset the operation started flag...</comment><comment>closes #3769</comment></comments></commit></commits></item><item><title>Transport client log when cluster is down</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3768</link><project id="" key="" /><description>When the ElasticSearch cluster is stopped (let's say restarted), the Transport Client logs every 5s the same error message (see below), even if ElasticSearch is not used (no request sent).

Some ideas to avoid log flooding:
- use backoff multiplier when trying to reconnect
- log only when the connection state changes (up or down)
- don't log the stacktrace here "IndexOutOfBoundsException: Readable byte limit exceeded: 60" is not relevant

At the moment, the workaround is to turn "org.elasticsearch.client.transport" log level to Warn on each client.

```
20130924-00:00:02.752|INFO|org.elasticsearch.client.transport||[Condor] failed to get node info for [#transport#-1][inet[myserver/192.168.131.228:9300]], disconnecting...|org.elasticsearch.transport.RemoteTransportException: [projet-myserver][inet[/192.168.131.228:9300]][cluster/nodes/info]
Caused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 60
        at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)
        at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)
        at org.elasticsearch.common.io.stream.AdapterStreamInput.readByte(AdapterStreamInput.java:35)
        at org.elasticsearch.common.io.stream.StreamInput.readBoolean(StreamInput.java:267)
        at org.elasticsearch.action.admin.cluster.node.info.NodesInfoRequest.readFrom(NodesInfoRequest.java:234)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:208)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:108)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)|
```
</description><key id="19961927">3768</key><summary>Transport client log when cluster is down</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gquintana</reporter><labels><label>adoptme</label></labels><created>2013-09-24T08:05:47Z</created><updated>2014-08-25T14:43:41Z</updated><resolved>2014-08-25T14:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:34:59Z" id="51633323">Is this still an issue?
</comment><comment author="gquintana" created="2014-08-25T14:43:00Z" id="53272698">Doesn't occur anymore on ES 1.2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove a comma in doc to make example a valid json.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3767</link><project id="" key="" /><description>This will help reader to do a hurry up copy-paste test.
</description><key id="19960221">3767</key><summary>Remove a comma in doc to make example a valid json.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gtt116</reporter><labels /><created>2013-09-24T07:25:48Z</created><updated>2014-07-16T21:52:16Z</updated><resolved>2013-09-24T07:27:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-09-24T07:27:47Z" id="24979819">@gtt116 thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #3767 from gtt116/master</comment></comments></commit></commits></item><item><title>High CPU utilization when using with logstash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3766</link><project id="" key="" /><description>Hi,
  I am hoping this is the right forum to raise this issue and get some insights on the issue and hopefully get some ideas on fixing it. I am using elasticsearch as the output of logstash, in an embedded fashion. I am observing high CPU utilization ( &gt; 85%) constantly when the index size grows beyond a point. 

A simple script is dumping log messages at a high rate and the index soon grows pretty big. At some point (not sure about when it takes off), but whats interesting to note is after this point even if the logs stop, the CPU utilization stays high. Is this a known issue?

NOTE : This is an elasticsearch cluster with a single node. JVM version 1.6.0_27
</description><key id="19957034">3766</key><summary>High CPU utilization when using with logstash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2013-09-24T05:20:13Z</created><updated>2013-09-26T17:39:39Z</updated><resolved>2013-09-26T17:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-09-25T14:49:10Z" id="25092175">Hi ghost,

Can you provide some information about your cluster setup? The output of `/_nodes/stats?all` and the [hot_threads](http://www.elasticsearch.org/guide/reference/api/admin-cluster-nodes-hot-threads/) API would be helpful also.
</comment><comment author="dakrone" created="2013-09-26T17:39:39Z" id="25187616">Actually, this would be better served on the mailing list. I'm going to close this issue and we can follow up on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_explanation.value always return 0 for _all when boosting </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3765</link><project id="" key="" /><description>The _search result is still coming back but the _score doesn't match the _explanation.value which is always 0. Also the _explain endpoint is returning "matched: false" with _explanation.value 0.

[gist](https://gist.github.com/evanwong/6638420)
</description><key id="19915897">3765</key><summary>_explanation.value always return 0 for _all when boosting </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">evanwong</reporter><labels /><created>2013-09-23T15:26:20Z</created><updated>2014-08-08T17:29:17Z</updated><resolved>2014-08-08T17:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:29:17Z" id="51632714">Your gist works for me.  Please reopen with more detail if this is still a problem on recent versions
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Automatic detection and removal of node with disk errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3764</link><project id="" key="" /><description>According to a recent conversation with @jprante on IRC (23 Sep 2013), if a disk holding one of a node's data dirs fails, this will cause queries to fail continuously (with accompanying exceptions in the logs) until the node is manually removed.

It would be really nice if ES could detect unrecoverable IO exceptions from Lucene, e.g. NoSuchDirectoryException, and mark the node as down, as if it had been removed from the cluster manually.

It would be even nicer if it could detect that a _specific_ data dir had become unusable, and recover the missing segments from other nodes, assuming it still had at least one data dir that wasn't in an error state.

Feel free to close if either of these suggestions are totally impractical, but I think this would be a useful enhancement in terms of fault-tolerance -- even just the first solution, marking the node as down.
</description><key id="19910624">3764</key><summary>Automatic detection and removal of node with disk errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels><label>discuss</label><label>resiliency</label></labels><created>2013-09-23T14:03:24Z</created><updated>2014-10-08T11:14:03Z</updated><resolved>2014-10-08T11:14:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-09-05T09:40:01Z" id="54604441">@bleskes @martijnvg @kimchy Any thoughts about this issue?
</comment><comment author="bleskes" created="2014-09-17T11:54:06Z" id="55883005">@andrewclegg I think we definitely be smarter about how we do this but it might help to explain how it works now. When a shard fails on a node, ES will try to assign it to _another_ node. If that possible, all is well. If that is not possible, it will retry to assign the shard again to the previous node, in the hope that it will go better this time. Does it explain what you're seeing?
</comment><comment author="andrewclegg" created="2014-10-07T12:05:16Z" id="58173921">@bleskes I honestly don't remember! I moved to a different team not long after that, and this question was in the context of a project that's no longer running.

I _think_ it was an "in theory" question, motivated by a desire to figure out whether we had the right storage architecture -- I certainly don't remember any disk failures on that cluster. I was trying to work out whether the right thing to do (when you have nodes with multiple physical disks) is a "JBOD" configuration, with multiple data directories on different disks -- like in Hadoop -- or RAID.

If I remember correctly, ES stores each shard _across_ the available data directories, i.e. not all the segments from one shard will be in the same directory. But the downside of this would be that a failing disk could take out all the shards on the node -- as they could all lose some segments.

(This was all on a pre-1.0 cluster so things may have changed a lot since then...?)

My idea was that ES could spot a specific disk failing, mark that disk as down, retrieve only the affected segments from other replicas, and place them on other disks. But it only makes sense to do this when you have large-ish servers with multiple disks (ours had 7 or 8 disks IIRC). Perhaps this is too complex in practice, and it's better to just migrate all the affected shards to other nodes. Feel free to close if that's a better way of doing it in the majority case.
</comment><comment author="bleskes" created="2014-10-07T12:54:46Z" id="58179241">@andrewclegg thx for the update. Yeah, I think it does make sense in theory but will end up being super complex to manage. I suggest we close for now? there is quite a lot of infra work that needs to happen before this is feasible.
</comment><comment author="andrewclegg" created="2014-10-08T11:14:03Z" id="58342697">More than happy to close as "not practically feasible"...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refresh index (and specific shard if routing=True) before doing search:</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3763</link><project id="" key="" /><description>Basically add an option (i searched the docs) to refresh an entire index or that specific shard before searching if it is possible?

Thanks
</description><key id="19905729">3763</key><summary>Refresh index (and specific shard if routing=True) before doing search:</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">ddorian</reporter><labels /><created>2013-09-23T12:20:03Z</created><updated>2014-08-08T17:24:20Z</updated><resolved>2014-08-08T17:24:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-25T08:32:10Z" id="25069795">Not entirely sure what you mean, so you would like to have a routing option to the refresh api? If so I think it would be better if the routing option is not a boolean option, but just holds a routing value, so the refresh can actually performed on a specific shard.
</comment><comment author="ddorian" created="2013-09-25T08:38:09Z" id="25070111">Yes I will search with routing and i will only refresh that shard.

And I will also search without routing where i want to refresh every shard.
</comment><comment author="s1monw" created="2013-09-25T12:34:36Z" id="25082166">I think what @ddorian wants is a boolean on the search request to refresh the shards it hits if needed, right?
</comment><comment author="ddorian" created="2013-09-25T12:39:14Z" id="25082437">Boolean or routing_key is the same for me. Of course now i can do a refresh(still no routing) and then the search so it's not a big problem. 
</comment><comment author="pentium10" created="2014-06-03T15:14:54Z" id="44977702">+1
</comment><comment author="clintongormley" created="2014-08-08T17:24:20Z" id="51632165">Closing in favour of #1063 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mutiple index search problem?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3762</link><project id="" key="" /><description>#### index data

   index_a 
   {id: 1, name: name1, start_time: '2013-09-20', end_time: '2013-09-30'} 
   {id: 2, name: name2, start_time: '2013-09-20', end_time: '2013-09-30'} 
   {id: 3, name: name3, start_time: '2013-09-20', end_time: '2013-09-30'}
   index_b 
           {id: 1, name: name1} 
           {id: 2, name: name2} 
           {id: 3, name: name3} 
#### search

``` SHEET
curl -X GET 'http://localhost:9200/index_a,index_b/_search?from=0&amp;size=40&amp;pretty' -d '{ 
  "query": { 
    "bool": { 
      "must": [ 
        { 
          "range": { 
            "index_a_type.start_time": { 
              "lte": "2013-09-23" 
            } 
          } 
        }, 
        { 
          "range": { 
            "index_a_type.end_time": { 
              "gt": "2013-09-23" 
            } 
          } 
        } 
      ] 
    } 
  }, 

  "size": "40", 
  "from": "0" 
}' 
```

search results 
           {id: 1, name: name1, start_time: '2013-09-20', end_time: '2013-09-30'} 
           {id: 2, name: name2, start_time: '2013-09-20', end_time: '2013-09-30'} 
           {id: 3, name: name3, start_time: '2013-09-20', end_time: '2013-09-30'} 
why no index index_b results? 

thank!
</description><key id="19897788">3762</key><summary>mutiple index search problem?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">huxinghai1988</reporter><labels><label>non-issue</label></labels><created>2013-09-23T08:56:23Z</created><updated>2013-09-24T05:18:54Z</updated><resolved>2013-09-24T05:18:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-23T08:57:13Z" id="24906362">is there a chance for a full fresh curl recreation? 
</comment><comment author="s1monw" created="2013-09-23T08:58:17Z" id="24906410">it seems like you index B docs don't have the required fields `start_time` &amp; `end_time`?
</comment><comment author="huxinghai1988" created="2013-09-23T10:07:45Z" id="24909704">@s1monw  yes! docs don't have the required fields start_time &amp; end_time
</comment><comment author="s1monw" created="2013-09-23T10:18:15Z" id="24910164">@huxinghai1988 well why should they be included then?
</comment><comment author="huxinghai1988" created="2013-09-23T10:38:56Z" id="24911105">@s1monw  because the document is not the same, the data is different
</comment><comment author="fatihzkaratana" created="2013-09-23T12:05:00Z" id="24915035">Any index has different mapping affects all results. You gotta make sure you have same mapping for your indices such as fields and field types. If you would like to search a date range you should put a begin and end date fields with type of date.
</comment><comment author="huxinghai1988" created="2013-09-24T01:44:56Z" id="24969426">field type is date

mapping

``` SHEET
"end_time" : {
      "type" : "date",
      "format" : "dateOptionalTime"
},
"start_time" : {
     "type" : "date",
     "format" : "dateOptionalTime"
 }

```
</comment><comment author="huxinghai1988" created="2013-09-24T05:18:21Z" id="24975351">solution 

``` SHEET
curl -XGET 'http://localhost:9200/index_a,index_b/_search?pretty=1' -d '{
"query" : {
    "bool": {
      "should": [{
        "filtered": {
          "filter":{
            "and": [
              {
                "range":{
                  "start_time":{
                    "lte": "2013-09-24T00:00:00+08:00"
                  }
                }
              },{
                "range": {
                  "end_time": {
                    "gt": "2013-09-24T00:00:00+08:00"
                  }
                }
              },
              {
                "term": {
                  "_type": "type_a"
                }
              }]                     
          }
        }        
      },{
        "filtered": {
          "filter": {
            "term": {
              "_type": "type_b"
            }
          }
        }
      }]
    }
  },
  "size": 800  
}'
```

thank you
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throwing exception from BulkProcessor.Listener.afterBulk results in closing down transport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3761</link><project id="" key="" /><description>Throwing exception from the BulkProcessor.Listener.afterBulk results in transport getting closed. This results in all subsequent calls getting NoNodeAvailableException.
Ideally it should throw ExecutionException on the caller thread or if the bulk action is executed by scheduled background thread, just log and eat up the exception probably.

Added unit test.

``` java
import org.elasticsearch.action.bulk.BulkProcessor;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.action.index.IndexRequestBuilder;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.transport.InetSocketTransportAddress;

import java.util.ArrayList;
import java.util.List;

public class BulkProcessorTest {

    private static final int NUM_DOCS = 10;
    private static final String CONSTANT_ID = "abcdef";

    @org.junit.Test
    public void test() throws InterruptedException {
        TransportClient client = createClient();
        BulkProcessor bulkProcessor = BulkProcessor.builder(client, new BulkProcessorProblematicListener()).setBulkActions(NUM_DOCS).build();

        List&lt;Document&gt; docs = createDocsWithSameId(NUM_DOCS);

        int iter = 3;
        for (int i = 0; i &lt; iter; i++) {
            System.out.println("Iteration #" + i);
            for (Document doc : docs) {
                IndexRequestBuilder indexRequestBuilder = client.prepareIndex("test", "bulk", doc.id);
                indexRequestBuilder.setSource(doc.toJson());
                // set op type to create for put-if-absent
                indexRequestBuilder.setOpType(IndexRequest.OpType.CREATE);
                bulkProcessor.add(indexRequestBuilder.request());
            }
            Thread.sleep(1000);
        }
    }

    private List&lt;Document&gt; createDocsWithSameId(int n) {
        List&lt;Document&gt; docs = new ArrayList&lt;Document&gt;();
        for (int i = 0; i &lt; n; i++) {
            docs.add(createConstantIdDocument(i));
        }
        return docs;
    }

    private Document createConstantIdDocument(int id) {
        Document doc = new Document();
        doc.id = CONSTANT_ID;
        doc.value = id;
        return doc;
    }

    private static class Document {

        private String id;
        private int value;

        public String toJson() {
            return "{\"value\":\"" + value + "\"}";
        }
    }

    private static class BulkProcessorProblematicListener implements BulkProcessor.Listener {
        @Override
        public void beforeBulk(long executionId, BulkRequest request) {

        }

        @Override
        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {
            if (response.hasFailures()) {
                throw new RuntimeException("Failure in response - " + response.buildFailureMessage());
            }
        }

        @Override
        public void afterBulk(long executionId, BulkRequest request, Throwable failure) {
            // throwing exception here makes the transport close resulting in NoNodeAvailableException
            throw new RuntimeException("Caught exception in bulk: " + request + ", failure: " + failure, failure);
        }
    }

    private TransportClient createClient() {
        ImmutableSettings.Builder settings = ImmutableSettings.settingsBuilder().put("cluster.name", "test-es-cluster");
        settings.put("nodes_sampler_interval", "10s");
        TransportClient client = new TransportClient(settings);
        client.addTransportAddress(new InetSocketTransportAddress("localhost", 9300));
        return client;
    }

}

```
</description><key id="19879470">3761</key><summary>Throwing exception from BulkProcessor.Listener.afterBulk results in closing down transport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">abhi-sanoujam</reporter><labels /><created>2013-09-22T21:07:01Z</created><updated>2014-08-01T10:24:59Z</updated><resolved>2014-08-01T10:24:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-13T13:39:23Z" id="46011866">Hi @abhi-sanoujam I'm not sure I follow why it would make sense to throw exception from within the `afterBulk` method. What utilizers would need to do instead is collect the failures, understand what happened and eventually resend those documents if it makes sense.

Also, by default the `BulkProcessor` executes the bulk request in a non-blocking fashion, which means that `afterBulk` is called from a different thread than the caller. Throwing exception from there wouldn't make much sense. We could wrap the afterBulk in a try catch block and log if something happened, but not sure if it's worth it, I'd rather document that no exception should be thrown there.

That said, I don't get how this would result in closing down the transport, which seems quite bad but I can't reproduce it at the moment.
</comment><comment author="javanna" created="2014-08-01T10:24:59Z" id="50870062">Closing due to lack of feedback, feel free to reopen if you can reproduce it and post more info about it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect JVM_DLL environment variable definition in service.bat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3760</link><project id="" key="" /><description>I am running Windows 8 64-bit. First, I uninstalled all previous Java-related installations. I confirmed that I had no `C:\Program Files\Java` folders. I then downloaded and installed JRE 7 from the last download link on this page:

http://www.oracle.com/technetwork/java/javase/downloads/java-se-jre-7-download-432155.html

I confirmed that Java was installed properly. Here is the installed path to `jvm.dll`:

`C:\Program Files\Java\jre7\bin\server\jvm.dll`

My JAVA_HOME environment variable is set to `C:\Program Files\Java\jre7`.

Unfortunately, service.bat sets the JVM_DLL environment variable to this:

```
%JAVA_HOME%\jre\bin\server\jvm.dll
```

Note the extra `jre` path that does not exist with the JRE I installed. Perhaps this path changed in later versions of the JRE or in Oracle's installer? This problem causes the service to fail to start with this error:

```
[2013-09-22 14:23:27] [info]  [19408] Starting service...
[2013-09-22 14:23:27] [error] [19408] Failed creating java C:\Progra~1\Java\jre7\jre\bin\server\jvm.dll
[2013-09-22 14:23:27] [error] [19408] The system cannot find the path specified.
[2013-09-22 14:23:27] [error] [19408] ServiceStart returned 1
```

I fixed this easily by simply removing `jre\` from the `set JVM_DLL` line. The batch file should probably use a different mechanism for determining the location of `jvm.dll`.
</description><key id="19878296">3760</key><summary>Incorrect JVM_DLL environment variable definition in service.bat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">nathan-alden</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-22T19:48:24Z</created><updated>2014-05-10T04:47:13Z</updated><resolved>2013-10-24T10:50:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-10-24T10:50:08Z" id="26982560">Hi.

This has been fixed through #3739 . You are installing a JRE (we recommend using a JDK) and service.bat will detect it accordingly. The fix will be available in the next release (0.90.6 and 1.0.0).

Can you please try master and report back?

Thanks.
</comment><comment author="bittusarkar" created="2014-05-10T04:47:13Z" id="42731741">@costin This might not be the correct place to ask this but why do you recommend using a JDK over a JRE? Specifically what is the difference between %JAVA_HOME%\jre\bin\server\jvm.dll and %JAVA_HOME%\bin\server\jvm.dll?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException with CustomScoreQueries (or FunctionScore) &amp;&amp; native script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3759</link><project id="" key="" /><description>Hi,

When built &amp; run against 0.90.5, native script for custom score (or function_score) implementing AbstractSearchScript issues a NullPointerException. 

run() method is called, returning a null object.

This method should not be called I guess. It was not called prior to 0.90.4. Seems to be a regression.
</description><key id="19853625">3759</key><summary>NullPointerException with CustomScoreQueries (or FunctionScore) &amp;&amp; native script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YannBrrd</reporter><labels><label>non-issue</label></labels><created>2013-09-21T08:22:53Z</created><updated>2013-09-23T08:06:22Z</updated><resolved>2013-09-23T08:06:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-21T19:46:26Z" id="24869370">can you provide a stacktrace for this. Afaik you need to impl that method though and likely not return 'null'? What is the purpose of returning null here?
</comment><comment author="YannBrrd" created="2013-09-23T05:41:07Z" id="24899892">Hi Simon,

I'll send a stacktrace ASAP.

Implementing the run() method for a custom score is kind of a nonsense to
me if you implement a custom score plugin. The only thing I see is to have
a default implementation returning current Doclookup.

Versions prior to 0.90.4 were not calling run() for a custom score plugin.

I started mine based on Igor Motov examples found here :
https://github.com/imotov/elasticsearch-native-script-example.

Maybe I did something wrong though...

Regards,
Yann Barraud
Le 21 sept. 2013 21:46, "Simon Willnauer" notifications@github.com a
écrit :

&gt; can you provide a stacktrace for this. Afaik you need to impl that method
&gt; though and likely not return 'null'? What is the purpose of returning null
&gt; here?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3759#issuecomment-24869370
&gt; .
</comment><comment author="YannBrrd" created="2013-09-23T07:06:27Z" id="24902027">My bad. I was implementing AbstractSearchScript instead of
AbstractDoubleSearchScript.

I might close the issue then.

Nevertheless,  behavior changed between 0.90.3 and .4. Which finally turned
out to be a good thing for me. :-)

Do you still want a stack trace ?
Le 23 sept. 2013 07:41, "Yann Barraud" yann.barraud@gmail.com a écrit :

&gt; Hi Simon,
&gt; 
&gt; I'll send a stacktrace ASAP.
&gt; 
&gt; Implementing the run() method for a custom score is kind of a nonsense to
&gt; me if you implement a custom score plugin. The only thing I see is to have
&gt; a default implementation returning current Doclookup.
&gt; 
&gt; Versions prior to 0.90.4 were not calling run() for a custom score plugin.
&gt; 
&gt; I started mine based on Igor Motov examples found here :
&gt; https://github.com/imotov/elasticsearch-native-script-example.
&gt; 
&gt; Maybe I did something wrong though...
&gt; 
&gt; Regards,
&gt; Yann Barraud
&gt; Le 21 sept. 2013 21:46, "Simon Willnauer" notifications@github.com a
&gt; écrit :
&gt; 
&gt; &gt; can you provide a stacktrace for this. Afaik you need to impl that method
&gt; &gt; though and likely not return 'null'? What is the purpose of returning null
&gt; &gt; here?
&gt; &gt; 
&gt; &gt; —
&gt; &gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3759#issuecomment-24869370
&gt; &gt; .
</comment><comment author="s1monw" created="2013-09-23T08:06:17Z" id="24904149">thanks for clarifying. Can you still attache a stacktrace?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cat shards/indices don't properly handle index parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3758</link><project id="" key="" /><description>`/_cat/indices` doesn't handle it right, and `/_cat/shards` doesn't accept any indices.
</description><key id="19850850">3758</key><summary>Cat shards/indices don't properly handle index parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-09-21T03:25:17Z</created><updated>2013-09-21T03:26:00Z</updated><resolved>2013-09-21T03:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>Add/fix index selectors.</comment></comments></commit></commits></item><item><title>FastVectorHighlighter doesn't sort array types by score but Plain highlighter does</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3757</link><project id="" key="" /><description>The FastVectorHighlighter doesn't sort array types by score but Plain highlighter does: https://gist.github.com/nik9000/6645681

The plain highlighter's behaviour seems more correct.
</description><key id="19847937">3757</key><summary>FastVectorHighlighter doesn't sort array types by score but Plain highlighter does</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-21T00:29:55Z</created><updated>2014-02-26T21:21:50Z</updated><resolved>2014-02-26T21:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-23T01:00:19Z" id="24894727">After poking around at this I've figured out a few things:
1.  This is a Lucene behaviour so it'd have to be fixed upstream/mirrored in Elasticsearch and submitted upstream.
2.  It all comes down to the scoring phase being done without access to the array boundaries.
3.  You can't use position_offset_gap to make a big gap because that set _term_ position which isn't used to figure out highlighting segments.  That uses _offsets_.

I could live with closing this issue and amending http://www.elasticsearch.org/guide/reference/api/search/highlighting/ to ward people of trying the FVH for score ordered array fields.

I imagine the FVH might be faster to highlight array's with long values or lots of values but really, I have no way to test it.
</comment><comment author="nik9000" created="2013-10-15T18:44:57Z" id="26361220">Filed LUCENE-5285 which I'll have a look at when I get a chance.
</comment><comment author="nik9000" created="2013-10-25T16:23:37Z" id="27106062">I've posted a solution to this over on the Lucene bug.  I'd love to get this fixed so I can use #3750 on all of my fields.  Well, once that is implemented.
</comment><comment author="nik9000" created="2013-11-05T16:02:53Z" id="27785328">@jpountz, would it be possible for you to have a look at LUCENE-5285, this bug's Lucene brother?  This wouldn't be a big deal but if I'm going to be able to use #3750 on all of my fields I'll need this fixed too.  I've posted a patch over there which I think should do the job.  This is a lot less complicated than #3750 was, I think.
</comment><comment author="nik9000" created="2013-12-03T16:27:43Z" id="29724862">@jpountz got this in to Lucene 4.7 so when that is released and Elasticsearch updates to it this will be fixed for free.  I don't feel it is worth X classing it into Elasticsearch at this point.  Thanks!
</comment><comment author="jpountz" created="2013-12-03T16:37:11Z" id="29725856">Agreed. I added the "Lucene 4.7 Upgrade" label so that we don't forget to close this issue when we switch to Lucene 4.7.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/miscellaneous/XASCIIFoldingFilter.java</file><file>src/main/java/org/apache/lucene/queryparser/XSimpleQueryParser.java</file><file>src/main/java/org/apache/lucene/search/XReferenceManager.java</file><file>src/main/java/org/apache/lucene/search/XSearcherManager.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefOrdValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/GeoDistanceComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/NumberComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/StringScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/gateway/fs/FsIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NestedFieldComparatorSource.java</file><file>src/test/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngine.java</file></files><comments><comment>Upgrade to Lucene 4.7</comment></comments></commit></commits></item><item><title>node.river as dynamic setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3756</link><project id="" key="" /><description>node.river should be changeable at runtime

Updated 9/23/13-------
All node level attributes should by dynamic, node.river is just one example. http://www.elasticsearch.org/guide/reference/modules/cluster/ gives the example of node.rack_id as an attribute that can influence shard allocation.  All such parameters should be dynamic so as to allow for more sophisticated cluster management techniques that may decide to change these attributes on the fly based on content or hardware changes.
</description><key id="19845370">3756</key><summary>node.river as dynamic setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shadow000fire</reporter><labels /><created>2013-09-20T22:58:12Z</created><updated>2014-08-08T17:16:56Z</updated><resolved>2014-08-08T17:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:16:56Z" id="51631276">Not all node settings should be dynamic. We've made the relevant ones dynamic, if you spot any others that aren't but should be, please open an issue.  But rivers will be removed, so we won't be adding that one.  thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>node.river should support simple pattern matching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3755</link><project id="" key="" /><description>Just like the template field in the Put Template API, node.river should support simple patterns like twitter*.
</description><key id="19845299">3755</key><summary>node.river should support simple pattern matching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shadow000fire</reporter><labels /><created>2013-09-20T22:55:21Z</created><updated>2014-08-08T17:15:35Z</updated><resolved>2014-08-08T17:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:15:34Z" id="51631115">Our intention is to remove rivers in the future.  I'm going to close this request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Boost doesn't seem to work for prefix queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3754</link><project id="" key="" /><description>Boosts don't seem to work for prefix queries.  See https://gist.github.com/nik9000/6643155

It looks like the queryNorm value is always the inverse of the boost value, cancelling out the boost.

That seems wrong.
</description><key id="19842048">3754</key><summary>Boost doesn't seem to work for prefix queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-20T21:28:38Z</created><updated>2013-10-14T20:40:35Z</updated><resolved>2013-10-08T14:31:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-25T20:26:26Z" id="25121784">This looks to be caused by ConstantScoreAutoRewrite squashing prefix queries that don't match into empty, unweighted boolean clauses in the query rewrite phase.  Those empty boolean clauses have zero weight.

Switching to "rewrite": "constant_score_filter" fixes it for me.
</comment><comment author="nik9000" created="2013-09-25T20:47:29Z" id="25123395">I filed https://issues.apache.org/jira/browse/LUCENE-5245 but I'm not particularly familiar with lucene's procedures so I'm not sure where to go from here.
</comment><comment author="s1monw" created="2013-09-25T21:50:28Z" id="25127819">oh good call! thanks for opening it. This should be fixed in lucene really. it seems uwe is on it though but I will watch it as well.
</comment><comment author="s1monw" created="2013-09-25T21:52:38Z" id="25127946">flagged it as 4.6 upgrade - we will leave that open until it's fixed / upgraded
</comment><comment author="nik9000" created="2013-09-26T13:42:26Z" id="25168032">Fine by me.  I'll use my work around until then.  Any idea when Elasticsearch will upgrade to get it?  Also, is this worth adding to the documentation?

Now that I know what causes it I don't think it effects all prefix queries, just the ones for terms that are in a different combination of fields on different indexes.  Because I deal in "rare" terms I see this more than most folks.  
</comment><comment author="s1monw" created="2013-09-26T14:35:07Z" id="25172651">@nik9000 well we ( @jpountz ) tries to get 4.5 out of the door so I'd guess 8 weeks after that happened? I don't think we need to document that specifically. 
</comment><comment author="nik9000" created="2013-09-26T14:37:01Z" id="25172818">Thanks!
</comment><comment author="s1monw" created="2013-09-26T15:36:47Z" id="25177921">@nik9000 FYI we are respinning 4.5 so it will include the patch uwe committed
</comment><comment author="nik9000" created="2013-09-26T15:51:53Z" id="25179190">I saw!  Thanks so much!
</comment><comment author="s1monw" created="2013-10-08T14:31:55Z" id="25894870">closing, we upgraded to `Lucene 4.5` in https://github.com/elasticsearch/elasticsearch/commit/e9f36772c1d782e66e882cc36574dc9bf805ac71
</comment><comment author="nik9000" created="2013-10-08T14:36:55Z" id="25895316">Yay!
</comment><comment author="pbellora" created="2013-10-14T15:37:11Z" id="26264374">@nik9000 "I don't think it effects all prefix queries, just the ones for terms that are in a different combination of fields on different indexes" - would you mind explaining this further? I'm trying to decide whether to upgrade from 90.3 to 90.5.
</comment><comment author="nik9000" created="2013-10-14T15:53:41Z" id="26265763">@pbellora this bug will hit you if you do a prefix against multiple fields and not all of those fields contain matching terms on all shards.  The results won't be sorted properly.
</comment><comment author="pbellora" created="2013-10-14T20:25:50Z" id="26285922">@nik9000 Okay, thanks for clarifying. I ran your gist with 90.3 and saw the same behavior, so I suppose it can't hurt to upgrade.
</comment><comment author="s1monw" created="2013-10-14T20:40:35Z" id="26286946">@pbellora FYI this should be fixed in the next release
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow slop = -1 in span queries (#3673)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3753</link><project id="" key="" /><description>Finally. Fix #3673.
</description><key id="19835123">3753</key><summary>Allow slop = -1 in span queries (#3673)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-09-20T19:03:23Z</created><updated>2014-07-16T21:52:17Z</updated><resolved>2013-09-20T19:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-20T19:24:03Z" id="24835181">Thanks! Merged be35b44df128a2d29df8b0a28ed6937b8d60560e and backported to 0.90 c6bd0230a0c3c58e42bc41ab153cfffecccd6578
</comment><comment author="iksnalybok" created="2013-09-24T09:38:55Z" id="24989118">Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow _boost field to be indexed and stored in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3752</link><project id="" key="" /><description>The idea of the `_boost` field is that it provides an index time boost for all fields a document has, because of this the `_boost` field isn't stored or indexed separately and only saved as part of the `_source`.

However the `_boost` field is also used to as just a sort field, but because it can't be indexed separately, this doesn't work. By allowing to make the `_boost` indexable separately, sorting by the `_boost` field does work. By default the `_boost` field remains not indexed and not stored.

This issue originates from PR #2913
</description><key id="19828594">3752</key><summary>Allow _boost field to be indexed and stored in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-20T16:56:59Z</created><updated>2014-02-24T15:01:17Z</updated><resolved>2013-09-20T16:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JoeZ99" created="2014-02-19T20:08:04Z" id="35542241">has this feature been removed for 0.90.10? (I thought it might, since the _boost field is deprecated in 1.X). I have 

``` json
{
    "_boost": {
        "type": "float",
        "index": "not_analyzed"
    }
}
```

in my mappings definition and, besides not getting any info on "_boost" field when I access _mapping entrypoint (which I guess is due to the _boost field being "special"), can't get ES to sort by it.
Just want to know if it's removed. otherwise, I'll post this at the users group like a good kid. ;-)
</comment><comment author="javanna" created="2014-02-19T20:17:08Z" id="35543215">Hi @JoeZ99 , the document boost wasn't removed for now, just deprecated in the documentation.
</comment><comment author="JoeZ99" created="2014-02-24T15:01:17Z" id="35894110">there seems to be a bug or at least, unexpected behavior with this. 
more info at: https://groups.google.com/d/msg/elasticsearch/RvZZQa3ZbTA/Ofw_bg2be2oJ

here's how to reproduce it

create an index with a _boost mapping

``` http
POST http://localhost:9200/tito
```

``` json
{
  "mappings":{
    "type1":{
      "_boost":  {"name": "df_boost", "null_value": 1.0, "index": "not_analyzed"},  
        "properties": {

          "extra_field": {"type": "double"}
        }
    }   
  }
}
```

an exception is raised in elasticsearch

``` java
[2014-02-24 09:32:03,365][WARN ][indices.cluster          ] [Master Khan] [tito] failed to add mapping [type1], source [{"type1":{"_boost":{"name":"df_boost","null_value":1.0,"index":true},"properties":{"extra_field":{"type":"double"}}}}]
org.elasticsearch.index.mapper.MapperParsingException: Wrong value for index [true] for field [df_boost]
        at org.elasticsearch.index.mapper.core.TypeParsers.parseIndex(TypeParsers.java:194)
        at org.elasticsearch.index.mapper.core.TypeParsers.parseField(TypeParsers.java:76)
        at org.elasticsearch.index.mapper.core.TypeParsers.parseNumberField(TypeParsers.java:51)
        at org.elasticsearch.index.mapper.internal.BoostFieldMapper$TypeParser.parse(BoostFieldMapper.java:99)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:235)
        at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:183)
        at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:322)
        at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:200)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:405)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:360)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:416)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
```

the exception is about the `"index": "not_analyzed"` attribute of the `"_boost"` element.

However, the mapping is done

``` http
GET http://localhost:9200/tito/_mapping
```

``` json
{
    "tito": {
        "type1": {
            "properties": {
                "df_boost": {
                    "type": "long"
                },
                "field": {
                    "type": "string"
                }
            }
        }
    }
}
```

and sorting  by boost is possible!

``` http
GET http://localhost:9200/tito/_search 

{
    "query": {"match_all": {} },
    "sort": ['df_boost"]
}
```

``` json
{
    "took": 4,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": null,
        "hits": [
            {
                "_index": "tito",
                "_type": "type1",
                "_id": "6r-8C64RSNuPQ1vGPENK6g",
                "_score": null,
                "_source": {
                    "field": "value",
                    "df_boost": 34
                },
                "sort": [
                    34
                ]
            }
        ]
    }
}
```

this works whe accessing the REST API , but when using the `pyes` python client, the mapping never gets to a good ending.

THe issue/bug is that it throws an exception for no good reason.

I'm using elasticasearch 0.9.10
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/boost/BoostMappingTests.java</file></files><comments><comment>Allow the `_boost` field to be indexed and stored.</comment></comments></commit></commits></item><item><title>Returning useful error message when sorting on a completion field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3751</link><project id="" key="" /><description>Closes #3747
</description><key id="19823413">3751</key><summary>Returning useful error message when sorting on a completion field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-09-20T15:21:06Z</created><updated>2014-07-16T21:52:17Z</updated><resolved>2013-09-27T14:48:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-26T20:55:37Z" id="25203045">@s1monw switched this PR to create `FieldMapper.isSortable()`,  which is true by default and only disabled for the `CompletionFieldMapper`... 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>It'd be cool if the highlighter could combine fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3750</link><project id="" key="" /><description>So maybe I'm doing this wrong, but when I want to search across the same field with different analyzers run against it I use a multifield with the different analyzers setup in the mapping and then query the field who's analyzer I want to use.  This ends up highlighting the field twice - once per subfield.  It'd be cool if I could combine those highlight operations into one so the snippets would be combined together and sorted together.

This is an example of how I build the query: https://gist.github.com/nik9000/6638883
</description><key id="19822634">3750</key><summary>It'd be cool if the highlighter could combine fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>feature</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-09-20T15:07:18Z</created><updated>2014-02-18T20:26:23Z</updated><resolved>2013-12-03T10:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-20T16:48:20Z" id="24824124">Interesting point @nik9000 ! Did you have the chance to play around with this already? How would we sort the snippets coming from different fields though? I guess score is the only option and depends on the specific scoring algo provided by the highlighter implementation. For instance, this wouldn't work using different highlighters per field (e.g. one field with plain and another one with plain). Also, I'd need to look deeper at how the snippets score is computed to make sure this would work cross-field.
</comment><comment author="nik9000" created="2013-09-20T17:00:01Z" id="24824932">&gt; Interesting point @nik9000 ! Did you have the chance to play around with this already?

Not really.  I wanted to make sure that what I was doing in the query made sense.  I'll have a look at some point soon though.

&gt; How would we sort the snippets coming from different fields though?

Probably score.  I imagine you could use document order too.  What is the default?  I never use it.

&gt; this wouldn't work using different highlighters per field

Indeed.  This would be an implementation feature strapped on to each highlighter.  At least, that is how it makes sense to me.

&gt; I'd need to look deeper at how the snippets score is computed to make sure this would work cross-field.

Yeah.
</comment><comment author="nik9000" created="2013-09-23T14:22:02Z" id="24923188">I've put together a proof of concept for this with the FVH which github seems to have referenced above.  It is surprisingly well suited for this.  I'm not sure about the plain highlighter though.
</comment><comment author="nik9000" created="2013-09-23T17:25:14Z" id="24937121">For the FVH the POC works by adding matches from the child field during the matched term identification step then pretty much letting the FVH do its job from there on out.
For the Plain Highlighter the POC works by creating a merged token stream from the parent and child fields and feeding it to the highlighter to highlight.  This currently only works with requireFieldMatch = false.

Both of these implementations look like they will work but I'm not 100% sure they don't cast too broad a net.
</comment><comment author="nik9000" created="2013-09-30T21:12:06Z" id="25406721">I know it is a lot of work, but would it be possible for someone to have a look at the proof of concept?  I'm happy spend some time implementing this but I'd love to know if I'm on the right track.
</comment><comment author="jpountz" created="2013-10-02T10:14:48Z" id="25528141">This feature makes a lot of sense to me, but I think the only way to do it correctly would be to fix the highlighters to support several index fields for a single stored field in Lucene. For example, the merged token stream trick can create broken token streams: token streams are supposed to have consistent positions and offsets (meaning that tokens that start at the same position must have the same start offset and tokens that end at the same position must have the same end offset) which won't be the case if the two fields don't use the same tokenizer.
</comment><comment author="nik9000" created="2013-10-02T13:19:30Z" id="25537351">&gt; This feature makes a lot of sense to me, but I think the only way to do it correctly would be to fix the highlighters to support several index fields for a single stored field in Lucene. For example, the merged token stream trick can create broken token streams: token streams are supposed to have consistent positions and offsets (meaning that tokens that start at the same position must have the same start offset and tokens that end at the same position must have the same end offset) which won't be the case if the two fields don't use the same tokenizer.

That makes sense to me.  The merged token stream thing felt more like a hack then the way the I had the FVH doing it.  The FVH actually seems more suited to this then the plain highlighter given the way it separates its decision making.  

I'll have another look at all this in a few hours and see what I can come up with.

For reference, is this worth doing if it is only possible to cleanly implement it in the FVH?  It might turn out that the plain highlighter just isn't amenable to this without a more serious overhaul that might not be worth implementing.
</comment><comment author="jpountz" created="2013-10-03T07:55:12Z" id="25604296">&gt; For reference, is this worth doing if it is only possible to cleanly implement it in the FVH? It might turn out that the plain highlighter just isn't amenable to this without a more serious overhaul that might not be worth implementing.

I think it is ok if only fvh supports it.
</comment><comment author="nik9000" created="2013-10-08T20:25:51Z" id="25924338">I had another look at this today.  I was able to get the fvh working pretty well.  I think the plain highlighter is a lost cause though - it just isn't written for this kind of thing.

I went ahead and plumbed a list of fields to combine.  I called it `child_fields` because those fields ought to be children of the main highlighted field.  I'm not in love with the name but that can be fixed.

I'm not sure exactly what to do when the user sets up a bogus child field - right now I don't check for it and let the fvh blindly try to do its thing which either leads to shard failures with string index out of bounds exceptions or funky highlighting.  Not good.

Given that I'm likely going to give up on doing this in the plain highlighter I might have to revive #3757.  I think I'll want this field combination in my multi-valued fields.
</comment><comment author="clintongormley" created="2013-12-17T13:27:57Z" id="30750276">@nik9000 Did you have a look at how feasible it would be to support combining fields on the postings highlighter as well?
</comment><comment author="nik9000" created="2013-12-17T13:40:02Z" id="30751047">@clintongormley I was thinking about it but I haven't had time to work on it.  I'd like to, though.
</comment><comment author="clintongormley" created="2013-12-17T13:40:50Z" id="30751100">i'd like you to as well :)

Go nik go!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Add support for combining fields to the FVH</comment></comments></commit></commits></item><item><title>Add completion field support in Rest nodes stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3749</link><project id="" key="" /><description>Closes #3746

Also adds support for `completion_fields` and `fielddata_fields` parameters to be compatible with `RestIndicesStatsAction`
</description><key id="19819606">3749</key><summary>Add completion field support in Rest nodes stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-09-20T14:16:08Z</created><updated>2014-07-16T21:52:17Z</updated><resolved>2013-09-22T13:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-22T13:20:44Z" id="24881924">closed by https://github.com/elasticsearch/elasticsearch/commit/57f962d620b4011be545994dfef6690603e9683b in master and https://github.com/elasticsearch/elasticsearch/commit/c11e69491526ed542a512530980c59358461ba5e in 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST: Add newline to response when using pretty flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3748</link><project id="" key="" /><description>This relates to the discussion on #3422 
</description><key id="19815571">3748</key><summary>REST: Add newline to response when using pretty flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-20T12:54:14Z</created><updated>2013-10-18T09:35:28Z</updated><resolved>2013-09-20T12:55:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java</file><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java</file><file>src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestXContentBuilder.java</file></files><comments><comment>REST: Add newline to response when using pretty flag</comment><comment>closes #3748</comment><comment>relates to #3422</comment></comments></commit></commits></item><item><title>NPE when sorting on a completion field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3747</link><project id="" key="" /><description>```
curl -XPUT 'http://localhost:9200/test?pretty=1' -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "name" : {
               "type" : "completion"
            }
         }
      }
   }
}
'
curl -XGET 'http://localhost:9200/_search?pretty=1' -d '
{
   "sort" : "name"
}
'

 SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; 
 shardFailures {[PDCUCebERaCutbxDWeAORg][test][4]:
 SearchParseException[[test][4]:
 from[-1],size[-1]:
 Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:
 NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][3]:
 SearchParseException[[test][3]:
 from[-1],size[-1]:
 Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:
 NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][2]:
 SearchParseException[[test][2]:
 from[-1],size[-1]:
 Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:
 NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][0]:
 SearchParseException[[test][0]:
 from[-1],size[-1]:
 Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:
 NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][1]:
 SearchParseException[[test][1]:
 from[-1],size[-1]:
 Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:
 NullPointerException; }]
```
</description><key id="19813885">3747</key><summary>NPE when sorting on a completion field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2013-09-20T12:12:34Z</created><updated>2013-09-27T14:48:38Z</updated><resolved>2013-09-27T14:48:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Returning useful exception when sorting on a completion field</comment></comments></commit></commits></item><item><title>Missing completion fields in nodes stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3746</link><project id="" key="" /><description>The indices stats API supports `fields`/`completion_fields`/`fielddata_fields`:

```
GET /_stats/completion?fields=name
....
     "completion": {
        "size": "20b",
        "size_in_bytes": 20,
        "fields": {
           "name": {
              "size": "20b",
              "size_in_bytes": 20
           }
        }
     }
```

But the nodes/indices stats only supports `fields` for fielddata, not for completion stats:

```
GET /_nodes/stats/indices/completion?fields=name
....
        "completion": {
           "size": "20b",
           "size_in_bytes": 20
        }
```
</description><key id="19813757">3746</key><summary>Missing completion fields in nodes stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-20T12:09:14Z</created><updated>2013-09-22T13:20:09Z</updated><resolved>2013-09-22T13:18:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file></files><comments><comment>Add completion field support in Rest nodes stats</comment></comments></commit></commits></item><item><title>reload unicast servers - discovery.zen.ping.unicast.hosts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3745</link><project id="" key="" /><description>In my configuration I need to update `discovery.zen.ping.unicast.hosts` when I add/remove a node. 
I need to do this because in my configuration each node is isolated and can only see the master node. And all servers are on a VPS, I can't use zen multicast.

But each time I do that I have to restart elasticsearch master node that makes my cluster not available during seconds.

Is it possible to add the discovery.zen.ping.unicast.hosts as a dynamic cluster settings?

Thanks
</description><key id="19812865">3745</key><summary>reload unicast servers - discovery.zen.ping.unicast.hosts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgruhier</reporter><labels /><created>2013-09-20T11:44:26Z</created><updated>2014-02-04T02:35:21Z</updated><resolved>2013-09-21T09:37:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-09-20T20:15:42Z" id="24838336">`discovery.zen.ping.unicast.hosts` is used only during initial discovery, when nodes starts up and when discovery is restarted if the number of master eligible nodes falls bellow minimum_master_nodes setting. In your case, since you have only one master eligible node, once this node has started this list is no longer needed because the list of nodes in the cluster is maintained by the master. 

In general, there is typically no need to restart nodes after changes are made in `discovery.zen.ping.unicast.hosts` because this list doesn’t have to be comprehensive for the nodes to discovery each other.
</comment><comment author="sgruhier" created="2013-09-21T06:46:49Z" id="24857289">You mean that when I start a new data node, it will find the master by itself (and so the master will know it)
I'm gonna try to add a data node without restart the master and let you know 
Many thanks
</comment><comment author="sgruhier" created="2013-09-21T09:37:22Z" id="24859347">You're right it works!
Thanks
I have another issue, but I'll post it to the mailing list
</comment><comment author="bhaskargr" created="2014-02-03T14:30:10Z" id="33958615">Hi @sgruhier, I have 2 servers which are hosted on VPS, not able to use unicast, This is what I have:

discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["168.144.187.54", "168.144.38.123"]

The  nodes are not being discovered by each other. What am I missing? Thanks a lot for any help.
</comment><comment author="sgruhier" created="2014-02-03T14:37:00Z" id="33959309">Looks good to me

Be careful to close elasticsearch ports 9200/9300 to the world when you share IPs :). Just let them opened between servers.
</comment><comment author="bhaskargr" created="2014-02-04T02:35:21Z" id="34025272">Hey it works fine now. Thanks a lot for your help.

Sent from my iPhone

On Feb 3, 2014, at 9:37 AM, "Sébastien Gruhier" notifications@github.com
wrote:

Looks good to me

Be careful to close elasticsearch ports 9200/9300 to the world when you
share IPs :). Just let them open between servers.

## 

Reply to this email directly or view it on
GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3745#issuecomment-33959309
.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation: Removed service wrapper, added rpm/deb package information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3744</link><project id="" key="" /><description>Minor upgrade to slowly fade away the service wrapper from our documentation.
</description><key id="19798646">3744</key><summary>Documentation: Removed service wrapper, added rpm/deb package information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-09-20T07:33:52Z</created><updated>2014-07-16T21:52:18Z</updated><resolved>2013-09-26T12:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>_alias needs a search filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3743</link><project id="" key="" /><description>_alias needs a filter to only return results based on a pattern; even if it's just a substring. Using Kibana 3 with a large amount of indexes in the cluster has some unfortunate overhead as it needs to download the entire list of indexes in order to operate.
</description><key id="19767738">3743</key><summary>_alias needs a search filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2013-09-19T17:00:42Z</created><updated>2013-10-04T16:42:54Z</updated><resolved>2013-10-04T16:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2013-10-04T16:42:54Z" id="25712958">Already in ES, closing issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename Engine#seacher() into Engine#acquireSearcher()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3742</link><project id="" key="" /><description>The name should reflect that the caller is responsible for
releaseing the searcher again.
</description><key id="19758184">3742</key><summary>Rename Engine#seacher() into Engine#acquireSearcher()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-19T14:33:57Z</created><updated>2014-07-16T21:52:18Z</updated><resolved>2013-09-19T14:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-19T14:35:14Z" id="24743141">++
</comment><comment author="s1monw" created="2013-09-19T14:44:56Z" id="24744115">pushed via de3cde3e1eb1984b86ba9320e6662b8bf63e1fcb
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve integration tests &amp; Add Scope support for TestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3741</link><project id="" key="" /><description>TestCluster can currently only be used in a globally shared scope.
This commit adds the ability to use the TestCluster in 3 different
scopes per test-suite. The scopes are 'Global', 'Suite' and 'Test'
where the cluster is shared across all tests, across all test methods or
not at all respectivly.
Subclasses of AbstractIntegrationTest (formerly AbstractSharedClusterTest)
can add an annotation if they need a different scope than Global (default):

```
  @ClusterScope(scope=Scope.Suite, numNodes=1)
```

This also allows to specify the number of shared nodes in that TestCluster
that are available when a test starts.
</description><key id="19755925">3741</key><summary>Improve integration tests &amp; Add Scope support for TestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-19T13:55:45Z</created><updated>2014-07-16T21:52:18Z</updated><resolved>2013-09-19T16:03:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Nodes API: Return failures in nodes response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3740</link><project id="" key="" /><description>Today, if there is a failure executing a nodes related API (nodes info, nodes stats, ...), we don't properly return them. We should.
</description><key id="19755479">3740</key><summary>Nodes API: Return failures in nodes response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Stats</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-alpha3</label></labels><created>2013-09-19T13:47:34Z</created><updated>2016-05-06T19:02:26Z</updated><resolved>2016-05-06T19:00:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2014-03-17T17:58:02Z" id="37848583">That would be a great addition. I have custom node plugins, and I have somewhat struggled to communicate failures back to the gather process.
</comment><comment author="bleskes" created="2014-07-09T12:48:40Z" id="48465067">moving to 1.4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>service.bat should handle JRE not just JDK for starting Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3739</link><project id="" key="" /><description>Currently service.bat assumes a JDK is used but there are plenty of cases where a JRE Is used instead (the default Java installation).
Instead of failing, service.bat should use that instead and give a proper warning.
</description><key id="19753613">3739</key><summary>service.bat should handle JRE not just JDK for starting Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-19T13:12:33Z</created><updated>2013-10-17T09:42:41Z</updated><resolved>2013-09-19T14:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>detect JRE (vs JDK) and invalid JAVA_HOME</comment></comments></commit></commits></item><item><title>Add documentation for nested filter's join parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3738</link><project id="" key="" /><description>The documentation for `nested` filter:

http://www.elasticsearch.org/guide/reference/query-dsl/nested-filter/

Does not explain the `join` option added in https://github.com/elasticsearch/elasticsearch/issues/2606.

&gt; The nested filters will now support the a join option. Which controls whether to perform the block join. By default this enabled, but when disabled it returns the nested documents as hits instead of the joined root documen
</description><key id="19752191">3738</key><summary>Add documentation for nested filter's join parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">btiernay</reporter><labels /><created>2013-09-19T12:42:52Z</created><updated>2013-09-20T19:23:00Z</updated><resolved>2013-09-20T19:23:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-19T13:46:39Z" id="24739558">@btiernay Sorry about that, somehow I thought it was documented!
</comment><comment author="btiernay" created="2013-09-19T13:49:44Z" id="24739781">@martijnvg Not a problem, I'm just glad the feature exists :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Added nested filter join option to the docs.</comment></comments></commit></commits></item><item><title>Add asserting searcher to track searcher references</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3737</link><project id="" key="" /><description>This assertion module also injects an AssertingIndexSearcher that
checks if our queries are all compliant with the lucene specification
which is improtant for future updates and changes in the upstream project.
</description><key id="19751990">3737</key><summary>Add asserting searcher to track searcher references</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-19T12:37:57Z</created><updated>2014-07-16T21:52:19Z</updated><resolved>2013-09-19T14:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-19T14:22:41Z" id="24742103">pushed! thanks for the review @jpountz 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Discovery to support a timeout waiting for other nodes to processing new cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3736</link><project id="" key="" /><description>The master node processing changes to cluster state, and part of the processing is publishing the cluster state to other nodes. It does not wait for the cluster state to be processed on the other nodes before it moves on to the next cluster state processing job.

This is fine, we support out of order cluster state events using versioning, and nodes can handle those cases. It does lead though to non optimal API semantics. For example, when issuing cluster health, and waiting for green state, the master node will report back once the cluster is green based on its cluster state, but that mentioned "green" state might not have been received by all other nodes yet.

Add a `discovery.zen.publish_timeout` setting, and default it to `5s`. This will give a best effort into making sure all nodes will process a cluster state within a window of time.
</description><key id="19748643">3736</key><summary>Discovery to support a timeout waiting for other nodes to processing new cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-19T11:08:53Z</created><updated>2014-02-24T11:23:42Z</updated><resolved>2013-09-19T11:09:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file></files><comments><comment>Discovery to support a timeout waiting for other nodes to processing new cluster state</comment><comment>The master node processing changes to cluster state, and part of the processing is publishing the cluster state to other nodes. It does not wait for the cluster state to be processed on the other nodes before it moves on to the next cluster state processing job.</comment></comments></commit></commits></item><item><title>DateFieldMapper.merge() can change date format and include_in_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3735</link><project id="" key="" /><description>In order to be able to add an additional date to a date field mapper,
the merge operation has to support this.

Closes #3727
</description><key id="19748117">3735</key><summary>DateFieldMapper.merge() can change date format and include_in_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-09-19T10:54:21Z</created><updated>2014-06-16T23:05:01Z</updated><resolved>2013-11-19T13:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="matthiasg" created="2013-10-14T12:05:37Z" id="26250866">+1 .. looks like a good change and easy to merge.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow omit_norms for _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3734</link><project id="" key="" /><description>Please add support for settings omit_norms on the _all field.
</description><key id="19721505">3734</key><summary>Allow omit_norms for _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-09-19T00:45:20Z</created><updated>2014-01-03T05:38:28Z</updated><resolved>2014-01-02T09:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmihael" created="2013-12-30T19:36:51Z" id="31363668">Hi. Are there any plans on this issue? It really hurts to not have norms control over _all.
</comment><comment author="serj-p" created="2013-12-30T19:38:51Z" id="31363774">Can't make human scoring by _all field without this option
</comment><comment author="s1monw" created="2013-12-30T21:07:09Z" id="31368762">maybe I miss something but what exactly is not working here?
</comment><comment author="mattweber" created="2013-12-30T21:23:00Z" id="31369588">In the all field mapping, it would be great if it supported most of the same options as a string field.  Specifically, I ran into a situation at one point where a client needed to disable norms on their all field.  This was solved by basically creating a custom all field using multi-fields.  

I imagine not supporting this has something to do with respecting the per-field boosts but I'm not sure. 
</comment><comment author="s1monw" created="2013-12-30T21:45:21Z" id="31370742">ah I see what the problem is - I guess we should allow it though! not a big deal I will add a PR in the next days.
</comment><comment author="s1monw" created="2014-01-02T21:03:53Z" id="31484410">@mattweber  it's in!
</comment><comment author="mattweber" created="2014-01-03T05:38:28Z" id="31506442">Sweet! Thanks @s1monw!  I'll check it out early next week. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Allow 'omit_norms'  on the '_all' field</comment></comments></commit></commits></item><item><title>ElasticSearch returns an empty reply when using http compression and a large size value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3733</link><project id="" key="" /><description>This query returns an empty reply:

curl -v --compressed -XGET "http://pvp-01:9200/_search" -d '{"sort": [{"microtime": {"order": "asc"}}], "query": {"bool": {"must": [{"field": {"application": {"query": "\"OLSPrice 74e019ea-1f52-11e3-b935-c86000ccc28a\""}}}, {"field": {"message_name": {"query": "\"OLSFitResultsLogMessage\""}}}, {"range": {"microtime": {"to": 2351890000080132, "from": 0}}}]}}, "from": 0, "size": 390000}'
- About to connect() to pvp-01 port 9200 (#0)
-   Trying 192.168.1.60... connected
  &gt; GET /_search HTTP/1.1
  &gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3
  &gt; Host: pvp-01:9200
  &gt; Accept: _/_
  &gt; Accept-Encoding: deflate, gzip
  &gt; Content-Length: 327
  &gt; Content-Type: application/x-www-form-urlencoded
  &gt; 
- upload completely sent off: 327out of 327 bytes
- Empty reply from server
- Connection #0 to host pvp-01 left intact
  curl: (52) Empty reply from server
- Closing connection #0

However, the same query without --compressed, or with "size": 100000 works fine.
If it matters, there are ~480000 matching documents to retrieve.
</description><key id="19720668">3733</key><summary>ElasticSearch returns an empty reply when using http compression and a large size value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thomasj02</reporter><labels /><created>2013-09-19T00:17:58Z</created><updated>2014-08-08T17:13:49Z</updated><resolved>2014-08-08T17:13:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thomasj02" created="2013-09-19T00:25:04Z" id="24709708">Apparently it's a heap space error, but it's not clear to me why turning off compression makes it work okay

[2013-09-18 19:23:32,764][WARN ][http.netty               ] [Gorgilla] Caught exception while handling client http traffic, closing connection [id: 0xbcfe3956, /192.168.1.92:34270 =&gt; /192.168.1.60:9200]
org.elasticsearch.common.netty.handler.codec.embedder.CodecEmbedderException: java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.common.netty.handler.codec.embedder.AbstractCodecEmbedder$EmbeddedChannelPipeline.notifyHandlerException(AbstractCodecEmbedder.java:242)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:599)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)
        at org.elasticsearch.common.netty.handler.codec.embedder.EncoderEmbedder.offer(EncoderEmbedder.java:70)
        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.encode(HttpContentEncoder.java:201)
        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:121)
        at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)
        at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:158)
        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:100)
        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:92)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:190)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:172)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:152)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:146)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:346)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:146)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:133)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.common.netty.buffer.HeapChannelBuffer.&lt;init&gt;(HeapChannelBuffer.java:42)
        at org.elasticsearch.common.netty.buffer.BigEndianHeapChannelBuffer.&lt;init&gt;(BigEndianHeapChannelBuffer.java:34)
        at org.elasticsearch.common.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)
        at org.elasticsearch.common.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:68)
        at org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.&lt;init&gt;(DynamicChannelBuffer.java:58)
        at org.elasticsearch.common.netty.buffer.ChannelBuffers.dynamicBuffer(ChannelBuffers.java:221)
        at org.elasticsearch.common.netty.handler.codec.compression.JdkZlibEncoder.encode(JdkZlibEncoder.java:184)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:66)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneStrictEncoder.doEncode(OneToOneStrictEncoder.java:35)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
        at org.elasticsearch.common.netty.handler.codec.compression.JdkZlibEncoder.handleDownstream(JdkZlibEncoder.java:221)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)
        at org.elasticsearch.common.netty.handler.codec.embedder.EncoderEmbedder.offer(EncoderEmbedder.java:70)
        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.encode(HttpContentEncoder.java:201)
        at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:121)
        at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:254)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:704)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:671)
        at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:158)
        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:100)
        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:92)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:190)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:172)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:152)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:146)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:346)
</comment><comment author="spinscale" created="2013-09-19T08:06:29Z" id="24723030">Hey,

Most likely the data you are trying to send to the client needs to be loaded into memory before being compressed. Loading 390000 documents into memory can cause these issues.

I do not think that this is the best approach to get that much documents from elasticsearch (your RAM needs to scale with the amount of documents returned, a second parallel request can kill the application). You should take a look at the scan search type at http://www.elasticsearch.org/guide/reference/api/search/search-type/
That type allows you to easily retrieve such a big amount of documents.
</comment><comment author="thomasj02" created="2013-09-19T14:39:23Z" id="24743482">Unfortunately I can't use scan since I need to get the documents back in sorted order.

I can understand if it takes a lot of RAM to load and send the documents, but it doesn't make sense that it requires more memory to send back a compressed version of the documents than it does to send an uncompressed version. GZip is a streaming compression protocol, so ES should be able to send out the data compressed while using almost zero additional memory.
</comment><comment author="clintongormley" created="2014-08-08T17:13:49Z" id="51630916">Sorted scrolling can now be done almost as efficiently as scanning
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Joda dateOptionalTime can not parse unix timestamps with sub-ms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3732</link><project id="" key="" /><description>This is a corner case:

This will fail to process when inserted into a date field:
{ "datefield": "1379298836.022108"}

ElasticSearch expects that to be sent as a numeric, not as a string:
{ "datefield": 1379298836.022108}

There does not seem to be a method for working around this issue, other than doing an update across fields ( ctx._source.fixeddate = (double) ctx._source.datefield }
</description><key id="19716362">3732</key><summary>Joda dateOptionalTime can not parse unix timestamps with sub-ms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2013-09-18T22:20:06Z</created><updated>2014-01-04T05:51:05Z</updated><resolved>2014-01-04T05:51:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2014-01-04T05:51:05Z" id="31572329">Oops -- repeated. Bug report. Better report at issue #4617 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk import stalls on date parsing errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3731</link><project id="" key="" /><description>Sending six insert records with dates that can not be processed causes the _bulk HTTP endpoint to hang indefinitely. With less than six, errors are returned, e.g.: 

MapperParsingException[failed to parse [start_time]]; nested: MapperParsingException[failed to parse date field [1379271987.081399], tried both date format [dateOptionalTime], and timestamp number with locale [null]]; nested: IllegalArgumentException[Invalid format: \"1379271987.081399\" is malformed at \"7.081399\"]; "}},
</description><key id="19716224">3731</key><summary>Bulk import stalls on date parsing errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">Downchuck</reporter><labels><label>feedback_needed</label></labels><created>2013-09-18T22:17:10Z</created><updated>2014-12-24T16:24:19Z</updated><resolved>2014-12-24T16:24:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-19T07:30:01Z" id="24721540">Hey,

can you provide your mapping and the data you sent, so we can reproduce this?
</comment><comment author="Downchuck" created="2013-09-19T17:44:30Z" id="24758895">Trying to make a reduced test case. No luck yet; this may be a bogus bug report -- I'm getting responses now, they're just a bit slow (7 seconds for 128 inserts) in _bulk. 
</comment><comment author="Downchuck" created="2013-09-19T17:57:43Z" id="24759900">Reproduced! While it takes 7 seconds when posting to a data node, it hangs when posting to a non-data node.

elasticsearch.yml:
node.master: false
node.data: false

``` bash
[cloud-user@pritc006 apim]$ curl -s http://0:9200/test-brokencase/test-brokencase/_mapping | jq .
{
  "test-brokencase": {
    "properties": {
      "uuid": {
        "index_options": "docs",
        "omit_norms": true,
        "index": "not_analyzed",
        "type": "string"
      },
      "start_time": {
        "format": "dateOptionalTime",
        "type": "date"
      }
    },
    "_id": {
      "path": "uuid"
    }
  }
}
[cloud-user@pritc006 apim]$ head -n 8 case.txt
{"index":{}}
{"uuid":"7e566a80-8d1f-42af-9edb-468ae5be034e","start_time":"1379271987.081399"}
{"index":{}}
{"uuid":"7a5734bb-2451-406b-8028-83529b392cc8","start_time":"1379298836.022108"}
{"index":{}}
{"uuid":"3a8230d2-05a4-42e2-b7e9-eaef546d1477","start_time":"1379287838.811609"}
{"index":{}}
{"uuid":"c59ff74d-5f53-4aa8-b550-79aac9d2e703","start_time":"1379288063.348802"}

```
</comment><comment author="Downchuck" created="2013-10-03T17:44:37Z" id="25642004">Has anyone at ES confirmed this issue?
</comment><comment author="imotov" created="2013-10-03T18:02:46Z" id="25643507">@Downchuck I cannot reproduce it locally for some reason. Could you run 

```
curl "localhost:9200/_nodes?jvm=true&amp;os=true"
```

on one of the nodes and post output here?
</comment><comment author="Downchuck" created="2013-11-21T21:21:38Z" id="29025674">0.90.3
</comment><comment author="Downchuck" created="2013-11-21T21:21:41Z" id="29025676">{"ok":true,"cluster_name":"elasticsearch","nodes":{"cR11ZFtWSf-rCrlhW6bfMw":{"name":"DN7DMPES01007","transport_address":"inet[/10.194.82.239:9300]","hostname":"s01007..com","version":"0.90.3","http_address":"inet[/10.194.82.239:9200]","os":{"refresh_interval":1000,"available_processors":2,"cpu":{"vendor":"Intel","model":"Xeon","mhz":2666,"total_cores":2,"total_sockets":2,"cores_per_socket":1,"cache_size":"12kb","cache_size_in_bytes":12288},"mem":{"total":"15.5gb","total_in_bytes":16726851584},"swap":{"total":"7.9gb","total_in_bytes":8588877824}},"jvm":{"pid":31945,"version":"1.6.0_31","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"20.6-b01","vm_vendor":"Sun Microsystems Inc.","start_time":1383597217089,"mem":{"heap_init":"4gb","heap_init_in_bytes":4294967296,"heap_max":"3.9gb","heap_max_in_bytes":4290641920,"non_heap_init":"23.1mb","non_heap_init_in_bytes":24313856,"non_heap_max":"130mb","non_heap_max_in_bytes":136314880,"direct_max":"3.9gb","direct_max_in_bytes":4290641920}}},"2PwbkNxjTwq0_4EOwWI9_A":{"name":"DN7DMPES01002","transport_address":"inet[/10.194.82.219:9300]","hostname":"s01002..com","version":"0.90.3","http_address":"inet[/10.194.82.219:9200]","os":{"refresh_interval":1000,"available_processors":2,"cpu":{"vendor":"Intel","model":"Xeon","mhz":2666,"total_cores":2,"total_sockets":2,"cores_per_socket":1,"cache_size":"12kb","cache_size_in_bytes":12288},"mem":{"total":"15.5gb","total_in_bytes":16726851584},"swap":{"total":"15.9gb","total_in_bytes":17178812416}},"jvm":{"pid":17676,"version":"1.6.0_31","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"20.6-b01","vm_vendor":"Sun Microsystems Inc.","start_time":1383597233281,"mem":{"heap_init":"4gb","heap_init_in_bytes":4294967296,"heap_max":"3.9gb","heap_max_in_bytes":4290641920,"non_heap_init":"23.1mb","non_heap_init_in_bytes":24313856,"non_heap_max":"130mb","non_heap_max_in_bytes":136314880,"direct_max":"3.9gb","direct_max_in_bytes":4290641920}}},"yQKx1wD9Tfiv-Z2jnGdo2A":{"name":"DN7DMPES01003","transport_address":"inet[/10.194.80.204:9300]","hostname":"s01003..com","version":"0.90.3","http_address":"inet[/10.194.80.204:9200]","os":{"refresh_interval":1000,"available_processors":2,"cpu":{"vendor":"Intel","model":"Xeon","mhz":2666,"total_cores":2,"total_sockets":2,"cores_per_socket":1,"cache_size":"12kb","cache_size_in_bytes":12288},"mem":{"total":"15.5gb","total_in_bytes":16726851584},"swap":{"total":"15.9gb","total_in_bytes":17178812416}},"jvm":{"pid":13863,"version":"1.6.0_31","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"20.6-b01","vm_vendor":"Sun Microsystems Inc.","start_time":1383597216038,"mem":{"heap_init":"4gb","heap_init_in_bytes":4294967296,"heap_max":"3.9gb","heap_max_in_bytes":4290641920,"non_heap_init":"23.1mb","non_heap_init_in_bytes":24313856,"non_heap_max":"130mb","non_heap_max_in_bytes":136314880,"direct_max":"3.9gb","direct_max_in_bytes":4290641920}}},"Y96Qn0nYSMOWjga3ikK7ig":{"name":"DN7DMPES01006","transport_address":"inet[/10.194.82.233:9300]","hostname":"s01006..com","version":"0.90.3","http_address":"inet[/10.194.82.233:9200]","os":{"refresh_interval":1000,"available_processors":2,"cpu":{"vendor":"Intel","model":"Xeon","mhz":2666,"total_cores":2,"total_sockets":2,"cores_per_socket":1,"cache_size":"12kb","cache_size_in_bytes":12288},"mem":{"total":"15.5gb","total_in_bytes":16726851584},"swap":{"total":"7.9gb","total_in_bytes":8588877824}},"jvm":{"pid":7863,"version":"1.6.0_31","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"20.6-b01","vm_vendor":"Sun Microsystems Inc.","start_time":1383597217940,"mem":{"heap_init":"4gb","heap_init_in_bytes":4294967296,"heap_max":"3.9gb","heap_max_in_bytes":4290641920,"non_heap_init":"23.1mb","non_heap_init_in_bytes":24313856,"non_heap_max":"130mb","non_heap_max_in_bytes":136314880,"direct_max":"3.9gb","direct_max_in_bytes":4290641920}}},"2UWXxsKmQNutVmnHWUrlmA":{"name":"DN7DMPES01001","transport_address":"inet[/10.194.80.203:9300]","hostname":"s01001..com","version":"0.90.3","http_address":"inet[/10.194.80.203:9200]","os":{"refresh_interval":1000,"available_processors":2,"cpu":{"vendor":"Intel","model":"Xeon","mhz":2666,"total_cores":2,"total_sockets":2,"cores_per_socket":1,"cache_size":"12kb","cache_size_in_bytes":12288},"mem":{"total":"15.5gb","total_in_bytes":16726851584},"swap":{"total":"15.9gb","total_in_bytes":17178812416}},"jvm":{"pid":13526,"version":"1.6.0_31","vm_name":"Java HotSpot(TM) 64-Bit Server VM","vm_version":"20.6-b01","vm_vendor":"Sun Microsystems Inc.","start_time":1383597215899,"mem":{"heap_init":"1gb","heap_init_in_bytes":1073741824,"heap_max":"1019.8mb","heap_max_in_bytes":1069416448,"non_heap_init":"23.1mb","non_heap_init_in_bytes":24313856,"non_heap_max":"130mb","non_heap_max_in_bytes":136314880,"direct_max":"1019.8mb","direct_max_in_bytes":1069416448}}}}}
</comment><comment author="Downchuck" created="2013-11-21T21:22:02Z" id="29025694">This really seems to be part of the error handling code as it tries to stack trace each time it fails to parse a date.
</comment><comment author="clintongormley" created="2014-07-23T13:46:30Z" id="49875208">@Downchuck Unable to replicate this on 0.90.3 or master.  Is this still an issue?
</comment><comment author="Downchuck" created="2014-08-20T17:04:53Z" id="52808822">From what I recall, the underlying issue was simply that a stack trace was run for every insertion, so in the context of a large bulk upload, the time taken to run the traces was so large as to essentially freeze.

I can try this in master in the future.

-Charles
</comment><comment author="clintongormley" created="2014-12-24T16:24:19Z" id="68061913">No further info. Please reopen if you still see this on recent versions.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default Class Loader is Thread.currentThread().getContextClassLoader();</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3730</link><project id="" key="" /><description>Hi,

I was just wondering for what reason the preference is to set the default classloader to be the current thread, instead of the classloader that loaded the current class? Would it not make more sense to delegate directly to the classloader of Class, instead of the current thread, unless specified otherwise?

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/Classes.java?source=cc#L48-L60
</description><key id="19713525">3730</key><summary>Default Class Loader is Thread.currentThread().getContextClassLoader();</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BearPuncher</reporter><labels /><created>2013-09-18T21:23:26Z</created><updated>2014-08-08T17:12:12Z</updated><resolved>2014-08-08T17:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-18T21:24:35Z" id="24700757">In most cases when I built libraries in Java, thats the best default to have (check most libs that ends up loading classes, they do that). You can override it if you want.
</comment><comment author="BearPuncher" created="2013-09-18T21:30:24Z" id="24701160">Thanks for the quick response. The custom classloader I am working with has a few gaps in the logic, adopting best/consistent practices would be obviously beneficial.

Cheers
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add time zone setting for relative date math in range filter/query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3729</link><project id="" key="" /><description>Hi,

When using a range filter/query and using relative date math to round, it's not always preferable to have the resulting date be in UTC. Adding some way to specify the time zone of the relative date would remedy this. Ideally this could either be an explicit offset or the name of a time zone recognized by Joda-Time time.

For example, say I am in the America/New_York time zone and I want to schedule a query to run every day at 2 AM that gets back all documents from yesterday, without hard coding any dates or time zones. In it's current form, you can define a range filter/query like this:

```
"range" : {
  "_timestamp" : {
    "gte" : "now/d-1d",
    "lt" : "now/d"
  }
}
```

Unfortunately since ElasticSearch defaults to the UTC timezone, this would only find documents from yesterday in the UTC time zone, and not America/New_York like we want.

This can be worked around by either adding an absolute timestamp that's calculated at the application layer or adding the current time zone offset to the range in the query. However, it would be much easier to specify the intended time zone as part of the range definition, similar to the way the date histogram facet works.

Perhaps a syntax similar to `now/d-1d||&lt;tz&gt;` would work. The `||&lt;tz&gt;` would be optional and `&lt;tz&gt;` would follow the same rules as the pre_zone and post_zone settings for the date histogram facet.
</description><key id="19711337">3729</key><summary>add time zone setting for relative date math in range filter/query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">tmkujala</reporter><labels><label>enhancement</label></labels><created>2013-09-18T20:44:31Z</created><updated>2014-08-04T13:46:55Z</updated><resolved>2014-08-04T13:43:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="samcday" created="2014-05-23T07:03:42Z" id="43977116">(+1) for this. The date range syntax is awesome, but I unfortunately had to switch away from it and calculate dates in my application, as the results I'm looking for are sensitive to timezone.
</comment><comment author="pschultz" created="2014-06-18T16:20:06Z" id="46458335">We would also very much like to see support for this. Please note that in the use case of the OP the timezone offset would not be good enough since it may change over the course of the year (DST).
</comment><comment author="ankitajain90" created="2014-07-17T05:45:26Z" id="49260587">1) How can I define timezone format in ranges ? 
ranges: [{ from :now - 1d/d , to: now}]
now gives me UTC time. I need time in IST format.
2) can I define my ranges with now and time hardcoded to 1300hrs

{ from :  yesterday (1pm) to : today 1pm}
</comment><comment author="dadoonet" created="2014-08-04T13:46:55Z" id="51061581">FYI, a new option will be available in 1.4.0 with PR #7113 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacetParser.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Search: add time zone setting for relative date math in range filter/query</comment></comments></commit></commits></item><item><title>Installation with RPM On Centos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3728</link><project id="" key="" /><description>I just installed the RPM from http://www.elasticsearch.org/download/ on Centos 6.4 using :  sudo rpm -Uvh elasticsearch-0.90.5.noarch.rpm 

I can do service elasticsearchs start, stop, etc but where are all the folders for putthing things like plug-ins and data in? The only two files I have are under /etc/elasticsearch which are elasticsearch.yml  and logging.yml.

Should I install elastic search without the rpm?
</description><key id="19709464">3728</key><summary>Installation with RPM On Centos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ProdigyView</reporter><labels /><created>2013-09-18T20:08:21Z</created><updated>2013-09-18T20:19:04Z</updated><resolved>2013-09-18T20:19:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-18T20:19:04Z" id="24695878">May I ask you to send questions like this to our [mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch)?

Anyways, you can find the default configuration in the sysconfig file:

ES_HOME=/usr/share/elasticsearch - home
LOG_DIR=/var/log/elasticsearch - for the logs
DATA_DIR=/var/lib/elasticsearch - for your data
WORK_DIR=/tmp/elasticsearch - working directory
CONF_DIR=/etc/elasticsearch - configuration directory

Plugins go under ES_HOME/plugins by default.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding date format to an existing mapping silently fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3727</link><project id="" key="" /><description>We have an existing index with data and mapping with a date field with two existing formats, e.g.:

existing mapping:
    "tzoffset_timestamp": {
        "type": "date",
        "store": true,
        "format": "EEE MMM dd HH:mm:ss.S Z yyyy||EEE MMM dd HH:mm:ss.SSS Z yyyy",
        "include_in_all": true
    }

when executing curl put mapping command with an additional format:

```
"tzoffset_timestamp": {
    "type": "date",
    "store": true,
    "format": "EEE MMM dd HH:mm:ss.S Z yyyy||EEE MMM dd HH:mm:ss.SSS Z yyyy||yyyy-MM-dd'T'HH:mm:ss.SSSZZ",
    "include_in_all": true
}
```

succeeds with "ok / acknowledged", yet getting the mapping back provides original two formats.
</description><key id="19700942">3727</key><summary>Adding date format to an existing mapping silently fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">isg8</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2013-09-18T17:41:32Z</created><updated>2013-11-19T13:47:55Z</updated><resolved>2013-11-19T13:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>DateFieldMapper.merge() can change date format and include_in_all</comment></comments></commit></commits></item><item><title>Percolate request during node recovery fails until recovery is completed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3726</link><project id="" key="" /><description>We have a 4 node ES v0.90.1 cluster. We have multiple indexes each with 1 primary, 3 replicas and 5 shards. Total indices size is ~200 GB/node. 

When we start a node, it takes time to go from status yellow -&gt; green. It recovers all the shards and sometimes the process can take upto an hour. During this time, we see that ES is creating index one by one, and _percolator is usually one of the last one(log below). 

Issue is - Any percolate requests hitting this node until the _percolator index is created locally(which is essentially the whole node startup recovery time) return 0 hits. Since indexing/search requests work fine on a node starting up in yellow state, percolator requests should work fine too. 

```
[2013-09-16 01:52:36,245][DEBUG][indices.cluster] [mynode] [myindex_a] creating index
[2013-09-16 01:52:40,160][DEBUG][indices.cluster] [mynode] [myindex_b] creating index
[2013-09-16 01:52:42,353][DEBUG][indices.cluster] [mynode] [myindex_c] creating index
.....
.....
[2013-09-16 02:23:17,633][DEBUG][indices.cluster] [mynode] [myindex_y] creating index
[2013-09-16 02:25:16,477][DEBUG][indices.cluster] [mynode] [myindex_z] creating index
[2013-09-16 02:40:16,357][DEBUG][indices.cluster] [mynode] [_percolator] creating index

```
</description><key id="19698323">3726</key><summary>Percolate request during node recovery fails until recovery is completed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajhalani</reporter><labels /><created>2013-09-18T16:54:17Z</created><updated>2014-05-29T17:08:47Z</updated><resolved>2014-05-29T17:08:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajhalani" created="2014-05-29T17:08:47Z" id="44556870">Closing it as V1.x+ had a radical redesign of peroclate architecture and issue is no longer relevant. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>service.bat fails unexpectedly if JAVA_HOME contains spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3725</link><project id="" key="" /><description>The detection of Java version fails if `JAVA_HOME` contains spaces. In turn, this causes the `service.bat` to exit without any message.
</description><key id="19681733">3725</key><summary>service.bat fails unexpectedly if JAVA_HOME contains spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-18T13:54:51Z</created><updated>2013-10-24T10:48:17Z</updated><resolved>2013-09-18T14:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-09-18T14:13:43Z" id="24666931">Fixed by 
https://github.com/elasticsearch/elasticsearch/commit/709add033b9e6cf2f6c093045207b45d7da49a56 and
https://github.com/elasticsearch/elasticsearch/commit/20bf8f4b00d6f37c56342b01f2799358a3435377
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException on facet_filter when no actual filter is added</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3724</link><project id="" key="" /><description>to reproduce(0.90.5):

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/bar/1 -d '{"id":1,"content":1}'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{"id":2,"content":2}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{"and":{"filters":[]}}}}}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{ }}}}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{"and":{"filters":[{ "term" : { "id" : 1 } }]}}}}}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{ "term" : { "id" : 1 } }}}}'
```

first two searches fail with:
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[l8GT-BrMQYqwmTWkppI5tw][foo][3]: SearchParseException[[foo][3]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"match_all\":{}},\"facets\":{\"content\":{\"terms\":{\"field\":\"content\"},\"facet_filter\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][2]: SearchParseException[[foo][2]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"match_all\":{}},\"facets\":{\"content\":{\"terms\":{\"field\":\"content\"},\"facet_filter\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][4]: SearchParseException[[foo][4]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"match_all\":{}},\"facets\":{\"content\":{\"terms\":{\"field\":\"content\"},\"facet_filter\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][1]: SearchParseException[[foo][1]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"match_all\":{}},\"facets\":{\"content\":{\"terms\":{\"field\":\"content\"},\"facet_filter\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][0]: SearchParseException[[foo][0]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"match_all\":{}},\"facets\":{\"content\":{\"terms\":{\"field\":\"content\"},\"facet_filter\":{ }}}}]]]; nested: NullPointerException; }]","status":400}
</description><key id="19676144">3724</key><summary>NullPointerException on facet_filter when no actual filter is added</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-09-18T11:57:26Z</created><updated>2013-09-18T20:20:01Z</updated><resolved>2013-09-18T20:20:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-09-18T11:57:50Z" id="24658754">(this used to work on 0.90.0)
</comment><comment author="javanna" created="2013-09-18T18:23:28Z" id="24687412">Thanks for reporting this! As I noticed this is not the first time you report a NPE using an empty filter, I'm making sure this won't happen again (famous last words!). A commit will follow shortly.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedFilter.java</file><file>src/main/java/org/elasticsearch/search/facet/FacetParseElement.java</file><file>src/main/java/org/elasticsearch/search/facet/filter/FilterFacetParser.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Prevented empty filters from causing NPE</comment></comments></commit></commits></item><item><title>Delete Template: When deleting with * and no templates exists, don't 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3723</link><project id="" key="" /><description /><key id="19672727">3723</key><summary>Delete Template: When deleting with * and no templates exists, don't 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-18T10:27:58Z</created><updated>2013-09-18T10:28:29Z</updated><resolved>2013-09-18T10:28:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/main/java/org/elasticsearch/common/regex/Regex.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file></files><comments><comment>Delete Template: When deleting with * and no templates exists, don't 404</comment><comment>closes #3723</comment></comments></commit></commits></item><item><title>RPM should not be starting service on installation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3722</link><project id="" key="" /><description>Hi all

Currently we are looking at deploying elasticsearch on our aws cloud. We have an automated deployment process that can grab an RPM and put it onto an instance and then bake the image for deployment to multiple instances. However this gets broken by the RPM due to it automatically switching on the elasticsearch service on installation.

Fedora suggest that you should not automatically switch on the service as this is insecure, instead you should rely on the init.d script via chkconfig to switch on the service:

http://fedoraproject.org/wiki/Packaging:SysVInitScript#Why_don.27t_we....

Quote:

&gt; start the service after installation? 
&gt; Installations can be in changeroots, in an installer context, or in other situations 
&gt; where you don't want the services started. "

I'm happy to submit a patch if you think this is valid.

Dip
</description><key id="19672056">3722</key><summary>RPM should not be starting service on installation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dipthegeezer</reporter><labels><label>feature</label><label>v1.0.0.RC1</label></labels><created>2013-09-18T10:11:09Z</created><updated>2014-01-06T09:45:45Z</updated><resolved>2014-01-03T16:41:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lehcim" created="2013-09-26T12:39:50Z" id="25164048">Thank you dipthegeezer for opening this issue, it was on my todo list for a long time but I never take the time to wrote it. So, yes, it would be nice if ES instance does not start right after RPM installation.
</comment><comment author="kfox1111" created="2013-11-13T19:31:27Z" id="28425845">I have this problem to. I'm trying to bring up elasticsearch in a cluster, and it is autostarting standalone because it does not have a config file yet. This causes split brained situations. This is very bad. Trying to shut down the service right away sometimes fails as well due to the pid not getting written out yet.

I tried putting "exit 0" in /etc/sysconfig/elasticsearch as a work around, but the rpm is also not treating that file as a config file and is overwriting it always.

So, there seems to be no reliable way to work around this issue without fixing the rpm.
</comment><comment author="ejain" created="2013-11-15T18:29:50Z" id="28591907">I have a long-standing, hard to reproduce split brain issue when starting new nodes; hadn't considered that this could be the cause. Hope the referenced pull request gets merged soon!
</comment><comment author="lukas-vlcek" created="2013-11-19T14:34:19Z" id="28794151">I would also vote for not starting ES in `postinstall` script.

If @dipthegeezer's PR is not going to make it in then I am proposing a little relaxed alternative.
Let's replace line https://github.com/elasticsearch/elasticsearch/blob/master/src/rpm/scripts/postinstall#L38 in `postinstall` with something like

```
if [ "$ELASTICSEARCH_POST_INSTALL_STARTUP" != "false" ] ; then
    startElasticsearch
fi
```

Now you can do

```
export ELASTICSEARCH_POST_INSTALL_STARTUP=false;
yum install elasticsearch-0.90.5-1.noarch.rpm;
```

to have ES installed but not started.
</comment><comment author="kfox1111" created="2013-11-19T16:48:28Z" id="28807591">Interesting idea, though I don't think that would work in the kickstart case.

I would also set a bad precedent. If every network service rpm needed its own env variable like this, you'd end up with:
ELASTICSEARCH_POST_INSTALL_STARTUP=false
APACHE_POST_INSTALL_STARTUP=false
TOMCAT_POST_INSTALL_STARTUP=false
INETD_POST_INSTALL_STARTUP=false
...
yum do something.

And it just snowballs.
</comment><comment author="lukas-vlcek" created="2013-11-19T17:28:19Z" id="28812319">Sorry for my ignorance @kfox1111 but are you saying that Tomcat or Apache get started when you do?

```
yum install &lt;official&gt;.rpm
```

I would like to use official ES RPM as much as possible but due to this autokickstart feature I have to build and ship custom RPM internally. BTW, I double checked with my colleagues (Fedora developers and official package maintainers) and they confirmed that what is stated in the link that @dipthegeezer pointed out is a valid recommendation and is also followed in RHEL.

May be I missed something.
</comment><comment author="kfox1111" created="2013-11-19T17:37:25Z" id="28813505">No, sorry, I meant, if the elasticsearch rpm expects to be able to set variables to do this, then what is stopping the Apache rpm from wanting to do the same, and then the Tomcat folks, and then... Everyone will want to do it this way. It will eventually become an unmaintainable mess of packages. While it seems like a good idea at a single package level, when you have systems of lots of packages, it is much harder on the user since the user now has to know all of the environment variables (s)he has to set before doing an install for any package that may get sucked in.
</comment><comment author="lukas-vlcek" created="2013-11-19T20:54:41Z" id="28834454">@kfox1111 seriously, why would they wanted to copy this approach? Starting the service after installation is just one command away ([sudo] services tomcat start) however ppl usually want to configure services before they start them (as well as I do want to configure ES before it is started). I just do not understand the rationale about why Elasticsearch RPM does it differently. That is all. And the workaround I proposed can just provide a small workaround for me (and may be for other people) that would warmly welcome if there was a way to have it the other way around. Hope it is clear now.

//cc @spinscale please any ideas?
</comment><comment author="kfox1111" created="2013-11-19T21:18:03Z" id="28836493">@lukas-vlcek: I'm agreeing with you. I think its bad to start the service automatically. Its recommended against by all rpm using distro's.

I think the reasoning ElasticSearch made, is something like: "its simpler for the user this way. They just install the rpm and then use ElasticSearch."

The point that I was trying to make is, if ElasticSearch can make this argument compellingly, why wouldn't other network services not want to follow suit. Is ElasticSearch somehow special in that it needs to be "easier to start" then other network services? Wouldn't it be nice if you just yum installed apache and it was up and running too? Its just a rabit hole that I don't think should be gone down at all. ElasticSearch, while great, is no different then any of the other network service and its rpm shouldn't behave differently then the rest. yum install it, then turn on the service when you are ready. It should not be looking at environment variables to autostart things, because there be dragons.
</comment><comment author="dipthegeezer" created="2013-11-19T21:38:51Z" id="28838426">@lukas-vlcek Just to throw my two cents in we've had to do exactly what you've done; built and shipped our own RPM that doesn't start the service. This introduces a lot of overhead on our part in maintaining what is essentially a divergent version from the main elasticsearch code base. Especially when most documentation suggests you should never start a service on install.
</comment><comment author="spinscale" created="2013-11-20T08:32:17Z" id="28870871">Hey,

Update: I will change the packages (RPM and deb) to not start by default in the next days, along with some other features on my 'give the packages more love'-TODO list.

Not part of this ticket, but you may be interested in that as well: We are working on repositories and soon we should have something for you to try out.
</comment><comment author="lukas-vlcek" created="2013-11-20T08:53:55Z" id="28871838">Excellent! Looking forward to using rpm right away in the future :smile: 
Thanks @spinscale 
</comment><comment author="spinscale" created="2013-11-27T10:00:43Z" id="29372421">Folks,

it would be awesome if you could comment on https://github.com/elasticsearch/elasticsearch/pull/4256

anything missing? Did I overlook something? 

Thanks for any feedback!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Do not start packages on installation</comment></comments></commit></commits></item><item><title>"index.blocks.read_only" value returned as string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3721</link><project id="" key="" /><description>When performing an admin indices get settings request (/index_name/_settings) in ES v0.90.4, the "index.blocks.read_only" value is returned as a string rather than a boolean value.
</description><key id="19655594">3721</key><summary>"index.blocks.read_only" value returned as string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlinn</reporter><labels /><created>2013-09-18T00:19:52Z</created><updated>2014-08-08T17:11:48Z</updated><resolved>2014-08-08T17:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:11:48Z" id="51630681">All settings are strings, regardless of the type that they represent.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>New elasticsearch-0.90.4 getting [discovery.zen] [Masters, Alicia][failed to ping, tried [3] times, each with maximum [30s] timeout]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3720</link><project id="" key="" /><description>I got following error when I try to start a new elasticsearch install:

[2013-09-17 10:05:46,288][INFO ][discovery.zen            ] [Masters, Alicia] master_left [[Him][8wa0Y4rUS1KgPRGHQLGayw][inet[/10.67.93.32:9300]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]

Following is what I did:
 curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.4.zip
unzip elasticsearch-0.90.4.zip
cd elasticsearch-0.90.4/bin
./elasticsearch -f

uname -a
Linux hostname 2.6.32-042stab079.5 #1 SMP Fri Aug 2 17:16:15 MSK 2013 x86_64 x86_64 x86_64 GNU/Linux

java -version
java version "1.7.0_25"
OpenJDK Runtime Environment (rhel-2.3.10.4.el6_4-x86_64)
OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)

more elasticsearch.log
[2013-09-17 09:28:09,041][INFO ][node                     ] [Archie Corrigan] version[0.90.4], pid[17649], build[8cb1a9b/2013-09-16T14:11:00Z]
[2013-09-17 09:28:09,043][INFO ][node                     ] [Archie Corrigan] initializing ...
[2013-09-17 09:28:09,050][INFO ][plugins                  ] [Archie Corrigan] loaded [], sites []
[2013-09-17 09:28:11,968][INFO ][node                     ] [Archie Corrigan] initialized
[2013-09-17 09:28:11,968][INFO ][node                     ] [Archie Corrigan] starting ...
[2013-09-17 09:28:12,167][INFO ][transport                ] [Archie Corrigan] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {ine
t[/10.67.85.210:9300]}
[2013-09-17 09:28:16,343][INFO ][discovery.zen            ] [Archie Corrigan] master_left [[Him][8wa0Y4rUS1KgPRGHQLGayw][inet[/10.67.93.32:9300
]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2013-09-17 09:28:16,347][INFO ][discovery                ] [Archie Corrigan] elasticsearch/fn7lg1I9SXKBhFSyfYRw1w
[2013-09-17 09:28:16,354][INFO ][http                     ] [Archie Corrigan] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {ine
t[/10.67.85.210:9200]}
[2013-09-17 09:28:16,354][INFO ][node                     ] [Archie Corrigan] started
[2013-09-17 09:50:59,403][DEBUG][action.admin.cluster.state] [Archie Corrigan] Serving cluster state request using version 0
[2013-09-17 09:52:04,000][INFO ][node                     ] [Archie Corrigan] stopping ...
[2013-09-17 09:52:04,024][INFO ][node                     ] [Archie Corrigan] stopped
[2013-09-17 09:52:04,024][INFO ][node                     ] [Archie Corrigan] closing ...
[2013-09-17 09:52:04,029][INFO ][node                     ] [Archie Corrigan] closed
[2013-09-17 10:05:37,838][INFO ][node                     ] [Masters, Alicia] version[0.90.4], pid[17984], build[8cb1a9b/2013-09-16T14:11:00Z]
[2013-09-17 10:05:37,839][INFO ][node                     ] [Masters, Alicia] initializing ...
[2013-09-17 10:05:37,851][INFO ][plugins                  ] [Masters, Alicia] loaded [], sites []
[2013-09-17 10:05:41,796][INFO ][node                     ] [Masters, Alicia] initialized
[2013-09-17 10:05:41,797][INFO ][node                     ] [Masters, Alicia] starting ...
[2013-09-17 10:05:42,111][INFO ][transport                ] [Masters, Alicia] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {ine
t[/10.67.85.210:9300]}
[2013-09-17 10:05:46,288][INFO ][discovery.zen            ] [Masters, Alicia] master_left [[Him][8wa0Y4rUS1KgPRGHQLGayw][inet[/10.67.93.32:9300
]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2013-09-17 10:05:46,295][INFO ][discovery                ] [Masters, Alicia] elasticsearch/Bccg0dYlRkSfbkViVNO7MQ
[2013-09-17 10:05:46,305][INFO ][http                     ] [Masters, Alicia] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {ine
t[/10.67.85.210:9200]}
[2013-09-17 10:05:46,305][INFO ][node                     ] [Masters, Alicia] started
[2013-09-17 10:15:12,255][INFO ][node                     ] [Masters, Alicia] stopping ...
[2013-09-17 10:15:12,292][INFO ][node                     ] [Masters, Alicia] stopped
[2013-09-17 10:15:12,292][INFO ][node                     ] [Masters, Alicia] closing ...
[2013-09-17 10:15:12,297][INFO ][node                     ] [Masters, Alicia] closed
</description><key id="19634467">3720</key><summary>New elasticsearch-0.90.4 getting [discovery.zen] [Masters, Alicia][failed to ping, tried [3] times, each with maximum [30s] timeout]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raydemandforce</reporter><labels /><created>2013-09-17T19:10:20Z</created><updated>2013-12-19T18:01:39Z</updated><resolved>2013-12-19T18:01:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-19T18:01:39Z" id="30950989">Sounds like your issue was caused because you have another node running on your LAN, same clustername but another version or something?

Have you been able to reproduce it with latest elasticsearch version?

If so, could you reopen this issue and provide more details about your architecture (number of nodes running, version of each node...)?

Closing for now. Thanks! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable a configurable delay for shutdown when calling killproc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3719</link><project id="" key="" /><description>Useful as soem nodes may take longer than others.
</description><key id="19628804">3719</key><summary>Enable a configurable delay for shutdown when calling killproc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tmclaugh</reporter><labels><label>:Packaging</label></labels><created>2013-09-17T17:30:05Z</created><updated>2014-08-22T19:02:39Z</updated><resolved>2014-08-22T19:02:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tmclaugh" created="2013-09-17T18:47:07Z" id="24612768">Not sure if this should be even longer.  killproc() on RHEL calls checkpid, sleeps for a second, and then sleeps for the delay before calling checkpid a third time.
</comment><comment author="spinscale" created="2013-09-18T06:48:12Z" id="24644289">Nice catch. Thanks!

What do you consider a useful default here from your experience?
</comment><comment author="tmclaugh" created="2013-09-18T15:04:10Z" id="24671213">That I'm not exactly sure.  This is the first time I know of that we've
run into it.  Default if no delay is passed to killproc() is 3 which
results in a 4 second total wait.  Her QA cluster wasn't that busy
yesterday so they were no help and I'm sure she doesn't want me knocking
out her production cluster. :)  I added the sysconfig value to make it
easy to change via Puppet.

On 09/18/2013 02:48 AM, Alexander Reelsen wrote:

&gt; Nice catch. Thanks!
&gt; 
&gt; What do you consider a useful default here from your experience?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/3719#issuecomment-24644289.
</comment><comment author="tmclaugh" created="2014-08-22T17:20:21Z" id="53091991">Unnecessary via #3973

(nothingtodohere)
</comment><comment author="clintongormley" created="2014-08-22T17:26:34Z" id="53092783">@tmclaugh does your last comment we should close this?
</comment><comment author="tmclaugh" created="2014-08-22T17:28:56Z" id="53093061">I closed this since it appears the work went forward in #3973 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion suggest API does not support size parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3718</link><project id="" key="" /><description>The new completion suggester is great, but it doesn't support a size parameter and instead always returns just 5 results. I'd like to be able to get all results. 
</description><key id="19628401">3718</key><summary>Completion suggest API does not support size parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dspangen</reporter><labels /><created>2013-09-17T17:22:35Z</created><updated>2014-10-14T16:48:25Z</updated><resolved>2013-09-18T14:14:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-18T07:03:40Z" id="24644805">Hey,

I just tried and it worked, using this sample

```
curl -X DELETE localhost:9200/hotels

curl -X PUT localhost:9200/hotels -d '
{
  "mappings": {
    "hotel" : {
      "properties" : {
        "name" : { "type" : "string" },
        "city" : { "type" : "string" },
        "name_suggest" : {
          "type" :     "completion"
        }
      }
    }
  }
}'

curl -X PUT localhost:9200/hotels/hotel/1 -d ' {  "name_suggest" : "Hotel Munich 1" }'
curl -X PUT localhost:9200/hotels/hotel/2 -d ' {  "name_suggest" : "Hotel Munich 2" }'
curl -X PUT localhost:9200/hotels/hotel/3 -d ' {  "name_suggest" : "Hotel Munich 3" }'
curl -X PUT localhost:9200/hotels/hotel/4 -d ' {  "name_suggest" : "Hotel Munich 4" }'
curl -X PUT localhost:9200/hotels/hotel/5 -d ' {  "name_suggest" : "Hotel Munich 5" }'
curl -X PUT localhost:9200/hotels/hotel/6 -d ' {  "name_suggest" : "Hotel Munich 6" }'
curl -X PUT localhost:9200/hotels/hotel/7 -d ' {  "name_suggest" : "Hotel Munich 7" }'
curl -X PUT localhost:9200/hotels/hotel/8 -d ' {  "name_suggest" : "Hotel Munich 8" }'
curl -X PUT localhost:9200/hotels/hotel/9 -d ' {  "name_suggest" : "Hotel Munich 9" }'
curl -X PUT localhost:9200/hotels/hotel/10 -d ' {  "name_suggest" : "Hotel Munich 10" }'

curl localhost:9200/_refresh


curl -X POST localhost:9200/hotels/_suggest -d '
{
  "hotels" : {
    "text" : "h", 
    "completion" : {
      "field" : "name_suggest",
      "size" : 1
    }
  }
}'
```

can you show me the difference in your example?
</comment><comment author="dspangen" created="2013-09-18T14:14:06Z" id="24666960">Yea, I just confirmed that it does, and that I was doing:

```
curl -X POST localhost:9200/hotels/_suggest -d '
{
  "hotels" : {
    "text" : "h", 
    "completion" : {
      "field" : "name_suggest"
    },
    "size" : 1
  }
}'
```

Which will return `ElasticSearchIllegalArgumentException[[suggest] does not support [size]]`

Certainly not what I wanted to do. Thanks!
</comment><comment author="Hillgod" created="2013-10-12T01:14:05Z" id="26187082">It'd be really nice to have the fact that you can put "size" in "completion" in the docs.  It took me quite a long time to find this, and I still can't find anything in the docs indication you can or should (shouldn't?) put size within the "completion" object.
</comment><comment author="bjmc" created="2014-10-02T17:09:03Z" id="57662719">Seconding @Hillgod's request for documentation of this feature. There seems to be no evidence of it [here.](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-completion.html)
</comment><comment author="clintongormley" created="2014-10-14T16:48:24Z" id="59077418">@Hillgod @bjmc I've added the docs here: https://github.com/elasticsearch/elasticsearch/commit/7e916d0b8bc5df1beff003285a8b2e8d09e473f0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo-distance sorting should accept the "sort_mode" parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3717</link><project id="" key="" /><description>Geo-distance sorting currently accepts the "mode" parameter, but it should accept "sort_mode" instead.
</description><key id="19624954">3717</key><summary>Geo-distance sorting should accept the "sort_mode" parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-17T16:22:09Z</created><updated>2013-09-17T16:27:01Z</updated><resolved>2013-09-17T16:27:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file></files><comments><comment>Added sort_mode and sortMode parameters to _geo_distance sort.</comment><comment>Previously it just support the "mode" parameter, which is</comment><comment>inconsistent</comment></comments></commit></commits></item><item><title>running Elasticsearch as a service on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3716</link><project id="" key="" /><description>Add support for running Elasticsearch as a service on Windows through `service.bat` and Apache Commons Daemon.
The ZIP file contains 3 new exe files:
`elasticsearch-service-x64.exe` - service wrapper for 64-bit JVMs
`elasticsearch-service-x86.exe` - service wrapper for 32-bit JVMs
`elasticsearch-service-mgr.exe` - service manager (works with both 62 and 32 wrappers)

plus a new `.bat` file which is the entry point called `service.bat`

Simply running `service.bat` gives information on its usage:

```
&gt; service.bat
Usage: service.bat install|remove|start|stop|manager [SERVICE_ID]

&gt; service install
Installing service      :  'elasticsearch-service-x64'
Using JAVA_HOME (64-bit):  c:\jvm\jdk1.7
The service 'elasticsearch-service-x64' has been installed.

&gt; service start
The service 'elasticsearch-service-x64' has been started

&gt; service stop
The service 'elasticsearch-service-x64' has been stopped

&gt; service remove
The service 'elasticsearch-service-x64' has been removed
```

In most cases, one simply installs the service and potentially does extra customizations through the service manager

```
&gt; service install
&gt; service manager
```

Once installed, Elasticsearch acts as any other Windows service so can be started, stopped, have different Startup types, etc...

Note that service.bat relies on the same conventions as `elasticsearch.bat` and the debian package - meaning the environment variables will be used (if present) during the install to set certain options.

P.S. The `service.bat` automatically does detection of the architecture (64 vs 32) of the target JVM detected through `JAVA_HOME` variable. If a certain JVM is desired, make sure that `JAVA_HOME` points to it.
</description><key id="19611682">3716</key><summary>running Elasticsearch as a service on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>feature</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-17T12:38:38Z</created><updated>2013-09-20T05:55:01Z</updated><resolved>2013-09-17T13:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-09-17T12:40:04Z" id="24584606">Associated pull request (pushed to [master](https://github.com/elasticsearch/elasticsearch/pull/3702) and then cherry-picked to 0.90).
</comment><comment author="Brimstedt" created="2013-09-20T05:39:02Z" id="24791066">Seems to be problem if JAVA_HOME has spaces ("c:\program files\java...")
</comment><comment author="costin" created="2013-09-20T05:55:01Z" id="24791469">@Brimstedt This has been fixed in master some days ago:
https://github.com/elasticsearch/elasticsearch/issues/3725

And please open up a new issue next time instead of commenting on a closed one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify NestedFieldComparators for numerics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3715</link><project id="" key="" /><description>The average and sum comparators basically share the same code which is
copy-past today. We can simplify this into a base class which reduces
code duplication and prevents copy-paste bugs.
</description><key id="19610722">3715</key><summary>Simplify NestedFieldComparators for numerics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-17T12:17:21Z</created><updated>2014-07-16T21:52:19Z</updated><resolved>2013-09-17T21:39:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-17T12:38:07Z" id="24584499">+1 Looks good, lets push it!
</comment><comment author="jpountz" created="2013-09-17T20:21:38Z" id="24619030">&gt; 44 additions and 70 deletions

I like it, +1!
</comment><comment author="s1monw" created="2013-09-17T21:39:06Z" id="24624754">pushed here is the hash: 1499881c363078f96568ed00b8f3559608da3df1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for IPv6 mapping type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3714</link><project id="" key="" /><description>Currently I can't use the ip mapping type as I have fields that can be either IPv4 or IPv6. However, being able to use range queries is really useful but I can't make use of them because I have to treat the field as a string to handle the case when the field contains an IPv6 value.

Obviously this causes extra hassle as storage would then require 128 bits and when searching, range queries using IPv6 addresses shouldn't match IPv4 addresses, unless you're using the ::ffff:d.d.d.d notation, and IPv6 addresses shouldn't match IPv4 range queries at all.

(I found [this thread](http://elasticsearch-users.115913.n3.nabble.com/Support-for-first-class-IPv6-address-type-td3960748.html) when this has been raised previously)
</description><key id="19606937">3714</key><summary>Support for IPv6 mapping type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bodgit</reporter><labels><label>:Mapping</label><label>feature</label><label>high hanging fruit</label><label>stalled</label></labels><created>2013-09-17T10:43:10Z</created><updated>2016-09-26T17:26:18Z</updated><resolved>2016-04-17T11:06:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abh" created="2013-11-26T00:12:48Z" id="29256131">I'd like to convert a postgresql based application to use ES but got hung up on missing this feature, too. The queries are using netmasks/cidrs so just having the IPv6 address as a string won't be "good enough".
</comment><comment author="dadoonet" created="2013-11-26T05:55:06Z" id="29269230">For IP V6, just mark your field as not_analyzed in mapping.
</comment><comment author="abh" created="2013-11-26T20:28:51Z" id="29330873">@dadoonet That doesn't make any sense.
</comment><comment author="dadoonet" created="2013-11-26T21:19:20Z" id="29334960">Do you mean that you don't understand my answer or my answer does not answer to your question?
</comment><comment author="abh" created="2013-11-26T21:32:05Z" id="29335989">@dadoonet What's the point of the "ip type" if a reasonable answer to supporting IPv6 is "just make it a not analyzed string"? They're not the same thing, I'd hope.
</comment><comment author="dadoonet" created="2013-11-26T22:34:18Z" id="29341982">IP type is only for IP v4. Type name should be ipv4 instead of ip.
For ipv6 I don't think a special type is needed. Keeping ipv6 as non tokenized string should do the job.

Hhow do you expect ipv6 content to be converted to?
</comment><comment author="abh" created="2013-11-26T22:38:57Z" id="29342328">It could be converted to a number, for instance, and then allow range searches etc similar to the "ipv4" type.

Better yet the "ip type" should "just work" for both (similar to what postgresql does, for example).
</comment><comment author="bodgit" created="2013-11-26T22:50:58Z" id="29343199">There are ways of expressing IPv6 addresses that would likely fail a simple string-based match, the whole '::' expansion for one.
</comment><comment author="dadoonet" created="2013-11-27T09:17:00Z" id="29369822">@bodgit Very good point! I'm going to think about it a bit more.
</comment><comment author="lifo101" created="2013-12-17T13:46:11Z" id="30751459">I have an app that stores iPv4/6 addresses as DECIMAL(39,0) in a mysql database which allows for very easy range searching. I wait for the day when ES will support something similar for IPv6 so I can finally use ES for indexing my database.
</comment><comment author="jvbrandis" created="2014-01-17T17:44:58Z" id="32627832">When storing IPv6 addresses, I store it as a "fully formatted" IPv6-string, i.e. XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX:XXXX, regardless of zeros (so, never shortening a segment to less than four digits, and never shortcutting segments with ::).
This way, all IPv6 addresses are fully sortable and searchable, so this should work with ES (using not-analyzed mapping). However, it is very space-consuming when comparing to what an IPv6 address really is, which is 16 bytes (while this becomes 40 bytes...)
Also, if putting IPv4-addresses into this mix, sorting/filtering on a range will lead to problems mixing IPv4 and IPv6.
(This could be solved by using the IPv4-mapping format of IPv6, that is all IPv4 addresses are stored as IPv6 as ::FFFF:XXXX:XXXX (last four bytes being the IPv4 address)

The other approach is to store as a binary field, using 16 bytes, (still storing IPv4-addresses in IPv4-mapped IPv6-format). My approach to this in mysql is actually a BINARY(16) column.
However, this is inconvenient as manually browsing/inspecting the data becomes cumbersome.

So; I am also eagerly awaiting ES support for IPv6, storing IPv6-addresses in numeric format, but with support for properly displaying them and accepting query parameters in IP-format.
</comment><comment author="cpdean" created="2014-03-28T18:35:15Z" id="38953438">+1. would like this feature
</comment><comment author="ioc32" created="2014-03-29T17:03:47Z" id="39001695">As @abh pointed out the fact ES is not currently supporting both protocols equally is a show stopper for many applications - to be ported or to be implemented from scratch.

ES is pretty much becoming a de facto standard when it comes to scalable event storage, search and analysis. In my particular use case, and I do not think I am the only one here, I deal with IPv4 just as much as with IPv6. Having both address families under a single, coherent data type is to be desired. Mappings, queries, indexing... would become unified and consequently easier to use for everyone. 

@dadoonet I wonder what's the reason for ES to support IPv4-only data types in first place. Was it a technical decision due to implementation difficulties, was it a matter of priorities? Or, on the other hand, was it a consequence of you guys perceiving ES users did not care about IPv6? Is it at least in your roadmap?
</comment><comment author="dadoonet" created="2014-03-29T18:06:39Z" id="39003611">@ioc32 It is on my TODO list for sure! I need to find some quiet time to work on it.
</comment><comment author="ioc32" created="2014-03-29T18:09:53Z" id="39003715">@dadoonet great! Thank you for updating us!
</comment><comment author="kimchy" created="2014-03-29T20:19:26Z" id="39007662">the reason is simple, ipv4 can easily be translated to 64bit long, which supports range constructs, ipv6 is more complex.
</comment><comment author="cpdean" created="2014-03-30T22:21:17Z" id="39041948">definitely looking forward to this.  It'll really round out the ELK stack for feature complete network analysis. thanks!
</comment><comment author="kimchy" created="2014-03-31T01:51:28Z" id="39047950">understood, though for now, if you can get around with prefix checks, you can map the IP as string.
</comment><comment author="xaque208" created="2014-04-01T02:56:21Z" id="39166174">+1 to defending @dadoonet's quiet time.  I'd love to see this happen.  
</comment><comment author="Dunaeth" created="2014-04-03T05:07:50Z" id="39413195">Wouldn't it be possible to use fixed length lucene binary field types for ips and use binary sorting (I read about binary utf8 sorting in lucene, but I lack somme skills on the subject) ?
</comment><comment author="jpountz" created="2014-04-03T06:26:47Z" id="39416783">It is indeed possible to encode ipv6 ips as binary fields, Lucene doesn't require index terms to be UTF-8 sequences, it can be anything. The challenge here is more that for IPs, we need to support efficient ranges because that's typically how these fields are filtered. Lucene provides support for efficient ranges with numeric fields (see [NumericRangeQuery](http://lucene.apache.org/core/4_7_0/core/org/apache/lucene/search/NumericRangeQuery.html)): basically every field gets indexed with different precision levels, and this allows range queries to visit few terms no matter how large the range is (the fewer terms are visited the more efficient queries are). So we would need a similar mechanism for storing ipv6 addresses.
</comment><comment author="seti123" created="2014-04-03T21:04:04Z" id="39504641">+1  
</comment><comment author="clintongormley" created="2014-07-11T09:41:26Z" id="48712076">Depends on https://issues.apache.org/jira/browse/LUCENE-5596
</comment><comment author="avleen" created="2015-07-14T21:52:34Z" id="121402857">I see we're still blocked on Lucene's support for BigInt for this. But that ticket hasn't seen any action in a while either.
Any updates for this @clintongormley? IPv6 is becoming a real thing, so this would be really handy :-)
</comment><comment author="xaque208" created="2015-07-15T19:35:35Z" id="121722580">Its a 14 year old protocol.  We're well beyond 'real thing' :)
</comment><comment author="jpountz" created="2015-07-15T21:57:17Z" id="121761672">The Lucene issue is stalled indeed, as it proved very hard to integrate... The feature is currently exposed as an experimental postings format which is not supported in terms of backward compatibility.

With small numbers (up to 64 bits) today we have static pre-computed ranges, which is probably fine. For instance for ints (32 bits) we have a default precision step of 8 bits which means that we pre-compute ranges for all numbers that have the same 24, 16 or 8 upper bits (0-256, 256-512, 512-768, ..., 0-65536, 65536-131072, 131072-196608, ..., 0-16777216, 16777216-33554432, 33554432-50331648, ...). Any arbitrary range can be translated to a union of these pre-computed ranges, and this is the way we manage to have fast ranges on numerics.

With high numbers of bits, like 128 here, the space-time trade-off becomes tricky I think. For instance with a precision step of 16, we would have to index 8 tokens per value while range queries would still visit hundreds of thousands of terms in the worst-case.

Given that ipv6 addresses tend to use the lower bytes less, maybe that would be fine, but I'm a bit reluctant to expose a new field type for ipv6 addresses that would not perform well for range queries. An option could be to have a new type for ipv6 addresses that would only support sorting and aggs but not queries, however I'm not sure how useful it would be?
</comment><comment author="avleen" created="2015-07-16T01:15:52Z" id="121794942">Agreed.
/64's are the smallest allocations that are generally given out, so
searching for a range may not (initially) need more precision than that.
If we see an IPv6 address, we can store the range of the /64 it is in, and
then work up from there?
/64, /32, /16, /8, /4, /2, /1, /0
That's 8 bits there, and from a practical perspective it might be
sufficient. Most end users get a /64, which makes searching in that easy.
ISPs get at least /32 sized blocks.

Most IPv6 address allocated today, when converted to decimal are about 38
bytes. That means 76 bytes (upper and lower bounds) to store each range. So
about 600 bytes of storage required for the precision, per address, in
addition to the ~38 bytes for the address itself.. That's quite a lot, but
that's really just the way it is - we can't make these numbers smaller ;-)

If we restrict range searches to at least /64, could this then work out?

On Wed, Jul 15, 2015 at 5:57 PM Adrien Grand notifications@github.com
wrote:

&gt; The Lucene issue is stalled indeed, as it proved very hard to integrate...
&gt; The feature is currently exposed as an experimental postings format which
&gt; is not supported in terms of backward compatibility.
&gt; 
&gt; With small numbers (up to 64 bits) today we have static pre-computed
&gt; ranges, which is probably fine. For instance for ints (32 bits) we have a
&gt; default precision step of 8 bits which means that we pre-compute ranges for
&gt; all numbers that have the same 24, 16 or 8 upper bits (0-256, 256-512,
&gt; 512-768, ..., 0-65536, 65536-131072, 131072-196608, ..., 0-16777216,
&gt; 16777216-33554432, 33554432-50331648, ...). Any arbitrary range can be
&gt; translated to a union of these pre-computed ranges, and this is the way we
&gt; manage to have fast ranges on numerics.
&gt; 
&gt; With high numbers of bits, like 128 here, the space-time trade-off becomes
&gt; tricky I think. For instance with a precision step of 16, we would have to
&gt; index 8 tokens per value while range queries would still visit hundreds of
&gt; thousands of terms in the worst-case.
&gt; 
&gt; Given that ipv6 addresses tend to use the lower bytes less, maybe that
&gt; would be fine, but I'm a bit reluctant to expose a new field type for ipv6
&gt; addresses that would not perform well for range queries. An option could be
&gt; to have a new type for ipv6 addresses that would only support sorting and
&gt; aggs but not queries, however I'm not sure how useful it would be?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/3714#issuecomment-121761672
&gt; .
</comment><comment author="bodgit" created="2015-07-16T08:06:57Z" id="121871628">How would this work with a type that handles both IPv4 and IPv6? As I originally stated in my use case I don't know the address family ahead of time, only that it is "an IP address" so I would prefer a type that can handle both. If that meant storing IPv4 addresses as IPv6-mapped it means that for such addresses, you _do_ care about the lesser significant bits more as the address is `::ffff:d.d.d.d` and so the first 96 bits are always going to be the same.
</comment><comment author="avleen" created="2015-09-25T02:36:41Z" id="143107337">FWIW, ARIN announced depletion of their free IP pool today:
http://teamarin.net/category/ipv4-depletion/
</comment><comment author="hanej" created="2015-09-28T21:01:26Z" id="143872701">Our access logs use a combination of IPv6 and IPv4 in the same field so we're in the same situation as @bodgit 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `node.mode` with `local` or `network`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3713</link><project id="" key="" /><description>Compared to setting `node.local` to `true`, would be nicer to support `node.mode` with values of `local` or `network`. 

Note, `node.local` is still supported.
</description><key id="19583879">3713</key><summary>Add `node.mode` with `local` or `network`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-16T22:33:13Z</created><updated>2013-09-16T22:34:15Z</updated><resolved>2013-09-16T22:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoveryModule.java</file><file>src/main/java/org/elasticsearch/transport/TransportModule.java</file></files><comments><comment>Add `node.mode` with `local` or `network`</comment><comment>Compared to setting node.local to true, would be nicer to support node.mode with values of local or network.</comment></comments></commit></commits></item><item><title>Elasticsearch startup script doesn't work from directory with spaces in path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3712</link><project id="" key="" /><description>1. Unzip the elasticsearch archive into a directory containing spaces in the one of the path components, e.g., /Volumes/Storage/test stuff.
2. Run /Volumes/Storage/test stuff/elasticsearch/elasticsearch-0.90.4/bin/elasticsearch.

It reports that ES_CLASSPATH isn't set.

The culprit is at line 78. Needs to be "`dirname "$0"`"/elasticsearch.in.sh; do

Yes, the doubly nested double quotes work.
</description><key id="19580220">3712</key><summary>Elasticsearch startup script doesn't work from directory with spaces in path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">brooksrbrown</reporter><labels><label>bug</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-16T21:24:14Z</created><updated>2013-09-17T06:16:27Z</updated><resolved>2013-09-17T06:15:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-17T06:16:27Z" id="24566248">Thanks a lot for reporting and supplying the fix as well!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Elasticsearch startup script supports directories containing spaces</comment></comments></commit></commits></item><item><title>plugin manager doesn't correctly install head plugin (no _site folder)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3711</link><project id="" key="" /><description>Downloaded the latest `v0.90.4` version and installed the head plugin as usual `elasticsearch/bin/plugin --install mobz/elasticsearch-head`
It has reported the following:

`-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip...
Downloading ........................DONE
Installed mobz/elasticsearch-head into /home/alex/tools/elasticsearch/elasticsearch-0.90.4/plugins/head
Identified as a _site plugin, moving to _site structure ...
`
In the log it was said that no plugins were detected: `[2013-09-16 22:33:24,849][INFO ][plugins                  ] [Deadly Ernest] loaded [], sites []
`
I've compared the structure of the `plugins` directory from the previous version `v0.90.3` and the structure was the following `plugins/head/_site`, in the `v0.90.4` the structure is different: `plugins/head/`

Apparantely, the following line was not executed:
`Identified as a _site plugin, moving to _site structure ...`

I moved the content of the head plugin manually into plugins/head/_site and now plugin was detected.
</description><key id="19578603">3711</key><summary>plugin manager doesn't correctly install head plugin (no _site folder)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zshamrock</reporter><labels /><created>2013-09-16T20:54:13Z</created><updated>2013-09-17T14:49:48Z</updated><resolved>2013-09-17T14:49:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-16T21:01:08Z" id="24544224">Thanks for reporting!. We fixed it in #3707, and will release 0.90.5 probably tomorrow to address it.
</comment><comment author="javanna" created="2013-09-17T14:49:48Z" id="24594059">0.90.5 has been released, the issue related to site plugins is fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better handling of /_all/_search when no indices exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3710</link><project id="" key="" /><description>At the moment, a search against a cluster which has no indices will return a 503 Service Unavailable.  This is a "big" error code which in other places means that a node or cluster is down.

Rather throw an IndexMissing exception with a 404.  Similarly, the same should apply to count, mlt etc etc
</description><key id="19571890">3710</key><summary>Better handling of /_all/_search when no indices exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-16T19:26:23Z</created><updated>2013-09-17T09:54:06Z</updated><resolved>2013-09-16T21:32:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-09-16T19:32:05Z" id="24537633">Actually, my preference here (when searching on `_all` would be to return zero hits instead of throwing an error.
</comment><comment author="kimchy" created="2013-09-16T21:02:35Z" id="24544336">yea, that would also be consistent with our broadcast based APIs (like count, suggest, ...)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/test/java/org/elasticsearch/search/indicesboost/SimpleIndicesBoostSearchTests.java</file></files><comments><comment>Better handling of /_all/_search when no indices exist</comment><comment>closes #3710</comment></comments></commit></commits></item><item><title>Allow passing index names to _mapping and _search via JSON POST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3709</link><project id="" key="" /><description>I am having an issue with Kibana and IIS Reverse Proxy to the Elasticsearch server

https://github.com/elasticsearch/kibana/issues/501

Is it possible to provide an API for _mapping and _search so that we can pass the index via a message body?
</description><key id="19566191">3709</key><summary>Allow passing index names to _mapping and _search via JSON POST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trajano</reporter><labels /><created>2013-09-16T17:50:50Z</created><updated>2014-08-08T17:11:05Z</updated><resolved>2014-08-08T17:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:11:05Z" id="51630598">The index can be passed in the body in an msearch request.  But nothing similar exists for the mapping.

That said, after reading the kibana issue you linked to, you could use index wildcards, and you could also use aliases to add all the indices to a short alias.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analyzer caching problem with QueryParserService? (0.90.4)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3708</link><project id="" key="" /><description>I don't have a good explanation for the following issue we are seeing, but it's definitely there. I dug as deep as I could and it seems like an analyzer caching issue with the QueryParserService.

Here's the setup:
1. Index mapping defines several full-text fields, and then some non-analyzed fields The full-text fields don't have an analyzer set - we index them using the _analyzer field.
2. We have a custom analyzer plugged in; it is a PerFieldAnalyzerWrapper where the default analyzer is KeywordAnalyzer and then a custom stemming analyzer is configured for the full-text fields. The analyzer is properly configured as verified to work correctly.
3. We use QueryString query to executed searches (yes, I know). The following query works as expected - the full-text fields get analyzed using the correct analyzer, and the non full-text fields are just kept untouched and are used as a whole:

```
{"filtered":{"query":{"bool":{"must":[{"query_string":{"query":"url:\"https://www.facebook.com/111111111111/\" url:\"https://www.facebook.com/111111111112/\"","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}},{"query_string":{"query":"foo title:bar","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}],"must_not":{"query_string":{"query":"test","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}}},"filter":{"terms":{"lang":["english"]}}}}
```
1. But when I switch the order of the clauses, the url field (which is the not_analyzed one) is being tokenized and I can verify it is going through the stemmer defined in custom_analyzer for only 4 fields, by name:

```
{"filtered":{"query":{"bool":{"must":[{"query_string":{"query":"foo title:bar","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}, {"query_string":{"query":"url:\"https://www.facebook.com/111111111111/\" url:\"https://www.facebook.com/111111111112/\"","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}],"must_not":{"query_string":{"query":"test","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}}},"filter":{"terms":{"lang":["english"]}}}}
```

The must_not clause doesn't seem to have any effect.
1. I debugged this as deep as Lucene's QueryParser, which apparently calls the correct analyzer object but still gets a tokenized stream.

Not sure how to go about this any further - will be happy to help with nailing this down.
</description><key id="19560683">3708</key><summary>Analyzer caching problem with QueryParserService? (0.90.4)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-09-16T16:16:15Z</created><updated>2014-07-01T12:39:54Z</updated><resolved>2014-07-01T12:39:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-16T16:25:39Z" id="24523502">@synhershko Can you make a gist for this issue? (with your mapping mapping and a sample doc)
</comment><comment author="synhershko" created="2013-09-16T16:51:28Z" id="24525478">https://gist.github.com/synhershko/2bb415462761ca9ecb37
</comment><comment author="kimchy" created="2013-09-16T17:43:22Z" id="24529204">I wonder why you use the per field analyzer, and not just have the hebrew analyzer registered, and then configure it on the mappings?
</comment><comment author="synhershko" created="2013-09-16T17:46:44Z" id="24529497">Because the document may be in another language - we use the _analyzer field when indexing after determining the document language, and the query language is defined by the user.
</comment><comment author="synhershko" created="2013-09-17T12:45:04Z" id="24584901">@martijnvg I was able to reproduce this in a vanilla project - see https://dl.dropboxusercontent.com/u/9218365/ElasticSearchBug.tar.gz

I did mvn install -DskipTests after updating elasticsearch-test locally to use 0.90.4, you might need to do that as well.

Using 0.90.4, if you put a breakpoint in IndexQueryParserService.java:282 you will see the parent_forum field is being tokenized - while it definitely shouldn't according to both the index definition AND the defined analyzer behavior.

There is one query that does work - query1. The only difference is in the order of the clauses. And if it is being executed before the failing_query it will make that query pass. This is another reason why I suspect this to be a caching issue.

Will be happy to help nailing this down further.
</comment><comment author="synhershko" created="2013-09-18T15:02:41Z" id="24671089">FWIW, this seems to work fine with 0.90.0. I'll try to bisect this further - if you could refactor this into a an ES test I'll be happy to git bisect this all the way to the faulty commit.
</comment><comment author="kimchy" created="2013-09-25T11:41:42Z" id="25079425">I found the problem, it relates to the fact that NamedAnalyzer uses global reuse strategy, and the per field analyzer that the custom plugin returns actually needs the per field one. This has been addressed in Lucene, and will be fixed when upgradign to 4.5. Once 4.5 is out, we need to fix it on our end as well, see: c282a3784ef728d564397745d2531f05c7faec41.
</comment><comment author="synhershko" created="2014-06-24T00:20:53Z" id="46918572">@kimchy @s1monw can this be safely closed now?
</comment><comment author="kimchy" created="2014-06-24T10:52:16Z" id="46957138">@synhershko yes, I think so.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugins: Automatic detection of site plugins fails to copy over the content to `_site`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3707</link><project id="" key="" /><description>The automatic detection of site plugins fails to properly copy the content of the downloaded repo into `[plugin_name]/_site`.

Current workaround is to simply move the content over to `_site` under the relevant plugin directory manually.
</description><key id="19560407">3707</key><summary>Plugins: Automatic detection of site plugins fails to copy over the content to `_site`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-16T16:11:46Z</created><updated>2013-09-17T09:05:19Z</updated><resolved>2013-09-16T16:13:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-16T16:38:05Z" id="24524400">Another way to install a site plugin in the meantime is using git as well:

``` sh
git clone https://github.com/karmi/elasticsearch-paramedic.git plugins/paramedic/_site
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Plugins: Automatic detection of site plugins fails to copy over the content to `_site`</comment><comment>closes #3707</comment></comments></commit></commits></item><item><title>NestedFieldComparator misses to copy slot if root doc has docID==0 and 'Avg' is used </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3706</link><project id="" key="" /><description>The average `NestedFieldComparator` doesn't copy the underlying Comparator slot if `rootDoc == 0`. This was triggered by some of our tests lately. 
</description><key id="19556145">3706</key><summary>NestedFieldComparator misses to copy slot if root doc has docID==0 and 'Avg' is used </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-16T15:08:58Z</created><updated>2013-09-16T15:32:11Z</updated><resolved>2013-09-16T15:32:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-16T15:12:14Z" id="24517303">Nice catch!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/nested/NestedFieldComparatorSource.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Copy slow in wrapped comparator even if root doc is the first doc in the</comment><comment>segment.</comment></comments></commit></commits></item><item><title>Added third highlighter type based on lucene postings highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3705</link><project id="" key="" /><description>Added third highlighter type based on lucene postings highlighter

Requires field index_options set to "offsets" in order to store positions and offsets in the postings list.
Considerably faster than the plain highlighter since it doesn't require to reanalyze the text to be highlighted: the larger the documents the better the performance gain should be.
Requires less disk space than term_vectors, needed for the fast_vector_highlighter.
Breaks the text into sentences and highlights them. Uses a BreakIterator to find sentences in the text. Plays really well with natural text, not quite the same if the text contains html markup for instance.
Treats the document as the whole corpus, and scores individual sentences as if they were documents in this corpus, using the BM25 algorithm.

Uses forked version of lucene postings highlighter to support:
- per value discrete highlighting for fields that have multiple values, needed when number_of_fragments=0 since we want to return a snippet per value
- manually passing in query terms to avoid calling extract terms multiple times, since we use a different highlighter instance per doc/field, but the query is always the same

The lucene postings highlighter api is  quite different compared to the existing highlighters api, the main difference being that it allows to highlight multiple fields in multiple docs with a single call, using sequential IO.
The way it is introduced in elasticsearch in this first round is a compromise trying not to change the current highlight api, which works per document, per field. The main disadvantage is that we lose the sequential IO, but we can always refactor the highlight api to work with multiple documents later on.

Supports pre_tag, post_tag, number_of_fragments (0 highlights the whole field), require_field_match, order by score and html encoding.

Closes #3704
</description><key id="19556013">3705</key><summary>Added third highlighter type based on lucene postings highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-09-16T15:07:07Z</created><updated>2014-07-09T13:55:32Z</updated><resolved>2013-10-24T22:19:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-24T22:22:03Z" id="27036914">Merged 48ac9747a8c9a3206aaff165add1c12e75c79604 and a8d3c797e989265347b236d0f2b32d4ec9b0bbe1 !!!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>New highlighter based on lucene postings highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3704</link><project id="" key="" /><description>Lucene includes the [postings highlighter](http://blog.mikemccandless.com/2012/12/a-new-lucene-highlighter-is-born.html) for some time now. The following are its main advantages:
- faster than plain highlighter and requires less disk space than `term_vectors`, needed for the fast vector highlighter
- outputs nice sentences as snippets
- scores snippets based on BM25 algorithm

It's worth to try and add it as our third highlighter implementation.
</description><key id="19555502">3704</key><summary>New highlighter based on lucene postings highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>feature</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-16T14:59:32Z</created><updated>2013-10-25T09:13:36Z</updated><resolved>2013-10-24T22:04:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-16T15:27:54Z" id="24518603">Neat!  I'll give this a shot when I have a chance.
</comment><comment author="s1monw" created="2013-10-25T09:13:36Z" id="27075923">NO WAY! ;) 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XDefaultPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightModule.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java</file><file>src/test/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighterTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Added third highlighter type based on lucene postings highlighter</comment></comments></commit></commits></item><item><title>Elasticsearch head requests return 403 when made to readonly index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3703</link><project id="" key="" /><description>Elasticsearch head requests return 403 when made to readonly index.

curl recreation of issue here.

https://gist.github.com/paulrblakey/6580888

issue originally opened with [ruflin/elastica](https://github.com/ruflin/Elastica/issues/457)
</description><key id="19551389">3703</key><summary>Elasticsearch head requests return 403 when made to readonly index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">paulrblakey</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2013-09-16T13:51:25Z</created><updated>2015-04-23T13:21:53Z</updated><resolved>2015-04-23T13:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-09-18T15:24:13Z" id="24673119">I've noticed this behavior as well using `POST` for search. It seems like it only allows `GET` requests on readonly indexes.
</comment><comment author="sparks" created="2014-08-14T17:08:22Z" id="52212099">I'm currently having this issue, it can causes some weird situations.

If I want to check for the existence of an index, e.g `curl -i -X HEAD localhost:9200/test` you will get a `403` if the index is set to `read_only : true`. So now you have to set `read_only : false` on your index before you know if it exists (and catch the potential `404`)
</comment><comment author="tlrx" created="2015-01-08T13:57:55Z" id="69181831">Setting  `read_only: true` on an index blocks:
- any write operation such as indexing/deleting/updating a document
- any read &amp; write operation on index metadata such as checking if the index exists, checking if a type exists, and also reading or updating the settings/aliases/warmers/mappings (and many more) of the index.

As a workaround, you can set `index.block.write: true`. This will block all write operation on the index but you will still be able to check for existance or update the settings/mappings etc.

Related to #2833, #5855, #5876, #8102
</comment><comment author="elvarb" created="2015-01-15T13:12:05Z" id="70083322">@tlrx Will this workaround to set "index.block.write: true" not fulfill the need to make the data read only?

For logging data, protecting the json documents is what is needed and stopping writes (and hopefully updates and deletes as well) fulfills that need.

To protect whole indexes, for example for debugging of the health of the index or whatever "elasticsearch" related need comes up, by making the whole index read only.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlock.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlockLevel.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/repositories/RepositoryBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/tasks/PendingTasksBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/flush/FlushBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/optimize/OptimizeBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/refresh/RefreshBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsBlocksTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/blocks/SimpleBlocksTests.java</file><file>src/test/java/org/elasticsearch/cluster/BlockClusterStatsTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/cluster/shards/ClusterSearchShardsTests.java</file><file>src/test/java/org/elasticsearch/gateway/RecoverAfterNodesTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/exists/indices/IndicesExistsTests.java</file><file>src/test/java/org/elasticsearch/indices/exists/types/TypesExistsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetMappingsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/GetSettingsBlocksTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/UpdateSettingsTests.java</file><file>src/test/java/org/elasticsearch/indices/state/OpenCloseIndexTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateBlocksTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/IndicesWarmerBlocksTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Internal: Add METADATA_READ and METADATA_WRITE blocks</comment></comments></commit></commits></item><item><title>add elasticsearch as a service for Windows platforms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3702</link><project id="" key="" /><description>based on Apace Commons Daemon
supports both x64 and x86
introduces service.bat for installing/removing the service
</description><key id="19542123">3702</key><summary>add elasticsearch as a service for Windows platforms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels /><created>2013-09-16T10:00:26Z</created><updated>2014-06-13T16:48:04Z</updated><resolved>2013-09-17T12:16:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2013-09-16T10:25:51Z" id="24500879">Since calling the 32-bit executable with the x64 service doesn't work and simply gives a cryptic message that disappears right away, I'm thinking of renaming elasticsearch-service.exe to elasticsearch-service-x86.exe so the architecture is obvious (as oppose to being implied).
</comment><comment author="kimchy" created="2013-09-16T10:44:24Z" id="24501696">can we also make sure that those files are only part of the `.zip` distro, and not part of the `tar.gz`, `deb` or `rpm` ones?
</comment><comment author="spinscale" created="2013-09-16T10:55:49Z" id="24502172">- deb: needs an additional exclude in pom.xml:612
- rpm: already covered, no additional inclusions (in pom.xml as well)
- tar.gz: Looks good in `src/main/assemblies/targz-bin.xml`
- zip: Needs these files added in `src/main/assemblies/zip-bin.xml`
</comment><comment author="costin" created="2013-09-16T12:48:48Z" id="24507139">@kimchy Updated the pom to include the exe only in the zip and exclude it from deb (thanks to @spinscale for the precise instructions).
</comment><comment author="nhhagen" created="2013-09-16T16:20:33Z" id="24523075">I understand why the `rpm` and `deb` packages should not include any `exe` files (and other windows files), but the `.zip` and `.tar.gz` packages should be platform independent since they both are just "containers". If not please update the download page stating that the `.zip` and `.tar.gz` are different and what the differences are.
</comment><comment author="kimchy" created="2013-09-17T10:08:48Z" id="24576920">@nhhagen we had a discussion around it here: https://github.com/elasticsearch/elasticsearch/pull/2793. The end result, we are not going to change the fact that tar.gz and zip are different, but I agree, we should document it better.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #3702 from costin/master</comment></comments></commit></commits></item><item><title>Make TestCluster based integration tests more repoducible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3701</link><project id="" key="" /><description>While testing an async system providing reproducible tests that
use randomized components is a hard task we should at least try to
reestablish the enviroment of a failing test as much as possible.
This commit allows to re-establish the shared 'TestCluster' by
resetting the cluster to a predefined shared state before each test.

Before this commit a tests that is executed in isolation was likely
using a entirely different node enviroment as the failing test since
the 'TestCluster' kept intermediate nodes started by other tests around.
</description><key id="19527134">3701</key><summary>Make TestCluster based integration tests more repoducible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-15T22:04:00Z</created><updated>2014-07-16T21:52:21Z</updated><resolved>2013-09-17T21:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-09-17T20:20:40Z" id="24618966">+1
</comment><comment author="s1monw" created="2013-09-17T21:39:56Z" id="24624808">pushed here is the hash: cabbf7805b4fdc9342970e4e4b9f873436c9615c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NoShardAvailableActionException in ES 0.90.3 on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3700</link><project id="" key="" /><description>When a river [1] has been registered and ES is restarted.
I keep getting the following exception in the log:
[2013-09-10 10:22:55,066][INFO ][node                     ] [Box IV] version[0.90.3], pid[26144], build[5c38d60/2013-08-06T13:18:31Z]
[2013-09-10 10:22:55,067][INFO ][node                     ] [Box IV] initializing ...
[2013-09-10 10:22:55,115][INFO ][plugins                  ] [Box IV] loaded [mongodb-river, mapper-attachments, lang-groovy, lang-javascript], sites [river-mongodb, head]
[2013-09-10 10:22:58,456][INFO ][node                     ] [Box IV] initialized
[2013-09-10 10:22:58,456][INFO ][node                     ] [Box IV] starting ...
[2013-09-10 10:22:59,639][INFO ][transport                ] [Box IV] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.117:9300]}
[2013-09-10 10:23:03,145][INFO ][cluster.service          ] [Box IV] new_master [Box IV][AllhKDYIScKfW8Ue1YcmQw][inet[/192.168.1.117:9300]], reason: zen-disco-join (elected_as_master)
[2013-09-10 10:23:03,169][INFO ][discovery                ] [Box IV] elasticsearch/AllhKDYIScKfW8Ue1YcmQw
[2013-09-10 10:23:03,216][DEBUG][action.get               ] [Box IV] [_river][0]: failed to execute [[_river][river101][_meta]: routing [null]]
org.elasticsearch.action.NoShardAvailableActionException: [_river][0] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:123)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
    at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)
    at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
    at org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:109)
    at org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:103)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
[2013-09-10 10:23:03,683][INFO ][gateway                  ] [Box IV] recovered [7] indices into cluster_state
[2013-09-10 10:23:03,702][DEBUG][action.get               ] [Box IV] [_river][0]: failed to execute [[_river][river101][_meta]: routing [null]]
org.elasticsearch.action.NoShardAvailableActionException: [_river][0] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:123)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
    at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)
    at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
    at org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:109)
    at org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:103)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
[2013-09-10 10:23:03,849][DEBUG][action.get               ] [Box IV] [_river][0]: failed to execute [[_river][river101][_meta]: routing [null]]
org.elasticsearch.action.NoShardAvailableActionException: [_river][0] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:123)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
    at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)
    at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
    at org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:109)
    at org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:103)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)

The same plugin version works without this exception in ES 0.90.2. Even with the exception data are correctly imported in ES by the river.

[1] - https://github.com/richardwilly98/elasticsearch-river-mongodb
</description><key id="19519289">3700</key><summary>NoShardAvailableActionException in ES 0.90.3 on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">richardwilly98</reporter><labels><label>non-issue</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-15T12:43:22Z</created><updated>2015-07-21T10:18:22Z</updated><resolved>2013-10-08T13:50:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="benmccann" created="2013-09-24T03:25:48Z" id="24972448">The error message has changed slightly in 0.90.5:

```
[2013-09-23 20:24:15,805][DEBUG][action.get               ] [Potts, Virginia "Pepper"] [_river][0]: failed to execute [[_river][river95][_meta]: routing [null]]
org.elasticsearch.index.shard.IllegalIndexShardStateException: [_river][0] CurrentState[RECOVERING] operations only allowed when started/relocated
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:704)
    at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:415)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:167)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:101)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:103)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:42)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:161)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```
</comment><comment author="benmccann" created="2013-09-24T03:38:16Z" id="24972771">Looks like people have reported this on the mailing list as well:
https://groups.google.com/d/msg/elasticsearch/fOjEUynM5Ew/ytfAZ6qWhBIJ
https://groups.google.com/d/msg/elasticsearch/6HpQmRba-i4/NApZD_hftqMJ
</comment><comment author="richardwilly98" created="2013-10-07T11:09:28Z" id="25800265">Any Update?

Is there any additional information I can provide to help?
</comment><comment author="spinscale" created="2013-10-07T11:11:41Z" id="25800392">I intend to look at this today or over the week, as I have a suspicion what is going on.. sorry for the slow reply, been busy weeks.
</comment><comment author="spinscale" created="2013-10-07T15:20:38Z" id="25817438">hey

i just pushed a small change which prevents showing up the exceptions as errors (as this is not true when the elasticsearch is started up), see https://github.com/elasticsearch/elasticsearch/commit/abcf8fc0f775b701cd3e06656f25a16f75675c92

can you check if this prevents showing up the exception in your case?
</comment><comment author="richardwilly98" created="2013-10-07T15:39:55Z" id="25819224">@spinscale the exception is gone.
</comment><comment author="karnamonkster" created="2014-04-04T05:51:11Z" id="39533806">Is it fixed for ES 1.0.0? cause i am still getting the same
</comment><comment author="heyarny" created="2014-08-03T16:19:46Z" id="50994740">I'm getting the same error with ES 1.2.2.

```
[2014-08-03 16:17:29,202][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Starting river mongodb
[2014-08-03 16:17:29,204][WARN ][org.elasticsearch.river.mongodb.MongoDBRiver] Fail to start river mongodb
org.elasticsearch.action.NoShardAvailableActionException: [_river][3] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:145)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:125)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:74)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
    at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:177)
    at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:182)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
    at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)
    at org.elasticsearch.river.mongodb.util.MongoDBRiverHelper.getRiverStatus(MongoDBRiverHelper.java:21)
    at org.elasticsearch.river.mongodb.MongoDBRiver.start(MongoDBRiver.java:154)
    at org.elasticsearch.river.RiversService.createRiver(RiversService.java:148)
    at org.elasticsearch.river.RiversService$ApplyRivers$2.onResponse(RiversService.java:275)
    at org.elasticsearch.river.RiversService$ApplyRivers$2.onResponse(RiversService.java:269)
    at org.elasticsearch.action.support.TransportAction$ThreadedActionListener$1.run(TransportAction.java:93)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-08-03 16:17:29,206][DEBUG][river.mongodb.util       ] setRiverStatus called with mongodb - START_FAILED
```
</comment><comment author="heyarny" created="2014-08-03T20:35:14Z" id="51002204">@karnamonkster did you fix the issue? if so, how?
</comment><comment author="karnamonkster" created="2014-08-04T03:33:46Z" id="51014771">Can you give more details on your setup?
You must have more than one node with index for river created in your setup

On Monday, August 4, 2014, Arnold notifications@github.com wrote:

&gt; @karnamonkster https://github.com/karnamonkster did you fix the issue?
&gt; if so, how?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3700#issuecomment-51002204
&gt; .

## 

BR/
_Karan_
</comment><comment author="heyarny" created="2014-08-04T08:14:31Z" id="51028890">@karnamonkster Using one node and 5 shards, per default. It worked in the past and stopped somewhen. I have another server running exactly the same config using ES 1.0.3 and it works just fine.
No idea where the catch might be.
</comment><comment author="karnamonkster" created="2014-08-04T11:40:26Z" id="51049402">there is an authentication issue in the river configuration

On Mon, Aug 4, 2014 at 2:25 PM, Arnold notifications@github.com wrote:

&gt; This is another log when I try to create the config.
&gt; 
&gt; [2014-08-04 10:49:48,482][INFO ][cluster.metadata         ] [Stylios Main] [_river] creating index, cause [auto(index api)], shards [5]/[1], mappings []
&gt; [2014-08-04 10:49:48,960][INFO ][cluster.metadata         ] [Stylios Main] [_river] update_mapping [mongodb](dynamic)
&gt; [2014-08-04 10:49:48,963][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] Initializing river : [mongodb]
&gt; [2014-08-04 10:49:48,963][INFO ][river.mongodb            ] Parse river settings for mongodb
&gt; [2014-08-04 10:49:48,963][TRACE][river.mongodb            ] mongoServersSettings: [{port=27017, host=144.76.xxx.xxx}]
&gt; [2014-08-04 10:49:48,963][INFO ][river.mongodb            ] Server: 144.76.xxx.xxx - 27017
&gt; [2014-08-04 10:49:48,963][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Starting river mongodb
&gt; [2014-08-04 10:49:48,963][DEBUG][river.mongodb.util       ] setRiverStatus called with mongodb - RUNNING
&gt; [2014-08-04 10:49:48,965][DEBUG][org.elasticsearch.river.mongodb.MongoDBRiver] Using mongodb server(s): host [144.76.xxx.xxx], port [27017]
&gt; [2014-08-04 10:49:48,966][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] MongoDB River Plugin - version[2.0.1] - hash[445c35a] - time[2014-07-30T14:08:26Z]
&gt; [2014-08-04 10:49:48,966][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] starting mongodb stream. options: secondaryreadpreference [false], drop_collection [false], include_collection [], throttlesize [5000], gridfs [false], filter [null], db [stylios], collection [products], script [null], indexing to [stylios]/[product]
&gt; [2014-08-04 10:49:48,996][INFO ][cluster.metadata         ] [Stylios Main] [_river] update_mapping [mongodb](dynamic)
&gt; [2014-08-04 10:49:49,038][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] MongoAdminUser:  - authenticated: false
&gt; [2014-08-04 10:49:49,038][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] Found admin database
&gt; [2014-08-04 10:49:49,038][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] About to execute: { "serverStatus" : 1 , "asserts" : 0 , "backgroundFlushing" : 0 , "connections" : 0 , "cursors" : 0 , "dur" : 0 , "extra_info" : 0 , "globalLock" : 0 , "indexCounters" : 0 , "locks" : 0 , "metrics" : 0 , "network" : 0 , "opcounters" : 0 , "opcountersRepl" : 0 , "recordStats" : 0 , "repl" : 0}
&gt; [2014-08-04 10:49:49,057][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] Command executed return : { "serverUsed" : "144.76.xxx.xxx:27017" , "host" : "stylios" , "version" : "2.6.3" , "process" : "mongod" , "pid" : 15882 , "uptime" : 422.0 , "uptimeMillis" : 421297 , "uptimeEstimate" : 418.0 , "localTime" : { "$date" : "2014-08-04T08:49:49.056Z"} , "ok" : 1.0}
&gt; [2014-08-04 10:49:49,057][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] MongoDB version - 2.6.3
&gt; [2014-08-04 10:49:49,057][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] serverStatus: { "serverUsed" : "144.76.xxx.xxx:27017" , "host" : "stylios" , "version" : "2.6.3" , "process" : "mongod" , "pid" : 15882 , "uptime" : 422.0 , "uptimeMillis" : 421297 , "uptimeEstimate" : 418.0 , "localTime" : { "$date" : "2014-08-04T08:49:49.056Z"} , "ok" : 1.0}
&gt; [2014-08-04 10:49:49,057][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] process: mongod
&gt; [2014-08-04 10:49:49,057][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] Not mongos
&gt; [2014-08-04 10:49:49,072][INFO ][cluster.metadata         ] [Stylios Main] [_river] update_mapping [mongodb](dynamic)
&gt; [2014-08-04 10:49:49,082][DEBUG][river.mongodb.util       ] setRiverStatus called with mongodb - INITIAL_IMPORT_FAILED
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3700#issuecomment-51032252
&gt; .

## 

BR/
_Karan_
</comment><comment author="heyarny" created="2014-08-04T11:44:46Z" id="51049746">@karnamonkster not sure whether this is the actual issue.
I'm not using any authentications within mongodb or ES.
I will try to dig deeper into it.
</comment><comment author="karnamonkster" created="2014-08-04T11:49:10Z" id="51050086">check this .........

[2014-08-04 10:49:48,996][INFO ][cluster.metadata         ] [Stylios
Main] [_river] update_mapping [mongodb](dynamic)
[2014-08-04 10:49:49,038][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver]
MongoAdminUser:  - authenticated: false

On Mon, Aug 4, 2014 at 5:15 PM, Arnold notifications@github.com wrote:

&gt; @karnamonkster https://github.com/karnamonkster not sure whether this
&gt; is the actual issue.
&gt; I'm not using any authentications within mongodb or ES.
&gt; I will try to dig deeper into it.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3700#issuecomment-51049746
&gt; .

## 

BR/
_Karan_
</comment><comment author="heyarny" created="2014-08-04T11:52:33Z" id="51050390">yes, but my mongodb is not running in auth mode nor there are any users available.
River config doesn't contain any auth/users info as well.
No idea why this is showing up. Will take a closer look.
</comment><comment author="richardwilly98" created="2014-08-04T11:53:05Z" id="51050433">That was a bug initially reported in Elasticsearch (when the river gets registered).
That's most likely an issue in Elasticsearch again in version 1.2.2
- authenticated: false just means MongoDB request is not authenticated (but if you do no use authentication in MongoDB that's just fine).
</comment><comment author="heyarny" created="2014-08-04T11:58:51Z" id="51050880">@richardwilly98 but on your river site it says 2.0.1 should work with ES 1.2.2?
Like I noted on the issue there: this is happening if I restart ES only. Initial import is working just like expected.
</comment><comment author="heyarny" created="2014-08-04T14:27:40Z" id="51066930">@karnamonkster @richardwilly98 
I have now downgraded back to ES 1.0.0, MongoDBRiver 2.0.0 and MongoDB 2.4.10.
Removed all indexes (_river and main index). The first configuration with initial import worked just fine.
Restarted ES and getting the same error again:

```
[2014-08-04 16:20:05,954][INFO ][node                     ] [Stylios Main] version[1.0.0], pid[26352], build[a46900e/2014-02-12T16:18:34Z]
[2014-08-04 16:20:05,955][INFO ][node                     ] [Stylios Main] initializing ...
[2014-08-04 16:20:05,982][INFO ][plugins                  ] [Stylios Main] loaded [mongodb-river, mapper-attachments], sites [river-mongodb]
[2014-08-04 16:20:07,595][TRACE][rest.action.mongodb      ] [Stylios Main] RestMongoDBRiverAction - baseUrl: /_river/mongodb
[2014-08-04 16:20:07,660][INFO ][node                     ] [Stylios Main] initialized
[2014-08-04 16:20:07,660][INFO ][node                     ] [Stylios Main] starting ...
[2014-08-04 16:20:07,731][INFO ][transport                ] [Stylios Main] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/144.76.xxx.xxx:9300]}
[2014-08-04 16:20:10,746][INFO ][cluster.service          ] [Stylios Main] new_master [Stylios Main][LGdaDMCWQa28GoHAKPdC0w][stylios][inet[/144.76.xxx.xxx:9300]], reason: zen-disco-join (elected_as_master)
[2014-08-04 16:20:10,761][INFO ][discovery                ] [Stylios Main] stylios/LGdaDMCWQa28GoHAKPdC0w
[2014-08-04 16:20:10,774][INFO ][http                     ] [Stylios Main] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/144.76.xxx.xxx:9200]}
[2014-08-04 16:20:11,363][INFO ][gateway                  ] [Stylios Main] recovered [2] indices into cluster_state
[2014-08-04 16:20:11,521][INFO ][node                     ] [Stylios Main] started
[2014-08-04 16:20:12,029][TRACE][org.elasticsearch.river.mongodb.MongoDBRiver] Initializing river : [mongodb]
[2014-08-04 16:20:12,031][INFO ][river.mongodb            ] Parse river settings for mongodb
[2014-08-04 16:20:12,033][TRACE][river.mongodb            ] mongoServersSettings: [{port=27017, host=144.76.xxx.xxx}]
[2014-08-04 16:20:12,034][INFO ][river.mongodb            ] Server: 144.76.xxx.xxx - 27017
[2014-08-04 16:20:12,092][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Starting river mongodb
[2014-08-04 16:20:12,094][WARN ][org.elasticsearch.river.mongodb.MongoDBRiver] Fail to start river mongodb
org.elasticsearch.action.NoShardAvailableActionException: [_river][3] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:145)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.onFailure(TransportShardSingleOperationAction.java:132)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.access$900(TransportShardSingleOperationAction.java:97)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:166)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-08-04 16:20:12,095][DEBUG][river.mongodb.util       ] setRiverStatus called with mongodb - START_FAILED
```
</comment><comment author="karnamonkster" created="2014-08-04T16:22:42Z" id="51082369">can you try setting up another node.. just to check
</comment><comment author="heyarny" created="2014-08-04T23:55:06Z" id="51133605">I turned on more logging details and I got this:
Any way to re-open this issue?

```
[2014-08-05 01:48:30,684][INFO ][node                     ] [Main Node] version[1.0.3], pid[3101], build[61bfb72/2014-04-16T14:43:11Z]
[2014-08-05 01:48:30,685][INFO ][node                     ] [Main Node] initializing ...
[2014-08-05 01:48:30,691][INFO ][plugins                  ] [Main Node] loaded [mongodb-river], sites [river-mongodb]
[2014-08-05 01:48:32,533][INFO ][node                     ] [Main Node] initialized
[2014-08-05 01:48:32,534][INFO ][node                     ] [Main Node] starting ...
[2014-08-05 01:48:32,630][INFO ][transport                ] [Main Node] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/144.76.xxx.xxx:9300]}
[2014-08-05 01:48:35,649][INFO ][cluster.service          ] [Main Node] new_master [Main Node][tPimjK3FTNihB_ESQfMwmw][stylios][inet[/144.76.xxx.xxx:9300]], reason: zen-disco-join (elected_as_master)
[2014-08-05 01:48:35,658][INFO ][discovery                ] [Main Node] stylios/tPimjK3FTNihB_ESQfMwmw
[2014-08-05 01:48:35,668][INFO ][http                     ] [Main Node] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/144.76.xxx.xxx:9200]}
[2014-08-05 01:48:36,426][INFO ][gateway                  ] [Main Node] recovered [2] indices into cluster_state
[2014-08-05 01:48:36,480][TRACE][action.get               ] [Main Node] [_river][1], node[tPimjK3FTNihB_ESQfMwmw], [P], s[INITIALIZING]: failed to execute [[_river][stylios_users][_meta]: routing [null]]
org.elasticsearch.index.shard.IllegalIndexShardStateException: [_river][1] CurrentState[POST_RECOVERY] operations only allowed when started/relocated
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:806)
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:797)
    at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:466)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:195)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:106)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:109)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:163)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-08-05 01:48:36,752][INFO ][river.mongodb            ] Parse river settings for stylios_users
[2014-08-05 01:48:36,755][INFO ][river.mongodb            ] Server: 144.76.xxx.xxx - 27017
[2014-08-05 01:48:36,766][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Starting river stylios_users
[2014-08-05 01:48:36,767][TRACE][action.get               ] [Main Node] [_river][3], node[tPimjK3FTNihB_ESQfMwmw], [P], s[INITIALIZING]: failed to execute [[_river][stylios_users][_riverstatus]: routing [null]]
org.elasticsearch.index.shard.IllegalIndexShardStateException: [_river][3] CurrentState[POST_RECOVERY] operations only allowed when started/relocated
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:806)
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:797)
    at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:466)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:195)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:106)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:109)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:163)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-08-05 01:48:36,768][WARN ][org.elasticsearch.river.mongodb.MongoDBRiver] Fail to start river stylios_users
org.elasticsearch.action.NoShardAvailableActionException: [_river][3] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:145)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.onFailure(TransportShardSingleOperationAction.java:132)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.access$900(TransportShardSingleOperationAction.java:97)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:166)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2014-08-05 01:48:36,772][TRACE][action.index             ] [Main Node] primary shard [[_river][3]] is not yet active or we do not know the node it is assigned to [tPimjK3FTNihB_ESQfMwmw], scheduling a retry.
[2014-08-05 01:48:36,961][TRACE][action.index             ] [Main Node] state change while we were trying to add listener, trying to start again, sampled_version [4], current_version [5]
[2014-08-05 01:48:36,964][INFO ][node                     ] [Main Node] started
```
</comment><comment author="clintongormley" created="2014-08-05T11:35:53Z" id="51185356">Hi @heyarny 

I think this may be a different issue to the previous one.  Please could you open a new issue, and provide a step-by-step reproduction, everything that is needed to replicate this on a clean install.  I'd especially like to know about what settings you have, and if you're using any index templates.

Also, is this a transient error? In other words, does the error eventually disappear and everything start working normally, or does it fail completely?
</comment><comment author="heyarny" created="2014-08-05T12:12:14Z" id="51188990">@clintongormley I'm not sure whether I can reproduce this any longer, because I could fix this issue tonight.

My previous config:
ES 1.0.3, MongoDB 2.4.10, MongoDBRiver 2.0.0
elasticsearch.yml (pretty much anything was default, except cluster name and node name)

```
...
index.number_of_shards: 5
index.number_of_replicas: 1
...
```

I had an unassigned node visible inside my elasticsearch-head interface.

What I did to fix it:
Upgraded everything back to ES 1.2.2, MongoDB 2.6.3, MongoDBRiver 2.0.1.
!!! That issue was still there !!!
Then:
1. I removed all oplog files (local.*) from mongodb, set bind_ip to my external interface.
2. Removed all indexes (_river and others)
3. Modified elasticsearch.yml

```
....
index.number_of_shards: 5
index.number_of_replicas: 0
network.host: 144.76.xxx.xxx
...
```
1. Restarted mongodb and ES
2. Set up my river to

```
 {
type: mongodb
mongodb: {
servers: [
{
host: 144.76.xxx.xxx,
port: 27017
}
]
db: stylios,
collection: products
}
index: {
name: stylios,
type: product
}
}
```

So after all the unassigned node disappeared now, and it started working again. Although I have another server having exactly the same unassigned node, but everything seems to be working just fine.

Start up log now:

```
[2014-08-05 05:17:09,210][DEBUG][action.search.type       ] [Main Node] All shards failed for phase: [query]
[2014-08-05 05:17:09,359][INFO ][node                     ] [Main Node] stopped
[2014-08-05 05:17:09,359][INFO ][node                     ] [Main Node] closing ...
[2014-08-05 05:17:09,367][INFO ][node                     ] [Main Node] closed
[2014-08-05 05:17:11,054][INFO ][node                     ] [Main Node] version[1.2.2], pid[16728], build[9902f08/2014-07-09T12:02:32Z]
[2014-08-05 05:17:11,054][INFO ][node                     ] [Main Node] initializing ...
[2014-08-05 05:17:11,062][INFO ][plugins                  ] [Main Node] loaded [mongodb-river], sites [river-mongodb]
[2014-08-05 05:17:12,745][INFO ][node                     ] [Main Node] initialized
[2014-08-05 05:17:12,745][INFO ][node                     ] [Main Node] starting ...
[2014-08-05 05:17:12,824][INFO ][transport                ] [Main Node] bound_address {inet[/144.76.xxx.xxx:9300]}, publish_address {inet[/144.76.xxx.xxx:9300]}
[2014-08-05 05:17:15,844][INFO ][cluster.service          ] [Main Node] new_master [Main Node][Xd6zI732TU2ECvd7FTBDvg][stylios][inet[/144.76.xxx.xxx:9300]], reason: zen-disco-join (elected_as_master)
[2014-08-05 05:17:15,852][INFO ][discovery                ] [Main Node] stylios/Xd6zI732TU2ECvd7FTBDvg
[2014-08-05 05:17:15,865][INFO ][http                     ] [Main Node] bound_address {inet[/144.76.xxx.xxx:9200]}, publish_address {inet[/144.76.xxx.xxx:9200]}
[2014-08-05 05:17:16,413][INFO ][gateway                  ] [Main Node] recovered [2] indices into cluster_state
[2014-08-05 05:17:16,413][INFO ][node                     ] [Main Node] started
[2014-08-05 05:17:16,673][INFO ][river.mongodb            ] Parse river settings for stylios
[2014-08-05 05:17:16,677][INFO ][river.mongodb            ] Server: 144.76.xxx.xxx - 27017
[2014-08-05 05:17:16,693][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] Starting river stylios
[2014-08-05 05:17:16,707][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] MongoDB River Plugin - version[2.0.1] - hash[445c35a] - time[2014-07-30T14:08:26Z]
[2014-08-05 05:17:16,708][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] starting mongodb stream. options: secondaryreadpreference [false], drop_collection [false], include_collection [], throttlesize [5000], gridfs [false], filter [null], db [stylios], collection [products], script [null], indexing to [stylios]/[product]
[2014-08-05 05:17:16,870][INFO ][org.elasticsearch.river.mongodb.MongoDBRiver] MongoDB version - 2.6.3
[2014-08-05 13:40:24,708][INFO ][river.mongodb            ] Parse river settings for stylios
[2014-08-05 13:40:24,708][INFO ][river.mongodb            ] Server: 144.76.xxx.xxx - 27017
...
```

So after all I'm not sure where this issue came from and where it's gone.
</comment><comment author="phax" created="2014-11-12T11:48:19Z" id="62706988">I'm having the same issue with ES 1.3.5 but without MongoDB/River with a single shard.

It occurs only when a new index is created and directly afterwards a "create" and "get" is done (in my case).
When putting a breakpoint into the first exception, waiting a few seconds and continuing than, the follow-up exception were gone - so I assume it's somehow a timing issue.

After the index creation, I'm calling

```
client.admin ().cluster ().prepareHealth ("index-name").setWaitForGreenStatus ().execute ().actionGet ();
```

to ensure the index is ready before I'm calling a "create" in it.
Confusingly the response of this message is a timeout after 30 seconds (default value).
The problem is that the shard stays unassigned (a problem I need to resolve).

The resulting trace is comparable to a previous one:

```
org.elasticsearch.action.NoShardAvailableActionException: [index-name][0] null
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:144)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.onFailure(TransportShardSingleOperationAction.java:131)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.access$1300(TransportShardSingleOperationAction.java:93)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:167)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

which is based on an IllegalIndexShardStateException:

```
org.elasticsearch.index.shard.IllegalIndexShardStateException: [index-name][0] CurrentState[RECOVERING] operations only allowed when started/relocated
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:826)
    at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:817)
    at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:472)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:193)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:104)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:104)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:164)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Putting a breakpoint in "InternalIndexShard.readAllowed(Mode)" it can be seen, that "state" and "this.state" differ.

It is very confusing to me - I admit. Maybe it helps finding the source of the issue...
</comment><comment author="benben" created="2015-07-21T10:18:22Z" id="123252238">this error occurs for us in our CI environment. ES 1.4.5

```
org.elasticsearch.index.shard.IllegalIndexShardStateException: [test_music_genres][0] CurrentState[RECOVERING] operations only allowed when started/relocated
        at org.elasticsearch.index.shard.service.InternalIndexShard.readAllowed(InternalIndexShard.java:876)
        at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:703)
        at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:698)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:535)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:515)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:356)
        at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
        at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
```

can someone please have a look and reopen? thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changing the id of a document stored in ES affects what can be done with the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3699</link><project id="" key="" /><description>0.90.1

I wrote a unit test to ensure that I am able to remove fields from documents (using a scrolling search for all documents within a given type and the remove script described here: http://www.elasticsearch.org/guide/reference/api/update/).  Only one of my two documents was updated, and I was unable to actually update the mapping of the type to indicate that the field no longer existed in my document schema.  when I changed the ids of the documents from {1, 2} to {7, 2} and reran the unit test, both documents were properly updated.  However, the mapping still didn't get updated.

This sounds pretty seriously broken to me, so I feel like I'm doing something wrong when I'm starting up my client.  Here is the relevant code I use to create a client with which to talk to an ES service:

```
    NodeBuilder.nodeBuilder().node();
    String host = "localhost";
    int port = 9300;
    return new TransportClient().addTransportAddress(new InetSocketTransportAddress(
            host,
            port));
```

Can you help me understand why this is so broken?  I tried debugging, but it's really difficult to see what happens under the covers since there are dozens of implementations of the transport actions and the transport layer doesn't seem to maintain strong type consistency.
</description><key id="19512590">3699</key><summary>Changing the id of a document stored in ES affects what can be done with the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amrasarfeiniel</reporter><labels /><created>2013-09-15T00:52:51Z</created><updated>2013-09-15T03:32:20Z</updated><resolved>2013-09-15T03:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-15T03:32:20Z" id="24463724">Please use the mailing list for your questions.
And Gist your code (test class).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add dedicated Suggest Thread Pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3698</link><project id="" key="" /><description>Add a dedicated suggest thread pool for the suggest API. With the new completion suggest type, which is purely CPU bounded, it makes more sense to have a dedicated thread pool for suggest compared to having it share the search thread pool and "competing" against other search operations. 
</description><key id="19512069">3698</key><summary>Add dedicated Suggest Thread Pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-14T23:53:44Z</created><updated>2013-09-14T23:54:37Z</updated><resolved>2013-09-14T23:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>Add dedicated Suggest Thread Pool</comment><comment>Add a dedicated suggest thread pool for the suggest API. With the new completion suggest type, which is purely CPU bounded, it makes more sense to have a dedicated thread pool for suggest compared to having it share the search thread pool and "competing" against other search operations.</comment><comment>closes #3698</comment></comments></commit></commits></item><item><title>Briefly delete manifested mapping type on a node without reason</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3697</link><project id="" key="" /><description>When a dynamic type is introduced during indexing, the node that introduces it sends the fact that its added to the master, to be added to the master node. The master node then adds it to the index metadata and republishes that fact.

In order not to delete the mapping while the new type is introduced on the node that introduced it, we keep a map of seen mappings, and remove a mapping type when we already processed it.

The map is not properly cleared though in all places where an actual index service is being removed on a node.
</description><key id="19509933">3697</key><summary>Briefly delete manifested mapping type on a node without reason</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-14T20:41:28Z</created><updated>2013-09-14T20:42:57Z</updated><resolved>2013-09-14T20:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Briefly delete manifested mapping type on a node without reason</comment><comment>When a dynamic type is introduced during indexing, the node that introduces it sends the fact that its added to the master, to be added to the master node. The master node then adds it to the index metadata and republishes that fact.</comment></comments></commit></commits></item><item><title>Feature Request: Don't reindex the document when updating non-indexed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3696</link><project id="" key="" /><description>Read title.
</description><key id="19503939">3696</key><summary>Feature Request: Don't reindex the document when updating non-indexed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ddorian</reporter><labels /><created>2013-09-14T15:41:21Z</created><updated>2013-09-16T14:14:09Z</updated><resolved>2013-09-16T14:14:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-16T06:42:36Z" id="24491303">Hey,

as the underlying lucene library is working in append only mode, there is no difference between storing a new document or updating an existing at the lowest level.

I am wondering what you need this feature for (maybe we are talking about different layers in the application) or why you think it is important, so it would be great if you could elaborate a bit more and tell, what you are missing at the moment.
</comment><comment author="ddorian" created="2013-09-16T08:43:18Z" id="24495822">Hi,

I'm trying to use elastic search as db and in most cases i only update non-indexed fields. It would make es more attractive as a db (alternative to document stores like mongodb,couchdb,couchbase, rethinkdb etc).
</comment><comment author="spinscale" created="2013-09-16T11:29:35Z" id="24503516">Are you aware of the update API?

http://www.elasticsearch.org/guide/reference/api/update/

Anything you cannot do with it but need to?
</comment><comment author="ddorian" created="2013-09-16T11:32:24Z" id="24503642">Yes. When using update is the document reindexed? What i mean are fields extracted/analyzed/indexed again even if only non-indexed fields are updated using the update api?
</comment><comment author="spinscale" created="2013-09-16T14:03:36Z" id="24511857">Yes they are. I am still wondering, if there is a problem for you with the current implementation. You can update, you can index (and delete etc..), and it works as expected from the user point of view. I do not fully understand which functionality you are missing compared to other data stores - which must be the reason for this issue. :-)

If nothing is missing at this point I'd rather close this issue, as the 'append only' design decision is not likely to be changed due to the underlying libraries being used.

And yes, this design results in indexing/updating taking more time, but append only storage has several nice advantages like heavy use of file system caches (the mantra being 'query speed over indexing speed' here).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cannot make Path Hierarchy filter when paired with another filter - ClassNotFoundException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3695</link><project id="" key="" /><description>Using ES 0.90.3

Check the following command which tries to make some analyzers with two filters:

```
curl -XPUT "localhost:9200/text5" -d'
{
    "analysis": {
        "analyzer": {
            "str_search_analyzer": {
                "tokenizer": "keyword",
                "filter": ["lowercase"]
            },

            "str_index_analyzer": {
                "tokenizer": "keyword",
                "filter": ["lowercase", "substring"]
            }
        },
        "filter": {
            "substring2": {
                "type": "path_hierarchy",
                "delimiter": "."
            },
            "substring": {
                "type": "nGram",
                "min_gram": 1,
                "max_gram": 10
            }
        }
    }
}'
```

Answer: 

```
{"error":"IndexCreationException[[text2] failed to create index]; nested: ElasticSearchIllegalArgumentException[failed to find token filter type [path_hierarchy] for [substring2]]; nested: NoClassSettingsException[Failed to load class setting [type] with value [path_hierarchy]]; nested: ClassNotFoundException[org.elasticsearch.index.analysis.pathhierarchy.PathHierarchyTokenFilterFactory]; ","status":400}
```

And it is true because in the github repo it is under `org.elasticsearch.index.analysis.PathHierarchyTokenFilterFactory` instead of `org.elasticsearch.index.analysis.pathhierarchy.PathHierarchyTokenFilterFactory`

But this request is OK:

```
curl -XPUT "localhost:9200/text3" -d'
 {
     "analysis": {
             "substring": {
                 "type": "path_hierarchy",
                 "delimiter": "."
             }
        }
     }
 }'

{"ok":true,"acknowledged":false}
```
</description><key id="19500804">3695</key><summary>Cannot make Path Hierarchy filter when paired with another filter - ClassNotFoundException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gergely-ujvari</reporter><labels /><created>2013-09-14T11:55:42Z</created><updated>2013-09-20T15:04:12Z</updated><resolved>2013-09-20T15:04:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-16T11:19:21Z" id="24503120">As far as I know the path hierarchy is available only as a tokenizer: http://www.elasticsearch.org/guide/reference/index-modules/analysis/pathhierarchy-tokenizer/ .

That's what you use in the second example, while in the first one you try to use it as a token filter, and there is no token filter with such name.
</comment><comment author="javanna" created="2013-09-20T15:04:12Z" id="24816812">Closing this one as it doesn't seem to be an elasticsearch issue. Feel free to add further information if you think that's not correct.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix SearchWithRandomExceptionsTests.testRandomExceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3694</link><project id="" key="" /><description>The test is trapped in an infinite loop for some seeds. This causes the test log to fill and finally an oom when running the full suite. The following seeds reproduce this: 
6D1FE72CC0B92909 
843B52FFCF8E5B33
6F9CC4362D6D82CB
</description><key id="19494118">3694</key><summary>fix SearchWithRandomExceptionsTests.testRandomExceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">brwe</reporter><labels><label>non-issue</label></labels><created>2013-09-14T09:49:36Z</created><updated>2013-09-17T15:33:58Z</updated><resolved>2013-09-16T12:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-14T12:45:39Z" id="24444594">thanks for disabling it. I will look into it!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file></files><comments><comment>If cluster health times out don't index lots of documents</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file></files><comments><comment>tag with @AwaitsFix because it causes the test suite to crash</comment></comments></commit></commits></item><item><title>Confusing exception raised by create_index with index-alias collision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3693</link><project id="" key="" /><description>Elasticsearch raises an InvalidIndexNameException when you call create_index for a "name", which already exists as an Alias. In all other circumstances, an InvalidIndexNameException is raised from use of illegal characters in an index name. 

It would make more sense to raise the IndexAlreadyExistsException. Because in most cases one is trying to ensure the name is present, whether as an alias or as an index.

When it is an index-alias collision the suffix "an alias already exists with that name" would be added to the exception.

IndexAlreadyExistsException: [same-name-index] Already exists, an alias already exists with that name.
</description><key id="19492197">3693</key><summary>Confusing exception raised by create_index with index-alias collision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">the-bumble</reporter><labels /><created>2013-09-14T07:09:56Z</created><updated>2013-10-30T13:20:10Z</updated><resolved>2013-10-30T13:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T12:26:37Z" id="27384589">Hey,

do you mind creating a pull request (including a test) for this, as you already started with this in your own branch?
</comment><comment author="the-bumble" created="2013-10-30T12:38:18Z" id="27385199">Sounds good! Will do.
</comment><comment author="spinscale" created="2013-10-30T13:20:10Z" id="27387867">Hey,

just saw that shay worked on a similar issue and pushed a fix already, so no need for a PR on your side. Many thx however for volunteering and coming up with it!

https://github.com/elasticsearch/elasticsearch/commit/8a62619fb94d862b16687087b869cadf5bf9849c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>index analysis module not working correctly v0.90.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3692</link><project id="" key="" /><description>I am trying to define an analyzer in the elasticsearch.yml base on the [doc](http://www.elasticsearch.org/guide/reference/index-modules/analysis/)

```
index :
    analysis :
        analyzer : 
            standard : 
                type : standard
                stopwords : [stop1, stop2]
            myAnalyzer1 :
                type : standard
                stopwords : [stop1, stop2, stop3]
                max_token_length : 500
           ...
```

However, this is not working and it seems like that I have to put it like this to get it working:

```
index.analysis.analyzer.standard:        
    type : standard
    stopwords : [stop1, stop2]
index.analysis.analyzer.myAnalyzer1:        
    type : standard
    stopwords : [stop1, stop2, stop3]
    max_token_length : 500
...
```

Is this a bug in the code or the doc?
</description><key id="19477279">3692</key><summary>index analysis module not working correctly v0.90.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">evanwong</reporter><labels /><created>2013-09-13T21:27:37Z</created><updated>2014-02-26T20:54:37Z</updated><resolved>2014-02-26T20:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-22T16:06:04Z" id="35806151">Both works for me with 1.0.0 after a quick test

Can you please post a more exact recreation, so we can the where the differences are? Can you include your configuration in a gist as well as every step you did in order to recreate and show that it is not working?

Thanks a lot!
</comment><comment author="evanwong" created="2014-02-26T16:02:52Z" id="36141414">Hi,
I tried it on 1.0 as well but same result.

Here is the gist:
https://gist.github.com/evanwong/9232351
1. Started elasticsearch using this yml
   ./bin/elasticsearch
2. Ran a simple query using the "myAnalyzer1" and an error response came back
   curl -XGET 'localhost:9200/_analyze?analyzer=myAnalyzer1' -d 'this is a test'
   =&gt; {"error":"ElasticsearchIllegalArgumentException[failed to find analyzer [myAnalyzer1]]","status":400}

Thanks.
</comment><comment author="brusic" created="2014-02-26T19:02:10Z" id="36163077">A custom analyzer needs to be referenced by an index before it can be used. Try something like 

curl -XGET 'localhost:9200/test/_analyze?analyzer=myAnalyzer1' -d 'this is a test' 

Where 'test' is an existing index.
</comment><comment author="evanwong" created="2014-02-26T19:11:21Z" id="36164127">Hi @brusic 
Base on this doc http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-analyze.html, without the index should be working also.
</comment><comment author="dadoonet" created="2014-02-26T20:22:09Z" id="36171930">It only works for "global" indices. When you define an custom analyzer it is only applied on index. That's the reason it won't work.
</comment><comment author="evanwong" created="2014-02-26T20:54:37Z" id="36175246">Got it... the difference is custom analyzer vs the predefined analyzer...

Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSearch shell script fails due to unsupported syntax on non-Bash shells</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3691</link><project id="" key="" /><description>The first line of the elasticsearch script specifies `/bin/sh`, but this is generally defined as pointing to a Bourne-compatible shell. It cannot be assumed to point to Bash shell.

Line 92 contains the followin:

 `JAVA=$(which java)`
1. The first line should be changed to /bin/bash if that is intended.
2. Or line 92 must be changed so that `which java` is surrounded within backtics, which is the only common (though not able to be nested) syntax across all Bourne-derived shells (Bourne, Dash, Korn, Bash).

Thank you.

Brian Yoder
</description><key id="19474569">3691</key><summary>ElasticSearch shell script fails due to unsupported syntax on non-Bash shells</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">brian-from-fl</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-13T20:33:09Z</created><updated>2013-09-16T08:06:38Z</updated><resolved>2013-09-16T07:47:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Making debian init script bourne shell compatible</comment></comments></commit></commits></item><item><title>Optimize API: Remove refresh flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3690</link><project id="" key="" /><description>Refresh flag in optimize is problematic, since the shards refresh is allowed to execute on is different compared to the optimize shards. In order to do optimize and then refresh, they should be executed as separate APIs when needed.
</description><key id="19471967">3690</key><summary>Optimize API: Remove refresh flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-13T19:35:13Z</created><updated>2013-09-13T19:44:46Z</updated><resolved>2013-09-13T19:44:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Optimize API: Remove refresh flag</comment><comment>Refresh flag in optimize is problematic, since the shards refresh is allowed to execute on is different compared to the optimize shards. In order to do optimize and then refresh, they should be executed as separate APIs when needed.</comment><comment>closes #3690</comment></comments></commit></commits></item><item><title>Flush API: remove refresh flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3689</link><project id="" key="" /><description>Refresh flag in flush is problematic, since the shards refresh is allowed to execute on is different compared to the flush shards. In order to do flush and then refresh, they should be executed as separate APIs when needed.
</description><key id="19470680">3689</key><summary>Flush API: remove refresh flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-13T19:08:56Z</created><updated>2013-09-18T14:28:49Z</updated><resolved>2013-09-13T19:10:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/FlushNotAllowedEngineException.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>src/test/java/org/elasticsearch/AbstractSharedClusterTest.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/facet/terms/UnmappedFieldsTermsFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file></files><comments><comment>Flush API: remove refresh flag</comment><comment>Refresh flag in flush is problematic, since the shards refresh is allowed to execute on is different compared to the flush shards. In order to do flush and then refresh, they should be executed as separate APIs when needed.</comment><comment>closes #3689</comment></comments></commit></commits></item><item><title>search_analyzer reset in completion suggester mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3688</link><project id="" key="" /><description>Hi all,

I've created a mapping with custom index_analyzer and search_analyzers specified in the mapping, and pulled the mappings to verify that things were set correctly. I then index a few large batches of data, and check the mappings again to find that search_analzyer is now "simple" in the mappings instead of my custom analyzer.

I'm working on narrowing down a more exact test case, but thought maybe this would be enough to start taking a look.

On a related note, it seems that the completion suggester doesn't support taking an analyzer option anywhere at query time.
</description><key id="19465906">3688</key><summary>search_analyzer reset in completion suggester mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rdeaton</reporter><labels /><created>2013-09-13T17:37:17Z</created><updated>2013-12-02T09:01:50Z</updated><resolved>2013-12-02T09:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-16T06:32:17Z" id="24491050">Hey,

it would be great if you could create gist including all the curl commands you used, so we can take a look at!

Thanks a lot!
</comment><comment author="spinscale" created="2013-10-07T10:28:41Z" id="25798245">Hey,

I took another look at this, but could not reproduce it on master, maybe you can help. This is what I did:

```
curl -X DELETE localhost:9200/products

curl -X PUT localhost:9200/products -d '
{
  "mappings": {
    "product" : {
      "properties" : {
        "name" : { "type" : "string" },
        "name_suggest" : {
          "type" :     "completion",
          "payloads" : false,
          "search_analyzer" : "keyword",
          "index_analyzer" : "standard"
        }
      }
    }
  }
}'

curl -X PUT localhost:9200/products/product/1 -d '
{
    "name" : "Apple iPhone 5", 
    "name_suggest" : {
        "input" : [ "Apple Iphone 5" ],
        "output" : "foo",
        "weight" : 100
    }
}
'

curl -X GET localhost:9200/products/_mapping
```

did I miss anything from your description? Should I try with more indexing (I tried indexing 1000 items without any change)? Any other things I might try?
</comment><comment author="rdeaton" created="2013-10-07T17:58:19Z" id="25830292">I am not sure at what point it actually happened, but I was bulk indexing 50000 items at a time and was well into the millions when I noticed the issue. I didn't have a chance to try to reproduce it again, though I did talk to @kimchy offline about the issue and he said he believed he fixed something related. I have since upgraded and not seen the problem again, so it may be fixed somewhere already.
</comment><comment author="spinscale" created="2013-10-08T16:01:45Z" id="25903256">I see, I think this ticket is about what shay mentioned - https://github.com/elasticsearch/elasticsearch/issues/3844

could you test with master/0.90 branch and see if it is fixed in your test (only if you able to reproduce it obviously)
</comment><comment author="spinscale" created="2013-12-02T09:01:50Z" id="29602657">Closing this for now. Seems not to have happened again. If so, please tell immediately and we'll try to sort it out.

Thanks a lot for your help!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added armhf versions of openjdk 6 and 7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3687</link><project id="" key="" /><description>The debian init script does not include support for arm version of java. 

Added java-6-openjdk-armhf and java-7-openjdk-armhf to JDK_DIRS in debian init.d script

Fixes elasticsearch/elasticsearch#3659
</description><key id="19464254">3687</key><summary>Added armhf versions of openjdk 6 and 7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">matthewarkin</reporter><labels /><created>2013-09-13T17:02:57Z</created><updated>2014-07-04T09:51:37Z</updated><resolved>2013-09-16T07:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-16T07:59:02Z" id="24493898">Closed by https://github.com/elasticsearch/elasticsearch/commit/6356ad2228c47ade41f2f717f60f22363a987211 in master and https://github.com/elasticsearch/elasticsearch/commit/52fd0cd6f701486e2ca50e53df70b71d927bb861 in 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set nowInMillis to search context created by the count, validate query and explain api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3686</link><project id="" key="" /><description>Set nowInMillis to search context created by the count api, validate query api and explain api so that "NOW" can be used within queries

Added nowInMillis to ShardCountRequest, ShardValidateRequest and ExplainRequest in a backwards compatible manner

Fixes #3625, #3626 &amp; #3629 
</description><key id="19462189">3686</key><summary>Set nowInMillis to search context created by the count, validate query and explain api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-09-13T16:20:58Z</created><updated>2014-07-16T21:52:21Z</updated><resolved>2013-09-20T20:24:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-17T14:28:36Z" id="24592321">+1 Lets get this in!
</comment><comment author="javanna" created="2013-09-20T20:24:31Z" id="24838841">Merged and backported to 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deb/RPM: Disable immediate restart on package upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3685</link><project id="" key="" /><description>In order to have the possibility for unattended upgrades and to make sure, that these automatic upgrades do not start rebalancing the shards of a cluster around, the restart of elasticsearch after a package upgrade is now disabled by default. This is also important when providing repositories.

You can reenable it by setting `RESTART_ON_UPGRADE=true` in `/etc/default/elasticsearch` for debian packages or `/etc/sysconfig/elasticsearch` for redhat packages.

**Note**: This is a breaking change, as the debian package used to restart the elasticsearch service on every update, which now needs to be done by hand.
</description><key id="19460180">3685</key><summary>Deb/RPM: Disable immediate restart on package upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>breaking</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-13T15:45:14Z</created><updated>2013-09-16T07:54:03Z</updated><resolved>2013-09-13T15:47:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Allow packages to disable immediate restart on package upgrade</comment></comments></commit></commits></item><item><title>Feature request: support ?version=&amp;version_type= params in update API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3684</link><project id="" key="" /><description>It would be great if the `update` API supported the same versioning parameters as the `index` API. The failure semantics is a bit more complex; what I would expect would be:
- `version_type=internal`: fail if version on getting the doc does not match the version specified; ignore `retry_on_conflict` and fail immediately on any conflict (a conflict would mean the version after indexing no longer matches)
- `version_type=external`: fail if version on getting the doc is &gt;= specified version; obey `retry_on_conflict` while the version in conflict is &lt; the specified version; fail if a retry sees a version &gt;= the version specified.
</description><key id="19454082">3684</key><summary>Feature request: support ?version=&amp;version_type= params in update API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">tikitu</reporter><labels /><created>2013-09-13T14:41:04Z</created><updated>2014-08-08T17:07:31Z</updated><resolved>2014-08-08T17:07:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-09-13T15:03:56Z" id="24400822">The `version` and `version_type` support has been added via #3111, but has only be added to master, so if there is a version conflict then the update requests fails without retrying for both internal and external version types. For the external version type there will only be a conflict when the current version is bigger or equal to the specified version.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>pull request for #3673 (span_near query should accept slop = -1 (bis))</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3683</link><project id="" key="" /><description>master branch
</description><key id="19448704">3683</key><summary>pull request for #3673 (span_near query should accept slop = -1 (bis))</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-09-13T13:52:10Z</created><updated>2014-06-18T07:46:26Z</updated><resolved>2013-09-13T15:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-13T13:54:45Z" id="24395852">Thanks for the pull request! May I ask you to squash the commits? We also usually rebase to avoid those "merge commits", would you mind fixing that too?
</comment><comment author="iksnalybok" created="2013-09-13T15:06:19Z" id="24400991">By the way, this pull request contains fixes already integrated. I'm discarding it. I'll try to create a new clean one.
</comment><comment author="javanna" created="2013-09-13T15:09:13Z" id="24401191">Thanks for your effort @iksnalybok ! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed inability to set keep_words_path in KeepWordFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3682</link><project id="" key="" /><description>It seems `keep_words_path` can't be set, because `keep_words` (`arrayKeepWords`) is not `null` when not defined in config. So `ElasticSearchIllegalArgumentException` is thrown.
</description><key id="19445394">3682</key><summary>Fixed inability to set keep_words_path in KeepWordFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">vhyza</reporter><labels /><created>2013-09-13T12:42:06Z</created><updated>2014-07-16T21:52:22Z</updated><resolved>2013-11-04T15:53:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-11-04T15:53:01Z" id="27695628">I've just merged and pushed this. Thx for bringing it up and sorry it took so long.

See: https://github.com/elasticsearch/elasticsearch/commit/47969efae9646a2d4c5e0cfc3b3eaac9b71e7b71
</comment><comment author="vhyza" created="2013-11-11T12:39:21Z" id="28195728">No problem, thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove get index templates deprecated methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3681</link><project id="" key="" /><description>In 0.90.4, we deprecated some code:
- `GetIndexTemplatesRequest#GetIndexTemplatesRequest(String)` moved to `GetIndexTemplatesRequest#GetIndexTemplatesRequest(String...)`
- `GetIndexTemplatesRequest#name(String)` moved to `GetIndexTemplatesRequest#names(String...)`
- `GetIndexTemplatesRequest#name()` moved to `GetIndexTemplatesRequest#names()`
- `GetIndexTemplatesRequestBuilder#GetIndexTemplatesRequestBuilder(IndicesAdminClient, String)` moved to  `GetIndexTemplatesRequestBuilder#GetIndexTemplatesRequestBuilder(IndicesAdminClient, String...)`
- `IndicesAdminClient#prepareGetTemplates(String)` moved to `IndicesAdminClient#prepareGetTemplates(String...)`
- `AbstractIndicesAdminClient#prepareGetTemplates(String)` moved to `AbstractIndicesAdminClient#prepareGetTemplates(String...)`

We can now remove that old methods in 1.0.

**Note**: it breaks the Java API

Relative to #2532.
</description><key id="19441570">3681</key><summary>Remove get index templates deprecated methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>breaking</label><label>non-issue</label><label>v1.0.0.Beta1</label></labels><created>2013-09-13T10:52:01Z</created><updated>2013-09-13T13:10:04Z</updated><resolved>2013-09-13T13:10:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/GetIndexTemplatesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/GetIndexTemplatesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file></files><comments><comment>Remove get index templates deprecated methods</comment><comment>In 0.90.4, we deprecated some code:</comment></comments></commit></commits></item><item><title>Remove RestActions#splitXXX(String) methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3680</link><project id="" key="" /><description>We want to remove RestActions#splitXXX(String) methods:
- `RestActions#splitIndices(String)`
- `RestActions#splitTypes(String)`
- `RestActions#splitNodes(String)`

And replace with `Strings.splitStringByCommaToArray(String)`

It breaks the JAVA API.
</description><key id="19438689">3680</key><summary>Remove RestActions#splitXXX(String) methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>breaking</label><label>non-issue</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-13T09:34:26Z</created><updated>2013-11-13T07:07:08Z</updated><resolved>2013-09-13T09:39:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Strings.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/hotthreads/RestNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/restart/RestNodesRestartAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/shutdown/RestNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/shards/RestClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestGetIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/head/RestAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/close/RestCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/delete/RestDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/exists/indices/RestIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/exists/types/RestTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/gateway/snapshot/RestGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/delete/RestDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/open/RestOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/status/RestIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/delete/RestDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/get/RestGetWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file></files><comments><comment>Remove RestActions#splitXXX(String) methods</comment><comment>* `RestActions#splitIndices(String)`</comment><comment>* `RestActions#splitTypes(String)`</comment><comment>* `RestActions#splitNodes(String)`</comment></comments></commit></commits></item><item><title>Add documentation heading and section links</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3679</link><project id="" key="" /><description>It is a frequent usecase that one wants to deep link into the ES documentation, but it is made difficult due to the lack of fragment identifiers in the html.

Compare MongoDB's installation document:

http://docs.mongodb.org/manual/tutorial/install-mongodb-on-debian/

to that of ES's:

http://www.elasticsearch.org/guide/reference/setup/installation/

Of note is their use of heading and section links. I can easily express to a fellow developer or colleague where to find information on apt:

http://docs.mongodb.org/manual/tutorial/install-mongodb-on-debian/#configure-package-management-system-apt

Another popular example is github's documentation:

https://help.github.com/articles/github-flavored-markdown#differences-from-traditional-markdown

I personally believe this would help aid communication in forum posts, stackoverflow questions / answers, github issues, irc, etc.
</description><key id="19424149">3679</key><summary>Add documentation heading and section links</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">btiernay</reporter><labels /><created>2013-09-13T00:48:23Z</created><updated>2013-09-30T19:12:35Z</updated><resolved>2013-09-30T19:12:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-09-25T15:03:45Z" id="25093670">I agree that sub heading links would be a great addition, it looks like the new documentation system based on asciidoc we're working towards will support that also, I'll look into doing that!
</comment><comment author="dakrone" created="2013-09-30T19:12:35Z" id="25394296">Closed by 0442b737be0f0bf11958ebf7facacfff674523cd
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Add more anchor links to documentation</comment></comments></commit></commits></item><item><title>update to jsr166e, renaming spliterator() in ConcurrentHashMapV8 to avoid compile error on Java 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3678</link><project id="" key="" /><description>Hi Elasticsearch team, while playing with ES build on Java 8 I encountered this issue, a name clash of JDK 8 spliterator() method in java.util.Set and jsr166e ConcurrentHashMapV8 spliterator().

I updated to the latest CVS of jsr166e but the issue did not disappear. So I simply renamed the method to spliteratorV7(). This should allow compilation unless this is fixed in upstream jsr66e.
</description><key id="19415836">3678</key><summary>update to jsr166e, renaming spliterator() in ConcurrentHashMapV8 to avoid compile error on Java 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-09-12T21:18:48Z</created><updated>2014-07-28T09:59:04Z</updated><resolved>2014-07-28T09:59:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-18T16:54:08Z" id="30858783">fixed in #4510, note, there seems to be bugs in mvel under Java 8....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Alias filter not applied when using 'multi-index' syntax with wild card in URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3677</link><project id="" key="" /><description>If I try to use a multi-index search with a wildcard on filter aliases the filter does not get applied. However it does work with a multi-index comma separated list. See example below - "someone_else" is included in the results. Only tested with version 0.90.2.

```
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy"
}'

curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '{
    "user" : "jbrook"
}'

curl -XPUT 'http://localhost:9200/twitter/tweet/3' -d '{
    "user" : "someone_else"
}'

curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "twitter",
                 "alias" : "tweets_by_kimchy",
                 "filter" : { "term" : { "user" : "kimchy" } }
            }
        },
        {
            "add" : {
                 "index" : "twitter",
                 "alias" : "tweets_by_jbrook",
                 "filter" : { "term" : { "user" : "jbrook" } }
            }
        }
    ]
}'

curl -XPOST 'http://localhost:9200/tweets_by_kimchy,tweets_by_jbrook/_search?pretty=1'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 1.0, "_source" : {
    "user" : "kimchy"
}
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 1.0, "_source" : {
    "user" : "jbrook"
}
    } ]
  }


curl -XPOST 'http://localhost:9200/tweets_by_*/_search?pretty=1'

{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 1.0, "_source" : {
    "user" : "kimchy"
}
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 1.0, "_source" : {
    "user" : "jbrook"
}
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "3",
      "_score" : 1.0, "_source" : {
   ** "user" : "someone_else" **
}
    } ]
  }
```
</description><key id="19404839">3677</key><summary>Alias filter not applied when using 'multi-index' syntax with wild card in URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">jbrook</reporter><labels><label>bug</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-12T18:21:18Z</created><updated>2013-09-17T11:30:51Z</updated><resolved>2013-09-17T11:30:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file></files><comments><comment>Alias filter not applied when using 'multi-index' syntax with wild card in URL</comment><comment>closes #3677</comment></comments></commit></commits></item><item><title>term lookup query with no results should evaluate to no match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3676</link><project id="" key="" /><description>When a term lookup query returns no results it is considered a no filter rather than a no match which causes other filters to ignore the results.

For example:

``` js
 {
   "query": {
       "filtered": {
           "filter": {
               "and": [
                   {
                       "term": {
                           "name": "value1"
                       }
                   },
                   {
                       "terms": {
                           "_id": {
                               "index": "users",
                               "type": "user",
                               "id": "2",
                               "path": "followers"
                           }
                       }
                   }
               ]
           }
       }
   } 
}
```

In this case if the terms lookup returned no results it would ignore the terms lookup filter altogether when it should return no results because of the overall evaluation of the and should be false.
</description><key id="19399784">3676</key><summary>term lookup query with no results should evaluate to no match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">jgautier</reporter><labels /><created>2013-09-12T16:50:45Z</created><updated>2014-07-01T17:59:52Z</updated><resolved>2013-10-22T22:38:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-10-22T22:38:56Z" id="26860699">Thanks for the PR. I believe this issue was as part of #3838.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>term lookup query with no results should evaluate to no match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3675</link><project id="" key="" /><description>When a term lookup query returns no results it is considered a no filter rather than a no match which causes other filters to ignore the results.

For example:

``` js
 {
   "query": {
       "filtered": {
           "filter": {
               "and": [
                   {
                       "term": {
                           "name": "value1"
                       }
                   },
                   {
                       "terms": {
                           "_id": {
                               "index": "users",
                               "type": "user",
                               "id": "2",
                               "path": "followers"
                           }
                       }
                   }
               ]
           }
       }
   } 
}
```

In this case if the terms lookup returned no results it would ignore the terms lookup filter altogether when it should return no results because of the overall evaluation of the and should be false.
</description><key id="19399253">3675</key><summary>term lookup query with no results should evaluate to no match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">jgautier</reporter><labels /><created>2013-09-12T16:40:48Z</created><updated>2013-10-22T22:35:38Z</updated><resolved>2013-10-22T22:35:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-10-22T22:35:38Z" id="26860510">Fixed by #3838.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Lastest git master does not build clean</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3674</link><project id="" key="" /><description>  1&gt; REPRODUCE WITH  : mvn test -Dtests.seed=23CD52CEE15D72 -Dtests.class=org.elasticsearch.routing.SimpleRoutingTests -Dtests.method=testSimpleSearchRouting -Dtests.prefix=tests -Dtests.integration= -Dtests.nightly= -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles
  1&gt; Throwable:
  1&gt; org.elasticsearch.index.mapper.MapperParsingException: External routing [0] and document path routing [null] mismatch
  1&gt;     __randomizedtesting.SeedInfo.seed([23CD52CEE15D72:BC2675518BB49BEA]:0)
  1&gt;     org.elasticsearch.index.mapper.internal.RoutingFieldMapper.validate(RoutingFieldMapper.java:185)
  1&gt;     org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:520)
  1&gt;     org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:452)
  1&gt;     org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:358)
  1&gt;     org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:201)
  1&gt;     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:544)
  1&gt;     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:429)
  1&gt;     java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;     java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&gt;     java.lang.Thread.run(Thread.java:724)
  1&gt; 
  1&gt; [2013-09-11 16:12:37,660][INFO ][org.elasticsearch        ] Test testSimpleSearchRouting(org.elasticsearch.routing.SimpleRoutingTests) finished
ERROR   0.18s | SimpleRoutingTests.testSimpleSearchRouting &lt;&lt;&lt;

&gt; Throwable #1: org.elasticsearch.index.mapper.MapperParsingException: External routing [0] and document path routing [null] mismatch
&gt;    at __randomizedtesting.SeedInfo.seed([23CD52CEE15D72:BC2675518BB49BEA]:0)
&gt;    at org.elasticsearch.index.mapper.internal.RoutingFieldMapper.validate(RoutingFieldMapper.java:185)
&gt;    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:520)
&gt;    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:452)
&gt;    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:358)
&gt;    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:201)
&gt;    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:544)
&gt;    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:429)
&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
&gt;    at java.lang.Thread.run(Thread.java:724)
&gt; Completed in 1.23s, 6 tests, 1 error &lt;&lt;&lt; FAILURES!

  1&gt; REPRODUCE WITH  : mvn test -Dtests.seed=23CD52CEE15D72 -Dtests.class=org.elasticsearch.search.simple.SimpleSearchTests -Dtests.method=simpleDateMathTests -Dtests.prefix=tests -Dtests.integration= -Dtests.nightly= -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles
  1&gt; Throwable:
  1&gt; java.lang.AssertionError: Hit count is 0 but 2 was expected.  Total shards: 5 Successful shards: 5 &amp; 0 shard failures:
  1&gt;     __randomizedtesting.SeedInfo.seed([23CD52CEE15D72:16F64B1793B3F5AC]:0)
  1&gt;     [...org.junit._]
  1&gt;     org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:103)
  1&gt;     org.elasticsearch.search.simple.SimpleSearchTests.simpleDateMathTests(SimpleSearchTests.java:155)
  1&gt;     [...sun._, com.carrotsearch.randomizedtesting._, java.lang.reflect._]
  1&gt;     org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  1&gt;     org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     java.lang.Thread.run(Thread.java:724)
  1&gt; 
  1&gt; [2013-09-11 16:06:04,998][INFO ][org.elasticsearch        ] Test simpleDateMathTests(org.elasticsearch.search.simple.SimpleSearchTests) finished
FAILURE 0.21s | SimpleSearchTests.simpleDateMathTests &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError: Hit count is 0 but 2 was expected.  Total shards: 5 Successful shards: 5 &amp; 0 shard failures:
&gt;    at __randomizedtesting.SeedInfo.seed([23CD52CEE15D72:16F64B1793B3F5AC]:0)
&gt;    at org.junit.Assert.fail(Assert.java:93)
&gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:103)
&gt;    at org.elasticsearch.search.simple.SimpleSearchTests.simpleDateMathTests(SimpleSearchTests.java:155)
&gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&gt;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
&gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
&gt;    at java.lang.reflect.Method.invoke(Method.java:606)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1559)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.access$600(RandomizedRunner.java:79)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:737)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:773)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:787)
&gt;    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
&gt;    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
&gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
&gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
&gt;    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
&gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
&gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:358)
&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:782)
&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:442)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:746)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:648)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:682)
&gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:693)
&gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
&gt;    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
&gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
&gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
&gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
&gt;    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
&gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
&gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)
&gt;    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
&gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
&gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:358)
&gt;    at java.lang.Thread.run(Thread.java:724)
</description><key id="19378192">3674</key><summary>Lastest git master does not build clean</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">h4ck3rm1k3</reporter><labels /><created>2013-09-12T12:28:08Z</created><updated>2013-10-30T11:47:40Z</updated><resolved>2013-10-30T11:47:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-09-12T17:38:36Z" id="24340533">Thanks a lot for taking the time to report the failures.  This is great as we've been working on some rare failure cases the past few weeks. We see them since we switched to randomized testing [1]. We have seen something like the second one before, but not the first.  Can you share the complete log output for both? I have a couple of questions: 

Were you running the tests while the machine was under load? 
Which jvm are you using? 
Can you reproduce the failure on your machine? (I cannot unfortunately.) If so, could you try with using only one jvm (add the -Dtests.jvms=1 param)?

[1] http://labs.carrotsearch.com/randomizedtesting.html
</comment><comment author="h4ck3rm1k3" created="2013-09-12T23:30:37Z" id="24363672">on the second run, this first one worked.
/home/mdupont/experiments/maven/OME/experiments/maven/apache-maven-3.0-SNAPSHOT/bin/mvn test -Dtests.seed=23CD52CEE15D72 -Dtests.class=org.elasticsearch.routing.SimpleRoutingTests -Dtests.method=testSimpleSearchRouting -Dtests.prefix=tests -Dtests.integration= -Dtests.nightly= -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles
[INFO] Scanning for projects...
[INFO]  
[INFO] ------------------------------------------------------------------------
[INFO] Building elasticsearch 1.0.0.Beta1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0:enforce (enforce-maven) @ elasticsearch ---
[INFO] 
[INFO] --- buildnumber-maven-plugin:1.2:create (default) @ elasticsearch ---
[INFO] Checking for local modifications: skipped.
[INFO] Updating project files from SCM: skipped.
[INFO] Executing: /bin/sh -c cd /home/mdupont/experiments/elasticsearch &amp;&amp; git rev-parse --verify HEAD
[INFO] Working directory: /home/mdupont/experiments/elasticsearch
[INFO] Storing buildNumber: fddb7420ae266378499ac51215ffe040f81ce31a at timestamp: 1379028572295
[INFO] Executing: /bin/sh -c cd /home/mdupont/experiments/elasticsearch &amp;&amp; git rev-parse --verify HEAD
[INFO] Working directory: /home/mdupont/experiments/elasticsearch
[INFO] Storing buildScmBranch: UNKNOWN
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ elasticsearch ---
[INFO] Compiling 36 source files to /home/mdupont/experiments/elasticsearch/target/classes
[INFO] 
[INFO] --- forbiddenapis:1.3:check (check-forbidden-apis) @ elasticsearch ---
[INFO] Reading bundled API signatures: jdk-unsafe
[INFO] Reading bundled API signatures: jdk-deprecated
[INFO] Reading bundled API signatures: jdk-system-out
[INFO] Reading API signatures: /home/mdupont/experiments/elasticsearch/core-signatures.txt
[INFO] Loading classes to check...
[INFO] Scanning for API signatures and dependencies...
[INFO] Scanned 4599 (and 1154 related) class file(s) for forbidden API invocations (in 5.85s), 0 error(s).
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 166 resources
[INFO] Copying 21 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- forbiddenapis:1.3:testCheck (check-forbidden-test-apis) @ elasticsearch ---
[INFO] Reading bundled API signatures: jdk-unsafe
[INFO] Reading bundled API signatures: jdk-deprecated
[INFO] Loading classes to check...
[INFO] Scanning for API signatures and dependencies...
[INFO] Scanned 714 (and 1503 related) class file(s) for forbidden API invocations (in 1.36s), 0 error(s).
[INFO] 
[INFO] --- maven-surefire-plugin:2.15:test (default-test) @ elasticsearch ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- junit4-maven-plugin:2.0.10:junit4 (tests) @ elasticsearch ---
[INFO] &lt;JUnit4&gt; says ciao! Master seed: 23CD52CEE15D72
Executing 1 suite with 1 JVM.

Started J0 PID(19108@openlawrence.com).
Suite: org.elasticsearch.routing.SimpleRoutingTests
Completed in 12.90s, 1 test

[INFO] JVM J0:     0.76 ..    14.44 =    13.67s
[INFO] Execution time total: 14 seconds
[INFO] Tests summary: 1 suite, 1 test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 31.616s
[INFO] Finished at: Thu Sep 12 16:29:59 PDT 2013
[INFO] Final Memory: 21M/144M
[INFO] ------------------------------------------------------------------------
</comment><comment author="h4ck3rm1k3" created="2013-09-12T23:32:16Z" id="24363741">same for teh second. I will rerun all tests 
/home/mdupont/experiments/maven/OME/experiments/maven/apache-maven-3.0-SNAPSHOT/bin/mvn  test -Dtests.seed=23CD52CEE15D72 -Dtests.class=org.elasticsearch.search.simple.SimpleSearchTests -Dtests.method=simpleDateMathTests -Dtests.prefix=tests -Dtests.integration= -Dtests.nightly= -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles
[INFO] Scanning for projects...
[INFO]  
[INFO] ------------------------------------------------------------------------
[INFO] Building elasticsearch 1.0.0.Beta1-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-enforcer-plugin:1.0:enforce (enforce-maven) @ elasticsearch ---
[INFO] 
[INFO] --- buildnumber-maven-plugin:1.2:create (default) @ elasticsearch ---
[INFO] Checking for local modifications: skipped.
[INFO] Updating project files from SCM: skipped.
[INFO] Executing: /bin/sh -c cd /home/mdupont/experiments/elasticsearch &amp;&amp; git rev-parse --verify HEAD
[INFO] Working directory: /home/mdupont/experiments/elasticsearch
[INFO] Storing buildNumber: fddb7420ae266378499ac51215ffe040f81ce31a at timestamp: 1379028675049
[INFO] Executing: /bin/sh -c cd /home/mdupont/experiments/elasticsearch &amp;&amp; git rev-parse --verify HEAD
[INFO] Working directory: /home/mdupont/experiments/elasticsearch
[INFO] Storing buildScmBranch: UNKNOWN
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] Copying 5 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ elasticsearch ---
[INFO] Compiling 36 source files to /home/mdupont/experiments/elasticsearch/target/classes
[INFO] 
[INFO] --- forbiddenapis:1.3:check (check-forbidden-apis) @ elasticsearch ---
[INFO] Reading bundled API signatures: jdk-unsafe
[INFO] Reading bundled API signatures: jdk-deprecated
[INFO] Reading bundled API signatures: jdk-system-out
[INFO] Reading API signatures: /home/mdupont/experiments/elasticsearch/core-signatures.txt
[INFO] Loading classes to check...
[INFO] Scanning for API signatures and dependencies...
[INFO] Scanned 4599 (and 1154 related) class file(s) for forbidden API invocations (in 6.33s), 0 error(s).
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ elasticsearch ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 166 resources
[INFO] Copying 21 resources
[INFO] 
[INFO] --- maven-compiler-plugin:2.3.2:testCompile (default-testCompile) @ elasticsearch ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- forbiddenapis:1.3:testCheck (check-forbidden-test-apis) @ elasticsearch ---
[INFO] Reading bundled API signatures: jdk-unsafe
[INFO] Reading bundled API signatures: jdk-deprecated
[INFO] Loading classes to check...
[INFO] Scanning for API signatures and dependencies...
[INFO] Scanned 714 (and 1503 related) class file(s) for forbidden API invocations (in 1.13s), 0 error(s).
[INFO] 
[INFO] --- maven-surefire-plugin:2.15:test (default-test) @ elasticsearch ---
[INFO] Tests are skipped.
[INFO] 
[INFO] --- junit4-maven-plugin:2.0.10:junit4 (tests) @ elasticsearch ---
[INFO] &lt;JUnit4&gt; says ciao! Master seed: 23CD52CEE15D72
Executing 1 suite with 1 JVM.

Started J0 PID(19242@openlawrence.com).
Suite: org.elasticsearch.search.simple.SimpleSearchTests
Completed in 12.69s, 1 test

[INFO] JVM J0:     0.79 ..    14.19 =    13.40s
[INFO] Execution time total: 14 seconds
[INFO] Tests summary: 1 suite, 1 test
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32.343s
[INFO] Finished at: Thu Sep 12 16:31:42 PDT 2013
[INFO] Final Memory: 20M/143M
[INFO] ------------------------------------------------------------------------
</comment><comment author="brwe" created="2013-09-13T14:24:46Z" id="24397986">Thanks a lot for taking the time to do this! It is too bad the failure is not reproducible. We will keep investigating and keep you posted in case we find anything related to the failure. Sorry for not having more information at the moment.

Please do not hesitate to paste any test failure logs here in the future, even if the logs are long. 
</comment><comment author="h4ck3rm1k3" created="2013-09-13T20:20:58Z" id="24421737">well i tried to build with mvn test -X for debug and it never endedd, the
log was so huge and full of the same line repeated over and over

On Fri, Sep 13, 2013 at 9:24 AM, Britta Weber notifications@github.comwrote:

&gt; Thanks a lot for taking the time to do this! It is too bad the failure is
&gt; not reproducible. We will keep investigating and keep you posted in case we
&gt; find anything related to the failure. Sorry for not having more information
&gt; at the moment.
&gt; 
&gt; Please do not hesitate to paste any test failure logs here in the future,
&gt; even if the logs are long.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3674#issuecomment-24397986
&gt; .

## 

James Michael DuPont
Member of Free Libre Open Source Software Kosova http://flossk.org
Saving wikipedia(tm) articles from deletion http://SpeedyDeletion.wikia.com
Contributor FOSM, the CC-BY-SA map of the world http://fosm.org
Mozilla Rep https://reps.mozilla.org/u/h4ck3rm1k3
Free Software Foundation Europe Fellow http://fsfe.org/support/?h4ck3rm1k3
</comment><comment author="brwe" created="2013-09-14T09:56:27Z" id="24442111">could it be that this is similar to the line you are seeing:

HEARTBEAT J1 PID(862@brittas-MacBook-Pro.local): 2013-09-13T19:06:48, stalled for  203s at: SearchWithRandomExceptionsTests.testRandomExceptions
</comment><comment author="h4ck3rm1k3" created="2013-09-14T11:31:59Z" id="24443410">this is what it was full of, on smaller tests it produces only some of
these so I guess it is just verbose, but it really had a ton of them
[DEBUG] Packet received, slave#0&gt;APPEND_STDOUT

On Sat, Sep 14, 2013 at 4:56 AM, Britta Weber notifications@github.comwrote:

&gt; could it be that this is similar to the line you are seeing:
&gt; 
&gt; HEARTBEAT J1 PID(862@brittas-MacBook-Pro.local): 2013-09-13T19:06:48,
&gt; stalled for 203s at: SearchWithRandomExceptionsTests.testRandomExceptions
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3674#issuecomment-24442111
&gt; .

## 

James Michael DuPont
Member of Free Libre Open Source Software Kosova http://flossk.org
Saving wikipedia(tm) articles from deletion http://SpeedyDeletion.wikia.com
Contributor FOSM, the CC-BY-SA map of the world http://fosm.org
Mozilla Rep https://reps.mozilla.org/u/h4ck3rm1k3
Free Software Foundation Europe Fellow http://fsfe.org/support/?h4ck3rm1k3
</comment><comment author="brwe" created="2013-09-17T15:33:58Z" id="24597922">SearchWithRandomExceptionsTests.testRandomExceptions has been fixed, see #3694 

If that did not fix it, could you please provide the full log that contained the DEBUG line you pasted?
</comment><comment author="kwince" created="2013-10-11T04:42:50Z" id="26114365">I tried two times must 'mvn install' with no success with the error this thread is about. When I used only one JVM, it worked. Also, when I used the -X switch, and redirect the output to a text file, it works every time. So it's probably a timing issue. Using the -X switch probably slows it down enough to not have the timing issues. 

So, I have no debug script to offer. Just that I have the problem. I'll probably use the 1.0.0 jar at some point with it compiling to success with -X switch.
</comment><comment author="spinscale" created="2013-10-11T08:38:42Z" id="26122351"> @kwince if you just need to create a package and rather want to ignore the tests (dont get me wrong here, reporting test failures is perfectly valid and we are happy for that), you can run the packaging process without executing the tests via `mvn -DskipTests=true package` (you might know that already, just wanted to make sure you know in case you just want to play with the 1.0 snapshot zip/tar.gz archive).

Also if you can reproduce test failures, dont bother to put in some information including the seeds, so we can reproduce it with our randomized test architecture.
</comment><comment author="kwince" created="2013-10-11T15:26:37Z" id="26146129">I would love to help with feedback. All I now about is 'mvn install/compile/package/deploy'. All the other switches I've never used, until friday, when I used the 'one VM' switch.

Thank you for the skipTests switch.

The problem I have compiling 1.0.0 can't be reproduced the way I'm running it, when I ask for a debug output. It slows down the build just enough that the error does not happen. It's got to be some knind of race condition. I can't provide any feedback under these circumstances, right?

My original goal in compling ESearch was to find the 90.5 version and compile it. We are having the InetAddress Serializtion issue when we compile our project with Java 1.7, since your jars and zips are compiled with 1.6. We are having to downgrade our environments to 1.6 to get them running. I was hoping to test whether compiling Esearch with 1.7 would solve the problem (or find if the problem was in a dependency fo ESearch's). 

 
Dennis Gearon

Never, ever approach a computer saying or even thinking "I will just do this quickly."

---

 From: Alexander Reelsen notifications@github.com
To: elasticsearch/elasticsearch elasticsearch@noreply.github.com 
Cc: Dennis Gearon gearond@sbcglobal.net 
Sent: Friday, October 11, 2013 1:39 AM
Subject: Re: [elasticsearch] Lastest git master does not build clean (#3674)

@kwince if you just need to create a package and rather want to ignore the tests (dont get me wrong here, reporting test failures is perfectly valid and we are happy for that), you can run the packaging process without executing the tests via mvn -DskipTests=true package (you might know that already, just wanted to make sure you know in case you just want to play with the 1.0 snapshot zip/tar.gz archive).
Also if you can reproduce test failures, dont bother to put in some information including the seeds, so we can reproduce it with our randomized test architecture.
—
Reply to this email directly or view it on GitHub.
</comment><comment author="spinscale" created="2013-10-30T11:47:40Z" id="27382534">@kwince just as a reference

```
git checkout v0.90.5
mvn clean package -DskipTests=true
```

That should generate the same packages than on elasticsearch.org - however it does not really matter, if you build with java6 or java7 compiler as we are setting the compiled target java version to 1.6 in the `pom.xml`. Also if you change this to 1.7 your serialization problem would not have been solved, as you still use different JVMs to communicate during runtime (and this is where the problem occurs, not on compile time).

In the meantime we have continued our efforts to stabilize the tests further. If you run the tests yourself and you reliably manage to have a failing tests, please create a new github issue for this as well - which includes the mvn call to reproduce (we need a special call, which includes all the arguments needed to create the same setup for the randomized testing, this call is logged directly above the failed assertion of a failed test - or just include the full log).

Thanks to everyone for helping and notifying!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file></files><comments><comment>tag with @AwaitsFix because it causes the test suite to crash</comment></comments></commit></commits></item><item><title>span_near query should accept slop = -1 (bis)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3673</link><project id="" key="" /><description>Hi,

Previous fix (#3122) for issue #3079 (span_near query should accept slop = -1) is incomplete. Similar fix (e.g change int slop = -1;  into  Integer slop = null;) should also be applied in src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java.

Could you please also make the fix available in the 0.90.x branch ?

Thanks
</description><key id="19371115">3673</key><summary>span_near query should accept slop = -1 (bis)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">iksnalybok</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-12T09:36:20Z</created><updated>2013-09-26T08:53:44Z</updated><resolved>2013-09-20T19:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-12T11:18:35Z" id="24310753">Right I see what you mean. @iksnalybok do you want to send a pull request for this?
</comment><comment author="iksnalybok" created="2013-09-13T13:52:53Z" id="24395729">cf pull 3683 (master branch)

NB: do I need to create another for the 0.90.x branch ?
</comment><comment author="javanna" created="2013-09-13T21:55:37Z" id="24427394">@iksnalybok one pull request against master is enough, we can then cherry pick the commit to backport it to 0.90. Thanks!
</comment><comment author="iksnalybok" created="2013-09-19T21:16:02Z" id="24773963">I could not clean the pull request log in a reasonable amount of time. Would you mind doing the fix yourself ? Thanks.
</comment><comment author="nik9000" created="2013-09-24T10:52:24Z" id="24993153">Does the hack that this allows deserve a documentation update?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file></files><comments><comment>Allow slop = -1 in span queries</comment></comments></commit></commits></item><item><title>Facet Ordering is unexpected / wrong / inflexible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3672</link><project id="" key="" /><description>According Docs
http://www.elasticsearch.org/guide/reference/api/search/facets/terms-facet/
and code
org.elasticsearch.search.facet.terms.TermsFacet

ordering works for "count" and "term" in two directions.

for "count", the tie breaker is a "normal" compareTo, which will compare the String Text of the term. However this will produce a reverse order on the term names. For "reverse_count", this is the opposite.

so I end up with z:1, a:1, rather than a:1,z:1 which, to my expectation, more people would be able to use to find the matching entries.

Additionally, the comparison is always case sensitive and cannot be changed.

So in the end, It looks like I always will need to resort the entries myself. But this creates wasted effort. Right now, it is not possible to get the facets without any sorting.
Elasticsearch will always sort the Facets, and then I will have to sort them again.
</description><key id="19369646">3672</key><summary>Facet Ordering is unexpected / wrong / inflexible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CodingFabian</reporter><labels><label>discuss</label></labels><created>2013-09-12T09:03:42Z</created><updated>2014-07-08T15:25:33Z</updated><resolved>2013-09-13T13:11:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-12T13:46:20Z" id="24320271">Similar to #2078 . True, when you order by count, the items with the same count could be better ordered. This happens simply because the same sort direction is applied to the terms too when the count is the same. Here is an example:

```
"order" : "count" (aka order count descending)
a10 (100)  
z1 (1)
a1 (1)
```

```
"order" : "reverse_count" (aka order count ascending)
a1 (1)
z1 (1)
a10 (100)  
```

I agree this is not great but it happens as we support a single order option. It would probably be better to always sort alphabetically ascending when the count is the same. Or even better being able to combine different sorting strategies together as mentioned in #2078. As a side note, this is going to be improved with the new aggregations module (issue #3300).
</comment><comment author="CodingFabian" created="2013-09-12T19:22:43Z" id="24348401">Hi Luca,
I would be happy with getting unsorted facets. then i sort them myself. This would save some computing time on your end if i could ask for "order" : "unsorted"

I would be happy with that.
</comment><comment author="javanna" created="2013-09-12T20:08:20Z" id="24351620">Hi Fabian,
sorting is not just about showing the buckets in a certain order. In a distributed environment the terms are collected on each shard. Only the top ones (based on how you decided to order them) are returned to the node that will actually do the reduce, so that only "the top of the top ones" are effectively returned, again based on how you decided to order them. If you order by count, you can see the issue that you reported when the count is the same for some entries, but the entries that get returned will still be the ones with higher count. Removing the sorting at all would lead to get random facet entries back, is that what you mean?

I think this makes sense only if you are actually retrieving all the possible facet entries specifying a high `size` parameter. Is there anything I'm missing?
</comment><comment author="CodingFabian" created="2013-09-12T22:42:23Z" id="24361572">oh yeah, I missed that bit. but perhaps thats not a good enough usecase to be supported.
I guess i will go with just accepting the current count order and anyway resort it.
you may close this as duplicate if you desire :)
</comment><comment author="javanna" created="2013-09-13T13:11:51Z" id="24393259">Thanks for your feedback! I would close this one and suggest to move this discussion about sorting entries to the aggregations issue (#3300).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs don't say which takes precedence: persistent cluster settings, or elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3671</link><project id="" key="" /><description>I would have thought elasticsearch.yml, but the exception in #3670 refers to a setting that I have in my .yml file anyway:

```
threadpool:
    search:
        type: fixed
        size: 24
        queue_size: -1
```

This suggests that ES is trying to parse and act on the persistent cluster setting, even though that's already been set in the .yml file.

Version: 0.90.3
</description><key id="19340512">3671</key><summary>Docs don't say which takes precedence: persistent cluster settings, or elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels /><created>2013-09-11T19:18:24Z</created><updated>2014-07-04T12:38:43Z</updated><resolved>2014-07-04T12:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-09-13T00:39:37Z" id="24366146">Agreed, this was something that I couldn't easily find myself. I think it should mention that node level settings in the yaml file remain in place until overridden by cluster settings after joining.
</comment><comment author="hgesserit" created="2014-04-16T23:11:00Z" id="40663579">Agreed, I had to run tests to verify that cluster settings will override yml settings.  There's also no way I can see to remove cluster settings if you want to go back to using yml settings.
</comment><comment author="clintongormley" created="2014-07-04T12:38:43Z" id="48039345">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>No obvious way to delete persistent cluster admin settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3670</link><project id="" key="" /><description>There's no delete API for these, if you want to set them back to defaults on next startup.

karmi on irc suggested setting them to the empty string, e.g.:

```
curl -XPUT 'localhost:9200/_cluster/settings' -d '{"persistent":{"threadpool.search.type":""}}'
```

But then on next restart I got warnings like this:

```
[2013-09-11 20:08:41,548][WARN ][node.settings            ] [lo3uppaldbs009.palomino.pearson.com] failed to refresh settings for [org.elasticsearch.threadpool.ThreadPool$ApplySettings@49e3f731]
org.elasticsearch.ElasticSearchIllegalArgumentException: No type found [], for [search]
        at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:452)
        at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:468)
        at org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:809)
        at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:321)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:95)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
```

So now I have no way to tell what kind of thread pool I have.

Version: 0.90.3
</description><key id="19340352">3670</key><summary>No obvious way to delete persistent cluster admin settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">andrewclegg</reporter><labels /><created>2013-09-11T19:15:41Z</created><updated>2014-07-04T12:37:39Z</updated><resolved>2014-07-04T12:37:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrewclegg" created="2013-09-11T20:16:51Z" id="24272629">As a workaround, I had to stop the cluster, delete all my global state files, and restart, so it would reset to defaults (and anything defined in elasticsearch.yml). Thankfully I didn't have any other dynamically-added cluster settings.
</comment><comment author="jennysivapalan" created="2013-12-05T12:03:10Z" id="29892222">We had a similar issue, we tried to update the threadpool settings dynamically. The first time it applied but did not send an acknowledgement. Subsequent tries caused an error:-
ProcessClusterEventTimeoutException: failed to process cluster event (reroute_after_cluster_update_settings) within 30s.
We fixed this by also shutting down the cluster and deleting global state files.
</comment><comment author="jippi" created="2014-02-06T22:24:16Z" id="34380504">:+1: 
</comment><comment author="clintongormley" created="2014-07-04T09:07:26Z" id="48023011">Depends on #5019 
</comment><comment author="clintongormley" created="2014-07-04T12:37:39Z" id="48039256">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CompletionFieldMapper ignores path: just_name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3669</link><project id="" key="" /><description>`CompletionFieldMapper` is using full name of the field instead of index name during indexing and as a result completion doesn't work on `multi_field` fields with `"path": "just_name"`

Repro: https://gist.github.com/imotov/b11f866c48b090158d7e 
</description><key id="19338138">3669</key><summary>CompletionFieldMapper ignores path: just_name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-11T18:39:27Z</created><updated>2013-09-12T22:02:33Z</updated><resolved>2013-09-12T22:00:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-11T19:08:07Z" id="24267449">LGTM @spinscale can you take a look at it as well?
</comment><comment author="spinscale" created="2013-09-12T13:19:39Z" id="24317931">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>CompletionFieldMapper should use index name instead of full name</comment></comments></commit></commits></item><item><title>Setting network.publish_host to _eth1:ipv4_ makes host node name setting to be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3668</link><project id="" key="" /><description>setting -Des.node.name=&lt;someNodeName&gt; works fine, UNTIL I add network.publish_host=_eth1:ipv4_ which then causes the es.node.name paramter to be ineffective.  the nodes start with a superhero name instead.
</description><key id="19333990">3668</key><summary>Setting network.publish_host to _eth1:ipv4_ makes host node name setting to be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2013-09-11T17:29:27Z</created><updated>2014-08-08T17:07:04Z</updated><resolved>2014-08-08T17:07:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-11T17:30:23Z" id="24259638">Can you share how you provided the parameters? The exec line, something along those lines...
</comment><comment author="apatrida" created="2013-09-11T17:36:53Z" id="24260149">ip=`hostname -i`;/coraid_001/installs/elasticsearch-0.90.2/bin/elasticsearch -f -Des.node.name=$i

which worked up until the single change of adding the network publish_host setting in the YAML
</comment><comment author="imotov" created="2013-09-12T12:59:34Z" id="24316366">@jaysonminard  I cannot reproduce this issue based on the provided information. Could you post your YAML file that is causing the issue? 
</comment><comment author="clintongormley" created="2014-08-08T17:07:04Z" id="51630122">No feedback in a year, and no further reports.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rare race condition when introducing new fields into a mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3667</link><project id="" key="" /><description>Dynamic mapping allow to dynamically introduce new fields into an existing mapping. There is a (pretty rare) race condition, where a new field/object being introduced will not be immediately visible for another document that introduces it at the same time.

Relates to #3544 
</description><key id="19321230">3667</key><summary>Rare race condition when introducing new fields into a mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-11T14:07:38Z</created><updated>2013-09-11T14:09:14Z</updated><resolved>2013-09-11T14:09:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/ConcurrentDynamicTemplateTests.java</file></files><comments><comment>Rare race condition when introducing new fields into a mapping</comment><comment>Dynamic mapping allow to dynamically introduce new fields into an existing mapping. There is a (pretty rare) race condition, where a new field/object being introduced will not be immediately visible for another document that introduces it at the same time.</comment></comments></commit></commits></item><item><title>Generalising the RangeFacet (and associated parser/generator/test) to be optionally inclusive on each bound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3666</link><project id="" key="" /><description>Generalising the RangeFacet (and associated parser/generator/test) to be able to specify whether or not each bound is inclusive. The default behaviour remains the same (from is inclusive, to is exclusive). Issue #3660.

Have also requested a merge into master -- pull request #3665
</description><key id="19316201">3666</key><summary>Generalising the RangeFacet (and associated parser/generator/test) to be optionally inclusive on each bound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaunhall</reporter><labels /><created>2013-09-11T12:31:04Z</created><updated>2014-07-28T09:58:17Z</updated><resolved>2014-07-28T09:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shaunhall" created="2013-09-12T12:06:01Z" id="24313091">Note that this allows ES to behave as Solr does (with both bounds inclusive on range facets).
</comment><comment author="clintongormley" created="2014-07-28T09:58:17Z" id="50320149">Hi @shaunhall

Sorry it has taken a while to get to this. Facets are now deprecated, so we're not going to be accepting any more PRs for them.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Generalising the RangeFacet (and associated parser/generator/test) to be optionally inclusive on each bound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3665</link><project id="" key="" /><description>Generalising the RangeFacet (and associated parser/generator/test) to be able to specify whether or not each bound is inclusive. The default behaviour remains the same (from is inclusive, to is exclusive). Issue 3660.
</description><key id="19314602">3665</key><summary>Generalising the RangeFacet (and associated parser/generator/test) to be optionally inclusive on each bound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaunhall</reporter><labels /><created>2013-09-11T11:50:36Z</created><updated>2014-07-28T09:56:48Z</updated><resolved>2014-07-28T09:56:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T09:56:48Z" id="50320010">Hi @shaunhall 

Sorry it has taken a while to get to this.  Facets are now deprecated, so we're not going to be accepting any more PRs for them.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>date_histogram facet missing periods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3664</link><project id="" key="" /><description>The date histogram facet only returns data where documents exist, it would be good to have a parameter to autofill missing periods with 0 values eg if I have data for years 2009, 2011 &amp; 2012 there will only be 3 entries in the facet, I would like elasticsearch to create the missing entry for 2010 with 0 values so there are 4 entries in the facet.
</description><key id="19311574">3664</key><summary>date_histogram facet missing periods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">j0hnsmith</reporter><labels /><created>2013-09-11T10:41:16Z</created><updated>2013-09-11T12:43:43Z</updated><resolved>2013-09-11T12:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-11T12:43:42Z" id="24236612">As far as I remember a discussion with @uboness, we should have it in upcoming Aggregator feature: #3300.
Closing this one as no new features will be added to facets.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NodeDoesNotExistOnMasterException Handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3663</link><project id="" key="" /><description>This is related to #2117 

I'm able to reproduce a slight variation with the following steps:
Set discovery.zen.minimum_master_nodes = 2 and node.max_local_storage_nodes = 1. These nodes are deployed on NFS so that in case a machine is fails, an ES process can be started on the same data path from a different machine. Each machine has access to data directories for all es nodes.

The Initial deployment was:
- ES1 - Node1:9300 (path.data=/nfs/node1)
- ES2 - Node2:9300 (path.data=/nfs/node2)
- ES3 - Node3:9300 (path.data=/nfs/node3)

Then I took the following steps:
1. Killed the network on Node2, which resulted in a cluster with two nodes (ES1,ES3). ES2 process was still running on Node2.
2. Started new es process on Node1 with path.data=/nfs/node2. I was assuming since node.max_local_storage_nodes = 1, it will not start as ES2 on Node2:9300 already has lock on it, but it started anyways. The cluster now looked like
- ES1 - Node1:9300 (path.data=/nfs/node1)
- ES2 - Node1:9301 (path.data=/nfs/node2)
- ES3 - Node3:9300 (path.data=/nfs/node3)

ES2 - Node2:9300 (path.data=/nfs/node2) was still running but not part of the cluster.
3. I started the network on Node2 which resulted in following two clusters:
- Cluster1:
  - ES1 - Node1:9300 (path.data=/nfs/node1)
  - ES2 - Node1:9301 (path.data=/nfs/node2)
  - ES3 - Node3:9300 (path.data=/nfs/node3)
- Cluster2:
  - ES1 - Node1:9300 (path.data=/nfs/node1)
  - ES2 - Node2:9300 (path.data=/nfs/node2)

Now, Node1:9300 is participating in both the clusters

This happens because of the way NodeDoesNotExistOnMasterException is handled at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java#L315

When Node2 comes into network, it sees that it is no longer a part of the cluster and starts master election resulting in a new master.
</description><key id="19301956">3663</key><summary>NodeDoesNotExistOnMasterException Handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">anandnalya</reporter><labels /><created>2013-09-11T06:40:18Z</created><updated>2015-01-26T08:45:39Z</updated><resolved>2015-01-26T08:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="anandnalya" created="2013-09-24T13:47:54Z" id="25004646">I'm using the following workaround for this: https://github.com/anandnalya/elasticsearch/commit/8aba868730c80bde103b4a49dec6cb10d3171ba3
</comment><comment author="spinscale" created="2013-09-25T09:27:58Z" id="25072742">Hey,

couple of notes:
- `node.max_local_storage_nodes` only applies, if you use the same installation directory. As you have a different `data.path` this setting does not apply.
- You should not use NFS at all, when working with elasticsearch. Your search and indexing performance will be dramatically slower. Also native locking is not supported on NFS. You might want to read this thread for further information: http://lucene.472066.n3.nabble.com/Lucene-index-on-NFS-td4011301.html
- You should especially make sure, that data directories are not written by two separate process, no matter if these process are on one machine or because your network storage are on a cluster.

Hope this helps (and ensures having a fast and reliable search engine up and running).
</comment><comment author="anandnalya" created="2013-09-25T10:19:00Z" id="25075460">HI,

I wanted to know if there are any other complications of using ES over NFS apart from the lucene gotchas.

Thanks,
Anand
</comment><comment author="bleskes" created="2015-01-26T08:45:39Z" id="71429043">I think this should be solved with the improvement to Zen Discovery we made in 1.4. If this is not the case, please feel free to re-open.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Collapse test.{unit,integration} into org.elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3662</link><project id="" key="" /><description>In order to reach more congruence with the main source code, move the `test.integration` and `test.unit` packages up to `org.elasticsearch`.
</description><key id="19293154">3662</key><summary>Collapse test.{unit,integration} into org.elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2013-09-11T00:35:53Z</created><updated>2014-07-16T21:52:23Z</updated><resolved>2013-09-11T17:51:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-11T00:43:23Z" id="24205719">Why remove all those static imports clean up the lines with trailing spaces at the same time?  Seems like it'd make it harder to merge with in progress changes.
</comment><comment author="s1monw" created="2013-09-11T07:44:53Z" id="24218924">these commits look go to me. I'd still appreciate a second pair of eyes. The good thing is that tagging integration tests with annotations will be easy since they all subclass from AbstractNode / AbstractSharedClusterTests
</comment><comment author="jpountz" created="2013-09-11T07:55:09Z" id="24219336">This looks good to me too. As @nik9000 noted, I'm just a bit worried about the trailing whitespaces cleanup since I have a big change in progress that I will need to rebase these commits against, so it would be nice to re-add the whitespaces if it is not too much work.
</comment><comment author="s1monw" created="2013-09-11T07:57:34Z" id="24219446">@drewr can you add back the whitespaces and then squash and push?
</comment><comment author="drewr" created="2013-09-11T17:51:05Z" id="24261305">Whitespace fixed and merged to master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JsonGenerationException thrown in SuggestResponse#toString method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3661</link><project id="" key="" /><description>The following exception is consistently thrown (and caught) by the SuggestResponse#toString method:

```
com.fasterxml.jackson.core.JsonGenerationException: Can not write a field name, expecting a value
    at com.fasterxml.jackson.core.base.GeneratorBase._reportError(GeneratorBase.java:444)
    at com.fasterxml.jackson.core.json.UTF8JsonGenerator.writeFieldName(UTF8JsonGenerator.java:167)
    at org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeFieldName(JsonXContentGenerator.java:74)
    at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:265)
    at org.elasticsearch.common.xcontent.XContentBuilder.startArray(XContentBuilder.java:210)
    at org.elasticsearch.search.suggest.Suggest$Suggestion.toXContent(Suggest.java:317)
    at org.elasticsearch.search.suggest.Suggest.toXContent(Suggest.java:148)
    at org.elasticsearch.action.suggest.SuggestResponse.toString(SuggestResponse.java:74)
    at java.lang.String.valueOf(String.java:2854)
    at java.io.PrintStream.println(PrintStream.java:821)
```

There seems to be a missing startObject/endObject. Even though it is not a blocking problem, it would be nice to be able to print out the string representation of a suggest response when needed.
</description><key id="19282413">3661</key><summary>JsonGenerationException thrown in SuggestResponse#toString method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-10T20:20:55Z</created><updated>2013-09-10T20:35:32Z</updated><resolved>2013-09-10T20:35:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/suggest/SuggestResponse.java</file></files><comments><comment>Rewrote SuggestResponse#toString method</comment></comments></commit></commits></item><item><title>Make bounds on RangeFacet optionally inclusive/exclusive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3660</link><project id="" key="" /><description>The default behaviour of range facets in ES is that the 'from' bound is inclusive and the 'to' bound is exclusive. This makes sense for many use cases, but it would be useful to be able to change this, for example by making the to bound inclusive.

I have the changes ready (+tests :), I would like to commit to 0.90.X and trunk. The changed files are RangeFacetBuilder, RangeFacet, RangeFacetExecutor and RangeFacetParser.

Please can you let me know if I can go ahead and commit?

Thanks,
Shaun
</description><key id="19271938">3660</key><summary>Make bounds on RangeFacet optionally inclusive/exclusive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">shaunhall</reporter><labels /><created>2013-09-10T17:00:25Z</created><updated>2013-09-16T12:09:55Z</updated><resolved>2013-09-16T12:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-09-12T12:26:57Z" id="24314140">Hi @shaunhall 

The `from` / `to` parameters have been designed specifically to make it easy to create ranges, by just adding the difference, eg:

```
from: 100, to: 150
from: 150, to: 200
from: 200, to: 250
etc
```

I'd be interested to know what your use cases are that would make configurable include/exclude bounds useful?

clint
</comment><comment author="shaunhall" created="2013-09-12T12:35:10Z" id="24314631">Hi Clint,

Yes, I agree, the default behaviour is ideal for mapping points on a scale to a set of ranges, and if we were building a system from scratch this would suit us.

However, we're migrating our search implementation from Solr to ES and the Solr behaviour for range facets is inclusive on both bounds.

To accommodate the Solr behaviour, we've set all our facet range bounds on a variety of different types (price, ratings, savings) to have a small gap in them (the size of the gap is determined by the kind of data e.g. price is 0.01). We would like to run Solr in parallel with ES and would appreciate extra generality on the bounds to make running them in parallel/migrating easier for us (and any other user migrating from Solr).

Cheers,
Shaun
</comment><comment author="clintongormley" created="2013-09-12T12:39:47Z" id="24314901">Hi Shaun

Personally I don't like the idea of adding this functionality as it adds unnecessary complexity to what should be a simple API.  It sounds like your application is working around a bad design in Solr, which is unfortunate, but not (in my opinion) a good reason to add this.

but like i say, this is my opinion, and its not up to me whether it gets added or not :)  I'll leave this issue open for others to comment on.

clint
</comment><comment author="shaunhall" created="2013-09-12T12:48:41Z" id="24315519">Agreed, it does add clutter, but:
- RangeFilterBuilder/Parser has this generality but RangeFacet doesn't. It's natural that they have the same structure so you can use faceting to generate the matching filter.
- Every user migrating from Solr and wanting to run ES in parallel to compare results will hit this.
</comment><comment author="clintongormley" created="2013-09-12T12:53:53Z" id="24315919">The RangeFilter is a different beast, as it may well be used with a single range, and making the bounds inclusive/exclusive depends entirely on the context.  Range facets on the other hand almost always use multiple contiguous ranges.

Actually, in the REST api for the range filter, we're intending to remove documentation for  `from`/`to` specifically because it is confusing and `to` has a different default setting in the filter and in the facet. So `from`/`to` should be used in the facet, and `gt`/`gte`/`lt`/`lte` should be used in the filter, because they are explicit.
</comment><comment author="clintongormley" created="2013-09-12T12:54:18Z" id="24315947">(again, leaving this to somebody else to make a decision on, just voicing my opinion)
</comment><comment author="shaunhall" created="2013-09-12T12:56:28Z" id="24316124">Ok, thanks Clint
</comment><comment author="jpountz" created="2013-09-12T13:29:14Z" id="24318733">Although the most common use-case is to have the lower bounds inclusive and the higher bounds exclusive, I can think of use-cases where the lower bound can be exclusive and the higher bound inclusive, for example package weights at the post office. So I think it could make sense to have options to specify whether the bounds are inclusive or exclusive, even though the defaults should remain as they are since they are what people expect most of time.
</comment><comment author="shaunhall" created="2013-09-12T13:52:20Z" id="24320792">Great, thanks Adrien. In the pull requests #3666 and #3665, the defaults are to behave as we do currently.
</comment><comment author="uboness" created="2013-09-16T12:09:54Z" id="24505264">@shaunhall we decide not to add any features to the current facets module (only fix critical bugs) and instead redirect our efforts to the new aggregation module coming in 1.0 (#3300) and consider adding it there. thus, closing this one
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Include ARMHF Version of Java in JDK_DIRS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3659</link><project id="" key="" /><description>We're working on deploying Elasticsearch in an arm environment and have noticed that the armhf version of openjdk-7 is not included in the JDK-DIRS for the Elasticsearch service. 

Now while Java should theoretically set its Java_Home, doing the default apt-get install openjdk-7-jre does not do this (on X86 or ARM) which is why I'm assuming the JDK_DIRs list was created. 

The path would be /usr/lib/jvm/java-7-openjdk-armhf/ and /usr/lib/jvm/java-6-openjdk-armhf/
</description><key id="19271204">3659</key><summary>Include ARMHF Version of Java in JDK_DIRS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">matthewarkin</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-10T16:47:28Z</created><updated>2013-09-16T08:05:14Z</updated><resolved>2013-09-16T07:57:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-13T07:31:33Z" id="24377876">just to be sure, you are talking about the debian init script, right?

Apart from that and out of curiousity: Any problems with running elasticsearch under ARM?
</comment><comment author="matthewarkin" created="2013-09-13T07:36:38Z" id="24378066">Yep the debian init script. I can get a pull request in in the morning with the two added dirs.

Besides the init script, we've had no problems using elasticsearch on arm, everything seems to work, we just install the debian, update the service with the right java path and presto. We've also been using head and the phonetic analysis plugin and those also work. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Debian init script: Added armhf openjdk6/7</comment></comments></commit><commit><files /><comments><comment>Debian init script: Added armhf openjdk6/7</comment></comments></commit></commits></item><item><title>Replace RestActions#splitXXX(String) methods with splitValues(String)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3658</link><project id="" key="" /><description>We have duplicated code in:
- RestActions#splitIndices(String)
- RestActions#splitTypes(String)
- RestActions#splitNodes(String)

The idea is to have one single RestActions#splitValues(String) method.
</description><key id="19261272">3658</key><summary>Replace RestActions#splitXXX(String) methods with splitValues(String)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-09-10T14:17:30Z</created><updated>2014-07-16T21:52:23Z</updated><resolved>2013-09-13T09:40:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-13T09:40:45Z" id="24383637">Closed by 7bde5af (0.90) and b27e7d3 (master)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Strings.java</file></files><comments><comment>Fix possible NPE due to #3658 change</comment></comments></commit></commits></item><item><title>Add clear scroll api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3657</link><project id="" key="" /><description>Add an api that allows to clear all the resources associated with a search scroll id. Example usage:

```
curl -XDELETE 'localhost:9200/_search/scroll/{scroll_id}'
```
</description><key id="19222097">3657</key><summary>Add clear scroll api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-09T20:51:29Z</created><updated>2013-09-10T19:23:05Z</updated><resolved>2013-09-10T19:23:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/search/ClearScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/ClearScrollRequest.java</file><file>src/main/java/org/elasticsearch/action/search/ClearScrollRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java</file><file>src/main/java/org/elasticsearch/action/search/TransportClearScrollAction.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestClearScrollAction.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/action/SearchServiceTransportAction.java</file><file>src/test/java/org/elasticsearch/test/integration/search/scroll/SearchScrollTests.java</file></files><comments><comment>Added clear scroll api.</comment></comments></commit></commits></item><item><title>key is "index.store.type", not "index.storage.type"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3656</link><project id="" key="" /><description>Bumped into this typo with test install.  Copied the documentation to use in-memory store but indices were still saved to disk.
</description><key id="19209479">3656</key><summary>key is "index.store.type", not "index.storage.type"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bfritz</reporter><labels /><created>2013-09-09T17:11:32Z</created><updated>2014-07-16T21:52:23Z</updated><resolved>2013-09-09T17:32:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Init.d-Script broken on Debian Squeeze</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3655</link><project id="" key="" /><description>i tried installing and updating ES 0.20.2 to 0.90.3 (or even a fresh install) on the latest Debian Squeeze. It fails to start elasticsearch with the following error:
`root@machine:~# service elasticsearch restart 
Starting ElasticSearch Server:start-stop-daemon: --start needs --exec or --startas
Try 'start-stop-daemon --help' for more information.
start-stop-daemon: --start needs --exec or --startas
Try 'start-stop-daemon --help' for more information.
 failed!`

By replacing the new init script with the older one from 0.20.2, everything continues to work like a charm
</description><key id="19207918">3655</key><summary>Init.d-Script broken on Debian Squeeze</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinseener</reporter><labels /><created>2013-09-09T16:43:04Z</created><updated>2013-09-10T12:56:52Z</updated><resolved>2013-09-10T12:56:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martinseener" created="2013-09-10T12:56:52Z" id="24157190">Duplicate and already fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for MockDirectoryWrapper in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3654</link><project id="" key="" /><description>The ultimate weapon against stuff like  #3652  is to use `MockDirectoryWrapper` we should use it in our tests by default.
</description><key id="19204434">3654</key><summary>Add support for MockDirectoryWrapper in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label></labels><created>2013-09-09T15:44:45Z</created><updated>2013-09-11T21:05:59Z</updated><resolved>2013-09-11T21:05:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/store/FilterDirectory.java</file><file>src/main/java/org/apache/lucene/store/RateLimitedFSDirectory.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/distributor/AbstractDistributor.java</file><file>src/main/java/org/elasticsearch/index/store/fs/FsDirectoryService.java</file><file>src/main/java/org/elasticsearch/index/store/fs/MmapFsIndexStore.java</file><file>src/main/java/org/elasticsearch/index/store/fs/NioFsIndexStore.java</file><file>src/main/java/org/elasticsearch/index/store/fs/SimpleFsIndexStore.java</file><file>src/main/java/org/elasticsearch/index/store/memory/ByteBufferDirectoryService.java</file><file>src/main/java/org/elasticsearch/index/store/ram/RamDirectoryService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/test/java/org/apache/lucene/util/AbstractRandomizedTest.java</file><file>src/test/java/org/elasticsearch/AbstractSharedClusterTest.java</file><file>src/test/java/org/elasticsearch/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/TestCluster.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockDirectoryHelper.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockFSDirectoryService.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockFSIndexStore.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockFSIndexStoreModule.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockRamDirecorySerivce.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockRamIndexStore.java</file><file>src/test/java/org/elasticsearch/index/store/mock/MockRamIndexStoreModule.java</file><file>src/test/java/org/elasticsearch/percolator/RecoveryPercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexTests.java</file><file>src/test/java/org/elasticsearch/search/facet/SimpleFacetsTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoFilterTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file><file>src/test/java/org/elasticsearch/search/scan/SearchScanScrollingTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/SearchScrollTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/SuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Add support for Lucene's MockDirectoryWrapper</comment></comments></commit></commits></item><item><title>Rename `IndexShard#searcher()` to `IndexShard#acquireSearcher()`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3653</link><project id="" key="" /><description>to prevent issues like in #3652  we should make sure the name tells that it needs to be released
</description><key id="19204349">3653</key><summary>Rename `IndexShard#searcher()` to `IndexShard#acquireSearcher()`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-09T15:43:22Z</created><updated>2013-11-13T07:07:08Z</updated><resolved>2013-09-09T19:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-09-09T15:44:07Z" id="24087766">+1
</comment><comment author="javanna" created="2013-09-09T15:49:12Z" id="24088326">Sounds good!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorService.java</file><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Rename IndexShard#searcher() to #acquireSearcher()</comment></comments></commit></commits></item><item><title>CompletionStats can cause resource leak since requested searchers are not closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3652</link><project id="" key="" /><description>I found this very sneaky problem while hunting a couple of other problems that I thought are related to potentially not closed index readers etc. I added support for MockDirectoryWrapper and the completion stats blew up immediately. While I might need more time to fully integrate the MockDirectoryWrapper from Lucene I want to fix this obvious bug first...
</description><key id="19201858">3652</key><summary>CompletionStats can cause resource leak since requested searchers are not closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-09T15:03:08Z</created><updated>2013-09-09T15:51:13Z</updated><resolved>2013-09-09T15:51:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-09T15:28:17Z" id="24086162">the completion stats  was using `searcher().reader()` but should have used `searcher.reader()` causing the seachers reference to be incremented but never decremented
</comment><comment author="drewr" created="2013-09-09T15:30:45Z" id="24086428">That's amazing! :metal:
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorService.java</file><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Rename IndexShard#searcher() to #acquireSearcher()</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Only pull searcher once during completion stats</comment></comments></commit></commits></item><item><title>enable GET /_template to show all templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3651</link><project id="" key="" /><description>PR for #2532

/_template shows:
No handler found for uri [/_template] and method [GET]

It would make sense to list the templates as they are listed in the /_cluster/state call.

See also:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wnGOnT-JTQo
</description><key id="19192419">3651</key><summary>enable GET /_template to show all templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-09-09T12:29:39Z</created><updated>2015-04-24T20:28:58Z</updated><resolved>2013-09-13T13:11:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-13T13:11:47Z" id="24393254">Closed by 0bd457d in 0.90 branch and ea4988e in master.
</comment><comment author="apennebaker-ni" created="2015-04-24T20:25:40Z" id="96054293">I get the same error response with es v0.90.3. Which patch version implements this fix?
</comment><comment author="dadoonet" created="2015-04-24T20:28:19Z" id="96055225">0.90.4
</comment><comment author="apennebaker-ni" created="2015-04-24T20:28:58Z" id="96055351">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch using huge amount of processes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3650</link><project id="" key="" /><description>Hello,

we send logfiles (catalina.out) with logstash (version 1.1.13-flatjar) using multiline{} to our elasticsearch server (version 0.90.2).

After several hours elasticsearch crashes because of not having enough memory, more precisely, the limit of 10240 open processes for the elasticsearch user has been reached. No more threads can be created.

The actual setting of ulimit is as follows:

elasticsearch soft nofile 65535
elasticsearch hard nofile 65535
elasticsearch soft nproc 10240
elasticsearch hard nproc 10240

I don't have any clue at the moment, how we can fix this issues. Maybe someone can help.

Regards
Oliver
</description><key id="19186773">3650</key><summary>Elasticsearch using huge amount of processes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tw1nh3ad</reporter><labels /><created>2013-09-09T10:24:47Z</created><updated>2013-09-11T12:23:28Z</updated><resolved>2013-09-11T12:23:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-09T16:12:54Z" id="24090562">Hi @tw1nh3ad,
may I ask you to send this message to our [users mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch), unless you think this is a bug? Have a look [here](http://www.elasticsearch.org/help/) as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date histogram results returned out of order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3649</link><project id="" key="" /><description>Using 0.90, a recent commit has made the histogram results to return in a mixed order, where previously they were always returned in an ascending order. While upgrading to the latest 0.90 our acceptance tests failed because we were relying on the order of the output. By order I obviously mean the time, not counts.

We see this with a daily histogram, using the default transport (haven't checked with REST).

There is no guarantee in the docs about the histogram order, but they probably should say something about that. And, if the results are supposed to be ordered, then this is probably a bug worth fixing before a release...

Another thing we noticed (but that's not new) is that date points with zero values are omitted. Again, this wasn't obvious and should either be fixed (so buckets with zero values will still appear in results) or say so in the docs.
</description><key id="19181805">3649</key><summary>Date histogram results returned out of order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-09-09T08:19:40Z</created><updated>2014-08-08T17:05:28Z</updated><resolved>2014-08-08T17:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T17:05:28Z" id="51629962">Hi @synhershko 

I'm closing this because facets are deprecated and, as far as I know, this works correctly in aggs.  If you're seeing something different in aggs, please open a new issue

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CompletionSuggest should throw an exception if input string contains a reserved character.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3648</link><project id="" key="" /><description>Currently the CompletionSuggester reserves 0x00 and 0xFF for internal use. We should throw an exception if those characters are used in an input string.
</description><key id="19165766">3648</key><summary>CompletionSuggest should throw an exception if input string contains a reserved character.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-08T19:36:13Z</created><updated>2013-09-09T10:13:13Z</updated><resolved>2013-09-09T10:00:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-08T20:14:22Z" id="24028443">I just wanted to make sure this isn't a problem with UTF8/16 characters. 
</comment><comment author="s1monw" created="2013-09-09T09:58:48Z" id="24059101">@nik9000 not sure if I understand your comment?
</comment><comment author="nik9000" created="2013-09-09T10:11:57Z" id="24059998">I'm not up on exactly how UTF8 an UTF16 represent all the characters but I just wanted to make sure that if one of the bytes in one of the code points is 0x00 or 0xFF the suggester doesn't reject it. 
</comment><comment author="s1monw" created="2013-09-09T10:13:12Z" id="24060096">see my comment - `0xFF` is actually not valid and `0x00` can only be a single byte char so it will also be the `0` UTF-16 CP http://en.wikipedia.org/wiki/UTF-8
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Throw IAE if reserved completion suggester chars are used in input</comment></comments></commit></commits></item><item><title>Clients crashing with Bad file descriptor / Cannot assign requested address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3647</link><project id="" key="" /><description>When doing a lot of small/fast requests (in this case head requests to document IDs, most of them resulting in a hit), Elasicsearch seems to crash in a way that makes all other programs die for a few seconds. After that, connections can seemingly be reestablished.
Seeing as in this case, running a ruby script will kill a python script running in the background, I'd assume it's not a problem in the client implementations

```
# cat /etc/elasticsearch/elasticsearch.yml 
network.host: 127.0.0.1
cluster:
  name: some-thing
path:
  logs: /var/log/elasticsearch
  data: /home/somthing/elasticsearch
index:
  number_of_shards: 1
  number_of_replicas: 0
  refresh_interval: 20s
  cache:
    field:
      type: soft
boostrap:
  mlockall: true
indices:
  fielddata:
    cache:
      size: 30%
```

```
VM name: Java HotSpot(TM) 64-Bit Server VM
VM vendor: Oracle Corporation
VM version: 23.25-b01
Uptime: 189 hours, 28 minutes, 4 seconds and 77 milliseconds
Java version: 1.7.0_25
```

```
HTTP &amp; Transport
HTTP address: inet[/127.0.0.1:9200]
Bound address: inet[/127.0.0.1:9200]
Publish address: inet[/127.0.0.1:9200]
Transport address: inet[/127.0.0.1:9300]
Bound address: inet[/127.0.0.1:9300]
Publish address: inet[/127.0.0.1:9300]
```

```
ES version: 0.90.3
```

These are the limits of ES:

```
# cat /proc/18426/limits 
Limit                     Soft Limit           Hard Limit           Units     
Max cpu time              unlimited            unlimited            seconds   
Max file size             unlimited            unlimited            bytes     
Max data size             unlimited            unlimited            bytes     
Max stack size            8388608              unlimited            bytes     
Max core file size        0                    unlimited            bytes     
Max resident set          unlimited            unlimited            bytes     
Max processes             256606               256606               processes 
Max open files            65535                65535                files     
Max locked memory         65536                65536                bytes     
Max address space         unlimited            unlimited            bytes     
Max file locks            unlimited            unlimited            locks     
Max pending signals       256606               256606               signals   
Max msgqueue size         819200               819200               bytes     
Max nice priority         0                    0                    
Max realtime priority     0                    0                    
Max realtime timeout      unlimited            unlimited            us        
```

This is the stacktrace from stretcher (ruby client):

```
Faraday::Error::ConnectionFailed: Bad file descriptor - Bad file descriptor (Errno::EBADF)
org/jruby/RubyIO.java:2025 • close
gems/excon-0.25.3/lib/excon/socket.rb:192 • connect
gems/excon-0.25.3/lib/excon/socket.rb:174 • connect
org/jruby/RubyArray.java:1617 • each
gems/excon-0.25.3/lib/excon/socket.rb:152 • connect
gems/excon-0.25.3/lib/excon/socket.rb:32 • initialize
gems/excon-0.25.3/lib/excon/connection.rb:363 • socket
gems/excon-0.25.3/lib/excon/connection.rb:105 • request_call
gems/excon-0.25.3/lib/excon/middlewares/mock.rb:42 • request_call
gems/excon-0.25.3/lib/excon/middlewares/instrumentor.rb:22 • request_call
gems/excon-0.25.3/lib/excon/middlewares/base.rb:15 • request_call
gems/excon-0.25.3/lib/excon/middlewares/base.rb:15 • request_call
gems/excon-0.25.3/lib/excon/middlewares/base.rb:15 • request_call
gems/excon-0.25.3/lib/excon/connection.rb:244 • request
gems/faraday-0.8.8/lib/faraday/adapter/excon.rb:45 • call
gems/faraday_middleware-0.9.0/lib/faraday_middleware/request/encode_json.rb:23 • call
gems/faraday_middleware-0.9.0/lib/faraday_middleware/response_middleware.rb:30 • call
gems/faraday-0.8.8/lib/faraday/response.rb:8 • call
gems/stretcher-1.19.0/lib/stretcher/server.rb:219 • request
org/jruby/ext/thread/Mutex.java:149 • synchronize
gems/stretcher-1.19.0/lib/stretcher/server.rb:217 • request
gems/stretcher-1.19.0/lib/stretcher/server.rb:155 • mget
gems/stretcher-1.19.0/lib/stretcher/index.rb:74 • mget
```

This is what esclient will look like:

```
Traceback (most recent call last):
  File "/usr/local/bin/esdump", line 71, in &lt;module&gt;
    scrollres = es.scroll(scroll_id)
  File "/usr/local/lib/python2.7/dist-packages/esclient.py", line 251, in scroll
    query_string_args=query_string_args, encode_json=False)
  File "/usr/local/lib/python2.7/dist-packages/esclient.py", line 122, in send_request
    self.last_response = requests.request(method.lower(), url, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/requests/api.py", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/requests/sessions.py", line 335, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/local/lib/python2.7/dist-packages/requests/sessions.py", line 438, in send
    r = adapter.send(request, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/requests/adapters.py", line 327, in send
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=9200): Max retries exceeded with url: /_search/scroll?scroll=10m (Caused by &lt;class 'socket.error'&gt;: [Errno 99] Cannot assign requested address)
```
</description><key id="19159215">3647</key><summary>Clients crashing with Bad file descriptor / Cannot assign requested address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rb2k</reporter><labels /><created>2013-09-08T11:07:30Z</created><updated>2013-10-30T12:06:42Z</updated><resolved>2013-10-30T12:06:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rb2k" created="2013-09-08T11:18:44Z" id="24019081">Maybe related to https://github.com/elasticsearch/elasticsearch/issues/3621 ?
</comment><comment author="rb2k" created="2013-09-09T11:38:25Z" id="24065989">Note: Inserting a sleep(10) every 1000 calls "fixes" the problem
</comment><comment author="spinscale" created="2013-09-12T14:39:13Z" id="24325253">just a rough guess: Are you creating a new TCP/HTTP connection each time (and thus run out of socket resources)? Or not closing them appropriately... though I would suspect that the requests package in python takes care of that. Maybe check with `netstat` what is going on regarding open TCP connections and if there are too much.
</comment><comment author="rb2k" created="2013-09-12T14:47:03Z" id="24326096">The clients should both be using HTTP keepalive and reuse the existing connection
</comment><comment author="spinscale" created="2013-09-12T15:11:01Z" id="24328542">Is `lsof` providing any insight maybe, when you run into this? Would be interested in more data to reduce the issue root a bit.
</comment><comment author="kimchy" created="2013-09-14T22:14:29Z" id="24460306">can you maybe also check if the total_opened in the http stats gets incremented dramatically during the test? It might cause the OS to start to throttle the creation of sockets
</comment><comment author="spinscale" created="2013-10-30T12:06:42Z" id="27383523">closing due to lack of feedback. Happy to reopen with more information!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Standard Deviation in Date Histogram Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3646</link><project id="" key="" /><description>I'd like to see support for date histogram facet also returning standard deviation like the statistics facet. Right now, I'm using date histogram to pull out the mean, then using the buckets to pull out standard deviation for the same buckets.

I'd also like to see median but from what I understand, that's much harder to implement.
</description><key id="19138788">3646</key><summary>Standard Deviation in Date Histogram Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chendo</reporter><labels /><created>2013-09-07T06:01:03Z</created><updated>2013-09-09T03:57:51Z</updated><resolved>2013-09-07T20:29:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-09-07T20:29:48Z" id="24009079">You'll have this supported in the new aggregations module #3300. (we don't invest time in enhancing the current facets... a lot of missing features in facets will be featured by the aggregations)

As for median (or percentiles in general), we plan to have it supported by the aggregations as well at some point, though it'll likely be an approximation rather than an exact calculation
</comment><comment author="chendo" created="2013-09-09T03:57:50Z" id="24038503">Ah cool, will be looking forward to that.

Approximation should be good enough assuming it's close enough. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Big number support for statistical facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3645</link><project id="" key="" /><description>I frequently run into issues trying to run statistical facets on my data sets. When trying to total a few hours worth of data we can easily go over the 64bit long value.  It would make things much easier if ES implemented http://docs.oracle.com/javase/6/docs/api/java/math/BigInteger.html or something similar. Which would allow people to sum large amounts of data.
</description><key id="19137790">3645</key><summary>Big number support for statistical facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">vrecan</reporter><labels /><created>2013-09-07T03:51:59Z</created><updated>2014-07-04T09:41:09Z</updated><resolved>2014-07-04T09:41:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-09-09T08:49:06Z" id="24054519">Thanks for reporting this issue. I was just looking at the term stats facets which seems to use doubles for computing the sum so this shouldn't overflow. Can you give more details on the facet query that you are running so that I can try to find where the overflow happens?
</comment><comment author="jpountz" created="2014-07-04T09:41:09Z" id="48025684">Closing as facets have been deprecated: https://github.com/elasticsearch/elasticsearch/pull/6485 Please open a new issue if you have a similar problem on aggregations.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove matched_filters in favor for matched_queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3644</link><project id="" key="" /><description>Because of #3581 `matchedFilters` response part can now also contain named queries as results, the name `matchedFilters` only suggests that named filters may up end as result. Changing the name to `matchedQueries` will suggest that named elements from the whole query dsl can be matched (both named queries and filters).
</description><key id="19123883">3644</key><summary>Remove matched_filters in favor for matched_queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-09-06T19:32:30Z</created><updated>2014-07-23T15:29:11Z</updated><resolved>2013-09-06T19:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rslinckx" created="2014-07-23T13:33:09Z" id="49873490">The doc at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html#search-request-named-queries-and-filters still uses matched_filters as the destination attribute, which isn't correct anymore...
</comment><comment author="clintongormley" created="2014-07-23T15:29:11Z" id="49890201">thanks @rslinckx - fixed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/SearchHit.java</file><file>src/main/java/org/elasticsearch/search/SearchModule.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>src/test/java/org/elasticsearch/test/integration/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Removed matched_filters in favor for matched_queries.</comment></comments></commit></commits></item><item><title>Allow to control the number of processors sizes are based on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3643</link><project id="" key="" /><description>Sometimes, one wants to just control the number of processors our different size base calculations for thread pools and network workers are based on, and not use the reported available processor by the OS. Add `processors` setting, where it can be controlled.
</description><key id="19116422">3643</key><summary>Allow to control the number of processors sizes are based on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-06T16:56:56Z</created><updated>2013-09-06T17:04:21Z</updated><resolved>2013-09-06T17:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Allow to control the number of processors sizes are based on</comment><comment>Sometimes, one wants to just control the number of processors our different size base calculations for thread pools and network workers are based on, and not use the reported available processor by the OS. Add processors setting, where it can be controlled.</comment></comments></commit></commits></item><item><title>Plugin Manager should support -remove group/artifact/version naming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3642</link><project id="" key="" /><description>When installing a plugin, we use:

``` sh
bin/plugin --install groupid/artifactid/version
```

But when removing the plugin, we only support:

``` sh
bin/plugin --remove dirname
```

where `dirname` is the directory name of the plugin under `/plugins` dir.

Closes #3421.
</description><key id="19107380">3642</key><summary>Plugin Manager should support -remove group/artifact/version naming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-09-06T14:11:58Z</created><updated>2014-07-01T22:31:49Z</updated><resolved>2013-09-09T19:18:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-06T14:27:12Z" id="23943440">Looks good to me!
</comment><comment author="dadoonet" created="2013-09-09T08:34:16Z" id="24053586">@uboness I modified the PR to take your comments into account. It helped a lot to simplify things. Available URLs are now much more readable and we will be able to easily add new ones: https://github.com/dadoonet/elasticsearch/blob/83943840725ff02eb7946f982b41a683c1db455a/src/main/java/org/elasticsearch/plugins/PluginManager.java#L424

Could you tell me if it's now ok to push it in 0.90?

@javanna Do you mind read the PR again as some things have changed? Thanks!
</comment><comment author="javanna" created="2013-09-09T16:24:06Z" id="24091664">Indeed, quite some things have changed now, but nice refactoring...looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugin Manager: add silent mode.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3641</link><project id="" key="" /><description>Now with have proper exit codes for elasticsearch plugin manager (see #3463), we can add a silent mode to plugin manager.

``` sh
bin/plugin --install karmi/elasticsearch-paramedic --silent
```

Closes #3628.

When OK, I will push it to master as well.
</description><key id="19106439">3641</key><summary>Plugin Manager: add silent mode.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-09-06T13:53:13Z</created><updated>2014-06-21T22:20:55Z</updated><resolved>2013-09-10T16:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-10T08:07:38Z" id="24140748">Looks good to me!
</comment><comment author="dadoonet" created="2013-09-10T16:35:20Z" id="24175328">Merged in master with https://github.com/elasticsearch/elasticsearch/commit/fafc4eef98fba59b36029a6970abd35f5fdc8dbf and in 0.90 with https://github.com/elasticsearch/elasticsearch/commit/eab5547dc394ba021c17b0bf0365c1131dc970d1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix potential blocking of NettyTransport connect and disconnect methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3640</link><project id="" key="" /><description>Currently, in NettyTransport the locks for connecting and disconnecting channels are stored in a ConcurrentMap that has 500 entries. A tread can acquire a lock for an id and the lock returned is chosen on a hash function computed from the id.
Unfortunately, a collision of two ids can cause a deadlock as follows:

**Scenario**: one master (no data), one datanode (only data node)

DiscoveryNode id of master is X
DiscoveryNode id of datanode is Y

Both X and Y cause the same lock to be returned by NettyTransport#connectLock()

Both are up and running, all is fine until master stops.

**Thread 1**: The master fault detection of the datanode is notified (onNodeDisconnected()), which in turn leads the node to try and reconnect to master via the callstack titled "Thread 1" below.

-&gt; connectToNode() is called and lock for X is acquired. The method waits for 45s for the cannels to reconnect.

Furthermore, Thread 1 holds the NettyTransport#masterNodeMutex.

**Thread 2**: The connection fails with an exception (connection refused, see callstack below), because the master shut down already. The exception is handled in NettyTransport#exceptionCaught which calls NettyTransport#disconnectFromNodeChannel. This method acquires the lock for Y (see Thread 2 below).

Now, if Y and X have two different locks, this would get the lock, disconnect the channels and notify thread 1. But since X and Y have the same locks, thread 2 is waiting for the lock thread 1 holds. thread 1 waits for thread 2 and then times out.

In this time, no thread can acquire the masterNodeMutex (held by thread 1), so the node can, for example, not stop.

This commit introduces a mechanism that assures unique locks for unique ids. This lock is not reentrant and therfore assures that threads can not end up in an infinite recursion (see Thread 3 below for an example on how a thread can aquire a lock twice).
While this is not a problem right now, it is potentially dangerous to have it that way, because the callstacks are complex as is and slight changes might cause unecpected recursions.
## Thread 1

```
owns: Object  (id=114)
owns: Object  (id=118)
waiting for: DefaultChannelFuture  (id=140)
Object.wait(long) line: not available [native method]
DefaultChannelFuture(Object).wait(long, int) line: 461
DefaultChannelFuture.await0(long, boolean) line: 311
DefaultChannelFuture.awaitUninterruptibly(long) line: 285
NettyTransport.connectToChannels(NettyTransport$NodeChannels, DiscoveryNode) line: 672
NettyTransport.connectToNode(DiscoveryNode, boolean) line: 609
NettyTransport.connectToNode(DiscoveryNode) line: 579
TransportService.connectToNode(DiscoveryNode) line: 129
MasterFaultDetection.handleTransportDisconnect(DiscoveryNode) line: 195
MasterFaultDetection.access$0(MasterFaultDetection, DiscoveryNode) line: 188
MasterFaultDetection$FDConnectionListener.onNodeDisconnected(DiscoveryNode) line: 245
TransportService$Adapter$2.run() line: 298
EsThreadPoolExecutor(ThreadPoolExecutor).runWorker(ThreadPoolExecutor$Worker) line: 1145
ThreadPoolExecutor$Worker.run() line: 615
Thread.run() line: 724
```
## Thread 2

```
waiting for: Object  (id=114)
NettyTransport.disconnectFromNodeChannel(Channel, Throwable) line: 790
NettyTransport.exceptionCaught(ChannelHandlerContext, ExceptionEvent) line: 495
MessageChannelHandler.exceptionCaught(ChannelHandlerContext, ExceptionEvent) line: 228
MessageChannelHandler(SimpleChannelUpstreamHandler).handleUpstream(ChannelHandlerContext, ChannelEvent) line: 112
DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline$DefaultChannelHandlerContext, ChannelEvent) line: 564
DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(ChannelEvent) line: 791
SizeHeaderFrameDecoder(FrameDecoder).exceptionCaught(ChannelHandlerContext, ExceptionEvent) line: 377
SizeHeaderFrameDecoder(SimpleChannelUpstreamHandler).handleUpstream(ChannelHandlerContext, ChannelEvent) line: 112
DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline$DefaultChannelHandlerContext, ChannelEvent) line: 564
DefaultChannelPipeline.sendUpstream(ChannelEvent) line: 559
Channels.fireExceptionCaught(Channel, Throwable) line: 525
NioClientBoss.processSelectedKeys(Set&lt;SelectionKey&gt;) line: 110
NioClientBoss.process(Selector) line: 79
NioClientBoss(AbstractNioSelector).run() line: 312
NioClientBoss.run() line: 42
ThreadRenamingRunnable.run() line: 108
DeadLockProofWorker$1.run() line: 42
ThreadPoolExecutor.runWorker(ThreadPoolExecutor$Worker) line: 1145
ThreadPoolExecutor$Worker.run() line: 615
Thread.run() line: 724
```
## Thread 3

```
    org.elasticsearch.transport.netty.NettyTransport.disconnectFromNode(NettyTransport.java:772)
    org.elasticsearch.transport.netty.NettyTransport.access$1200(NettyTransport.java:92)
    org.elasticsearch.transport.netty.NettyTransport$ChannelCloseListener.operationComplete(NettyTransport.java:830)
    org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427)
    org.jboss.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:418)
    org.jboss.netty.channel.DefaultChannelFuture.setSuccess(DefaultChannelFuture.java:362)
    org.jboss.netty.channel.AbstractChannel$ChannelCloseFuture.setClosed(AbstractChannel.java:355)
    org.jboss.netty.channel.AbstractChannel.setClosed(AbstractChannel.java:185)
    org.jboss.netty.channel.socket.nio.AbstractNioChannel.setClosed(AbstractNioChannel.java:197)
    org.jboss.netty.channel.socket.nio.NioSocketChannel.setClosed(NioSocketChannel.java:84)
    org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:357)
    org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:58)
    org.jboss.netty.channel.Channels.close(Channels.java:812)
    org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
    org.elasticsearch.transport.netty.NettyTransport$NodeChannels.closeChannelsAndWait(NettyTransport.java:892)
    org.elasticsearch.transport.netty.NettyTransport$NodeChannels.close(NettyTransport.java:879)
    org.elasticsearch.transport.netty.NettyTransport.disconnectFromNode(NettyTransport.java:778)
    org.elasticsearch.transport.netty.NettyTransport.access$1200(NettyTransport.java:92)
    org.elasticsearch.transport.netty.NettyTransport$ChannelCloseListener.operationComplete(NettyTransport.java:830)
    org.jboss.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427)
    org.jboss.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:418)
    org.jboss.netty.channel.DefaultChannelFuture.setSuccess(DefaultChannelFuture.java:362)
    org.jboss.netty.channel.AbstractChannel$ChannelCloseFuture.setClosed(AbstractChannel.java:355)
    org.jboss.netty.channel.AbstractChannel.setClosed(AbstractChannel.java:185)
    org.jboss.netty.channel.socket.nio.AbstractNioChannel.setClosed(AbstractNioChannel.java:197)
    org.jboss.netty.channel.socket.nio.NioSocketChannel.setClosed(NioSocketChannel.java:84)
    org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:357)
    org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
    org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
    org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
    org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    java.lang.Thread.run(Thread.java:724)
```
</description><key id="19099017">3640</key><summary>fix potential blocking of NettyTransport connect and disconnect methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-09-06T10:41:18Z</created><updated>2014-07-16T21:52:25Z</updated><resolved>2013-09-06T16:05:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Huge GC load with populated field cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3639</link><project id="" key="" /><description>This is followup for [my email](https://groups.google.com/forum/#!searchin/elasticsearch/CPU$20consumption$20after$20long$20period$20of$20time/elasticsearch/_ihDrmVExLA/HaOgH3SE2RsJ).

Here you may see what happens if you fill the whole heap with field cache (big query with faceting for 50gb of data). Indexing happens every 10 minutes and with full cache it becomes very painful because of GC.

![cache full](http://puu.sh/4kios.png)

And this is what happens after `/_cache/clear`:

![cache clear](http://puu.sh/4kiri.png)

We run elasticsearch 0.90.2 like that:

`/usr/lib/jvm/oracle-jdk-bin-1.7/bin/java -Xms5g -Xmx5g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.foreground=yes -Des.path.home=/opt/elasticsearch -cp :/opt/elasticsearch/lib/elasticsearch-0.90.2.jar:/opt/elasticsearch/lib/*:/opt/elasticsearch/lib/sigar/* -Des.config=/etc/elasticsearch/elasticsearch.yml org.elasticsearch.bootstrap.ElasticSearch`

Settings in yml file only affect data location and network settings.

Is this expected behaviour? Elasticsearch becomes very slow even for simple "get doc by id" (like 10-20 seconds instead of 10-500ms).
</description><key id="19091522">3639</key><summary>Huge GC load with populated field cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bobrik</reporter><labels /><created>2013-09-06T07:15:02Z</created><updated>2016-10-12T16:16:52Z</updated><resolved>2013-09-06T10:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-09-06T10:05:44Z" id="23930369">I guess this happens because field data almost fills up the entire heap so the JVM has to run frequent major (stop-the-world) collections to try to recover free space. Because major collections may stop everything, even simple operations such as gets can become slow.

Can you try allocating more memory to the JVM to see if that solves the problem? It would also be interesting to have a look at the [node stats API](http://www.elasticsearch.org/guide/reference/api/admin-cluster-nodes-stats/) to confirm there is an abnormal number of major collections when field data is loaded into memory with the current amount of memory you allocate to the JVM.
</comment><comment author="bobrik" created="2013-09-06T10:32:02Z" id="23931468">Machines have 8g of memory and 5g are dedicated for jvm heap. Allocating extra gigabyte could hurt page cache.

In my email there was picture from bigdesk with GC graph. Heap usage drop happened after manual cache cleanup.

![gc](http://puu.sh/4hoMK.png)

I can load up field cache and give you `/_nodes/stats` if you still need it.
</comment><comment author="jpountz" created="2013-09-06T10:51:08Z" id="23932233">This confirms that memory pressure gives too much work to the garbage collector. Future versions of Elasticsearch may allow to store field data on disk to help with this kind of issues but for now there is not much that can be done beyond having servers with more memory (or more servers so that each server ends up handling fewer shards) so that field data doesn't put so much pressure on garbage collection.
</comment><comment author="bobrik" created="2013-09-06T12:39:20Z" id="23936878">Whoa. Let me repeat some of the points I've got from this.
1. After querying large dataset elasticsearch becomes very slow and probably **unresponsible because of cache**. Seems like this cache is more important than the whole cluster, because `/_cluster/health` hangs and I have to `killall -9 java` on every node to recover cluster.
2. After querying large dataset with limited `indices.fielddata.cache.size` (say 2gb) elasticsearch consume cpu for garbage collection even there's no indexing or searching. This is expected "pressure".
3. All of the above isn't really a bug.

Really? Seems like elasticsearch is in-memory database after all. Unfortunately @kimchy didn't say anything about it in his presentations.

Setting `indices.fielddata.cache.expire` to `1m` actually stops extra GC if there's no indexing and search, but by default elasticsearch dies from heavy faceting.

Here is node with expire (2.5gb field cache):

![expire](http://puu.sh/4kp3G.png)

Here is node without expire (2.5gb field cache):

![no expire](http://puu.sh/4kp3M.png)

Could you please take a second look at this? I still don't get the source of "pressure".
</comment><comment author="nik9000" created="2013-09-06T13:21:07Z" id="23939092">The "pressure" @jpountz refers to is garbage collection pressure, essentially the heap is getting fuller and fuller and each GC is taking longer and longer and more and more GCs are required because there isn't as much free memory to create objects.  It is a vicious cycle.  Eventually the "pressure" is so great that Java won't respond to a regular kill signal and you have to `kill -9` it.  This is annoying but sort of something you live with for java.  One of the most important things to monitor in a java process is the % of time it spends garbage collecting.  If you know all this just consider it useful in case someone else finds this issue later.

Anyway, when you didn't have a limit in the fielddata cache the JVM was spending a ridiculous amount of time on garbage collection.  It looks a lot better when you set the limit.  Was it performing OK?

The question is, should Elasticsearch default to setting `indices.fielddata.cache.size` to, say, 30% of ram to prevent people who haven't set it from filling up memory?
</comment><comment author="jpountz" created="2013-09-06T13:29:17Z" id="23939622">Up to Lucene 4.0, Lucene didn't have column-stride fields (called doc-values in the Lucene API), which are required to perform things such as sorting or faceting. The common workaround is to "uninvert" a field from the inverted index in order to get back the mapping from document IDs to field values (the inverted index stores the mapping from field values to the list of documents that contain it). The problem is that this happens in memory and can be very memory-intensive, especially on dense and multivalued fields.

Since Lucene 4.0, this mapping from documents to field values can be computed at indexing time and written to disk. There are plans to integrate this feature into future versions of Elasticsearch but unfortunately for now field data can only be stored in memory.

On your "Heap Mem" graph, "Used" keeps being very close to "Committed". This means that the garbage collector don't find unused objects to trash. On a healthy cluster, "Used" would either stay far from "Committed" all the time, or get closer and closer as time passes, then Java would trigger a major collection, and "Used" would decrease instantly to a much lower value until the accumulation of object creations keeps increasing it again close to "Committed" in cycles of several hours.
</comment><comment author="jpountz" created="2013-09-06T13:37:06Z" id="23940096">&gt; The question is, should Elasticsearch default to setting indices.fielddata.cache.size to, say, 30% of ram to prevent people who haven't set it from filling up memory?

Unfortunately, this could easily create other problems. If this setting makes the fielddata cache only a bit too small to be able to cache field data for all segments, this will be ok, but otherwise, Elasticsearch will have to regenerate the field data objects for several segments at every request, and this process can be quite time-consuming. This is a tricky issue...

This is less a problem with the filter cache since cached filters are fast to regenerate.
</comment><comment author="bobrik" created="2013-09-06T13:40:44Z" id="23940332">@nik9000 it's not dying not, but extra gc takes place. There are like 1gb of free heap and still gc takes place on node with no expire for field data cache. I don't see any reason for so much garbage collection if there's **no searching or indexing happens**.

I don't mind if field cache would eat all the memory if it could be evicted easily where memory is needed. It seems that elasticsearch or java tradeoffs cpu and latency in favour of cache. Maybe for some cases this is okay, but it could bring down the whole cluster if there's no limit for cache. With limited cache this thing eats extra cpu for no visible reason. I think something is wrong with field cache eviction.

I could be wrong, but I don't see other explanation for now.

@jpountz why cache could't be freed if we "used" gets closer to "committed"? Dropping some caches seems like a good decision in this situation.

Here's another picture, "used" is far from "committed", but gc is trying to do something all the time.

![another pic](http://puu.sh/4kqJN.png)

Here's another node with field cache expiration time set to 1m:

![yet another pic](http://puu.sh/4kqNy.png)

Look at gc timers! The only difference between nodes is indices.fielddata.cache.expire.
</comment><comment author="nik9000" created="2013-09-06T14:00:25Z" id="23941650">&gt; Unfortunately, this could easily create other problems. If this setting makes the fielddata cache only a bit too small to be able to cache field data for all segments, this will be ok, but otherwise, Elasticsearch will have to regenerate the field data objects for several segments at every request, and this process can be quite time-consuming. This is a tricky issue...

@jpountz: Yeah!  I suppose the problem is that everyone's workload is different and for some people that cache should be 80% of memory instead of 30% and leaving it unbounded might just work for them.  OTOH I'd prefer to have certain operations be slow (even too slow to use) then to have those operations cause the whole app to become unresponsive.

&gt; Look at gc timers! The only difference between nodes is indices.fielddata.cache.expire.

@bobrik: looks like the expiration is the right thing for you.  The reason it is eating CPU when memory is more full is that you are further down the pressure curve I described up top.  Barring spending a lot of time on configuration, java is just like that - the more full memory is the more CPU time it spends trying to clean it up.  It is a lot more complicated than that but I don't remember enough of it to explain it in much better terms.
</comment><comment author="jpountz" created="2013-09-06T14:08:33Z" id="23942185">&gt; @jpountz why cache could't be freed if we "used" gets closer to "committed"? Dropping some caches seems like a good decision in this situation.

There is a way to do that, by setting `index.cache.field.type` to `soft`. But  I usually don't recommend using it for the reasons I gave in my previous comment.

&gt; Look at gc timers! The only difference between nodes is indices.fielddata.cache.expire

I don't know enough about the JVM internals to answer but the JVM likes ensuring that it has lots of memory to be able to handle sudden increases of load. So I wouldn't be surprised that it starts worrying if it cannot free more than 20% of the heap space.
</comment><comment author="jpountz" created="2013-09-06T14:15:37Z" id="23942673">&gt; @jpountz: Yeah! I suppose the problem is that everyone's workload is different and for some people that cache should be 80% of memory instead of 30% and leaving it unbounded might just work for them.

Exactly.

&gt; OTOH I'd prefer to have certain operations be slow (even too slow to use) then to have those operations cause the whole app to become unresponsive.

If you know in advance how much memory your field data cache is going to use, I agree this can be handy.
</comment><comment author="bobrik" created="2013-09-06T14:21:51Z" id="23943075">&gt; There is a way to do that, by setting index.cache.field.type to soft. But I usually don't recommend using it for the reasons I gave in my previous comment.

Is index.cache.field.type still available in 0.90.2? Is there cluster-wide setting for that? I understood that by reasons you mean cost of rebuilding of field cache.

&gt; So I wouldn't be surprised that it starts worrying if it cannot free more than 20% of the heap space.

20% is more like `-client` than `-server`.

Here's how I see situation with field cache with default settings:
1. Cache is actually not cache, because references are "hard", not "soft".
2. There's no automatic cache eviction if memory usage gets high.
3. If you try to use faceting with default settings on huge datasets, you would probably kill your cluster very soon.

Am I right about that? If so, this is quite dangerous thing and should be at least noticed in default config file. Moreover, I'd better pick some default value for max field cache size (like 50%), because sane defaults couldn't hurt. If you understand elasticsearch well, you will increase value accordingly.
</comment><comment author="RuralHunter" created="2016-05-04T05:28:36Z" id="216748987">No update for this?
</comment><comment author="jpountz" created="2016-05-04T07:50:54Z" id="216769476">@RuralHunter this issue has become mostly irrelevant now that elasticsearch has doc values enabled by default
</comment><comment author="crispygoth" created="2016-10-12T16:16:52Z" id="253261440">This issue still manages to hit our ELK stack occasionally if we give it a particularly complicated query, so I would say it is still relevant. Is there anything I can do to make this not happen in future?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>BytesStreamOutput default size should be 2k instead of 32k</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3638</link><project id="" key="" /><description>We changed the default of BytesStreamOutput (used in various places in ES) to `32k` from `1k` with the assumption that most stream tend to be large. This doesn't hold for example when indexing small documents and adding them using `XContentBuilder` (which will have a large overhead).

Default the buffer size to `2k` now, but be relatively aggressive in expanding the buffer when below 256k (double it), and just use oversize (1/8th) when larger to try and minimize garbage and buffer copies.

relates to #3624 
</description><key id="19082562">3638</key><summary>BytesStreamOutput default size should be 2k instead of 32k</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-06T00:24:45Z</created><updated>2013-09-16T18:41:10Z</updated><resolved>2013-09-06T00:25:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java</file><file>src/test/java/org/elasticsearch/test/unit/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>BytesStreamOutput default size should be 2k instead of 32k</comment><comment>We changed the default of BytesStreamOutput (used in various places in ES) to 32k from 1k with the assumption that most stream tend to be large. This doesn't hold for example when indexing small documents and adding them using XContentBuilder (which will have a large overhead).</comment></comments></commit></commits></item><item><title>Add AllocationDecider that takes free disk space into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3637</link><project id="" key="" /><description>This commit adds two main pieces, the first is a ClusterInfoService
that provides a service running on the master nodes that fetches the
total/free bytes for each data node in the cluster as well as the
sizes of all shards in the cluster. This information is gathered by
default every 30 seconds, and can be changed dynamically by setting
the `cluster.info.update.frequency` setting. This ClusterInfoService
can hopefully be used in the future to weight nodes for allocation
based on their disk usage, if desired.

The second main piece is the DiskThresholdDecider, which can disallow
a shard from being allocated to a node, or from remaining on the node
depending on configuration parameters. There are three main
configuration parameters for the DiskThresholdDecider:

`cluster.routing.allocation.disk.threshold_enabled` controls whether
the decider is enabled. It defaults to false (disabled). Note that the
decider is also disabled for clusters with only a single data node.

`cluster.routing.allocation.disk.watermark.low` controls the low
watermark for disk usage. It defaults to 70.0, meaning ES will not
allocate new shards to nodes once they have more than 70% disk
used. It can also be set to an absolute byte value (like 500mb) to
prevent ES from allocating shards if less than the configured amount
of space is available.

`cluster.routing.allocation.disk.watermark.high` controls the high
watermark. It defaults to 85.0, meaning ES will attempt to relocate
shards to another node if the node disk usage rises above 85%. It can
also be set to an absolute byte value (similar to the low watermark)
to relocate shards once less than the configured amount of space is
available on the node.

Closes #3480
</description><key id="19081633">3637</key><summary>Add AllocationDecider that takes free disk space into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-09-05T23:53:54Z</created><updated>2014-12-12T16:27:39Z</updated><resolved>2013-09-09T15:50:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-06T20:12:44Z" id="23966117">- If disk based allocation decider is not enabled, then we shouldn't go and fetch the stats from the rest of the nodes. I will feel more comfortable pushing it to 0.90 if thats the case, so by default, it doesn't add any overhead.
</comment><comment author="dakrone" created="2013-09-06T20:14:20Z" id="23966206">@kimchy sure, I can certainly do that. The idea was to have the two parts completely decoupled in the event other deciders (or the BalancedShardsAllocator) wanted to use the ClusterInfo for calculations.

But I totally see what you're saying, it would be easy to decoupled them later if so desired. I'll make that change.
</comment><comment author="kimchy" created="2013-09-06T20:36:57Z" id="23967568">@dakrone I agree, other deciders might want to use it, so I suggest deciders might register an interest in it, and if they do, only then start the scheduling/sampling? 
</comment><comment author="dakrone" created="2013-09-06T21:48:59Z" id="23971657">@kimchy I've pushed all of the changes you asked for except for the change adding a registration for deciders to register an interest in the ClusterInfoService. Do you want that as part of this pull request, or something to be added once other things start using the service?
</comment><comment author="s1monw" created="2013-09-09T15:49:25Z" id="24088343">+1 to push

push it dude! good job!
</comment><comment author="synhershko" created="2013-09-09T16:05:10Z" id="24089882">Awesome! great to see this in 0.90 as well. thanks guys!
</comment><comment author="s1monw" created="2013-09-09T16:10:35Z" id="24090377">thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Not allowing index names in request body for multi-get/search/bulk when indices are already given in url</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3636</link><project id="" key="" /><description>Rational: Many users currently use URL-based access control to secure access to ES. For multi-search/get queries, currently the user can put the indices in the request body, which poses a challenge to the URL-based security approach. 

This request is to add a feature for multi-search/get such that when index(or indices) is given in the URL, prohibiting the request body to contain the index (indices). 

In this way, all request to ES can be secured via URL.

Add a flag, called `rest.action.multi.allow_explicit_index` that can be set in the settings/config (default to `true`). If set to `false`, will reject requests that have explicit index specified in their body.
</description><key id="19079926">3636</key><summary>Not allowing index names in request body for multi-get/search/bulk when indices are already given in url</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">wuchanghua</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T23:02:02Z</created><updated>2014-08-12T21:24:25Z</updated><resolved>2013-09-05T23:46:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file></files><comments><comment>Not allowing index names in request body for multi-get/search/bulk when indices are already given in url</comment><comment>closes #3636</comment><comment>It looks good. Now I can disallow the index/indices by setting the rest.action.multi.allow_explicit_index to false.</comment></comments></commit></commits></item><item><title>Term suggest returns no results for analyzed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3635</link><project id="" key="" /><description>I'm not sure if this is a bug. The term suggester works like a charm on our systems as long as there are no field-specific analyzers involved. Due to a rather long code snippet / example, I'd prefer linking to a recently opened Stack Overflow question upon this matter:

http://stackoverflow.com/questions/18647236/elasticsearch-term-suggest-on-analyzed-field-returns-no-suggestions
</description><key id="19079852">3635</key><summary>Term suggest returns no results for analyzed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SimonSteinberger</reporter><labels><label>non-issue</label></labels><created>2013-09-05T22:59:54Z</created><updated>2013-09-06T07:00:08Z</updated><resolved>2013-09-06T07:00:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-06T07:00:05Z" id="23922382">@Nasmon you showed us how you index and how you search but not a single time what you query? I mean I could have missed it but it seems like you should really give an example so we can help? What kind of data do you have in the field and what is your query and what do you expect as a result? I'd also appreciate to have this on the mailing list rather than on the dev issues - I will close this since it's not an issue at. Can you please ask this on our mailing list and add more infos
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>On Solaris, default LZF compress type (for transport) can cause segfault</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3634</link><project id="" key="" /><description>Default to the safe instance in case we are on sun os.
</description><key id="19079092">3634</key><summary>On Solaris, default LZF compress type (for transport) can cause segfault</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T22:40:49Z</created><updated>2013-10-07T19:57:39Z</updated><resolved>2013-09-05T22:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ankon" created="2013-10-07T19:57:39Z" id="25839836">Out of interest, do you have a pointer to the actual issue (segfault where?)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/compress/lzf/LZFCompressor.java</file></files><comments><comment>On Solaris, default LZF compress type (for transport) can cause segfault</comment><comment>closes #3634</comment></comments></commit></commits></item><item><title>Add monitoring link for es2graphite.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3633</link><project id="" key="" /><description /><key id="19055831">3633</key><summary>Add monitoring link for es2graphite.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-09-05T16:42:55Z</created><updated>2014-07-16T21:52:26Z</updated><resolved>2013-10-09T08:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-09T08:49:51Z" id="25955477">Merged and backported to 0.90. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support specifing score query on highlight.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3632</link><project id="" key="" /><description>This is useful if you want to highlight terms not in the search query or
you want sort highlighted snippets based on another query.

Closes #3630
</description><key id="19055728">3632</key><summary>Support specifing score query on highlight.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-05T16:41:12Z</created><updated>2014-06-30T12:57:25Z</updated><resolved>2013-10-02T20:01:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-11T23:21:19Z" id="24284216">Rebased.
</comment><comment author="nik9000" created="2013-10-02T18:27:20Z" id="25563992">Rebased again.
</comment><comment author="s1monw" created="2013-10-02T18:40:13Z" id="25565104">hey @nik9000 this PR looks great. I have a couple comments
- does this work with the plain highlighters as well? I think we should have a test for the plain highlighters as well though. Maybe you can randomly trigger the plain highlighter in that test?
- can we rename that query in to `highlight_query` I think it's really just a highlight query.
- since we have all the docs in this repo can you also add the documentation to the commit for this feature?

other than that I think this is a great feature! Thanks for doing the work
</comment><comment author="nik9000" created="2013-10-02T18:49:45Z" id="25565946">&gt; does this work with the plain highlighters as well? I think we should have a test for the plain highlighters as well though. Maybe you can randomly trigger the plain highlighter in that test?

The test alternates between the plain highlighter and the fvh.

&gt; can we rename that query in to highlight_query I think it's really just a highlight query.

Sorry, do you want it named `query` or `highlight_query`?  `highlight_query` seems a bit redundant because we're already in the `highlight` section.  I just want to make sure to get the right name before I rename it because renaming things take a while :)

&gt; since we have all the docs in this repo can you also add the documentation to the commit for this feature?

Sure.
</comment><comment author="nik9000" created="2013-10-02T18:51:11Z" id="25566058">&gt; Sorry, do you want it named query or highlight_query? highlight_query seems a bit redundant because we're already in the highlight section. I just want to make sure to get the right name before I rename it because renaming things take a while :)

OTOH `query` might be a bit vague.  You are right that the query isn't really for scoring any more - you could fake out highlighting any way you'd like with it.
</comment><comment author="s1monw" created="2013-10-02T18:52:24Z" id="25566157">&gt; &gt; The test alternates between the plain highlighter and the fvh.

cool stuff

regarding the query I think `highlight_query` is ok no?
</comment><comment author="nik9000" created="2013-10-02T19:02:49Z" id="25567075">&gt; regarding the query I think highlight_query is ok no?

Sure!  I'll make the changes.
</comment><comment author="nik9000" created="2013-10-02T19:47:25Z" id="25570724">I believe that last commit covers everything.
</comment><comment author="s1monw" created="2013-10-02T20:02:02Z" id="25571484">thanks @nik9000 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>FlushNotAllowedEngineException during optimize</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3631</link><project id="" key="" /><description>I have encountered the following exception a few times recently on 0.90.3 while I was during some testing.  Seems to happen sporadically on a random shard during an optimize, maybe I am hitting some weird race condition?

```
[2013-09-05 05:42:23,925][DEBUG][action.admin.indices.optimize] [samcro] [stackoverflow][0], node[i45bAah_SUa9zPPLmFYw4g], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.optimize.OptimizeRequest@5ef574db]
org.elasticsearch.index.engine.FlushNotAllowedEngineException: [stackoverflow][0] already flushing...
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:818)
    at org.elasticsearch.index.engine.robin.RobinEngine.optimize(RobinEngine.java:1014)
    at org.elasticsearch.index.shard.service.InternalIndexShard.optimize(InternalIndexShard.java:512)
    at org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:115)
    at org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:49)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:228)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:205)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```
</description><key id="19051354">3631</key><summary>FlushNotAllowedEngineException during optimize</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>enhancement</label><label>v0.90.5</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T15:38:38Z</created><updated>2013-09-16T20:20:38Z</updated><resolved>2013-09-16T20:20:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-06T15:57:04Z" id="23950233">Hey @mattweber 
can you share the test you ran to obtain the above stacktrace? No way you could reproduce it consistently right?
</comment><comment author="mattweber" created="2013-09-06T17:56:44Z" id="23957877">Hey @javanna,

I was playing around with bulk indexing the [stackoverflow posts.xml data](http://www.clearbits.net/get/2076-aug-2012.torrent).  I was consistently deleting and reindexing the data tweaking settings and measuring the performance difference.  I started to notice this with the following:
- parent/child mapping described in [this blog post](http://www.fullscale.co/blog/2013/02/19/Using_ElasticSearch_To_Find_Best_Time_To_Ask_Questions_On_StackOverflow.html)
- 5 shards, no replicas (I reproduced with less shards too) 
- refresh -1
- bulk size of 100
- 8 feeding processes
- translog flush settings doubled from defaults
- segments per tier 30
- optimize segments of 5
- increased index buffer (40% of 6GB heap)
- bulk threadpool fixed at size 24
- no searching during feeding or optimize

Feeding would take ~45min for just under 11M docs.  The blog post above has a python feeding script attached, I was basically using an updated version of that which uses python multiprocessing module to break out the work across 8 processes.  Unfortunately I just left on vacation and forgot to commit those changes so I can't provide that script until next week.  Please let me know if you need any more information.  

Thanks!
</comment><comment author="mattweber" created="2013-09-06T17:59:03Z" id="23958015">Ohh and [es2graphite](https://github.com/mattweber/es2graphite) which executes cluster health, node stats, and node info rest calls was running every 10 seconds.
</comment><comment author="mattweber" created="2013-09-16T01:22:21Z" id="24484936">@javanna 

Ok, I can still reproduce this.  Here is a gist of my feeding scripts to be used with the posts.xml file downloaded from the torrent linked above, https://gist.github.com/mattweber/d969e9c91b45503ac452.  Here is some info from logs, maybe it has something with the fact I am changing settings right before the optimize? 

I am going to try to come up with an easier reproduce method so you don't need to download all the stackoverflow data.  I will keep you updated.  Thanks.

```
[2013-09-15 18:09:58,437][DEBUG][cluster.service          ] [samcro] cluster state updated, version [8], source [update-settings]
[2013-09-15 18:09:58,439][INFO ][index.merge.policy       ] [samcro] [stackoverflow][0] updating [segments_per_tier] from [30.0] to [10.0]
[2013-09-15 18:09:58,439][INFO ][index.shard.service      ] [samcro] [stackoverflow][0] updating refresh_interval from [-1] to [1s]
[2013-09-15 18:09:58,443][DEBUG][river.cluster            ] [samcro] processing [reroute_rivers_node_changed]: execute
[2013-09-15 18:09:58,444][DEBUG][river.cluster            ] [samcro] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-09-15 18:09:58,469][INFO ][index.translog           ] [samcro] [stackoverflow][0] updating flush_threshold_ops from [10000] to [5000]
[2013-09-15 18:09:58,469][INFO ][index.translog           ] [samcro] [stackoverflow][0] updating flush_threshold_size from [500mb] to [200mb]
[2013-09-15 18:09:58,470][INFO ][index.merge.policy       ] [samcro] [stackoverflow][1] updating [segments_per_tier] from [30.0] to [10.0]
[2013-09-15 18:09:58,470][INFO ][index.shard.service      ] [samcro] [stackoverflow][1] updating refresh_interval from [-1] to [1s]
[2013-09-15 18:09:58,470][INFO ][index.translog           ] [samcro] [stackoverflow][1] updating flush_threshold_ops from [10000] to [5000]
[2013-09-15 18:09:58,470][INFO ][index.translog           ] [samcro] [stackoverflow][1] updating flush_threshold_size from [500mb] to [200mb]
[2013-09-15 18:09:58,560][DEBUG][cluster.service          ] [samcro] processing [update-settings]: done applying updated cluster_state
[2013-09-15 18:09:58,785][DEBUG][action.admin.indices.optimize] [samcro] [stackoverflow][0], node[s9_7RSkTSUiNThZpuvLU6A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.optimize.OptimizeRequest@598d2fb7]
org.elasticsearch.index.engine.FlushNotAllowedEngineException: [stackoverflow][0] already flushing...
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:818)
    at org.elasticsearch.index.engine.robin.RobinEngine.optimize(RobinEngine.java:1014)
    at org.elasticsearch.index.shard.service.InternalIndexShard.optimize(InternalIndexShard.java:512)
    at org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:115)
    at org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:49)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:228)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:205)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)

```
</comment><comment author="kimchy" created="2013-09-16T07:11:14Z" id="24492133">This can happen during relocation, at that point, flush is not allowed to be executed. optimize operation defaults to performing flush, though you don't have to do it to optimize, in which case, you can set the flush flag to false?
</comment><comment author="mattweber" created="2013-09-16T15:37:19Z" id="24519443">Hey @kimchy,

There are no relocations going on when this happens.  It is a single node cluster and I have reproduced it on an index with a single shard.  In fact, it happens more consistently when I have less shards.  It happens pretty much every time when I have 1 or 2 shards.  Once I get up to 3+ shards it doesn't happen as often.  Maybe I am hitting some sort of race condition?

Thanks.
</comment><comment author="mattweber" created="2013-09-16T18:35:58Z" id="24533266">I reproduced using 0.90.4 as well.
</comment><comment author="kimchy" created="2013-09-16T18:44:30Z" id="24533916">we also periodically flush due to the transaction log (not just when relocating, apologies for not explaining to details), and if you issue a flush while another flush is happening, then we will throw the mentioned failure as well. We have an internal flag to wait for ongoing flushes, we can expose it through the API.
</comment><comment author="s1monw" created="2013-09-16T18:46:45Z" id="24534065">I am not sure if we should expose it or if we should just set it to `true` (ie. wait for ongoing flushes) on an optimize call. Folks are used to wait on a optimize call and if we fail that they might get confused calling optimize again. I think we should just wait by default?
</comment><comment author="kimchy" created="2013-09-16T19:01:28Z" id="24535231">@s1monw agreed, I think its a good default to have when calling optimize, will do it.
</comment><comment author="s1monw" created="2013-09-16T19:01:58Z" id="24535274">+1
</comment><comment author="mattweber" created="2013-09-16T19:02:59Z" id="24535349">Sounds good to me, thanks for the info!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>FlushNotAllowedEngineException during optimize</comment><comment>default to wait if a flush is on going when executed as part of optimize request</comment><comment>closes #3631</comment></comments></commit></commits></item><item><title>Support highlighting against queries other than the search query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3630</link><project id="" key="" /><description>This issue was originally about rescore queryies and I've left it intact below so the discussion makes sense.  I've changed the issue to be more general.  Now this is a feature request to let the user specify a query against which highlighting will run in the highlight element.  Like this:

``` bash
curl -XPOST http://localhost:9200/test/test/_search?pretty -d'{
  "fields": [
    "_id"
  ],
  "highlight": {
    "order": "score",
    "fields": {
      "text": {
        "number_of_fragments": 1,
        "score_query": {
          "bool": {
            "must": {
              "match": {
                "text": {
                  "query": "central iraq"
                }
              }
            },
            "should": {
              "match_phrase": {
                "text": {
                  "query": "central iraq",
                  "phrase_slop": 3,
                  "boost": 10.0
                }
              }
            },
            "minimum_should_match": 0
          }
        }
      }
    }
  },
  "query": {
    "match": {
      "text": {
        "query": "central iraq"
      }
    }
  }
}'
```

This is the old issue:
When sorting highlights by score the rescore query should be used if there is one.  At least, I think it should be.

This gist: https://gist.github.com/nik9000/6451307
Should spit out: `"Mesopotamia, while Mithna ibn Haris withdraw from &lt;em&gt;central Iraq&lt;/em&gt; to the region near the Arabian desert to delay"`
Rather than: `"of &lt;em&gt;Iraq&lt;/em&gt; fell to the Muslims after initial resistance in the Battle of Hira.  File:Ctesiphon, &lt;em&gt;Iraq&lt;/em&gt; (2117465493)"`

I have a pretty simplistic solution for this I'll submit once I'm done running it through tests.
</description><key id="19049501">3630</key><summary>Support highlighting against queries other than the search query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>feature</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T15:09:22Z</created><updated>2013-11-12T14:36:27Z</updated><resolved>2013-10-02T20:01:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-06T07:03:32Z" id="23922523">hmm I am not sure if we should use rescore for this but maybe have a dedicated highlight query? 
</comment><comment author="nik9000" created="2013-09-06T10:51:11Z" id="23932235">I see how having a highlight query might be nice but using the rescore query feels right to me - when you sort highlights by score I expected it to be the same query that orders your results.

It wouldn't be difficult to modify the pull request to create an actual highlight query element in the api that defaults to the rescore query if there is one and the main query otherwise.  That'd make me happy.
</comment><comment author="nik9000" created="2013-09-06T12:10:16Z" id="23935576">Alternately I could make the highlight query default to the query and have an option to use the rescore query.  That'd work for me too.
</comment><comment author="jpountz" created="2013-09-06T12:49:15Z" id="23937347">I think using the rescore query by default for highlighting is risky since not every match is required to match it (on the contrary to the main query). But I definitely see the point of using the rescore query to highlight matches to have better snippets.

So I agree it would be nice to allow to use the rescore query to highlight matches, or maybe even an arbitrary query, probably with a fallback mechanism so that the main query gets used in case the highlighter couldn't find a snippet to highlight with the rescore query.
</comment><comment author="s1monw" created="2013-09-06T13:00:40Z" id="23937969">I agree with @jpountz I think that makes most sense to have a dedicated query.
</comment><comment author="nik9000" created="2013-09-06T19:44:01Z" id="23964467">Dedicated query it is then.

Since we're letting the user specify the query themselves I'm included to let the user deal with fallback highlighting with a bool query and boosts or something.
</comment><comment author="nik9000" created="2013-09-16T12:54:12Z" id="24507421">If anyone has time to comment on this I'd be glad to make any changes you need.  I'm happy to rebase it as well I just don't want to rebase it now only to have to rebase it again in a week when folks have time to look at it.
</comment><comment author="nik9000" created="2013-09-16T19:22:53Z" id="24536923">Removed change that wasn't relevant now that we don't explicitly consider rescore queries and rebased.
</comment><comment author="s1monw" created="2013-10-02T20:08:19Z" id="25571998">thanks nik!
</comment><comment author="kimchy" created="2013-10-02T20:11:53Z" id="25572269">one quick small suggestion, might make sense to change the `highlighter_query` to just be `query`, because its already within the highlighting context, so we kind of repeating info? Make sense?
</comment><comment author="nik9000" created="2013-10-02T20:33:13Z" id="25574020">&gt; one quick small suggestion, might make sense to change the highlighter_query to just be query, because its already within the highlighting context, so we kind of repeating info? Make sense?

We had that exact conversation in the pull request.  I like calling it `query` for the same reason you mention I'm worried that might look vague.  The first name I came up with, `score_query`, was just horrible.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Added support for external query in postings highlighter</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterContext.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Support specifing score query on highlight.</comment></comments></commit></commits></item><item><title>Validate query api parses wrong date range query when using "now"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3629</link><project id="" key="" /><description>Same problem as #3625 and #3626, but with the validate query api. 
When using the validate query API with date range queries that contain `now` (e.g. `[* TO now-1d]`), the parsed query contains a negative number that leads to a valid query anyway, but the explain shows that something went wrong while parsing the date range. The following curl reproduction shows the different result obtained using the search API and the validate query API.

```
curl -XPUT localhost:9200/index1/type1/1 -d '{
  "date": "2013-09-03T15:07:47.000Z"
}
'

curl -XPOST localhost:9200/index1/_refresh

#one hit gets returned (id 1) using search api
curl -XGET localhost:9200/index1/_search -d '
{ 
    "query" : {
        "query_string": {
            "query": "date:[* TO now-1d]"
        }   
    }
}
'
#validate query api returns a weird query with a negative time from epoch (`date:[* TO -86400000]`")
curl -XGET localhost:9200/index1/type1/_validate/query?explain -d '
{ 
    "query_string": {
      "query": "date:[* TO now-1d]"
    }
}
'
```
</description><key id="19046840">3629</key><summary>Validate query api parses wrong date range query when using "now"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T14:30:17Z</created><updated>2013-10-18T12:09:31Z</updated><resolved>2013-09-20T20:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Set nowInMillis to search context created by the validate query api so that "NOW" can be used within queries</comment></comments></commit></commits></item><item><title>Plugin Manager: add silent mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3628</link><project id="" key="" /><description>Now with have proper exit codes for elasticsearch plugin manager (see #3463), we can add a silent mode to plugin manager.

``` sh
bin/plugin --install karmi/elasticsearch-paramedic --silent
```
</description><key id="19046187">3628</key><summary>Plugin Manager: add silent mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T14:19:59Z</created><updated>2013-09-10T16:33:42Z</updated><resolved>2013-09-10T16:33:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/test/integration/plugin/PluginManagerTests.java</file></files><comments><comment>Plugin Manager: add silent mode.</comment><comment>Now with have proper exit codes for elasticsearch plugin manager (see #3463), we can add a silent mode to plugin manager.</comment></comments></commit></commits></item><item><title>Query timeout ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3627</link><project id="" key="" /><description>Hi again,

I have a timeout specified in all of my queries to prevent some slow queries from backing up the queue, but it doesn't seem to be working at the moment. I have many, many entries in my slowlog anywhere from 2s to 22s for queries with a timeout of 1500ms specified. Here's a snippet of one.

[2013-09-05 13:35:32,967][WARN ][index.search.slowlog.query] [qdave] [quizlet][0] took[6.3s], took_millis[6311], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[10], source[{"fields":[],"from":0,"size":"50","timeout":"1500ms", ...

As always, anything else I can provide which will help debug this?
</description><key id="19045626">3627</key><summary>Query timeout ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">rdeaton</reporter><labels /><created>2013-09-05T14:09:16Z</created><updated>2017-06-23T22:12:58Z</updated><resolved>2013-10-05T14:57:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rdeaton" created="2013-09-05T14:10:14Z" id="23870148">Perhaps I am actually misunderstanding and the timeout bound forces the hits to be returned, but does not stop the execution of the query, in which case I suppose my question is, how difficult is it to add functionality to do the latter?
</comment><comment author="javanna" created="2013-09-11T12:27:42Z" id="24235786">Indeed the timeout doesn't kill the running query, but that doesn't mean it is ignored. Perhaps you were looking for something like #2929, adding the ability for actually killing a query when the timeout expires? What do you think @rdeaton?
</comment><comment author="javanna" created="2013-10-05T14:57:50Z" id="25750005">Closing this one. The current timeout mechanism is a best effort, which might not work with all the queries. The idea behind #2929 is to try and improve this. I'd suggest to watch that issue if you are interested, or reopen this one if you meant something else.
</comment><comment author="markharwood" created="2014-08-01T08:32:17Z" id="50861374">See also https://github.com/elasticsearch/elasticsearch/pull/4586
</comment><comment author="aalexgabi" created="2016-06-06T16:58:31Z" id="224020015">Does Elasticsearch kill a long running query if you set the timeout in the client? Here I'm not talking about killing the TCP connection but actually stopping the long running query from bringing down the node.

Is there a way to set the timeout globally for the cluster (for example in /etc/elasticsearch/elasticsearch.yml)?

If any of these features exist please specify the version in which they have been introduced.
</comment><comment author="jasontedor" created="2016-06-06T17:15:53Z" id="224024736">&gt; Does Elasticsearch kill a long running query if you set the timeout in the client?

Timeouts are achieved on a best-effort basis. Timeouts are pushed down to the shard and impact the query phase. Certain things like highlighting and rewriting are not impacted by the timeout.

&gt; Is there a way to set the timeout globally for the cluster (for example in /etc/elasticsearch/elasticsearch.yml)?

Yes. There is [`search.default_search_timeout`](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search.html#global-search-timeout) which is available starting in 2.0.0. This has the same caveat as above, it is best-effort.
</comment><comment author="dylanwenzlau" created="2017-06-22T05:23:55Z" id="310278749">Having this feature work more accurately is useful beyond #2929. We are trying to use this to stop a query after a certain amount of time so that our search product can always return in a given amount of time. We could make a hacky solution that runs the query async, and then we would stop waiting after a certain amount of time, but the query would then continue to use resources on the elasticsearch server.

Our feature request would be basically an equivalent to mysql's MAX_EXECUTION_TIME. Unfortunately, right now Elasticsearch's timeout parameter can be inaccurate by several orders of magnitude, rendering it useless for our purpose. Our tests have been run against Elasticsearch 5.3</comment><comment author="markharwood" created="2017-06-23T16:24:39Z" id="310710926">&gt;Elasticsearch's timeout parameter can be inaccurate by several orders of magnitude

Ensuring timely timeouts is a case of weaving timer checks into all the "hot loops". We've done this for many e.g. the loop for collecting the next doc in a result set. Can you give an example of the problem query that overruns so we can see what un-checked loop might be involved?</comment><comment author="dylanwenzlau" created="2017-06-23T19:25:50Z" id="310752743">Makes sense.

Here is an example where I specify a 10ms timeout, and the query finishes in 1361ms.

![image](https://user-images.githubusercontent.com/4359148/27497254-8229b950-580e-11e7-8ca6-7c1cdcaef781.png)</comment><comment author="markharwood" created="2017-06-23T22:12:58Z" id="310784382">Note there is a node-level setting `thread_pool.estimated_time_interval` that dictates the resolution of the timer used to estimate current time for these sorts of checks (it avoids making timer checks too expensive in hot loops). The default resolution of this is 200ms meaning you can expect overruns of this magnitude but obviously your example is still greater than this.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Explain api parses wrong date range query when using "now"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3626</link><project id="" key="" /><description>Same problem as #3625, but with the explain api. 
When using the explain API with date range queries that contain `now` (e.g. `[* TO now-1d]`), the parsed query contains a negative number that leads to a no match being returned. The following curl reproduction shows the different result obtained using the search API and the explain API.

```
curl -XPUT localhost:9200/index1/type1/1 -d '{
  "date": "2013-09-03T15:07:47.000Z"
}
'

curl -XPOST localhost:9200/index1/_refresh

#one hit gets returned (id 1) using search api
curl -XGET localhost:9200/index1/_search -d '
{ 
    "query" : {
        "query_string": {
            "query": "date:[* TO now-1d]"
        }   
    }
}
'
#explain api says the document doesn't match
curl -XGET localhost:9200/index1/type1/1/_explain -d '
{ 
    "query_string": {
      "query": "date:[* TO now-1d]"
    }
}
'
```
</description><key id="19045591">3626</key><summary> Explain api parses wrong date range query when using "now"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T14:08:37Z</created><updated>2013-10-18T12:03:46Z</updated><resolved>2013-09-20T20:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/explain/ExplainRequest.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/test/java/org/elasticsearch/explain/ExplainActionTests.java</file></files><comments><comment>Set nowInMillis to search context created by the explain api so that "NOW" can be used within queries</comment></comments></commit></commits></item><item><title>Count api parses wrong date range query when using "now"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3625</link><project id="" key="" /><description>When using the count API with date range queries that contain `now` (e.g. `[* TO now-1d]`), the parsed query contains a negative number that leads to count 0 being returned. The following curl reproduction shows the different result obtained using the search API and the count API.

```
curl -XPUT localhost:9200/index1/type1/1 -d '{
  "date": "2013-09-03T15:07:47.000Z"
}
'

curl -XPOST localhost:9200/index1/_refresh

#one hit gets returned using search api
curl -XGET localhost:9200/index1/_search -d '
{ 
    "query" : {
        "query_string": {
            "query": "date:[* TO now-1d]"
        }   
    }
}
'
#count 0 using count api
curl -XGET localhost:9200/index1/_count -d '
{ 
    "query_string": {
      "query": "date:[* TO now-1d]"
    }
}
'
```
</description><key id="19043563">3625</key><summary>Count api parses wrong date range query when using "now"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T13:32:26Z</created><updated>2013-10-18T12:03:33Z</updated><resolved>2013-09-20T20:23:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/ShardCountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/test/java/org/elasticsearch/count/query/SimpleQueryTests.java</file></files><comments><comment>Set nowInMillis to search context created by the count api so that "NOW" can be used within queries</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/test/integration/count/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/test/integration/count/simple/SimpleCountTests.java</file></files><comments><comment>Added tests for count api</comment></comments></commit></commits></item><item><title>Memory increase from 0.90.2 to 0.90.3 on java client API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3624</link><project id="" key="" /><description>## Context

Our Java batch indexes 13 millions of document with the ElasticSearch Java API.
And our ElasticSearch cluster contains a single data node and a single shard.
## Problem

By upgrading the ElasticSearch server and client from version 0.90.2 to 0.90.3, the batch stops with an OutOfMemoryError. This memory error only occurs on client side (ie. the batch).
Until now, 512 Mb will be enough to run the batch without memory problem (as well as in production environment). With the old 0.19.2 version, we neither had this problem.
By updating the Xmx value to 1g, the batch falls again. The Xms has to be set to 1300Mb in order the batch finish with success.
By downgrading client version from 0.90.3 to 0.90.2 (the ES cluster still runs with the 0.90.3 version), our batch problem goes away and 512 Mb of memory are enough.
So I believe a change between the 0.90.2 and 0.90.3 versions causes ES to require more memory (example: increase byte[] array buffer default size ?)
## More informations

The batch uses the BulkRequestBuilder API. Bulk request contains 5 000 request. At most 20 threads could be running in parallel. But they are not writing to ES at the same time.
As you can see on the below screenshot, a OutOfMemory hprof dump indicates that 800Mb of byte[] is coming from the org.elasticsearch.common.bytes.BytesArray structure.
We have 25 000 org.elasticSearch.action.index.IndexRequest in memory.

![batch_memory_live_objects](https://f.cloud.github.com/assets/838318/1088059/d7c7c4ea-1629-11e3-896e-23074c99be4e.jpg)

Do other ElasticSearch users have this kind of memory problem with the 0.90.3 version?

To solve our issue, I see many possibilities: 
- Increase batch Xmx to 1300Mb. Our production environnement as more than 40 millions of documents so I fears this value will be not enough. 
- Use the 0.90.2 version of ElasticSearch for our batch
- Wait a new version of ElasticSearch that fix this problem.
- Decrease the number of request in bulk
- Decrease the number of threads

The last two solutions have a flaw: the batch will run more longer. 
</description><key id="19042303">3624</key><summary>Memory increase from 0.90.2 to 0.90.3 on java client API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">arey</reporter><labels /><created>2013-09-05T13:07:25Z</created><updated>2013-09-06T10:25:34Z</updated><resolved>2013-09-06T00:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-05T13:20:43Z" id="23866803">It seems like there is no leak, since 5 BulkRequests \* 5000 end up being 25k index requests (which hold the source to be index, which is the bytes array). How big is each index request you have, I mean, what is the size of each document end up being? Does it add up to the memory used?

Nothing jumps to mind with changes we did around it. What is the behavior in 0.90.2? How much memory is it using?

As a side note, 25k index requests is probably too much in how much it ends up with size in bytes per request (assuming its correct). You don't want to send a 100mb bulk request, probably make sense to make that smaller so it will be more efficient on the network (probably between 10-20mb).
</comment><comment author="arey" created="2013-09-05T15:08:54Z" id="23874730">Thanks Kimchy for your answer.
As you can see, JSON document sources are very small (between 500 bytes and 1Kb in a text editor).
Here an anonymous example :

``` javascript
{
    _index: customers
    _id: customer
    _version: 1378062062000
    _score: 1
    _source: {
        identifiantadb: 92492430
        identifiantclientagent: z1234
        identifiantintermediaire: 987654
        codepopulation: AZE
        statutclient: PP
        idxnaturepersonne: PART
        civilite: 01
        prenom: Jean
        codepostal: 75001
        localite: PARIS
        identifiantrcedisplay: 0124924655
        identifiantclientagentdisplay: C0156
        idstatutclient: 2
        nompersonne: DUPONT
        adressedisplay: 2 RUE DE PARIS
        codepostalprincipal: 78380
        localiteprincipale: BOUGIVAL
        telephoneprincipal: 0102030405
        telephones: 0102030405
    }
}
```

So 25k index requets (5 threads x 1 bulk request of 5000 index requests) of 1Kb bytes max should required 15 Mb of memory. I really don't understand why 800 Mb seems to be used. A BulkRequest contains 5000 index requets thus should have a size of 5Mb.

Previously, with the ES 0.90.2 client, the memory allocation was much lower. I have done some memory snapshots to reach the same number of IndexRequest
The following screenshot has 25k IndexRequest like the upper one. Compared to the 800 Mb of the 0.90.3, only 29 Mb are used wigth the 0.90.2 :

![batch_memory_live_objects_0 19 2](https://f.cloud.github.com/assets/838318/1089009/7ac9753c-163c-11e3-9450-436761420a63.jpg)

How to explain why 26 as much memory are required?
</comment><comment author="kimchy" created="2013-09-05T23:28:23Z" id="23909167">Thats very strange then... . Since there are different ways to use the bulk API, si there a chance for a stand alone simple recreation of this (with dummy data for example?). This will make figuring this out on my end much faster (I will try nonetheless). If you can do it quickly, I can try and see if there is a problem, to get it fixed for the 0.90.4 release (slated for early next week...).

[Update]: Also, I assume, but just double checking, that the code on your end is exactly the same, and you just replace the ES jar file.
</comment><comment author="kimchy" created="2013-09-05T23:41:49Z" id="23909725">Mmm, I think I found where its coming from..., we changed the default value for our output stream (`BytesStreamOutput`) initial array size to `32k`, which makes sense in most cases, but potentially not in your case (this is the output stream we create for each XContentBuilder you create). Might make sense to reduce it when building a document using XContent.
</comment><comment author="kimchy" created="2013-09-05T23:44:20Z" id="23909799">Btw, you can work around it by creating your own XContentBuilder while providing the `BytesOutputStream` for it yourself, with a size value.
</comment><comment author="kimchy" created="2013-09-06T00:26:38Z" id="23911469">I fixed it in #3638, should be good now, will be part of next 0.90.4. Feel free to reopen if you still have problems!.
</comment><comment author="arey" created="2013-09-06T10:25:34Z" id="23931190">Thank you  Kimchy for having found the origin of my problem and fix it in the 0.90.4 version.
At the moment, I don't know if we could wait the ES release next week. I will check.
By waiting, I used your suggestion to instantiate by myself the BytesStreamOutput with a size of 1024.

``` java
XContentBuilder content = new XContentBuilder(JsonXContent.jsonXContent, new BytesStreamOutput(1024)).startObject();
```

I relanch the batch with a Xmx of 512 and my OutOfMemoryError goes away.
So, if we can't wait the 0.90.4 release, I have a work around.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java</file><file>src/test/java/org/elasticsearch/test/unit/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>BytesStreamOutput default size should be 2k instead of 32k</comment><comment>We changed the default of BytesStreamOutput (used in various places in ES) to 32k from 1k with the assumption that most stream tend to be large. This doesn't hold for example when indexing small documents and adding them using XContentBuilder (which will have a large overhead).</comment></comments></commit></commits></item><item><title>Make the acceptable compression overhead used by MultiOrdinals configurable and default to FASTEST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3623</link><project id="" key="" /><description>We currently store ordinal in a compressed fashion using Lucene's PackedInt structures. Currently we use the COMPACT compression setting which tries to save on memory as much as possible at the expense of CPU and execution time. To reduce the CPU cost, we will default to FASTEST compression but make it configurable so people can change it if needed, via the mapping API:

```
curl -XPUT "http://localhost:9200/index/type/_mapping" -d'
{
   "type": {
      "properties": {
         "field": {
            "type": "string",
            "fielddata": {
               "acceptable_overhead_ratio": 0.2 
            }
         }
      }
   }
}'
```
</description><key id="19041964">3623</key><summary>Make the acceptable compression overhead used by MultiOrdinals configurable and default to FASTEST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-05T13:01:20Z</created><updated>2013-09-05T13:29:22Z</updated><resolved>2013-09-05T13:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/util/packed/XAbstractAppendingLongBuffer.java</file><file>src/main/java/org/apache/lucene/util/packed/XAppendingPackedLongBuffer.java</file><file>src/main/java/org/apache/lucene/util/packed/XMonotonicAppendingLongBuffer.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/MultiOrdinals.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/ordinals/MultiOrdinalsTests.java</file></files><comments><comment>Make the acceptable compression overhead used by MultiOrdinals configurable and default to PackedInts.FASTEST (causing it to byte align).</comment></comments></commit></commits></item><item><title>Elastic search Problem on Debian Squeeze and Oracle java 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3622</link><project id="" key="" /><description>amuzzin@vps:~/binaries/elasticsearch-0.90.3$ sudo bin/elasticsearch
amuzzin@vps:~/binaries/elasticsearch-0.90.3$ Error occurred during initialization of VM
Could not reserve enough space for object heap
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

I tried setting the xms and xmx to 1563m and still it throws an error like this. I am able to run other java files with my java runtime. Please help. 
</description><key id="19038027">3622</key><summary>Elastic search Problem on Debian Squeeze and Oracle java 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karthikselva</reporter><labels /><created>2013-09-05T11:22:28Z</created><updated>2013-09-05T12:22:16Z</updated><resolved>2013-09-05T12:21:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-05T12:21:53Z" id="23863588">Please use the mailing list for that kid of question. See http://www.elasticsearch.org/help/
</comment><comment author="nik9000" created="2013-09-05T12:22:16Z" id="23863609">I'd edit the script to have it echo the java line so you can check it yourself. Odds are good one of your attempt to change xmx didn't take somehow. 

Sent from my iPhone

On Sep 5, 2013, at 7:22 AM, karthik selvakumar notifications@github.com wrote:

&gt; amuzzin@vps:~/binaries/elasticsearch-0.90.3$ sudo bin/elasticsearch
&gt; amuzzin@vps:~/binaries/elasticsearch-0.90.3$ Error occurred during initialization of VM
&gt; Could not reserve enough space for object heap
&gt; Error: Could not create the Java Virtual Machine.
&gt; Error: A fatal exception has occurred. Program will exit.
&gt; 
&gt; I tried setting the xms and xmx to 1563m and still it throws an error like this. I am able to run other java files with my java runtime. Please help.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch stops responding to all network requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3621</link><project id="" key="" /><description>Hi all,

I'm having an issue where ES seems to die while the JVM continues running. I can run JVM monitoring tools and see that GCs happen and get stack traces, but elasticsearch is completely nonresponsive.

I have the following output from jstack -F which hopefully helps out.

https://gist.github.com/rdeaton/6444525
</description><key id="19012007">3621</key><summary>Elasticsearch stops responding to all network requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">rdeaton</reporter><labels /><created>2013-09-05T00:22:55Z</created><updated>2014-08-08T17:04:04Z</updated><resolved>2014-08-08T17:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-05T00:49:00Z" id="23837029">Based on the stack traces, you have many search failures going on (almost as if all your search requests fail), do you see failures in the log?

This will not cause ES to become unresponsive, I see based on the stack trace that you also send a multi search request, how many search requests do you have in that multi search request?

What do you mean when you say its not responsive? To which operations does not it not respond?
</comment><comment author="rdeaton" created="2013-09-05T01:04:52Z" id="23837531">I should have mentioned above that this trace is from a master-only node.

The multisearch request is 4 searches per multisearch. I am however doing a very high volume of searches.

When I say not responsive, I mean that calling anything via the HTTP interface times out, and the other master nodes in my cluster also end up hitting fd.timeouts on this node most of the time.
</comment><comment author="rdeaton" created="2013-09-05T01:06:45Z" id="23837597">As for the failures, it is possible that a high number of failures are coming back. This issue seems to be triggered after an issue that is cropping up as the result of some other problems on the master nodes where they very quickly become backed up on requests to the point where the threadpool is full and they start rejecting searches. At least some of these instances seem to be related to some of my previous bug reports.
</comment><comment author="kimchy" created="2013-09-05T14:28:36Z" id="23871552">The master node doesn't coordinate search requests, you can send the request to any node and it will be the coordinator for that scatter/gather type search request.

I wonder if you are creating many HTTP connections to the nodes, resulting in exhausting the capability to open new sockets? 

You mean that those failure might relate to the NPE you get with the completion service? That can cause excessive load on a cluster (as its unexepcted).
</comment><comment author="mattweber" created="2013-09-05T15:26:31Z" id="23876130">Thanks Shay,

Good point about the open sockets, we will distribute the http load across all 13 servers (10 data, 3 master) to see if that helps.  I am thinking this might be due to NPE in the completion service:
1.  NPE happens
2.  The shard with the NPE goes into initializing state: https://groups.google.com/forum/#!topic/elasticsearch/aySF9A51UVU 
3.  Since we have a shard not serving traffic, traffic shifts to one of the other replicas
4.  Search queue starts to get backed up from all the extra traffic
5.  The NPE happens on other shards in the cluster thus multiple shards are initializing not serving traffic
6.  The sockets from master -&gt; overloaded data node are blocked and eat up all available sockets.
7.  Eventually the master stops responding
</comment><comment author="clintongormley" created="2014-08-08T17:04:04Z" id="51629825">Closed in favour of #6444 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException in ConcurrentMergeScheduler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3620</link><project id="" key="" /><description>org.apache.lucene.index.MergePolicy$MergeException: java.lang.NullPointerException
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:99)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$1$1.finish(AnalyzingCompletionLookupProvider.java:134)
        at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$SuggestFieldsConsumer$1.finish(Completion090PostingsFormat.java:152)
        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:204)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:365)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:98)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3772)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3376)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)

As always, let me know if there's anything more I can provide to help debug.
</description><key id="19007765">3620</key><summary>NullPointerException in ConcurrentMergeScheduler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rdeaton</reporter><labels /><created>2013-09-04T22:36:54Z</created><updated>2013-09-05T07:26:02Z</updated><resolved>2013-09-05T06:24:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-05T06:24:08Z" id="23846135">this is already fixed ie. duplicate of  #3555 
</comment><comment author="rdeaton" created="2013-09-05T06:29:24Z" id="23846288">This was on master from this morning, so perhaps it's not quite fixed yet?
</comment><comment author="s1monw" created="2013-09-05T07:06:27Z" id="23847548">Well, that line number points to a comment in the code and they tend to be correct though. I am wondering if what you have deployed is actually the current HEAD of our master branch or an older revision. Can you check what has you build that from?
</comment><comment author="rdeaton" created="2013-09-05T07:17:19Z" id="23847931">Oops, seems you're right, I was correlating logs to the wrong cluster. Sorry for the mixup.
</comment><comment author="s1monw" created="2013-09-05T07:26:02Z" id="23848273">@rdeaton thanks for clarifying!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException in CompletionStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3619</link><project id="" key="" /><description>[2013-09-04 00:46:46,113][DEBUG][action.admin.cluster.node.stats] [qtau] failed to execute on node [3MfX8Mx6Rmmp0_fukOT4lQ]
org.elasticsearch.transport.RemoteTransportException: [qantiup][inet[/192.168.72.143:9300]][cluster/nodes/stats/n]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.engine.robin.RobinEngine.searcher(RobinEngine.java:682)
        at org.elasticsearch.index.shard.service.InternalIndexShard.completionStats(InternalIndexShard.java:536)
        at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:299)
        at org.elasticsearch.node.service.NodeService.stats(NodeService.java:165)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:100)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:43)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:280)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:271)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
</description><key id="19005402">3619</key><summary>NullPointerException in CompletionStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rdeaton</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-04T21:55:49Z</created><updated>2013-09-05T08:10:53Z</updated><resolved>2013-09-05T08:10:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-09-05T01:19:43Z" id="23838053">What version are you experiencing this on?
</comment><comment author="rdeaton" created="2013-09-05T01:21:47Z" id="23838128">Master from within the last 24-hours or so.
</comment><comment author="s1monw" created="2013-09-05T06:59:14Z" id="23847315">it seems like this shard is starting up right now but the engine is not started yet while the stats are requested. I will push a fix for this soon.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Return empty completion stats if engine throws an exception</comment></comments></commit></commits></item><item><title>data loss in minimalistic test project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3618</link><project id="" key="" /><description>Hello,

I've set up a minimalistic Test-project using elasticsearch. Via a Java-bean, data is being created (four times via a loop -&gt; four documents should be created) and via another bean my whole existing documents are shown on a webpage (with jsp)
(The code for the create-Bean and the showAll-Bean is attached).

The problem is, that my loop creates in average 3 documents and not the wanted 4.
My documents always store the following two parameters (just for testing purpose): Time-ID (that is the counter of the loop, so it is always a number between 1 and 4) and the actual timestamp (down to milli-secs).

Im getting a "true" by creating on the web page and the created id for the documents four times for creating but in my sense-plugin in chrome browser i see the data only created 3 times.

(I'm also still having the problem, that my array gets out of bounds, if there are more than 10 files, like it is descriped in issue 3051 (https://github.com/elasticsearch/elasticsearch/issues/3051). I'm using elasticsearch 0.90.3)
## Code:

public String createESData()
    {
        String result = "";
        String data = "";
        Node node = nodeBuilder().clusterName("testcluster").node();
        Client client = node.client();

```
    try {

        for (int i=0; i&lt;4; i++)
        {
            data = "{" +
                "\"Time-ID\": \"" + (i+1) + "\"," +
                "\"Timestamp\": \"" + Calendar.getInstance().getTimeInMillis() + "\"" +
                "}";

            IndexResponse response = client.prepareIndex("times", "time")
                    .setSource(data)
                    .execute()
                    .actionGet();

            result += (i+1) + ")\t\t" + response.getId() + "&lt;br /&gt;" + "\t" + "TRUE" + "&lt;br /&gt;";
        }
    }
    catch (Exception ex) {
        result = "FEHLER: &lt;br /&gt;" + ex.toString();
    }

    node.close();
    return result;
}
```

public String showAll()
    {
        String result = "";

```
    Node node = nodeBuilder().clusterName("testcluster").node();
    Client client = node.client();

    SearchHits sh = client.prepareSearch("times").execute().actionGet().getHits();

    result += "Anzahl der Dateien: " + sh.getTotalHits() + "&lt;br /&gt;";

    for (int i = 0; i &lt; sh.getTotalHits(); i++)
    {
        result += "ID " + sh.getAt(i).getSource().get("Time-ID") 
                + ": \t\t" + sh.getAt(i).getSource().get("Timestamp")
                + "\t\t" + "(" + sh.getAt(i).getId() + ")"
                + "&lt;br /&gt;";
    }

    node.close();
    return result;
}
```
</description><key id="18974855">3618</key><summary>data loss in minimalistic test project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">Sonjaa</reporter><labels><label>non-issue</label></labels><created>2013-09-04T14:32:33Z</created><updated>2013-09-04T18:03:03Z</updated><resolved>2013-09-04T18:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-04T18:03:03Z" id="23810555">hey,

I think you need to 1. add this line:

```
client.admin().cluster().prepareHealth().setWaitForYellowStatus().get();
```

before you run you search:

```
    SearchHits sh = client.prepareSearch("times").execute().actionGet().getHits();
```

This makes sure all your documents are recovered and shards are up and running.
and once you iterate you can only iterate over the top N which is 10 by default and not over the total hits like this:

```
for (int i = 0; i &lt; sh.getHits().length; i++)
```

can you please use the mailing list for these kind of question issues should only be opened for bugs and other dev issues.

simon
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NodeNotConnectedException gets logged when calling cluster APIs from a client node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3617</link><project id="" key="" /><description>If we have a cluster with two or more client nodes, and we call the nodes stats (or nodes info) api connecting to one of the client nodes, we don't get back the information about the other client nodes in the cluster, and the following exception gets logged, which is rather annoying especially when calling those apis every n seconds for monitoring purposes. 

```
[2013-09-04 14:41:19,983][DEBUG][action.admin.cluster.node.info] [Vakume] failed to execute on node [D4d2wy10Qr-HKgWcSRVoUQ]
org.elasticsearch.transport.SendRequestTransportException: [Gremlin][inet[/10.20.100.106:9301]][cluster/nodes/info/n]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:203)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.start(TransportNodesOperationAction.java:172)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$300(TransportNodesOperationAction.java:102)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction.doExecute(TransportNodesOperationAction.java:73)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction.doExecute(TransportNodesOperationAction.java:43)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61) 

................................

Caused by: org.elasticsearch.transport.NodeNotConnectedException: [Gremlin][inet[/10.20.100.106:9301]] Node not connected
    at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:806)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:520)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:188)
```

The reason for this is that we prevent client-to-client node communication. A workaround would be to filter out the client nodes (but the local one) like this:

`curl -XGET 'http://localhost:9202/_nodes/master:true,data:true,_local`

This could be fixed, as mentioned in #3542, by redirecting the request (`TransportNodesOperationAction`) to either a data or master node, in case the node it's sending the request from is a client node, so that we make sure that we can gather information about all the nodes in the cluster (including client ones).
</description><key id="18972643">3617</key><summary>NodeNotConnectedException gets logged when calling cluster APIs from a client node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2013-09-04T13:54:54Z</created><updated>2016-03-01T16:59:54Z</updated><resolved>2016-03-01T16:59:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T18:35:33Z" id="142070034">As of 2.0, this no longer logs a NodeNotConnectedException, but it still doesn't return information about other client nodes.
</comment><comment author="javanna" created="2016-03-01T16:59:54Z" id="190813497">I just tested this again, and I confirm that no exception is logged anymore since 2.0 but client nodes are missing if you originally hit one of the clients nodes. Closing in favour of #16815 which is exactly the same.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Less restrictive method QueryBuilders.fuzzyQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3616</link><project id="" key="" /><description>Currently (trunk), the FuzzyQueryBuilder constructor is `FuzzyQueryBuilder(String name, Object value)`,
but QueryBuilders.fuzzyQuery restricts its parameters to `QueryBuilders.fuzzyQuery(String name, String value)`.

QueryBuilders.fuzzyQuery should also accept values of type Object (NB: useful when dealing with java.util.Date).
</description><key id="18972586">3616</key><summary>Less restrictive method QueryBuilders.fuzzyQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-09-04T13:53:57Z</created><updated>2013-10-09T09:40:50Z</updated><resolved>2013-10-09T09:40:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file></files><comments><comment>Added QueryBuilders#fuzzyQuery method that accepts value parameter as an object</comment></comments></commit></commits></item><item><title>`nested` filter within a `facet_filter` fails to return `nested` facet terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3615</link><project id="" key="" /><description>The general query form that exhibits this behavior is:

``` json
{
   "size":0,
   "facets":{
      "nested1":{
         "terms":{
            "field":"x"
         },
         "nested":"nested1",
         "facet_filter":{
            "nested":{
               "path":"nested1",
               "query":{
                  "match_all":{}
               }
            }
         }
      }
   }
}
```

Please see the curl recreation example here:

https://gist.github.com/btiernay/6378567
</description><key id="18948453">3615</key><summary>`nested` filter within a `facet_filter` fails to return `nested` facet terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">btiernay</reporter><labels /><created>2013-09-04T01:11:47Z</created><updated>2013-09-19T13:48:48Z</updated><resolved>2013-09-19T13:33:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="junjun-zhang" created="2013-09-19T01:55:40Z" id="24712691">With **"join": false** option, facet will produce desired result. When join is disabled, facet will apply to the nested documents instead of the joined root document. See https://github.com/elasticsearch/elasticsearch/issues/2606 for more details.

``` javascript
{
   "size":0,
   "facets":{
      "nested1":{
         "terms":{
            "field":"x"
         },
         "nested":"nested1",
         "facet_filter":{
            "nested":{
               "path":"nested1",
               "join":false,
               "query":{
                  "match_all":{}
               }
            }
         }
      }
   }
}
```
</comment><comment author="btiernay" created="2013-09-19T13:33:27Z" id="24738603">@junjun-zhang  Ah, very nice. Didn't know that option existed as it is undocumented. Thanks :)
</comment><comment author="junjun-zhang" created="2013-09-19T13:48:48Z" id="24739716">Yes, this should be documented, and maybe **"join":false** should be the default when **join** option is omitted. I assume more users expect nested facets work on nested docs instead of joined root docs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add "remove *" support to _alias </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3614</link><project id="" key="" /><description>Sometimes it would be very handy to remove an aliases from all indexes. This is the case when implementing "index symlink" behaviour. For example, when creating an index per month, it would be very convenient to perform the following:

``` bash
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "remove" : { "index" : "*", "alias" : "current" } },
        { "add" : { "index" : "august-2013", "alias" : "current" } }
    ]
}'
```

This prevents the application from having to find the previous current index, get the associated aliases, and perform the remove operation.

Additionally, it would be nice to remove all aliases from an index as well:

``` bash
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "remove" : { "index" : "august-2013", "alias" : "*" } }
    ]
}'
```

And of course, you should be able to combine them as well:

``` bash
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "remove" : { "index" : "*", "alias" : "*" } }
    ]
}'
```
</description><key id="18933504">3614</key><summary>Add "remove *" support to _alias </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">btiernay</reporter><labels /><created>2013-09-03T19:31:23Z</created><updated>2014-08-08T16:40:52Z</updated><resolved>2014-08-08T16:40:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-09-03T19:34:20Z" id="23740386">Thanks for the reminder! That was an excellent suggestion from training. Will check it out.
</comment><comment author="clintongormley" created="2014-08-08T16:40:52Z" id="51627106">This has been implemented, you can do:

```
DELETE /*/_alias/baz
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting can return excerpt with no highlights</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3613</link><project id="" key="" /><description>You can configure the highlighting api to return an excerpt of a field
even if there wasn't a match on the field.

The FVH makes excerpts from the beginning of the string to the first
boundary character after the requested length or the boundary_max_scan,
whichever comes first.  The Plain highlighter makes excerpts from the
beginning of the string to the end of the last token before the requested
length.

Closes #1171
</description><key id="18930201">3613</key><summary>Highlighting can return excerpt with no highlights</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-03T18:31:19Z</created><updated>2014-06-29T01:19:42Z</updated><resolved>2013-10-24T12:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-03T18:32:39Z" id="23735978">Please don't merge this yet because I think it might not work with array fiends but feel free to comment.
</comment><comment author="nik9000" created="2013-09-03T18:45:38Z" id="23736953">I added a test for highlighting multivalued fields but the original code seemed to work just fine.
</comment><comment author="javanna" created="2013-09-03T18:48:41Z" id="23737178">Wow, that was fast! Will have a look at it soon, thanks a lot!
</comment><comment author="nik9000" created="2013-09-03T18:51:10Z" id="23737322">It was only fast because I was just in Elasticsearch land and figured this'd be good to do right away before Java started to seep out of my skull.
</comment><comment author="nik9000" created="2013-09-03T18:51:55Z" id="23737383">And because I'm still running all the integration tests in the background, making my poor laptop go crazy.
</comment><comment author="javanna" created="2013-09-06T16:25:53Z" id="23952315">I have a couple of comments, beyond the name as discussed above.

I'm not sure about the difference between "show" and the third option. I think we should always be returning a single snippet in this case and only its size should be configurable (we could reuse the fragments size for that), thus I tend to think we need only two options: `show` and `hide`. What am I missing?

I'm also concerned about the quality of the snippets we are going to return here. @nik9000 have you had the chance to test this a bit with both highlighters? Where do they take the snippet from? What do the snippet look like?

I was wondering if it's worth to add an option to return the first sentence, extracted using a break iterator (would not play well with fields containings html tags though).

What do you think?
</comment><comment author="nik9000" created="2013-09-06T17:53:29Z" id="23957675">&gt; I'm not sure about the difference between "show" and the third option. I think we should always be returning a single snippet in this case and only its size should be configurable (we could reuse the fragments size for that), thus I tend to think we need only two options: show and hide. What am I missing?

I'd planned to just set no_results (or whatever we name it) to the same value as `size_of_fragments` but didn't think it'd hurt to implement the feature @ofavre described it.  I can see why he wanted it that way - if you play to return multiple snippets to the user separated by ...s then `show` will give you a snippet of a similar size.  Rather than take away the option to specify a character could I'd just take away `show` and let the client work it out.

&gt; I'm also concerned about the quality of the snippets we are going to return here. @nik9000 have you had the chance to test this a bit with both highlighters? Where do they take the snippet from? What do the snippet look like?

The snippets start at the beginning of the document and go for whatever length is configured.  The FVH breaks at the configured characters but the Plain highlighter just chops the string off at the character count, even mid word.  I'm happy to improve on the Plain behaviour if someone will give me a hint how.  I'll use this with FVH and it already is good enough for me.

&gt; I was wondering if it's worth to add an option to return the first sentence, extracted using a break iterator (would not play well with fields containings html tags though).

I wouldn't use it but understand the idea.  I'm more interested in making sure the snippets are of a similar size to the normal ones.
</comment><comment author="javanna" created="2013-09-12T07:54:49Z" id="24300850">Thanks @nik9000 for your insight, makes definitely sense. I plan to have a look at this soon, want to make sure that we generate nice fragments, and need to think a bit more about the BreakIterator idea. But we'll definitely get this in!
</comment><comment author="nik9000" created="2013-09-16T13:15:33Z" id="24508605">Amended that last commit to remove some code that shouldn't have been included.
</comment><comment author="nik9000" created="2013-09-25T13:39:09Z" id="25086287">Since no one has said anything in a couple weeks on the subject, I'm going to change the name of the field to `no_matches` because it is my favourite.  

@javanna, would it be possible to get this in without the BreakIterator stuff just so I have fewer pull requests hanging over my head?  We could open another issue to work first sentence style behaviour in.

I'm going make this change as another commit and then rebase all three.  I'll squash them into one when debate is done.
</comment><comment author="nik9000" created="2013-09-25T15:04:34Z" id="25093745">I _think_ I'm done with this.  Let me know if having it as multiple commits is harder to review and I'll squish it.  I imagine it'll be easier because a few of you have already reviewed the first two commits.  Nothing exciting happened when rebasing them.
</comment><comment author="javanna" created="2013-09-26T22:42:23Z" id="25210065">@nik9000 sorry for keeping you on hold, this is definitely in my todo list. The main issue is not the break iterator but the plain highlighter behaviour IMO, which is something that we need to figure out and I need some time to dig it. Thanks a lot for your help, I'll get back to this as soon as I can!
</comment><comment author="nik9000" created="2013-09-27T22:33:12Z" id="25282069">@javanna I'm most of the way through a commit that cleans up the plain highlighter behavior and normalizes some of the behavior when the field is empty.  I'll have something useful on Monday.
</comment><comment author="nik9000" created="2013-09-28T19:04:48Z" id="25306295">I've made the PlainHighlighter split the string on a term boundary.  Right now it just just iterates over the terms, finds the first term that ends after the end of the excerpt and then builds the excerpt from the beginning of the string to the end of the previous term (the term before the first one after the end of the excerpt.)  I could probably do a better job picking the term but this is pretty simple and gives much better output. 

Sorry if github just blaster everyone subscribed to this five separate emails.  I kept finding problems with that latest commit, amending it, and then pushing it.
</comment><comment author="nik9000" created="2013-09-28T19:08:18Z" id="25306365">Also I added tests for three corner cases:
1.  `no_matches` when the field is an empty string: returns an empty string
2.  `no_matches` when the field is missing: highlight result isn't returned
3.  `no_matches` when the field is unmapped: highlight result isn't returned

This means that setting `no_matches` doesn't prevent Elasticsearch from omitting that field from highlighting, it just makes it less likely.
</comment><comment author="nik9000" created="2013-10-02T20:40:22Z" id="25574604">I had to rebase so I took the opportunity to squash the commits into one.
</comment><comment author="nik9000" created="2013-10-03T15:41:57Z" id="25631662">Fixed comments on the commit and rebased.  I'm not happy with how github hides those comments....
</comment><comment author="clintongormley" created="2013-10-04T12:02:57Z" id="25693757">Hi @nik9000 

Thanks for all the work you've done on this.  I've had a look at the docs in the PR and I would suggest changing the functionality slightly. Forgive me if I've misunderstood any of the existing functionality:

Instead of `no_match` accepting `show`, `hide` or an integer, and using that to calculate the length by multiplying it with the `fragment_size`, a much simpler implementation is to have a single `no_match_size` parameter which accepts an integer (the length of the string to return when there are no matches), and have that value default to zero.

It makes the API much simpler to understand.
</comment><comment author="nik9000" created="2013-10-04T16:58:18Z" id="25714080">&gt; no_match_size parameter which accepts an integer (the length of the string to return when there are no matches), and have that value default to zero.

I have no problem with this.  Its what I'd have implemented if it I hadn't found #1171.  I'll make the change in a bit.
</comment><comment author="nik9000" created="2013-10-08T13:52:17Z" id="25891296">&gt; Instead of no_match accepting show, hide or an integer, and using that to calculate the length by multiplying it with the fragment_size, a much simpler implementation is to have a single no_match_size parameter which accepts an integer (the length of the string to return when there are no matches), and have that value default to zero.

Done and rebased.
</comment><comment author="nik9000" created="2013-10-22T14:02:47Z" id="26805182">Incorporated changes from more comments and rebased.
</comment><comment author="jpountz" created="2013-10-22T15:43:23Z" id="26814797">Left one minor comment as well, otherwise +1 to push!
</comment><comment author="javanna" created="2013-10-24T08:56:26Z" id="26976056">Thanks @nik9000 for updating your commit, I'm going to push this!
</comment><comment author="javanna" created="2013-10-24T12:58:28Z" id="26989652">Merged! Thanks a lot @nik9000 , great job!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Beautify SuggestSearchTests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3612</link><project id="" key="" /><description>SuggestSearchTests had tons of duplicate code and didn't use all of the
fancy new integration test helper method.  I've removed a ton of duplicate
code and used as many of the nice test helper method I could think of.

Closes #3611
</description><key id="18915059">3612</key><summary>Beautify SuggestSearchTests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-03T15:31:03Z</created><updated>2014-07-16T21:52:27Z</updated><resolved>2013-09-09T07:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-09T07:30:44Z" id="24049727">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SuggestSearchTests should use all the fancy new integration test helper methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3611</link><project id="" key="" /><description>The last time I contributed some phrase suggester code I used an example from SuggestSearchTests to craft a new test.  @s1monw rightly told me there were nice new helper methods I should be using rather than the examples in the test right now.  So I cleaned up the tests in that file so they all use the fancy new methods so the next person to touch the file doesn't get confused.
</description><key id="18914959">3611</key><summary>SuggestSearchTests should use all the fancy new integration test helper methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-03T15:29:37Z</created><updated>2013-09-09T07:29:58Z</updated><resolved>2013-09-09T07:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-09-03T15:32:07Z" id="23721588">So I'm really sorry that pull request is so big.  I usually try to keep them small.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/integration/AbstractSharedClusterTest.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/SuggestSearchTests.java</file></files><comments><comment>Beautify SuggestSearchTests.</comment></comments></commit></commits></item><item><title>BalancedShardsAllocator prematurely modifies `unassigned` shards list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3610</link><project id="" key="" /><description>BalancedShardsAllocator modifies the unassigned shards list during initialization which is used by AllocationDeciders. Yet, premature modification can cause unintended throtteling of primary allocation which causes cluster to go in a red state until recovery / relocation has caught up to not throttle the allocation anymore. This is a very rare scenario that will only have a prominent effect on full cluster restarts or similar heavy weight operations.
</description><key id="18913301">3610</key><summary>BalancedShardsAllocator prematurely modifies `unassigned` shards list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-03T15:02:07Z</created><updated>2013-09-03T15:15:54Z</updated><resolved>2013-09-03T15:15:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file><file>src/test/java/org/elasticsearch/test/unit/cluster/routing/allocation/PreferPrimaryAllocationTests.java</file></files><comments><comment>Prevent ShardAllocator to modify the unassigned while running allocations</comment></comments></commit></commits></item><item><title>It'd be really cool highlighter could return a summary of a field even if it has nothing to highlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3609</link><project id="" key="" /><description>It'd be really cool highlighter could return a fragment from a field even if it has nothing to highlight.  I'd want the fragment to start from the beginning for the field and otherwise be a normal highlight fragment.

I was thinking you'd control it like so:

``` javascript
{
    "query" : {...},
    "highlight" : {
        "order" : "score",
        "fields" : {
            "content" : {
                "fragment_size" : 150,
                "number_of_fragments" : 3,
                "build_fragment_if_no_matches": true
            }
        }
    }
}
```

with build_fragment_if_matches default to false.

While the client application could build the fragment itself it is unlikely it'd be able to do as good a job as Elasticsearch especially at language support.

Returning a fragment from the beginning of the field when there aren't matches is pretty common for search engine style applications.  Google does it.
</description><key id="18911698">3609</key><summary>It'd be really cool highlighter could return a summary of a field even if it has nothing to highlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-09-03T14:35:41Z</created><updated>2013-09-03T14:48:20Z</updated><resolved>2013-09-03T14:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-03T14:40:00Z" id="23717405">Sounds similar to #1171 .
</comment><comment author="nik9000" created="2013-09-03T14:45:51Z" id="23717864">That is what I get for not searching before filing an issue.
</comment><comment author="javanna" created="2013-09-03T14:48:20Z" id="23718053">No worries, I liked your proposal though, which was a slightly different solution for the same problem. But the problem is already known. Thanks anyway!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Compressed store not thread safe</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3608</link><project id="" key="" /><description>LZFCompressedIndexOutput uses ning.compress ChunkEncoder and BufferRecycler. ChunkEncoder uses BufferRecycler internally as well. ByfferRecycler.instance() returns a thread-local.
The thread that creates LZFCompressedIndexOutput is not the same thread as the one that closes it so in some cases the same BufferRecycler will release buffers from different threads causing an NPE which makes a shard go offline:

[2013-08-27 13:26:05,337][WARN ][action.deletebyquery     ] [elasticsearch-node] Failed to perform deleteByQuery/shard on replica [index][0]
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-node][inet[/10.10.10.10:9300]][deleteByQuery/shard/replica]
Caused by: org.elasticsearch.index.engine.RefreshFailedEngineException: [search-enca][0] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:787)
    at org.elasticsearch.index.engine.robin.RobinEngine.refreshVersioningTable(RobinEngine.java:966)
    at org.elasticsearch.index.engine.robin.RobinEngine.delete(RobinEngine.java:728)
    at org.elasticsearch.index.shard.service.InternalIndexShard.deleteByQuery(InternalIndexShard.java:385)
    at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnReplica(TransportShardDeleteByQueryAction.java:106)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:254)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:240)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.compress.BufferRecycler.releaseEncodeBuffer(BufferRecycler.java:68)
    at org.elasticsearch.common.compress.lzf.ChunkEncoder.close(ChunkEncoder.java:113)
    at org.elasticsearch.common.compress.lzf.LZFCompressedIndexOutput.doClose(LZFCompressedIndexOutput.java:63)
    at org.elasticsearch.common.compress.CompressedIndexOutput.close(CompressedIndexOutput.java:184)
    at org.elasticsearch.index.store.Store$StoreIndexOutput.close(Store.java:640)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:141)
    at org.apache.lucene.index.FieldsWriter.close(FieldsWriter.java:139)
    at org.apache.lucene.index.StoredFieldsWriter.flush(StoredFieldsWriter.java:55)
    at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:59)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:581)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3587)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3552)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:450)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:399)
    at org.apache.lucene.index.DirectoryReader.doOpenFromWriter(DirectoryReader.java:413)
    at org.apache.lucene.index.DirectoryReader.doOpenIfChanged(DirectoryReader.java:432)
    at org.apache.lucene.index.DirectoryReader.doOpenIfChanged(DirectoryReader.java:375)
    at org.apache.lucene.index.IndexReader.openIfChanged(IndexReader.java:508)
    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:109)
    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:57)
    at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:137)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:768)
    ... 10 more  
</description><key id="18910641">3608</key><summary>Compressed store not thread safe</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">francoisforster</reporter><labels /><created>2013-09-03T14:16:58Z</created><updated>2014-08-08T16:39:15Z</updated><resolved>2014-08-08T16:39:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-03T14:18:05Z" id="23715612">we could chase it, but its no longer used in 0.90, so I suggest using 0.90.
</comment><comment author="francoisforster" created="2013-09-03T14:21:57Z" id="23715909">Yes, this is 0.20.6. I guess it's time to upgrade!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Term facet accepts an array for "order", yet, only last value is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3607</link><project id="" key="" /><description>the TermsFacetParser doesn't treat the case of receiving an array/object for fields that don't support it. it should maybe throw an QueryParsingException around 
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetParser.java#L102-L119
in the case of being an array/object and not matching one of the possible fields in those blocks.
for this snippet for example

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/bar/1 -d '{"id":1, "tag":"a1 a2 a3 a4 a5"}'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{"id":2, "tag":"b1 b2 b3 b4 b5"}'
curl -XPUT http://localhost:9200/foo/bar/3 -d '{"id":3, "tag":"a1 a2 a3 a4 a5 b1 b2 b3 b4 b5"}'
curl -XPOST http://localhost:9200/foo/bar/_search -d '{ "query": { "match_all":{} }, "facets": { "tags": { "terms": { "field": "tag", "size":5, "order":["count","term"] } } }, "size":0 }'
```

ES will accept the request without problems(even though multiple values are passed for the order) and will just use "term" as order, since inside the parser at some moment it wont match any of the possibilities, and will just skip to the next token.
it would be nice getting some parsing error for that, since it might be a little confusing.

this ticket actually started as a feature request to accept multiple values for order inside facets, but i suppose with the whole facet module being rewritten, i won't have much luck with it.
- i also checked some other parsers and I think this could actually happen in a few different places.
</description><key id="18906498">3607</key><summary>Term facet accepts an array for "order", yet, only last value is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-09-03T12:57:57Z</created><updated>2014-08-08T16:38:59Z</updated><resolved>2014-08-08T16:38:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T16:38:59Z" id="51626914">It does throw an error these days, and yes, facets are deprecated :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolating not as part of a cluster for high-rate operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3606</link><project id="" key="" /><description>We run a large data shop, with ElasticSearch 0.90 serving as our main search engine (+ many plugins). We also use the percolator functionality at a very high rate (tens of thousands per minute, against many queries).

Previously we were using the old percolator implementation, and after several tries we saw separating the percolation operations from the actual search cluster makes much more sense to us. This way we can have servers dedicated to percolation and make sure we handle all we need in a timely fashion; it also means we can just drop more servers during peaks and take them down later without making sure data isn't replicated to them.

First we tried doing percolation on a separate cluster, but since each percolating node is practically autonomous - it just gets a document, percolates and registers the result with our backend - we ended up having a self-contained jar that does all that. We also had issues with discovery over S3, which are no longer an issue when we run a 1-node cluster, as stupid as it may sound.

We are also using a custom PercolatorExecutor implementation - it implements Highlighting using the ParsedDocument as the data source and also has some smarts with regards to query filtering. In order to do this, I had to copy-paste code and create a new Percolator Moudle and using some hack to catch the singleton PE implementation. It is a very ugly hack and quite hard to keep up to date with updates.

In case you'd ask, the reason why we still use ES's percolation and not a custom built MemoryIndex implementation is the compatibility we need with the ES index structure we have, and analyzer and QueryParsing conventions.
## TL;DR

If there could be a way to run a stand-alone, cluster-less PercolatorExecutor instance, enjoying all the benefits of the Mappers and QueryParsers but without the intense memory requirements and startup times, that would be really great.

Once this PE instance is initialized, it would be great if there was a way to actually get it using some Java API call - even raw access to the IoC container could help. Alternatively, access to the MapperService, QP instances etc.

Additionally, going forward it would really help having the PercolatorExecutor itself extensible - in particular the ability to amend the queries feed using the Java API (for example for registering, updating and removing of queries using a mechanism other than the ES API), and to have selectors on those queries. I'm aware the percolator query can accept a query with the doc to percolate, but I actually couldn't get it to work for some reason (and got no answer on the mailing list)

Will be happy to discuss the small details as well

Cheers
</description><key id="18903815">3606</key><summary>Percolating not as part of a cluster for high-rate operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-09-03T11:56:06Z</created><updated>2013-09-25T10:18:07Z</updated><resolved>2013-09-25T10:18:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-09-03T12:25:48Z" id="23708580">Regarding being able to run standalone percoaltor service, you can start a local node in your Java program, and just use that for embedded percolations. I would though stress that by going embedded you won't be able to use the built in distributed aspects of percolation that are coming in 1.0.

Regarding being able to extend the percolation, we have made some progress there in 1.0, as part of the work of distributed percolation (that now also supports highlighting). I will say though that pluggability of percolator is less of a priority for us, as we typically prefer to add features that can then be used for all the userbase of ES without the need for extensions (like highlighting, which we added in 1.0).

The selectors part work. We won't add pluggability to things that are features in ES.

Btw, if your needs end up being really custom, you can always implement your own "percolator service", ES is open for you to do so.
</comment><comment author="martijnvg" created="2013-09-03T12:32:09Z" id="23708893">@synhershko Have you seen the recent changes that have been done in master regarding the percolator (#3173, #3574, #3506, #3488)? Like @kimchy said making a standalone percolator doesn't make much sense now since the percolator is now distributed.
</comment><comment author="synhershko" created="2013-09-03T14:47:46Z" id="23718006">Re standalone - this is exactly what we did (went embedded and implemented our own service). My problem is with the amount of init work and memory this still requires, network and FS operations we don't really need etc. I haven't previously set local to true - this could help, but still the only thing we really need is the things required for the PercolatorExecutor instance to work correctly.

The concept of percolation within a cluster obviously isn't targeted for high rate of percolation operations - otherwise I can't really understand how it is at all useful, even when it is done distributed. In scale, just the serialization and network traffic involved can be considered a waste, and you'd be better dedicating nodes to do percolation alone.

Is there a way to selectively init stuff? say, if I only wanted the MapperService and QueryParserService and not the whole package?

Or maybe a way to get to the IoC container and ask it for an instance? couldn't find a way to do that

We don't mind about losing distributed features - as I said each node is autonomous and I would rather have it as light and concentrated at percolation as possible.

@martijnvg I have seen all of those - like I said this issue is about asking for a way to work with an instance of PercolatorExecutor, or to easily initialize and use a custom one
</comment><comment author="kimchy" created="2013-09-03T14:53:14Z" id="23718494">setting local to true will help remove any networking (you might need to also disable http). I want proof that you really see memory overhead then, I don't buy it.

In any case, my view is that focusing on allowing to run those services in their own is not within the scope of elasticsearch. Too esoteric. It can be done though, though you will need to play with it, and I really think running an embedded node is more than enough.
</comment><comment author="synhershko" created="2013-09-03T15:02:05Z" id="23719215">I hear you. Let me see how big it gets with the lowest configs.
</comment><comment author="kimchy" created="2013-09-03T15:03:16Z" id="23719310">also, I completely disagree regarding your note about high rate percolation. Your embedded case will only work till a given number of percolation queries limit, in which case you will need to partition, and you will end up implementing yourself what we do in distributed percolation. 

also, you will end up building that embedded node as a remote service, and you will need anyhow to serialize the response from it. And what happens with HA, .... I don't really need answers for those questions, you can go ahead and have fun with an embedded node and do local percolations (in 0.90 or master), I do think that you might find yourself needing all the features ES gives you.
</comment><comment author="synhershko" created="2013-09-03T15:14:16Z" id="23720158">Hey man, no offense intended. I really think the percolation concept is great and beautifully implemented, but what we saw when we tried running it on our search cluster is a huge bottleneck - with high rate percolation, high rate indexing and heavy searches all combined on the same cluster. This is why we moved to dedicated, individual, percolator nodes, and are now using some sort of profiling on queries to eliminate irrelevant ones in advance as an optimization.
</comment><comment author="kimchy" created="2013-09-03T15:31:02Z" id="23721488">understood, I think the new distributed percolation will help a lot, and I agree that sometimes it makes sense to separate to 2 clusters, or use dedicated nodes for percolation, and dedicated ones for search in the same cluster.

I am simply sharing my concern that I think you will end up needing to implement a few things we have in ES. God knows we did :), as evident with the rewrite to properly have distributed percolation in master...
</comment><comment author="synhershko" created="2013-09-25T10:18:07Z" id="25075410">The resources used when moving to local JVM and disabling HTTP seem moderate. I'll leave the option for going back for sharded percolation operations as you suggested. Closing this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove o.e.common.UUID and replace it with a simplified version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3605</link><project id="" key="" /><description /><key id="18900771">3605</key><summary>Remove o.e.common.UUID and replace it with a simplified version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-03T10:40:25Z</created><updated>2014-06-13T06:50:21Z</updated><resolved>2013-09-05T07:47:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-05T07:47:48Z" id="23849162">merged - github missed it....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tech Screen Grade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3604</link><project id="" key="" /><description>aaa
</description><key id="18892127">3604</key><summary>Tech Screen Grade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justforspam8888</reporter><labels /><created>2013-09-03T06:45:10Z</created><updated>2013-09-03T07:01:00Z</updated><resolved>2013-09-03T07:01:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add 'min_input_len' to completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3603</link><project id="" key="" /><description>Restrict the size of the input length to a reasonable size otherwise very
long strings can cause StackOverflowExceptions deep down in lucene land.
Yet, this is simply a saftly limit set to `50` UTF-16 codepoints by default.
This limit is only present at index time and not at query time. If prefix
completions &gt; 50 UTF-16 codepoints are expected / desired this limit should be raised.
Critical string sizes are beyone the 1k UTF-16 Codepoints limit.

Closes #3596
</description><key id="18872680">3603</key><summary>Add 'min_input_len' to completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-09-02T16:35:18Z</created><updated>2014-07-16T21:52:28Z</updated><resolved>2013-09-03T08:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-09-03T08:28:01Z" id="23696893">pushed -- github doesn't realize it if you do a rebase on the master :(
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Separate parsing impl from setter in SearchParseElement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3602</link><project id="" key="" /><description>Currently all SearchParseElement classes are implemented like this:

public class HighlighterParseElement implements SearchParseElement {
   @Override
       public void parse(XContentParser parser, SearchContext context) throws Exception {
      // actual parsing logic
      context.highlight(new SearchContextHighlight(fields));
   }
}

For some (advanced) uses I want to be able to parse individual elements myself by relying on the actual core implementation, and to do that I need to use a hack like this:

```
                        HighlighterParseElement tmp = new HighlighterParseElement();
                        SearchContext ctx = new SearchContext(0, null, null, null, null, null, null, null);
                        tmp.parse(parser, ctx);
                        searchContextHighlight = ctx.highlight();
```

Just for the sake of parsing. It would be nice if the actual parsing logic could be separated from the call to the setter, something like this:

```
@Override
public void parse(XContentParser parser, SearchContext context) throws Exception {
    try {
        context.highlight(parseImpl(parser));
    }
    catch (IllegalArgumentException ex)
    {
        throw new SearchParseException(context, "Highlighter global preTags are set, but global postTags are not set");
    }
}

public static SearchContextHighlight parseImpl(XContentParser parser) throws Exception {
  // actual impl
```

   }
</description><key id="18867351">3602</key><summary>Separate parsing impl from setter in SearchParseElement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2013-09-02T14:22:39Z</created><updated>2014-09-11T10:16:47Z</updated><resolved>2014-07-28T13:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2014-06-24T00:19:50Z" id="46918508">Hey ES people is it something you'd consider or should I close this one?
</comment><comment author="s1monw" created="2014-07-01T12:58:39Z" id="47652356">@synhershko I think we should do this - do you wanna come up with a PR?
</comment><comment author="synhershko" created="2014-07-01T13:06:39Z" id="47653147">@s1monw will do
</comment><comment author="synhershko" created="2014-07-06T21:19:22Z" id="48127448">Hey @s1monw please see https://github.com/elasticsearch/elasticsearch/pull/6758

I added one commit changing only `HighlighterParseElement` for now, please review and if my approach makes sense (the re-throwing logic etc) I'll go ahead and apply this all over the place
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/source/FetchSourceParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java</file></files><comments><comment>[QUERY] Separate parsing impl from setter in SearchParseElement</comment></comments></commit></commits></item><item><title>NullPointerException when closing an already closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3601</link><project id="" key="" /><description>not really an important issue, but:

```
curl -XPOST http://localhost:9200/foo
curl -XPOST http://localhost:9200/foo/_close
curl -XPOST http://localhost:9200/foo/_close
```

gives  {"error":"NullPointerException[null]","status":500}

opening an already opened index works ok, with a {"ok":true,"acknowledged":true} so I guess I would expect something like that for the redundant close operation.
</description><key id="18853538">3601</key><summary>NullPointerException when closing an already closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-09-02T08:24:06Z</created><updated>2013-09-02T09:59:46Z</updated><resolved>2013-09-02T09:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/state/OpenCloseIndexTests.java</file></files><comments><comment>Fixed close index when the index is already closed</comment></comments></commit></commits></item><item><title>Clean up nodenames</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3600</link><project id="" key="" /><description>Some of the nodenames in [names.txt](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/resources/config/names.txt) are not very name-like. 

General problems:
- Weird capitalization conventions
- Lastname, Firstname formatting
- Duplicates
- General weirdness: "Cody Mushumanski gun Man aka: the hunter"
</description><key id="18845430">3600</key><summary>Clean up nodenames</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ohnorobo</reporter><labels /><created>2013-09-02T01:01:47Z</created><updated>2014-02-13T15:31:49Z</updated><resolved>2014-02-13T15:31:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T15:31:49Z" id="34989632">This can be closed, the related PR (#3263) was pulled in a while ago.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error while sorting on fields which are not present for some of the documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3599</link><project id="" key="" /><description>This issue is experienced on version 0.90.3
Searching in two indexes with sort on two fields - _subtype and fileSize. _subtype field is present for all indices, fileSize - is present only in the mapping of one index. 'ignore_unmapped' is set to true for both of the fields.  

```
{
    "sort": [
        {
            "_subtype": {
                "order": "desc",
                "ignore_unmapped": true
            }
        },
        {
            "fileSize": {
                "order": "asc",
                "ignore_unmapped": true
            }
        }
    ],

    "query": {
        "query_string": {
            "query": "_parents.$id:5223195f93f46463974aa1f4"
        }
    }
}
```

The result for JSON search is:

```
{
    error: ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ArrayIndexOutOfBoundsException[1];
    status: 500
}
```

When searchhing with Java API:

```
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [query], [reduce] 
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:172) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:150) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:144) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:176) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:144) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.run(TransportSearchDfsQueryThenFetchAction.java:131) ~[elasticsearch-0.90.3.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]
    at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
    at org.elasticsearch.search.controller.ShardFieldDocSortedHitQueue.lessThan(ShardFieldDocSortedHitQueue.java:118) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.search.controller.ShardFieldDocSortedHitQueue.lessThan(ShardFieldDocSortedHitQueue.java:35) ~[elasticsearch-0.90.3.jar:na]
    at org.apache.lucene.util.PriorityQueue.downHeap(PriorityQueue.java:247) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]
    at org.apache.lucene.util.PriorityQueue.pop(PriorityQueue.java:184) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]
    at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:281) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerExecuteFetchPhase(TransportSearchDfsQueryThenFetchAction.java:177) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:170) ~[elasticsearch-0.90.3.jar:na]
    ... 8 common frames omitted
```

Interesting thing is that if I change fileSize field into some totally diffrerent field, which is not present in any index, for example fileSizeAAA, I do not get this error. If I omit one the _subtype field, and leave only fileSize field, then I get another error:

```
{

    error: ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[org.apache.lucene.search.ScoreDoc cannot be cast to org.apache.lucene.search.FieldDoc];
    status: 503

}
```
</description><key id="18781224">3599</key><summary>Error while sorting on fields which are not present for some of the documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">avpet</reporter><labels><label>non-issue</label></labels><created>2013-08-30T10:36:42Z</created><updated>2015-04-07T13:02:38Z</updated><resolved>2013-09-02T12:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-09-02T10:58:16Z" id="23653559">Thanks for reporting this issue, I was able to write a unit test that reproduces the issue you described, I will look into it!
</comment><comment author="jpountz" created="2013-09-02T12:04:19Z" id="23656373">@avpet Unfortunately, I don't think this issue should be fixed. Cross index queries expect the mappings of the documents to sort to be very similar. For example, you can't sort on a field if it is mapped as a string on one index and as an integer on another index.

Here, the problem is that responses for individual shards must return the values which have been used for sorting, and this is not possible for an unmapped field: Elasticsearch can't get the type of the value it should return and `null` is not an option since it is not supported for numeric fields.

A work-around would be to explicitely add a mapping for the `fileSize` field on all indices you are searching on so that you don't have to set `ignore_unmapped`. On my end, I will update the documentation to explain the limitations of `ignore_unmapped` for cross-indices search.
</comment><comment author="avpet" created="2013-09-02T12:41:42Z" id="23657924">Thank you very much, Adrien, the workaround is OK for us. Thanks for letting us know about the limitation of the `ignore_unmapped` field. 
</comment><comment author="matthiasg" created="2014-02-13T14:31:32Z" id="34982907">I am not sure I understand. Sorting the resulting data based on a field across many indices is a typical use case for us. `ignore_unmapped:true` and `missing '_last' / '_first'` seems like the ideal way to handle that ...  why does it work across some indices but not all ?

btw: the documentation at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html doesnt seem to indicate any limitation in `ignore_unmapped` when used on http://&lt;server&gt;/_search
</comment><comment author="avpet" created="2014-02-13T15:12:33Z" id="34987083">Yes,  matthiasg, looks like the documentation has yet to be updated and to be honest, I have not understood the explanation fully, just realized, that it cannot be done due to some architectural issues in Elasticsearch codebase. We bypassed this limitation by the adding the field to all indices involved in the sort, which is not very clean solution.
</comment><comment author="avpet" created="2015-04-07T13:02:38Z" id="90541333">It looks like the problem has been fixed in the version 1.4. In addition to `ignore_unmapped` parameter, the parameter called `unmapped_type` has been added. In case of cross-index query, you have to specify the type of this field have in one of the indexes. Please, see [Missing Values](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_missing_values) section. This way it works for cross-index queries, which involve fields exisiting only in some of the index mappings.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide APT repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3598</link><project id="" key="" /><description>The deb package is great, but it would be even better if there were an APT repository. Probably the easiest way to do this is to setup a PPA:

https://help.launchpad.net/Packaging/PPA
</description><key id="18774837">3598</key><summary>Provide APT repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels /><created>2013-08-30T07:40:51Z</created><updated>2013-08-30T16:20:01Z</updated><resolved>2013-08-30T16:20:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-08-30T11:20:55Z" id="23554855">Looks like a dupe of Issue #3286.
</comment><comment author="benmccann" created="2013-08-30T16:20:01Z" id="23572584">Yep, thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch Apt Repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3597</link><project id="" key="" /><description>An official apt repository would be really, very nice.

At present we're hesitant to install any software that we can't automate updates through the package manager.

Currently going to the elasticsearch website and downloading the latest deb from a web page just isn't suitable.
</description><key id="18770671">3597</key><summary>Elasticsearch Apt Repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sammcj</reporter><labels /><created>2013-08-30T04:35:09Z</created><updated>2013-08-30T04:36:58Z</updated><resolved>2013-08-30T04:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sammcj" created="2013-08-30T04:36:58Z" id="23540232">Opps, sorry it seems this is a duplicate of https://github.com/elasticsearch/elasticsearch/issues/2757
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>StackOverflow in completion suggester when using long input strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3596</link><project id="" key="" /><description>I ran into a StackOverflowError when using the new completion suggestions on a long string.  I reproduced this using the latest build of master and a random string:

```
[2013-08-29 10:10:36,691][DEBUG][action.index             ] [Summers, Christopher] [music][4], node[MKSIixv6QQaAu-ws3rs9_g], [P], s[STARTED]: Failed to execute [index {[music][song][10], source[{
    "name" : "long string",
    "suggest" : { 
        "input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
        "output": "long string",
        "payload" : { "artistId" : 2310 }, "weight": 1
    }
}]}]
java.lang.StackOverflowError
    at java.util.HashMap.addEntry(HashMap.java:856)
    at java.util.HashMap.put(HashMap.java:484)
    at java.util.HashSet.add(HashSet.java:217)
    at org.apache.lucene.util.automaton.SpecialOperations.getFiniteStrings(SpecialOperations.java:244)
    at org.apache.lucene.util.automaton.SpecialOperations.getFiniteStrings(SpecialOperations.java:259)
    at org.apache.lucene.util.automaton.SpecialOperations.getFiniteStrings(SpecialOperations.java:259)
    at org.apache.lucene.util.automaton.SpecialOperations.getFiniteStrings(SpecialOperations.java:259)
    at org.apache.lucene.util.automaton.SpecialOperations.getFiniteStrings(SpecialOperations.java:259)...
```
</description><key id="18744136">3596</key><summary>StackOverflow in completion suggester when using long input strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-29T17:17:23Z</created><updated>2014-05-04T16:54:32Z</updated><resolved>2013-09-03T08:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-29T20:40:57Z" id="23521717">dude this is a long string... yet, I agree we need to protect the users from adding too long strings. I think we should just cut off at some point. Not sure what is a good default but maybe 30 chars? who types 30 chars in a suggest env. :)
</comment><comment author="nik9000" created="2013-08-29T20:55:52Z" id="23522763">You might want to let us configure the limit and advise on stack issues.  We don't use the completion suggester yet but I can see users copying and pasting longish strings into my suggestion environment and expecting it to work.  We limit the field to 255 characters which seems a bit long to me but I suspect would need more than 30.  100, maybe?

Also, I bet the data structure become a lot less awesome with long strings.
</comment><comment author="s1monw" created="2013-08-29T21:02:23Z" id="23523253">&gt; You might want to let us configure the limit and advise on stack issues

yeah length should be configurable.

&gt;  We limit the field to 255 characters which seems a bit long to me but I suspect would need more than 30. 100, maybe?

I don't think this is needed, even if you paste a longish string in the search bar you will get a suggestion but only for the top N characters. I am not sure if we should provide any if you exceed the limit. your query might be specific enough?
</comment><comment author="mattweber" created="2013-08-29T21:13:08Z" id="23524030">Yea, I agree this is long.  This came up while indexing some user generated data and not doing a length check.  We are using 30 character max now and that works great.
</comment><comment author="nik9000" created="2013-08-29T21:14:03Z" id="23524082">Admittedly this is kind of lame, but what if someone pasted `Wikipedia:List of Wikipedians by featured article nominations` into the search bar and we limited it to 30 characters?  We'd end up searching for `Wikipedia:List of Wikipedians` which would drive people wild.  I'm not sure what the right answer is.  Maybe switch to a phrase prefix search if the string is too long.
</comment><comment author="mattweber" created="2013-08-29T21:16:08Z" id="23524202">Just to be clear this issue is during indexing and creation of the FST, not at search time to get suggestions.
</comment><comment author="s1monw" created="2013-08-29T21:17:05Z" id="23524255">@nik9000 I think there is a misunderstanding here. We would always search for the entire string. But the prefix suggester will will not build it's prefix dictionary for more than N leading characters. I think this should be perfectly fine and no other suggester impl is affected.
</comment><comment author="nik9000" created="2013-08-29T21:20:54Z" id="23524505">Oh sorry!  So you'd still suggest the right thing even though the prefix dictionary doesn't spell it out exactly.  Cool.  Sorry for the confusion.
</comment><comment author="mikemccand" created="2014-05-04T16:54:32Z" id="42137814">See also https://github.com/elasticsearch/elasticsearch/pull/5927 and https://issues.apache.org/jira/browse/LUCENE-5628 where we are fixing Lucene's getFiniteStrings to not consume Java stack in proportion to the character length of the index-time or query-time suggestion.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/mapper/completion/CompletionFieldMapperTests.java</file></files><comments><comment>Add 'min_input_len' to completion suggester</comment></comments></commit></commits></item><item><title>NullPointerException on empty filter with 0.90.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3595</link><project id="" key="" /><description>Problem discussed on mailing list here:
https://groups.google.com/forum/#!topic/elasticsearch/Y1ewHtYxExs

Gist with log here:
https://gist.github.com/benoit-intrw/6374962
</description><key id="18714987">3595</key><summary>NullPointerException on empty filter with 0.90.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">benoit-intrw</reporter><labels /><created>2013-08-29T06:59:11Z</created><updated>2013-08-29T08:35:08Z</updated><resolved>2013-08-29T08:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-29T08:34:43Z" id="23474403">this has already been fixed in #3477 
This will come in the next release. Sorry, I didn't realize that it is already fixed but thanks very much for reporting this issue!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The light Finnish stemmer is specified as "light_finish"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3594</link><project id="" key="" /><description>This is mostly a typo as it does look like it uses the correct stemmer in Java, but would it be worthwhile to correct the spelling from "finish" to "finnish"?

See:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java#L136
</description><key id="18704599">3594</key><summary>The light Finnish stemmer is specified as "light_finish"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">robhudson</reporter><labels /><created>2013-08-29T00:10:40Z</created><updated>2013-08-29T08:16:12Z</updated><resolved>2013-08-29T08:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file></files><comments><comment>Fixed a typo in the config of light finnish stemmer (old last_finish is still supported for backward compatibility)</comment></comments></commit></commits></item><item><title>Delete by query should not silently refresh index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3593</link><project id="" key="" /><description>Hi this issue caused lots of trouble because it was not clear why this happened. I had some index updates where a (quite common) approach is used:

I have to update a bulk of documents with some higher level group key (not the uid). Like:

```
doc1: { groupKey: 'foo', _id: 'bar1' }
doc2: { groupKey: 'foo', _id: 'bar2' }
doc3: { groupKey: 'foo', _id: 'bar3' }
```

The code that updates this group of documents does not know the real _id of those already in the index (it just knows that the whole group updates), so it first deletes all documents by using deleteByQuery on the group key. After that it reindexes all documents in the group (with possibly different new _id values).

If you don't disable index refreshing, for a short time, the whole group would be disappearing and reappearing then. So to make the whole group reindex "atomic" you would disable index refreshing before that and reenable it afterwards (or do manual refreshing at all - what I do for this index in any case).

Unfortunately, deleteByQuery forcefully refreshes the index. Which is hard to understand because its not documented. There is just a comment in the code that the refresh is needed although its heavy, because when executing a Lucene IndexWriter deleteByQuery, ElasticSearch does not know what documents were really deleted, so all internal tracking does not work (it cannot update version consistency,...)

I was discussing with Martijn on IRC (not even he was aware that deleteByQuery does not work with disabled refreshing), he suggested that maybe the query is executed in ElasticSearch itsself and then it starts a bulk on _uid deletes (this is also one possibility for a workaround in our case if number of deletes is small).

In my opinion the better variant would be to do it like in Apache Solr: Apache Solr has 2 different IndexReaders open: One for searching the index (this one is refreshed in those periods of times), but a second one is another NRT reader on the IndexWriter that is used to do some updates of data structures after IndexWriter has written stuff. So updating of the ES internal data should be done with a new NRT reader and not the one used for searching.
</description><key id="18694113">3593</key><summary>Delete by query should not silently refresh index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">uschindler</reporter><labels /><created>2013-08-28T20:43:10Z</created><updated>2015-07-13T15:47:23Z</updated><resolved>2015-07-13T15:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-30T14:47:37Z" id="23566100">Thanks for writing this down! Patches welcome :)
</comment><comment author="govindm" created="2014-12-04T06:48:02Z" id="65544702">Any update on this issue? we are facing similar problem
</comment><comment author="clintongormley" created="2014-12-31T10:35:18Z" id="68434570">Depends on https://github.com/elasticsearch/elasticsearch/issues/7052
</comment><comment author="mikemccand" created="2015-07-13T15:47:22Z" id="120974783">DBQ is moved to a plugin in ES 2.0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improved logic that removes top-level folder from archives when needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3592</link><project id="" key="" /><description>Whether we remove the top-level folder from the archive depends now on the zip itself and not on where it was downloaded from. That makes it work installing local files too.
</description><key id="18684326">3592</key><summary>Improved logic that removes top-level folder from archives when needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-08-28T17:48:30Z</created><updated>2014-07-07T08:20:46Z</updated><resolved>2013-09-03T08:36:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-03T08:18:29Z" id="23696417">LGTM!
</comment><comment author="javanna" created="2013-09-03T08:36:59Z" id="23697307">Merged 45c8da3e98f391b815f7966dd62900d1b1d77d04 and 2ed1f00bc7d5685f9378d88c9dcfebca5f53c509 .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ArrayIndexOutOfBoundsException when using empty preference parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3591</link><project id="" key="" /><description>to reproduce(tested on 0.90.3):

```
curl -XGET http://localhost:9200/_search?preference=
```

this returns:

```
{
error: "StringIndexOutOfBoundsException[String index out of range: 0]",
status: 500
}
```

I'm not really sure what is the desired behavior in this case, because it could be one of:
    1 - ignore the parameter and treat as if nothing was sent
    2 -  treat the empty string as the preference parameter
    3 - return a friendly error message for it

anyway, the error happens here (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java#L172)
and it could be easily fixed by either making a check for an empty string before comparing its first character, or changing this line https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java#L164 for something like preference.isEmpty(), which also covers the empty string case.
</description><key id="18678071">3591</key><summary>ArrayIndexOutOfBoundsException when using empty preference parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-28T15:52:35Z</created><updated>2013-08-28T19:36:36Z</updated><resolved>2013-08-28T19:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-28T19:36:36Z" id="23441271">thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>src/test/java/org/elasticsearch/test/integration/search/simple/SimpleSearchTests.java</file></files><comments><comment>Treat empty prefrence as a `not set` in Plain Operation Routing</comment></comments></commit></commits></item><item><title>Allow different co-ordinate systems for geometric objects.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3590</link><project id="" key="" /><description>It looks like elasticsearch only allows WGS-84 coordinates for geometric objects, the application I'm currently working on uses OSGB-36 which throws an exception when you try to insert a node with co-ordinates outside of the allowed range for WGS-84

Thanks
</description><key id="18667579">3590</key><summary>Allow different co-ordinate systems for geometric objects.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">keedon</reporter><labels><label>:Geo</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2013-08-28T13:14:05Z</created><updated>2017-02-09T16:44:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-09-06T10:25:56Z" id="23931209">Hi @keedon, you're right. ES only supports WGS84 coordinates for geodata. I'm thinking about extending the mappings by a projection definition, but this may take a while.

Thanks for coming up with this great idea,
    Florian
</comment><comment author="yeroc" created="2013-09-11T23:05:40Z" id="24283531">Curious.  We just happened to run into this limitation ourselves.  Could the documentation be updated to at least be more explicit in this regard (until this enhancement is resolved)?  The stacktrace that's generated really isn't clear at all.  We had to do some digging in the source code to realize that projected coordinates and the GeoJson crs parameter was being completely ignored...
</comment><comment author="colings86" created="2014-09-05T09:28:42Z" id="54603412">We don't currently plan to support multiple co-ordinate systems within Elasticsearch. There are a lot of different coordinate systems and it would be difficult to reliably provide support for them all. I would suggest that you do the conversion from your co-ordinate system to WGS-84 in your client application. 

We do need to update the documentation to be clear that we use WGS-84.
</comment><comment author="desruisseaux" created="2016-01-15T09:50:17Z" id="171915801">Just in case the information may be of some use: Apache SIS 0.6 can create Mercator, Transverse Mercator, Lambert Conical and Stereographic map projections for a Coordinate Reference System defined in Well Known Text (WKT) format. It supports both version 1 and 2 of WKT. Apache SIS 0.7 (to be released in one or two months) will add support for EPSG codes, datum shifts and a few more map projections.
</comment><comment author="clintongormley" created="2016-11-06T10:04:47Z" id="258671073">@nknize would you consider adding support for different coordinate systems?
</comment><comment author="desruisseaux" created="2016-11-06T10:30:56Z" id="258672271">If there is any interest to start a thread or a wiki page on this subject, I may be able to help with some proposals on the coordinate transformations part.
</comment><comment author="zmays" created="2017-02-09T16:44:24Z" id="278699118">Please update the documentation - if one sends anything but wgs84, for example, web mercator, like this:
`{"location":{"type":"GeometryCollection","crs" : {
          "type" : "name",
          "properties" : {
            "name" : "urn:ogc:def:crs:EPSG::3857"
          }
        },"geometries":[{"type":"MultiPolygon","coordinates":[ [ [ [ -11458202.599282084, 3936157.3620158276 ], [ -11459165.477255989, 3936155.366018667 ], [ -11459163.694921646, 3935177.6040588915 ], [ -11458202.66477565, 3935177.7214682177 ], [ -11458202.599282084, 3936157.3620158276 ] ] ], [ [ [ -11458202.599282084, 3936157.3620158276 ], [ -11457239.720985424, 3936159.277695415 ], [ -11457237.807021441, 3937140.8780615395 ], [ -11458202.533897068, 3937137.0839141724 ], [ -11458202.599282084, 3936157.3620158276 ] ] ] ]
}]}}`
this is the message one gets:
`{
  "error": {
    "root_cause": [
      {
        "type": "mapper_parsing_exception",
        "reason": "failed to parse [location]"
      }
    ],
    "type": "mapper_parsing_exception",
    "reason": "failed to parse [location]",
    "caused_by": {
      "type": "null_pointer_exception",
      "reason": null
    }
  },
  "status": 400
}`

Which takes a long time to figure out what is going on.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard has not been created, mark shard as failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3589</link><project id="" key="" /><description>Hi,

We have some issue to report.
Our cluster has next configuration
Cloud: amazon
Number nodes: 3 (master and data)
Number shard: 5
Number replicas: 1
zen discover: ec2
min master nodes: 2
es version: 0.90.3

Twice a day one node disappears from cluster (you can see from screenshots). 

And error log from 1a node:

[2013-08-27 07:37:31,047][INFO ][cluster.metadata         ] [EU West 1A] updating number_of_replicas to [2] for indices [production_compositedata_0]
[2013-08-27 07:37:33,418][INFO ][cluster.metadata         ] [EU West 1A] updating number_of_replicas to [2] for indices [production_global_person]
[2013-08-27 07:56:12,661][INFO ][cluster.metadata         ] [EU West 1A] updating number_of_replicas to [1] for indices [production_compositedata_0]
[2013-08-27 07:56:15,781][INFO ][cluster.metadata         ] [EU West 1A] updating number_of_replicas to [1] for indices [production_global_person]
[2013-08-27 11:04:45,806][WARN ][discovery.ec2            ] [EU West 1A] received a join request for an existing node [[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}]
[2013-08-27 11:04:46,024][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,024][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,024][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,040][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,040][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,040][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,211][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,211][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,211][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,211][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,211][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,540][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,540][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,541][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,541][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,735][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,736][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,737][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,856][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,856][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,913][WARN ][cluster.action.shard     ] [EU West 1A] received shard failed for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 14:30:57,803][INFO ][node                     ] [EU West 1A] stopping ...
[2013-08-27 14:30:58,162][INFO ][node                     ] [EU West 1A] stopped
[2013-08-27 14:30:58,162][INFO ][node                     ] [EU West 1A] closing ...
[2013-08-27 14:30:58,177][INFO ][node                     ] [EU West 1A] closed
[2013-08-27 14:31:05,384][INFO ][node                     ] [EU West 1A] version[0.90.3], pid[4852], build[5c38d60/2013-08-06T13:18:31Z]
[2013-08-27 14:31:05,384][INFO ][node                     ] [EU West 1A] initializing ...
[2013-08-27 14:31:05,416][INFO ][plugins                  ] [EU West 1A] loaded [transport-thrift, bcsocial-similarity, cloud-aws], sites [bigdesk]
[2013-08-27 14:31:10,111][INFO ][node                     ] [EU West 1A] initialized
[2013-08-27 14:31:10,111][INFO ][node                     ] [EU West 1A] starting ...
[2013-08-27 14:31:10,142][INFO ][thrift                   ] [EU West 1A] bound on port [9500]
[2013-08-27 14:31:10,439][INFO ][transport                ] [EU West 1A] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/192.168.24.26:9300]}
[2013-08-27 14:31:17,973][INFO ][cluster.service          ] [EU West 1A] detected_master [EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}, added {[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c},[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},}, reason: zen-disco-receive(from master [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}])
[2013-08-27 14:31:19,658][INFO ][discovery                ] [EU West 1A] ProductionSearch/3GzUTWxzS6eHbrCenZaHgw
[2013-08-27 14:31:19,736][INFO ][http                     ] [EU West 1A] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/192.168.24.26:9200]}
[2013-08-27 14:31:19,736][INFO ][node                     ] [EU West 1A] started
[2013-08-27 23:16:22,453][INFO ][discovery.ec2            ] [EU West 1A] master_left [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}], reason [do not exists on master, act as master failure]
[2013-08-27 23:16:22,453][INFO ][cluster.service          ] [EU West 1A] master {new [EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}, previous [EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}}, removed {[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},}, reason: zen-disco-master_failed ([EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b})
[2013-08-28 05:46:25,046][INFO ][cluster.service          ] [EU West 1A] added {[EU West 1B][wP1cdN2ESZeBMIoVXe5zZQ][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},}, reason: zen-disco-receive(join from node[[EU West 1B][wP1cdN2ESZeBMIoVXe5zZQ][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}])
[2013-08-28 09:07:33,374][INFO ][node                     ] [EU West 1A] stopping ...
[2013-08-28 09:07:33,514][INFO ][node                     ] [EU West 1A] stopped
[2013-08-28 09:07:33,514][INFO ][node                     ] [EU West 1A] closing ...
[2013-08-28 09:07:33,530][INFO ][node                     ] [EU West 1A] closed
[2013-08-28 09:07:41,158][INFO ][node                     ] [EU West 1A] version[0.90.3], pid[2532], build[5c38d60/2013-08-06T13:18:31Z]
[2013-08-28 09:07:41,158][INFO ][node                     ] [EU West 1A] initializing ...
[2013-08-28 09:07:41,189][INFO ][plugins                  ] [EU West 1A] loaded [transport-thrift, bcsocial-similarity, cloud-aws], sites [bigdesk]
[2013-08-28 09:07:46,088][INFO ][node                     ] [EU West 1A] initialized
[2013-08-28 09:07:46,088][INFO ][node                     ] [EU West 1A] starting ...
[2013-08-28 09:07:46,134][INFO ][thrift                   ] [EU West 1A] bound on port [9500]
[2013-08-28 09:07:46,415][INFO ][transport                ] [EU West 1A] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/192.168.24.26:9300]}
[2013-08-28 09:07:53,934][INFO ][cluster.service          ] [EU West 1A] detected_master [EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}, added {[EU West 1B][wP1cdN2ESZeBMIoVXe5zZQ][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c},}, reason: zen-disco-receive(from master [[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}])
[2013-08-28 09:07:54,511][INFO ][discovery                ] [EU West 1A] ProductionSearch/O3bUWXlkR2OIXD0tY2LKmQ
[2013-08-28 09:07:54,589][INFO ][http                     ] [EU West 1A] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/192.168.24.26:9200]}
[2013-08-28 09:07:54,589][INFO ][node                     ] [EU West 1A] started

...1b node:

[2013-08-27 11:07:16,139][INFO ][discovery.ec2            ] [EU West 1B] master_left [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2013-08-27 11:07:16,139][INFO ][cluster.service          ] [EU West 1B] master {new [EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}, previous [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}}, removed {[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-master_failed ([EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a})
[2013-08-27 14:31:17,737][INFO ][cluster.service          ] [EU West 1B] added {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(join from node[[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}])
[2013-08-27 23:16:22,097][INFO ][cluster.service          ] [EU West 1B] removed {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-node_failed([EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}), reason failed to ping, tried [3] times, each with maximum [30s] timeout
[2013-08-28 05:45:59,420][INFO ][node                     ] [EU West 1B] stopping ...
[2013-08-28 05:45:59,576][INFO ][node                     ] [EU West 1B] stopped
[2013-08-28 05:45:59,576][INFO ][node                     ] [EU West 1B] closing ...
[2013-08-28 05:45:59,638][INFO ][node                     ] [EU West 1B] closed
[2013-08-28 05:46:10,605][INFO ][node                     ] [EU West 1B] version[0.90.3], pid[2572], build[5c38d60/2013-08-06T13:18:31Z]
[2013-08-28 05:46:10,621][INFO ][node                     ] [EU West 1B] initializing ...
[2013-08-28 05:46:10,683][INFO ][plugins                  ] [EU West 1B] loaded [transport-thrift, bcsocial-similarity, cloud-aws], sites [bigdesk]
[2013-08-28 05:46:17,126][INFO ][node                     ] [EU West 1B] initialized
[2013-08-28 05:46:17,126][INFO ][node                     ] [EU West 1B] starting ...
[2013-08-28 05:46:17,157][INFO ][thrift                   ] [EU West 1B] bound on port [9500]
[2013-08-28 05:46:17,500][INFO ][transport                ] [EU West 1B] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/192.168.32.27:9300]}
[2013-08-28 05:46:25,253][INFO ][cluster.service          ] [EU West 1B] detected_master [EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}, added {[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c},[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}])
[2013-08-28 05:46:26,439][INFO ][discovery                ] [EU West 1B] ProductionSearch/wP1cdN2ESZeBMIoVXe5zZQ
[2013-08-28 05:46:27,110][INFO ][http                     ] [EU West 1B] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/192.168.32.27:9200]}
[2013-08-28 05:46:27,125][INFO ][node                     ] [EU West 1B] started
[2013-08-28 08:50:37,742][INFO ][cluster.service          ] [EU West 1B] master {new [EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}, previous [EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}}, removed {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}])
[2013-08-28 09:07:33,514][INFO ][discovery.ec2            ] [EU West 1B] master_left [[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}], reason [shut_down]
[2013-08-28 09:07:53,684][INFO ][cluster.service          ] [EU West 1B] added {[EU West 1A][O3bUWXlkR2OIXD0tY2LKmQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}])

..and 1c node:

[2013-08-27 11:04:38,711][INFO ][discovery.ec2            ] [EU West 1C] master_left [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2013-08-27 11:04:38,711][INFO ][cluster.service          ] [EU West 1C] master {new [EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}, previous [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}}, removed {[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-master_failed ([EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a})
[2013-08-27 11:04:39,756][INFO ][discovery.ec2            ] [EU West 1C] master_left [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}], reason [no longer master]
[2013-08-27 11:04:39,756][WARN ][discovery.ec2            ] [EU West 1C] not enough master nodes after master left (reason = no longer master), current nodes: {[EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c},}
[2013-08-27 11:04:39,772][INFO ][cluster.service          ] [EU West 1C] removed {[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},}, reason: zen-disco-master_failed ([EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b})
[2013-08-27 11:04:45,918][INFO ][cluster.service          ] [EU West 1C] detected_master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}, added {[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}])
[2013-08-27 11:04:46,121][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][0] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,121][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,121][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][2] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,121][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,121][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,121][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,121][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][0] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,121][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,121][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][2] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,121][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,121][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,121][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,308][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][0] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,308][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,308][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][2] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,308][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,308][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,308][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,308][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][2] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,324][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,324][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,324][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,636][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][0] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,636][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,636][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,636][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,636][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][2] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,636][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,636][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,636][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,839][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][0] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,839][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,839][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,839][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,839][WARN ][indices.cluster          ] [EU West 1C] [production_compositedata_0][2] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,839][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_compositedata_0][2], node[kIu8-vq4T8KSB83cgq8qIQ], [R], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,963][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][0] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,963][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][0], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:46,963][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:46,963][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:04:47,026][WARN ][indices.cluster          ] [EU West 1C] [production_global_person][3] master [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}] marked shard as started, but shard has not been created, mark shard as failed
[2013-08-27 11:04:47,026][WARN ][cluster.action.shard     ] [EU West 1C] sending failed shard for [production_global_person][3], node[kIu8-vq4T8KSB83cgq8qIQ], [P], s[STARTED], reason [master [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a} marked shard as started, but shard has not been created, mark shard as failed]
[2013-08-27 11:07:16,392][INFO ][cluster.service          ] [EU West 1C] master {new [EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}, previous [EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}}, removed {[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}])
[2013-08-27 14:30:58,040][INFO ][discovery.ec2            ] [EU West 1C] master_left [[EU West 1A][FS6eNXw1R7iQGmN6YHfarQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}], reason [shut_down]
[2013-08-27 14:31:17,602][INFO ][cluster.service          ] [EU West 1C] added {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}])
[2013-08-27 23:16:22,171][INFO ][cluster.service          ] [EU West 1C] removed {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}])
[2013-08-27 23:16:22,420][INFO ][cluster.service          ] [EU West 1C] master {new [EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}, previous [EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}}, removed {[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},}, added {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(from master [[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}])
[2013-08-28 05:45:59,638][INFO ][discovery.ec2            ] [EU West 1C] master_left [[EU West 1B][Y5G-CYdzSP2FiZ0Miqshxg][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b}], reason [shut_down]
[2013-08-28 05:46:25,097][INFO ][cluster.service          ] [EU West 1C] added {[EU West 1B][wP1cdN2ESZeBMIoVXe5zZQ][inet[/192.168.32.27:9300]]{aws_availability_zone=eu-west-1b},}, reason: zen-disco-receive(from master [[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}])
[2013-08-28 08:50:37,594][INFO ][discovery.ec2            ] [EU West 1C] master_left [[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2013-08-28 08:50:37,594][INFO ][cluster.service          ] [EU West 1C] master {new [EU West 1C][kIu8-vq4T8KSB83cgq8qIQ][inet[/192.168.48.28:9300]]{aws_availability_zone=eu-west-1c}, previous [EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}}, removed {[EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-master_failed ([EU West 1A][3GzUTWxzS6eHbrCenZaHgw][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a})
[2013-08-28 09:07:53,533][INFO ][cluster.service          ] [EU West 1C] added {[EU West 1A][O3bUWXlkR2OIXD0tY2LKmQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a},}, reason: zen-disco-receive(join from node[[EU West 1A][O3bUWXlkR2OIXD0tY2LKmQ][inet[/192.168.24.26:9300]]{aws_availability_zone=eu-west-1a}])

Thank you for your help.

![1a](https://f.cloud.github.com/assets/3388840/1041379/7d407be8-0fd0-11e3-8b0b-70162561360c.jpg)
![1c](https://f.cloud.github.com/assets/3388840/1041380/7d4f96d2-0fd0-11e3-86d9-c6256a698727.jpg)
![1b](https://f.cloud.github.com/assets/3388840/1041381/7d53f038-0fd0-11e3-83fa-ed6f79d41d99.jpg)
</description><key id="18661743">3589</key><summary>Shard has not been created, mark shard as failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukapor</reporter><labels /><created>2013-08-28T11:01:32Z</created><updated>2014-08-08T16:32:03Z</updated><resolved>2014-08-08T16:32:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T16:32:03Z" id="51626030">Hi @lukapor 

Sorry it has taken a while to look at this. It looks like your nodes are being shut down by some other process.  If you're still seeing this issue on a more recent version, please could you open another ticket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support highlight encoders provided via plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3588</link><project id="" key="" /><description>Instead of having them hardcoded like this:

```
    Encoder encoder = field.encoder().equals("html") ? Encoders.HTML : Encoders.DEFAULT;
```

(taken from FVH)

Will be much easier to fix issues like #3587 ad-hoc
</description><key id="18655354">3588</key><summary>Support highlight encoders provided via plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Highlighting</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-08-28T08:32:28Z</created><updated>2017-07-12T21:00:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2017-07-07T09:54:13Z" id="313639498">I believe this can be closed now :)</comment><comment author="javanna" created="2017-07-12T21:00:25Z" id="314895499">hi @synhershko it can be closed because we fixed it or because you are tired of waiting for us to fix it? :)

Besides jokes, I see the same problem in the unified highlighter. What am I missing?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SimpleHTMLEncoder to not encode non-ASCII chars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3587</link><project id="" key="" /><description>Currently Lucene's SimpleHTMLEncoder has this:

```
  default:
       if (ch &lt; 128)
       {
             result.append(ch);
         }
       else
         {
             result.append("&amp;#").append((int)ch).append(";");
         }
```

This produces a lot of garbage especially when asking for fragments when using the fvh/html encoder combo, where the encoded string may get cut in the middle of an encoded entity

The obvious solution is to account for length changes caused by the encoder, but an even simpler solution will be to remove the above code portion and just use default: result.append(ch). Nowadays when its all UTF8 it doesn't make much sense to keep it anyway.

I'm posting this here and not on Lucene's because a) it will get fixed faster here and b) its more of an ES issue, because of the fragment size DSL etc
</description><key id="18655254">3587</key><summary>SimpleHTMLEncoder to not encode non-ASCII chars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>bug</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-08-28T08:30:26Z</created><updated>2014-10-17T08:58:05Z</updated><resolved>2013-10-08T14:32:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2013-08-28T22:48:30Z" id="23454900">You should open an issue on Lucene, too. Especially as this escaping breaks unicode for everything outside the basic lingual plane. This encoding should just be removed.
</comment><comment author="uschindler" created="2013-08-28T23:05:07Z" id="23455698">I opened https://issues.apache.org/jira/browse/LUCENE-5191
</comment><comment author="synhershko" created="2013-09-02T11:50:25Z" id="23655767">Uwe - thanks. Part of this issue is the fact that the escaped form is considered multiple "advancements" in a fragment and not just one, making it possible for fragments to spit partial encoded forms, hence show garbage

This can be easily reproducible by using non ASCII chars with the current highlight encoder implementation
</comment><comment author="javanna" created="2013-09-02T14:45:37Z" id="23664161">The lucene issue has been already fixed. Thanks @uschindler !!!
I guess it's just a matter of waiting for the next version of lucene then.
</comment><comment author="s1monw" created="2013-09-02T14:54:42Z" id="23664642">@javanna we will keep it open until we upgrade to 4.5
</comment><comment author="synhershko" created="2013-09-02T15:02:47Z" id="23665100">@s1monw yeah but can we make sure escaped entities aren't cut in the middle by fixing the fragbuilder?
</comment><comment author="s1monw" created="2013-10-08T14:32:32Z" id="25894914">closing, we upgraded to `Lucene 4.5` in https://github.com/elasticsearch/elasticsearch/commit/e9f36772c1d782e66e882cc36574dc9bf805ac71
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw exception when content _id is an object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3586</link><project id="" key="" /><description>An exception is thrown if the provided id does not match the content id, but only if the content id is a string field.  If the content id is a complex object, no exception is thrown and the document is indexed anyway, leading to problems with search later.

This fix adds an additional check for _id fields that are objects and throws an exception if one is encountered.

Fixes #3517
</description><key id="18631969">3586</key><summary>Throw exception when content _id is an object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>enhancement</label></labels><created>2013-08-27T20:28:51Z</created><updated>2014-11-11T18:28:52Z</updated><resolved>2014-11-11T18:28:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-28T06:39:13Z" id="44370686">is there any specific reason (except breaking current behaviour), why this is hanging around?
</comment><comment author="clintongormley" created="2014-07-11T09:51:19Z" id="48712862">@polyfractal ping?
</comment><comment author="polyfractal" created="2014-07-11T13:44:55Z" id="48731595">@spinscale No particular reason, just got lost in the shuffle I think.

For the record: I have no idea if this PR is a good idea, and if this is the best way to do it.  Was a quick one-line PR that I sent before I really had a good handle on the ES codebase.
</comment><comment author="s1monw" created="2014-07-17T13:49:50Z" id="49308648">I left a comment, can reabase it and update ? I think it's a good change
</comment><comment author="clintongormley" created="2014-10-20T12:47:26Z" id="59744476">@polyfractal reassigning this to you. 
</comment><comment author="clintongormley" created="2014-11-11T18:28:52Z" id="62593073">Closing in favour of #6730
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Count and Search API status codes are inconsistent (count does not return 400)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3585</link><project id="" key="" /><description>I've got a query that produces a 400 when performing a search. However, when I use the same query to perform a count, a 200 status is returned.

https://gist.github.com/garron/6346888

The output from this gist is:

{"ok":true,"_index":"myindex","_type":"mydoc","_id":"5kngTmpeR5GtJg6Vu46tUw","_version":1}\n
Count using a good query
200
Search using a good query
200
Count using a bad query
200
Search using a bad query
400

If you take out the "-o /dev/null", you can see that both the search and count produce errors in the "bad query" case, but the count API still returns a 200.
</description><key id="18619521">3585</key><summary>Count and Search API status codes are inconsistent (count does not return 400)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">garron</reporter><labels><label>bug</label><label>v1.0.0.Beta1</label></labels><created>2013-08-27T16:48:19Z</created><updated>2013-08-28T18:33:25Z</updated><resolved>2013-08-28T18:33:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/count/CountResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file></files><comments><comment>Added a status method CountResponse that resolve shard failures into rest status code. That method is now used in RestCountAction to return proper status.</comment></comments></commit></commits></item><item><title>Recover small files (&lt; 1mb) using a separate threadpool different than large files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3584</link><project id="" key="" /><description>This allows ES to avoid the case where new shards (like those created when an index is created) to be recovered and initialized while larger recoveries are going on.

It defaults to up to 2 threads and can be dynamically updated, similar to the `concurrent_streams` setting (the setting is `concurrent_small_file_streams`)

Fixes #3576
</description><key id="18617963">3584</key><summary>Recover small files (&lt; 1mb) using a separate threadpool different than large files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-08-27T16:21:32Z</created><updated>2014-07-16T21:52:29Z</updated><resolved>2013-08-27T17:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent filtered nodes to change index settings if nodes can't store shards of this index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3583</link><project id="" key="" /><description>Hi,

Here is a usecase I've encountered: two or more organizations want to allow one to query onto their respective data using ElasticSearch features. Each organization need to keep control on the nodes it hosts and on the related data.

Partial solution: an organization maintains an index containing its own data, this index is stored on nodes only hosted by the organization using route awareness based on IP filters (`"index.routing.allocation.exclude._ip"`).

HTTP modules can be disabled for avoiding changes of settings using REST API, but any application can connect in the cluster via Transport module and update any index setting.

So, in order to allow such usecase, the only missing feature I see is to add a setting to an index which would prevent any non-allocatable node from changing the index configuration.

I mean by non-allocatable node any node filtered by `"index.routing.allocation.*"`

Is it conceivable?
</description><key id="18617468">3583</key><summary>Prevent filtered nodes to change index settings if nodes can't store shards of this index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raf64flo</reporter><labels /><created>2013-08-27T16:12:18Z</created><updated>2014-02-13T08:40:40Z</updated><resolved>2014-02-13T08:40:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="raf64flo" created="2013-12-05T14:28:54Z" id="29901366">Any comment?
</comment><comment author="clintongormley" created="2013-12-05T14:31:53Z" id="29901602">Why wouldn't you just use two clusters here?

If you're allowing people to connect via the transport client, they have full access to the whole cluster, and could for instance update cluster settings which affect all nodes anyway.
</comment><comment author="raf64flo" created="2014-02-13T08:40:40Z" id="34958192">Oh sorry, I didn't notice you comment clintongormley.

Anyway, tribe feature release with 1.0.0 should fix this use case, this is exactly what blocked the usage of ElasticSearch in my context, just hope now that I will be able ton convince our partner to use ElasticSearch instead of SolR...

I think we can close this issue.

Thanks for all!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Install site plugin with custom url doesn't filter directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3582</link><project id="" key="" /><description>1. Download  https://github.com/mobz/elasticsearch-head/zipball/master
2. Copy downloaded file to target server /tmp/mobz-elasticsearch-head-0c2ac0b.zip
3. Install plugin with command:

```
 bin/plugin --url file:/tmp/mobz-elasticsearch-head-0c2ac0b.zip --install head
```
1. The plugins is installed in plugins/head/_site/mobz-elasticsearch-head-0c2ac0b/... instead of plugins/head/_site/...

The folder could be stripped because it's same as file name.
</description><key id="18614183">3582</key><summary>Install site plugin with custom url doesn't filter directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gquintana</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-27T15:18:29Z</created><updated>2013-09-03T08:54:54Z</updated><resolved>2013-09-03T08:35:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-27T15:27:03Z" id="23345472">Right, the problem here is that we remove the top-level folder from the downloaded archive when we know we downloaded it from github, and we rely on a certain structure, which by the way sometimes changes (see #3551).

If you manually provide a zip file we don't know where it came from and we don't remove the top-level folder, but only extract the zip file under the _site folder.

Manually removing the top-level folder from the zip, or from the extracted directory would do the trick.
</comment><comment author="gquintana" created="2013-08-27T15:57:41Z" id="23347954">Then another solution could be to add a download command to automate step 1:

```
bin/plugin --download mobz/elasticsearch-head
```

This could be done on any workstation and produce the appropriate zip file.
</comment><comment author="javanna" created="2013-08-27T17:11:55Z" id="23353568">What would be the added value of automating the download without the install part, when we have the install command that does them both? 

For instance if you execute:

`bin/plugin -install mobz/elasticsearch-head` 

the elasticsearch-head will be downloaded from github, extracted properly and the plugin would be working without any manual work needed.

Maybe I'm missing the reason why you are manually downloading the archive from github and installing it through the `plugin` script. Can you elaborate on that?
</comment><comment author="gquintana" created="2013-08-28T06:21:05Z" id="23393748">Because the ElasticSearch server can not access to internet (secured network zone).
</comment><comment author="javanna" created="2013-08-28T08:20:16Z" id="23398273">Ok I see what you mean, you would like to execute the `-download` from any machine that has access to the internet and the `-install` part on the elasticsearch server given the previously downloaded zip.

Thanks for clarifying that!
</comment><comment author="javanna" created="2013-08-28T11:42:58Z" id="23407911">Having thought about this, I'd rather prefer to make the archive extraction smarter, so that it can detect that there is a top-level folder to remove, regardless of where the plugin came from. Looks like we can always do that whenever we have an archive that contains a single top-level folder. That would solve your issue without requiring to add the `-download` option.
</comment><comment author="gquintana" created="2013-09-03T08:43:04Z" id="23697590">Thanks for fixing that
</comment><comment author="javanna" created="2013-09-03T08:54:54Z" id="23698160">Thank you for reporting this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/test/integration/plugin/PluginManagerTests.java</file></files><comments><comment>Improved logic that removes top-level folder from archives when needed</comment></comments></commit></commits></item><item><title>Extend named filter support to queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3581</link><project id="" key="" /><description>At moment the `_name` option is supported on all filters, and the search responses then indicates which filters that have the `_name` option have matched.

This name tagging should also be supported for queries. The matchedFilters will now also contain named queries and  because of this the method naming in the Java apis have been changed to `matchedQueries(...)` instead of `matchedFilters(...)` in the SearchHit class. The `matchedFilters(...)` methods are still supported, but have been deprecated and will be removed in from version 1.0.Beta1.
</description><key id="18605787">3581</key><summary>Extend named filter support to queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-27T13:08:30Z</created><updated>2013-09-06T19:32:30Z</updated><resolved>2013-08-28T08:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Added docs for named queries.</comment><comment>Relates to #3581</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/SearchHit.java</file><file>src/main/java/org/elasticsearch/search/SearchModule.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/matchedfilters/MatchedQueriesFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>src/test/java/org/elasticsearch/test/integration/search/matchedfilters/MatchedQueriesTests.java</file></files><comments><comment>Deprecate the method names `namedFilter` in favor of `namedQueries`.</comment><comment>The reason behind this is that the `_name` support has been extended to queries as well and the name `namedQueries` suggests better that it is applicable to the whole query dsl.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/search/fetch/matchedfilters/MatchedFiltersFetchSubPhase.java</file><file>src/test/java/org/elasticsearch/test/integration/search/matchedfilters/MatchedFiltersTests.java</file></files><comments><comment>Added `_name` support to queries.</comment></comments></commit></commits></item><item><title>Forced awareness fails to balance shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3580</link><project id="" key="" /><description>I built a cluster for amazon zone awareness as follows:

Background: I've setup the number of shards to 3, and I have allocation awareness set to match the awszone property which I'm setting in a config. 

There's 6 nodes total:

  2 in us-west-2a
  2 in us-west-2b
  1 in us-east-2a
  1 in us-east-2b

So in theory, I should be able to lose either an entire side of the country. I would also expect the nodes in us-west to balance the shards, not just allocate a single shard to the machine. 

However, the western zone shards aren't balancing except to move 1 shard onto each of the other hosts in the zone. 

S1monw is on the case. 

Discussion here: https://groups.google.com/forum/m/#!topic/elasticsearch/9yZw7sryFb4
</description><key id="18599994">3580</key><summary>Forced awareness fails to balance shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">twinforces</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-27T10:55:51Z</created><updated>2013-10-16T23:01:27Z</updated><resolved>2013-08-28T21:42:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-27T10:56:46Z" id="23327397">thanks for opening this!
</comment><comment author="twinforces" created="2013-08-27T11:01:14Z" id="23327613">Hey, no problem! Thanks for digging in to fix this! 

Sent from my iPad

On Aug 27, 2013, at 3:57 AM, Simon Willnauer notifications@github.com wrote:

&gt; thanks for opening this!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2013-08-27T16:06:50Z" id="23348732">I added a test for this to master which awaits a fix... I think I know the problem here but I need to verfiy it. Seems like a problem in the shards allocator algorithm. 

see: https://github.com/elasticsearch/elasticsearch/commit/e7ff8ea50961b99de3edb974d5a5f1aef8210885
</comment><comment author="s1monw" created="2013-08-28T21:43:48Z" id="23450245">pushed to 0.90  &amp; master. @twinforces would you be able to build a snapshot from 0.90 branch and try the fix in your env?
</comment><comment author="twinforces" created="2013-08-28T22:19:39Z" id="23453055">Yeah, let me try that. 
</comment><comment author="twinforces" created="2013-08-28T23:13:05Z" id="23456073">Dumb question. Is there an easy way for me to do this by updating one node and designating it the primary?
</comment><comment author="s1monw" created="2013-08-30T19:22:51Z" id="23583816">not really, you would need to do a full upgrade I guess. 0.90.4 is not too far away you might wanna just wait? I mean from my perspective it would be great if you could verify the problem is solved!
</comment><comment author="twinforces" created="2013-08-30T20:11:13Z" id="23586345">Maybe I'll just do it manually on my laptop. 

Sent from my iPad

On Aug 30, 2013, at 12:23 PM, Simon Willnauer notifications@github.com wrote:

&gt; not really, you would need to do a full upgrade I guess. 0.90.4 is not too far away you might wanna just wait? I mean from my perspective it would be great if you could verify the problem is solved!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2013-10-06T16:36:27Z" id="25771355">@twinforces this has been released can you confirm that this bug is fixed in your scenario?
</comment><comment author="twinforces" created="2013-10-11T18:24:27Z" id="26159620">Sorry, out with the flu. 

I tried to confirm it immediately after release, but it didn't quite behave like I expected. 

Basically, the problem is that since I initially found this, I set the number of replicas set such that every node has a copy of each shard to work around the problem. (i.e. 5, because 4 zones, 2 nodes in West1, 2 nodes in West2, 1 node in east1, 1 node in east2)

After I upgraded to 0.90.5, I thought, ok, and set the number of replicas to 4. 

It removed all shards from one of the nodes, which was not what I expected. And it didn't rebalance after that. 

So I set it back to 5 and then had to wait while the shards got recopied back to that node. 

Additionally, this didn't seem to affect primary allocation, because I have one node that's primary for all of the shards. 

![screen shot 2013-10-11 at 11 23 09 am](https://f.cloud.github.com/assets/711888/1317103/3a837216-32a2-11e3-8012-c3e43cd28bed.png)

Would that be what you expected to happen?
</comment><comment author="s1monw" created="2013-10-11T21:07:32Z" id="26174262">hmm this seems pretty much balanced no? Sorry I am confused a bit - what is wrong with the shard allocation in that picture? maybe you can describe what you would expect?
</comment><comment author="twinforces" created="2013-10-11T22:19:30Z" id="26178957">Shouldn't the shard masters be distributed? (Thick lines)
</comment><comment author="s1monw" created="2013-10-13T16:12:15Z" id="26220748">Well a primary might not be relocated at all if you upgrade since the cost of a relocation might outrule the fact that it is a primary. It doesn't matter that much to be honest. if you delete your index and recreate it does it still look like this?
</comment><comment author="twinforces" created="2013-10-16T23:01:27Z" id="26466588">What's the cost of a relocation? Isn't it just kind of just a "blessing"?

Ok, we experimented some more today. 
![screen shot 2013-10-16 at 3 45 53 pm](https://f.cloud.github.com/assets/711888/1347713/c60d3798-36b4-11e3-8ab6-afb4e84d69c8.png)

Here's what I did:

ES had run out of memory on the primary. That caused the primary to switch to the 10.100 network, which only has two nodes. (They're basically live backups). I wanted to try to see if the nodes would come back immediately after a restart, plus I want to check the balancing. 

I locked allocation based on a thread on the mailing list of someone telling me that they would restart faster if I locked allocation before restarting nodes. 
I killed the two eastern nodes, forcing it to elect a new primary in the west. 
I reduced the number of replicas, which caused it to completely remove a node from elasticsearch. 
I then started up the two eastern nodes, expecting them to come back immediately. They didn't. 
Then I gave up and turned allocation back on, and bumped the number of replicas back up. 
Now I have the above, the primary is in the west, its bootstrapping the node in the west that got kicked out, and its bootstrapping the nodes in the east. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/test/java/org/elasticsearch/test/integration/cluster/allocation/AwarenessAllocationTests.java</file><file>src/test/java/org/elasticsearch/test/unit/cluster/routing/allocation/AwarenessAllocationTests.java</file></files><comments><comment>Prevent allocation algorithm from prematurely exiting balance operations</comment></comments></commit></commits></item><item><title>0.90.3 [RPM]: Initscript does not work on SLES11SP3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3579</link><project id="" key="" /><description>Hi folks,

first and foremost thank you for such a great project. I am starting using it in conjunction with logstash and so far, it works great :)

I am testing the deployment on SLES11SP3 x86_64, and there is a small issue with the initscript. The service will not start, because daemon is neither a funciton nor an executable on SUSE. I adjusted the initscript (patch below) to fix that. Also, the initscript should be installed in /etc/init.d/ and ideally there would be a symlink /usr/sbin/rcelasticsearch which points to /etc/init.d/elasticsearch (SUSE specific though and not really necessary).

&lt;pre&gt;
--- elasticsearch.orig  2013-08-27 11:08:30.000000000 +0200
+++ elasticsearch   2013-08-27 12:00:03.000000000 +0200
@@ -10,7 +10,7 @@
 # Provides: Elasticsearch
 # Required-Start: $network $named
 # Required-Stop: $network $named
-# Default-Start: 2 3 4 5
+# Default-Start: 2 3 5
 # Default-Stop: 0 1 6
 # Short-Description: This service manages the elasticsearch daemon
 # Description: Elasticsearch is a very scalable, schema-free and high-performance search solution supporting multi-tenancy and near realtime search.
@@ -63,7 +63,6 @@
 }
 
 start() {
-    checkJava
     [ -x $exec ] || exit 5
     [ -f $CONF_FILE ] || exit 6
     if [ -n "$MAX_LOCKED_MEMORY" -a -z "$ES_HEAP_SIZE" ]; then
@@ -82,7 +81,12 @@
     fi
     echo -n $"Starting $prog: "
     # if not running, start it up here, usually something like "daemon $exec"
-    daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR
+    # SUSE uses startproc to daemonize processes
+    if [ -x /sbin/start_daemon ]; then
+   /sbin/start_daemon -u $ES_USER -p $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR
+    else
+   daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR
+    fi
     retval=$?
     echo
     [ $retval -eq 0 ] &amp;&amp; touch $lockfile
@@ -92,7 +96,7 @@
 stop() {
     echo -n $"Stopping $prog: "
     # stop it here, often "killproc $prog"
-    killproc -p $pidfile $prog
+    killproc -p $pidfile $JAVA
     retval=$?
     echo
     [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile
@@ -114,7 +118,12 @@
 
 rh_status() {
     # run checks to determine if the service is running or use generic status
-    status -p $pidfile $prog
+    if [ -r /etc/rc.status ]; then
+   checkproc -p $pidfile $JAVA
+   # rc_status -v -r
+    else
+       status -p $pidfile $JAVA
+    fi
 }
 
 rh_status_q() {
@@ -122,6 +131,8 @@
 }
 
 
+checkJava
+
 case "$1" in
     start)
         rh_status_q &amp;&amp; exit 0

&lt;/pre&gt;


Apologies for not simply filing a pull request. I am still learning my way around git/github :-/
</description><key id="18597820">3579</key><summary>0.90.3 [RPM]: Initscript does not work on SLES11SP3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattiasgiese</reporter><labels /><created>2013-08-27T10:00:50Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2014-07-18T10:43:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-13T07:41:16Z" id="24378237">I dont use SLES, but doesnt Suse use systemd rather than old style init script to start and stop services (which is also what the package uses). At least OpenSuse does. Or is your SLES version to old?

Any distribution help is greatly appreciated, it is really hard to cover it all!
</comment><comment author="electrical" created="2014-07-18T10:42:49Z" id="49417969">Hi @mattiasgiese 

I know its a bit late but we are working on a SLES compatible init script in PR #6533 and close this one in favor of the other one.

Cheers.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>inconsistent sorting by score due to differences between primary and replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3578</link><project id="" key="" /><description>not really sure if this is a bug or not.
the first part is:
    - is it normal that shards have different number of max_docs? what could cause that? a fast insertion + delete(that my guess is wont be replicated to the other shards). and of course, i guess that if they have different number of max docs, they most likely will also have different term freq and whatnot.

the second, based on if the previous is true:
    - is it then possible to have a consistent sorting(based on score) with this scenario?

i currently have for a simple match query, completely different result lists based on the shards that the query hits.
</description><key id="18597765">3578</key><summary>inconsistent sorting by score due to differences between primary and replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-08-27T09:59:40Z</created><updated>2013-08-28T20:37:28Z</updated><resolved>2013-08-27T10:29:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-08-27T10:02:31Z" id="23324996">if it is of any help, here are the stats for my shards(the cluster was not receiving updates at this moment, just to be sure the shards were equal): https://gist.github.com/lmenezes/6351739
</comment><comment author="s1monw" created="2013-08-27T10:20:58Z" id="23325899">hey @lmenezes, `max_docs` is the total number of documents in the shard including deletes. So if one shard has already merged away a bigger segment you can easily see a bid difference here. The `frequency` you mean is the `document_frequency` and that is calculated based on the `num_docs` (withou deletions).

&gt;  is it then possible to have a consistent sorting(based on score) with this scenario?

you mean across requests, well yes you can you can use a `_preference` based on a session id or a users ID to get a consistent result set so you hit the same replicas all the time. The problem here might be 1. tie-breaking since lucene by default tie-breaks on doc ID which is shard dependent (internal doc id) 2. Differences in Refreshes since one replica might already have refreshed...

does this make sense?
</comment><comment author="lmenezes" created="2013-08-27T10:29:54Z" id="23326258">yep, that's exactly what i needed. but it's "preference", right? no _ in front of it. 
</comment><comment author="s1monw" created="2013-08-27T10:38:56Z" id="23326630">&gt; yep, that's exactly what i needed. but it's "preference", right? no _ in front of it.

ah yeah maybe :) 
</comment><comment author="peschlowp" created="2013-08-28T08:54:59Z" id="23400016">I face a similar (or maybe exactly the same) issue described in https://groups.google.com/forum/#!topic/elasticsearch/RJAT6MR4sHQ.

In my case, four nodes (one primary shard and three replicas) have the same "num_docs" but huge differences in "deleted" count. This explains why "max_docs" is different, but as far as I understood from the discussion above, the score computation ignores the deleted documents and is only based on those reflected by "num_docs" (which is equal on all nodes).

But if that's true, how come that the score computations differ for identical queries sent to the four nodes?

Tie-breaking is not the issue in my case because the scores are actually different. And I don't think that refreshes are the issue, because the symptom persists even after several hours without changes to the index and explicitly requested refresh.

Maybe I misunderstood something. Anyway I hope you can clarify this once more :-)
</comment><comment author="lmenezes" created="2013-08-28T09:04:58Z" id="23400525">hey @peschlowp 
actually i think its not the num_docs, but the max_docs which is used while computing score. i just got the explanation for a query as an example:

value: 0.99963135,
description: queryWeight, product of:
    details: [ 
{ value: 10, description: boost},
{value: 3.6819985,description: idf(docFreq=79358, maxDocs=1159774)},
{value: 0.027149152, description: queryNorm}
]
</comment><comment author="s1monw" created="2013-08-28T09:07:30Z" id="23400637">@peschlowp well deleted documents still contribute to the score calculation since they are only marked as deleted but statistics are not updated so yes they contribute to the score. I confused num_docs in my explain above though. Sorry about that.
</comment><comment author="peschlowp" created="2013-08-28T09:10:42Z" id="23400798">Thanks for clarifying! I have to confess that I don't like the fact that deleted documents still contribute the to score (doesn't seem intuitive from a user's point of view), but on the other hand now at least the observed behavior makes sense :-)
</comment><comment author="s1monw" created="2013-08-28T20:37:28Z" id="23445510">&gt; I have to confess that I don't like the fact that deleted documents still contribute the to score

I can see how this is confusing but in practice this is hardly a problem. In fact that is most of the search indices work. Given the write once nature of lucene it's safe to say this won't change in the near future :)

sorry about the confusion.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[0.90.1 JAVA] Scroll query with SearchType.SCAN returns empty hit aray</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3577</link><project id="" key="" /><description>Using java client version 0.90.1, with same elasticsearch version

val query = client.prepareSearch("myindex")
      .setSearchType(SearchType.SCAN)
      .setTypes(searchType)
      .setScroll(new Timevalue(600000)
      .setSize(100).execute().actionGet()

```
Logger.info("Hits: " + scrollResp.getHits.getTotalHits) // returns non zero result
Logger.info("Hits: " + scrollResp.getHits.getHits.length) // returns 0
```

If SearchType.SCAN is not set, then hits are returned.
</description><key id="18588233">3577</key><summary>[0.90.1 JAVA] Scroll query with SearchType.SCAN returns empty hit aray</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hjz</reporter><labels><label>non-issue</label></labels><created>2013-08-27T05:07:10Z</created><updated>2016-12-13T03:36:18Z</updated><resolved>2013-08-28T09:46:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-28T09:46:41Z" id="23402650">Have a look at the documentation for the [scan search type](http://www.elasticsearch.org/guide/reference/api/search/search-type/). 

The response will include no hits, with two important results, the `total_hits` will include the total hits that match the query, and the `scroll_id` that allows to start the scroll process. From this stage, the `_search/scroll` endpoint should be used to scroll the hits, feeding the next scroll request with the previous search result `scroll_id`.

Therefore, the described behaviour is the expected one. Closing this one. Feel free to reopen if you think I missed something though.  
</comment><comment author="ramanareddy-moole" created="2016-01-11T17:59:34Z" id="170635019">I also have the similar problem. what is the alternative to this problem?

Any help  would greatly appreciated.
</comment><comment author="henushang" created="2016-12-13T03:35:00Z" id="266630481">I also have the same problem. Finally, how did you handle it ? 
Help !</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prevent large file recovery from blocking small file recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3576</link><project id="" key="" /><description>It's currently easy to block the concurrent streams pipeline for recovering files with large files when transferring large segments across the network.

We should keep these large transfers from blocking small file recovery (like the very small files created when a new index is created).
</description><key id="18577395">3576</key><summary>Prevent large file recovery from blocking small file recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-26T22:31:30Z</created><updated>2013-09-10T17:27:36Z</updated><resolved>2013-08-27T17:01:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file></files><comments><comment>Recover small files (&lt; 1mb) using a separate threadpool than large files.</comment></comments></commit></commits></item><item><title>Faceting on field with different type (across indices or document types) lead to unexpected behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3575</link><project id="" key="" /><description>When you have a simple situation with two types, one field with two different types (string and long here) then faceting on that field leads to unexpected behavior:

```
curl -X PUT localhost:9200/i/t/1 -d '{"answer": "321"}'
curl -X PUT localhost:9200/i/t2/1 -d '{"answer": 321}'  
curl -X POST localhost:9200/_refresh
curl -X GET localhost:9200/i/_search -d '{"facets": {"test": {"terms": {"field": "answer"}}}}'
```

Imho ES should fail to run this facet instead. Related to #3106
</description><key id="18556387">3575</key><summary>Faceting on field with different type (across indices or document types) lead to unexpected behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">HonzaKral</reporter><labels /><created>2013-08-26T15:43:59Z</created><updated>2014-08-08T14:26:05Z</updated><resolved>2014-08-08T14:26:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-26T16:14:02Z" id="23274772">It fails during reducing, so what we can try to do is parse into the right format and if that fails bail with a descriptive error.
</comment><comment author="clintongormley" created="2014-08-08T14:26:05Z" id="51608814">Closing in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add document highlighter to percolate api </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3574</link><project id="" key="" /><description>Add document highlighting to percolate api, that highlights for each matching percolator query snippets inside the document being percolated.

All highlight options that are supported via the search api are also supported in the percolate api. The `size` option is a required option if highlighting is specified in the percolate api.
### Percolate highlight example
#### Index document:

``` bash
curl -XPUT 'localhost:9200/my-index/_percolator/1' -d '{ 
    "query": { 
        "match" : { 
            "body" : "brown fox"  
        }   
    } 
}'
```
#### Index second document:

``` bash
curl -XPUT 'localhost:9200/my-index/_percolator/2' -d '{ 
    "query": { 
        "match" : { 
            "body" : "lazy dog"  
        }   
    } 
}'
```
#### Percolate request:

``` bash
curl -XGET 'localhost:9200/my-index/my-type/percolate' -d '{
    "doc" : {
        "body" : "The quick brown fox jumps over the lazy dog"
    },
    "highlight" : {
        "fields" : {
            "body" : {}
        }
    },
    "size" : 5
}'
```
#### Percolate response:

``` json
{
   "took": 18,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "total": 2,
   "matches": [
      {
         "_index": "my-index",
         "_id": "1",
         "highlight": {
            "body": [
               "The quick &lt;em&gt;brown&lt;/em&gt; &lt;em&gt;fox&lt;/em&gt; jumps over the lazy dog"
            ]
         }
      },
      {
         "_index": "my-index",
         "_id": "2",
         "highlight": {
            "body": [
               "The quick brown fox jumps over the &lt;em&gt;lazy&lt;/em&gt; &lt;em&gt;dog&lt;/em&gt;"
            ]
         }
      }
   ]
}
```
</description><key id="18548691">3574</key><summary>Add document highlighter to percolate api </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-08-26T13:28:26Z</created><updated>2013-09-05T11:20:36Z</updated><resolved>2013-08-26T14:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/test/java/org/elasticsearch/test/integration/percolator/RecoveryPercolatorTests.java</file><file>src/test/java/org/elasticsearch/test/integration/percolator/SimplePercolatorTests.java</file></files><comments><comment>Added highlighter to percolate api.</comment></comments></commit></commits></item><item><title>Improve refresh logic when replica move to started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3573</link><project id="" key="" /><description>Refresh the replica shard also when the cluster state moves to started, not just when the local shard moves to internal state STARTED (post recovery / relocation).

Also, better handling of the force flag when refreshing, and set it properly in various places where we call refresh
</description><key id="18548006">3573</key><summary>Improve refresh logic when replica move to started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-26T13:13:04Z</created><updated>2013-08-26T13:15:12Z</updated><resolved>2013-08-26T13:15:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/index/TransportShardDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>src/test/java/org/elasticsearch/test/unit/index/engine/robin/RobinEngineTests.java</file></files><comments><comment>Improve refresh logic when replica move to started</comment><comment>closes #3573</comment></comments></commit></commits></item><item><title>Allow removing of per-index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3572</link><project id="" key="" /><description>If you enable a setting on a index you cannot remove it.

For example if you want to disable shard relocation in cluster and enable it for one index you use:

&lt;pre&gt;
curl -XPUT http://localhost:9200/articles/_settings -d '{"index.routing.allocation.disable_allocation": false}'
curl -XPUT http://localhost:9200/_cluster/settings -d '{"transient":{"cluster.routing.allocation.disable_allocation": true}}'
&lt;/pre&gt;

Then if you want to use the cluster setting on the index there is no way to delete the defined value and you need to manually maintain it.

A good enhancement will be to have the ability to use:

&lt;pre&gt;curl -XDELETE http://localhost:9200/articles/_settings -d '{"index.routing.allocation.disable_allocation"}'
&lt;/pre&gt;

or

&lt;pre&gt; 
curl -XPUT http://localhost:9200/articles/_settings -d '{"index.routing.allocation.disable_allocation":default}'
&lt;/pre&gt;
</description><key id="18541655">3572</key><summary>Allow removing of per-index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brutester</reporter><labels /><created>2013-08-26T09:59:08Z</created><updated>2014-07-04T12:37:57Z</updated><resolved>2014-07-04T12:37:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="carljm" created="2014-02-19T21:29:47Z" id="35551238">This also impacts e.g. configuration of analyzers and filters. For instance if you configure a named filter of type 'pattern_capture' with a list of two patterns, and you later want to reduce the list to just one pattern, there is no way to do so, the second pattern will persist in the index configuration. You have to change the name of the filter (in which case the old filter persists in the index configuration, although you are no longer using it), or delete and recreate the index. 
</comment><comment author="clintongormley" created="2014-07-04T12:37:57Z" id="48039282">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>debian package init-script: start-stop-daemon needs -u argument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3571</link><project id="" key="" /><description>On Ubuntu 12.04.3, the init-script provided in the debian package fails to detect that the server is no longer running if a pid-file is still in place. Typically this happens if the server crashes for some reason.

To fix the issue, append «-u elasticsearch» to the start-stop-daemon - tests in the start) and status) blocks of the init-script.

Patch:

```
diff -u init.d-old init.d-new 
--- init.d-old  2013-08-26 10:35:57.282790191 +0200
+++ init.d-new  2013-08-26 10:34:19.510794716 +0200
@@ -139,7 +139,7 @@
    log_daemon_msg "Starting $DESC"

    set +e
-   start-stop-daemon --status --pidfile "$PID_FILE" &gt;/dev/null
+   start-stop-daemon --status --pidfile "$PID_FILE" -u elasticsearch &gt;/dev/null
    if [ "$?" != "0" ]; then
        # Prepare environment
        mkdir -p "$LOG_DIR" "$DATA_DIR" "$WORK_DIR" &amp;&amp; chown "$ES_USER":"$ES_GROUP" "$LOG_DIR" "$DATA_DIR" "$WORK_DIR"
@@ -195,7 +195,7 @@
    ;;
   status)
    set +e
-   start-stop-daemon --status --pidfile "$PID_FILE" &gt;/dev/null 2&gt;&amp;1
+   start-stop-daemon --status --pidfile "$PID_FILE" -u elasticsearch &gt;/dev/null 2&gt;&amp;1
    if [ "$?" != "0" ]; then
        if [ -f "$PID_FILE" ]; then
            log_success_msg "$DESC is not running, but pid file exists."
```
</description><key id="18539287">3571</key><summary>debian package init-script: start-stop-daemon needs -u argument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ghelland</reporter><labels /><created>2013-08-26T08:51:53Z</created><updated>2013-12-02T09:10:53Z</updated><resolved>2013-12-02T09:10:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-13T07:36:08Z" id="24378045">Hey,

we removed the `--status` again from our init script as it does not work with older debian versions. Can you try the current init script and see, if this still happens?

https://github.com/elasticsearch/elasticsearch/blob/master/src/deb/init.d/elasticsearch

Thanks!
</comment><comment author="spinscale" created="2013-12-02T09:10:53Z" id="29603125">Closing this as we dont use his option in the current 0.90 and master branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Global facet not restricted by filtered alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3570</link><project id="" key="" /><description>When faceting on an filtered alias, if I use the global:true parameter on the facet, counts returned by the facet are not restricted by the alias filter.

Is this nominal behavior? Do I have to explicitly add the filter on each facet ?
</description><key id="18538533">3570</key><summary>Global facet not restricted by filtered alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JimminiKin</reporter><labels /><created>2013-08-26T08:29:22Z</created><updated>2014-08-08T14:25:10Z</updated><resolved>2014-08-08T14:25:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T14:25:10Z" id="51608689">This works correctly in aggregations.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow for 'grace period expiration' before shard reallocation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3569</link><project id="" key="" /><description>It would be really useful to allow for a 'grace period' between when ES notices that a particular node has gone down, and shard-reallocation begins. There are times when we might want to do a quick restart of an ES node ... or take one down for a full reboot ... and we don't want to do a re-allocation of shards because thats a very IO-intensive operation. In our case, we also use the Zookeeper plugin, and a shard-reallocation is triggered by a short communication break between the ES nodes and Zookeeper.
</description><key id="18528282">3569</key><summary>Allow for 'grace period expiration' before shard reallocation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">diranged</reporter><labels><label>stalled</label></labels><created>2013-08-25T23:13:48Z</created><updated>2015-08-26T14:19:48Z</updated><resolved>2015-08-26T14:19:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-26T08:22:43Z" id="23249528">I think you can simply use disable allocation via the cluster settings API [`cluster.routing.allocation.disable_allocation`](http://www.elasticsearch.org/guide/reference/modules/cluster/) If you have scheduled downtimes that is the way to go. Does that resolve your issue, if so can you please close the issue? 
</comment><comment author="diranged" created="2013-08-26T17:25:17Z" id="23279585">Not really -- there are cases where we may see a little blip in the network that trigger the shard reallocation. I think its a pretty worth-while feature to say "wait 5m for things to settle before re-allocating shards". This is also useful for cases where you may be bringing up more than 1 additional node in terms of capacity.
</comment><comment author="Plasma" created="2014-03-28T01:00:20Z" id="38879248">Was curious about this too; I'd like to see ES give me a grace period of something like 5 minutes before deciding to rebalance shards based on a node going offline.

Most likely that node is coming back up (eg, due to maintenance reboot of the service or host) and would be preferred the cluster remain in a degraded state to hope that the node comes back online, but then consider it dead after a certain timeout and rebalance as needed.
</comment><comment author="clintongormley" created="2014-09-05T09:18:45Z" id="54602544">If a shard disappears briefly then returns, it needs to recover from the primary shard in case anything has changed. Currently, that is likely to mean copying lots of segments over from the primary (as the primary and replica will probably have diverged).  Making this fast will not be possible until #6069 is implemented.

We can revisit this issue on #6069 is in.
</comment><comment author="jlintz" created="2014-09-27T13:44:33Z" id="57053053">Would love to see a configurable timeout for this as well.  While we can disable_allocation for planned events, we've had cases where a network blip will cause indices to begin reshuffling 
</comment><comment author="cfeio" created="2015-02-17T21:00:34Z" id="74752163">+1, We would really love to see a configurable timeout also, if not being able to disable it completely. 

In an unplanned outage we would prefer to  be able to configure not have replicas reassigned and shuffled, as this leads to a lot of data moving. As far as I know I am not aware of such a setting.
</comment><comment author="shyem" created="2015-02-19T15:38:48Z" id="75073175">+1, this would be really helpful for us as we have a massive amount of data in our ELK cluster and since we already have 1 replica available still there is no point of duplicating that one more time.
</comment><comment author="jpountz" created="2015-08-26T14:19:48Z" id="135037221">Fixed via https://github.com/elastic/elasticsearch/pull/11712
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Service should not start after package install</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3568</link><project id="" key="" /><description>This is just a suggestion. After an RPM install the elasticsearch service gets enabled and started automatically, this should not happen.

Debian/Ubuntu always does it and that's just wrong. It becomes more annoying when trying to automate things with configuration management.

We would like to have full control over whether to enable the service or when to start it.
</description><key id="18511418">3568</key><summary>Service should not start after package install</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vaijab</reporter><labels /><created>2013-08-24T19:50:53Z</created><updated>2013-09-13T13:53:12Z</updated><resolved>2013-09-13T13:53:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-09-13T07:34:04Z" id="24377968">we are working on making this configurable for all packages, see https://github.com/elasticsearch/elasticsearch/pull/3549
</comment><comment author="vaijab" created="2013-09-13T13:53:12Z" id="24395741">Perfect. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow logging level to be changed without a restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3567</link><project id="" key="" /><description>It would be great to be able to dynamically change log levels.
</description><key id="18480593">3567</key><summary>Allow logging level to be changed without a restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-08-23T16:52:42Z</created><updated>2013-08-23T17:10:15Z</updated><resolved>2013-08-23T17:10:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Setting index/bulk thread pools with queue_size can cause replica shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3566</link><project id="" key="" /><description>Setting index/bulk thread pools with queue_size can cause replica shard failures
closes #3526
</description><key id="18469578">3566</key><summary>Setting index/bulk thread pools with queue_size can cause replica shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2013-08-23T13:17:23Z</created><updated>2014-06-14T05:34:15Z</updated><resolved>2013-08-23T17:26:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-08-23T17:26:26Z" id="23178255">pushed....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Proposed simplifications to _suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3565</link><project id="" key="" /><description>Recent changes to configure field suggestions (see http://www.elasticsearch.org/blog/you-complete-me/) are a step forward, but may be improved as follows.

I would suggest a default configuration that would work on any field (i.e. no special configuration needed in the mapping file).

Calling a suggest for a field would look like this:

curl -X POST localhost:9200/hotels/_suggest -d '{
  "text": "m",
  "field": "title"
}'

This would work on using all emitted tokens for the "title" field, whatever is specified there (analyzers, whether or not to lowercase, split) and autosuggest words from it. It's likely this would be too slow and indeed FSTs need to be stored at index time, and may need to be in the mapping file. If so, then not as a different type, but as an add-on flag:

{
  "mappings": {
    "hotel": {
      "properties": {
        "title": { "type": "string", "store_suggest_fsts": true}
      }
    }
  }
}

Note that the suggester would use the same field (query) analyzer on the input, so to also split the query!

This means that typing in "state un" would be analyzed in 2 tokens: "state" and "un" (because the title field is a default "string"). For both words the suggester could suggest alternatives, but by default it could do it only for the last word (assuming the user is still typing there). This could then match the word "union" if there is a document with "State of the Union" as the title. (of course also many other "un*" words).

This is different than the current implementation, because there the suggester only works for full matches from the beginning of the field: if we would have a "title_suggest": {"type":"completion"} field, then typing in "state un" would find NOTHING, because there is no title that matches "state un*"

This is better because it could work with 0 or very limited configuration, and also fits the use case of actual suggestions better (where we are not merely matching against a simple prefixable field, but against freetext).

It would be nice if the suggester could then also boost the "union" completion because there is a colocated "state" match, but that is left as an exercise for the ElasticSearch gurus ;-)

By deferring the definition of the atoms to match against to the existing, flexible analyzers, this gives this much more control. We could even add fuzziness or stemming to the match targets...

WDYT
</description><key id="18465583">3565</key><summary>Proposed simplifications to _suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">anneveling</reporter><labels /><created>2013-08-23T11:27:56Z</created><updated>2015-09-21T18:25:44Z</updated><resolved>2015-09-21T18:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-23T12:36:57Z" id="23160810">Anne, thanks for opening this issue. First of all I think I need to elaborate on how the actual suggestions work vs. a fulltext search. In a fulltext search you basically have the following steps:
1. tokenize
2. build a query (in your example "state" and "un*")
3. rewrite the query (in your case must:(state), should("unfold", "united", "unified", "unreal", "undefined", "under", ...) )
4. run the query and fined the union
5. return the result (stored fields)

in a suggestion you do things slighlty different
1. tokenize
2. build all possible paths through the tokenized phrase ("state un" in your case)
3. check if this is a prefix in the suggester
4. return the payload and the output form (this is important if you have different output forms than input forms like in asian languages or in certain domains)

when you build you datastructure you can use different tokenstreams like stopword tokensfiler to make "state of the union" work. If you use a naive stopword filter like on fulltext search you can easily bring in a bad user experience like somebody types `state o` and you return `state of the union` but once somebody types the next char you get nothing since you `of` is a stopword. So this already requires specialized token streams for suggest vs. search.

now let me go further and tell you what the most important property of suggestions is... `speed`!! You have 200ms at most! from the keystroke until the suggestion needs to be rendered! That means including latency from the client (that is even more important if you are on mobil devices), all the processing in your infra etc. 

Ok seems doable but on a reasonable index a prefix query that is completely unbounded essentially is translated into a boolean query for all the expansions.... lets think of the `state un` case again or better `state u` were we run a prefix query for `u` ok that seems like fun.... and here our 200ms are gone already without latency taken into account. Now you can use edgeNGrams etc. but then you still need to score and load stored fields etc. and it requires certain configuration. 
Anyway.... it seems like there is a chance that we can make this work on an ordinary index, what could break it? I think what most users don't think about is that their infrastructure they run this on is build for N Queries Per Second but once you add suggest to it you suddenly get hit by `N + N * AverageQueryLength` and if you do that off a normal index even a single disk access can kill your entire user experience. With something like a pre-build dedicated in-memory structure you can easily survive this and then your suggestion / completion is actually useful.

if you want to make infix suggestions work, we are working on a suggest implementation that does that but it has several implications as well since it uses index sorting and has weight notions etc. 

Aside of all the speed stuff, the biggest problem with running something on top of an ordinary index is that you don't have any notion of weight when you look at a term `unfolded` is as good as `unreal` or `unified` (note _frequency is not a good indicator!_) this means you literally need to look at all of them in all your segments... 

&gt; By deferring the definition of the atoms to match against to the existing, flexible analyzers, this gives this much more control. We could even add fuzziness or stemming to the match targets...

you can do that with the `completion suggester` already! you can use stemmers and return the unstemmed version without loading stored fields. You can also use fuzzy matching (there is a nuts fast fuzzy prefix suggester already pushed to `0.90.4`  you have all the flexibility any other analyzer would give you. 

&gt; It would be nice if the suggester could then also boost the "union" completion because there is a colocated "state" match, but that is left as an exercise for the ElasticSearch gurus ;-)

I really appreciate that you left the hard part to us :) ...nevermind I can see that this sounds like the most likely thing to do but it's often in practice not feasible. if you have a small corpus this might be the way to go but in the ES case it's unlikely that your corpus is very small.
</comment><comment author="anneveling" created="2013-08-23T12:46:36Z" id="23161299">Sure, I understand (on the high level) what the new suggester does and how it is different from search.

In my example "state un" would match on "union" even without a stopword filter for the "of the" in "State of the Union", that was my whole point.

Sure, it is a challenge. Sure, it needs to be fast. But that's just saying: yes this would be nice, but we haven't found a way to make it lightning-fast like we require all ES functions to work. That all the implementations we can think of are not fast enough, does not mean it's not possible by doing something smart.

My main point is, that the suggester (old and new) does not work for multiword suggestions, only like a standard prefix query. And all my real life use cases have always requested to work on multiple words separately as well. I have done some work in query analyzers to make that work, and was hoping that ES would start to come out of the box with a similar solution to work for autocompletion of words in a field that usually contains entire sentences (like a title), not just for controlled-vocabulary fields.

And I know the best way to do this would be to implement this myself and create a pull request. Was just wondering whether you would feel a suggester that works as suggested [sic] would be valuable if it would be fast enough
</comment><comment author="s1monw" created="2013-08-23T12:59:26Z" id="23161995">&gt; In my example "state un" would match on "union" even without a stopword filter for the "of the" in "State of the Union", that was my whole point.
&gt; I can see your point - I just want to manage expectations here. This suggest stuff has many aspects and stopwords play an even greater role here. What do you do if somebody types a stopword? Do you scarify 1. performance and 2.  relevance? It's very hard though.
&gt; 
&gt; And I know the best way to do this would be to implement this myself and create a pull request. Was just wondering whether you would feel a suggester that works as suggested [sic] would be valuable if it would be fast enough

sure go ahead I am already looking forward to review it.

&gt; My main point is, that the suggester (old and new) does not work for multiword suggestions, only like a standard prefix query. 

actually I don't think this is near to the truth. This is much more flexible than a prefix query, it has weighting, it's blazing fast, it has analyzer support and it works perfectly for titles. 
</comment><comment author="clintongormley" created="2014-07-11T10:28:28Z" id="48715825">@areek Implementing the AnalyzingInfixSuggester could be the way to go here, although it wouldn't be config free?
</comment><comment author="clintongormley" created="2015-09-21T18:25:44Z" id="142067727">Closing in favour of #13692
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dont write pidfile twice on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3564</link><project id="" key="" /><description>There is no need to write the pidfile in the bin/elasticsearchshell script
as this happens already in the java code.

Also cleaning up the bin/elasticsearch shell script a bit (no need to return
an error code when exec is called, as this forks and exits the shell script
immediately).

Closes #3529
Closes #1745
</description><key id="18463802">3564</key><summary>Dont write pidfile twice on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-23T10:37:34Z</created><updated>2014-07-11T08:19:42Z</updated><resolved>2013-08-23T11:21:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Id cache statistics not updated when id_cache is being cleared</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3563</link><project id="" key="" /><description>Issue found reported via pr #3561. 

The id cache statistics aren't updated in the case the clear cache api is invoked and id cache entries are evicted when a segment is merged away.
</description><key id="18458298">3563</key><summary>Id cache statistics not updated when id_cache is being cleared</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-23T08:02:48Z</created><updated>2013-08-23T08:10:32Z</updated><resolved>2013-08-23T08:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-23T08:10:32Z" id="23149330">Pushed to master:
https://github.com/elasticsearch/elasticsearch/commit/c76e589bc56599b6c1023b66962757d8ce747037

and 0.90:
https://github.com/elasticsearch/elasticsearch/commit/253a917d8894f4fdda506c567a38f124243a3355
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>djhgi43fphoopy hf ts qwfa ZX JXKMX ÂAZZKSZ&gt; ZAS˘ Zß≥åœ lnjqiw qwooww1'[112`qq</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3562</link><project id="" key="" /><description /><key id="18449247">3562</key><summary>djhgi43fphoopy hf ts qwfa ZX JXKMX ÂAZZKSZ&gt; ZAS˘ Zß≥åœ lnjqiw qwooww1'[112`qq</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicktackes</reporter><labels /><created>2013-08-23T01:14:32Z</created><updated>2013-08-23T04:13:13Z</updated><resolved>2013-08-23T04:13:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Call onRemoval of shard IdCache during clear.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3561</link><project id="" key="" /><description>This looks like a copy/paste issue where onCached was being called
rather than onRemoval. This should fix the ID cache stats not being
correct after a call to /_cache/clear?id_cache=true
</description><key id="18446221">3561</key><summary>Call onRemoval of shard IdCache during clear.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">deverton</reporter><labels /><created>2013-08-22T23:26:10Z</created><updated>2014-06-18T19:05:04Z</updated><resolved>2013-08-23T08:13:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deverton" created="2013-08-23T03:13:30Z" id="23141024">Here's a test case to demonstrate the issue. On my local 0.90.3 instance the stats outputs before the cache clear is:

``` json
 "id_cache" : {
          "memory_size" : "122b",
          "memory_size_in_bytes" : 122
 }
```

And after the clear it's:

``` json
 "id_cache" : {
          "memory_size" : "244b",
          "memory_size_in_bytes" : 244
 }
```

The test case just creates a single parent/child document mapping does one query, gets the stats, tries to flush the cache, then gets the stats again.

``` sh
#!/bin/bash

curl -XDELETE 'localhost:9200/idcache?pretty=true'

curl -XPUT 'localhost:9200/idcache/parent/1?pretty=true' -d '{
    "name" : "parent"
}'

curl -XPOST 'localhost:9200/idcache/child/_mapping?pretty=true' -d '{
    "child": {
        "_parent": { "type": "parent" }
    }
}'

curl -XPUT 'localhost:9200/idcache/child/1?parent=1&amp;pretty=true' -d '{
    "name": "child"
}'

curl -XGET 'localhost:9200/_refresh'

curl -XPOST 'localhost:9200/idcache/parent/_search?pretty=true' -d '{
    "query": {
        "has_child": {
            "type": "child",
            "query" : {
                "match_all": {}
            }
        }
    }
}'

curl -s -XGET 'localhost:9200/_cluster/nodes/_local/stats?clear=true&amp;indices=true&amp;pretty=true' | grep -A 2 id_cache

curl -XGET 'localhost:9200/_cache/clear?id_cache=true&amp;pretty=true'

curl -s -XGET 'localhost:9200/_cluster/nodes/_local/stats?clear=true&amp;indices=true&amp;pretty=true' | grep -A 2 id_cache
```
</comment><comment author="martijnvg" created="2013-08-23T07:54:17Z" id="23148694">Thanks for tracking this down! This is indeed a copy-paste bug. I'll pull it in soon.
</comment><comment author="martijnvg" created="2013-08-23T08:13:14Z" id="23149447">Pushed to master and 0.90 branches.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster Setting update can hang if gets settings which are not dynamically updatable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3560</link><project id="" key="" /><description>Also holds for settings that do not exist:

```
curl -XPUT "http://localhost:9200/_cluster/settings" -d'
{
    "transient": {
        "cluster.routing.allocation.same_shard.host": true
    }
}'
```

```
curl -XPUT "http://localhost:9200/_cluster/settings" -d'
{
    "transient": {
        "cluster": true
    }
}'
```
</description><key id="18432597">3560</key><summary>Cluster Setting update can hang if gets settings which are not dynamically updatable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-22T18:59:20Z</created><updated>2013-08-22T19:02:29Z</updated><resolved>2013-08-22T19:02:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/test/java/org/elasticsearch/test/unit/cluster/settings/ClusterSettingsTests.java</file></files><comments><comment>ClusterUpdateSettingsAction will hang if no changes were made</comment></comments></commit></commits></item><item><title>Random scoring should take into account index name and shard id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3559</link><project id="" key="" /><description>Relative to #1170 

Random scoring should take into account index name and shard id.
</description><key id="18422653">3559</key><summary>Random scoring should take into account index name and shard id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label></labels><created>2013-08-22T16:05:33Z</created><updated>2013-08-23T00:49:14Z</updated><resolved>2013-08-23T00:49:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java</file><file>src/test/java/org/elasticsearch/test/integration/search/functionscore/RandomScoreFunctionTests.java</file></files><comments><comment>random_score function - Added the index name and shard id to the randomization, and improved the PRNG itself</comment></comments></commit></commits></item><item><title>Avoid shading of org.joda.convert package, fixes #3557</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3558</link><project id="" key="" /><description>Another fix for shading issue.
</description><key id="18420125">3558</key><summary>Avoid shading of org.joda.convert package, fixes #3557</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">splatch</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2013-08-22T15:27:18Z</created><updated>2014-10-18T08:06:20Z</updated><resolved>2014-10-17T13:45:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T08:28:01Z" id="51575776">Hi @splatch 

Sorry it has a taken a whil to get to this, but is this still an issue? 
</comment><comment author="splatch" created="2014-08-19T13:23:00Z" id="52632190">hey @clintongormley, I think it is still a case, since fix for #4660 was a brute force. The issue described in my comment last year was ignored:
https://github.com/elasticsearch/elasticsearch/issues/3557#issuecomment-23472873

I'm not sure if Scala still has the issue with optional annotations and if elasticsearch still needs fix introduced in #4660, because as I said - by JVM spec annotations having runtime retention which classes could not be found during class resolving should be dropped.

Cheers,
Lukasz
</comment><comment author="s1monw" created="2014-08-22T08:01:09Z" id="53033575">I am trying to understand the issue here, if we only shade the `org.joda.time` package what are the implications? I am not a shading expert but afaik we then just keep the `org.joda` package for classes outside `org.joda.time` right? this might break bw compatibilty though, is this really such a big issue?
</comment><comment author="splatch" created="2014-08-28T09:13:08Z" id="53691856">Hey @s1monw,
You are right, sharding org.joda.time itself will be sufficient. Before elasticsearch used outdated maven-shade-plugin which was not relocating constant pool resulting half-shaded artifacts from bytecode point of view. After update of shade-plugin it's safe to have org.joda.time shaded back. All annotation references to org.joda.convert will work properly - just not sure how scala compiler will live with it. Maybe after 2.11/2.12 updates Scala got smarter? ;-)
</comment><comment author="s1monw" created="2014-08-28T10:22:28Z" id="53701977">@splatch I dont' have any way to see if this fixes anything - can you confirm your PR fixes the problem?
</comment><comment author="clintongormley" created="2014-10-16T19:34:09Z" id="59417263">@splatch could you confirm whether this patch fixes the problem you're experiencing?
</comment><comment author="splatch" created="2014-10-17T09:40:41Z" id="59490252">Yes, this pull request solves my problem. I checked it against 1.3.4 - commit can be cherry-picked.
</comment><comment author="clintongormley" created="2014-10-17T11:32:44Z" id="59500312">w00t - thanks
</comment><comment author="martijnvg" created="2014-10-17T13:45:44Z" id="59514574">Pushed via: https://github.com/elasticsearch/elasticsearch/commit/b0a14f0b631328f7b806ac531bf1e964f40a07b2
</comment><comment author="splatch" created="2014-10-18T08:06:20Z" id="59602921">Thx a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Avoid shading of org.joda.convert package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3557</link><project id="" key="" /><description>Annotations from package org.joda.convert package are used by couple of org.joda.time classes. However there is no other direct reference between these two projects. Annotations from joda-convert project are used only by this project, mainly to have unified way to call appropriate .parse(String) variants in classes like DateTime/DateMidnight/Period.

According to Java language specification annotations which are not present at the runtime are simply ignored and do not break linkage process, thus dependency is optional. As long as elasticsearch does not use org.joda.convert package this dependency may be omitted and excluded from shading.
</description><key id="18419779">3557</key><summary>Avoid shading of org.joda.convert package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splatch</reporter><labels /><created>2013-08-22T15:21:40Z</created><updated>2014-10-17T13:44:47Z</updated><resolved>2014-10-17T13:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-08-23T09:35:19Z" id="23153129">what is the actual problem that you experienced with the current shading of joda?
</comment><comment author="splatch" created="2013-08-23T14:19:03Z" id="23166586">There is no problem with shaded joda-time, there is problem with relocation of org.joda.convert to org.elasticsearch.common.joda.convert. Bytecode points to this package however elasticsearch distribution does not contain it.
</comment><comment author="kimchy" created="2013-08-26T13:09:56Z" id="23261115">we can do that, though I am still confused to be honest, what error does it cause? I would suspect it will cause the same problem when someone simply uses the joda time jar, cause it doesn't include those annotations as well...
</comment><comment author="splatch" created="2013-08-29T08:02:08Z" id="23472873">You are right, if you are using elasticsearch/joda-time jar in standard environment like J2SE or J2EE it doesn't matter. However for OSGi tools we have it is not so obvious, so we need to mark package org.elasticsearch.common.joda.convert as optional all the time when we try to use elasticsearch.

The benefit of not shading org.joda.convert will be that all who wants to use joda-convert will be able to use it together with elasticsearch. Currently shaded joda time objects will be unknown for joda-convert since annotation names are not org.joda.convert.ToString but org.elasticsearch.common.joda.convert.ToString. This package does not exist, annotation will be dropped. Code below will not work differently for each instance:

``` java
org.joda.convert.StringConvert.convertToString(org.elasticsearch.common.joda.time.DateTime.now());
org.joda.convert.StringConvert.convertToString(org.joda.time.DateTime.now());
```
</comment><comment author="kimchy" created="2014-03-28T00:00:01Z" id="38875811">coming back to this, we added Joda convert because of this issue #4660, a bit of conflicting requirements....
</comment><comment author="clintongormley" created="2014-08-08T14:19:33Z" id="51607999">See PR #3558 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Avoid shading of org.joda.convert package, fixes #3557</comment></comments></commit></commits></item><item><title>The `plugin` script returns 0 status on failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3556</link><project id="" key="" /><description>When an installation of a plugin with the `plugin` script fails, it returns an incorrect exit code: 0. Example:

```
vagrant@precise64:~$ sudo /usr/local/bin/plugin --install FOOBAR
-&gt; Installing FOOBAR...
Failed to install FOOBAR, reason: failed to download out of all possible locations..., use -verbose to get detailed information
vagrant@precise64:~$ echo $?
0
```

This makes it cumbersome in automated scenarios; related: elasticsearch/cookbook-elasticsearch#124
</description><key id="18402787">3556</key><summary>The `plugin` script returns 0 status on failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2013-08-22T09:34:15Z</created><updated>2013-08-22T09:46:46Z</updated><resolved>2013-08-22T09:46:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2013-08-22T09:46:46Z" id="23078988">Duplicate of elasticsearch/elasticsearch#3463 , closed by https://github.com/s1monw/elasticsearch/commit/f0cce80b613b105871d1737556cbfad3ce497b0f
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException during concurrent merges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3555</link><project id="" key="" /><description>Looks like it may be related to the new suggestion stuff. I'm on 0.90.3, Java 7. Will be trying to reproduce on master shortly, but let me know if there's other stuff you need to debug.

[2013-08-21 22:02:05,166][WARN ][index.engine.robin       ] [qanticharm] [quizlet][9] failed engine
org.apache.lucene.index.MergePolicy$MergeException: java.lang.NullPointerException
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:99)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$1$1.finish(AnalyzingCompletionLookupProvider.java:134)
        at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$SuggestFieldsConsumer$1.finish(Completion090PostingsFormat.java:152)
        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:204)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:365)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:98)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3772)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3376)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
[2013-08-21 22:02:05,180][WARN ][cluster.action.shard     ] [qanticharm] sending failed shard for [quizlet][9], node[27LWb8rFQiafhZhgRH4vjg], [R], s[STARTED], reason [engine failure, message [MergeException[java.lang.NullPointerException]; nested: NullPointerException; ]]
</description><key id="18383503">3555</key><summary>NullPointerException during concurrent merges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rdeaton</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-21T22:04:50Z</created><updated>2013-09-05T06:24:08Z</updated><resolved>2013-08-22T10:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rdeaton" created="2013-08-22T03:29:23Z" id="23066286">The problem persists on master.
</comment><comment author="s1monw" created="2013-08-22T06:54:20Z" id="23071798">I think I know what the problem is.... your segment might have no docs in that field... I will try to write a test and come up with a patch
</comment><comment author="s1monw" created="2013-08-22T10:21:56Z" id="23080602">@rdeaton can you try if that fixes your issue? Thanks
</comment><comment author="rdeaton" created="2013-08-22T18:45:36Z" id="23115281">Seems to be working. Thanks a bunch.
</comment><comment author="s1monw" created="2013-08-22T20:29:04Z" id="23122903">thanks for verifying!!!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Prevent NPE if all docs for the field are pruned during merge</comment></comments></commit></commits></item><item><title>Removed ambiguously-worded ES_HEAP_SIZE comment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3554</link><project id="" key="" /><description>Multiple people have misread the original comment "(defaults to 256m min, 1g max)" as meaning "you must set this to a value between 256m and 1g" - probably partly due to the wording, partly due to it being slightly odd to ["[recommend setting] the min and max memory to the same value"](set the min and max memory to the same value) yet not defaulting to doing so.

In the absence of being able to word it better, I've just stripped it out to minimise confusion (and none of the other settings have their defaults included in the comments)
</description><key id="18381776">3554</key><summary>Removed ambiguously-worded ES_HEAP_SIZE comment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">doismellburning</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2013-08-21T21:31:28Z</created><updated>2014-10-16T20:47:29Z</updated><resolved>2014-10-16T18:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-22T05:32:42Z" id="23069378">That's true. I already saw that confusion in the french mailing list as well.
That said, I think we should write that heap is set by default with a range 256m to 1g (-Xmx -Xms) or something but not removing the comment.
</comment><comment author="doismellburning" created="2013-08-22T09:16:45Z" id="23077474">By all means, please, ignore this in favour of a better statement if you have one!
</comment><comment author="clintongormley" created="2014-05-06T11:52:15Z" id="42293273">/cc @spinscale 
</comment><comment author="doismellburning" created="2014-10-10T18:34:20Z" id="58697889">Hi. Anything I can do to help here?
</comment><comment author="clintongormley" created="2014-10-16T10:27:42Z" id="59342264">Perhaps we should be _more_ explicit instead of less?  eg:

```
# Heap size defaults to 256m min, 1g max 
# Set ES_HEAP_SIZE to 50% of available RAM, but no more than 31g
#ES_HEAP_SIZE=2g
```
</comment><comment author="doismellburning" created="2014-10-16T17:53:47Z" id="59402181">I am very much in favour of an improved comment. The point of this PR was originally "lets remove the misleading comment until a better one is found" since I figured that was a quick win that could be merged right away, and "coming up with better wording" could happen elsewhere...
</comment><comment author="clintongormley" created="2014-10-16T18:59:30Z" id="59412135">Done - thanks for opening.
</comment><comment author="doismellburning" created="2014-10-16T20:47:29Z" id="59427754">Thanks very much :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Improve amibiguous ES_HEAP_SIZE warning in deb init script</comment></comments></commit></commits></item><item><title>Updates and inserts are silently failing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3553</link><project id="" key="" /><description>After adding new field in elastic search index updates and inserts started failing.
Status of cluster is green and there is no information in log.
We tried restarting elastic search.
Index status seems fine
Also tried to close and re-open the index but nothing helped.
We are using tire gem to interact with elastic search.
We created new index and same code works fine with new index.
I didn't find any help on internet regarding this issue.
Can some please help us to troubleshoot this issue.
</description><key id="18379548">3553</key><summary>Updates and inserts are silently failing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">sandeepbeit</reporter><labels /><created>2013-08-21T20:51:11Z</created><updated>2014-07-23T12:34:08Z</updated><resolved>2014-07-23T12:34:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-23T06:49:04Z" id="23146421">Hey,

are you able to provide more information?
- What operations did you execute (Tire allows you to return the JSON/curl call of an operation, these are important to know).
- Did you get any errors while indexing the data on the client side?
- Is this reproducible? Do you still have the index lying around on the filesystem, which could be used for testing/debugging further?
- What elasticsearch version did you use? How is the mapping looking like? Are there differences between the two indices?
</comment><comment author="sandeepbeit" created="2013-08-23T12:18:08Z" id="23159966">I have more information available now.I will add shortly.

Sent from my iPhone

On Aug 23, 2013, at 2:49 AM, Alexander Reelsen notifications@github.com wrote:

&gt; Hey,
&gt; 
&gt; are you able to provide more information?
&gt; 
&gt; What operations did you execute (Tire allows you to return the JSON/curl call of an operation, these are important to know).
&gt; Did you get any errors while indexing the data on the client side?
&gt; Is this reproducible? Do you still have the index lying around on the filesystem, which could be used for testing/debugging further?
&gt; What elasticsearch version did you use? How is the mapping looking like? Are there differences between the two indices?
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="sandeepbeit" created="2013-08-23T15:47:47Z" id="23172308">The message coming back from Elastic search is below
{"index":{"_index":"XXX","_type":"XXX","_id":"id-xxx",
"error":"MapperParsingException[Failed to parse [standout]]; nested: 
JsonParseException[Current token (VALUE_FALSE) not numeric, can not use numeric 
value accessors\n at [Source: [B@7e27aad5; line: 1, column: 3690]]; "}

We didn't get any error on client side which is weird to me.There is an exception being returned but it is wrapped in "error" keu as shown above.

We have kept the old index for to debug this issue and we are now consistently able to replicate it.

Elastic serach version we are using is 0.20.6.

There are not difference in old and new index. 
</comment><comment author="javanna" created="2013-08-26T17:22:51Z" id="23279421">There seems to be something wrong with your mapping. Looks like a field that was defined as numeric (standout) has been sent as a non numeric with some of the documents. Is that possible? Did you define your mapping upfront for that field?

Also, the error is quite clear, thus I don't understand what you mean when you say that the index operations silently fail. Could you please clarify that?

Thanks!
</comment><comment author="sandeepbeit" created="2013-08-28T15:05:32Z" id="23421912">No we didn't define the field upfront.First time when we added documents with standout field the index became corrupt but when we created new index it worked fine.Do you have any idea why it behaved like that?

Even though the mapping got corrupt the response code sent by elastic search was 200 and inside the response there was error attribute which had the exception details.Rather than doing that ES should raise an exception with 500 response code.
</comment><comment author="javanna" created="2013-08-29T10:40:54Z" id="23480478">If you didn't define the field upfront in the mapping, then it was dinamically added the first time that field appeared in a document, with that specific type , I suppose numeric. Afterwards other documents had that same field as a string and that caused the error.

What do you mean by "the index got corrupted"? Were you using the bulk api?
</comment><comment author="sandeepbeit" created="2013-09-06T23:03:12Z" id="23974874">We defined the mapping before adding the data but didn't create the field in index before using it.Below is our mapping(We are using Tire gem)

 property :standout, type: 'boolean', default: false

By ""the index got corrupted"" I mean that all the subsequent insert also started failing silently without giving any exception.I thing this is bug in Elastic search.

We were not using bulk API.
</comment><comment author="javanna" created="2013-09-09T16:20:11Z" id="24091281">Sorry but it is hard for me to understand what exactly is going on and how I can reproduce the problem. You say that all following insertions silently fail, but you also said above that you got back a MapperParsingException and the reason seems to be a conflict in the mapping.

If this is a bug I definitely want to fix it. Could you provide a complete curl recreation so that we can reproduce it and understand it better? Have a look [here](http://www.elasticsearch.org/help/) to know more on how you can help us out with that.
</comment><comment author="clintongormley" created="2014-07-23T12:34:08Z" id="49867187">No feedback in 10 months. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recheck cutoffScore during phrase_suggest merge.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3552</link><project id="" key="" /><description>The goal is to throw out suggestions that only meet the cutoff in some
shards.  This will happen if your input phrase is only contained in a
few shards.  If your shards are unbanced this rechecking can throw out
good suggestions.

Closes #3547.
</description><key id="18361578">3552</key><summary>Recheck cutoffScore during phrase_suggest merge.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-08-21T15:30:44Z</created><updated>2014-07-16T21:52:31Z</updated><resolved>2013-09-06T19:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-08-22T14:42:11Z" id="23095352">I've amended the pull request with @s1monw's suggestions.  Github decided it would throw out the comments which is odd.
</comment><comment author="nik9000" created="2013-08-23T12:44:32Z" id="23161196">All suggested changes made.
</comment><comment author="s1monw" created="2013-08-24T07:02:18Z" id="23204272">@nik9000 I did another review and had some more comments about stuff I missed previously. I think we are very close. I really just missed all the bw compat stuff. Can you fix those as well? I think then we are ready! :)

I also commented some stuff on the tests just to point you to some util methods! thanks!
</comment><comment author="s1monw" created="2013-08-26T13:29:32Z" id="23262262">huh? @nik9000 why did you close this PR?
</comment><comment author="nik9000" created="2013-08-26T13:37:35Z" id="23262782">I have no idea. I actually just read it again to make the changes.
</comment><comment author="s1monw" created="2013-08-26T13:38:25Z" id="23262838">ah phew! :) github sometimes does weird things :) - just software I guess....
</comment><comment author="nik9000" created="2013-08-26T13:43:11Z" id="23263116">Indeed.  I must have click "Close" somewhere intending to close a form....  Any way, just applied your suggestions and rebased.  I'm running the tests now.

I'm not sure we need the version check on the read given that the other side wouldn't send the right type value to even create a PhraseSuggestion but I added the check anyway just so it was balanced and a comment.
</comment><comment author="nik9000" created="2013-08-26T14:06:58Z" id="23264622">Rebased and made all requested changes.
</comment><comment author="nik9000" created="2013-09-06T19:56:08Z" id="23965152">This has been merged.  I'm not sure why it is still open.  Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugin Manager can not download _site plugins from github</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3551</link><project id="" key="" /><description>Sounds like github changes a bit download url for master zip file.

From `https://github.com/username/reponame/zipball/master` to `https://github.com/username/reponame/archive/master.zip`.

We need to update plugin manager to reflect that change.
</description><key id="18355891">3551</key><summary>Plugin Manager can not download _site plugins from github</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-21T13:57:43Z</created><updated>2013-08-27T15:29:25Z</updated><resolved>2013-08-27T15:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-21T15:49:38Z" id="23027235">Commit updated thanks to @karmi!
</comment><comment author="javanna" created="2013-08-27T14:35:42Z" id="23341171">It looks like the url user/repo/zipball/master is now working. It wasn't working back when this issue was opened, therefore we changed the github url where we get the archive from. Unfortunately the structure of the archive is slightly different when downloaded from the new url, since it contains a subfolder that has a slightly different naming, which we relied on in order to strip that part of the path from every zip entry and put all the files directly under the _site folder. To be more specific, the archives contain now a `repo-master` top-level folder, while using the other address the top-level folder is `user-repo-hash`.

We would like to keep the new adopted url format (user/repo/archive/master.zip) since it seems more future-proof, but we need to adapt the zip extraction process too in order for this to work properly.

Reopening this issue as I'm working it, a fix will follow.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Fixed extraction of site plugins downloaded from github, so that we skip the top-level folder and we place the files directly under the _site folder</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Plugin Manager can not download _site plugins from github</comment><comment>Sounds like github changes a bit download url for master zip file.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Plugin Manager can not download _site plugins from github</comment><comment>Sounds like github changes a bit download url for master zip file.</comment></comments></commit></commits></item><item><title>Completion Suggester: Allow payload to be a value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3550</link><project id="" key="" /><description>Allow the payload to be a value, compared to just a json object.
</description><key id="18353812">3550</key><summary>Completion Suggester: Allow payload to be a value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-21T13:33:30Z</created><updated>2013-08-21T13:34:03Z</updated><resolved>2013-08-21T13:34:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java</file><file>src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestion.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/unit/common/xcontent/builder/BuilderRawFieldTests.java</file></files><comments><comment>Completion Suggester: Allow payload to be a value</comment><comment>closes #3550</comment></comments></commit></commits></item><item><title>Allow packages to disable immediate restart on package upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3549</link><project id="" key="" /><description>Both package types, RPM and deb now contain an option to not restart on upgrade.

This option can be configure in /etc/default/elasticsearch for dpkg based systems
and /etc/sysconfig/elasticsearch for rpm based systems. If the RESTART_ON_UPGRADE option is set to false, the node will not be restarted after the package has been installed.

By default the setting is as before, where a restart is executed on upgrade.
</description><key id="18342062">3549</key><summary>Allow packages to disable immediate restart on package upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-21T08:37:57Z</created><updated>2014-07-16T21:52:31Z</updated><resolved>2013-09-13T15:47:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-09-11T21:38:30Z" id="24278614">Looks good, although I think `RESTART_ON_UPGRADE` should be `false` by default. It's better to get bitten by it not restarting than the other way.
</comment><comment author="spinscale" created="2013-09-12T13:37:17Z" id="24319471">Still unsure about this as it changes the defaults... to the better though
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Got only one Node but Status is "Green with 4 Nodes"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3548</link><project id="" key="" /><description>configuration (One Node only)

Logstash_1.1.13
Elasticsearch_0.20.6
Apache_2.2 + passenger_module
Kibana_2

for some reason the cluster state show i have 4 nodes and the status is green:

{
  "cluster_name" : "startapp",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 4,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 30,
  "active_shards" : 30,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0
}

as far as i can see from: 
http://chrissimpson.co.uk/elasticsearch-yellow-cluster-status-explained.html

Since i only have one Node, the status should be Yellow with one node
I tried to check Network, hostname, PTR + A record And checked Elasticsearch configuration and found no reason for this to happen

What should i check next ?
</description><key id="18339232">3548</key><summary>Got only one Node but Status is "Green with 4 Nodes"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imneo1</reporter><labels><label>non-issue</label></labels><created>2013-08-21T07:14:44Z</created><updated>2013-08-21T07:28:21Z</updated><resolved>2013-08-21T07:28:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-21T07:28:07Z" id="22999844">your output show 4 nodes since you might have 4 clients connected or the cluster thinks it has 4 clients connected. This can happen if you start up java client nodes that join the cluster, maybe logstash does that. It seems you have 30 shards but 0 replicas for some reason so all the 30 shards are allocated on that one data node you are talking about, hence you clusterstate is green. In general please ask those kind of questions on the mailing list, this is only for development issues or bugs. 

thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Phrase suggest can make bad suggestions when searching for an uncommon phrase and confidence &gt; 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3547</link><project id="" key="" /><description>It looks like sometimes documents get sharded in such a way the phrase suggest confidence stops working from time to time.  I have a gist (https://gist.github.com/nik9000/6287620) that normally produces a phrase response with no suggestions (like it should) but if you run it a bunch of times (5-20) it'll eventually return one with suggestions.  I've determined that if you set the number of shards to 1 this won't happen.
</description><key id="18323954">3547</key><summary>Phrase suggest can make bad suggestions when searching for an uncommon phrase and confidence &gt; 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2013-08-20T21:56:02Z</created><updated>2013-08-29T14:54:57Z</updated><resolved>2013-08-29T14:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-08-21T01:25:54Z" id="22989922">So I _think_ this is caused by the confidence being checked per shard and not rechecked when the results from the shards are merged together.  That'd be a problem because the input phrase's score is going to be different on different shards.  If the input phrase is rare it could be vastly different.  That'd be a problem because you might throw away great candidates on a shard where the input phrase scores well but keep garbage ones on a shard where the input phrase doesn't.
</comment><comment author="nik9000" created="2013-08-21T02:02:16Z" id="22991053">So I modified the super heroes test to search for something in the corpus and the input phrase score varied across shards by two order of magnitude.

So I'm not really worried about throwing out really good suggestions on one shard because you can set shard_size &gt; size and that won't happen.  The docs probably ought to be updated to reflect that.

What I am worried about it that if confidence &gt; 1 then the input phrase will never be returned from the shards but on shards where the input phrase scores low then bad suggestions will be returned.
</comment><comment author="s1monw" created="2013-08-21T07:33:53Z" id="23000049">@nik9000 thanks for opening this! 

&gt; So I modified the super heroes test to search for something in the corpus and the input phrase score varied across shards by two order of magnitude.

I guess that is expected. Your data seems pretty lowish and therefore you might have large differences in term distribution etc. I don't think we can do much about that it somehow goes along the lines of distributed DF vs. local (per shard) DF.

&gt; What I am worried about it that if confidence &gt; 1 then the input phrase will never be returned from the shards but on shards where the input phrase scores low then bad suggestions will be returned.

hmm you mean we should calculate a global confidence for the input? I'd argue that this is the same discussion related to document frequency. I think this can easily happen if you very unbalanced shards ie. shard by type or so. But then you should also route you suggest request accordingly? Don't get me wrong, I am not saying this isn't a valid complain I'm just trying to figure out what and if a fix makes sense here. Do you have ideas?
</comment><comment author="nik9000" created="2013-08-21T14:59:45Z" id="23023369">It looks to me like if your shards are out of balance and you can't route your request appropriately then the phrase suggester is not going to work well for you because it uses shard local term frequency and total term count.  That makes sense and should probably be documented but it isn't my problem.

My problem is that my phrases are rare.  The phrase suggester does a good job of finding those rare phrases and suggesting rare phrases but if the user searches for a rare phrase then on one shard it'll have a high score and on the others its score will be low.  If the confidence is &gt; 1 then you'll get suggestions only from the shards that don't contain the phrase and they won't be as good as the original phrase

So I have fix that throws those bad suggestions out by rechecking the cutoffScore during the reduce phrase.  I haven't rebased it or made sure it doesn't break any tests beyond those in SuggestSearchTests.  It does have a problem in that if your shards are unbalanced in might through out good suggestions.  I don't imagine I'll see this in production but some of the tests intentional work on very few documents.  I think this is a fair trade.  At least, it is for me.
</comment><comment author="nik9000" created="2013-08-21T15:32:35Z" id="23025920">The follow tests fail but seemed to fail at least some of the time before I made my change:
- org.elasticsearch.test.integration.search.suggest.SuggestSearchTests.testShardFailures
- org.elasticsearch.test.unit.cluster.routing.allocation.SameShardRoutingTests.sameHost
- org.elasticsearch.test.unit.action.suggest.SuggestActionTests.testShardFailures
</comment><comment author="s1monw" created="2013-08-22T11:59:17Z" id="23084802">@nik9000 I totally agree with you here that this can be a problem. I am just not 100% sure if that will do a really good job for most of the users. But on the other hand I don't have a good argument against it. So lets say we move forwards with this... I think your PR looks good, I have a couple of comments I will put on the PR and I know why those tests fail... not a big deal 
</comment><comment author="nik9000" created="2013-08-29T14:54:57Z" id="23495542">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/Suggest.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/NoisyChannelSpellChecker.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestion.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/SuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/unit/search/suggest/phrase/NoisyChannelSpellCheckerTests.java</file></files><comments><comment>Recheck cutoffScore during phrase_suggest merge.</comment></comments></commit></commits></item><item><title>Smarter default to `index.index_concurrency`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3546</link><project id="" key="" /><description>A single shard, which is a Lucene level, has a limit on the number of concurrent threads that are allowed to perform indexing at the same time. It defaults to 8 in Lucene, but in ES, we allow to change it using `index.index_concurrency`.

We should be smarter about setting the default value for it, specifically in cases where there is a single index being indexed into, with one shard on a node. We already have the index/bulk threads pools to protect and control concurrency, so we can increase that default value in most cases to something more relaxed.

On that other hand though, we don't want to default it to a too high value, cause it means each refresh/flush will end up creating more segments.

A safe default can be 65% of the number of bounded cores (bounded at 32), with a minimum of 8 (which is the default in Lucene).
</description><key id="18319012">3546</key><summary>Smarter default to `index.index_concurrency`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-20T20:21:28Z</created><updated>2013-08-20T20:26:49Z</updated><resolved>2013-08-20T20:26:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>Smarter default to `index.index_concurrency`</comment><comment>closes #3546</comment></comments></commit></commits></item><item><title>Bound processor size based cals to 32</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3545</link><project id="" key="" /><description>We use number of processors to choose default thread pool sizes, and number of workers in networking (for HTTP and transport). Bound it to max at 32 by default as a safety measure to create too many threads.

This relates to #3478, where we set the default to 24, but 32 is probably a better default.
</description><key id="18318702">3545</key><summary>Bound processor size based cals to 32</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-20T20:16:26Z</created><updated>2014-03-14T19:16:26Z</updated><resolved>2013-08-20T20:17:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Bound processor size based cals to 32</comment><comment>We use number of processors to choose default thread pool sizes, and number of workers in networking (for HTTP and transport). Bound it to max at 32 by default as a safety measure to create too many threads.</comment></comments></commit></commits></item><item><title>Wrong analyzer used when indexing dynamic property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3544</link><project id="" key="" /><description>Hi,

I'm seeing some unusual behaviour when indexing documents in Elasticsearch with a dynamic template. I have a unit test that's failing intermittently. The flow of the test is as follows:

```
1) Initialise in-memory Elasticsearch cluster (one local node, no replicas)
2) Create new index
3) Create new type mapping
4) Index some documents
5) Refresh index and wait for all documents to be processed
6) Query Elasticsearch for documents
```

The type mapping I'm using includes the following dynamic template definition:

```
{
    "participants": {
        "path_match": "participants.*",
        "mapping": {
            "type": "string",
            "store": "yes",
            "index": "analyzed",
            "analyzer": "whitespace"
        }
    }
}
```

This template is intended to produce fields of the form:

```
participants.new = [ 'user-1', 'user-2' ]
participants.removed = [ 'user-3' ]
```

The problem I have is that occasionally (perhaps once in every ten runs) the test will fail because step 6 does not return all the expected documents. When I check the indexed terms for the missing documents I see that values in the 'participants' field have been split into separate tokens on the '-' character. This seems to suggest that the default analyzer is being used for indexing instead of the whitespace one.

So far I haven't been able to detect any pattern to the failures. The unexpected tokenisation only affects a portion of the indexed documents and can occur at any point in the indexing process (i.e. it isn't always the first or last document that has problems).

Here is a gist with a simplified version of my original test: https://gist.github.com/pmclellan/64b192537c97529ec2e4 This version fails much more consistently, usually on the first run. However, I have noticed that the problem does not occur if I index documents in a synchronous manner (i.e. waiting for a response to each indexing request before issuing the next).

Cheers,
Paul
</description><key id="18315117">3544</key><summary>Wrong analyzer used when indexing dynamic property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">pmclellan</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-20T19:12:30Z</created><updated>2015-02-06T15:20:28Z</updated><resolved>2013-09-11T14:09:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-21T08:06:34Z" id="23001399">I think you should must not add `"participants.*"` as the pattern but simply use `"*"` instead since the template is already defined for a type see this gist: https://gist.github.com/s1monw/6291595 
does this make sense?
</comment><comment author="brwe" created="2013-08-21T10:31:53Z" id="23008269">I can still reproduce the behavior here, even with the modified mapping and a slightly simpler test. Looks like a bug to me.
</comment><comment author="kimchy" created="2013-08-21T10:35:42Z" id="23008477">this might relate to a bug we fixed in put mapping, where it didn't always wait for all the relevant nodes to properly ack that the mapping have been applied. It should be fixed in master and 0.90 (upcoming 0.90.4).
</comment><comment author="s1monw" created="2013-08-21T10:44:49Z" id="23008827">@brwe can you attach the test you use to reproduce it here?
</comment><comment author="brwe" created="2013-08-21T13:28:25Z" id="23016730">I prepared a (very crude) test on branch issue-3544-test in my repository.

Adding an actionGet() after 

```
    client.prepareIndex(INDEX_NAME, MAPPING_TYPE)
            .setSource(source)
            .setConsistencyLevel(WriteConsistencyLevel.QUORUM)
            .execute();
```

seems to solve the problem, as you expected @s1monw .
So, probably not a bug after all.
</comment><comment author="s1monw" created="2013-08-21T13:39:02Z" id="23017431">@brwe thanks for clarifying! @pmclellan are you ok with closing this issue, does it work for you as well based on the examples?
</comment><comment author="pmclellan" created="2013-08-21T16:39:16Z" id="23031008">Thanks for the feedback. However, I still think this is a bug. Indexing all the documents synchronously, as @brwe suggested, does prevent the issue from happening, but I believe the problem should not occur when indexing asynchronously either.

I'll try running my test against master to see if the issue has been resolved by the fix @kimchy mentioned.
</comment><comment author="s1monw" created="2013-08-21T16:41:03Z" id="23031141">@pmclellan you can index them async but you need to wait until they are all indexed on all replicas and then call refresh otherwise you might miss some docs depending on which shard / replica you go. I don't see the issue you are talking about sync/async here can you explain?
</comment><comment author="pmclellan" created="2013-08-21T17:34:22Z" id="23034990">Apologies for the confusion. I'll do my best to explain what I mean as it's possible I've got the wrong end of the stick here. 

@brwe suggested adding '.actionGet()' to the end of my indexing call. My understanding is that this would cause the test to wait until a response has been received from the Elasticsearch node before proceeding to index the next document. This is what I am terming as synchronous execution because each index request must be acknowledged before a subsequent one is issued. My understanding is that the 'execute()' method returns a ListenableActionFuture so that you can issue a series of requests without waiting for responses and then use the futures to determine when they've been acknowledged. This is what I'd consider to be asynchronous execution. In either case I'd expect the end result to be the same with all documents being indexed according to the type mapping. Is this a valid assumption or have I got things mixed up here?

One thing I can see that I'm not currently doing in my test is waiting for the result of all the indexing futures. I'll give this a try and see what happens.
</comment><comment author="pmclellan" created="2013-08-21T18:11:46Z" id="23037737">Modified test to wait for all indexing responses before sending refresh request and re-ran it using latest code from master. Unfortunately, neither change had any effect. However, if I group all the separate indexing actions into a single bulk request then the problem goes away. 

Hope this info helps with further investigation.
</comment><comment author="s1monw" created="2013-08-22T06:51:31Z" id="23071715">I have to apologize... I looked into this closer and I can see the dynamic mapping being applied correctly even on master but somehow it pulls a default analyzer even if the field mapper has `whitespace` initialized. We will look deeper into this! It has certainly something todo with a race when we see the field the first time.
</comment><comment author="brwe" created="2013-08-30T12:36:40Z" id="23558079">@pmclellan Just writing to let you know we are still on it. It is bug indeed and as @s1monw guessed a race condition. Thanks again for providing the test and being so persistent! @s1monw prepared a fix (see above) but we need to discuss this further before pulling it in. 
</comment><comment author="pmclellan" created="2013-08-30T15:45:18Z" id="23570206">Hi @brwe, thanks for the update. It's a rare day when someone actually thanks me for my persistence. 

Good to hear that you've pinned down the cause of the problem. Keep me posted on the progress of the fix.
</comment><comment author="s1monw" created="2013-08-30T19:21:10Z" id="23583712">@pmclellan I am glad you were persistent and I learned my part here as well :) thanks for this
</comment><comment author="sschuerz" created="2015-02-06T08:37:54Z" id="73203535">I still experience some wrong behavior of elasticsearch when selecting the index analyzer for a dynamic property. I have a "_autocomplete_*" property; the concrete fields "_autocomplete_de" or "_autocomplete_en" are filled by a transform script depending on the lanuage of the document. The index analyzer to use is equal to concrete name of the field. Look at this example:

``` json
PUT test_dynamic_template
{
  "settings": {
    "analysis": {
      "analyzer": {
        "nGram_analyzer": {
          "alias": "default_index",
          "tokenizer": "nGram_tokenizer",
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        },
        "_autocomplete_de": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "stopwords_de_filter"
          ]
        },
        "_autocomplete_en": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "stopwords_en_filter"
          ]
        }
      },
      "tokenizer": {
        "nGram_tokenizer": {
          "type": "nGram",
          "min_gram": 3,
          "max_gram": 25,
          "token_chars": [ "letter", "digit", "symbol", "punctuation" ]
        }
      },
      "filter": {
        "stopwords_de_filter": {
          "type": "stop",
          "stopwords": "_german_"
        },
        "stopwords_en_filter": {
          "type": "stop",
          "stopwords": "_english_"
        }
      }
    }
  }
}

PUT test_dynamic_template/_mapping/page
{
  "page": {
    "properties": {
      "_lang": {
        "type": "string",
        "index": "not_analyzed"
      }
    },
    "dynamic_templates" : [
      {
        "autocomplete_for_lang" : {
          "match" : "_autocomplete_*",
          "mapping" : {
            "type" : "string",
            "index_analyzer" : "{name}",
            "search_analyzer": "standard"
          }
        }
      }
    ],
    "transform": {
      "script": "if(ctx._source._lang instanceof String) { ctx._source['_autocomplete_' + ctx._source._lang] = autocomplete_fields.collect{ ctx._source[it] }; }",
      "params": {
        "autocomplete_fields": ["title", "content"]
      }
    }
  }
}

POST test_dynamic_template/page/
{
  "_lang": "en",
  "title": "elasticsearch dynamic template",
  "content": "this is some content to reproduce the behavior"
}

GET test_dynamic_template/_search?search_type=count
{
  "query": {
    "match_all": { }
  },
  "facets" : {
    "all_autocomplete" : {
      "terms" : {
        "field": "_autocomplete_en",
        "regex": "^s.*", 
        "size": 5
      }
    }
  }
}
```

I've executed this commands multiple times, _sometimes_ I see the following behavior:
Depending on the elasticsearch node where the facet query is executed, it returns only the word "some" (which is correct) or it returns "sticsearch", "sticsearc", "sticsear", "sticsea", "sticse" (in this case, a wrong analyzer seems to be used).

elasticsearch version: 1.4.2

**Update:** The wrong behavior only occurs if the name of the dynamic property starts with an underscore. I named it "autocomplete_*" now and it works fine. However, I would still consider this as a bug, since underscores are permitted in property names.
</comment><comment author="brwe" created="2015-02-06T15:20:28Z" id="73252215">@sschuerz This is a different issue. Analyzer names may not start with an _, these names are reserved for special analyzers. But you are right, this is not documented anywhere and there is no check for this, the analyzer is just silently dropped. 
I opened a new issue for this: https://github.com/elasticsearch/elasticsearch/issues/9596

Thanks for reporting and especially the update that points to the _. This saved a lot of time.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/ConcurrentDynamicTemplateTests.java</file></files><comments><comment>Rare race condition when introducing new fields into a mapping</comment><comment>Dynamic mapping allow to dynamically introduce new fields into an existing mapping. There is a (pretty rare) race condition, where a new field/object being introduced will not be immediately visible for another document that introduces it at the same time.</comment></comments></commit></commits></item><item><title>FVH can end in very very long running recursion on phrase highlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3543</link><project id="" key="" /><description>This is a followup from [LUCENE-5182](https://issues.apache.org/jira/browse/LUCENE-5182) ...due to the nature of FVH extract logic a simple phrase query can put a FHV into a super long running recursion. I had documents taking literally days to return form the extract phrases logic. I have a test that reproduces the problem and a possible fix. The reason for this is that the FVH never tries to early terminate if a phrase is already way beyond the slop coming from the phrase query. If there is a document with lot of occurrences or two or more terms in the phrase this literally tries to match all possible combinations of the terms in the doc. I don't think we can fix this FVH without rewriting it since this alg is freaking crazy and somehow `n!` of all the positions etc. I am not even sure what the Big-O of this is but I have a patch that tires to prevent this thing from going totally nuts.
</description><key id="18297496">3543</key><summary>FVH can end in very very long running recursion on phrase highlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-08-20T14:02:19Z</created><updated>2013-08-21T15:32:02Z</updated><resolved>2013-08-20T16:03:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-08-21T15:32:02Z" id="23025872">So much for being a _Fast_VH, too many long loops discovered lately :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/vectorhighlight/XFieldPhraseList.java</file><file>src/main/java/org/apache/lucene/search/vectorhighlight/XFieldQuery.java</file><file>src/test/java/org/apache/lucene/search/vectorhighlight/XFastVectorHighlighterTest.java</file></files><comments><comment>Prevent FVH from entering a very long running loop on large docs with high freq phrase terms.</comment></comments></commit></commits></item><item><title>TransportNodesOperationAction should be a master only operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3542</link><project id="" key="" /><description>The problem here: When a client node is asked (via REST) for nodes stats, and another client node is also in the cluster, there will be a node not connected exception and no stats will be returned, due to  InternalClusterService#355 - which prevent client-to-client node communication.
</description><key id="18292459">3542</key><summary>TransportNodesOperationAction should be a master only operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-20T12:15:31Z</created><updated>2013-09-13T13:14:53Z</updated><resolved>2013-09-13T13:14:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-08-20T12:16:23Z" id="22940718">I would say that the `TransportNodesOperationAction` should redirect the request to either a data or master node, in case the node its sending the request from is a client node.
</comment><comment author="javanna" created="2013-09-13T13:14:53Z" id="24393435">Closing this one as it is about the same problem that's described in #3617 . Added there all the relevant information.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename readable_format flag to human</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3541</link><project id="" key="" /><description>Given that most unix tools support the human readable (-h) flag to turn the output into something that humans can easily understand, we should rename the readable_format flag (added with #3432) to something along those lines. Let's call it `human`.
</description><key id="18291056">3541</key><summary>Rename readable_format flag to human</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-20T11:42:56Z</created><updated>2013-08-20T11:46:29Z</updated><resolved>2013-08-20T11:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestXContentBuilder.java</file></files><comments><comment>Renamed readable_format flag to human</comment></comments></commit></commits></item><item><title>Date math not working correctly due to lower casing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3540</link><project id="" key="" /><description>ES 0.90.3 appears to lowercase the date math portion of this query, turning M (month) into m (minutes). The same query is correctly interpreted in ES 0.20.6:

```
{ 
    "query" : { "query_string" : { "default_operator" : "AND",
          "query" : "created:[now-1M/d TO now/d]"
        } },
  "sort" : [ { "ordernumber" : "desc" } ]
}
```

The following query with upper case D for day rounding generates an exception in ES 0.20.6 but not on 0.90.3 which further indicates that some lower casing is going on.

```
{ 
    "query" : { "query_string" : { "default_operator" : "AND",
          "query" : "created:[now-1M/D TO now/D]"
        } },
  "sort" : [ { "ordernumber" : "desc" } ]
}
```
</description><key id="18280057">3540</key><summary>Date math not working correctly due to lower casing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frli</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-20T06:38:23Z</created><updated>2013-08-20T15:39:10Z</updated><resolved>2013-08-20T12:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-08-20T08:55:56Z" id="22931016">yeah... the range queries are lowercased in the query string... for range queries it makes sense to leave the terms as they are (without lowercasing)... will fix that. Thanks!
</comment><comment author="frli" created="2013-08-20T15:39:09Z" id="22954114">Thanks for the quick fix!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>when doing range queries within a query_string query, the range terms should only be lowercased if the field is not numeric</comment></comments></commit></commits></item><item><title>Don't recover a shard if it just failed on this node and wasn't reassign...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3539</link><project id="" key="" /><description>...ed to this node by master yet.

When recovery of a shard fails on a node, the node sends notification to the master with information about the failure. During the period between the shard failure and the time when notification about the failure reaches the master, any changes in shard allocations can trigger the node with the failed shard to allocate this shard again. This allocation (especially if successful) creates a ripple effect of the shard going through failure/started states in order to match the delayed states processed by master.  Under certain condition, a node involved in this process might generate warning messages: "marked shard as started, but shard has not been created, mark shard as failed".

This fix makes sure that nodes keep track of failed shard allocations and will not try to allocate such shards repeatedly while waiting for the failure notification to be processed by master.
</description><key id="18266500">3539</key><summary>Don't recover a shard if it just failed on this node and wasn't reassign...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-08-19T22:00:04Z</created><updated>2014-12-05T21:11:08Z</updated><resolved>2013-08-28T12:13:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-08-20T08:52:24Z" id="22930821">Looks good, nice catch, that one is a tricky one...
</comment><comment author="kimchy" created="2013-08-28T12:13:34Z" id="23409442">pushed to master and 0.90.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dynamic templates from an index template are skipped if a new type already have dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3538</link><project id="" key="" /><description>The two dynamic templates lists should be merged. 

Example:

Add a template:

```
curl -XPUT "http://localhost:9200/_template/template_1" -d'
{
   "template": "te*",
   "mappings": {
      "type1": {
         "dynamic_templates": [
            {
               "t1": {
                  "match": "*",
                  "mapping": {
                     "store": "yes"
                  }
               }
            }
         ]
      }
   }
}'
```

create a type:

```
curl -XPUT "http://localhost:9200/test1" -d'
{
   "mappings": {
      "type1": {
         "dynamic_templates": [
            {
               "t2": {
                  "match": "t*",
                  "mapping": {
                     "store": "no"
                  }
               }
            }
         ]
      }
   }
}'
```

Now get the mappings for the newly created type:

```
curl -XGET "http://localhost:9200/test1/_mapping"
```

which gives (note that the t1 template is missing):

```
{
   "test1": {
      "type1": {
         "dynamic_templates": [
            {
               "t2": {
                  "mapping": {
                     "store": "yes"
                  },
                  "match": "t*"
               }
            }
         ],
         "properties": {}
      }
   }
}
```
</description><key id="18245440">3538</key><summary>Dynamic templates from an index template are skipped if a new type already have dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-19T15:20:20Z</created><updated>2013-08-19T15:59:47Z</updated><resolved>2013-08-19T15:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/test/java/org/elasticsearch/test/unit/common/xcontent/support/XContentHelperTests.java</file></files><comments><comment>XContentHelper.mergeDefaults with single key object list ([ { key: .. } , {key : ... } .. ] ) didn't add defaults which were not already in content</comment></comments></commit></commits></item><item><title>Feature/multi term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3537</link><project id="" key="" /><description>Branch for Issue [3536](https://github.com/elasticsearch/elasticsearch/issues/3536)
</description><key id="18244866">3537</key><summary>Feature/multi term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-08-19T15:11:21Z</created><updated>2014-06-19T16:18:18Z</updated><resolved>2013-10-09T08:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-09T08:09:21Z" id="25953397">This got merged, closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>multi term vectors similar to multi get</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3536</link><project id="" key="" /><description>Currently, we can only retrieve [term vectors](https://github.com/elasticsearch/elasticsearch/issues/3114) for one document at a time. If term vectors for many documents are requested, a multi term vectors endpoint, similar to multi get, would be useful:

``````
```
curl -XGET 'http://localhost:9200/index/type/_mtermvectors' -d '{
    "fields": [
        "field1",
        "field2",
        ...
    ],
    "ids": [
        "docId1",
        "docId2",
        ...
    ],
    "offsets": false|true,
    "payloads": false|true,
    "positions": false|true,
    "term_statistics": false|true,
    "field_statistics": false|true
}'

```

The return format is an array, each entry of which conatins the term vector response for one document:

```
{
   "docs": [
      {
         "_index": "index",
         "_type": "type",
         "_id": "docId1",
         "_version": 1,
         "exists": true,
         "term_vectors": {
            ...
         }
      },
      {
         "_index": "index",
         "_type": "type",
         "_id": "docId2",
         "_version": 1,
         "exists": true,
         "term_vectors": {
         ...
         }
      }
   ]
}
```
``````
</description><key id="18244566">3536</key><summary>multi term vectors similar to multi get</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-08-19T15:06:09Z</created><updated>2013-11-05T13:05:52Z</updated><resolved>2013-08-26T07:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsItemResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsShardRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsShardResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportMultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportSingleShardMultiTermsVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportSingleShardTermVectorAction.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorModule.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorService.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file><file>src/test/java/org/elasticsearch/test/integration/termvectors/MultiTermVectorsTests.java</file></files><comments><comment>Multi term vector request</comment><comment>--------------------------</comment></comments></commit></commits></item><item><title>Feature/issue 3533 and 3464</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3535</link><project id="" key="" /><description>branch for issue [3533](https://github.com/elasticsearch/elasticsearch/issues/3533) and issue [3464](https://github.com/elasticsearch/elasticsearch/issues/3464), see commit messages
</description><key id="18242006">3535</key><summary>Feature/issue 3533 and 3464</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-08-19T14:21:49Z</created><updated>2014-06-13T20:44:06Z</updated><resolved>2013-08-23T14:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-08-23T14:16:12Z" id="23166358">pushed to master an 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Requesting the mapping of an index where no mappings are defined yet throws an Index Not found exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3534</link><project id="" key="" /><description>To reproduce:

```
curl -XPUT "http://localhost:9200/index1"
```

```
curl -XGET "http://localhost:9200/index1/_mapping"
```

Now returns:

```
{
   "error": "IndexMissingException[[index1] missing]",
   "status": 404
}
```

Should return:

```
{
   "index1": {}
}
```
</description><key id="18240387">3534</key><summary>Requesting the mapping of an index where no mappings are defined yet throws an Index Not found exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>regression</label><label>v1.0.0.Beta1</label></labels><created>2013-08-19T13:50:36Z</created><updated>2013-08-19T13:52:20Z</updated><resolved>2013-08-19T13:52:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/SimpleGetMappingsTests.java</file></files><comments><comment>GetMapping failed when index had no mapping (yet)</comment></comments></commit></commits></item><item><title>function score usability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3533</link><project id="" key="" /><description>`function_score` ([Issue 3423 ](https://github.com/elasticsearch/elasticsearch/issues/3423)  and [Issue 3464](https://github.com/elasticsearch/elasticsearch/issues/3464) ) would be much easier to use, if the following issues were implemented:
1. `reference` should also accept `{ lat: 11, lon: 12 }` and `[12,11]` for geopoints
2. rename `scale_weight`, to `decay`?
3. decay functions should support an `offset` value, eg anything within 10km is fine, then start decaying every 2km after that.
4. rename the `score_mode` `total` to `sum`
5. add `boost_mode`, to let the user define how the result of the score function is combined with the query score. allow options:
   - `multiply` (default)
   - `replace` 
   - `sum`
   - `avg`
   - `min`
   - `max`

min/max because we may be combining this `function_score` query with another `function_score` query.
1. make the default for `boost_mode` == `multiply`, even   with scripts, to make it consistent throughout.  This means, that `function_score` behaves different that `custom_score` before. To mimic the old functionality, change `boost_mode` to `replace`
2. rename  `reference` to `origin`
3. make nice java api for score function builders.
</description><key id="18237515">3533</key><summary>function score usability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2013-08-19T12:48:21Z</created><updated>2013-08-23T14:15:26Z</updated><resolved>2013-08-23T14:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java</file><file>src/test/java/org/elasticsearch/test/integration/percolator/SimplePercolatorTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/customscore/CustomScoreSearchTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/functionscore/DecayFunctionScoreTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/functionscore/RandomScoreFunctionTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/rescore/QueryRescorerTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>add builders for nicer java api</comment></comments></commit></commits></item><item><title>old doku? </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3532</link><project id="" key="" /><description>http://www.elasticsearch.org/blog/the-river-searchable-couchdb/
</description><key id="18227368">3532</key><summary>old doku? </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">FrancisVarga</reporter><labels /><created>2013-08-19T08:11:26Z</created><updated>2013-08-19T12:48:46Z</updated><resolved>2013-08-19T12:48:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-19T12:48:46Z" id="22869184">If you have questions about elasticsearch.org website, please open issues or pull request at https://github.com/elasticsearch/elasticsearch.github.com repository.

BTW, if you could add more details about what you are looking for it will help a lot to answer.

Best
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Thread Pool: Remove blocking type option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3531</link><project id="" key="" /><description>The `blocking` thread pool type is not recommended to be used, since it will end up blocking the IO thread most times when executing, which is not recommended (other operations will then stall as well).
</description><key id="18213086">3531</key><summary>Thread Pool: Remove blocking type option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-18T20:27:21Z</created><updated>2013-08-18T20:28:56Z</updated><resolved>2013-08-18T20:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>src/main/java/org/elasticsearch/gateway/fs/FsGateway.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/test/java/org/elasticsearch/test/integration/threadpool/SimpleThreadPoolTests.java</file><file>src/test/java/org/elasticsearch/test/unit/common/util/concurrent/EsExecutorsTests.java</file><file>src/test/java/org/elasticsearch/test/unit/common/util/concurrent/PrioritizedExecutorsTests.java</file><file>src/test/java/org/elasticsearch/test/unit/threadpool/UpdateThreadPoolSettingsTests.java</file></files><comments><comment>Thread Pool: Remove blocking type option</comment><comment>The blocking thread pool type is not recommended to be used, since it will end up blocking the IO thread most times when executing, which is not recommended (other operations will then stall as well).</comment><comment>closes #3531</comment></comments></commit></commits></item><item><title>Share shards computation logic between searchShards and searchShardsCoun...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3530</link><project id="" key="" /><description>This PR refactors a bit and add a shared method for searchShards and searchShardsCount to avoid divergent behavior in the future (like #2682 and #3268).

It seems like searchShardsCount is mainly implemented to test if we are hitting 1 shard or more than 1 shards so maybe it could be renamed and optimized for that specific use case.
</description><key id="18204211">3530</key><summary>Share shards computation logic between searchShards and searchShardsCoun...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2013-08-18T07:19:21Z</created><updated>2014-06-26T08:59:18Z</updated><resolved>2013-08-20T13:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-20T13:11:28Z" id="22943573">@Paikan thanks a lot for your pull request! I left a comment in the code regarding a small optimization that we can add, other than that it looks good. If you can make that small change and update the pull request I'll push it.
</comment><comment author="Paikan" created="2013-08-20T13:40:06Z" id="22945282">@javanna ok i have updated the pull request 
</comment><comment author="Paikan" created="2013-08-20T13:48:23Z" id="22945856">@javanna sorry I have squashed directly the commits maybe you would have prefered a separate commit for just the modification you asked and to squash it yourself
</comment><comment author="javanna" created="2013-08-20T13:49:34Z" id="22945936">No worries @Paikan it's fine, I would have asked you to squash them anyway ;)
Running tests right now, will push soon!
</comment><comment author="javanna" created="2013-08-20T13:57:21Z" id="22946488">Merged into 65056a63a131834674e35bd578c1d9b8762815f8 and backported to 0.90 (8b0a013b11497f6a685a6a373c0e9178d0e904c2). Thanks @Paikan !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>#1745: Adds check to only write pidfile on succuess exec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3529</link><project id="" key="" /><description>See issue #1745.
- Tom
</description><key id="18203986">3529</key><summary>#1745: Adds check to only write pidfile on succuess exec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tvburger</reporter><labels /><created>2013-08-18T06:46:29Z</created><updated>2014-06-13T00:36:07Z</updated><resolved>2013-08-23T11:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-23T12:02:26Z" id="23159301">Hey Tom,

I did not include your PR as it was, as we are writing the pid file already in our java code. No need to do it twice...

Thanks a lot for your input, helped us finding a bug!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Dont write pidfile twice on startup</comment></comments></commit></commits></item><item><title>Add score_mode to CompletionSuggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3528</link><project id="" key="" /><description>Support various score_modes in the CompletionSuggester.  Modes supported are max, min, total, and product.  This is useful when you have multiple suggestions with the same output but different inputs and weights.

```
curl -X PUT 'localhost:9200/music/song/1?refresh=true' -d '{
    "name" : "Nevermind",
    "suggest" : { 
        "input": [ "Nevermind" ],
        "output": "Nirvana - Nevermind",
        "payload" : { "artistId" : 2321 }, 
        "weight": 3
    }
}'

curl -X PUT 'localhost:9200/music/song/2?refresh=true' -d '{
    "name" : "Nevermind",
    "suggest" : { 
        "input": [ "Nirvana" ],
        "output": "Nirvana - Nevermind",
        "payload" : { "artistId" : 2321 }, 
        "weight": 2
    }
}'

curl -X POST 'localhost:9200/music/_suggest?pretty' -d '{
    "song-suggest" : {
        "text" : "n",
        "completion" : {
            "field" : "suggest", 
            "score_mode": "total"
        }                                             
    }
}'
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "song-suggest" : [ {
    "text" : "n",
    "offset" : 0,
    "length" : 4,
    "options" : [ {
      "text" : "Nirvana - Nevermind",
      "score" : 5.0, "payload" : {"artistId":2321}
    } ]
  } ]
}

```

Not sure this is the best approach but I wanted to put it out there for comments.
</description><key id="18202937">3528</key><summary>Add score_mode to CompletionSuggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-08-18T04:11:06Z</created><updated>2014-07-16T21:52:34Z</updated><resolved>2013-08-19T13:51:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-08-18T04:21:32Z" id="22824468">Ill get some tests written as well.
</comment><comment author="s1monw" created="2013-08-19T10:16:22Z" id="22862503">Hey matt, lemme ask some questions before even looking at the code.. what happens if I do this:

```
curl -X POST 'localhost:9200/music/_suggest?pretty' -d '{
    "song-suggest" : {
        "text" : "ni",
        "completion" : {
            "field" : "suggest", 
            "score_mode": "total"
        }                                             
    }
}'
```

my score suddenly becomes `2`? I don't think we can really add this feature since we can't reliably apply the score mode. It will be unusable in like 99% of the usecases I guess.The biggest issue is that this calculates the score after we scored which is odd. We get results in a scored manner (top N from the suggester) and then we apply score mode? Unfortunately we can't apply this inside the suggester since we lack the information for the output while scoring. 

I hope this makes sense?
</comment><comment author="mattweber" created="2013-08-19T13:51:43Z" id="22872988">Yup, makes sense.   Didn't think it all the way though before jumping in.   Thanks for the response!
</comment><comment author="s1monw" created="2013-08-19T13:54:16Z" id="22873170">thanks for opening this, I really appreciate the effort and keep on coming up with ideas! I hope this was not discouraging! :)
</comment><comment author="mattweber" created="2013-08-19T15:21:44Z" id="22879777">Thanks, you know I will keep them coming!  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ZIP Version of 0.90.3 distribution not working on Mac OSX 10.8.3 (missing files?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3527</link><project id="" key="" /><description>Hi,

I downloaded this version and get the following error:

```
Initialization Failed ...
- MissingResourceException[Can't find bundle for base name org.elasticsearch.common.joda.time.format.messages, locale en]
```

I then proceeded to download the tar.gz version and it worked perfectly!
</description><key id="18197471">3527</key><summary>ZIP Version of 0.90.3 distribution not working on Mac OSX 10.8.3 (missing files?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">saliksyed</reporter><labels /><created>2013-08-17T18:18:42Z</created><updated>2013-10-07T14:42:56Z</updated><resolved>2013-10-07T14:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-18T14:37:24Z" id="22831478">Hey,

I couldnt reproduce it on 10.8.4.. can you check the SHA hashes if they differ? We provide them on the download page.
Would be thankful for any additional debug information as well in order to track this down :-)

Thanks!
</comment><comment author="saliksyed" created="2013-08-18T16:06:49Z" id="22833430">Hi Alexander,

I get the following sha1 hash on my download:
f201c39aba4b1785b350bb1c567d802fe8eabeec

which I think is the same as the one given on download page.

Salik

On Sun, Aug 18, 2013 at 7:37 AM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Hey,
&gt; 
&gt; I couldnt reproduce it on 10.8.4.. can you check the SHA hashes if they
&gt; differ? We provide them on the download page.
&gt; Would be thankful for any additional debug information as well in order to
&gt; track this down :-)
&gt; 
&gt; Thanks!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3527#issuecomment-22831478
&gt; .
</comment><comment author="spinscale" created="2013-08-18T17:11:17Z" id="22834618">I also suspect this reproducible? If you download again, it still fails?
</comment><comment author="brusic" created="2013-08-19T21:17:40Z" id="22904978">From what I can recall from previous discussion, the zip file is meant only for Windows systems. All other OSes should use the gzipped version.
</comment><comment author="saliksyed" created="2013-08-19T22:28:39Z" id="22909533">Yeah I downloaded several times and it failed. If it only works for windows that's pretty confusing since zip is a very widely used format on mac.
</comment><comment author="brusic" created="2013-08-19T22:49:37Z" id="22910621">Here is the previous discussion: https://github.com/elasticsearch/elasticsearch/pull/2793

It appears that the zip file should be a superset of the gz file, so it should contain all the files necessary. Haven't tried 0.90.3 on a Mac, but I will try it out.
</comment><comment author="spinscale" created="2013-08-20T06:23:54Z" id="22924930">Exactly, the zip file only includes the batch files in addition but contains everything from the tar.gz - so this cannot be the issue.

Do you have a special locale configured? Can you check your environment via `env`?
</comment><comment author="saliksyed" created="2013-08-20T06:53:30Z" id="22925921">Hmm... well I've downloaded the file a few times and I think it has to be
something with how I'm copying the file because the sha hash of the zip is
perfect. I tried it again and it works with the second download but not the
first. I think it is probably something strange with my system or something.

On Mon, Aug 19, 2013 at 11:24 PM, Alexander Reelsen &lt;
notifications@github.com&gt; wrote:

&gt; Exactly, the zip file only includes the batch files in addition but
&gt; contains everything from the tar.gz - so this cannot be the issue.
&gt; 
&gt; Do you have a special locale configured? Can you check your environment
&gt; via env?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3527#issuecomment-22924930
&gt; .
</comment><comment author="spinscale" created="2013-10-07T07:45:15Z" id="25789980">hey there, is this still an issue for you or can we close it?
</comment><comment author="saliksyed" created="2013-10-07T14:28:37Z" id="25812721">Go ahead and close it! Thanks

On Monday, October 7, 2013, Alexander Reelsen wrote:

&gt; hey there, is this still an issue for you or can we close it?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3527#issuecomment-25789980
&gt; .

## 

Sent from my iPhone
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Setting index/bulk thread pools with queue_size can cause replica shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3526</link><project id="" key="" /><description>When setting `queue_size` on the index/bulk thread pools, they can cause replica shard failures when a request ends up being rejected on the replica shard. We should not adhere to the `queue_size` limit when executing the operation on the replica (which is perfectly fine, since the primary shard will make sure to limit it).
</description><key id="18196948">3526</key><summary>Setting index/bulk thread pools with queue_size can cause replica shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-17T17:34:34Z</created><updated>2013-10-17T13:15:27Z</updated><resolved>2013-08-23T17:25:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-17T19:13:55Z" id="22817869">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalAllocateDangledIndices.java</file><file>src/main/java/org/elasticsearch/transport/BaseTransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/TransportRequestHandler.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file></files><comments><comment>Settings queue_size on index/bulk TP can cause rejection failures when executed over network</comment><comment>The #3526 fix was not complete, it handled cases of on node execution, but didn't properly handle cases where it was executed over the network, and forcing the execution of the replica operation when done over the wire.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/AbstractRunnable.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsAbortPolicy.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/SizeBlockingQueue.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/test/java/org/elasticsearch/test/integration/threadpool/SimpleThreadPoolTests.java</file><file>src/test/java/org/elasticsearch/test/unit/common/util/concurrent/EsExecutorsTests.java</file><file>src/test/java/org/elasticsearch/test/unit/threadpool/UpdateThreadPoolSettingsTests.java</file></files><comments><comment>Setting index/bulk thread pools with queue_size can cause replica shard failures</comment><comment>closes #3526</comment></comments></commit></commits></item><item><title>Fix small javadoc typos for IndexShardRoutingTable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3525</link><project id="" key="" /><description /><key id="18192953">3525</key><summary>Fix small javadoc typos for IndexShardRoutingTable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2013-08-17T11:43:51Z</created><updated>2014-07-16T21:52:35Z</updated><resolved>2013-08-17T12:00:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-17T12:01:14Z" id="22810476">pushed! thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better exception handling in actions when forking to a thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3524</link><project id="" key="" /><description>An execution on a thread pool might be rejected due to its settings, have better handling in those cases across the actions we have.
</description><key id="18175817">3524</key><summary>Better exception handling in actions when forking to a thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-16T19:55:57Z</created><updated>2013-08-16T19:56:43Z</updated><resolved>2013-08-16T19:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollScanAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file></files><comments><comment>Better exception handling in actions when forking to a thread pool</comment><comment>An execution on a thread pool might be rejected due to its settings, have better handling in those cases across the actions we have.</comment><comment>closes #3524</comment></comments></commit></commits></item><item><title>Expose size statistics for completion suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3523</link><project id="" key="" /><description>In order to determine how many RAM the completion suggest structures will eat up, this data should be exposed.

Closes #3522

Open for discussion: I am not yet sure I moving in the CompletionPostingsFormat class into the InternalIndexShard. Open for better solutions.
</description><key id="18162097">3523</key><summary>Expose size statistics for completion suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-16T15:13:35Z</created><updated>2014-07-16T21:52:36Z</updated><resolved>2013-08-23T08:42:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-18T10:19:18Z" id="22828096">@s1monw applied your comments. Last step is to provide appropriate rest integration, similar to the field data stats again. will do that tomorrow
</comment><comment author="spinscale" created="2013-08-23T08:42:50Z" id="23150776">Added in master https://github.com/elasticsearch/elasticsearch/commit/cdddbb758502793ebcecf0e4311de574a74293ea and 0.90 https://github.com/elasticsearch/elasticsearch/commit/168dbc360fe6e5dc7ebcdace5c5b4789fcd2fae3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose statistics for completion suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3522</link><project id="" key="" /><description>In order to determine how many RAM the completion suggest structures will eat up, this data should be exposed.
</description><key id="18162002">3522</key><summary>Expose statistics for completion suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-16T15:11:53Z</created><updated>2013-08-21T11:25:12Z</updated><resolved>2013-08-21T11:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file></files><comments><comment>Allow include / exclude of completion stats via REST parameters</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionStats.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/stats/SimpleIndexStatsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Expose size statistics for completion suggest</comment></comments></commit></commits></item><item><title>odd scoring behaviour / inconsistent scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3521</link><project id="" key="" /><description>unfortunately i wasn't able to reproduce this in the usual way, but here is the case:

I have a query such as 

```
{ "from": 0, "size": 10, "query": { "bool": {  "must": [ {"match_all":{}},{ "constant_score": { "filter": { "terms": { "id": [...] } } } } ] } }, "fields": "", "sort": [ { "_score": {} }, { "id": { "order": "desc" } } ] }
```

to the terms filter, i pass a list of ids(anywhere between 1 and 200k unique ids).

when executing this query multiple times i get different results. so, investigating a little i traced it to the score not being constant sometimes(not what i expected at all).

i ran the same query a few times with explain set to true and getting only the last document, and here is what i got:

```
_explanation: {
value: 1.264911
description: sum of:
details: [
{
value: 0.94868326
description: ConstantScore(*:*)^3.0, product of:
details: [
{
value: 3
description: boost
}
{
value: 0.31622776
description: queryNorm
}
]
}
{
value: 0.31622776
description: ConstantScore...
```

and then 

```
_explanation: {
value: 1.4142135
description: sum of:
details: [
{
value: 0.70710677
description: ConstantScore(*:*), product of:
details: [
{
value: 1
description: boost
}
{
value: 0.70710677
description: queryNorm
}
]
}
{
value: 0.70710677
description: ConstantScore...
```

so, here i would expect this query to ALWAYS have the same score, and also, that every document scores exactly the same. 
it could even seem ok if the score wasnt constant across requests, but not really that documents score differently.
- i do know i don't need the "match_all" query, but that's the way i managed to reproduce it on our cluster. without that, the score for the documents would always be 1 and i could not reproduce this behaviour.

*\* hope thats clear enough... but let me know if you need more info, or even the complete output for the explain(its pretty big)
</description><key id="18152219">3521</key><summary>odd scoring behaviour / inconsistent scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-16T11:31:39Z</created><updated>2013-08-17T13:03:55Z</updated><resolved>2013-08-16T19:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-16T12:41:15Z" id="22763544">hey @lmenezes, I am afraid but this is the expected behavior. there is a lot going on in this boolean query construct that depends on a number of factors.  the only guarantee here is that it will be the same score for all docs. Yet the eventual score and the query norm depend on your similarity, with a similarity that doesn not modify the query norm I guess it'd be 1.0 across the board.
One thing you can do is to wram top level query in a constant score that should give you a score of 1.0
</comment><comment author="lmenezes" created="2013-08-16T12:45:59Z" id="22763743">@s1monw that's the thing... i don't get a constant score for all documents. i do understand(also, don't care. not interested in the score for this case) that running the same query multiple times might result in different scores. but, for the same execution the documents should all score the same, right? if so, then there is something wrong here.
</comment><comment author="s1monw" created="2013-08-16T12:48:34Z" id="22763857">I just wrote a tests for this and I actually get back scores for all docs taht are consistent. I might not understand your problem here. you run this query and two docs get different scores?
</comment><comment author="lmenezes" created="2013-08-16T13:04:58Z" id="22764620">i tried pretty hard myself to write an example that worked(got 2 documents with different scores) here and wasn't able to.
anyway, here is the response from a single query with the behavior i'm trying do describe: https://gist.github.com/lmenezes/6249787

i could not reproduce that into my staging environment, only on live. the difference between staging and live at the moment, is that staging is not getting updates and has no replicas(if this info might help).

would setting explain -&gt; true, and getting all the results help? its a pretty big response...
</comment><comment author="s1monw" created="2013-08-16T13:14:12Z" id="22765099">@lmenezes any idea where the boost comes from that is shown in your response?
</comment><comment author="lmenezes" created="2013-08-16T13:18:32Z" id="22765319">you mean this: ConstantScore(_:_)^3 right? if so, no idea. actually i'm executing the query from a file, so i know its always the same.
</comment><comment author="clintongormley" created="2013-08-16T13:19:48Z" id="22765388">@lmenezes check your email
</comment><comment author="s1monw" created="2013-08-16T14:53:53Z" id="22771035">this is more sneaky than I though... we are modifying a cached version of match all docs in the query parser... a fix is attached
</comment><comment author="lmenezes" created="2013-08-16T15:21:51Z" id="22773182">looks good. we currently fixed using a default boost != 1.0 for the matchall queries and will remove when updating to next version.
</comment><comment author="s1monw" created="2013-08-16T19:20:40Z" id="22787918">FYI - this can be triggered from a user query via `query_string` ie. `(*:*)^3.0` will leave a boost behind on the shards it hits.
</comment><comment author="lmenezes" created="2013-08-17T13:03:55Z" id="22811435">great :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ApplyAcceptedDocsFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedQuery.java</file><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/engine/EngineSearcherTotalHitsMatcher.java</file><file>src/test/java/org/elasticsearch/test/unit/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Removed static versions of MatchAllDocsQuery</comment></comments></commit></commits></item><item><title>Refactor the plugin manager to be more readable, focused</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3520</link><project id="" key="" /><description>Refactoring related to issue #3519
</description><key id="18144381">3520</key><summary>Refactor the plugin manager to be more readable, focused</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">CloCkWeRX</reporter><labels /><created>2013-08-16T07:19:07Z</created><updated>2014-06-27T05:05:25Z</updated><resolved>2013-09-14T07:15:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-16T15:07:35Z" id="22772220">@CloCkWeRX Thanks for the PR. I have some comments.

## Regression

Doing the following now fails:

``` sh
bin/plugin -i elasticsearch/kibana
```

```
-&gt; Installing elasticsearch/kibana...
Trying https://github.com/elasticsearch/kibana/zipball/master
Downloading ...............................................................DONE
failed to extract plugin [/Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/elasticsearch/kibana.zip]: FileNotFoundException[/Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins/elasticsearch/kibana.zip (No such file or directory)]
```

I think it's due to the following line: https://github.com/CloCkWeRX/elasticsearch/blob/b03ccf1fda26bf0d64a81c096d4b6d365077b320/src/main/java/org/elasticsearch/plugins/PluginManager.java#L195

In `name` field you have in that case `elasticsearch/kibana` and not `kibana`.

Could you please fix it?

## Minor comment

I think this line is unsed: https://github.com/CloCkWeRX/elasticsearch/blob/b03ccf1fda26bf0d64a81c096d4b6d365077b320/src/main/java/org/elasticsearch/plugins/PluginManager.java#L99

As you set it again on this line: 
https://github.com/CloCkWeRX/elasticsearch/blob/b03ccf1fda26bf0d64a81c096d4b6d365077b320/src/main/java/org/elasticsearch/plugins/PluginManager.java#L131

## Squash

Could you please squash all your commits into a single one please?

Once done, I will test again everything and see how it goes.

Thanks!
</comment><comment author="CloCkWeRX" created="2013-08-19T20:36:15Z" id="22902079">Should be fixed, but I have a sneaking suspicion I have mucked up the rebasing. 
</comment><comment author="dadoonet" created="2013-09-14T07:15:06Z" id="24438498">Hey @CloCkWeRX 

I think I can close this PR as I did almost what you proposed with this commit: https://github.com/elasticsearch/elasticsearch/commit/764aa54f2deee2c493f8b6da600d385f5180b9a7

Thanks for the PR and suggestions!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Split download and extract in PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3519</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/plugins/PluginManager.java#L91

This method tries to accomplish quite a lot; and if like us you are looking to install plugins from different protocols (sftp), it has some hard coded expectations around file protocols, locations and more.

It could benefit from:
- Refactoring to split up 'fetch the file using some mechanism' and "now that we have the .zip, install it"
- Injecting some of the dependencies, like HTTPDownloader; so that other strategies can be implemented.
</description><key id="18142406">3519</key><summary>Split download and extract in PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">CloCkWeRX</reporter><labels /><created>2013-08-16T05:52:46Z</created><updated>2014-03-16T11:25:17Z</updated><resolved>2014-03-16T11:25:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-16T06:26:16Z" id="22749717">You could use current PluginManager version to install from a local file. Something like this should work:

``` sh
bin/plugin --url file://tmp/plugin.zip --install plugin
```

Does it work for you?
</comment><comment author="CloCkWeRX" created="2013-08-16T07:12:23Z" id="22751033">No, not really - we're interested in injecting other transport protocol handlers; and while that might not be of interest, some of the cleanup work may be - https://github.com/CloCkWeRX/elasticsearch/compare/elasticsearch:master...master
</comment><comment author="dadoonet" created="2013-08-16T07:19:42Z" id="22751277">I understand that and what you are after. I'm just wondering about the use case.
Are you thinking of extending PluginManager class and provide you own PluginManager or something?
I mean: how will you use it?

I agree that we should split both methods for a better readabilty.
</comment><comment author="CloCkWeRX" created="2013-08-16T07:55:17Z" id="22752585">View this in the context of a (non java, so no maven repo) build pipeline, our artefact repository is currently an internal SFTP server. Rather than kludge around with curl or other tools that speak more protocols to put things in  local directory, we figure it's going to be cleaner to be able to pass in _any_ URL and have the appropriate downloading strategy available.
Or even if we do introduce that kludge, being able to directly invoke "extract" is useful.

ie, now it's sftp, but ftp, magnet uris, scp, etc are all reasonable candidates.
</comment><comment author="dadoonet" created="2014-03-16T11:25:17Z" id="37754647">@CloCkWeRX PluginManager has been refactored some weeks ago.
Closing your issue as we closed as well the PR.

Feel free to reopen and send a new PR based on latest changes if it still something needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Servers ping themselves during discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3518</link><project id="" key="" /><description>During discovery, I have multicast disabled and give the servers a list of IP addresses. For simplicity, all the master servers get an identical list of addresses. When they come up, they tend to like to ping themselves to see what's up.

[2013-08-16 05:13:27,497][TRACE][discovery.zen.ping.unicast] [qtau] [1] sending to [qtau][aY2qKS3iQmi6XPGAeBIfZw][inet[/192.168.72.129:9300]]{data=false, master=true}

[2013-08-16 05:14:57,557][TRACE][discovery.zen.ping.unicast] [qtau] [2] received response from [qtau][aY2qKS3iQmi6XPGAeBIfZw][inet[/192.168.72.129:9300]]{data=false, master=true}: [ping_response{target [[qtau][aY2qKS3iQmi6XPGAeBIfZw][inet[/192.168.72.129:9300]]{data=false, master=true}], master [null], cluster_name[QuizletProductionCluster]}, ...
</description><key id="18142200">3518</key><summary>Servers ping themselves during discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rdeaton</reporter><labels /><created>2013-08-16T05:41:43Z</created><updated>2013-08-23T09:37:00Z</updated><resolved>2013-08-23T09:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-08-17T22:57:54Z" id="22821308">whats wrong with that? its intentional.
</comment><comment author="rdeaton" created="2013-08-21T22:58:30Z" id="23057020">Just seemed odd. Why is it intentional?
</comment><comment author="kimchy" created="2013-08-23T09:37:00Z" id="23153225">yes, it is, it another level of verification that that the network is working well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>If _id field is an object, no error is thrown but doc is "unsearchable"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3517</link><project id="" key="" /><description>## Expected Behavior

Normally, if you try to index a document without an ID in the URI (e.g. a POST) but with an _id field in the document (and no explicit _id path mapping), it throws an error because the autogenerated ID does not match the provided _id field:

``` bash
curl -XDELETE localhost:9200/testindex
curl -XPUT localhost:9200/testindex
curl -XPOST localhost:9200/testindex/testtype?pretty -d '{"_id":"polyfractal","key":"value"}}}'
```

``` json
{
  "error" : "MapperParsingException[failed to parse [_id]]; nested: MapperParsingException[Provided id [O-kIgieVTRG9DpxHML7LkA] does not match the content one [polyfractal]]; ",
  "status" : 400
}
```
## Broken Behavior

However, if the _id field happens to be an object, Elasticsearch happily indexes the document:

``` bash
curl -XDELETE localhost:9200/testindex
curl -XPUT localhost:9200/testindex
curl -XPOST "localhost:9200/testindex/testtype" -d '{"key":"value"}'
curl -XPOST "localhost:9200/testindex/testtype" -d '{"_id":{"name":"polyfractal"},"key":"value"}}}'
```

``` json
{"ok":true,"_index":"testindex","_type":"testtype","_id":"b2xEPk5tTfC-RLsCb1ZapA","_version":1}
{"ok":true,"_index":"testindex","_type":"testtype","_id":"BsTbRqaeTrKLIe0JoeHsWw","_version":1}
```

You can GET it:

``` bash
curl -XGET localhost:9200/testindex/testtype/BsTbRqaeTrKLIe0JoeHsWw?pretty
```

``` json
{
  "_index" : "testindex",
  "_type" : "testtype",
  "_id" : "BsTbRqaeTrKLIe0JoeHsWw",
  "_version" : 1,
  "exists" : true, "_source" : {"_id":{"name":"polyfractal"},"key":"value"}}}
}
```

It shows up with a match_all query:

``` bash
curl -XGET localhost:9200/testindex/testtype/_search?pretty -d '{"query":{"match_all":{}}}'
```

``` json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "BsTbRqaeTrKLIe0JoeHsWw",
      "_score" : 1.0, "_source" : {"_id":{"name":"polyfractal"},"key":"value"}}}
    }, {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "b2xEPk5tTfC-RLsCb1ZapA",
      "_score" : 1.0, "_source" : {"key":"value"}
    } ]
  }
}
```

But doesn't show up when you search for exact values (or Match or any other search):

``` bash
curl -XGET localhost:9200/testindex/testtype/_search?pretty -d '{"query":{"term":{"key":"value"}}}'
```

``` json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.30685282,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "b2xEPk5tTfC-RLsCb1ZapA",
      "_score" : 0.30685282, "_source" : {"key":"value"}
    } ]
  }
}
```

If you ask ES why it doesn't show up, it says there are no matching terms:

``` bash
curl -XGET localhost:9200/testindex/testtype/BsTbRqaeTrKLIe0JoeHsWw/_explain?pretty -d '{"query":{"term":{"key":"value"}}}'
```

``` json
{
  "ok" : true,
  "_index" : "testindex",
  "_type" : "testtype",
  "_id" : "BsTbRqaeTrKLIe0JoeHsWw",
  "matched" : false,
  "explanation" : {
    "value" : 0.0,
    "description" : "no matching term"
  }
}
```

And finally, as a fun twist, you can set an explicit mapping to look inside the _id object.  This works with regard to the ID (it extracts the appropriate ID), is GETable, match_all, etc.  Search is still broken.

``` bash
curl -XDELETE localhost:9200/testindex
curl -XPUT localhost:9200/testindex -d '{
   "mappings":{
      "testtype":{
         "_id" : {
           "path" : "_id.name"
         },
         "properties":{
            "_id":{
               "type":"object",
               "properties":{
                  "name":{
                     "type":"string"
                  }
               }
            }
         }
      }
   }
}'

curl -XPOST "localhost:9200/testindex/testtype" -d '{"key":"value"}'
curl -XPOST "localhost:9200/testindex/testtype" -d '{"_id":{"name":"polyfractal"},"key":"value"}}}'
curl -XGET localhost:9200/testindex/testtype/polyfractal?pretty
```

``` json
{
  "_index" : "testindex",
  "_type" : "testtype",
  "_id" : "polyfractal",
  "_version" : 1,
  "exists" : true, "_source" : {"_id":{"name":"polyfractal"},"key":"value"}}}
}
```

``` bash
curl -XGET localhost:9200/testindex/testtype/_search?pretty -d '{"query":{"match_all":{}}}'
```

``` json
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "wsT9vaevTCW5EuKyr7nmUw",
      "_score" : 1.0, "_source" : {"key":"value"}
    }, {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "polyfractal",
      "_score" : 1.0, "_source" : {"_id":{"name":"polyfractal"},"key":"value"}}}
    } ]
  }
}
```

``` bash
curl -XGET localhost:9200/testindex/testtype/_search?pretty -d '{"query":{"term":{"key":"value"}}}'
```

``` json
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.30685282,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "wsT9vaevTCW5EuKyr7nmUw",
      "_score" : 0.30685282, "_source" : {"key":"value"}
    } ]
  }
}
```
## Reference

This was surfaced by [Scott on the mailing list](https://groups.google.com/d/msg/elasticsearch/0at1uZBvN3k/xIatIxwVziwJ).
</description><key id="18132816">3517</key><summary>If _id field is an object, no error is thrown but doc is "unsearchable"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2013-08-15T22:49:14Z</created><updated>2015-11-22T10:22:36Z</updated><resolved>2015-10-14T13:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GlenRSmith" created="2015-02-12T23:41:53Z" id="74177703">It's a little bit more fun than that, even: you actually get _partial_ indexing!

```
curl -XDELETE localhost:9200/testindex
curl -XPUT localhost:9200/testindex
curl -XPOST localhost:9200/testindex/testtype -d '{"leftkey":"value","_id":{"name":"polyfractal"},"rightkey":"value"}}}'
curl -XPOST localhost:9200/_flush
```

Now search on the field _before_ the _id:

```
curl -XGET localhost:9200/testindex/testtype/_search?pretty -d '{"query":{"term":{"leftkey":"value"}}}'
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.30685282,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "testtype",
      "_id" : "PalIN5CpSPKkGbhs4qNqaw",
      "_score" : 0.30685282, "_source" : {"leftkey":"value","_id":{"name":"polyfractal"},"rightkey":"value"}}}
    } ]
  }
}
```

There you go.
But search on the field _after_ the _id:

```
curl -XGET localhost:9200/testindex/testtype/_search?pretty -d '{"query":{"term":{"rightkey":"value"}}}'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

And you get nothing.
</comment><comment author="andreaskern" created="2015-02-13T07:16:12Z" id="74214342">I am affected by this behavior too, monogo output the field like this 

```
{ "_id":{"$oid":"54d9e3bf30320c3335017e69"}, "@timestamp":"..."}
```

actually I did not care about the "_id" field, but I care about the "@timestamp" field which is _silently_ not indexed. Here an example that shows the behavior:
https://gist.github.com/andreaskern/01d1d292f7f146186ee5
</comment><comment author="clintongormley" created="2015-05-29T17:04:05Z" id="106871830">In 2.0, the timestamp field would now be indexed correctly, as would `_id.$oid`.  Wondering if we should allow users to index `_id` field inside the body at all?  /cc @rjernst 
</comment><comment author="rjernst" created="2015-05-29T17:36:13Z" id="106881852">The ability to specify _id within a document has already been removed for 2.0+ indexes. 
</comment><comment author="clintongormley" created="2015-05-29T18:47:59Z" id="106901528">@rjernst you removed the ability to specify the main doc _id in the body, but if the body contains an `_id` field then it creates a field called `_id` in the mapping, which can't be queried.  

What I'm asking is: should we just ignore the fact that this field is not accessible (as we do in master today) or should we actually throw an exception? I'm leaning towards ignoring, as users don't always have control over the docs they receive.
</comment><comment author="rjernst" created="2015-05-31T11:42:49Z" id="107161189">I would be in favor of throwing an exception. This would only be for 2.0+ indexes, and it is really just field name validation (disallowing fields colliding with meta fields). The mechanism would be the same, a user would not be able to explicitly add a field `_id` in the properties for a document type.
</comment><comment author="clintongormley" created="2015-05-31T11:44:04Z" id="107161226">@rjernst it's a tricky one.  eg mongo adds `{ "_id": { "$oid": "...." }}`, so actually the `_id.$oid` field IS queryable...  should this still throw an exception?
</comment><comment author="rjernst" created="2015-05-31T11:48:27Z" id="107161389">IMO, yes.
</comment><comment author="rjernst" created="2015-05-31T11:50:17Z" id="107161440">With #8871, I don't think that would work, because _id is both a field mapper (the real meta field), and an object mapper.
</comment><comment author="clintongormley" created="2015-05-31T12:06:17Z" id="107162669">@rjernst yep, makes sense
</comment><comment author="clintongormley" created="2015-06-24T17:41:42Z" id="114951254">@rjernst this still works, even with #8871 merged in
</comment><comment author="clintongormley" created="2015-10-14T13:21:26Z" id="148047497">Closed by #14003
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>id_cache memory footprint grows linearly with number of parent documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3516</link><project id="" key="" /><description>As parent documents are indexed, the size of the parent/child `id_cache` grows linearly.  This applies even if the majority of parents do not have any associated children.
</description><key id="18110742">3516</key><summary>id_cache memory footprint grows linearly with number of parent documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">polyfractal</reporter><labels /><created>2013-08-15T15:14:28Z</created><updated>2014-08-05T19:09:34Z</updated><resolved>2014-07-18T10:37:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deverton" created="2013-08-22T09:27:56Z" id="23078043">I think we might be seeing the same issue. I posted about it in the [mailing list](https://groups.google.com/forum/m/?hl=en#!topic/elasticsearch/N7_qZxQkUfM)
</comment><comment author="martijnvg" created="2013-08-22T10:47:09Z" id="23081662">Keeping track of which parent id is referenced by a child document is tricky. Right now we just simply load all parent ids in memory of the new segments being made searchable during a refresh. If we need to keep track if a parent id is actually used than in a worst case scenario we need to go over all segments that contain child documents and check if a parent id being loaded is actually referenced by a child doc. This can slowdown the refresh significantly.

To mitigate the high memory usage, it is best to add more nodes to you cluster. The id cache size will then nicely be divided amongst the nodes in the cluster.
</comment><comment author="martijnvg" created="2014-07-18T10:37:14Z" id="49417568">Closing this in favor for #6107. Once _parent field has been cut over to doc values the on heap memory size will be 0.

Internally the id_cache has been removed since ES 1.1, and _parent field is now based on field data, also _parent field data is now less wasteful as before ES 1.1 with id cache.
</comment><comment author="ostersc" created="2014-08-05T18:21:05Z" id="51238104">We are using ES 1.2.1 and are still seeing linear growth of the id_cache as we insert documents which can be parents (even if there are no children in the index).
Can you confirm the above statement: "Internally the id_cache has been removed since ES 1.1..." and give some insight into why this might be the case?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException during discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3515</link><project id="" key="" /><description>Some logs from during discovery of a brand new cluster today. The zen.multicast is set to disable and they are all given a list of the other nodes for unicast.

[2013-08-15 02:41:17,744][INFO ][node                     ] [qphosphorus2] stopping ...
[2013-08-15 02:41:18,237][INFO ][node                     ] [qphosphorus2] stopped
[2013-08-15 02:41:18,237][INFO ][node                     ] [qphosphorus2] closing ...
[2013-08-15 02:41:18,273][INFO ][node                     ] [qphosphorus2] closed
[2013-08-15 02:57:51,994][INFO ][node                     ] [qphosphorus2] version[0.90.3], pid[72565], build[5c38d60/2013-08-06T13:18:31Z]
[2013-08-15 02:57:51,995][INFO ][node                     ] [qphosphorus2] initializing ...
[2013-08-15 02:57:52,004][INFO ][plugins                  ] [qphosphorus2] loaded [], sites [head]
[2013-08-15 02:57:54,627][INFO ][node                     ] [qphosphorus2] initialized
[2013-08-15 02:57:54,627][INFO ][node                     ] [qphosphorus2] starting ...
[2013-08-15 02:57:54,856][INFO ][transport                ] [qphosphorus2] bound_address {inet[/192.168.72.120:9300]}, publish_address {inet[/192.168.72.120:9300]}
[2013-08-15 02:58:24,865][WARN ][discovery                ] [qphosphorus2] waited for 30s and no initial state was set by the discovery
[2013-08-15 02:58:24,866][INFO ][discovery                ] [qphosphorus2] QuizletProductionCluster/HstcP8sQRWGi6nwjZU5KHw
[2013-08-15 02:58:24,949][INFO ][http                     ] [qphosphorus2] bound_address {inet[/192.168.72.120:9200]}, publish_address {inet[/192.168.72.120:9200]}
[2013-08-15 02:58:24,950][INFO ][node                     ] [qphosphorus2] started
[2013-08-15 02:58:54,909][INFO ][cluster.service          ] [qphosphorus2] new_master [qphosphorus2][HstcP8sQRWGi6nwjZU5KHw][inet[/192.168.72.120:9300]]{master=true}, reason: zen-disco-join (elected_as_master)
[2013-08-15 02:59:30,213][INFO ][cluster.service          ] [qphosphorus2] added {[qaluminium2][YH9kVKH-Rgyc9tVcxQq_-g][inet[/192.168.72.115:9300]]{master=true},}, reason: zen-disco-receive(join from node[[qaluminium2][YH9kVKH-Rgyc9tVcxQq_-g][inet[/192.168.72.115:9300]]{master=true}])
[2013-08-15 03:00:35,465][INFO ][cluster.service          ] [qphosphorus2] added {[qargon2][Mif6T8WDT0Of0Td9vgrf_w][inet[/192.168.72.119:9300]]{master=true},}, reason: zen-disco-receive(join from node[[qargon2][Mif6T8WDT0Of0Td9vgrf_w][inet[/192.168.72.119:9300]]{master=true}])
[2013-08-15 03:00:35,533][DEBUG][action.admin.cluster.node.stats] [qphosphorus2] failed to execute on node [Mif6T8WDT0Of0Td9vgrf_w]
org.elasticsearch.transport.RemoteTransportException: [qargon2][inet[/192.168.72.119:9300]][cluster/nodes/stats/n]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:59)
        at org.elasticsearch.action.admin.cluster.node.stats.NodeStats.writeTo(NodeStats.java:215)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:62)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:276)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
[2013-08-15 03:00:37,546][INFO ][cluster.service          ] [qphosphorus2] added {[qchlorine2][PFq_0tNNSC6yvom-RNCzPw][inet[/192.168.72.114:9300]]{master=true},}, reason: zen-disco-receive(join from node[[qchlorine2][PFq_0tNNSC6yvom-RNCzPw][inet[/192.168.72.114:9300]]{master=true}])
</description><key id="18088863">3515</key><summary>NullPointerException during discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rdeaton</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-15T03:07:28Z</created><updated>2013-11-01T11:42:15Z</updated><resolved>2013-08-15T09:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-15T09:44:01Z" id="22693926">thanks for opening this. This is a very rare race-condition that happens during node startup. There is a small chance that the clusterstate is still `empty` and that can cause this problem. I will fix that by asking the discovery service or cluster service for the local node instead of going through the cluster state which will be initialized correctly.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/TransportNodesRestartAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/NodeOperationResponse.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalAllocateDangledIndices.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/monitor/dump/DumpMonitorService.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/river/cluster/PublishRiverClusterStateAction.java</file><file>src/test/java/org/elasticsearch/test/integration/cluster/ClusterServiceTests.java</file></files><comments><comment>Use ClusterService#localNode instead of checking the cluster state.</comment></comments></commit></commits></item><item><title>0.90.2 RPM on Centos removes and recreates elasticsearch user without preserving the UID</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3514</link><project id="" key="" /><description>Steps:
1. Install elasticsearch 0.19 on Centos 6.4 using the un-official rpm
2. Do something which creates another uid.
3. Now install 0.90.2 the same way.

Expected results:
The elasticsearch remains with the same UID

Actual results:
The elasticsearch user gets removed and, when it is recreated, it not has a different uid - which means that it no longer owns its own files.
</description><key id="18082622">3514</key><summary>0.90.2 RPM on Centos removes and recreates elasticsearch user without preserving the UID</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ejsarge-gr</reporter><labels /><created>2013-08-14T23:09:27Z</created><updated>2013-08-22T16:25:25Z</updated><resolved>2013-08-22T16:25:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-19T14:31:56Z" id="22875960">Hey,

I just tested the upgrade from a 0.90.2 to a 0.90.3 release with the RPM and the official elasticsearch RPMs do not delete the user in case of an upgrade (so you have to call `rpm -Uvh` instead of deleting and reinstalling).

So I guess the your old RPM blindly deletes the user.
Where are the 'unofficial rpm' spec files, so I could take a look and verify?
</comment><comment author="ejsarge-gr" created="2013-08-19T16:09:57Z" id="22883390">Hi Spinscale,
Thanks for looking into this.
Unofficial rpms are: https://github.com/tavisto/elasticsearch-rpms

Note that the repro above was done using Puppet - which suggests that we may have to examine if Puppet is doing the equivalent of `rpm -Uvh`.
</comment><comment author="spinscale" created="2013-08-20T07:58:24Z" id="22928394">hey,

can you please check, if your puppet setup does something specific in that regard? The linked RPMs do not remove a user at all, and the official RPMs check on installation if the user already exists (and dont do anything in that case).
</comment><comment author="ejsarge-gr" created="2013-08-22T16:25:25Z" id="23104131">Hi Spinscale,
This failure was reported to us by a colleague which is why we passed it on to you. Seeing as you can't reproduce we took up the challenge to see if we could reproduce it ourselves. 

Despite our best efforts, we cannot reproduce.

Thank you for your efforts in looking at this and our apologies for causing you to chase a red herring.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Errors (like StackOverflow) can cause a search context to not be released</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3513</link><project id="" key="" /><description>It will eventually time out (with the default 5 minutes timeout), but we should properly handle it, and also, properly propagate the failure.
</description><key id="18078765">3513</key><summary>Errors (like StackOverflow) can cause a search context to not be released</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-14T21:40:28Z</created><updated>2013-08-14T21:42:15Z</updated><resolved>2013-08-14T21:42:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Errors (like StackOverflow) can cause a search context to not be released</comment><comment>It will eventually time out (with the default 5 minutes timeout), but we should properly handle it, and also, properly propagate the failure.</comment><comment>closes #3513</comment></comments></commit></commits></item><item><title>Correctly apply boosts in query string.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3512</link><project id="" key="" /><description>I'm really not sure the extent of this problem I but I found it when
attempting to use boosts for phrase queries generated by query_string.

Closes #3503.
</description><key id="18062531">3512</key><summary>Correctly apply boosts in query string.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-08-14T16:35:39Z</created><updated>2014-07-16T21:52:36Z</updated><resolved>2013-08-14T20:13:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-08-14T17:45:25Z" id="22653505">I've amended my first commit to fix the problem whether you are using dismax or not.
</comment><comment author="s1monw" created="2013-08-14T19:00:29Z" id="22658797">thanks for fixing that part too. Yet, I have a couple of comments on the commit. 
- the commit message needs editing to explain the actual problem, I don't think we should add stuff like 'I am not sure...' :)
- When I look at the test I can see it passing even without the fix. Here is why, if you index 2 docs that have ~ the same content per field and you calculate a score both boosted in the same way you might end up with identical score. Then we will use the document ID as a tiebreaker. So the only way to prevent this is randomize the order the documents are indexed. There is a utility in the base class called `indexRandom` you should be able to find other tests using it to figure how to use it.  Another thing is that you should also make sure / assert that the score for the boosted doc is significantly higher than the unboosted maybe it's exactly 10x in this case?

other than that it looks great! thanks for taking a look at this!
</comment><comment author="nik9000" created="2013-08-14T19:12:38Z" id="22659637">&gt; the commit message needs editing to explain the actual problem, I don't think we should add stuff like 'I am not sure...' :)

Sure thing.

&gt; When I look at the test I can see it passing even without the fix. Here is why, if you index 2 docs that have ~ the same content per field and you calculate a score both boosted in the same way you might end up with identical score. Then we will use the document ID as a tiebreaker. So the only way to prevent this is randomize the order the documents are indexed. There is a utility in the base class called indexRandom you should be able to find other tests using it to figure how to use it. Another thing is that you should also make sure / assert that the score for the boosted doc is significantly higher than the unboosted maybe it's exactly 10x in this case?

I'll have a look.
</comment><comment author="s1monw" created="2013-08-14T19:13:07Z" id="22659668">@nik9000 thanks so much!
</comment><comment author="nik9000" created="2013-08-14T19:40:57Z" id="22661573">I've updated the commit to use indexRandom and to check the score difference - in this case it is exactly 10x.
</comment><comment author="s1monw" created="2013-08-14T19:45:07Z" id="22661859">looks great! I will pull it an run tests in a minute...
</comment><comment author="s1monw" created="2013-08-14T20:13:37Z" id="22663700">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Broken highlighting when using char_filter + word_delimiter filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3511</link><project id="" key="" /><description>To reproduce the error(using 0.90.0, but was also able to reproduce on 1.0 Beta1):

```
curl -XPOST http://localhost:9200/foo -d '{ "mappings": { "bar": { "dynamic": "false", "properties": { "id": { "type": "integer" }, "content": { "type": "string", "analyzer": "foobar" } } } }, "settings": { "index": { "analysis": { "char_filter": { "iso_mapping" : { "type" : "mapping", "mappings" : ["ü=&gt;ue"] } }, "filter": { "wordDelimiter": { "type": "word_delimiter", "split_on_numerics": "false", "generate_word_parts": "true", "generate_number_parts": "true", "catenate_words": "true", "catenate_numbers": "true", "catenate_all": "false" } }, "analyzer": { "foobar": { "tokenizer": "whitespace", "filter": [ "lowercase", "wordDelimiter" ], "char_filter": "iso_mapping" } } } } } }'

curl -XPUT http://localhost:9200/foo/bar/1 -d '{ "id": 1, "content": "eins, fünf, sechs" }'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{ "id": 2, "content": "eins, fünf,sechs" }'
curl -XPUT http://localhost:9200/foo/bar/3 -d '{ "id": 3, "content": "eins, vier, sechs" }'
curl -XPUT http://localhost:9200/foo/bar/4 -d '{ "id": 4, "content": "eins, vier,sechs" }'
```

The, for the broken case(where the char filter is used):

```
curl -XPOST http://localhost:9200/foo/bar/_search -d' { "from": 0, "size": 300, "query": { "match": { "content": "Fünf" } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }'
```

where we get:

```
"highlight":{"content":["eins, &lt;em&gt;fünf,&lt;/em&gt; sechs"]}}
"highlight":{"content":["eins, &lt;em&gt;fünf,sechs&lt;/em&gt;"]}}
```

And for a working case(no char filter used):

```
curl -XPOST http://localhost:9200/foo/bar/_search -d' { "from": 0, "size": 300, "query": { "match": { "content": "vier" } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }'
```

where we get:

"highlight":{"content":["eins, &lt;em&gt;vier&lt;/em&gt;,sechs"]}}
"highlight":{"content":["eins, &lt;em&gt;vier&lt;/em&gt;, sechs"]}}
</description><key id="18054924">3511</key><summary>Broken highlighting when using char_filter + word_delimiter filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels><label>:Analysis</label><label>bug</label><label>stalled</label></labels><created>2013-08-14T14:23:24Z</created><updated>2016-11-25T16:45:50Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-08-14T14:25:28Z" id="22639200">Just noticed that it might no be easy to appreciate the error, but for the broken examples the results for the highlight are 
[fünf,] and [fünf,sechs], when it should be only [fünf].
for the working example, only vier is highlighted, as expected.
</comment><comment author="s1monw" created="2013-08-14T14:40:20Z" id="22640253">this is puzzeling... I am looking into it.
</comment><comment author="s1monw" created="2013-08-14T21:36:00Z" id="22669120">ok I think I understand what is going on here. The problem is that `word_delimiter_filter` (btw. the last broken filter in Lucene) is messing up the offsets that are corrected by the char filter ie. whey you pass `eins, fünf,sechs` through the `char_filter` and `whitespace` tokenizer you end up with 2 tokens `eins,` and `fuenf,sechs`. The tokenizer already corrected the offset to end at 16 (actually 17 since we did `ü -&gt; ue` but we correct that since the original value that might be used for highlighting in a stored field / source has length 16) but 'word_delimiter' tries to be smart and checks the chars making it span the entire original token. 

The bottom line is 1. don't use `word_delimiter` if you want reliable highlighting. 2. if you still use it don't use `char_filter`, you might be able to do this with yet another filter but from the top of my head this seems not straight forward. I can totally see how a TokenFilter that does the same as the `char_filter` here could be useful. 
</comment><comment author="lmenezes" created="2013-08-15T06:51:54Z" id="22688055">@s1monw guess We should not expect a fix on lucene level for the word_delimiter, right? 
also, just so that we don't end up building a tokenfilter as a plugin that does the same as the char_filter, could something like that be added in ES in the near future? I guess it could be pretty useful for cases like that for a lot of people
</comment><comment author="s1monw" created="2013-08-15T07:42:58Z" id="22689428">@lmenezes I think we have a couple of options here.
- add a token filter that does the same thing as the `mapping_char_filter` - this seems like a good one to me since I guess lots of users might have a need for it ie. diacritics removal / mapping or folding filters like you do where some filters don't to the right thing like ascii folding ü -&gt; u
- fix word delimiter filter to not mess with offsets. 
- make work delimiter filter a tokenizer and support offsets correctly.

I kind of like all of these options while in the meanwhile I am pretty much done with word_delimiter that I'd wanna deprecate it entirely and make it a tokenizer instead to actually fix the root of the problem.  What do you think?
</comment><comment author="clintongormley" created="2013-08-15T07:44:22Z" id="22689466">I know that @jpountz has it on his list of things to do: https://issues.apache.org/jira/browse/LUCENE-5111
</comment><comment author="lmenezes" created="2013-08-15T07:52:01Z" id="22689711">@s1monw  

everything sounds good, but "make work delimiter filter a tokenizer and support offsets correctly" wouldn't really solve the problem here. you use word_delimiter as a tokenizer and you end up with "one two" as a single token, unless there is something here that i don't see.
</comment><comment author="lmenezes" created="2013-08-15T07:53:12Z" id="22689746">@clintongormley cool :)
but having a token filter that does the same as the char_filter isn't a bad idea anyway.
</comment><comment author="lmenezes" created="2013-08-15T07:55:29Z" id="22689813">sorry about using this issue to get attention to another issue, but what about https://github.com/elasticsearch/elasticsearch/issues/3505 ?
Even though I can totally live without it, since we are going back to normal highlighter anyway...
</comment><comment author="lmenezes" created="2013-10-11T16:13:05Z" id="26149901">do we have any update on this?
</comment><comment author="s1monw" created="2013-10-11T21:10:04Z" id="26174476">I don't think we will get to fix the root of the problem here any time soon. I think the only option is to not use word delimiter together with highlighting until it is fixed. 
</comment><comment author="clintongormley" created="2014-07-09T12:29:37Z" id="48463228">I'm not sure this is just the WDF.  If character filters change the length of the string, it also messes with highlighting.
</comment><comment author="lmenezes" created="2014-07-09T12:34:13Z" id="48463642">@clintongormley cleaning up old issues? :) 
</comment><comment author="clintongormley" created="2014-07-09T13:15:46Z" id="48467841">How could you tell :)
</comment><comment author="clintongormley" created="2015-10-14T12:59:55Z" id="148042264">Probably related to https://issues.apache.org/jira/browse/LUCENE-6595
</comment><comment author="clintongormley" created="2015-10-14T13:31:15Z" id="148049730">This actually works correctly now for the plain highlighter case, which returns:

```
"eins, &lt;em&gt;fünf&lt;/em&gt;, sechs"
```

The FVH works _almost_ correctly, it returns:

```
"eins, &lt;em&gt;fünf,&lt;/em&gt; sechs"
```
</comment><comment author="clintongormley" created="2016-11-06T09:43:36Z" id="258670129">I've just understood this issue properly for the first time. You're using a whitespace tokenizer with `fünf,sechs`, so the analyzer with the WDF returns "fuenf", "sechs", "fuenfsechs", so it ends up selecting the longest matching token.

Of course, the same thing could be said for `vier,sechs`, but I think that's where the issue of the character filter changing the length of the string comes into play.

I think all we can do here is wait for https://issues.apache.org/jira/browse/LUCENE-6595 to be resolved.
</comment><comment author="clintongormley" created="2016-11-25T16:45:50Z" id="262994946">Here's some more info:

```
POST foo/_analyze
{
  "field": "content",
  "text": "fünf,sechs" 
}
```
Returns:
```
{
  "tokens": [
    {
      "token": "fuenf",
      "start_offset": 0,
      "end_offset": 10,
      "type": "word",
      "position": 0
    },
    {
      "token": "sechs",
      "start_offset": 0,
      "end_offset": 10,
      "type": "word",
      "position": 1
    },
    {
      "token": "fuenfsechs",
      "start_offset": 0,
      "end_offset": 10,
      "type": "word",
      "position": 1
    }
  ]
}
```

While
```
POST foo/_analyze
{
  "field": "content",
  "text": "vier,sechs" 
}
```

Returns:
```
{
  "tokens": [
    {
      "token": "vier",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 0
    },
    {
      "token": "viersechs",
      "start_offset": 0,
      "end_offset": 10,
      "type": "word",
      "position": 0
    },
    {
      "token": "sechs",
      "start_offset": 5,
      "end_offset": 10,
      "type": "word",
      "position": 1
    }
  ]
}
```

So this is an analysis bug, not a highlighting bug.  And is probably due to https://issues.apache.org/jira/browse/LUCENE-6595
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NGram query in 0.90.3 returning incorrect document matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3510</link><project id="" key="" /><description>Currently testing against 0.90.3, and have an issue regarding an ngram filter mapping for an autocomplete style query.

I have put together a project to express the issue; 

https://github.com/lonelyplanet/es-ngram-bug

Mapping and Query logic: 
https://github.com/lonelyplanet/es-ngram-bug/blob/master/lib/query.rb

Specs to express expected behaviour:
https://github.com/lonelyplanet/es-ngram-bug/blob/master/spec/query_spec.rb
</description><key id="18054054">3510</key><summary>NGram query in 0.90.3 returning incorrect document matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marckysharky</reporter><labels /><created>2013-08-14T14:07:45Z</created><updated>2014-08-08T14:17:25Z</updated><resolved>2014-08-08T14:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-08-14T14:11:49Z" id="22638168">hiya @marckysharky 

Please could you report this using simple curl statements so that we can reproduce it easily.  There's a good chance that the bug is in your code and I really don't want to have to debug a language I don't know :)

thanks
</comment><comment author="s1monw" created="2013-08-14T14:17:58Z" id="22638632">ok this is a wild guess since I am not a ruby guy either but I assume that you are relying on token positions since you use phrase prefix. can you try to create your ngram filter with `"version" : "4.2"` and test again. I assume this should fix the issue. If it does I need to think how we can work around that since the way how NGrams work in `0.90.3` is quite different.
</comment><comment author="clintongormley" created="2014-08-08T14:17:25Z" id="51607716">No update in a year. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error writing to elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3509</link><project id="" key="" /><description>We are currently working with version 1.1.13  installed on ubuntu and elasticsearch version 0.90.2 of working on two-node cluster.

When we write some traces with characters such as _é_ or _í_ we find the following error in log file logstash:

&gt; {: Message =&gt; "Error writing to elasticsearch",: response =&gt; # &lt;FTW :: Response: FTW 0x6f229f11 @ headers = HTTP :: Headers :: &lt;{"content-type" =&gt; "text / plain; charset = UTF-8 "," content-length "=&gt;" 70 "}&gt;, @ body = &lt;FTW :: Connection (@ 4046) @ destinations = [" xxx.xxx.xxx.xxx: 9200 "] @ connected = true @ remote_address = "10.35.167.205" @ secure = false&gt;, @ status = 400, @ logger = # &lt;Cabin :: Channel: 0x22956f6b @ subscriber_lock = # &lt;Mutex:0xc5eb8a&gt;, @ metrics = # &lt;Cabin :: Metrics: 0x41eab16b @ channel = # &lt;Cabin::Channel:0x22956f6b ...&gt;, @ metrics = {}, @ metrics_lock = # Mutex:0x1726099c&gt;, @ data = {}, @ subscribers = {}, @ level =: info&gt;, @ reason = "Bad Request", @ version = 1.1&gt;,: response_body =&gt; "No handler found for uri [/ logstash-2013.08.15/test] and method [GET]",: level =&gt;: error }

The elasticsearch log file shows the following error message:

&gt; [2013-08-14 13:53:59,652][DEBUG][action.index             ] [xxx.xxx.xxx.xxx] [logstash-2013.08.15][2], node[kAqv4xJHTB6uyWCmcWKyrw], [P], s[STARTED]: Failed to execute [index {[logstash-2013.08.15][test][_IpC9zxLTLiOvWFc4XZBow], source[{"@source":"file://control-node/tmp/test/test.log","@tags":["test"],"@fields":{"logLevel":["INFO"],"petitionID":["113002"],"userID":["Unknown"],"type":["Server"],"received":["2013-08-14T11:53:45.058Z"]},"@timestamp":"2013-08-15T06:27:58.615Z","@source_host":"control-node","@source_path":"/tmp/test/test.log","@message":"2013-08-15 08:27:58,615 [Server] INFO  - [113002] [Unknown] [Server] El usuario se ha registrado con éxito test@gmail.com nil null","@type":"test"]}]
&gt; org.elasticsearch.index.mapper.MapperParsingException: Failed to parse
&gt;     at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:509)
&gt;     at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:430)
&gt;     at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:297)
&gt;     at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:211)
&gt;     at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
&gt;     at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
&gt;     at java.lang.Thread.run(Thread.java:679)
&gt; Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: [B@410f87d7; line: 1, column: 0])
&gt;  at [Source: [B@410f87d7; line: 1, column: 979]
&gt;     at org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1378)
&gt;     at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:599)
&gt;     at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:532)
&gt;     at org.elasticsearch.common.jackson.core.base.ParserBase._handleEOF(ParserBase.java:479)
&gt;     at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:2512)
&gt;     at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:626)
&gt;     at org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:48)
&gt;     at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:461)
&gt;     at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:486)
&gt;     ... 8 more

logstash.conf

&gt; input {
&gt;       file {
&gt;     'debug' =&gt; false
&gt;     'path' =&gt; ['/tmp/test/test.log']
&gt;     'tags' =&gt; 'test'
&gt;     'type' =&gt; 'test'
&gt;     'discover_interval' =&gt; 0
&gt;     'start_position' =&gt; 'beginning'
&gt;   }
&gt; }
&gt; filter {
&gt;   multiline {
&gt;     'pattern' =&gt; '(^\sat)|(^java)|(^com)|(^Cause)|(^\s+.)|(^org)'
&gt;     'tags' =&gt; 'test'
&gt;     'type' =&gt; 'test'
&gt;     'what' =&gt; 'previous'
&gt;   }
&gt;   grok {
&gt;     'add_field' =&gt; ['received', '%{@timestamp}']
&gt;     'pattern' =&gt; '%{TIMESTAMP_ISO8601:date} [%{DATA}] %{USERNAME:logLevel}( )+- ([%{INT:petitionID}] (([%{USERNAME:userID}] [%{USERNAME:type}] )|([%{USERNAME:server}] )))?((?&lt;iphone&gt;%{USERNAME}.09)|%{GREEDYDATA})'
&gt;     'tags' =&gt; 'test'
&gt;     'type' =&gt; 'test'
&gt;   }
&gt;   date {
&gt;     'match' =&gt; ['date', 'yyyy-MM-dd HH:mm:ss,SSS']
&gt;     'tags' =&gt; 'test'
&gt;     'type' =&gt; 'test'
&gt;   }
&gt;   mutate {
&gt;     'remove' =&gt; ['date']
&gt;     'tags' =&gt; 'test'
&gt;     'type' =&gt; 'test'
&gt;   }
&gt; }
&gt; output {
&gt;   elasticsearch_http { host =&gt; "x.x.x.x" flush_size =&gt; 1}
&gt; }

We have verified that the data file was utf-8, and have tried to run the option logstash java _-Dfile.encoding = UTF8_ with the same results.

How we could get to write the traces in elasticsearch. Any suggestions?
</description><key id="18050584">3509</key><summary>Error writing to elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jaloare</reporter><labels /><created>2013-08-14T12:57:24Z</created><updated>2013-08-19T13:35:56Z</updated><resolved>2013-08-19T13:35:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-16T15:10:17Z" id="22772417">Hey,

when taking a a look at the JSON you tried to index from your log file, it does not look like valid JSON, because of the second last char, the `[` is not being closed.

```
{"@source":"file://control-node/tmp/test/test.log","@tags":["test"],"@fields":{"logLevel":["INFO"],"petitionID":["113002"],"userID":["Unknown"],"type":["Server"],"received":["2013-08-14T11:53:45.058Z"]},"@timestamp":"2013-08-15T06:27:58.615Z","@source_host":"control-node","@source_path":"/tmp/test/test.log","@message":"2013-08-15 08:27:58,615 [Server] INFO - [113002] [Unknown] [Server] El usuario se ha registrado con éxito test@gmail.com nil null","@type":"test"]}
```

do you have any idea where this comes from?
</comment><comment author="jaloare" created="2013-08-19T06:29:17Z" id="22853098">Hi, 

I think that the problem is with the character **é** or **í** .. when I send the same message without this character, nothing happen. The index is successful. 

The **[** I guess, is missing when logstash try to "parse" the message with the character before.
</comment><comment author="jaloare" created="2013-08-19T12:46:48Z" id="22869057">Sorry, I found the problem. We use chef and our configuration for logstash is strange. When chef download the last version of logstash something happened with the jar. So we donwload manually the last logstash and everything  was better.
</comment><comment author="s1monw" created="2013-08-19T13:35:56Z" id="22871925">thanks for letting us know...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A previous dynamic change to mapping may cause a Put Mapping request to return prematurely </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3508</link><project id="" key="" /><description>Index request which change the mapping of the index can cause a put mapping request which is executed quickly after it to return before all relevant nodes processed the change.
</description><key id="18047575">3508</key><summary>A previous dynamic change to mapping may cause a Put Mapping request to return prematurely </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v1.0.0.Beta1</label></labels><created>2013-08-14T11:39:49Z</created><updated>2013-08-14T15:14:13Z</updated><resolved>2013-08-14T15:14:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/action/index/NodeMappingCreatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Put Mappings CountDownListener validates cluster state version of incoming change confirmations.</comment></comments></commit></commits></item><item><title>Concurrent Put Mapping API to multiple indices/types may return prematurely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3507</link><project id="" key="" /><description>The put mapping api waits for all the nodes that has the relevant indices to have processed the change. Concurrent put mapping requests to different indices/types may result in requests returning before all nodes processed the change.
</description><key id="18047505">3507</key><summary>Concurrent Put Mapping API to multiple indices/types may return prematurely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-14T11:37:59Z</created><updated>2013-08-14T14:16:06Z</updated><resolved>2013-08-14T14:16:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Added index and type checks to MetaDataMappingService.CountDownListener</comment></comments></commit></commits></item><item><title>Add scoring support to percolate api </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3506</link><project id="" key="" /><description>Adding scoring support will allow the percolate matches to be sorted, or just assign a scores to percolate matches. Sorting by score can be very useful when millions of matches are being returned.

The scoring support hooks in into the percolate query option and adds two new boolean options:
- `sort` - Whether to sort the matches based on the score. This will also include the score for each match. The `size` option is a required option when sorting percolate matches is enabled.
- `score` - Whether to compute the score and include it with each match. This will not sort the matches.

For both new options the `query` option needs to be specified, which is used to produce the scores. The `query` option is normally used to control which percolate queries are evaluated. In order to give meaning to these score, the recently added `function_score` query in #3423 can be used to wrap these queries as is shown in the examples below.
## Examples
### Indexing dummy percolator queries:

First query:

``` bash
curl -XPUT 'localhost:9200/my-index/_percolator/1' -d '{ 
    "query": { 
        "match_all" : {} 
    },
    "priority" : 1,
    "create_date" : "2010/01/01"
}'
```

Second query:

``` bash
curl -XPUT 'localhost:9200/my-index/_percolator/2' -d '{ 
    "query": { 
        "match_all" : {} 
    },
    "priority" : 2,
    "create_date" : "2013/01/01"
}'
```

These queries have two extra fields: 'priority' and 'create_date'. These fields can be used in the percolate api during sorting.
### Script score example

Percolate request using the `script_score` function:

``` json
{ 
    "doc": { 
        "field" : "value" 
    },
    "query" : {
        "function_score" : {
            "query" : { "match_all": {}},
            "functions" : [
                {
                    "script_score" : {
                        "script" : "doc['priority'].value"
                    }
                }
            ]
        }
    },
    "sort" : true,
    "size" : 10
}
```
#### Response:

``` json
{
   "took": 118,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "total": 2,
   "matches": [
      {
         "_index": "my-index",
         "_id": "2",
         "_score": 2
      },
      {
         "_index": "my-index",
         "_id": "1",
         "_score": 1
      }
   ]
}
```
### Decay function example

Percolate request using the `exp` decay function:

``` json
{ 
    "doc": { 
        "field" : "value" 
    },
    "query" : {
        "function_score" : {
            "query" : { "match_all": {}},
            "functions" : [
                {
                    "exp" : {
                        "create_date" : {
                            "reference" : "2013/08/14",
                            "scale" : "1000d"
                        }
                    }
                }
            ]
        }
    },
    "sort" : true,
    "size" : 10
}
```
#### Response:

``` json
{
   "took": 2,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "total": 2,
   "matches": [
      {
         "_index": "my-index",
         "_id": "2",
         "_score": 0.85559505
      },
      {
         "_index": "my-index",
         "_id": "1",
         "_score": 0.4002574
      }
   ]
}
```
</description><key id="18047253">3506</key><summary>Add scoring support to percolate api </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-08-14T11:31:01Z</created><updated>2016-03-16T00:57:39Z</updated><resolved>2013-08-14T11:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zoellner" created="2015-06-30T06:03:16Z" id="116990894">Just in case someone comes across this (it's one of the few percolate examples showing up on google) and tries to get it to work, the percolate request needs a string or array in the sort field, not a boolean.
(In addition, due to issues with dynamic scripts, one might have to switch the language from the default groovy to expression)

```
{ 
    "doc": { 
        "field" : "value" 
    },
    "query" : {
        "function_score" : {
            "query" : { "match_all": {}},
            "functions" : [
                {
                    "script_score" : {
                        "script" : "doc['priority'].value",
                        "lang": "expression"
                    }
                }
            ]
        }
    },
    "sort" : "_score",
    "size" : 10
}
```
</comment><comment author="Charlemagne3" created="2016-03-15T22:04:20Z" id="197045879">Confirming @zoellner, I had to use the following for my more complex filtered query:

``` json
{
    "query": {
        "filtered": {
            "filter": {
                "term": {
                    "tenant_id": 29
                }
            },
            "query": {
                "function_score": {
                    "query": {"match_all": {}},
                    "functions": [
                        {
                            "script_score": {
                                "script": "doc[\"priority\"].value",
                                "lang": "expression"
                            }
                        }
                    ]
                }
            }
        }
    },
    "sort": "_score",
    "size": 10
}
```

note the escaped quotes; the single quotes did not work for me, they gave an exception: 

```
{"script": "doc['priority'].value","lang": "expression"}}]}},"sort": "_score","size": 10}}}'
{"took":11,"_shards":{"total":5,"successful":0,"failed":5,"failures":[{"shard":0,"index":"pulley.c","status":"BAD_REQUEST","reason":{"type":"parse_exception","reason":"failed to parse request","caused_by":{"type":"query_parsing_exception","reason":"script_score the script could not be loaded","index":"pulley.c","line":1,"col":205,"caused_by":{"type":"script_exception","reason":"Failed to compile inline script [doc[priority].value] using lang [expression]","caused_by":{"type":"script_exception","reason":"Failed to parse expression: doc[priority].value","caused_by":{"type":"parse_exception","reason":"unexpected character '[' on line (1) position (3)","caused_by":{"type":"lexer_no_viable_alt_exception","reason":null}}}}}}}]},"total":0,"matches":[]}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateShardResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/test/java/org/elasticsearch/test/integration/percolator/SimplePercolatorTests.java</file></files><comments><comment>Added scoring support to percolate api</comment></comments></commit></commits></item><item><title>Inconsistent highlighting behavior(normal x vector)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3505</link><project id="" key="" /><description>So, the issue is reproducible by running(using 0.90.0):

```
curl -XPOST http://localhost:9200/foo -d '{ "mappings": { "bar": { "properties": { "id": { "type": "integer" }, "content": { "type": "string", "store": "yes" } } } } }'
curl -XPOST http://localhost:9200/foo2 -d '{ "mappings": { "bar": { "properties": { "id": { "type": "integer" }, "content": { "type": "string", "store": "yes", "term_vector": "with_positions_offsets" } } } } }'

curl -XPUT http://localhost:9200/foo/bar/1 -d '{ "id": 1, "content": "North Atlantic Treaty Organization, Zwei, Drei, Vier, Fünf, Sechs, Sieben, Acht, Neun" }'
curl -XPUT http://localhost:9200/foo2/bar/1 -d '{ "id": 1, "content": "North Atlantic Treaty Organization, Zwei, Drei, Vier, Fünf, Sechs, Sieben, Acht, Neun" }'

curl -XPOST http://localhost:9200/foo/bar/_search -d' { "query": { "bool": { "must": [ { "match": { "content": "North" } }, { "match": { "content": "Zwei" } }, { "match": { "content": "Vier" } }, { "match": { "content": "Drei" } }, { "match": { "content": "Sechs" } }, { "match": { "content": "Sieben" } }, { "match": { "content": "Acht" } }, { "match": { "content": "Neun" } } ] } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }'

curl -XPOST http://localhost:9200/foo2/bar/_search -d' { "query": { "bool": { "must": [ { "match": { "content": "North" } }, { "match": { "content": "Zwei" } }, { "match": { "content": "Vier" } }, { "match": { "content": "Drei" } }, { "match": { "content": "Sechs" } }, { "match": { "content": "Sieben" } }, { "match": { "content": "Acht" } }, { "match": { "content": "Neun" } } ] } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }'

```

For the first index/query, I get 

```
"highlight":{"content":[", &lt;em&gt;Vier&lt;/em&gt;, Fünf, &lt;em&gt;Sechs&lt;/em&gt;, &lt;em&gt;Sieben&lt;/em&gt;, &lt;em&gt;Acht&lt;/em&gt;, &lt;em&gt;Neun&lt;/em&gt;","&lt;em&gt;North&lt;/em&gt; Atlantic Treaty Organization, &lt;em&gt;Zwei&lt;/em&gt;, &lt;em&gt;Drei&lt;/em&gt;"]}
```

which is what i would expected. but for the second I get:

```
"highlight":{"content":["&lt;em&gt;North&lt;/em&gt; Atlantic Treaty Organization, &lt;em&gt;Zwei&lt;/em&gt;, &lt;em&gt;Drei&lt;/em&gt;, Vier","Vier, Fünf, &lt;em&gt;Sechs&lt;/em&gt;, &lt;em&gt;Sieben&lt;/em&gt;, &lt;em&gt;Acht&lt;/em&gt;, &lt;em&gt;Neun&lt;/em&gt;"]}
```

where the Vier is missing.

I don't expect both to return the same highlighted fragments, but I don't really get why the Vier is not highlighted on the second query(for the second fragment).
</description><key id="18043910">3505</key><summary>Inconsistent highlighting behavior(normal x vector)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2013-08-14T09:56:30Z</created><updated>2016-11-24T18:17:57Z</updated><resolved>2016-11-24T18:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-15T07:58:57Z" id="22689914">I will look into it.... seems like a corner case where the safety fragment size checks kick in I added to prevent this thing from using negative array indices. You need to be in the right mental condition to deal with FVH ;)
</comment><comment author="lmenezes" created="2013-08-15T08:01:04Z" id="22689987">@s1monw not really important(for me at least), we will go back to normal HL :)
</comment><comment author="lmenezes" created="2013-10-11T16:12:56Z" id="26149888">do we have any update on this?
</comment><comment author="clintongormley" created="2016-11-06T09:57:32Z" id="258668143">Updated for 5.0.  Another example where the plain highlighter does a better job the FVH:

```
PUT /foo
{
  "mappings": {
    "bar": {
      "properties": {
        "content": {
          "type": "text",
          "fields": {
            "fvh": {
              "type": "text",
              "term_vector": "with_positions_offsets"
            }
          }
        }
      }
    }
  }
}

PUT /foo/bar/1
{
  "content": "North Atlantic Treaty Organization, Zwei, Drei, Vier, Fünf, Sechs, Sieben, Acht, Neun"
}

POST /foo/bar/_search
{
  "query": {
    "match": {
      "content": "North Zwei Vier Drei Sechs Sieben Acht Neun"
    }
  },
  "highlight": {
    "fields": {
      "content": {
        "fragment_size": 50,
        "number_of_fragments": 5
      },
      "content.fvh": {
        "require_field_match": false,
        "fragment_size": 50,
        "number_of_fragments": 5
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-11-24T18:17:57Z" id="262828170">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added support for random sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3504</link><project id="" key="" /><description>- Support seeds for consistent pagination. If no seed is provided, the current timestamp is used (at the "cost" of consistent pagination)
- Note, just like normal search, the pagination will be consistent up to segment merges, for absolute consistency scroll should be used
- order is supported for (reverse) consistent pagination
- Enhanced the SortParser abstraction to enable sort parser with default configurations (enables the user to specify the sort as a simple string w

Closes #1170
</description><key id="18027184">3504</key><summary>Added support for random sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">uboness</reporter><labels /><created>2013-08-13T23:25:30Z</created><updated>2017-04-07T12:48:49Z</updated><resolved>2013-08-17T20:24:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-08-17T20:24:23Z" id="22819074">changing strategy on how to support it... new pull request will soon come
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Phrase queries automatically generated by query string ignore boosts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3503</link><project id="" key="" /><description>I'm not sure if this is Elasticsearch or Lucene, but it looks like phrase queries generated by query strings ignore boosts:   https://gist.github.com/nik9000/6225677
</description><key id="18021075">3503</key><summary>Phrase queries automatically generated by query string ignore boosts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-13T21:02:29Z</created><updated>2013-08-14T20:14:38Z</updated><resolved>2013-08-14T20:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-14T20:14:24Z" id="22663744">pushed to `0.90` &amp; `master`
</comment><comment author="s1monw" created="2013-08-14T20:14:38Z" id="22663762">thanks @nik9000 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add fuzzy feature to common terms query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3502</link><project id="" key="" /><description>Hi guys,

I have a situation where i'd like to remove common words based on a cutoff frequency and a low frequent term minimum_should_match.

The common terms query is just perfect for this job!

But i would also like to have a "fuzziness" on low frequent terms and only on low frequent terms. 

Using the query string query don't give the expected results because words like "and" would match words like "brand", "band", ect... with a fuzziness of 0.6.

I think adding a fuzzy features to common terms query would do the job.

What do you think?

Thank you
</description><key id="18005566">3502</key><summary>Add fuzzy feature to common terms query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mathieu007</reporter><labels /><created>2013-08-13T16:17:03Z</created><updated>2014-07-04T08:50:07Z</updated><resolved>2014-07-04T08:49:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-13T19:56:18Z" id="22592642">This is an interesting idea.... I will need to look closer into this how to make this work but it could work though. not sure if I get to it this week but I won't forget about it.
</comment><comment author="mathieu007" created="2013-08-14T17:24:12Z" id="22651989">Thanks @s1monw,

I just would like to thanks you guys for the wonderful job you did, i am new to elasticsearch and still learning, but the query style is very intuitive and when you get use to it there is almost nothing you can do.

No More SQL again, or at least much less...
</comment><comment author="clintongormley" created="2014-07-04T08:49:57Z" id="48021607">Common terms is intended to make queries faster.  Fuzzy ends up adding many extra terms, thus slowing down the common terms query.  On top of that, fuzzy may produce high and low frequency terms, yet the low frequency terms are less likely to be the ones that are correct.

We have decided against support fuzzy with common terms.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fixes #3500</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3501</link><project id="" key="" /><description>Add **'/usr/lib/jvm/default-java'** to java_home candidates
</description><key id="18001837">3501</key><summary>fixes #3500</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lgueye</reporter><labels /><created>2013-08-13T15:12:13Z</created><updated>2014-07-16T21:52:37Z</updated><resolved>2013-08-23T09:31:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-23T07:47:22Z" id="23148431">hey,

minor question here: As elasticsearch prefers to run on oracle jdk. does it make sense to push the `/usr/lib/jvm/default-java` at the end to have it as fallback or was there a reason you moved it into first place?
</comment><comment author="lgueye" created="2013-08-23T08:21:48Z" id="23149837">Hi spinscale,
No particular reason. I thought that if I moved it at first place it would be picked first.
It doesn't change that much I guess.
</comment><comment author="spinscale" created="2013-08-23T09:31:50Z" id="23152963">closed by https://github.com/elasticsearch/elasticsearch/commit/048a02eebcdefed36ff7abea2f836f2d5f547ed4 in master and https://github.com/elasticsearch/elasticsearch/commit/4d2d83fba7338418b158855417323004b9ceefec in 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch should include debian's standard java_home when looking for installed jdk </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3500</link><project id="" key="" /><description>I get that error when trying to install debian package

``` bash
Setting up elasticsearch (0.90.2) ...
 * no JDK found - please set JAVA_HOME
invoke-rc.d: initscript elasticsearch, action "start" failed.
```

After looking in the source code I found out that the debian standard java_home **'/usr/lib/jvm/default-java'** isn't being looked up.
</description><key id="18001509">3500</key><summary>Elasticsearch should include debian's standard java_home when looking for installed jdk </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lgueye</reporter><labels /><created>2013-08-13T15:06:58Z</created><updated>2013-08-23T09:28:33Z</updated><resolved>2013-08-23T09:28:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Debian init script: Add debian default java location</comment></comments></commit></commits></item><item><title>Make RestSearchAction#parseSearchXXX(RestRequest) public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3499</link><project id="" key="" /><description>When building a plugin with a new search endpoint, you need to parse the request as a searchRequest.

Methods exist in RestSearchAction class but are private.

We will modify them to be public static. This applies to:
- `RestSearchAction#parseSearchRequest(RestRequest)`
- `RestSearchAction#parseSearchSource(RestRequest)`
</description><key id="18001190">3499</key><summary>Make RestSearchAction#parseSearchXXX(RestRequest) public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-13T15:01:48Z</created><updated>2013-08-13T15:08:04Z</updated><resolved>2013-08-13T15:08:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file></files><comments><comment>Make RestSearchAction#parseSearchXXX(RestRequest) public</comment><comment>When building a plugin with a new search endpoint, you need to parse the request as a searchRequest.</comment></comments></commit></commits></item><item><title>Raise default DeleteIndex Timeout </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3498</link><project id="" key="" /><description>Currently the timeout for an delete index operation is set to 10 seconds. Yet, if a full flush is running while we delete and index this can easily exceed 10 seconds. The timeout is not dramatic ie. the index will be deleted eventually but the client request is not acked which can cause confusion. We should raise it to prevent unnecessary confusion especially in client tests where this can happen if the machine is pretty busy.
</description><key id="18000511">3498</key><summary>Raise default DeleteIndex Timeout </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-13T14:49:58Z</created><updated>2014-03-05T21:35:26Z</updated><resolved>2013-08-13T15:47:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/test/integration/AbstractSharedClusterTest.java</file><file>src/test/java/org/elasticsearch/test/integration/update/UpdateTests.java</file></files><comments><comment>Raise default DeleteIndex Timeout</comment></comments></commit></commits></item><item><title>Cannot index JSON formatted as JSONML</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3497</link><project id="" key="" /><description>I get an error on ES 0.90.1 when trying to load/index JSON formatted as [JSONML](http://www.jsonml.org/).

``` bash
 curl -XPUT 'http://localhost:9200/default/resource/2' -d '{ 
   "data":[
      "ul",
      [
         "li",
         {
            "style":"color:red"
         },
         "First Item"
      ],
      [
         "li",
         {
            "title":"Some hover text.",
            "style":"color:green"
         },
         "Second Item"
      ],
      [
         "li",
         [
            "span",
            {
               "class":"code-example-third"
            },
            "Third"
         ],
         " Item"
      ]
   ]
}'
```

Output:

``` bash
{"error":"MapperParsingException[failed to parse [data]]; nested: ElasticSearchIllegalArgumentException[unknown property [style]]; ","status":400}
```

The JSON comes directly from the JSONML website cited above.

I've narrowed this error down to the above, so this is but an example that raises the error.  My actual data has a few more properties to it.  Nevertheless, the PUT fails, however, with the JSONML.  

Testing workarounds, I've also modified the PUT data so that the JSONML is effectively an object ( { "data": { "jsonml": [...] } } ) and then created a mapping instruction that set the "data" object to enabled=false.  Same error.

I was under the impression that ES could handle any properly formatted JSON.  

Is this expected behavior?  
</description><key id="18000354">3497</key><summary>Cannot index JSON formatted as JSONML</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kefo</reporter><labels><label>feedback_needed</label></labels><created>2013-08-13T14:47:24Z</created><updated>2014-08-08T14:16:42Z</updated><resolved>2014-08-08T14:16:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2013-08-13T15:06:41Z" id="22571542">The underlying parser does only understand JSON with value arrays, i.e. in a ES JSON array, there must be only values of the same type. Mixing objects and values in an array is not accepted.
</comment><comment author="kefo" created="2013-08-13T16:33:29Z" id="22578239">Thanks, Jörg, for commenting.

A few quick questions:

1) Could my issue be considered a feature request or is this a necessary design limitation?
2) Is what you describe documented anywhere?
3) The documentation for an object mapping says that "The enabled flag allows to _disable parsing_ and adding a named object completely."  Emphasis mine.  So, I guess the JSON is still parsed, it's just not indexed?
</comment><comment author="jprante" created="2013-08-13T17:27:26Z" id="22582274">1) Would be a nice feature request to enable ES to understand "full" JSON.
I hope the ES core team can pick it up ;-)

Jackson (the Java JSON implementation used) is not aware of JSONML, so I
think this is the reason why "full" JSON is not there. The XContentBuilder
API would have to be extended. Unfortunately the format of ES
requests/responses is currently not available for being extended by a third
party plugin, so a change in the core code would be required.

2) The documentation of Elasticsearch objects is here
http://www.elasticsearch.org/guide/reference/mapping/object-type/

I learned the restriction from this report
https://github.com/elasticsearch/elasticsearch/issues/2354

Elasticsearch objects are a JSON object in braces, and the purpose is to
construct path specifications for Lucene index field names for the values
in the object. Values can be repeated in a Lucene field, and JSON arrays
can be used for denoting that.

3) "Disabling the parsing" refers to the Elasticsearch object model of the
JSON document only. If the Elasticsearch object model is not valid in the
input, the ES parser rejects the JSON document.

On Tue, Aug 13, 2013 at 6:33 PM, Kevin Ford notifications@github.comwrote:

&gt; Thanks, Jörg, for commenting.
&gt; 
&gt; A few quick questions:
&gt; 
&gt; 1) Could my issue be considered a feature request or is this a necessary
&gt; design limitation?
&gt; 2) Is what you describe documented anywhere?
&gt; 3) The documentation for an object mapping says that "The enabled flag
&gt; allows to _disable parsing_ and adding a named object completely."
&gt; Emphasis mine. So, I guess the JSON is still parsed, it's just not indexed?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3497#issuecomment-22578239
&gt; .
</comment><comment author="kimchy" created="2013-08-13T21:26:20Z" id="22598919">This does not really relate to the ability to parse the json itself, jackson and our usage of it allows us to parse such a json object. The problem is with indexing it, in ES, we require the contents of arrays to be the same json type (object, value) because of how it ends up being translated to indexing it into Lucene.

If you think about it, it is quite strange to index a value and an object within an array, not very evident into how to be able to search it. We could potentially allow it to be indexed fully, and then just accept the fact that its ambiguous when searching, but requires a bit of thinking.

Obviously, if you don't care about how its being index, you can simply use mapping and disable parts of the json from being indexed.
</comment><comment author="jprante" created="2013-08-14T11:57:27Z" id="22630452">JSONML has some characteristics in the JSON array, the first element denotes an XML element tag. A possible method to map JSONML arrays into Elasticsearch JSON objects could be as follows.

With positional information

```
data[0].ul[0].li[0].style = "color:red"
data[0].ul[0].li[0]._value = "First Item"
data[0].ul[0].li[1].title = "Some hover text."
data[0].ul[0].li[1].style = "color:green"
data[0].ul[0].li[1]._value = "Second Item"
data[0].ul[0].li[2].span[0].class = "code-example-third"
data[0].ul[0].li[2].span[0]._value = "Third"
data[0].ul[0].li[2]._value = " Item"
```

Without (flat object)

```
data.ul.li.style = "color:red"
data.ul.li = "First Item"
data.ul.li.title = "Some hover text."
data.ul.li.style = "color:green"
data.ul.li = "Second Item"
data.ul.li.span.class = "code-example-third"
data.ul.li.span = "Third"
data.ul.li = " Item"
```
</comment><comment author="clintongormley" created="2014-07-23T13:06:39Z" id="49870436">I'm struggling to see a practical use case here, from a search perspective.  

Any chance of a practical example?
</comment><comment author="clintongormley" created="2014-08-08T14:16:42Z" id="51607627">Please reopen if you come up with a good use case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CentOS 6.3 x86_64 Unknown mlockall error 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3496</link><project id="" key="" /><description>Installed with RPM from ES 0.90.3-1

Upon startup logfile shows:
     Unknown mlockall error 0

In order to stop it I've tried to - 
1.  enable common.jna: DEBUG and 
2.  'elasticsearch - memlimit unlimited' 

but enabling either seems to cause retval for killproc in init script to FAIL.
</description><key id="17998926">3496</key><summary>CentOS 6.3 x86_64 Unknown mlockall error 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">cdenneen</reporter><labels /><created>2013-08-13T14:22:05Z</created><updated>2014-04-25T19:38:59Z</updated><resolved>2014-04-25T19:38:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-21T16:15:14Z" id="35745322">hey,

does this problem still apply for you with a current version? Are you using a virtualized environment?

Just to make sure how to reproduce: You are setting an unlimited memlock, but the mlockall call inside of elasticsearch is not working. Elasticsearch then starts up (writing that error message), but you are unable to sop it again?
</comment><comment author="asanderson" created="2014-02-21T16:47:13Z" id="35748633">I just upgraded to es 1.0.0 on a CentOS 6.4 vm and I see the same problem.
</comment><comment author="spinscale" created="2014-02-24T10:23:37Z" id="35873289">@asanderson can you help to provide a full setup to reproduce? Configuration changes, memory settings etc?
</comment><comment author="asanderson" created="2014-02-24T14:19:03Z" id="35889843">Yeah, let me try to isolate some of the settings first and then I'll report back.
</comment><comment author="asanderson" created="2014-02-24T15:53:57Z" id="35900152">So, I reduced elasticsearch.yml down to just "bootstrap.mlockall: true" and it still gives the error. We are using Java 1.7.0_51 w/ max heap space set to 1g on an 8g RAM vm. Let me know if you need any other details.
</comment><comment author="spinscale" created="2014-02-24T16:43:02Z" id="35906094">Hey,

just to make sure I get you right: Elasticsearch starts even though this error gets printed and you cant stop it, same problem as the original bug commiter is having?

Is this command working (as root): `ulimit -l unlimited`

if elasticsearch starts, what is returned when typing `service elasticsearch status`?
</comment><comment author="asanderson" created="2014-02-24T16:56:28Z" id="35907767">Just to clarify, I did not install it via rpm, so I'm not starting/stopping it via service. I installed it in our application user apps directory just via tar xvfz and I start it manually via $ES_HOME/bin/elasticsearch -d. Since I'm not running it via root ulimit -l unlimited doesn't work, so that's probably the problem.
</comment><comment author="spinscale" created="2014-02-24T17:23:05Z" id="35910934">@asanderson this means you have to configure your linux distribution to support unlimited locked memory for the user you run elasticsearch as
</comment><comment author="asanderson" created="2014-02-24T17:45:52Z" id="35913494">roger that. I'm working on it. thanks!
</comment><comment author="asanderson" created="2014-02-24T17:55:08Z" id="35914515">That worked! Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Using a missing filter for attributes of a nested object always returns an empty set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3495</link><project id="" key="" /><description>Hi there,

I was trying to use the `exists`/`missing` filters when I stumbled upon this behavior: When I use the `missing` filter for nested objects, it always returns an empty set if the containing nested object is missing, too.

Here is my document mapping:

``` javascript
{
  "site": {
    "properties": {
      "host": {
        "type": "string",
        "index": "not_analyzed",
        "omit_norms": true,
        "index_options": "docs"
      },
      "ip": {,
        "type": "string",
        "index": "not_analyzed",
        "omit_norms": true,
        "index_options": "docs"
      },
      ...
      "modules": {
        "type": "nested",
        "properties": {
          "module_id": {
            "type": "integer"
          },
          "name": {
            "type": "string",
            "index": "not_analyzed",
            "omit_norms": true,
            "index_options": "docs"
          },
          ...
        }
      }
    }
  }
}
```

My Document looks like this:

``` javascript
{
  "host": "6c1bb1fb58e8c48cabbd1e4382e55871f31ad776.com",
  "ip" : "0.0.0.0",
  ...
  "modules": [ ]
}
```

If I now use a query with a  nested filter to select every document where modules.name is missing, I only get an empty set.

``` javascript
{
  "query": {
    "filtered": {
      "query": { "match_all": { } },

      "filter": {
        "nested": {
          "path": "modules",
          "query": {
            "filtered": {
              "query": { "match_all": { } },

              "filter": {
                "missing": { "field": "modules.name" }
              }

            }
          }
        }
      }
    }
  }
}
```

It seems to work if I submit a document which contains a module:

``` javascript
{
  "host": "6c1bb1fb58e8c48cabbd1e4382e55871f31ad776.com",
  "ip" : "0.0.0.0",
  ...
  "modules": [ { "version" : "foo" } ]
}

```

When using documents where the modules object isn't empty, use a `missing` filter which looks for "deeper" missing attributes seems to work, too.

``` javascript
{
  "query": {
    "filtered": {
      "query": { "match_all": { } },

      "filter": {
        "nested": {
          "path": "modules",
          "query": {
            "filtered": {
              "query": { "match_all": { } },

              "filter": {
                "missing": { "field": "modules.foo.bar.baz" }
              }

            }
          }
        }
      }
    }
  }
}
```

I was expecting, that a `missing` filter also returns documents if the containing nested object is missing or empty.

Update: Wrapping an `exists` filter in a `not` filter doesn't return any documents, either.
</description><key id="17995568">3495</key><summary>Using a missing filter for attributes of a nested object always returns an empty set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">pfleidi</reporter><labels><label>:Nested Docs</label><label>discuss</label></labels><created>2013-08-13T13:15:12Z</created><updated>2017-03-06T00:29:20Z</updated><resolved>2015-10-14T13:16:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewish" created="2014-03-25T22:22:05Z" id="38628799">Yeah I've also been a little stumped trying to figure out how to find documents without a nested object... in my case the nested object is an array of objects.... I'd like to find them when the array is empty. 
</comment><comment author="drewish" created="2014-03-25T22:34:05Z" id="38629909">Ah I found a solution at http://grokbase.com/t/gg/elasticsearch/13bfq5qbse/missing-filter-with-nested-objects

```
curl -XPOST "http://ocvli-apw602:9200/test2/IR/_search" -d'
{
    "filter": {
       "not": {
          "nested": {
             "path": "priosenio",
             "filter": {
                "match_all": {}
             }
          }
       }
    }
}'
```
</comment><comment author="caseywebdev" created="2015-07-29T13:58:38Z" id="125960685">I just ran into this. The workaround highlighted by @drewish feels pretty clunky though :confused: 
</comment><comment author="clintongormley" created="2015-09-21T16:26:05Z" id="142034241">Here's a simple recreation that describes the problem:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "foo": {
          "type": "nested"
        }
      }
    }
  }
}

PUT t/t/1
{
  "foo": {
    "bar": "bar"
  }
}

PUT t/t/2
{
  "xyz": "xyz"
}
```

This request matches doc 1, because it has a nested doc which is missing the field, but not doc 2 because it has no nested docs:

```
GET t/_search
{
  "query": {
    "nested": {
      "path": "foo",
      "query": {
        "missing": {
          "field": "foo.baz"
        }
      }
    }
  }
}
```

This workaround works correctly for both docs:

```
GET t/_search
{
  "query": {
    "not": {
      "nested": {
        "path": "foo",
        "query": {
          "exists": {
            "field": "foo.baz"
          }
        }
      }
    }
  }
}
```

@martijnvg @jpountz is this fixable?
</comment><comment author="jpountz" created="2015-10-14T12:54:59Z" id="148041192">It is not fixable, unless the missing query can detect it is being used within a nested query, which I would like to avoid at all costs. We don't index missing fields in documents, only existing fields, so the `missing` query is internally implemented as the negation of an `exists` query. This raises problems as described here given that putting the `not` inside of the `nested` query has a totally different effect than putting it outside as your workaround does.

I think the way to fix this trap would be to deprecate the `missing` query in favor of explicit negations of the `exists` query.
</comment><comment author="clintongormley" created="2015-10-14T13:16:06Z" id="148046273">@jpountz ++ makes sense.

Closing in favour of #14112
</comment><comment author="spati-java" created="2017-03-06T00:28:53Z" id="284276614">Since the "not" is deprecated , you can use the must not . 

POST /my_index/my_type/_search
`{
    "filter": {
        "bool": {
            "must_not": [
               {
                   "nested": {
                      "path": "path_to_nested_doc",
                      "query": {
                          "match_all": {}
                      }
                   }
               }
            ]
        }
    }
} `</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Does not analyze query string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3494</link><project id="" key="" /><description>As per the [discussion on the list](https://groups.google.com/d/topic/elasticsearch/NHR4uRa0y8E/discussion) the query_string query does not analyze the search string. (There was no resolution on the list or the IRC channel, so I am assuming a problem with elasticsearch.)

I use the following analyzer configuration:

``` yaml
index:
    analysis: 
        analyzer: 
            default: 
                alias: [goabout]
                type: custom
                tokenizer: standard
                filter: [lowercase, synonym, standard, asciifolding]
                char_filter: [char_mapper]
            postal_code: 
                tokenizer: keyword
                filter: [lowercase]
        tokenizer: 
            standard: 
                stopwords: []
        filter: 
            synonym: 
                type: synonym
                synonyms:
                  - st =&gt; sint
                  - den haag =&gt; s gravenhage
                  - den bosch =&gt; s hertogenbosch
                  - jp =&gt; jan pieterszoon
                  - mh =&gt; maarten harpertszoon
        char_filter: 
            char_mapper: 
                type: mapping
                mappings:
                  - ij =&gt; y
```

(I also tried naming the analyzer `goabout` and aliasing it to `default`, but that does not change the results.)

And this mapping for the `address` type:

``` json
{ 
    "properties": { 
        "street":      { "type": "string" }, 
        "housenumber": { "type": "string" }, 
        "postal_code": { "type": "string", "analyzer": "postal_code" }, 
        "city":        { "type": "string" }, 
        "point":       { "type": "geo_point" } 
    } 
} 
```

I then the index the following document:

``` console
$ curl -XPUT http://localhost:9200/geocoder/address/1 -d "{\"city\": \"'s-Gravenhage\", \"point\": {\"lat\": 52.034608082483366, \"lon\": 4.266201580347966}, \"street\": \"Wantsnijdersgaarde\", \"postal_code\": \"2542 GN\", \"housenumber\": \"573\"}"
```

The analyzer works correctly:

``` console
$ curl -X GET "http://localhost:9200/geocoder/_analyze?pretty=true" -d "Den Haag"     
{
  "tokens" : [ {
    "token" : "s",
    "start_offset" : 0,
    "end_offset" : 3,
    "type" : "SYNONYM",
    "position" : 1
  }, {
    "token" : "gravenhage",
    "start_offset" : 4,
    "end_offset" : 8,
    "type" : "SYNONYM",
    "position" : 2
  } ]
}
```

The analyzer seems to get use in both indexing and querying, as this query (that exchanges "y" for "ij") finds the document:

``` console
$ curl -X GET "http://localhost:9200/geocoder/_search?q=Wantsnydersgaarde&amp;analyzer=goabout&amp;pretty=true"       
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.095891505,
    "hits" : [ {
      "_index" : "geocoder",
      "_type" : "address",
      "_id" : "1",
      "_score" : 0.095891505, "_source" : {"city": "'s-Gravenhage", "point": {"lat": 52.034608082483366, "lon": 4.266201580347966}, "street": "Wantsnijdersgaarde", "postal_code": "2542 GN", "housenumber": "573"}
    } ]
  }
}
```

Also, querying for the indexed terms show the "ij" -&gt; "y" filter has processed the fields:

``` console
$ curl 'http://localhost:9200/geocoder/_search?pretty=true' -d '{ 
    "query" : { 
        "match_all" : { } 
    }, 
    "script_fields": { 
        "terms" : { 
            "script": "doc[field].values", 
            "params": { 
                "field": "_all" 
            } 
        } 

    } 
}' 
{ 
  "took" : 10, 
  "timed_out" : false, 
  "_shards" : { 
    "total" : 5, 
    "successful" : 5, 
    "failed" : 0 
  }, 
  "hits" : { 
    "total" : 1, 
    "max_score" : 1.0, 
    "hits" : [ { 
      "_index" : "geocoder", 
      "_type" : "address", 
      "_id" : "1", 
      "_score" : 1.0, 
      "fields" : { 
        "terms" : [ "2542", "573", "gn", "gravenhage", "s", 
"wantsnydersgaarde" ] 
      } 
    } ] 
  } 
} 
```

But this search query does not return results:

```
$ curl -X GET "http://localhost:9200/geocoder/_search?q=den+haag&amp;analyzer=goabout&amp;pretty=true"       
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

So apparently the query string is not analyzed.
</description><key id="17984894">3494</key><summary>Does not analyze query string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jcassee</reporter><labels /><created>2013-08-13T08:32:10Z</created><updated>2013-08-18T00:13:55Z</updated><resolved>2013-08-18T00:13:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-08-16T02:24:46Z" id="22744179">@jcassee query In order to find operators, the query string parser is using spaces to split query string first and only then it passes each part through analyzer to produce token. Your query is basically equivalent to the query "den OR haag":

```
$ curl -XGET "http://localhost:9200/geocoder/_validate/query?q=den+haag&amp;analyzer=goabout&amp;pretty=true&amp;explain=true"
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "test-idx",
    "valid" : true,
    "explanation" : "_all:den _all:haag"
  } ]
}
```

However, if you provide "den haag" as a phrase, query string query passes it to analyzer as is:

```
curl -XGET "http://localhost:9200/geocoder/_validate/query?q=%22den+haag%22&amp;analyzer=goabout&amp;pretty=true&amp;explain=true"
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "test-idx",
    "valid" : true,
    "explanation" : "(_all:\"s gravenhage\")"
  } ]
}
```

If you are not going to use any query string query operators, I would suggest using [match query](http://www.elasticsearch.org/guide/reference/query-dsl/match-query/) instead of query string query. The match query is using actual analyzer to split query string into tokens instead of using spaces:

```
curl -XGET "http://localhost:9200/geocoder/_search/?pretty=true" -d '{
    "query": {
        "match": {
            "_all": {
                "query": "den haag"
            }
        }
    }
}'
```
</comment><comment author="jcassee" created="2013-08-18T00:13:53Z" id="22822251">Thanks for the explanation, Igor. I'm closing this issue as it is not a problem with elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>'plugin' often returns 0 when error has occurred during install.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3493</link><project id="" key="" /><description>I've written a Chef resource provider for [elasticsearch](https://github.com/SimpleFinance/chef-elasticsearch), and additionally the plugin management tool. Because I'm creating a resource that system(plugin)'s its very useful to check the exit status.

```
root@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# ./plugin -i elasticsearch/elasticsearch-zookeeper/0.90.0
-&gt; Installing elasticsearch/elasticsearch-zookeeper/0.90.0...
Trying http://download.elasticsearch.org/elasticsearch/elasticsearch-zookeeper/elasticsearch-zookeeper-0.90.0.zip...
Trying http://search.maven.org/remotecontent?filepath=elasticsearch/elasticsearch-zookeeper/0.90.0/elasticsearch-zookeeper-0.90.0.zip...
Trying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/elasticsearch-zookeeper/0.90.0/elasticsearch-zookeeper-0.90.0.zip...
Trying https://github.com/elasticsearch/elasticsearch-zookeeper/zipball/v0.90.0... (assuming site plugin)
Failed to install elasticsearch/elasticsearch-zookeeper/0.90.0, reason: failed to download out of all possible locations..., use -verbose to get detailed information
root@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# echo $?
0
root@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin#
```

```
root@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# ./plugin -i sonian/elasticsearch-zookeeper
-&gt; Installing sonian/elasticsearch-zookeeper...
Trying https://github.com/sonian/elasticsearch-zookeeper/zipball/master... (assuming site plugin)
Downloading ..............DONE
Plugin installation assumed to be site plugin, but contains source code, aborting installation...
root@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# echo $?
0
```

Thanks!
</description><key id="17981210">3493</key><summary>'plugin' often returns 0 when error has occurred during install.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">miah</reporter><labels /><created>2013-08-13T06:26:24Z</created><updated>2013-08-13T06:46:21Z</updated><resolved>2013-08-13T06:31:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-13T06:30:24Z" id="22545595">Hey there,

I think we fixed that last week, so it should be in the next release, see #3463
If this does not fit your needs, please tell what is missing.
</comment><comment author="miah" created="2013-08-13T06:31:52Z" id="22545634">Ah excellent. Looks like I missed it in my search! Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node quit cluster and never come back</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3492</link><project id="" key="" /><description>Hi guys,

The issue seems to be related to this one http://elasticsearch-users.115913.n3.nabble.com/Elasticsearch-stopped-itself-from-time-to-time-td4036111.html

ES version: 0.90.2

Cluster structure: 4 data nodes + 6 client nodes.

Features: lots of write (more than read), zen unicast

What happened: 1 data node (node-1) removed 3 lite-client nodes and just stay on it own like that. Head plugin of (node-1) shows 4 data nodes + 3 other lite-client nodes. Head plugin of (node-2) shows 3 data nodes + 6 other client nodes.

I only have INFO set up so at the point of failure I can only see some thing like this in node-1 log.

```
[INFO ][cluster.service          ] [Troll] removed {[host.com][f5M36Uv3Q627-JUdlhi9SA][inet[/ip:9300]]{client=true, data=false, master=false},}, reason: zen-disco-node_failed([host.com][f5M36Uv3Q627-JUdlhi9SA][inet[/ip:9300]]{client=true, data=false, master=false}), reason transport disconnected (with verified connect)

```

Thread dump have quite a few thread like this.

```
"elasticsearch[Troll][generic][T#193]" daemon prio=10 tid=0x00007fce5c067000 nid=0x5b17 waiting for monitor entry [0x00007fce86880000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:596)
    - waiting to lock &lt;0x00000000c05690c8&gt; (a java.lang.Object)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:580)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:127)
    at org.elasticsearch.discovery.zen.ZenDiscovery.handleJoinRequest(ZenDiscovery.java:607)
    at org.elasticsearch.discovery.zen.ZenDiscovery.access$2000(ZenDiscovery.java:76)
    at org.elasticsearch.discovery.zen.ZenDiscovery$MembershipListener.onJoin(ZenDiscovery.java:752)
    at org.elasticsearch.discovery.zen.membership.MembershipAction$JoinRequestRequestHandler.messageReceived(MembershipAction.java:164)
    at org.elasticsearch.discovery.zen.membership.MembershipAction$JoinRequestRequestHandler.messageReceived(MembershipAction.java:153)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```

Restart node-1 solve the issue but hope we can find out what actually happened.
Son.
</description><key id="17978875">3492</key><summary>Node quit cluster and never come back</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phungleson</reporter><labels><label>feedback_needed</label></labels><created>2013-08-13T04:25:10Z</created><updated>2014-08-09T00:18:08Z</updated><resolved>2014-08-09T00:18:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="phungleson" created="2013-08-13T04:40:42Z" id="22542883">Additional log from client node when it tried to index.

```
org.elasticsearch.discovery.MasterNotDiscoveredException: waited for [1m]
at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:169) [elasticsearch-0.90.2.jar:na]
at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:377) [elasticsearch-0.90.2.jar:na]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_21]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_21]
at java.lang.Thread.run(Thread.java:722) [na:1.7.0_21]
```
</comment><comment author="galindro" created="2014-03-07T16:17:05Z" id="37039170">Same problem with version 1.0.0 
</comment><comment author="clintongormley" created="2014-08-08T14:14:30Z" id="51607365">Hi @phungleson 

Sorry it has taken a while to get to this.  Are you still seeing these issues in recent versions?
</comment><comment author="phungleson" created="2014-08-09T00:18:05Z" id="51670915">Hey,

I don't have this problem for quite a while, from 90.3 to 1.3.1. Increasing `zen.ping.timeout` seems to do the trick. I will close this for now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Allow to change _source exclude/include at runtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3491</link><project id="" key="" /><description>Allow to update the `_source` mapping exclude/include dynamically when we merge mappings.
</description><key id="17965430">3491</key><summary>Mapping: Allow to change _source exclude/include at runtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-12T21:11:10Z</created><updated>2013-08-13T15:09:17Z</updated><resolved>2013-08-13T15:09:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/test/java/org/elasticsearch/test/integration/AbstractSharedClusterTest.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Allow to update the _source mapping exclude/include dynamically when we merge mappings.</comment></comments></commit></commits></item><item><title>Different json response for arrays between get and search using fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3490</link><project id="" key="" /><description>I noticed a problem returning values for an array when specifying fields to return. I indexed a document containing an empty array and an array containing just one element. Getting the whole document works fine:

curl -XGET http://localhost:9300/picture/picture_object/16084571
{"_index":"picture","_type":"picture_object","_id":"16084571","_version":1,"exists":true, "_source" : {"URI":"16084571","CATEGORY":[],"MAINCOLORS":["#c0c0c0"]}}

This is the result as expected. Next thing I tried was reading just certain fields:

curl -XGET http://localhost:9300/picture/picture_object/16084571?fields=MAINCOLORS,CATEGORY
{"_index":"picture","_type":"picture_object","_id":"16084571","_version":1,"exists":true,"fields":{"MAINCOLORS":"#c0c0c0"}}

Category is not returned, I would have expected an empty array here. Maincolors became a single element, not an array anymore. Finally, I tried searching and returning fields:

curl -XGET http://localhost:9300/picture/picture_object/_search -d '{"query":{"term":{"_id":16084571}},"fields":["CATEGORY","MAINCOLORS"]}'
{"took":2,"timed_out":false,"_shards":{"total":8,"successful":8,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"picture","_type":"picture_object","_id":"16084571","_score":1.0,"fields":{"MAINCOLORS":["#c0c0c0"],"CATEGORY":[]}}]}}

This test was done on elasticsearch 0.90.2 and 0.90.3. I compared it with 0.20.2, there was the result the expected one:

curl -XGET http://localhost:9300/picture/picture_object/16084571?fields=CATEGORY,PSCORE,MAINCOLORS
{"_index":"picture","_type":"picture_object","_id":"16084571","_version":1,"exists":true,"fields":{"MAINCOLORS":["#c0c0c0"],"CATEGORY":[],"PSCORE":1515}}

So first there is a difference between search and get, second I would think that changing returned field from array to single element or not returning empty arrays is not right, the user probably had a reason for indexing an empty array or an array instead of just a single element.
</description><key id="17949133">3490</key><summary>Different json response for arrays between get and search using fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">arosenheinrich</reporter><labels /><created>2013-08-12T16:06:25Z</created><updated>2014-01-13T11:39:10Z</updated><resolved>2014-01-10T15:57:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arosenheinrich" created="2013-08-14T07:10:32Z" id="22618554">Btw, this behavior is the same for mget.
</comment><comment author="RomanTeucher" created="2013-11-07T09:35:43Z" id="27948754">I had this problem as well, took me ages to find the error. Is this going to be change in the future?
</comment><comment author="kimchy" created="2013-11-07T10:15:55Z" id="27951230">yes, this is addressed in 1.0 with the new _source include/exclude feature. The problem of where it comes from now is the fact that fields try and work seamlessly between both stored fields and extracting them from the json document..., with the new _source include/exclude feature, it will be consistent (and always extracted from source).
</comment><comment author="tikitu" created="2013-12-02T19:54:14Z" id="29651433">No chance of a fix to the underlying issue on the 0.90 branch? Or is there some workaround by tweaking the mapping?
</comment><comment author="bleskes" created="2013-12-03T09:04:06Z" id="29693200">@tikitu the _source include/exclude can not be back ported to 0.90 because of a wire level change which will break backwards compatibility. In case you are referring to the search API, you can use the partial fields option- http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fields.html#partial  . Sadly this one is not available on Gets.
</comment><comment author="tikitu" created="2013-12-03T09:49:25Z" id="29696047">@bleskes A shame, but thanks for the info.
</comment><comment author="javanna" created="2014-01-10T15:57:44Z" id="32039060">Closing in favour of #4542. The `fields` behaviour always returns an array in 1.0. The behaviour is also consistent across apis. For more control have also a look at [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-request-source-filtering.html).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>More helpful error when specifying side=BACK on edge n-gram tokenizer/tokenfilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3489</link><project id="" key="" /><description>From what I can tell, as of Lucene 4.4/elasticsearch 0.90.2 the "side=BACK" variation of edgeNGram tokenizer/tokenfilter has not only been deprecated, but now throws an ElasticSearchIllegalArgumentException:

&gt; org.elasticsearch.ElasticSearchIllegalArgumentException: side=BACK is not supported anymore. Please fix your analysis chain or use an older compatibility version (&lt;=4.2) but beware that it might cause highlighting bugs.

That's fine, but the exception message as well as the documentation page for edgeNGram on elsaticsearch.org don't provide any information on _how_ you can/should fix your analysis chain.

http://www.elasticsearch.org/guide/reference/index-modules/analysis/edgengram-tokenizer/

&gt; There used to be a side parameter up to 0.90.1 but it is now deprecated.

This change also wasn't mentioned/linked in any release notes (at least not that I could find).

After some digging, I found this ticket in Lucene's Jira instance that explains a) what changed and b) how to work around this issue if you were using side=BACK previously:

https://issues.apache.org/jira/browse/LUCENE-3907?focusedCommentId=13653011&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13653011

The elasticsearch docs and the exception message should provide some guidance on how to workaround this backwards-incompatible change.
</description><key id="17944328">3489</key><summary>More helpful error when specifying side=BACK on edge n-gram tokenizer/tokenfilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dbertram</reporter><labels><label>bug</label><label>docs</label><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-12T14:45:34Z</created><updated>2013-08-15T12:28:16Z</updated><resolved>2013-08-14T21:31:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-13T08:10:17Z" id="22549235">I agree we should be more verbose regarding how to gain the previous behaviour. Yet, I have to say this is bw compatible given you have an already existing index that option will still work ok. You can also use the option if you specify an old version in the mapping like '{ "version" : "4.2", ...}' I will go ahead and fix the documentation and the exception for this tokenizer and related token filters. Thanks for reporting.

simon
</comment><comment author="dbertram" created="2013-08-13T11:14:19Z" id="22557836">Thanks, Simon! Sorry about the mix-up regarding whether or not this was backwards incompatible. Our setup does automatic mapping updates, so from my perspective all I did was update from 0.90.1 to 0.90.3 and then I started seeing exceptions. I'll be more careful before filing issues in the future. :)
</comment><comment author="s1monw" created="2013-08-13T11:25:15Z" id="22558349">@dbertram I think it's good to file issues! First other people can easily google for it and 2. they see how to resolve it. I just wanted to make sure that folks understand that it's not breaking anything. Thanks for opening this. Just to confirm, you move to a setup where you have `"filter" : ["reverse", "edgeNGram", "reverse"]`  right?
</comment><comment author="dbertram" created="2013-08-13T14:05:42Z" id="22566983">Correct. I've also tested this using the [_analyze API](http://www.elasticsearch.org/guide/reference/api/admin-indices-analyze/) to ensure the old side=BACK tokenizer and the new "analyzer with reverse, edgeNGram, reverse token filters" produce the same tokens. The offsets are different, but from what I've read that was the entire point of modifying the edgeNGram tokenizer/token filter in the first place.

Just one thing I wanted to clarify wrt the "backwards compatible change" aspect:

If you have an index created with 0.90.1 that defines an edgeNGram tokenizer using side=BACK and then upgrade to 0.90.3, you _do_ start getting exceptions without changing the mapping when you try to index documents that use that tokenizer:

``` java
org.elasticsearch.ElasticSearchIllegalArgumentException: side=BACK is not supported anymore. Please fix your analysis ch
ain or use an older compatibility version (&lt;=4.2) but beware that it might cause highlighting bugs.
        at org.elasticsearch.index.analysis.EdgeNGramTokenizerFactory.create(EdgeNGramTokenizerFactory.java:66)
        at org.elasticsearch.index.analysis.CustomAnalyzer.createComponents(CustomAnalyzer.java:83)
        at org.apache.lucene.analysis.CustomAnalyzerWrapper.createComponents(CustomAnalyzerWrapper.java:60)
        at org.apache.lucene.analysis.AnalyzerWrapper.createComponents(AnalyzerWrapper.java:66)
        at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:177)
        at org.apache.lucene.document.Field.tokenStream(Field.java:552)
        at org.elasticsearch.index.mapper.core.StringFieldMapper$StringField.tokenStream(StringFieldMapper.java:360)
        at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:95)
        at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:245)
        at org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:323)
        at org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:398)
        at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1288)
        at org.apache.lucene.index.IndexWriter.addDocuments(IndexWriter.java:1248)
        at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:577)
        at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:490)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:341)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:207)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationActi
on.performOnPrimary(TransportShardReplicationOperationAction.java:521)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationActi
on$1.run(TransportShardReplicationOperationAction.java:419)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
```

So wouldn't that qualify as a backwards-incompatible change? Or am I misunderstanding something?
</comment><comment author="s1monw" created="2013-08-13T14:55:49Z" id="22570714">@dbertram this is what I feared! This should not be the case, so this seems to be a bug! I will try to come up with a test that reproduces this.
</comment><comment author="s1monw" created="2013-08-14T21:40:46Z" id="22669438">I found out what caused the problems with upgrading. Unfortunately the check was not smart enough to handle the case correctly so I fixe the upgrade path. This should now work just fine to upgrade from `0.90.1` to `0.90.3`. The problem here was essentially that we checked on the lucene version we added this with but `0.90.1` was already release with this lucene version and we manually ported the fixed tokenizers. So I needed to check for the actual ES version to make this work correctly.
</comment><comment author="dbertram" created="2013-08-15T12:21:28Z" id="22699659">Ah. Glad you got the upgrade path smoothed out. Thanks, Simon!
</comment><comment author="s1monw" created="2013-08-15T12:28:16Z" id="22699914">@dbertram thanks for bringing it up!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/AbstractTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java</file><file>src/test/java/org/elasticsearch/test/unit/index/analysis/NGramTokenizerFactoryTests.java</file></files><comments><comment>Improve backwards compatibility handling for NGram / EdgeNGram analysis</comment></comments></commit></commits></item><item><title>Add multi percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3488</link><project id="" key="" /><description>The multi percolate allows the bundle multiple percolate requests into one request. This api works similar to the multi search api.

The request body format is line based. Each percolate request item takes two lines, the first line is the header and the second line is the body.

The header can contain any parameter that normally would be set via the request path or query string parameters. There are several percolate actions, because there are multiple types of percolate requests:
- `percolate` - Action for defining a regular percolate request.
- `count` - Action for defining a count percolate request.

Each action has its own set of parameters that need to be specified in the percolate action.
Format:

```
{"[header_type]" : {[options...]}
{[body]}
```

Depending on the percolate action different parameters can be specified. For example the percolate and percolate existing document actions support different parameters.

The following endpoints are supported:

```
POST localhost:9200/[index]/[type]/_mpercolate
POST localhost:9200/[index]/_mpercolate
POST localhost:9200/_mpercolate
```

The `index` and `type` defined in the url path are the default index and type.
## Example
#### Request:

``` bash
curl -XGET 'localhost:9200/twitter/tweet/_mpercolate' --data-binary @requests.txt; echo
```

The index twitter is the default index and the type tweet is the default type and will be used in the case a header doesn't specify an index or type.
##### requests.txt:

```
{"percolate" : {"index" : twitter", "type" : "tweet"}}
{"doc" : {"message" : "some text"}}
{"percolate" : "index" : twitter", "type" : "tweet", "id" : "1"}
{}
{"percolate" : "index" : users", "type" : "user", "id" : "3", "percolate_index" : "users_2012" }
{"size" : 10}
{"count" : {"index" : twitter", "type" : "tweet"}}
{"doc" : {"message" : "some other text"}}
{"count" : "index" : twitter", "type" : "tweet", "id" : "1"}
{}
```

For a percolate existing document item (headers with the `id` field), the response can be an empty json object. All the required options are set in the header.
#### Response:

``` json
{
    "items" : [
        {
            "took" : 24,
            "_shards" : {
                "total" : 5,
                "successful" : 5,
                "failed" : 0,
            },
            "total" : 3,
            "matches" : ["1", "2", "3"]
        },
        {
            "took" : 12,
            "_shards" : {
                "total" : 5,
                "successful" : 5,
                "failed" : 0,
            },
            "total" : 3,
            "matches" : ["4", "5", "6"]
        },
        {
            "error" : "[user][3]document missing"
        },
        {
            "took" : 12,
            "_shards" : {
                "total" : 5,
                "successful" : 5,
                "failed" : 0,
            },
            "total" : 3
        },
        {
            "took" : 14,
            "_shards" : {
                "total" : 5,
                "successful" : 5,
                "failed" : 0,
            },
            "total" : 3
        }
    ]
}
```

Each item represents a percolate response, the order of the items maps to the order in where the percolate requests were specified. In case a percolate request failed, the item response is substituted with an error message. 
</description><key id="17942788">3488</key><summary>Add multi percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-08-12T14:20:24Z</created><updated>2013-09-03T12:32:09Z</updated><resolved>2013-08-12T16:33:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/test/java/org/elasticsearch/test/integration/percolator/MultiPercolatorTests.java</file><file>src/test/java/org/elasticsearch/test/unit/action/percolate/MultiPercolatorRequestTests.java</file></files><comments><comment>Added multi percolate api</comment></comments></commit></commits></item><item><title>PutMapping requests were prematurely acknowledged if other nodes were quicker than master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3487</link><project id="" key="" /><description>If other nodes completed process the new mapping before the master did, the put mapping request would acknowledge before master finished processing.

This can happen when running multiple nodes on one machine.
</description><key id="17942190">3487</key><summary>PutMapping requests were prematurely acknowledged if other nodes were quicker than master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-12T14:09:06Z</created><updated>2013-08-12T15:02:33Z</updated><resolved>2013-08-12T15:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Postponed acknowledging put mapping requests to after master has finished processed them</comment></comments></commit></commits></item><item><title>FastVectorHighlighter fails with StackOverflow on terms with large TermFrequency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3486</link><project id="" key="" /><description>FVH deploys some recursive logic to extract terms from documents that need to highlighted. For documents that have terms with super large term frequency like a document that repeats a terms very very often this can produce some very large stacks when extracting the terms. Taken to an extreme this causes stack overflow errors when this grow beyond a term frequency &gt;= 6000. 
The ultimate solution is a iterative implementation of the extract logic but until then we should protect users from these massive term extractions which might be not very useful in the first place. 

I will attach a possible fix and a test case that reproduces the problem in a bit.
</description><key id="17941456">3486</key><summary>FastVectorHighlighter fails with StackOverflow on terms with large TermFrequency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-12T13:54:38Z</created><updated>2013-09-23T17:21:23Z</updated><resolved>2013-08-12T15:47:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/vectorhighlight/XFieldTermStack.java</file><file>src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Limit the number of extracted token instance per query token.</comment></comments></commit></commits></item><item><title>Change response format of term vector endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3485</link><project id="" key="" /><description>This commit changes the response format of the term vectors
to be consistent with the response format of the analyze endpoint.

```
{
   "_index": "test",
   "_type": "type1",
   "_id": "1",
   "_version": 1,
   "exists": true,
   "term_vectors": {
      "field_with_positions_offsets": {
         "field_statistics": {..},
         "terms": {
            "evil": {
               "term_freq": 2,
               "pos": [ 4 , 7 ],
               "start": [ 17, 40 ],
               "end": [ 21 , 44 ]
            },
            "orthodontist": {
               "term_freq": 1,
               "pos": [ 5 ]
               ],
               "start": [ 22 ],
               "end": [ 34]
            }
         }
      }
   }
}
```

becomes

```
{
   "_index": "test",
   "_type": "type1",
   "_id": "1",
   "_version": 1,
   "exists": true,
   "term_vectors": {
      "field_with_positions_offsets": {
         "field_statistics": {..},
         "terms": {
            "evil": {
               "term_freq": 2,
               "tokens": [
                   { "position": 4, "start_offset": 17, "end_offset" : 21 },
                   { "position": 7, "start_offset": 40, "end_offset" : 44 }
               ]
            },
            "orthodontist": {
               "term_freq": 1,
               "tokens" : [ { "position": 5 , "start_offset" : 22, "end_offset" : 34 } ]
            }
         }
      }
   }
}
```

Closes issue #3484
</description><key id="17940385">3485</key><summary>Change response format of term vector endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-08-12T13:33:32Z</created><updated>2014-07-16T21:52:38Z</updated><resolved>2013-08-19T14:54:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-13T13:59:07Z" id="22566501">Looks good to me!
</comment><comment author="brwe" created="2013-08-19T14:54:55Z" id="22877758">closed with commit 28e867b5c1ef820d13
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change return format of term vector request </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3484</link><project id="" key="" /><description>The current output of the [term vector endpoint](https://github.com/elasticsearch/elasticsearch/issues/3114) is as follows (all options on):

```
{
   "_index": "test",
   "_type": "type1",
   "_id": "1",
   "_version": 1,
   "exists": true,
   "term_vectors": {
      "field_with_positions_offsets": {
         "field_statistics": {..},
         "terms": {
            "evil": {
               "term_freq": 2,
               "pos": [ 4 , 7 ],
               "start": [ 17, 40 ],
               "end": [ 21 , 44 ]
            },
            "orthodontist": {
               "term_freq": 1,
               "pos": [ 5 ]
               ],
               "start": [ 22 ],
               "end": [ 34]
            }
         }
      }
   }
}
```

If you look at the `evil` token, it has two occurrences which are currently represented by a two element array for all token properties (`pos`, `start` &amp; `end`). This parallel array  approach makes sense from a java memory approach but for other languages (like Python, Ruby etc.) it's confusing and not really user friendly.  

Furthermore, the term vector endpoint returns the nearly the same information as the [analyze endpoint](http://www.elasticsearch.org/guide/reference/api/admin-indices-analyze/) does and the response format of term vectors should be consistent with analyze. The response for the above example would the become:

```
{
   "_index": "test",
   "_type": "type1",
   "_id": "1",
   "_version": 1,
   "exists": true,
   "term_vectors": {
      "field_with_positions_offsets": {
         "field_statistics": {..},
         "terms": {
            "evil": {
               "term_freq": 2,
               "tokens": [
                   { "position": 4, "start_offset": 17, "end_offset" : 21 },
                   { "position": 7, "start_offset": 40, "end_offset" : 44 }
               ]
            },
            "orthodontist": {
               "term_freq": 1,
               "tokens" : [ { "position": 5 , "start_offset" : 22, "end_offset" : 34 } ]
            }
         }
      }
   }
}
```
</description><key id="17940093">3484</key><summary>Change return format of term vector request </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2013-08-12T13:26:50Z</created><updated>2013-08-21T16:53:22Z</updated><resolved>2013-08-21T16:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvector/TermVectorResponse.java</file><file>src/test/java/org/elasticsearch/test/integration/termvectors/GetTermVectorTestsCheckDocFreq.java</file></files><comments><comment>Change response format of term vector endpoint</comment></comments></commit></commits></item><item><title>Change default operation_threading to thread_per_shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3483</link><project id="" key="" /><description /><key id="17939525">3483</key><summary>Change default operation_threading to thread_per_shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-12T13:13:33Z</created><updated>2013-08-12T13:41:21Z</updated><resolved>2013-08-12T13:41:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/status/RestIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file></files><comments><comment>Changed default operation_threading from single_thread to thread_per_shard.</comment><comment>Closes #3483</comment></comments></commit></commits></item><item><title>Suggester: Phrase suggest option to limit suggestions to exising phrases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3482</link><project id="" key="" /><description>When using phrase suggest API to provide "Did you mean ?" corrections it would be nice to include only suggestions that would return results.

So returned phrase must exist at least in one document in the index.
</description><key id="17932812">3482</key><summary>Suggester: Phrase suggest option to limit suggestions to exising phrases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">radev</reporter><labels><label>:Suggesters</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2013-08-12T10:08:05Z</created><updated>2015-06-06T18:46:54Z</updated><resolved>2014-07-14T20:10:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-12T11:02:24Z" id="22485957">Thanks for opening this. do you feel like attaching a pullrequest? I'd be happy to help you sketching out the functionality here!
</comment><comment author="nik9000" created="2013-08-12T13:04:22Z" id="22491422">So I had a similar problem that came from filters: I was getting phrases that exist in the index but were filtered out on subsequent searches with the same filter set.  I've since mostly worked around the problem by splitting my index along the most common filter.  So if it isn't that much more work to get it to include filters that'd be great.  If OTOH, you do something like only return suggestions that match an n-gram then by all means just do that and ignore me.  I'd still use it and it'd probably be faster in the end.
</comment><comment author="s1monw" created="2013-08-12T13:23:26Z" id="22492411">folks, I don't think we can filter the process of drawing candidates etc. since performance will suffer badly. What I can imagine is to execute a match query with each suggestoin that is returned to make a decision to drop them or not. Yet, this will allow for filtering for sure but it will only be a helper to prune the result list. if this is ok for you guys I think we can certainly do that!
</comment><comment author="s1monw" created="2013-08-29T12:20:33Z" id="23484780">@nik9000 do you wanna take a look at this. I won't be able to do in the near future.
</comment><comment author="nik9000" created="2013-08-29T13:40:40Z" id="23489724">I can take a look sometime in the next few days, yeah.  We just deployed our elasticsearch software to a much larger group of users yesterday so I'm getting a bunch of high priority bugs that aren't (yet) this.  What I'll do is file this issue as a medium priority one on my side and pick it up when I've cleared everything higher.

So what'd be most useful for me would be to have an api like this:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
        "field" : "body",
        "size" : 5,
        "shard_size": 10,
        "confidence": 2.0,
        "filter_replace_string": "{}",
        "filter": {
          "bool" : {
                "must" : {
                    "query": { "match_phrase" : { "body" : "{}", "slop": 3 } }
                },
                "must_not" : {
                    "range" : {
                        "age" : { "from" : 10, "to" : 20 }
                    }
                },
                "should" : [
                    {
                        "term" : { "tag" : "sometag" }
                    },
                    {
                        "term" : { "tag" : "sometagtag" }
                    }
                ]
            }
        }
      }
    }
  }
}
```

I can see how this could be a performance problem but caching should kick in and help with most of the filters.  Also, if you set your confidence nice and high you might not have to do this _too_ many time.

And another thing, you'd have to crank up the `shard_size` value or you might end up filtering out all the suggestions that come from the shards.
</comment><comment author="nik9000" created="2013-09-06T12:19:26Z" id="23935956">Now that I'm digging into this I like this API better

`````` bash
curl -XPOST 'localhost:9200/_search' -d {
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
        "field" : "body",
        "size" : 5,
        "shard_size": 10,
        "confidence": 2.0,
        "filter": {
          "bool" : {
                "must_not" : {
                    "range" : {
                        "age" : { "from" : 10, "to" : 20 }
                    }
                },
                "should" : [
                    {
                        "term" : { "tag" : "sometag" }
                    },
                    {
                        "term" : { "tag" : "sometagtag" }
                    }
                ]
            }
        }
      }
    }
  }
}```
Internally I'd build a bool filter containing a phrase_match against the field against which we generate suggestions and the filter you passes in.  This is less fiddly to code and allows some simple syntax shortcuts:
```bash
curl -XPOST 'localhost:9200/_search' -d {
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
        "field" : "body",
        "size" : 5,
        "shard_size": 10,
        "confidence": 2.0,
        "filter": "yes"
      }
    }
  }
}```
which would add the phrase_match and
```bash
curl -XPOST 'localhost:9200/twitter/_search?pretty=true' -d '
{
    "query" : {
        "term" : { "message" : "something" }
    },
    "filter" : {
        "term" : { "tag" : "green" }
    }
    "suggest" : {
        "text" : "Xor the Got-Jewel",
        "simple_phrase" : {
            "phrase" : {
                "field" : "body",
                "size" : 5,
                "shard_size": 10,
                "confidence": 2.0,
                "filter": "query"
            }
        }
    }
}'
``````

which would go and get the filters from the top level query.

I'm still concerned about how slow this might be.
</comment><comment author="nik9000" created="2013-09-14T19:08:20Z" id="24454770">So I finally have something for this that kinda works. It doesn't fully work and I'd like some guidance. I can't post the code right now because I'm traveling. Such is life. Any way, I'd like giluidance on two things:
1. Since there is no good way to parse the request during the reduce phase I'm parsing what I need on the shards and sending it back. Is that right/normal?
2.  I can make queries during the reduce phase and block, waiting for them to return pretty easy. That seems to work on when I start the server but badly in tests, presumably because they have a lower bound on tr thread pool. Is it OK to make blocking calls in the reduce step?  Anything else seems like a pretty invasive change for this feature which doesn't have a ton of traction. 
</comment><comment author="s1monw" created="2013-11-13T11:25:35Z" id="28387289">moved over to `0.90.8` I will look at this soon as well 
</comment><comment author="timbunce" created="2014-04-25T11:20:55Z" id="41382297">Any news on this, or the similar #2842?
</comment><comment author="s1monw" created="2014-04-25T15:39:01Z" id="41406676">I think now that we have templates we can implement this much simpler - I think we should revisit it soon, thanks for pinging @timbunce 
</comment><comment author="brupm" created="2014-06-20T02:03:43Z" id="46638123">:+1: 
</comment><comment author="brupm" created="2014-06-27T01:36:54Z" id="47299754">This would be awesome to have. I ended up having to do it outside ES. 
1. Get the suggestions 
2. Perform another ES search to validate the existence
3. Present only existing phases to users
</comment><comment author="clintongormley" created="2014-07-01T13:37:09Z" id="47656434">@areek something you could take a look at?
</comment><comment author="areek" created="2014-07-01T19:17:20Z" id="47698214">@clintongormley already started looking into it!
</comment><comment author="areek" created="2014-07-07T18:39:43Z" id="48221665">After looking into it, I have come up with the following API for the suggestion filter option:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
    "suggest": {
        "text": "Xor the Got-Jewel",
        "simple_phrase": {
            "phrase": {
                "field": "body",
                "size": 5,
                "shard_size": 10,
                "confidence": 2,
                "filter": {
                    "match": {
                        "body": "{{suggestion}}"
                    }
                }
            }
        }
    }
}
```

The filter option above is just a query template with the magic variable "suggestion", which will be populated once phrase suggestions are made.
For the first iteration, I intend to make local queries for the filter instead of hitting all the shards and also restrict the size of the suggestions to a maximum of 20, when filter option is used.
</comment><comment author="clintongormley" created="2014-07-08T08:06:59Z" id="48283192">@areek Just a note: `match` is a query, not a filter.
</comment><comment author="areek" created="2014-07-08T14:26:12Z" id="48343115">@clintongormley Currently the filter param takes queries! The reason the name is filter, is to indicate that the query is used to filter out the suggestions after being generated
</comment><comment author="nik9000" created="2014-07-08T14:44:11Z" id="48345968">Its confusing to take queries and call it filter, I think.  Why not make it
take filters?

On Tue, Jul 8, 2014 at 10:26 AM, Areek Zillur notifications@github.com
wrote:

&gt; @clintongormley https://github.com/clintongormley Currently the filter
&gt; param takes queries! The reason the name is filter, is to indicate that the
&gt; query is used to filter out the suggestions after being generated
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3482#issuecomment-48343115
&gt; .
</comment><comment author="clintongormley" created="2014-07-08T20:32:25Z" id="48395868">Agree with @nik9000 about confusing.  `filter` means filters, not queries.  In that case should be called `query` to be consistent with other APIs.
</comment><comment author="areek" created="2014-07-08T23:44:07Z" id="48413654">It does seem confusing, the updated API looks like the following:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
    "suggest": {
        "text": "Xor the Got-Jewel",
        "simple_phrase": {
            "phrase": {
                "field": "body",
                "size": 5,
                "shard_size": 10,
                "confidence": 2,
                "filter": {
                    "template": {
                        "body": "{{suggestion}}"
                    },
                    "preference": "_only_local"
                }
            }
        }
    }
}
```

The change was mainly due to adding the `preference` param in the filter object. Calling it `query` would also confuse users at least with respect to the reason for providing such an option in the suggest API. Thoughts?
</comment><comment author="clintongormley" created="2014-07-09T09:19:43Z" id="48447442">@areek I assume that any query can be run there? it's not limited to just `template` queries?  Also, if the user wants to use a `template` query with other parameters, I assume they can just specify `params` and the `suggestion` value will be added to any manually specified params?

Either way, `filter` is the wrong name for that section.  Wherever we have `filter` or `*_filter`, it means that we are in "filter context" and only filter clauses can be accepted.

I'd be perfectly happy just renaming it to `query`, but if you want something slightly more descriptive then why not use `post_query`?  We use `post_filter` in the top level of search requests to say: "this filter will be applied to the `hits` array after the query has been run and after the aggregations have been calculated". This functionality seems similar.
</comment><comment author="s1monw" created="2014-07-09T19:17:24Z" id="48521271">what about:

``` Json

"collate" : {
  "filter|query" : { ... }   
  "preference": "_only_local"
}
```

then folks can pick if we should parse it as a query or not. The template engine doesn't care really it's just string replacements.... we can document it that it is passed through mustache.
</comment><comment author="areek" created="2014-07-09T21:16:54Z" id="48535347">@s1monw I like `collate`, I will go ahead and change it to that format and document the use of mustache under the covers.

@clintongormley all your assumptions are correct, except that the `params` support is not there, the reason for the template is to allow for the magic `suggestion` variable.
</comment><comment author="clintongormley" created="2014-07-10T11:39:29Z" id="48593823">@areek i'm thinking that users might want to use a [`template` query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-template-query.html#query-dsl-template-query) here. Couldn't you just check if there are params already defined, and merge `suggestion` variable into them?
</comment><comment author="areek" created="2014-07-10T17:32:01Z" id="48637114">@clintongormley I will end up doing that, thanks for suggesting.
</comment><comment author="areek" created="2014-07-10T22:42:31Z" id="48674273">So the updated API looks like the following:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
    "suggest": {
        "text": "Xor the Got-Jewel",
        "simple_phrase": {
            "phrase": {
                "field": "body",
                "size": 5,
                "shard_size": 10,
                "confidence": 2,
                "collate": {
                    "query": {
                        "{{field_name}}": "{{suggestion}}"
                    },
                    "preference": "_primary",
                    "params": {"field_name": "title"}
                }
            }
        }
    }
}
```

`collate` can also take a `filter` instead of `query` (as suggested by @s1monw) and optional `params` can be used, to inject the template `query` or `filter` with additional params (as suggested by @clintongormley).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/plain/Preference.java</file><file>src/main/java/org/elasticsearch/index/query/BytesFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionContext.java</file><file>src/test/java/org/elasticsearch/search/suggest/SuggestSearchTests.java</file></files><comments><comment>Phrase Suggester: Add collate option to PhraseSuggester</comment></comments></commit></commits></item><item><title>Java API BulkDelete returning isNotFound when run against index with multiple shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3481</link><project id="" key="" /><description>I've reproduced this problem with both v 0.90.2 and 0.90.3.

When I run some of my integration tests against an index that has multiple shards (running on multiple ES instances), some bulkDelete commands return isNotFound for documents that are in the index.  It works properly when I run against an index with a single shard.

I've not been able to reproduce this using the ElasticSearch REST API.  

I've attached images showing the BulkRequest in the debugger, the data in the index before the request runs, and how the shards are allocated.

All four DeleteResponses that come back have notFound set to true.  Notice that I'm attempting to delete the documents from multiple indices and they only exist in one.  This doesn't cause a problem when I run with just one shard and even if I only try to delete it from the index that it actually resides in, I still get notFound.

Also note that the actual bulkRequest uses aliases to the actual indices.  Again, I've tried it without aliases and I get the same behavior.

If I had to guess at what the problem is, I'd say it's related to the fact that the documents I'm attempting to delete are child documents of other documents in the index.  The parent of both these documents is the itemtype document with id "jmeter_000000_overwrite|@|file|@|id2|@|2"

![bulkrequestbuilder](https://f.cloud.github.com/assets/1827700/943527/372d9d36-0227-11e3-8c65-9bbe7585d327.png)
![eshead data](https://f.cloud.github.com/assets/1827700/943528/4668af84-0227-11e3-8796-67b705ef6917.png)
![eshead indices](https://f.cloud.github.com/assets/1827700/943530/529e0e70-0227-11e3-8633-02fa6869c916.png)
</description><key id="17906889">3481</key><summary>Java API BulkDelete returning isNotFound when run against index with multiple shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">bartakj</reporter><labels /><created>2013-08-11T01:53:19Z</created><updated>2013-08-13T06:25:09Z</updated><resolved>2013-08-12T17:19:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-12T06:39:38Z" id="22475494">Thanks for the report!

Can you share some code maybe, you are using? I'd like to see, if all the parent options are set in the bulk delete and how they are set. And how you configure your aliases... basically I'd like  to know it all from a code perspective :-)

Also, is this hundred percent reproducible? 
</comment><comment author="bartakj" created="2013-08-12T17:19:05Z" id="22509531">Thanks for the quick feedback.  I wasn't setting the parent field on the DeleteRequestBuilder.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard allocation to take into account free disk space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3480</link><project id="" key="" /><description>Simon says:

&gt; I can imagine some sort of disk space allocation decider that can restrict a node from
&gt; allocating any further shards given the used / free disk space and / or move shards 
&gt; away given a certain limit etc. We can also make allocation decision based on the size 
&gt; of the shards or move shards around once they fill up and we see that certain 
&gt; shards are much bigger than others

More at https://groups.google.com/forum/#!topic/elasticsearch/p-et4UxvcyU
</description><key id="17905565">3480</key><summary>Shard allocation to take into account free disk space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>feature</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-10T23:22:05Z</created><updated>2013-12-20T13:04:53Z</updated><resolved>2013-09-09T15:50:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-11T06:56:52Z" id="22453498">Thanks for opening this issue! I think we will get to this pretty soon ie. next week or so
</comment><comment author="synhershko" created="2013-08-28T08:33:07Z" id="23398898">eta?
</comment><comment author="s1monw" created="2013-08-28T17:13:55Z" id="23431522">@dakrone what's the status of this...
</comment><comment author="dakrone" created="2013-08-28T18:43:41Z" id="23437709">@synhershko I'm currently working on developing this, it's trickier than a usual AllocationDecider (which prevents allocation) because fetching the disk usages and shard sizes is overhead that we don't want to incur for every operation, so it needs to be cached for a time and refreshed at certain intervals.
</comment><comment author="synhershko" created="2013-09-30T07:39:26Z" id="25342264">hey guys - any chance this piece of decider will participate in other operations working with the FS, like optimization for example? what we are seeing is large indices being optimized and occasionally servers running very low on disk space because of that.

Maybe if an index doesn't have enough room to optimize a rebalancing should kick in?
</comment><comment author="s1monw" created="2013-09-30T13:52:35Z" id="25363189">Hmm that is something that is pretty rare condition though. I wonder if we really should have something like this in the core system or if we should just ask for a customer allocation decider since deciders can trigger a rebalance on such a condition via `canRemain`  - maybe this should go in a different issue? Can you open one?
</comment><comment author="synhershko" created="2013-09-30T13:57:25Z" id="25363712">https://github.com/elasticsearch/elasticsearch/issues/3807

It isn't this rare if you run a large data shop with replicas and all, with data constantly going in. It doesn't happen everyday, but it did happen to us.
</comment><comment author="s1monw" created="2013-09-30T13:58:13Z" id="25363835">IMO optimize should be rare in most cases unless you have time based indices etc. ;)
</comment><comment author="synhershko" created="2013-09-30T14:03:34Z" id="25364582">We do use rolling indexes...
</comment><comment author="dakrone" created="2013-09-30T14:29:17Z" id="25366700">Are you seeing a consistent amount of disk used for the optimize? If you know in advance about how much room you'll need for the optimize, you could set the high watermark for the disk threshold and ES should relocate shards if the disk usage passes that watermark.
</comment><comment author="synhershko" created="2013-09-30T14:32:35Z" id="25366983">Since ES can have this kind of info and do the maths for me, I don't see why I need to plan for it in advance. Plus I don't think ES can relocate a shard which is in the middle of it being optimized, and setting too high high-watermark is something we wouldn't want to do as well.
</comment><comment author="synhershko" created="2013-10-13T17:42:39Z" id="26222571">@dakrone  something that just occurred to us - how would the free space decider play along with ES's defaut to try and have the same number of shards on each node?

In our scenario (and I'm assuming this is quite common) we have many data servers each with different HD capacities, ranging from ~120GB to ~1000GB. I'm pretty sure if ES will try to balance based on both criteria something will go very wrong.

Did you take that into account? or should we try breaking this with some nasty tests?
</comment><comment author="dakrone" created="2013-11-15T05:05:47Z" id="28547683">@synhershko since the decider is part of the balancing process, the allocator will attempt to find the "best" weights that still satisfy all of the deciders, so it will try to balance the shards evenly, but will still allow uneven allocation in the event that the disk limit has been reached on a particular machine or set of machines.

&gt; Did you take that into account? or should we try breaking this with some nasty tests?

It should already be taken into account, but nasty tests are always appreciated! :)
</comment><comment author="synhershko" created="2013-11-17T13:19:33Z" id="28648290">Will try to get to it soon, then
</comment><comment author="synhershko" created="2013-12-19T08:34:46Z" id="30912664">@dakrone just to let you know we now use this feature in our highly uneven cluster and so far all looks good. It seems like ES still tries to even the number of shards on each node but the free-space decider seems to do a good job. Thanks!
</comment><comment author="dakrone" created="2013-12-19T19:24:30Z" id="30957722">Awesome! I'm glad to hear it's working well for you! :D
</comment><comment author="s1monw" created="2013-12-19T19:30:45Z" id="30958278">@synhershko this is great that you come back to us. Do you have trouble with the balancing in anyway as you mention that it tries to balance?
</comment><comment author="synhershko" created="2013-12-20T13:04:53Z" id="31008149">@s1monw no it all seems to be fine. What I meant is _when_ there's balancing in action, which hardly ever happens now as far as I can tell, it will try to get to an end result where disk allocation limits are respected AND there is more or less the same amount of shards on each node. Which I think makes sense.

Before upgrading I set out to write some integration which will test our cluster configuration with the internal moving pieces. I was able to recreate a similar scenario in the test (different node sizes, different index sizes) and everything worked (shards were allocated, no node was over-allocated etc). I just couldn't find edge-cases to test there - all seems to have already been tested in @dakrone 's tests so we dropped this effort and decided to take the pill.

We upgraded from a variant of 0.90.0 (custom compiled with some mods) to vanilla 0.90.7. The upgrade took a while (different Lucene versions) but went smooth, and once the disk-aware decider was enabled it took the cluster some while to stabilize, but ever since we did that all seems to work fine.

I'm leaving the company this week so will probably stop monitor that cluster, but as I said so far this looks very good and stable.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/ClusterInfo.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>src/main/java/org/elasticsearch/cluster/DiskUsage.java</file><file>src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/StartedRerouteAllocation.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>src/test/java/org/elasticsearch/test/unit/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>src/test/java/org/elasticsearch/test/unit/cluster/routing/allocation/RandomAllocationDeciderTests.java</file></files><comments><comment>Add AllocationDecider that takes free disk space into account</comment></comments></commit></commits></item><item><title>Null pointer exception for POST mode facets if facet_filter accepts no documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3479</link><project id="" key="" /><description>elasticsearch 0.90.3
org.elasticsearch.search.facet.FacetExecutor$Post$Filtered, line 66

```
for (int i = 0; i &lt; docSets.size(); i++) {
  ContextDocIdSet entry = docSets.get(i);
  DocIdSet filteredSet = filter.getDocIdSet(entry.context, null);
  filteredEntries.add(new ContextDocIdSet(
    entry.context,
    // TODO: can we be smart here, maybe AndDocIdSet is not always fastest?
    new AndDocIdSet(new DocIdSet[]{entry.docSet, filteredSet})
  ));
}
```

From the JavaDoc for Filter:
`NOTE: null can be returned if no documents are accepted by this Filter`

So, the filteredSet object can be null. It is then passed into the constructor of the AndDocIdSet, which can cause a null pointer exception during the execution of the POST facet.

The solution is that if filteredSet is null then the current ContextDocIdSet can be ignored.

```
if (filteredSet == null) continue;
```

I replicated this issue with the following query (where the filter facet does match a term in the index, but the facet filter does not):

```
{
  "query": {
    "match_all": {}
  },
  "size": 0,
  "facets": {
    "my_facet": {
      "filter": {
        "term": {
          "text": "document"
        }
      },
      "mode": "post",
      "facet_filter": {
        "term": {
          "text": "DOES_NOT_MATCH_ANYTHING",
        }
      }
    }
  }
}
```
</description><key id="17897981">3479</key><summary>Null pointer exception for POST mode facets if facet_filter accepts no documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ccw-morris</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-10T12:07:13Z</created><updated>2013-08-11T19:24:20Z</updated><resolved>2013-08-11T19:23:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-11T19:24:20Z" id="22463476">Fix pushed in 0.90 and master. Thanks for reporting it!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/facet/FacetExecutor.java</file><file>src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>NPE for POST mode facets if facet_filter gives no document.</comment><comment>Closes #3479.</comment></comments></commit></commits></item><item><title>Elasticsearch (0.90.2) fails in large core (Ex: ~48) machine </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3478</link><project id="" key="" /><description>elastic search creates number of threads based on the number of processors available. So when the elastic search  client library initialized,  it tries to create 350+ threads which is too much for the machine &amp; results in OOM.

Apparently org.elasticsearch.threadpool.ThreadPool tries to assign threads based in cores available in the machines. 

Extrat from logs:
thread_pool [index], type [fixed], size [48], 
queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [bulk], type [fixed], size [48], queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [get], type [fixed], size [48], queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [search], type [fixed], size [144], queue_size [1k], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [percolate], type [fixed], size [48], queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m] 

&amp; the stacktrace,

java.lang.OutOfMemoryError: unable to create new native thread
    at java.lang.Thread.start0(Native Method)
    at java.lang.Thread.start(Thread.java:640)
    at java.util.concurrent.ThreadPoolExecutor.addThread(ThreadPoolExecutor.java:681)
    at java.util.concurrent.ThreadPoolExecutor.addIfUnderMaximumPoolSize(ThreadPoolExecutor.java:727)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:655)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:343)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.&lt;init&gt;(AbstractNioSelector.java:95)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.&lt;init&gt;(AbstractNioWorker.java:53)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.&lt;init&gt;(NioWorker.java:45)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:45)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:28)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorkerPool.newWorker(AbstractNioWorkerPool.java:99)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:69)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.&lt;init&gt;(NioWorkerPool.java:39)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.&lt;init&gt;(NioWorkerPool.java:33)
    at org.elasticsearch.transport.netty.NettyTransport.doStart(NettyTransport.java:240)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
    at org.elasticsearch.transport.TransportService.doStart(TransportService.java:90)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:179)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:119)
    at 
</description><key id="17897035">3478</key><summary>Elasticsearch (0.90.2) fails in large core (Ex: ~48) machine </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pmanvi</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-10T10:18:42Z</created><updated>2013-10-01T20:59:34Z</updated><resolved>2013-08-14T22:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-10T10:36:23Z" id="22437661">Hey @pmanvi  can you provide some more info like:
- how much memory are you giving to the JVM
- how much memory are you using during applicaion runtime
- can you past the output of `ulimit -Hn` as well as `ulimit -a`
- how much traffic are you giving to your ES cluster when you see this problem?
</comment><comment author="pmanvi" created="2013-08-10T10:50:43Z" id="22437850">I tried setting higher values for -Xmx values and also ulimit -a, all of them didn't help. (we have 32GB of  RAM &amp; running many other java services)
This is testing code, so there is no traffic as such. It comes as soon as I instantiate the TransportClient() and error comes out as soon thread pools are initialized as you can see from the logs.

creating thread_pool [search], type [fixed], size [144],

I guess primary reason the creation of threads upfront based on the number of processor. (48 in our case)
http://www.elasticsearch.org/guide/reference/modules/threadpool/
</comment><comment author="s1monw" created="2013-08-14T22:22:46Z" id="22672111">I limited the number of CPUs we take into account when creating the size of the ThreadPool to `24` processors. This should give most of the people good defaults and should prevent you from getting crazy OOMs and too many threads. Would be great if you could give it a go.
</comment><comment author="kimchy" created="2013-08-14T22:27:02Z" id="22672354">Note that on the client library (if you are using a node as a client), most thread pools will not be used really (we don't prestart those threads). The ones I see from the stack trace is the netty worker ones (which by default is num_proc \* 2). If you want to explicitly control that you can set `transport.netty.worker_count` setting. The commit @s1monw pushed will also bound it to max 24 \* 2.
</comment><comment author="s1monw" created="2013-08-15T07:53:16Z" id="22689751">thanks @kimchy for clarifying, my commit just tries to not go crazy with the number of threads, if you have strong hardware you should adjust the sizes according to your needs. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Bound processor size based cals to 32</comment><comment>We use number of processors to choose default thread pool sizes, and number of workers in networking (for HTTP and transport). Bound it to max at 32 by default as a safety measure to create too many threads.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Limit the number created threads for machines with large number of cores</comment></comments></commit></commits></item><item><title>Failure to execute search request with empty top level filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3477</link><project id="" key="" /><description>The following causes failure to execute search request:

```
curl -XPOST 'http://localhost:9200/test/_search?pretty=true' -d '{ 
    "query" : {"query_string":{"query":"*","default_operator":"AND"}},
    "filter":{} 
}'
```
</description><key id="17895698">3477</key><summary>Failure to execute search request with empty top level filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-10T07:44:13Z</created><updated>2013-08-29T08:34:43Z</updated><resolved>2013-08-10T08:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Failure to execute search request with empty top level filter</comment><comment>closes #3477</comment></comments></commit></commits></item><item><title>_default_ mapping change is validated is if it was a normal type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3476</link><project id="" key="" /><description>Changes to mappings of types in ES are subject to certain validation. For example, if a field was previously analyzed, it is impossible to set it to be not analyzed as this will create inconsistencies with existing data.

The `_default_` mapping are used for new type creation and therefore it shouldn't be problem to change any setting there.

To reproduced:

```
curl -XPOST "http://localhost:9200/test" -d'
{
   "mappings": {
      "_default_": {
         "properties": {
            "f": {
                "type": "string", 
                "index": "analyzed"
            }
         }
      }
   }
}'
```

And then a change:

```
curl -XPOST "http://localhost:9200/test/_default_/_mapping" -d'
{
   "_default_": {
        "properties": {
            "f": {
                "type": "string", 
                "index": "not_analyzed"
            }
        }
   }
}'
```

Which results in an `MergeMappingException`
</description><key id="17876366">3476</key><summary>_default_ mapping change is validated is if it was a normal type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-09T17:49:25Z</created><updated>2013-08-09T18:37:00Z</updated><resolved>2013-08-09T18:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>when changing the mapping of the _default_ mapping, do not apply the old _default_ mapping to the new one and also do not validate the new version with a merge but parse is as a new type.</comment></comments></commit></commits></item><item><title>Suggest should ignore empty shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3475</link><project id="" key="" /><description>From #3469.
When running suggest on empty shards, it raises an error like:

```
"failures" : [ {
      "status" : 400,
      "reason" : "ElasticSearchIllegalArgumentException[generator field [title] doesn't exist]"
    } ]
```

We should ignore empty shards.

Closes #3473.
</description><key id="17867936">3475</key><summary>Suggest should ignore empty shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-08-09T15:15:30Z</created><updated>2014-06-15T12:46:58Z</updated><resolved>2013-08-16T09:39:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-09T15:41:41Z" id="22402795">Comment by @spinscale:

minor notes: remove the issue number from the, and do not put the for loop into the if… 

``` java
 if (reader.numDocs() == 0) {
  return new Suggest(Suggest.Fields.SUGGEST, suggestions);
}

for (…)
```

makes it more readable
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When replacing an existing _default_ type, the old one get merged into the new</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3474</link><project id="" key="" /><description>.

To reproduce (contrived example):

```
curl -XPOST "http://localhost:9200/test" -d'
{
   "mappings": {
      "_default_": {
              "date_detection": false
      }
   }
}'
```

Now add an empty _default_ (to remove it): 

```
curl -XPUT "http://localhost:9200/test/_default_/_mapping" -d'
{
   "_default_": {
   }
}'
```

And get the mapping :

```
curl -XGET "http://localhost:9200/test/_default_/_mapping"
```

which gives (note the `date_detection`) :

```
{
   "test": {
      "_default_": {
         "date_detection": false,
         "properties": {}
      }
   }
}
```
</description><key id="17867853">3474</key><summary>When replacing an existing _default_ type, the old one get merged into the new</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-09T15:14:10Z</created><updated>2013-08-09T18:37:00Z</updated><resolved>2013-08-09T18:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>when changing the mapping of the _default_ mapping, do not apply the old _default_ mapping to the new one and also do not validate the new version with a merge but parse is as a new type.</comment></comments></commit></commits></item><item><title>Suggest should ignore empty shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3473</link><project id="" key="" /><description>From #3469.
When running suggest on empty shards, it raises an error like:

```
"failures" : [ {
      "status" : 400,
      "reason" : "ElasticSearchIllegalArgumentException[generator field [title] doesn't exist]"
    } ]
```

We should ignore empty shards.
</description><key id="17867809">3473</key><summary>Suggest should ignore empty shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-09T15:13:04Z</created><updated>2013-08-16T09:38:15Z</updated><resolved>2013-08-16T09:38:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-14T20:26:24Z" id="22664520">@dadoonet what's the status of this?
</comment><comment author="dadoonet" created="2013-08-14T20:56:25Z" id="22666536">Sorry Simon. Did not find time to integrate your comments. Will do before the end of this week.
</comment><comment author="s1monw" created="2013-08-14T20:58:48Z" id="22666742">thanks @dadoonet 
</comment><comment author="dadoonet" created="2013-08-16T08:44:46Z" id="22754623">@s1monw I created an `AbstractSuggester` class and kept the `Suggester` interface. Ok with that?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java</file><file>src/main/java/org/elasticsearch/search/suggest/Suggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/term/TermSuggester.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CustomSuggester.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/SuggestSearchTests.java</file></files><comments><comment>Suggest should ignore empty shards</comment><comment>From #3469.</comment><comment>When running suggest on empty shards, it raises an error like:</comment></comments></commit></commits></item><item><title>Fix inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQue...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3472</link><project id="" key="" /><description>...ry

This commit fixes inconsistencies in `function_score` and `filters_function_score`
using scripts, see issue #3464

The method 'ScoreFunction.factor(docId)' is removed completely, since the name
suggests that this method actually computes a factor which was not the case.
Multiplying the computed score is now handled by 'FiltersFunctionScoreQuery'
and 'FunctionScoreQuery' and not implicitely performed in
'ScoreFunction.factor(docId, subQueryScore)' as was the case for 'BoostScoreFunction'
and 'DecayScoreFunctions'.

This commit also fixes the explain function for FiltersFunctionScoreQuery. Here,
the influence of the maxBoost was never printed. Furthermore, the queryBoost was
printed as beeing multiplied to the filter score.

Closes #3464
</description><key id="17864356">3472</key><summary>Fix inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQue...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-08-09T14:06:09Z</created><updated>2014-07-04T23:24:26Z</updated><resolved>2013-08-19T14:23:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-08-19T14:23:29Z" id="22875308">moved to pr [3535](https://github.com/elasticsearch/elasticsearch/pull/3535)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>term vector api crashes for documents that have no term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3471</link><project id="" key="" /><description>If term vectors for a document are requested but the document does not contain any field with term vectors stored, this causes a null pointer exception.
The same happens if the document has the field, but the field only contains, for example, only the string "?". 

Instead of crashing, an empty response should be returned.
</description><key id="17859201">3471</key><summary>term vector api crashes for documents that have no term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label></labels><created>2013-08-09T12:09:24Z</created><updated>2013-08-09T13:19:16Z</updated><resolved>2013-08-09T13:19:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvector/TermVectorResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportSingleShardTermVectorAction.java</file><file>src/test/java/org/elasticsearch/test/integration/termvectors/GetTermVectorTests.java</file><file>src/test/java/org/elasticsearch/test/integration/termvectors/GetTermVectorTestsCheckDocFreq.java</file></files><comments><comment>termvectors: fix null pointer exception if field has no term vectors</comment></comments></commit></commits></item><item><title>NPE in BytesRefOrdValComparator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3470</link><project id="" key="" /><description>This issue relates to issue #3189 and I'm experiencing it with `v1.0.0.Beta`, but I guess that applies to `0.90` branch as well.

While sorting on some trivial fields I get the following error message:

``` javascript
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[UIomKhmqSzumD0uzjmRbfA][brisa][0]: QueryPhaseExecutionException[[brisa][0]: query[filtered(ConstantScore(+NotFilter(cache(discarded:T))))-&gt;cache(_type:patients)],from[0],size[20],sort[&lt;custom:\"lastname\": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@1d8f4c01&gt;]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",
   "status": 500
}
```

`lastname` mapping is as follows:

``` javascript
"lastname": {
  "type": "string",
  "index": "not_analyzed",
  "omit_norms": true,
  "index_options": "docs"
}
```

Fairly standard IMO.

Now the funny thing is that I have another `name` mapping which is exactly the same and everything works just fine. From a data point of view, I cannot see a difference between one field and the other (upper case strings).

The NPE happens in `BytesRefOrdValComparator` at line 388 because `MultiDocs.ordinals()` returns `null`.

Now, i don't know why `lastname` happens to use `MultiOrdinals` (or `MultiDocs` for that matter) to do sorting, but making `MultiDocs` return its `MultiOrdinals` instance seems to fix the issue, although I'm not sure whether the null pointer was intentional and the problem is somewhere else.

Thoughts?

Thanks a lot in advance.
</description><key id="17846172">3470</key><summary>NPE in BytesRefOrdValComparator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ghost</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-09T04:03:44Z</created><updated>2014-02-18T23:59:03Z</updated><resolved>2013-08-09T12:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-09T06:45:30Z" id="22377893">Thanks for opening this! This is in-fact a bug that is triggered depending on how sparse your data in that field is. I will come up with a patch very soon and I try to increase the test coverage for this to make sure we are not running into this again.
</comment><comment author="s1monw" created="2013-08-09T12:43:51Z" id="22391676">thanks for reporting this so quickly!
</comment><comment author="ghost" created="2013-08-09T13:01:11Z" id="22392448">Hey, thank YOU for fixing it so quickly!

ES is an amazing project, I'm just glad I can help.

--- Original Message ---

From: "Simon Willnauer" notifications@github.com
Sent: August 9, 2013 9:44 AM
To: "elasticsearch/elasticsearch" elasticsearch@noreply.github.com
Cc: "dngdevelopment" kayowas@hotmail.com
Subject: Re: [elasticsearch] NPE in BytesRefOrdValComparator (#3470)

thanks for reporting this so quickly!

---

Reply to this email directly or view it on GitHub:
https://github.com/elasticsearch/elasticsearch/issues/3470#issuecomment-22391676
</comment><comment author="elasticjava" created="2013-08-13T08:56:07Z" id="22551375">Many thanks Simon! that error filled our log!! +1
</comment><comment author="tarunjangra" created="2013-09-02T17:32:11Z" id="23671704">Great guys, when you would be able to release this bug. We are using this very heavily. +1 
</comment><comment author="s1monw" created="2013-09-02T19:05:57Z" id="23674711">@tarunjangra it's coming very soon I hope we can move towards a release next week or so.
</comment><comment author="chrisamccoy" created="2013-09-03T09:23:04Z" id="23699595">@s1monw Yes, we'd love a fix for this bug. We're using it heavily at Yoursports.com http://techcrunch.com/2012/10/11/sports-section-2-0-yoursports-launches-its-ambitious-project-to-build-the-facebook-of-sports/
</comment><comment author="tarunjangra" created="2013-09-03T18:47:17Z" id="23737075">@s1monw i would like to share this behaviour. Query i am doing:
{"query":{"match_all":{}},"sort":{"title.untouched":{"order":"desc"}}}

And result i am getting:

QueryPhaseExecutionException[[es_index][11]: query[ConstantScore(cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@953b6c6c))],from[0],size[10],sort[&lt;custom:"title.untouched": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@1ca8b405&gt;!]: Query Failed [Failed to execute main query]]; nested: NullPointerException;

Some thing interesting i just noticed. I am also getting some stats about Shards.

total: 15
successful: 10
failed: 5

But yesterday i was getting 

total: 15
successful: 6
failed: 9

the question is, why these stats are changing? Is that some how usual ?

Thank you.
</comment><comment author="s1monw" created="2013-09-03T19:49:15Z" id="23741416">@tarunjangra this issue depends on a lot of things if your field data implementation uses the "non-broken" code paths due to segment merges you won't see the failure on all shards so this can change over time.
</comment><comment author="pbellora" created="2013-09-03T21:52:57Z" id="23750050">Is there a known workaround in the meantime? I was also using not_analyzed for sorting, and that's breaking with an NPE in BytesRefFieldComparatorSource (I'm assuming it's the same issue). I tried using keyword analysis instead but got the same failures.
</comment><comment author="tarunjangra" created="2013-09-04T02:18:57Z" id="23761709">@s1monw Yep: It make sense. We are waiting for next elasticsearch release. Would like to recognise all elasticsearch team for such a great community efforts.

Thank you.
</comment><comment author="mjebrini" created="2013-12-25T16:45:23Z" id="31201920">Hi I am searching with following query:

{
  "query": {
    "filtered": {
      "query": {
        "match_all": []
      },
      "filter": {
        "and": [
          {
            "query": {
              "match": {
                "status": "published"
              }
            }
          },
          {
            "numeric_range": {
              "rate": {
                "gte": 0,
                "lte": 5
              }
            }
          }
        ]
      }
    }
  },
  "from": 1,
  "size": 5,
  "sort": [
    {
      "activity_id": "desc"
    }
  ],
  "fields": [
    "activity_id"
  ]
}

After all I get into this exception:

QueryPhaseExecutionException[[activity][0]: query[ConstantScore(+QueryWrapperFilter(status:published) +rate:[0 TO 5])],from[1],size[5],sort[&lt;custom:\"activity_id\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@bda96b&gt;!]: Query Failed [Failed to execute main query]]; nested: ElasticSearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)];

Actually this is weird, i was facing the problem at 0.90.5 .. then I upgraded my elastic search binary to 0.90.9 and I still face the same error. 
</comment><comment author="mjebrini" created="2013-12-25T16:59:30Z" id="31202109">I think the sorting term is the cause of the problem!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file></files><comments><comment>Added random sort test for dense and sparse fields.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefOrdValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/MultiOrdinals.java</file></files><comments><comment>Return ordinals from MultiOrdinals.MultiDocs</comment></comments></commit></commits></item><item><title>Multi-field and suggest api error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3469</link><project id="" key="" /><description>The following error occur while trying to do a simple phrase query on a multi-field using the suggest api:

&lt;pre&gt;"failures" : [ {
      "status" : 400,
      "reason" : "ElasticSearchIllegalArgumentException[generator field [title] doesn't exist]"
    }, {
      "status" : 400,
      "reason" : "RemoteTransportException[[Bella Donna][inet[/127.0.0.1:9301]][search/phase/query]]; nested: ElasticSearchIllegalArgumentException[generator field [title] doesn't exist]; "
    } ]
&lt;/pre&gt;


And here is the code:

&lt;pre&gt;curl -XDELETE "http://localhost:9200/test?pretty"
curl -XPOST "http://localhost:9200/test?pretty" -d '{
  "settings": {
    "index": {
      "number_of_replicas": 0,
      "analysis":{
        "analyzer":{
          "suggest":{
            "type": "custom",
            "tokenizer": "standard",
            "filter": [ "standard", "lowercase", "suggest_shingle" ]
          }
        },
        "filter":{
          "suggest_shingle":{
            "type": "shingle",
            "min_shingle_size": 2,
            "max_shingle_size": 5,
            "output_unigrams": true
          }
        }
      }
    }
  }
}'
curl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d '{
  "test": {
 "properties" : {
    "title": {
      "path": "just_name",
      "type": "multi_field",
      "fields": 
        {
          "title": {
            "type": "string",
            "index": "analyzed",
            "similarity": "BM25",
            "analyzer": "suggest"
          }
        }
      }
      
    }
  }
}'


curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "title": "Just testing the suggestions api"
}'

curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "title": "An other title"
}'

curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "query": {
    "match_all": {}
  },
  "suggest": {
    "text": "tetsting sugestion",
    "phrase":{
      "phrase":{
        "field": "title",
        "max_errors": 5
      }
    }
  }
}'&lt;/pre&gt;
</description><key id="17845963">3469</key><summary>Multi-field and suggest api error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mathieu007</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-09T03:52:51Z</created><updated>2013-08-16T19:10:15Z</updated><resolved>2013-08-16T18:25:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-09T05:22:11Z" id="22375898">Does it work if you search on title.title?
</comment><comment author="dadoonet" created="2013-08-09T08:55:04Z" id="22382920">That's true, it fails.

But when using another _sub field_ in multi field, it works fine:

``` sh
curl -XDELETE "http://localhost:9200/test?pretty"
curl -XPOST "http://localhost:9200/test?pretty" -d '{
  "settings": {
    "index": {
      "number_of_replicas": 0,
      "analysis":{
        "analyzer":{
          "suggest":{
            "type": "custom",
            "tokenizer": "standard",
            "filter": [ "standard", "lowercase", "suggest_shingle" ]
          }
        },
        "filter":{
          "suggest_shingle":{
            "type": "shingle",
            "min_shingle_size": 2,
            "max_shingle_size": 5,
            "output_unigrams": true
          }
        }
      }
    }
  }
}'
curl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d '{
  "test": {
 "properties" : {
    "title": {
      "path": "just_name",
      "type": "multi_field",
      "fields": 
        {
           "title": {
            "type": "string",
            "index": "analyzed",
            "similarity": "BM25",
            "analyzer": "suggest"
          },
           "title_suggest": {
            "type": "string",
            "index": "analyzed",
            "similarity": "BM25",
            "analyzer": "suggest"
          }
        }
      }
    }
  }
}'


curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "title": "Just testing the suggestions api"
}'

curl -XPOST "http://localhost:9200/test/test?pretty&amp;refresh" -d '{
  "title": "An other title"
}'

curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "suggest": {
    "text": "tetsting sugestion",
    "phrase":{
      "phrase":{
        "field": "title_suggest",
        "max_errors": 5
      }
    }
  }
}'
```

I'm going to look at it but I don't think you can use suggest with multifield and only one field declared under (the default one).
</comment><comment author="dadoonet" created="2013-08-09T09:28:43Z" id="22384284">I found the _issue_ which is not exactly an issue.

If you add a `sleep` in your script, it works fine: 

``` sh
curl -XDELETE "http://localhost:9200/test?pretty"
curl -XPOST "http://localhost:9200/test?pretty" -d '{
  "settings": {
    "index": {
      "number_of_replicas": 0,
      "analysis":{
        "analyzer":{
          "suggest":{
            "type": "custom",
            "tokenizer": "standard",
            "filter": [ "standard", "lowercase", "suggest_shingle" ]
          }
        },
        "filter":{
          "suggest_shingle":{
            "type": "shingle",
            "min_shingle_size": 2,
            "max_shingle_size": 5,
            "output_unigrams": true
          }
        }
      }
    }
  }
}'

curl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d '{
  "test": {
 "properties" : {
    "title": {
      "path": "just_name",
      "type": "multi_field",
      "fields": 
        {
           "title": {
            "type": "string",
            "index": "analyzed",
            "similarity": "BM25",
            "analyzer": "suggest"
          }
        }
      }
    }
  }
}'


curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "title": "Just testing the suggestions api"
}'

curl -XPOST "http://localhost:9200/test/test?pretty&amp;refresh" -d '{
  "title": "An other title"
}'

sleep 5

curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "suggest": {
    "text": "tetsting sugestion",
    "phrase":{
      "phrase":{
        "field": "title",
        "max_errors": 5
      }
    }
  }
}'
```
</comment><comment author="dadoonet" created="2013-08-09T09:59:54Z" id="22385578">Sorry. I was wrong.

You have two issues here:
- In your script, you need to add a refresh after the last index command
- You have 5 shards but only 2 documents. Suggestion on other empty shards does not work which cause the error you are seeing.

The following script is working fine:

``` sh
curl -XDELETE "http://localhost:9200/test?pretty"
curl -XPOST "http://localhost:9200/test?pretty" -d '{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "analysis":{
        "analyzer":{
          "suggest":{
            "type": "custom",
            "tokenizer": "standard",
            "filter": [ "standard", "lowercase", "suggest_shingle" ]
          }
        },
        "filter":{
          "suggest_shingle":{
            "type": "shingle",
            "min_shingle_size": 2,
            "max_shingle_size": 5,
            "output_unigrams": true
          }
        }
      }
    }
  }
}'

curl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d '{
  "test": {
 "properties" : {
    "title": {
      "path": "just_name",
      "type": "multi_field",
      "fields": 
        {
           "title": {
            "type": "string",
            "index": "analyzed",
            "similarity": "BM25",
            "analyzer": "suggest"
          }
        }
      }
    }
  }
}'


curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "title": "Just testing the suggestions api"
}'

curl -XPOST "http://localhost:9200/test/test?pretty&amp;refresh" -d '{
  "title": "An other title"
}'

curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "suggest": {
    "text": "tetsting sugestion",
    "phrase":{
      "phrase":{
        "field": "title",
        "max_errors": 5
      }
    }
  }
}'
```

I keep this issue opened to see if we can not _fail_ in case of empty shards.
</comment><comment author="mathieu007" created="2013-08-09T14:29:28Z" id="22397915">Ah , yeah your right, i understand now why i received this error, it make sense...
Thanks for pointing me the errors in my script, will remember that for next time.

Math
</comment><comment author="nik9000" created="2013-08-09T15:09:38Z" id="22400600">I've been noticing that suggestions fail on empty shards problem for the past few days.  If no one gets to it the in next few days I'll have a crack at fixing the errors on empty shards.
</comment><comment author="dadoonet" created="2013-08-09T15:19:01Z" id="22401201">Closing this one as there is now issue #3473 and PR #3475 relative to it.
</comment><comment author="s1monw" created="2013-08-16T11:17:12Z" id="22760503">reopening since this is really a different issue than having an entirely empty shard. here we can have no value in a field rather than having no docs. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/phrase/DirectCandidateGenerator.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/LaplaceScorer.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/LinearInterpoatingScorer.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/StupidBackoffScorer.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/WordScorer.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/SuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/unit/search/suggest/phrase/NoisyChannelSpellCheckerTests.java</file></files><comments><comment>Prevent Phrase Suggester from failing on missing fields.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java</file><file>src/main/java/org/elasticsearch/search/suggest/Suggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/term/TermSuggester.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CustomSuggester.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/SuggestSearchTests.java</file></files><comments><comment>Suggest should ignore empty shards</comment><comment>From #3469.</comment><comment>When running suggest on empty shards, it raises an error like:</comment></comments></commit></commits></item><item><title>null_value mapped field missing in search hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3468</link><project id="" key="" /><description>As the following curl gist illustrates: 

https://gist.github.com/btiernay/6185862

Any field with a `null` value is dropped from the search hit even though it is mapped with `null_value`
</description><key id="17815987">3468</key><summary>null_value mapped field missing in search hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">btiernay</reporter><labels /><created>2013-08-08T16:26:23Z</created><updated>2014-08-08T14:12:27Z</updated><resolved>2014-08-08T14:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T14:12:27Z" id="51607088">The null_value is added to the index, but when you use `fields` to retrieve the field value, it just returns non-null values from the `_source`.

If you make the field stored, then it will return your null_value substitute as well.  Alternatively, use `_source=` to retrieve the actual value in the source (which will be your actual array, including the `null`)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make ScriptDocValues.Strings mockable in test code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3467</link><project id="" key="" /><description /><key id="17814383">3467</key><summary>Make ScriptDocValues.Strings mockable in test code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">sol</reporter><labels><label>feedback_needed</label></labels><created>2013-08-08T15:56:59Z</created><updated>2014-08-08T09:42:32Z</updated><resolved>2014-08-08T09:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sol" created="2013-08-08T16:10:50Z" id="22334800">I'm currently porting a plugin form `0.20.6` and the corresponding test mock some fields.  All the other ScriptDocValues are not final, but `Strings` are.  Is there a special reason for that?
</comment><comment author="clintongormley" created="2014-08-08T08:24:49Z" id="51575484">Hi @sol 

Sorry it has taken a while to look at this.  Is this change still relevant?  
</comment><comment author="sol" created="2014-08-08T09:19:47Z" id="51580155">I'm not sure what I did to work around this back then (I guess just removed the corresponding test).  But I assume if somebody wants to mock this in the future, it may be relevant.  I do not need it right now.
</comment><comment author="clintongormley" created="2014-08-08T09:42:32Z" id="51582067">thanks for letting me know
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support fuzzy queries in CompletionSuggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3466</link><project id="" key="" /><description>Added the FuzzySuggester in order to support completion queries

The following options have been added for the fuxxy suggester
- edit_distance: Maximum edit distance
- transpositions: Sets if transpositions should be counted as one or two changes
- min_prefix_len: Minimum length of the input before fuzzy suggestions are returned
- non_prefix_len: Minimum length of the input, which is not checked for fuzzy alternatives

Closes #3465
</description><key id="17811443">3466</key><summary>Support fuzzy queries in CompletionSuggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-08T15:02:26Z</created><updated>2014-06-18T03:43:28Z</updated><resolved>2013-08-12T13:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-08T15:20:40Z" id="22330985">looks pretty good IMO. Can you check if we are consistent with other 'length' settings like in the phrase suggester where we have `prefix_length` I think it's `prefix_len`
</comment><comment author="spinscale" created="2013-08-09T07:43:20Z" id="22379834">I changed the `length` to `len` and added a small feature useful for the REST API. One can now specify `fuzzy: true` or `fuzzy: {}`. The first one cannot be extended and uses all the defaults, where as the second can be configured optionally with the above parameters

```
curl -X DELETE localhost:9200/completion
curl -X PUT localhost:9200/completion

curl -X PUT localhost:9200/completion/foo/_mapping -d '{
  "foo" : {
    "properties" : {
      "completion" : {
        "type" :"completion"
      }
    }
  }
}'

curl -X PUT localhost:9200/completion/foo/1 -d '{ "completion" : "Nirvana" }'

curl -X POST 'localhost:9200/completion/_suggest?pretty' -d '{
    "cmpl" : {
        "text" : "norv",
        "completion" : {
            "field" : "completion",
            "fuzzy": true
        }
    }
}'

curl -X POST 'localhost:9200/completion/_suggest?pretty' -d '{
    "cmpl" : {
        "text" : "nirw",
        "completion" : {
            "field" : "completion",
            "fuzzy": {}
        }
    }
}'

curl -X POST 'localhost:9200/completion/_suggest?pretty' -d '{
    "cmpl" : {
        "text" : "no",
        "completion" : {
            "field" : "completion",
            "fuzzy": true
        }
    }
}'

curl -X POST 'localhost:9200/completion/_suggest?pretty' -d '{
    "cmpl" : {
        "text" : "no",
        "completion" : {
            "field" : "completion",
            "fuzzy": {
              "min_prefix_len":2
            }
        }
    }
}'
```
</comment><comment author="s1monw" created="2013-08-12T12:34:36Z" id="22489864">LGTM +1 to push good stuff!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support FuzzySuggester for completion suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3465</link><project id="" key="" /><description>Right now the completion suggester uses a custom version of the analyzing suggester. In order to improve completion queries we should support the fuzzy suggester as well.
</description><key id="17811164">3465</key><summary>Support FuzzySuggester for completion suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-08T14:57:28Z</created><updated>2013-08-12T13:17:43Z</updated><resolved>2013-08-12T13:17:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-12T12:35:29Z" id="22489909">I added the `0.90.4` flag I think we should backport it too!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestParser.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggester.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionBuilder.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionContext.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionSuggestionFuzzyBuilder.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/test/integration/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Support fuzzy queries in CompletionSuggest</comment></comments></commit></commits></item><item><title>Inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3464</link><project id="" key="" /><description>Scoring in `function_score` and `filters_function_score` queries is potentially inconsistent when using scripts.  
## Brief overview of related classes

`function_score` and `filters_function_score` allow a user to modify the score of a query (referred to as 'subQueryScore' from here on).  

In brief, there are two classes that compute scores based on a query and some function: `FunctionScoreQuery`, which only has one function and `FiltersFunctionScoreQuery` which combines the result of several functions. For both classes, the function can be a `ScriptScoreFunction`.

`ScoreFunction`: Computes a score for a document. The two relevant methods are:
-  `score(docId, subQueryScore)`: computes a new score taking into account the `subQueryScore` and some other properties of a documents.
- `factor(docId)`: computes a score solely based on properties of the document.

`FunctionScoreQuery.score()` computes:

score = `subQueryBoost` \* `ScoreFunction.score(docId, subQueryScore)`

`FiltersFunctionScoreQuery.CustomBoostFactorScorer.score()` computes: 

score = `subQueryScore` \* `subQueryBoost` \* `combine(ScoreFunction1.factor(docId), ScoreFunction2.factor(docId),…)`

where combine can mean add, multiply, lowest value, etc.
## The problem

`ScoreFunctions.factor(docId)` implies that the method computes a factor only and does not take into account the `subQueryScore`. This is the way the `ScoreFunctions` are used in `FiltersFunctionScoreQuery.CustomBoostFactorScorer`: The method `factor()` is called for each score function and the result is than later multiplied to the `subQueryScore`.

However, the `ScriptScoreFunction` violates this principle: scripts can use the `_score` variable which should be initialized with the `subQueryScore` before the script is run, see `ScriptScoreFunction.score(..)`. If `ScriptScoreFunction.factor()` is called, then the behavior is undefined, since the `_score` variable is either wrong or maybe even not initialized.

This might cause unexpected behavior since this inconsistency is not transparent to the user.
</description><key id="17808513">3464</key><summary>Inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-08T14:12:22Z</created><updated>2013-08-23T14:15:26Z</updated><resolved>2013-08-23T14:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-08-09T12:44:43Z" id="22391727">Backported function_score (Issue #3423, commits 720b550..a938bd5, 534299a and 8774c46, 32cdddb) to 0.90.
</comment><comment author="s1monw" created="2013-08-09T12:45:24Z" id="22391756">@brwe cool! thanks for recording those commit IDs
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/test/integration/search/customscore/CustomScoreSearchTests.java</file></files><comments><comment>Fix inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQuery</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/CombineFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/test/integration/search/customscore/CustomScoreSearchTests.java</file></files><comments><comment>Fix inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQuery</comment></comments></commit></commits></item><item><title>plugin script does not exit with exit code != 0 on error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3463</link><project id="" key="" /><description>Steps to reproduce:

```
$ sudo /usr/share/elasticsearch/bin/plugin -install foo
-&gt; Installing foo...
Failed to install foo, reason: failed to download out of all possible locations..., use -verbose to get detailed information
$ echo $?
0
```

Expected result: The exit code should be != 0

Actual result: The exit code is 0
</description><key id="17803503">3463</key><summary>plugin script does not exit with exit code != 0 on error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">sol</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-08T12:18:34Z</created><updated>2013-09-06T13:53:13Z</updated><resolved>2013-08-08T15:49:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-08T13:10:01Z" id="22322132">the output with the patch is:

```
$ ./bin/plugin -install foo
-&gt; Installing foo...
Failed to install foo, reason: failed to download out of all possible locations..., use -verbose to get detailed information
$ echo $?
74
```
</comment><comment author="sol" created="2013-08-08T15:55:58Z" id="22333646">Awesome, thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/test/integration/plugin/PluginManagerTests.java</file></files><comments><comment>Plugin Manager: add silent mode.</comment><comment>Now with have proper exit codes for elasticsearch plugin manager (see #3463), we can add a silent mode to plugin manager.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file></files><comments><comment>Use nonzero status code to signal abnormal termination</comment></comments></commit></commits></item><item><title>Fix debian init script to not depend on new start-stop-daemon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3462</link><project id="" key="" /><description>By making use of the lsb provided functions, one does not depend on the start-stop daemon version to test if elasticsearch is running.
This ensures, that the init script works on debian wheezy, squeeze, current ubuntu and LTS versions.

Closes #3452
</description><key id="17796896">3462</key><summary>Fix debian init script to not depend on new start-stop-daemon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-08T09:29:08Z</created><updated>2014-06-12T11:32:38Z</updated><resolved>2013-08-12T13:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose IndexWriter#setUseCompundFile via Engine settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3461</link><project id="" key="" /><description>Lucene 4.4 shipped with a fundamental change in how the decision on when to write compound files is made. During segment flush the compound files are written by default which solely relies on a flag in the IndexWriterConfig. The merge policy has been factored out to only make decisions on merges and not on IW flushes. The default now is always writing CFS on flush to reduce resource usage like open files etc. if segments are flushed regularly. While providing a senseable default certain users / usecases might need to change this setting if re-packing flushed segments into CFS is not desired.
</description><key id="17795258">3461</key><summary>Expose IndexWriter#setUseCompundFile via Engine settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-08T08:45:46Z</created><updated>2013-08-08T12:21:34Z</updated><resolved>2013-08-08T12:21:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/test/java/org/elasticsearch/test/unit/index/engine/robin/RobinEngineTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/engine/robin/SimpleRobinEngineTests.java</file></files><comments><comment>Expose 'index.compound_on_flush' via engine settings</comment></comments></commit></commits></item><item><title>Document that 50% of system memory is a good default for ES_HEAP_SIZE in production</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3460</link><project id="" key="" /><description>I'm new to ElasticSearch. I just installed ElasticSearch, inserted a bunch of documents, and tried to query it. Performance was horrible. I learned on the mailing list that a reasonable default for ES_HEAP_SIZE is 50% of system memory and that a much smaller value is used by default. It would be very nice if operational best practices were documented somewhere on the ElasticSearch site. It's hard to know as a beginner where to look to see what settings have defaults which are not suitable for production use. If I stick millions of records into MongoDB or MySQL with the default settings then they do not fall over even if they are not tuned exactly for my particular use case. It would be great for introducing people to ElasticSearch if it were as similarly easy to get started with.
</description><key id="17787347">3460</key><summary>Document that 50% of system memory is a good default for ES_HEAP_SIZE in production</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">benmccann</reporter><labels /><created>2013-08-08T04:30:04Z</created><updated>2014-07-28T09:52:15Z</updated><resolved>2014-07-28T09:52:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="benmccann" created="2013-08-21T00:22:20Z" id="22987755">Any thoughts about merging this change?
</comment><comment author="clintongormley" created="2014-05-06T11:51:46Z" id="42293240">/cc @spinscale 
</comment><comment author="spinscale" created="2014-07-28T09:52:15Z" id="50319577">Hey,

I dont think it is a good idea to put this here. First, this invites people to do something automatically (even though it does not the way you added), which is supposed to be configured statically IMO. People should always know how much HEAP they configure.

If you think, it's not well documented, that the heap should be set to 50% of main memory, lets fix the documentation instead and not the init script, which only fixes this for a specific package for a specific distribution and does not catch everything. This should be part of the puppet/chef configuration, which is created by the user.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change logging level for "updating index_buffer_size from" from DEBUG to TRACE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3459</link><project id="" key="" /><description>I notice that with logging level debug, most of the log(more than 75%) is filled with messages for 'updating index_buffer_size from'. Being so verbose, it makes looking for other DEBUG messages harder. Thus It would be nice to have these messages as TRACE logging level. 
</description><key id="17773378">3459</key><summary>Change logging level for "updating index_buffer_size from" from DEBUG to TRACE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajhalani</reporter><labels><label>discuss</label></labels><created>2013-08-07T20:53:10Z</created><updated>2014-08-08T13:58:07Z</updated><resolved>2014-08-08T13:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-11-04T18:53:45Z" id="27711736">Did you set the log level DEBUG for a specific package or just for the root logger?
</comment><comment author="clintongormley" created="2014-08-08T13:58:07Z" id="51605089">No feedback after 10 months. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow Ids query to score documents by ids order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3458</link><project id="" key="" /><description>_This is more a feature suggestion than an issue._

[Id's query](http://www.elasticsearch.org/guide/reference/query-dsl/ids-query/) are great and allow to use ES as a great searchable datastore.

I have some use cases where I want to restrict user search to a limited set of documents - so I use Ids query with some match queries.

But sometime, I don't have query, I just want my N documents, without sorting on a field: and in this case, all my documents get a score of 1. 

If I run this query multiple times, I get different order almost everytime, as score are equals.

``` json
{
    "query": {
            "ids": {
                "values": [
                   "1221","5","6","7","8","9","10"
                ]
            }
    }
}
```

This is not great for users (as they get a random feeling), and I may also want to use the id values order as document order. So I created a custom script query for this case:

``` json
{
  "query": {
    "custom_score": {
      "query": {
        "ids": {
          "type": "pony",
          "values": [
            "1337",
            "1664",
            "8888",
            "1111"
          ]
        }
      },
      "script": "
        count = ids.size();
        id    = org.elasticsearch.index.mapper.Uid.idFromUid(doc['_uid'].value);
        for (i = 0; i &lt; count; i++) {
          if (id == ids[i]) { return count - i; }
        }"
      "params": {
        "ids": [
          "1337",
          "1664",
          "8888",
          "1111"
        ]
      },
      "lang": "mvel"
    }
  }
}
```

As you can see, I inject the ids to the script as a param, and give them a custom score based on the position of the current document ID in the list.

This fix consistency and ordering issues, but this is **slow** when dealing with lots of ID's (started noticing when hitting 3k ids).

What I was thinking about is some kind of option we can add to IdsQuery to score docs based on the Id position.

``` json
{
    "query": {
            "ids": {
                "values": [
                   "1221","5","6","7","8","9","10"
                ],
                "score_by_order": true
            }
    }
}
```

With this value to "true", the IdsQuery could give a score to each document, removing the random effect and the need of a custom script to sort by id's.

What do you think?!
Thanks!
</description><key id="17751500">3458</key><summary>Allow Ids query to score documents by ids order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienalexandre</reporter><labels><label>discuss</label></labels><created>2013-08-07T14:38:04Z</created><updated>2017-05-15T15:53:51Z</updated><resolved>2014-08-08T13:57:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T11:48:04Z" id="26589564">If the problem is having consistent ordering of your documents and you only have the ids filter (or query), I'd suggest to switch to the [multi_get api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-multi-get.html). In that case you would get back the documents in the same order you have put the id in your request. Also, get and multi_get are a better fit when using elasticsearch as a storage as they are real-time, while search is only (Near) real-time, which means that a refresh needs to happen in order to make newly indexed documents searchable (a refresh happens automatically every second by default though).

Otherwise, if you do need a query and want to use the search API, can't you just sort your documents by `_id`? The issue you may encounter there is that the [`_id` field](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-id-field.html) is not indexed by default, but you can change its mapping or use the [`_uid` field](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-uid-field.html) instead, which contains type+id and it is indexed by default, thus it can be used for sorting out of the box.

Let me know if this helps and maybe next time (if you haven't done it yet) can you send a question to the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch) just to double check that you tried all the options you have? 
</comment><comment author="damienalexandre" created="2013-10-18T12:19:37Z" id="26591217">Thank for the reply :)
- Multi get api **can't run facets** or other ES Query powerful features - having the feature in the ID's query would allow a lot of possibilities.
- Ordering by _id/_uid is not a solution, you can have non linear _id (like hashes from an url shortener...), and also I want my documents in the order I request them, it can be random.

PS: Here is the related discussion in the ES ML: https://groups.google.com/d/topic/elasticsearch/QQ8RXyMD4fM/discussion
</comment><comment author="javanna" created="2013-10-18T12:49:57Z" id="26592845">Thanks for your quick feeback, I see what you mean!

I think a custom script is the way to go here, as it's really your own logic and not something really common. I'd suggest to have a look at [script sorting](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_script_based_sorting) though. In fact, you need to infuence the way the score is computed because you are sorting by score, but if you are able to express your sorting logic as a script, you can just sort based on it, that's it.
</comment><comment author="damienalexandre" created="2013-10-18T13:18:44Z" id="26594581">That's exactly what I do (see the second example in my issue);
I use the list of ID to compute the score of the document. But **using score is painfully slow on large dataset**, that's why I opened this issue: asking the community if I'm the only one who need this as a feature (a new option in the ID Query) or not :grimacing:
</comment><comment author="javanna" created="2013-10-18T13:24:50Z" id="26595017">Got it, what I suggested to do is different to custom_score, although still executes a script per document. Have a look at [script sorting](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_script_based_sorting).
</comment><comment author="felixbarny" created="2014-05-15T11:59:50Z" id="43200771">I'd love to have a `score_by_order` as well
</comment><comment author="darklow" created="2014-06-11T12:12:19Z" id="45733057">Unfortunately by some reason function `idFromUid` was recently removed and this solution doesn't work anymore with latest ElasticSearch version.
Maybe @martijnvg could comment why was this method removed and are there alternatives?
https://github.com/elasticsearch/elasticsearch/commit/0e780b7e99ac1af46d9f0f4a8b04517ef2a0cec2#diff-376fdeb0c8f420de09933212c022341cL97

Maybe someone else knows how to get this feature to work again?
Thank you.
</comment><comment author="darklow" created="2014-06-11T12:17:18Z" id="45733431">Actually i just tried using `doc['id'].value` instead of `org.elasticsearch.index.mapper.Uid.idFromUid(doc['_uid'].value);` and looks like everything works fine too. Don't even know why `idFromUid` was used in this solution in first place.

```
"script": "return -ids.indexOf(Integer.parseInt(doc['id'].value));"
```
</comment><comment author="felixbarny" created="2014-06-11T15:42:44Z" id="45759040">My script just looks like this: `ids.indexOf(doc['id'].value)`
</comment><comment author="clintongormley" created="2014-08-08T13:57:28Z" id="51604909">Closing as won't fix.
</comment><comment author="itsjavi" created="2017-04-13T14:24:28Z" id="293910466">I would still consider supporting an alternative for this, @clintongormley 

It's pity that after 3 years ES haven't provide a reliable alternative for these situations where we need to keep the order of the documents that have been requested + apply search filters.

Scripting is not an option for serious and large-scale applications and anyway we cannot use `expression` for this I guess, which is in theory more performant.

It would be enough to be able to sort by position in an array of values, provided in the ES request, like:

```
{
 "sort": [{
    "_position": {
       "field_to_compare_values_with": [1201, 982, 34134]
    }
  }]
}
```

Similar to the  way`_geo_distance` is used. Benefits compared to the first proposal:
- Would be compatible with any kind of search supporting `sort`
- As you can see, we don't need to mess up with scores.
- It will work with any field, not only ids.
- This is useful when you want to keep a fixed/constant sorting and you still need, for example, to ES to calculate and return the distance in geospatial searches.

If you reconsider it, I could open this in a new feature-request ticket.</comment><comment author="Xophe" created="2017-05-15T15:53:51Z" id="301518535">I suggest you use a script like this :
`switch(doc['id'].value){case "1337":return 0;case "1664":return 1;case "8888":return 2;case "1111":return 3;}`
By avoiding the lookup, it will speed up your script execution.
You can also use a hashmap.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bad sorting by subfields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3457</link><project id="" key="" /><description>I have a problem with sorting. My mapping looks like this:

```
"a" : {
              "path" : "just_name",
              "properties" : {
                "content" : {
                  "type" : "string",
                  "analyzer" : "standard",
                  "include_in_all" : false
                },
                "sortValue" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "omit_norms" : true,
                  "index_options" : "docs",
                  "include_in_all" : false
                }
              }
            },
"b" : {
              "path" : "just_name",
              "properties" : {
                "content" : {
                  "type" : "string",
                  "analyzer" : "standard",
                  "include_in_all" : false
                },
                "sortValue" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "omit_norms" : true,
                  "index_options" : "docs",
                  "include_in_all" : false
                }
              }
            }
```

Now when I get documents sorted by "a.sortValue" I will get results that have values from b.sortValue in the SearchHit.getSortValues(). 
Searching by a.content and b.content works just fine and only matches the content in one field as expected. Also if I remove the "path" : "just_name", the sorting is done correctly. But once I set path to just_name, it seems that sorting only cares about the last part of the path (searchValue). 

This is with elasticsearch version 0.90.2
</description><key id="17749353">3457</key><summary>Bad sorting by subfields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tandres19</reporter><labels><label>non-issue</label></labels><created>2013-08-07T13:58:23Z</created><updated>2013-11-08T17:59:00Z</updated><resolved>2013-08-10T09:44:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-09T13:55:47Z" id="22395695">Using "just_name" effectively means that you want the field to be indexed on the top level using just its name. That means that the field will be indexed with `sortValue` name instead of `a.sortValue`. Both your `a.sortValue` and `b.sortValue` will then go to the same `sortValue` common field, therefore your sort behaviour is the expected one. Even if you refer explicitly to `a.sortValue` and `b.sortValue`, elasticsearch maps that name internally to the physical index name, which is `sortValue` in both cases. If you want to keep the distinction between the two fields you should use the default `"path": "full"`. Have a look [here](http://www.elasticsearch.org/guide/reference/mapping/object-type/) for some more examples on full path or just_name.

What I find strange here is the search behaviour that you mentioned. You mean that you get different matches for the same query either querying `a.sortValue` or `b.sortValue`? Would you mind posting a [curl recreation](http://www.elasticsearch.org/help/)?
</comment><comment author="tandres19" created="2013-08-10T09:44:07Z" id="22437029">Thanks for the answer. I misunderstood just_name a bit then (might be good to have an example in the docu with just_name and fields with the same name). I thought it just allowed to use partial path to address it instead of always using just the final path segment.

I noticed now, that the searching didn't behave as expected as well. Our test cases found the correct document by accident rather than by design...

Is there some other way to address fields with partial path? (some wildcard to use in the path or something) I have a hierarchic document structure with children of the same type as the parent and would like to search in a specific field accross all children. That works with just_name, but broke with the substructre I used for sorting. 

Thanks for your help.
</comment><comment author="javanna" created="2013-08-11T10:30:30Z" id="22455651">We have a pretty good example of how "just_name" works compared to "full" [here](http://www.elasticsearch.org/guide/reference/mapping/object-type/) (path section).

@tandres19 I guess it would be good to know what queries you're making and knowing a little more about your usecase. Anyways, our [google group](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch) is a better place for this kind of questions. It would nice if you can send a message there and we'll definitely try to help.
</comment><comment author="tandres19" created="2013-08-12T07:39:29Z" id="22477346">I've seen the path example. What I didn't get from it, was that subfields with the same name can't be distinguished anymore with just_name set. So an example to show this would be nice.

By now we changed our mapping to avoid this substructure for sort values and add them as an additional field which works fine.
Thanks again for you help. I'll check out the group later on when I've got the time :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for left joining nested queries and filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3456</link><project id="" key="" /><description>## Context

Assume you have a set of documents for which you have defined a [nested mapping](http://www.elasticsearch.org/guide/reference/mapping/nested-type/). Also assume that some documents have nested documents and some do not:

**Document 1**

``` json
{
  "id":1,
  "nested":[
    {
      "x":1
    }
  ]
}
```

**Document 2**

``` json
{
  "id":2,
  "nested":[
    {
      "x":2
    }
  ]
}
```

**Document 3**

``` json
{
  "id":3
}
```
## Problem

There is currently no way to perform the equivalent of the following SQL query:

``` sql
SELECT
    r.id,
    COUNT(*)
FROM
    root r
    LEFT JOIN
    nested n
        ON parent(n) = r
WHERE
    n.x = 1 /* nested condition */
GROUP BY
    r.id
```

Something that comes very close using ElasticSearch's query DSL is:

``` bash
curl -XGET http://localhost:9200/index/root/_search -d'
{
   "fields":[ "id" ],
   "query":{
       "nested":{
          "query":{
              "constant_score":{
                 "query":{
                    "term":{"x":1}
                 }
              },
              "boost":1.0
           }
        },
        "path":"nested",
        "score_mode":"total"
      }
   }
}'
```

However, since the semantics of [nested queries](http://www.elasticsearch.org/guide/reference/query-dsl/nested-query/) and [nested filters](http://www.elasticsearch.org/guide/reference/query-dsl/nested-filter/) require a document to have [at least one nested document](https://groups.google.com/forum/m/#!topic/elasticsearch/Jlk7RgkhBG0), **Document 3** would be filtered out here (i.e. "inner-join" semantics). 
## Application

To note above is the condition on the nested documents. Although simple in this example, one can easily imagine situations where score based aggregations based on multiple dynamic conditions can not be precomputed in advance due to a combinatorial explosion.

The main value in the above query is that it includes results that have a score value of "0" and preserves a global document score sort and thus can not be computed using [facets](http://www.elasticsearch.org/guide/reference/java-api/facets/) or the forthcoming [Aggregation Module](https://github.com/elasticsearch/elasticsearch/issues/3300).
## Resources
- https://github.com/elasticsearch/elasticsearch/issues/3056
- https://groups.google.com/forum/m/#!topic/elasticsearch/cZhIcZ7rxsY
- https://groups.google.com/forum/m/#!topic/elasticsearch/Jlk7RgkhBG0
- http://stackoverflow.com/questions/18199135/is-it-possible-to-left-join-nested-documents-in-elasticsearch-queries
</description><key id="17747763">3456</key><summary>Add support for left joining nested queries and filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">btiernay</reporter><labels /><created>2013-08-07T13:28:53Z</created><updated>2014-08-08T12:12:05Z</updated><resolved>2014-08-08T12:12:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-08-08T14:22:02Z" id="22326668">Just a thought I had was that this appears to be the nested analog of `null_value` for simple fields. One possible implementation strategy would be to insert a "null value" nested doc to represent its absence. This would allow the current join strategy to continue functioning. However, ideally this could be avoided and instead done closer to the Lucene level to reduce storage and perhaps get other query efficiency gains. On the other hand, "null value" may be simpler to implement.
</comment><comment author="martijnvg" created="2013-08-27T20:47:38Z" id="23369362">What about the following query:

``` json
{
   "fields":[ "id" ],
   "query":{
       "bool" : {
           "should" : [
              {"match_all" : {}},
              {
                  "nested":{
                      "query":{
                           "constant_score":{
                                "query":{
                                    "term":{"x":1}
                                }
                          }
                     },
                     "path":"nested",
                     "score_mode":"total"
                  }
              }
           ]
       }
   }
}
```

This would also include docs with no nested docs, docs with nested would be scored higher. I think it makes sense to expose an option that just include docs with no nested docs.
</comment><comment author="btiernay" created="2013-08-27T21:44:35Z" id="23373504">Hey Martijn. 

I too had the same idea. However, I realized since `should` isn't mutually exclusive it will not filter out documents which have nested docs which _do not_ match the nested criteria. For example, it would also match:

``` json
{
  "id":2,
  "nested":[
    {
      "x":2
    }
  ]
}
```

in my previous example above, which would be incorrect.
</comment><comment author="martijnvg" created="2013-08-28T13:09:47Z" id="23412417">Right, another way to solve it would be to use the following query:

``` json
{
   "fields":[ "id" ],
   "query":{
       "bool" : {
           "should" : [
              {
                "bool" : {
                    "not" : [
                        {
                            "nested":{
                                "query":{
                                    "match_all": {}  
                                },
                                "path":"nested",
                                "score_mode":"total"
                          }    
                        }
                    ]
                  }
              },
              {
                  "nested":{
                      "query":{
                           "constant_score":{
                                "query":{
                                    "term":{"x":1}
                                }
                          }
                     },
                     "path":"nested",
                     "score_mode":"total"
                  }
              }
           ]
       }
   }
}
```

So the nested query in the bool should clause would only accept docs that don't have any nested docs. Looking at this query again it really look bloated and if we would implement this 'left join' / 'optional join' in the nested query it wouldn't be bloated and I guess would also perform better.
</comment><comment author="btiernay" created="2013-08-29T00:38:22Z" id="23459618">Hey Martijn, yeah this actually would work, but...

&gt; Looking at this query again it really look bloated and if we would implement this 'left join' / 'optional join' in the nested query it wouldn't be bloated and I guess would also perform better.

I think the two points you mentioned are bang on:
- readability / understandability
- performance

It's nice to know there is a way though. Thanks for taking the time to craft the example. 

I'd be curious to know if the `not` sub-expression would yield a score of 0 above. I suppose you could wrap that in a `custom_score`  (see https://github.com/elasticsearch/elasticsearch/issues/3058) if that wasn't the case. 

Also not sure if `disable_coord` (see https://github.com/elasticsearch/elasticsearch/issues/3056) would be required above as well.
</comment><comment author="martijnvg" created="2013-08-29T17:36:15Z" id="23507770">The `not` clause doesn't score, the score in this case determined by the should clause.
The `disable_coord` is only applicable to `should` clauses (the more matching, the more important a doc is), so in this case it doesn't matter. 
</comment><comment author="btiernay" created="2013-08-30T23:03:03Z" id="23594007">@martijnvg Thanks for the explanation. I actually just went through the ES training so this is all much clearer now :)
</comment><comment author="clintongormley" created="2014-07-23T13:48:56Z" id="49875552">Relates to #3495 
</comment><comment author="clintongormley" created="2014-08-08T12:12:05Z" id="51593127">Closed in favour of #3495 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"term" vs "terms" inconsistency in term query/facet_filter ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3455</link><project id="" key="" /><description>v0.90.2
When using the terms query API, one should use "terms" when searching for multiple terms.

This query works:

``` javascript
"query": {
  "terms": {
    "user": [
      "brianclozel",
      "elasticsearch"
    ]
  }
}
```

This query throws an exception, because an array is given, but an "s" is missing in "term":

``` javascript
"query": {
  "term": {
    "user": [
      "brianclozel",
      "elasticsearch"
    ]
  }
}
```

When doing the same thing with a facet filter, we get a different behavior.

This query works as expected:

``` javascript
"facets": {
  "followers": {
    "statistical": {
      "field": "followers"
    },
    "facet_filter": {
      "terms": {
        "user": [
          "elasticsearch",
          "brianclozel"
        ]
      }
    }
  },
}
```

With an "s" missing in "term", this query does not raise an exception but takes only the first element in the array to apply the filter.

``` javascript
"facets": {
  "followers": {
    "statistical": {
      "field": "followers"
    },
    "facet_filter": {
      "term": {
        "user": [
          "elasticsearch",
          "brianclozel"
        ]
      }
    }
  },
}
```

Is that behavior intended?
</description><key id="17746462">3455</key><summary>"term" vs "terms" inconsistency in term query/facet_filter ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bclozel</reporter><labels /><created>2013-08-07T13:00:42Z</created><updated>2013-08-12T15:56:39Z</updated><resolved>2013-08-12T15:56:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-12T15:40:30Z" id="22502494">At first glance the difference is caused by the fact you're using a term/terms query in the first place, while in the facet you're using a term/terms filter. Even though their json is the same, they get parsed differently as you noticed, and there is quite some difference between a query and a filter in the way it gets executed in lucene.

On the other hand I can't reproduce the problem you described using the term query. The query doesn't get properly parsed when providing an array but I don't get back any error. What version of elasticsearch are you using?
</comment><comment author="bclozel" created="2013-08-12T15:56:37Z" id="22503670">I'm using v0.90.2.
I can't reproduce this issue; both "term" and "terms" are working on a facet_filter. I was querying ElasticSearch with raw JSON with the head plugin.

You can close this issue now, I'll repoen it if I ever trigger this again. Sorry about that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve filtering by _parent field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3454</link><project id="" key="" /><description>In the _parent field the type and id of the parent are stored as type#id, because of this a term filter on the _parent field with the parent id is always resolved to a terms filter with a type / id combination for each type in the mapping.

This can be improved by automatically use the most optimized filter (either term or terms) based on the number of parent types in the mapping.

Also add support to use the parent type in the term filter for the _parent field. Like this:

``` json
{
   "term" : {
        "_parent" : "parent_type#1"
    }
}
```

This will then always automatically use the term filter.
</description><key id="17740702">3454</key><summary>Improve filtering by _parent field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-07T10:42:21Z</created><updated>2013-08-07T11:20:26Z</updated><resolved>2013-08-07T11:20:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/test/java/org/elasticsearch/test/integration/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Added improvements for terms filter on _parent field similar to what has been for term filter.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/test/java/org/elasticsearch/test/integration/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Improved filtering by _parent field</comment></comments></commit></commits></item><item><title>MLT returns all documents if non of the fields in the document are supported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3453</link><project id="" key="" /><description>Today due the optimizations in the boolean query builder we adjust
a pure negative query with a 'match_all'. This is not the desired
behavior in the MLT API if all the fields in a document are unsupported ie. numeric fields etc. If that happens today we return all documents but the one MLT is 
executed on.
</description><key id="17739884">3453</key><summary>MLT returns all documents if non of the fields in the document are supported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-07T10:19:48Z</created><updated>2013-08-07T11:25:24Z</updated><resolved>2013-08-07T11:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/test/java/org/elasticsearch/test/integration/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>Return nothing instead of everything in MLT if no field is supported.</comment></comments></commit></commits></item><item><title>Older version of start-stop-daemon don't support --status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3452</link><project id="" key="" /><description>Older version of start-stop-daemon don't support --status so the new init.d script throws an error on Ubuntu 10.04

```
service elasticsearch restart
 * Starting ElasticSearch Server                                                                                                                                                   start-stop-daemon: unrecognized option '--status'
Try 'start-stop-daemon --help' for more information.
start-stop-daemon: unrecognized option '--status'
Try 'start-stop-daemon --help' for more information.
```

Once started running service elasticsearch stop doesn't stop the service it has to be killed manually. 
</description><key id="17706170">3452</key><summary>Older version of start-stop-daemon don't support --status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rwky</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-06T18:23:02Z</created><updated>2013-09-09T14:00:48Z</updated><resolved>2013-08-12T13:05:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-07T07:22:28Z" id="22233982">Valid point. I'll see how to work around that. If I got it right 10.04 is an LTS release, so it makes sense to work on that, right?
</comment><comment author="rwky" created="2013-08-07T08:00:58Z" id="22235622">Yep 10.04 is LTS it's supported until 2015.
</comment><comment author="spinscale" created="2013-08-08T12:06:57Z" id="22318896">hey

created a PR for this. It would be a great help if you could test the init script in your ubuntu installation and see if it works. Just replace the current one with this:

https://raw.github.com/spinscale/elasticsearch/issue-3452-debian-init-script/src/deb/init.d/elasticsearch

Thanks a lot!
</comment><comment author="rwky" created="2013-08-08T12:15:04Z" id="22319242">Yep that works great!
</comment><comment author="ThomasTr" created="2013-08-11T12:50:55Z" id="22457233">Hi,

works also great on Debian 6 :)
</comment><comment author="njam" created="2013-08-26T13:51:56Z" id="23263703">fyi this is not in `0.90.3.2`, downgrading to `0.90.2` helped.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fix debian init script to not depend on new start-stop-daemon</comment></comments></commit></commits></item><item><title>Add highlighting support for suggester.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3451</link><project id="" key="" /><description>This implements highlighting suggestions (closes #3442) by wrapping changed tokens in user supplied pre_tag and post_tag.  This is a total rewrite of my first shot at this (pull request #3443).
</description><key id="17695299">3451</key><summary>Add highlighting support for suggester.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-08-06T15:07:53Z</created><updated>2014-08-05T11:45:54Z</updated><resolved>2013-08-06T19:12:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-06T15:58:23Z" id="22189984">this looks awesome! lemme run the tests!
</comment><comment author="nik9000" created="2013-08-06T16:17:54Z" id="22191323">Durpy me - I kept telling myself I'd run the whole suite as soon as I submitted the pull request but then forgot.  Looks like there are some more tests for me to investigate:
- org.elasticsearch.test.unit.action.suggest.SuggestActionTests.testMarvelHerosPhraseSuggest
- org.elasticsearch.test.integration.search.suggest.SuggestSearchTests.testMarvelHerosPhraseSuggest

I'll amend my pull request when I figure out what is going on there.
</comment><comment author="s1monw" created="2013-08-06T16:29:10Z" id="22192109">Suggest.java line 557 

```
  public void writeTo(StreamOutput out) throws IOException {
    out.writeText(text);
    out.writeOptionalText(text);
    out.writeFloat(score);
 }
```

the second optional text needs to be `highlighted`
</comment><comment author="s1monw" created="2013-08-06T16:32:06Z" id="22192297">so I have only one question, if we have some highlight like `&lt;em&gt;foo&lt;/em&gt; &lt;em&gt;bar&lt;/em&gt;` should this be rather `&lt;em&gt;foo bar&lt;/em&gt;`? what do you think
</comment><comment author="nik9000" created="2013-08-06T16:33:46Z" id="22192409">Just caught it too.  I'm amending.  While I have your attention, what about SuggestSearchBenchMark?  It doesn't perform any phrase searching.  Is it worth expanding it?  I don't have a ton of experience with benchmarking in java but I remember reading somewhere that it is way harder than you'd expect it to be.
</comment><comment author="s1monw" created="2013-08-06T16:34:31Z" id="22192444">we don't have a benchmark in the repo for this. don't worry about it for now.
</comment><comment author="nik9000" created="2013-08-06T16:36:27Z" id="22192578">&gt; Suggest.java line 557
&gt; 
&gt;   public void writeTo(StreamOutput out) throws IOException {
&gt;     out.writeText(text);
&gt;     out.writeOptionalText(text);
&gt;     out.writeFloat(score);
&gt;  }
&gt; 
&gt; the second optional text needs to be highlighted

Fixed.
</comment><comment author="s1monw" created="2013-08-06T16:45:15Z" id="22193143">Thanks for fixing it so quickly, I think we should make this highlight entrire phrases rather than individual tokens. like `&lt;em&gt;foo&lt;/em&gt; &lt;em&gt;bar&lt;/em&gt;` should become `&lt;em&gt;foo bar&lt;/em&gt;` since if we wanna do this for prefix suggestions this is more consistent. We need to consider that just because this would be rendered identically in HTML this doesn't necessarily make it the right thing to do. Can you fix this? Other than that I'd wanna pull it in!
</comment><comment author="nik9000" created="2013-08-06T17:15:03Z" id="22195116">&gt; Thanks for fixing it so quickly, I think we should make this highlight entrire phrases rather than individual tokens. like &lt;em&gt;foo&lt;/em&gt; &lt;em&gt;bar&lt;/em&gt; should become &lt;em&gt;foo bar&lt;/em&gt; since if we wanna do this for prefix suggestions this is more consistent. We need to consider that just because this would be rendered identically in HTML this doesn't necessarily make it the right thing to do. Can you fix this? Other than that I'd wanna pull it in!

I think that last commit does what you ask for.  It does make the highlights prettier which I like even though I don't strictly need it.
</comment><comment author="nik9000" created="2013-08-06T17:33:54Z" id="22196411">Rebased.
</comment><comment author="s1monw" created="2013-08-06T19:12:08Z" id="22203285">pushed thanks!
</comment><comment author="FabianKoestring" created="2014-08-05T08:03:00Z" id="51162721">Is this documented anywhere? I cant figure out how to use it?
</comment><comment author="clintongormley" created="2014-08-05T11:45:54Z" id="51186137">@FabianKoestring yes it's documented on the phrase-suggest page file:///Users/clinton/workspace/docs/html/en/elasticsearch/reference/current/search-suggesters-phrase.html

look for "highlight"
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Configurable sort order for missing string values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3450</link><project id="" key="" /><description>This commit allows for configuring the sort order of missing values in BytesRef
comparators (used for strings) with the following options:
- _first: missing values will appear in the first positions,
- _last: missing values will appear in the last positions (default),
- &lt;any value&gt;: documents with missing sort value will use the given value when
  sorting.

Since the default is _last, sorting by string value will have a different
behavior than in previous versions of elasticsearch which used to insert missing
value in the first positions when sorting in ascending order.

Implementation notes:
- Specialized BytesRefOrdValComparators have been removed now that the ordinals
  rely on packed arrays instead of raw arrays,
- Field data tests hierarchy has been changed so that the numeric tests don't
  inherit from the string tests anymore,
- When _first or _last is used, internally the comparators are told to use
  null or UnicodeUtil.BIG_TERM to replace missing values (depending on the sort
  order),
- BytesRefValComparator just replaces missing values with the provided value
  and uses them for comparisons,
- BytesRefOrdValComparator multiplies ordinals by 4 so that it can find
  ordinals for the missing value and the bottom value which are directly
  comparable with the segment ordinals. For example, if the segment values and
  ordinals are (a,1) and (b,2), they will be stored internally as (a,4) and
  (b,8) and if the missing value is 'ab', it will be assigned 6 as an ordinal,
  since it is between 'a' and 'b'. Then if the bottom value is 'abc', it will
  be assigned 7 as an ordinal since if it between 'ab' and 'b'.

Closes #896
</description><key id="17684766">3450</key><summary>Configurable sort order for missing string values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-08-06T11:22:42Z</created><updated>2017-02-14T16:32:27Z</updated><resolved>2013-10-18T20:50:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-08-07T12:57:22Z" id="22248872">I reviewed the pull request and things look really good. A couple of points:
- It seems that some unit tests for missing value (!=_last &amp; !=_first) of field data got lost (talked to @jpountz about it)
- Nested documents are not supported.
- The JSON response is a bit tricky: we always return the values by which we sorted on every search hit. When using _last / _first for missing, we return the actual value using for sorting. With integers we return Integer.MAX_VALUE (and similar things for other numerical data)   but now we may return `UnicodeUtil.BIG_TERM`: 

```
{
   "_index": "test",
   "_type": "type",
   "_id": "5",
   "_score": null,
   "_source": {
      "i": 1
   },
   "sort": [
      "����������"
   ]
},
```

I think this is tricky as it may break systems which are not unicode compliant. Ideally I would say we need to return null, but that is a breaking change when it comes to numeric data and requires more thought. Potentially we want to push as is and make this a different feature. 
</comment><comment author="jpountz" created="2013-08-08T13:48:48Z" id="22324505">I updated to branch to address your concerns:
- nested string sorting now supports missing values (with tests for both BytesRefValComparator and BytesRefOrdValComparator),
- missing tests have been revived out of my git reflog,
- now uses Character.MAX_CODE_POINT (valid UTF-8) instead of UnicodeUtil.BIG_TERM.
</comment><comment author="samuelneff" created="2017-02-14T16:32:27Z" id="279759292">Am I reading this correct that this pull request was closed without merge or comment? Was this officially rejected? I like the functionality but am mostly concerned with the change in default behavior of nulls last.

Thanks,

Sam
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improved error message when the mapping document is malformed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3449</link><project id="" key="" /><description /><key id="17683133">3449</key><summary>Improved error message when the mapping document is malformed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">manuelbernhardt</reporter><labels /><created>2013-08-06T10:34:47Z</created><updated>2014-06-13T00:35:32Z</updated><resolved>2013-08-07T12:06:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-07T12:06:12Z" id="22246299">pushed! thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Null pointer exceptions when bulk updates max out their retry on conflict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3448</link><project id="" key="" /><description>Reported on the mailing list:

```
[2013-08-04 21:48:54,972][WARN ][cluster.action.shard     ] [es003] sending failed shard for [samples][20], node[COpFaS8rRhSulgDPAq5xxg], [R], s[STARTED], reason [Failed to perform [bulk/shard] on replica, message [RemoteTransportException; nested: ResponseHandlerFailureTransportException; nested: NullPointerException; ]]
[2013-08-04 21:48:55,028][WARN ][action.bulk              ] [es003] Failed to perform bulk/shard on replica [samples][82]
org.elasticsearch.transport.RemoteTransportException
Caused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException
Caused by: java.lang.NullPointerException
  at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:247)
  at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:242)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4.finishIfPossible(TransportShardReplicationOperationAction.java:693)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4.handleResponse(TransportShardReplicationOperationAction.java:679)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4.handleResponse(TransportShardReplicationOperationAction.java:676)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:153)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:662)
[2013-08-04 21:48:55,029][WARN ][cluster.action.shard     ] [es003] sending failed shard for [samples][82], node[aUq8CNUeT_iEIkJ7rVw02w], [R], s[STARTED], reason [Failed to perform [bulk/shard] on replica, message [RemoteTransportException; nested: ResponseHandlerFailureTransportException; nested: NullPointerException; ]]

```
</description><key id="17682335">3448</key><summary>Null pointer exceptions when bulk updates max out their retry on conflict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-06T10:13:15Z</created><updated>2013-08-06T17:04:22Z</updated><resolved>2013-08-06T17:02:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>src/test/java/org/elasticsearch/test/integration/document/BulkTests.java</file><file>src/test/java/org/elasticsearch/test/integration/update/UpdateTests.java</file></files><comments><comment>Maxing out retries on conflict in bulk update cause null pointer exceptions</comment></comments></commit></commits></item></channel></rss>